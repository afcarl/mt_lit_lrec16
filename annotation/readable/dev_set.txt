<SURVEYSTART>
id:  W13-0801
title: A Semantic Evaluation of Machine Translation Lexical Choice
link: http://www.aclweb.org/anthology/W13-0801
first_name: Matic
last_name: Horvat
goal #1 : find better MT evaluation metric
tech #1 : add better rule context
------data------
1	train	bitext	Europarl	900k sentences	Spanish	English
2	train	bitext	WMT2012		Spanish	English
3	tune	bitext	WMT2012	3000	Spanish	English
4	test	bitext	SemEval2010 Cross-Lingual WSD task		Spanish	English
=====result=====
1	1,2,3	4	PBSMT	Precision	23.72
1	1,2,3	4	PBSMT	Recall	23.69
1	1,2,3	4	PBSMT	BLEU	62.72
1	1,2,3	4	WSD	Precision	25.96
1	1,2,3	4	WSD	Recall	25.58
1	1,2,3	4	WSD	BLEU	76.06
diff_metric:
extra:
<SURVEYEND>
<SURVEYSTART>
id:  P14-1072
title: Kneser-Ney Smoothing on Expected Counts
link: http://www.aclweb.org/anthology/P14-1072
first_name: Hui
last_name: Zhang
goal #1 : improve word/phrase alignment
tech #1 : reduce overfitting
------data------
1	train	bitext	NIST 2009 constrained task	5M sentences	Arabic	English	All data was used except for: United Nations pro- ceedings (LDC2004E1
2	tune	bitext	NIST 2008		Arabic	English
3	test	bitext	NIST 2009		Arabic	English
=====result=====
1	1	3	none (baseline)	BLEU	37.0
1	1	3	expected KN	BLEU	38.2
diff_metric:
extra:
<SURVEYEND>
<SURVEYSTART>
id:  P10-1062
title: Error Detection for Statistical Machine Translation Using Linguistic Features
link: http://www.aclweb.org/anthology/P10-1062
first_name: Eunsol
last_name: Choi
goal #1 : post edit results
tech #1 : improve post-processing
------data------
1	train	bitext	MT-02	878 sentences	Chinese	English
2	tune	bitext	MT-05	1082 sentences	Chinese	English
3	test	bitext	MT-03	919 sentences	Chinese	English
=====result=====
1	1,2	3	Baseline	other	47.57
1	1,2	3	MaxEnt (dwpp + wd + pos + link)	other	38.76
diff_metric:CER (%)
extra:
<SURVEYEND>
<SURVEYSTART>
id:  P10-1049
title: Training Phrase Translation Models with Leaving-One-Out
link: http://www.aclweb.org/anthology/P10-1049
first_name: Kevin
last_name: Knight
goal #1 : improve translation quality
tech #1 : reduce overfitting
------data------
1	train	bitext	WMT08	1 311 815 sentences	German	English
2	tune	bitext	WMT08	2 000 sentences	German	English
3	test	bitext	WMT08	2 000 sentences	German	English
4	train	monolingual	WMT08			English
=====result=====
1	1,4	2	baseline	BLEU	25.7
1	1,4	2	fixed interpol.	BLEU	27.0
1	1,4	3	baseline	BLEU	26.3
1	1,4	3	fixed interpol.	BLEU	27.7
diff_metric:
extra:
<SURVEYEND>
<SURVEYSTART>
id:  W08-0306
title: Using Syntax to Improve Word Alignment Precision for Syntax-Based Machine Translation
link: http://www.aclweb.org/anthology/W08-0306
first_name: Kevin
last_name: Knight
goal #1 : improve word/phrase alignment
tech #1 : improve syntax
------data------
1	train	bitext	unknown	400 sentences	Chinese	English
2	test	bitext	unknown	400 sentences	Chinese	English
3	train	bitext	unknown	1500 sentences	Chinese	English
4	test	bitext	unknown	1500 sentences	Chinese	English
5	train	bitext	unknown	1500 sentences	Arabic	English
6	test	bitext	unknown	1500 sentences	Arabic	English
7	train	bitext	unknown	9.8m words	Chinese	English
8	tune	bitext	NIST02	25.9k words	Chinese	English
9	test	bitext	NIST03	29.0k words	Chinese	English
10	train	bitext	unknown	12.3m words	Chinese	English
11	tune	bitext	unknown	42.9k words	Chinese	English
12	test	bitext	unknown	42.1k words	Chinese	English
13	train	bitext	unknown	174.8m words	Arabic	English
14	tune	bitext	NIST04-05	35.8k words	Arabic	English
15	test	bitext	NIST04-05	40.3k words	Arabic	English
16	test	bitext	unknown	53.0k words	Arabic	English
=====result=====
1	1	2	GIZA++ union	F1	63.44
1	1	2	Link deletion	F1	75.14
1	3	4	GIZA++ union	F1	47.16
1	3	4	Link deletion	F1	62.24
1	5	6	GIZA++ union	F1	73.87
1	5	6	Link deletion	F1	75.85
1	7	9	GIZA++ union	BLEU	41.17
1	7	9	Link deletion	BLEU	41.93
1	10	12	GIZA++ union	BLEU	41.39
1	10	12	Link deletion	BLEU	41.88
1	13	12	GIZA++ union	BLEU	54.73
1	13	12	Link deletion	BLEU	55.57
1	13	12	GIZA++ union	BLEU	50.9
1	13	12	Link deletion	BLEU	51.08
1	13	12	GIZA++ union	BLEU	38.16
1	13	12	Link deletion	BLEU	38.72
diff_metric:
extra:
<SURVEYEND>