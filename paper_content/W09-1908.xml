<document>
  <filename>W09-1908</filename>
  <authors>
    <author>Vamshi Ambati</author>
  </authors>
  <title>Proactive Learning for Building Machine Translation Systems for Minority Languages</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>Building machine translation (MT) for many minority languages in the world is a serious challenge. For many minor languages there is little machine readable text, few knowledgeable linguists, and little money available for MT development. For these reasons, it becomes very important for an MT system to make best use of its resources, both labeled and unlabeled, in building a quality system. In this paper we argue that traditional active learning setup may not be the right fit for seeking annotations required for building a Syntax Based MT system for minority languages. We posit that a relatively new variant of active learning, Proactive Learning, is more suitable for this task.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Building machine translation (MT) for many minority languages in the world is a serious challenge.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For many minor languages there is little machine readable text, few knowledgeable linguists, and little money available for MT development.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For these reasons, it becomes very important for an MT system to make best use of its resources, both labeled and unlabeled, in building a quality system.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In this paper we argue that traditional active learning setup may not be the right fit for seeking annotations required for building a Syntax Based MT system for minority languages.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We posit that a relatively new variant of active learning, Proactive Learning, is more suitable for this task.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Speakers of minority languages could benefit from fluent machine translation (MT) between their native tongue and the dominant language of their region. But scarcity in capital and know-how has largely restricted machine translation to the dominant languages of first world nations. To lower the barriers surrounding MT system creation, we must reduce the time and resources needed to develop MT for new language pairs. Syntax based MT has proven to be a good choice for minority language scenario (Lavie et al., 2003). While the amount of parallel data required to build such systems is orders of magnitude smaller than corresponding phrase based statistical systems (Koehn et al., 2003), the variety of linguistic annotation required is greater. Syntax based MT systems require lexicons that provide coverage for the target translations, synchronous grammar rules that define the divergences in word-order across the language-pair. In case of minority languages one can only expect to find meagre amount of such data, if any. Building such resources effectively, within a constrained budget, and deploying an MT system is the need of the day.
We first consider &#8216;Active Learning&#8217; (AL) as a framework for building annotated data for the task of MT. However, AL relies on unrealistic assumptions related to the annotation tasks. For instance, AL assumes there is a unique omniscient oracle. In MT, it is possible and more general to have multiple sources of information with differing reliabilities or areas of expertise. A literate bilingual speaker with no extra training can produce translations for word, phrase or sentences and even align them. But it requires a trained linguist to produce syntactic parse trees. AL also assumes that the single oracle is perfect, always providing a correct answer when requested. In reality, an oracle (human or machine) may be incorrect (fallible) with a probability that should be a function of the difficulty of the question. There is also no notion of cost associated with the annotation task, that varies across the input space. But in MT, it is easy to see that length of a sentence and cost of translation are superlinear. Also not all annotation tasks for MT have the same level of difficulty or cost. For example, it is relatively cheap to ask a bilingual speaker whether a word, phrase or sentence was correctly translated by the system, but a bit more expensive to ask for a correction. Assumptions like these render active learning unsuit-
able for our task at hand which is building an MT system for languages with limited resources. We make the case for &#8220;Proactive Learning&#8221; (Donmez and Carbonell, 2008) as a solution for this scenario.
In the rest of the paper, we discuss syntax based MT approach in Section 2. In Section 3 we first discuss active learning approaches for MT and detail the characteristics of MT for minority languages problem that render traditional active learning unsuitable for practical purposes. In Section 4 we discuss proactive learning as a potential solution for the current problem. We conclude with some challenges that still remain in applying proactive learning for MT.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Speakers of minority languages could benefit from fluent machine translation (MT) between their native tongue and the dominant language of their region.</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>But scarcity in capital and know-how has largely restricted machine translation to the dominant languages of first world nations.</text>
              <doc_id>6</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>To lower the barriers surrounding MT system creation, we must reduce the time and resources needed to develop MT for new language pairs.</text>
              <doc_id>7</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Syntax based MT has proven to be a good choice for minority language scenario (Lavie et al., 2003).</text>
              <doc_id>8</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>While the amount of parallel data required to build such systems is orders of magnitude smaller than corresponding phrase based statistical systems (Koehn et al., 2003), the variety of linguistic annotation required is greater.</text>
              <doc_id>9</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Syntax based MT systems require lexicons that provide coverage for the target translations, synchronous grammar rules that define the divergences in word-order across the language-pair.</text>
              <doc_id>10</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In case of minority languages one can only expect to find meagre amount of such data, if any.</text>
              <doc_id>11</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Building such resources effectively, within a constrained budget, and deploying an MT system is the need of the day.</text>
              <doc_id>12</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We first consider &#8216;Active Learning&#8217; (AL) as a framework for building annotated data for the task of MT.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, AL relies on unrealistic assumptions related to the annotation tasks.</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For instance, AL assumes there is a unique omniscient oracle.</text>
              <doc_id>15</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In MT, it is possible and more general to have multiple sources of information with differing reliabilities or areas of expertise.</text>
              <doc_id>16</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>A literate bilingual speaker with no extra training can produce translations for word, phrase or sentences and even align them.</text>
              <doc_id>17</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>But it requires a trained linguist to produce syntactic parse trees.</text>
              <doc_id>18</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>AL also assumes that the single oracle is perfect, always providing a correct answer when requested.</text>
              <doc_id>19</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In reality, an oracle (human or machine) may be incorrect (fallible) with a probability that should be a function of the difficulty of the question.</text>
              <doc_id>20</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>There is also no notion of cost associated with the annotation task, that varies across the input space.</text>
              <doc_id>21</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>But in MT, it is easy to see that length of a sentence and cost of translation are superlinear.</text>
              <doc_id>22</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Also not all annotation tasks for MT have the same level of difficulty or cost.</text>
              <doc_id>23</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>For example, it is relatively cheap to ask a bilingual speaker whether a word, phrase or sentence was correctly translated by the system, but a bit more expensive to ask for a correction.</text>
              <doc_id>24</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Assumptions like these render active learning unsuit-</text>
              <doc_id>25</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>able for our task at hand which is building an MT system for languages with limited resources.</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We make the case for &#8220;Proactive Learning&#8221; (Donmez and Carbonell, 2008) as a solution for this scenario.</text>
              <doc_id>27</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the rest of the paper, we discuss syntax based MT approach in Section 2.</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Section 3 we first discuss active learning approaches for MT and detail the characteristics of MT for minority languages problem that render traditional active learning unsuitable for practical purposes.</text>
              <doc_id>29</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In Section 4 we discuss proactive learning as a potential solution for the current problem.</text>
              <doc_id>30</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We conclude with some challenges that still remain in applying proactive learning for MT.</text>
              <doc_id>31</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Syntax Based Machine Translation</title>
        <text>In recent years, corpus based approaches to machine translation have become predominant, with Phrase Based Statistical Machine Translation (PB- SMT) (Koehn et al., 2003) being the most actively progressing area. Recent research in syntax based machine translation (Yamada and Knight, 2001; Chiang, 2005) incorporates syntactic information to ameliorate the reordering problem faced by PB-SMT approaches. While traditional approaches to syntax based MT were dependent on availability of manual grammar, more recent approaches operate within the resources of PB-SMT and induce hierarchical or linguistic grammars from existing phrasal units, to provide better generality and structure for reordering (Yamada and Knight, 2001; Chiang, 2005; Wu, 1997).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In recent years, corpus based approaches to machine translation have become predominant, with Phrase Based Statistical Machine Translation (PB- SMT) (Koehn et al., 2003) being the most actively progressing area.</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Recent research in syntax based machine translation (Yamada and Knight, 2001; Chiang, 2005) incorporates syntactic information to ameliorate the reordering problem faced by PB-SMT approaches.</text>
              <doc_id>33</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>While traditional approaches to syntax based MT were dependent on availability of manual grammar, more recent approaches operate within the resources of PB-SMT and induce hierarchical or linguistic grammars from existing phrasal units, to provide better generality and structure for reordering (Yamada and Knight, 2001; Chiang, 2005; Wu, 1997).</text>
              <doc_id>34</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Resources for Syntax MT</title>
            <text>Syntax based approaches to MT seek to leverage the structure of natural language to automatically induce MT systems. Depending upon the MT system and the paradigm, the resource requirements may vary and could also include modules such as morphological analyzers, sense disambiguation modules, generators etc. A detailed discussion of the comprehensive pipeline, may be out of the scope of this paper, more so because such resources can not be expected in a low-resource language scenario. We only focus on the quintessential set of modules for MT pipeline - data acquisition, word-alignment, syntactic analysis etc. The resources can broadly be categorized as &#8216;monolingual&#8217; vs &#8216;bilingual&#8217; depending upon whether it requires knowledge in one language or both languages for annotation. A sample of the different kinds of data and annotation that is expected by an MT system is shown below. Each of the additional information can be seen as extra annotations for the &#8216;Source&#8217; sentence. The language of target in the example is &#8216;Hindi&#8217;.
&#8226; Source: John ate an apple
&#8226; Target: John ne ek seb khaya
&#8226; Alignment: (1,1),(2,5),(3,3),(4,4)
&#8226; SourceParse: (S (NP (NNP John)) (VP (VBD ate) (NP (DT an) (NN apple))))
&#8226; Lexicon: (seb &#8594; apple),(ate &#8594; khaya)
&#8226; Grammar: VP: V NP &#8594; NP V</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Syntax based approaches to MT seek to leverage the structure of natural language to automatically induce MT systems.</text>
                  <doc_id>35</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Depending upon the MT system and the paradigm, the resource requirements may vary and could also include modules such as morphological analyzers, sense disambiguation modules, generators etc. A detailed discussion of the comprehensive pipeline, may be out of the scope of this paper, more so because such resources can not be expected in a low-resource language scenario.</text>
                  <doc_id>36</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We only focus on the quintessential set of modules for MT pipeline - data acquisition, word-alignment, syntactic analysis etc.</text>
                  <doc_id>37</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The resources can broadly be categorized as &#8216;monolingual&#8217; vs &#8216;bilingual&#8217; depending upon whether it requires knowledge in one language or both languages for annotation.</text>
                  <doc_id>38</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>A sample of the different kinds of data and annotation that is expected by an MT system is shown below.</text>
                  <doc_id>39</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Each of the additional information can be seen as extra annotations for the &#8216;Source&#8217; sentence.</text>
                  <doc_id>40</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The language of target in the example is &#8216;Hindi&#8217;.</text>
                  <doc_id>41</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Source: John ate an apple</text>
                  <doc_id>42</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Target: John ne ek seb khaya</text>
                  <doc_id>43</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Alignment: (1,1),(2,5),(3,3),(4,4)</text>
                  <doc_id>44</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; SourceParse: (S (NP (NNP John)) (VP (VBD ate) (NP (DT an) (NN apple))))</text>
                  <doc_id>45</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Lexicon: (seb &#8594; apple),(ate &#8594; khaya)</text>
                  <doc_id>46</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Grammar: VP: V NP &#8594; NP V</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Active Learning for MT</title>
        <text>Modern syntax based MT rides on the success of both Statistical Machine Translation and Statistical Parsing. Active learning has been applied to Statistical Parsing (Hwa, 2004; Baldridge and Osborne, 2003) to improve sample selection for manual annotation. In case of MT, active learning has remained largely unexplored. Some attempts include training multiple statistical MT systems on varying amounts of data, and exploring a committee based selection for re-ranking the data to be translated and included for re-training. But this does not apply to training in a low-resource scenario where data is scarce.
In the rest of the section we discuss the different scenarios that arise in gathering of annotation for MT under a traditional &#8216;active learning&#8217; setup and discuss the characteristics of the task that render it difficult.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Modern syntax based MT rides on the success of both Statistical Machine Translation and Statistical Parsing.</text>
              <doc_id>48</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Active learning has been applied to Statistical Parsing (Hwa, 2004; Baldridge and Osborne, 2003) to improve sample selection for manual annotation.</text>
              <doc_id>49</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In case of MT, active learning has remained largely unexplored.</text>
              <doc_id>50</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Some attempts include training multiple statistical MT systems on varying amounts of data, and exploring a committee based selection for re-ranking the data to be translated and included for re-training.</text>
              <doc_id>51</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>But this does not apply to training in a low-resource scenario where data is scarce.</text>
              <doc_id>52</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the rest of the section we discuss the different scenarios that arise in gathering of annotation for MT under a traditional &#8216;active learning&#8217; setup and discuss the characteristics of the task that render it difficult.</text>
              <doc_id>53</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Multiple Oracles</title>
            <text>For each of the sub-tasks of annotation, in reality we have multiple sources of information or multiple oracles. We can elicit translations for building a parallel corpus from bilingual speakers who speak both the languages with certain accuracy or from a linguist who is well educated in the formal sense of the languages. With the success of collaborative sites like Amazon&#8217;s &#8216;Mechanical Turk&#8217; 1 , one
1 http://www.mturk.com/
can provide the task of annotation to multiple oracles on the internet (Snow et al., 2008). The task of word alignment can be posed in a similar fashion too. More interestingly, there are statistical tools like GIZA 2 that take as input un-annotated parallel data and propose automatic correspondences between words in the language-pair, giving scope to &#8216;machine oracles&#8217;.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>For each of the sub-tasks of annotation, in reality we have multiple sources of information or multiple oracles.</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We can elicit translations for building a parallel corpus from bilingual speakers who speak both the languages with certain accuracy or from a linguist who is well educated in the formal sense of the languages.</text>
                  <doc_id>55</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>With the success of collaborative sites like Amazon&#8217;s &#8216;Mechanical Turk&#8217; 1 , one</text>
                  <doc_id>56</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 http://www.mturk.com/</text>
                  <doc_id>57</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>can provide the task of annotation to multiple oracles on the internet (Snow et al., 2008).</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The task of word alignment can be posed in a similar fashion too.</text>
                  <doc_id>59</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>More interestingly, there are statistical tools like GIZA 2 that take as input un-annotated parallel data and propose automatic correspondences between words in the language-pair, giving scope to &#8216;machine oracles&#8217;.</text>
                  <doc_id>60</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Varying Quality and Reliability</title>
            <text>Oracles also vary on the correctness of the answers they provide (quality) as well as their availability (robustness) to answer. One typical distinction is &#8216;human oracles&#8217; vs &#8216;machine oracles&#8217;. Human oracle produce higher quality annotations when compared to a machine oracle. We would prefer a tree bank of parse trees that were manually created over automatically generated tree banks. Similar is the case with word-alignment and other tasks of translation. Some oracles are &#8216;reluctant&#8217; to produce an output, for example parsers tend to break on really long sentences, but when they produce an output we can associate some confidence with it about the quality. One can expect a human oracle to produce parse trees for long sentences, but the quality could be questionable.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Oracles also vary on the correctness of the answers they provide (quality) as well as their availability (robustness) to answer.</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>One typical distinction is &#8216;human oracles&#8217; vs &#8216;machine oracles&#8217;.</text>
                  <doc_id>62</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Human oracle produce higher quality annotations when compared to a machine oracle.</text>
                  <doc_id>63</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We would prefer a tree bank of parse trees that were manually created over automatically generated tree banks.</text>
                  <doc_id>64</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Similar is the case with word-alignment and other tasks of translation.</text>
                  <doc_id>65</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Some oracles are &#8216;reluctant&#8217; to produce an output, for example parsers tend to break on really long sentences, but when they produce an output we can associate some confidence with it about the quality.</text>
                  <doc_id>66</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>One can expect a human oracle to produce parse trees for long sentences, but the quality could be questionable.</text>
                  <doc_id>67</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Non-uniform costs</title>
            <text>Each of the annotation tasks has a non-uniform cost associated with it, the distribution of which is dependent upon the difficulty over the input space. Clearly, length of the sentence is a good indicator of the cost. It takes much longer to translate a sentence of 100 words than to translate one with 10 words. It takes at least twice as long to create word-alignment correspondences for a sentence-pair with 40 tokens than a pair with 20 tokens. Similarly, a human takes much longer to manually create parse tree for a long sentence than a short sentence.
It is also the case that not all oracles have the same non-uniform cost distribution over the input space. Some oracles are more expensive than the others. For example a practicing linguist&#8217;s time is perhaps costlier than that of an undergraduate who is a bilingual speaker. As noticed above, this may reflect upon the quality of annotation for the task,
2 http://www.fjoch.com/GIZA++.html
but sometimes a tradeoff to make is cost vs quality. We can not afford to introduce a grammar rule of low-quality into the system, but can possibly do away with an incorrect word-correspondence link.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Each of the annotation tasks has a non-uniform cost associated with it, the distribution of which is dependent upon the difficulty over the input space.</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Clearly, length of the sentence is a good indicator of the cost.</text>
                  <doc_id>69</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It takes much longer to translate a sentence of 100 words than to translate one with 10 words.</text>
                  <doc_id>70</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>It takes at least twice as long to create word-alignment correspondences for a sentence-pair with 40 tokens than a pair with 20 tokens.</text>
                  <doc_id>71</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Similarly, a human takes much longer to manually create parse tree for a long sentence than a short sentence.</text>
                  <doc_id>72</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It is also the case that not all oracles have the same non-uniform cost distribution over the input space.</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Some oracles are more expensive than the others.</text>
                  <doc_id>74</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For example a practicing linguist&#8217;s time is perhaps costlier than that of an undergraduate who is a bilingual speaker.</text>
                  <doc_id>75</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>As noticed above, this may reflect upon the quality of annotation for the task,</text>
                  <doc_id>76</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 http://www.fjoch.com/GIZA++.html</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>but sometimes a tradeoff to make is cost vs quality.</text>
                  <doc_id>78</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We can not afford to introduce a grammar rule of low-quality into the system, but can possibly do away with an incorrect word-correspondence link.</text>
                  <doc_id>79</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Proactive Learning</title>
        <text>Proactive learning (Donmez and Carbonell, 2008) is a generalization of active learning designed to relax unrealistic assumptions and thereby reach practical applications. Active learning seeks to select the most informative unlabeled instances and ask an omniscient oracle for their labels, so as to retrain the learning algorithm maximizing accuracy. However, the oracle is assumed to be infallible (never wrong), indefatigable (always answers), individual (only one oracle), and insensitive to costs (always free or always charges the same). Proactive learning relaxes all these four assumptions, relying on a decisiontheoretic approach to jointly select the optimal oracle and instance, by casting the problem as a utility optimization problem subject to a budget constraint.
maximize E[V (S)] subject to B max S&#8712;UL E[V (S)] &#8722; &#955;( &#8721; t k &#8727; C k )s.t
k
&#8721; t k &#8727; C k = B
The above equation can be interpreted as maximizing the expected value of labeling the input set S under the budget constraint B. The subscript k denotes the oracle from which the answer was elicited under a cost function C. A greedy approximation of the above results in the equation 1, where E k [V (x)] is the expected value of information of the example x corresponding to oracle k. One can design interesting functions that calculate V (x) in case of MT. For example, selecting short sentences with an unresolved linguistic issue could maximize the utility of the data at a low cost.
(x&#8727;, k&#8727;) = argmax x&#8712;U E k [V (x)] subject to B (1)
We now turn to how proactive learning framework helps solve the issues raised for active learning in MT in section 3. We can address the issue of multiple oracles where one oracle is fallible or reluctant to answer, by factoring into Equation 2 its probability
k
function for returning an answer. The score returned by such a factoring can be called the utility associated with that input for a particular oracle. We call this U(x, k). A similar factorization can be done in order to address the issue of oracles that are fallible.
Since we do not have the P (ans/x, k) distribution information for each oracle, proactive learning proposes to discover this in a discovery phase under some allocated budget B d . Once we have an estimate from the discovery phase, the rest of the labeling proceeds according to the optimization function. For more details of the algorithms refer (Donmez and Carbonell, 2008). Finally, we can also relax the assumption of uniform cost per annotation, but replacing the C k term in the above equations with a C non&#8722;unifk function denoting the non-uniform cost function associated with the oracle.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Proactive learning (Donmez and Carbonell, 2008) is a generalization of active learning designed to relax unrealistic assumptions and thereby reach practical applications.</text>
              <doc_id>80</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Active learning seeks to select the most informative unlabeled instances and ask an omniscient oracle for their labels, so as to retrain the learning algorithm maximizing accuracy.</text>
              <doc_id>81</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, the oracle is assumed to be infallible (never wrong), indefatigable (always answers), individual (only one oracle), and insensitive to costs (always free or always charges the same).</text>
              <doc_id>82</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Proactive learning relaxes all these four assumptions, relying on a decisiontheoretic approach to jointly select the optimal oracle and instance, by casting the problem as a utility optimization problem subject to a budget constraint.</text>
              <doc_id>83</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>maximize E[V (S)] subject to B max S&#8712;UL E[V (S)] &#8722; &#955;( &#8721; t k &#8727; C k )s.t</text>
              <doc_id>84</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>k</text>
              <doc_id>85</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8721; t k &#8727; C k = B</text>
              <doc_id>86</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The above equation can be interpreted as maximizing the expected value of labeling the input set S under the budget constraint B.</text>
              <doc_id>87</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The subscript k denotes the oracle from which the answer was elicited under a cost function C.</text>
              <doc_id>88</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A greedy approximation of the above results in the equation 1, where E k [V (x)] is the expected value of information of the example x corresponding to oracle k.</text>
              <doc_id>89</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>One can design interesting functions that calculate V (x) in case of MT.</text>
              <doc_id>90</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>For example, selecting short sentences with an unresolved linguistic issue could maximize the utility of the data at a low cost.</text>
              <doc_id>91</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(x&#8727;, k&#8727;) = argmax x&#8712;U E k [V (x)] subject to B (1)</text>
              <doc_id>92</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We now turn to how proactive learning framework helps solve the issues raised for active learning in MT in section 3.</text>
              <doc_id>93</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We can address the issue of multiple oracles where one oracle is fallible or reluctant to answer, by factoring into Equation 2 its probability</text>
              <doc_id>94</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>k</text>
              <doc_id>95</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>function for returning an answer.</text>
              <doc_id>96</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The score returned by such a factoring can be called the utility associated with that input for a particular oracle.</text>
              <doc_id>97</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We call this U(x, k).</text>
              <doc_id>98</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>A similar factorization can be done in order to address the issue of oracles that are fallible.</text>
              <doc_id>99</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Since we do not have the P (ans/x, k) distribution information for each oracle, proactive learning proposes to discover this in a discovery phase under some allocated budget B d .</text>
              <doc_id>100</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Once we have an estimate from the discovery phase, the rest of the labeling proceeds according to the optimization function.</text>
              <doc_id>101</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For more details of the algorithms refer (Donmez and Carbonell, 2008).</text>
              <doc_id>102</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we can also relax the assumption of uniform cost per annotation, but replacing the C k term in the above equations with a C non&#8722;unifk function denoting the non-uniform cost function associated with the oracle.</text>
              <doc_id>103</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>5 Future Challenges</title>
        <text>While proactive learning is a good framework for building MT systems for minority languages, there are however a few issues that still remain that need careful attention. Joint Utility: In a complex system like MT where different models combine forces to produce the translation we have a situation where we need to optimize not only for an input and the oracle, but also the kind of annotation we would like to elicit. For example given a particular translation model, we do not know if the most optimal thing at a given point is to seek more word-alignment annotation from a particular &#8217;alignment oracle&#8217; or seek parse annotation from a &#8217;parsing oracle&#8217;. Machine oracles vs Human oracles: The assumption with an oracle is that the knowledge and expertise of the oracle does not change over the course of annotation. We do not assume that the oracle learns over time and hence the speed of annotation or perhaps the accuracy of annotation increases. This is however very common with &#8217;machine oracles&#8217;. For example, an oracle that suggests automatic alignment of data using statistical concordances may initially be unreliable due to the less amount of data it is trained on, but as it receives more data, the estimates get better and so the system gets more reliable. Evaluation: Performance of underlying system is typically done by well understood metrics like precision/recall. However, evaluation of MT output is quite subjective and automatic evaluation metrics may be too coarse to distinguish the nuances of translation. This becomes quite important in an online active learning setup, where we add annotated data incrementally, and the immediately trained translation models are not sufficient to make a difference in the scores of the evaluation metric.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>While proactive learning is a good framework for building MT systems for minority languages, there are however a few issues that still remain that need careful attention.</text>
              <doc_id>104</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Joint Utility: In a complex system like MT where different models combine forces to produce the translation we have a situation where we need to optimize not only for an input and the oracle, but also the kind of annotation we would like to elicit.</text>
              <doc_id>105</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For example given a particular translation model, we do not know if the most optimal thing at a given point is to seek more word-alignment annotation from a particular &#8217;alignment oracle&#8217; or seek parse annotation from a &#8217;parsing oracle&#8217;.</text>
              <doc_id>106</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Machine oracles vs Human oracles: The assumption with an oracle is that the knowledge and expertise of the oracle does not change over the course of annotation.</text>
              <doc_id>107</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We do not assume that the oracle learns over time and hence the speed of annotation or perhaps the accuracy of annotation increases.</text>
              <doc_id>108</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This is however very common with &#8217;machine oracles&#8217;.</text>
              <doc_id>109</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>For example, an oracle that suggests automatic alignment of data using statistical concordances may initially be unreliable due to the less amount of data it is trained on, but as it receives more data, the estimates get better and so the system gets more reliable.</text>
              <doc_id>110</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Evaluation: Performance of underlying system is typically done by well understood metrics like precision/recall.</text>
              <doc_id>111</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>However, evaluation of MT output is quite subjective and automatic evaluation metrics may be too coarse to distinguish the nuances of translation.</text>
              <doc_id>112</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>This becomes quite important in an online active learning setup, where we add annotated data incrementally, and the immediately trained translation models are not sufficient to make a difference in the scores of the evaluation metric.</text>
              <doc_id>113</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables/>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Jason Baldridge</author>
          <author>Miles Osborne</author>
        </authors>
        <title>Active learning for hpsg parse selection.</title>
        <publication>In Proc. of the HLTNAACL</publication>
        <pages>17--24</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>A hierarchical phrase-based model for statistical machine translation.</title>
        <publication>In Proc. 43rd ACL,</publication>
        <pages>263--270</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Pinar Donmez</author>
          <author>Jaime G Carbonell</author>
        </authors>
        <title>Proactive learning: cost-sensitive active learning with multiple imperfect oracles.</title>
        <publication>In CIKM &#8217;08,</publication>
        <pages>619--628</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Rebecca Hwa</author>
        </authors>
        <title>Sample selection for statistical parsing.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz Josef Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical phrase-based translation.</title>
        <publication>In Proc. of the HLT/NAACL,</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Alon Lavie</author>
          <author>Stephan Vogel</author>
          <author>Lori Levin</author>
          <author>Erik Peterson</author>
          <author>Katharina Probst</author>
          <author>Ariadna Font Llitj&#243;s</author>
          <author>Rachel Reynolds</author>
          <author>Jaime Carbonell</author>
          <author>Richard Cohen</author>
        </authors>
        <title>Experiments with a hindi-to-english transfer-based mt system under a miserly data scenario.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Rion Snow</author>
          <author>Brendan O&#8217;Connor</author>
          <author>Daniel Jurafsky</author>
          <author>Andrew Ng</author>
        </authors>
        <title>Cheap and fast &#8211; but is it good? evaluating non-expert annotations for natural language tasks.</title>
        <publication>In Proceedings of the EMNLP</publication>
        <pages>254--263</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Dekai Wu</author>
        </authors>
        <title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1997</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Baldridge and Osborne, 2003</string>
        <sentence_id>41938</sentence_id>
        <char_offset>68</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>41909</sentence_id>
        <char_offset>78</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>41910</sentence_id>
        <char_offset>320</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>2</reference_id>
        <string>Donmez and Carbonell, 2008</string>
        <sentence_id>41890</sentence_id>
        <char_offset>43</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>2</reference_id>
        <string>Donmez and Carbonell, 2008</string>
        <sentence_id>41943</sentence_id>
        <char_offset>20</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>2</reference_id>
        <string>Donmez and Carbonell, 2008</string>
        <sentence_id>41965</sentence_id>
        <char_offset>42</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>3</reference_id>
        <string>Hwa, 2004</string>
        <sentence_id>41938</sentence_id>
        <char_offset>57</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>4</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>41872</sentence_id>
        <char_offset>149</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>4</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>41908</sentence_id>
        <char_offset>150</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>5</reference_id>
        <string>Lavie et al., 2003</string>
        <sentence_id>41871</sentence_id>
        <char_offset>79</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>7</reference_id>
        <string>Wu, 1997</string>
        <sentence_id>41910</sentence_id>
        <char_offset>334</char_offset>
      </citation>
    </citations>
  </content>
</document>
