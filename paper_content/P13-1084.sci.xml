<PAPER>
  <FILENO/>
  <TITLE>Statistical Machine Translation Improves Question Retrieval in Community Question Answering via Matrix Factorization</TITLE>
  <AUTHORS>
    <AUTHOR>Guangyou Zhou</AUTHOR>
    <AUTHOR>Fang Liu</AUTHOR>
    <AUTHOR>Yang Liu</AUTHOR>
    <AUTHOR>Shizhu He</AUTHOR>
  </AUTHORS>
  <ABSTRACT>
    <A-S ID="S-37263">Community question answering (CQA) has become an increasingly popular research topic.</A-S>
    <A-S ID="S-37264">In this paper, we focus on the problem of question retrieval.</A-S>
    <A-S ID="S-37265">Question retrieval in CQA can automatically find the most relevant and recent questions that have been solved by other users.</A-S>
    <A-S ID="S-37266">However, the word ambiguity and word mismatch problems bring about new challenges for question retrieval in CQA.</A-S>
    <A-S ID="S-37267">State-of-the-art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models.</A-S>
    <A-S ID="S-37268">While useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question-answer pairs) in the absence of which they are troubled by noise issue.</A-S>
    <A-S ID="S-37269">In this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages.</A-S>
    <A-S ID="S-37270">Our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via matrix factorization.</A-S>
    <A-S ID="S-37271">Experiments conducted on a real CQA data show that our proposed approach is promising.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-37272">With the development of Web 2.0, community question answering (CQA) services like Yahoo!</S>
        <S ID="S-37273">Answers, 1 Baidu Zhidao 2 and WkiAnswers 3 have attracted great attention from both academia and industry (<REF ID="R-12" RPTR="16">Jeon et al., 2005</REF>; <REF ID="R-22" RPTR="26">Xue et al., 2008</REF>; <REF ID="R-00" RPTR="0">Adamic et al., 2008</REF>; <REF ID="R-19" RPTR="23">Wang et al., 2009</REF>; <REF ID="R-05" RPTR="6">Cao et al., 2010</REF>).</S>
        <S ID="S-37274">In CQA, anyone can ask and answer questions on any topic, and people seeking information are connected to those who know the answers.</S>
        <S ID="S-37275">As answers are usually explicitly provided by human, they can be helpful in answering real world questions.</S>
      </P>
      <P>
        <S ID="S-37276">In this paper, we focus on the task of question retrieval.</S>
        <S ID="S-37277">Question retrieval in CQA can automatically find the most relevant and recent questions (historical questions) that have been solved by other users, and then the best answers of these historical questions will be used to answer the users&#8217; queried questions.</S>
        <S ID="S-37278">However, question retrieval is challenging partly due to the word ambiguity and word mismatch between the queried questions and the historical questions in the archives.</S>
        <S ID="S-37279">Word ambiguity often causes the retrieval models to retrieve many historical questions that do not match the users&#8217; intent.</S>
        <S ID="S-37280">This problem is also amplified by the high diversity of questions and users.</S>
        <S ID="S-37281">For example, depending on different users, the word &#8220;interest&#8221; may refer to &#8220;curiosity&#8221;, or &#8220;a charge for borrowing money&#8221;.</S>
      </P>
      <P>
        <S ID="S-37282">Another challenge is word mismatch between the queried questions and the historical questions.</S>
        <S ID="S-37283">The queried questions may contain words that are different from, but related to, the words in the relevant historical questions.</S>
        <S ID="S-37284">For example, if a queried question contains the word &#8220;company&#8221; but a relevant historical question instead contains the word &#8220;firm&#8221;, then there is a mismatch and the historical</S>
      </P>
      <P>
        <S ID="S-37285">1 http://answers.yahoo.com/ 2 http://zhidao.baidu.com/ 3 http://wiki.answers.com/</S>
      </P>
      <P>
        <S ID="S-37286">question may not be easily distinguished from an irrelevant one.</S>
        <S ID="S-37287">Researchers have proposed the use of wordbased translation models (<REF ID="R-01" RPTR="1">Berger et al., 2000</REF>; <REF ID="R-12" RPTR="17">Jeon et al., 2005</REF>; <REF ID="R-22" RPTR="27">Xue et al., 2008</REF>; <REF ID="R-08" RPTR="13">Lee et al., 2008</REF>; <REF ID="R-02" RPTR="2">Bernhard and Gurevych, 2009</REF>) to solve the word mismatch problem.</S>
        <S ID="S-37288">As a principle approach to capture semantic word relations, wordbased translation models are built by using the IBM model 1 (<REF ID="R-03" RPTR="3">Brown et al., 1993</REF>) and have been shown to outperform traditional models (e.g., VSM, BM25, LM) for question retrieval.</S>
        <S ID="S-37289">Besides, <REF ID="R-15" RPTR="19">Riezler et al. (2007)</REF> and <REF ID="R-24" RPTR="31">Zhou et al. (2011)</REF> proposed the phrase-based translation models for question and answer retrieval.</S>
        <S ID="S-37290">The basic idea is to capture the contextual information in modeling the translation of phrases as a whole, thus the word ambiguity problem is somewhat alleviated.</S>
        <S ID="S-37291">However, all these existing studies in the literature are basically monolingual approaches which are restricted to the use of original language of questions.</S>
        <S ID="S-37292">While useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question-answer pairs) in the absence of which they are troubled by noise issue.</S>
        <S ID="S-37293">In this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages.</S>
        <S ID="S-37294">Through other languages, various ways of adding semantic information to a question could be available, thereby leading to potentially more improvements than using the original language only.</S>
      </P>
      <P>
        <S ID="S-37295">Taking a step toward using other languages, we propose the use of translated representation by alternatively enriching the original questions with the words from other languages.</S>
        <S ID="S-37296">The idea of improving question retrieval with statistical machine translation is based on the following two observations: (1) Contextual information is exploited during the translation from one language to another.</S>
        <S ID="S-37297">For example in Table 1, English words &#8220;interest&#8221; and &#8220;bank&#8221; that have multiple meanings under different contexts are correctly addressed by using the state-of-the-art translation tool &#8722;&#8722;Google Translate.</S>
        <S ID="S-37298">4 Thus, word ambiguity based on contextual information is naturally involved when questions are translated.</S>
        <S ID="S-37299">(2) Multiple words that have similar meanings in one language may be translated into an unique word or a few words in a foreign language.</S>
        <S ID="S-37300">For example in Table 1, English words such as &#8220;company&#8221; and &#8220;firm&#8221; are translated into &#8220; &#20844; &#21496; (g&#333;ngs&#299;)&#8221;, &#8220;rheum&#8221; and &#8220;catarrh&#8221; are translated into &#8220; &#24863; &#20882; (g&#462;nm&#224;o)&#8221; in Chinese.</S>
        <S ID="S-37301">Thus, word mismatch problem can be somewhat alleviated by using other languages.</S>
      </P>
      <P>
        <S ID="S-37302">Although <REF ID="R-25" RPTR="34">Zhou et al. (2012)</REF> exploited bilingual translation for question retrieval and obtained the better performance than traditional monolingual translation models.</S>
        <S ID="S-37303">However, there are two problems with this enrichment: (1) enriching the original questions with the translated words from other languages increases the dimensionality and makes the question representation even more sparse; (2) statistical machine translation may introduce noise, which can harm the performance of question retrieval.</S>
        <S ID="S-37304">To solve these two problems, we propose to leverage statistical machine translation to improve question retrieval via matrix factorization.</S>
      </P>
      <P>
        <S ID="S-37305">The remainder of this paper is organized as follows.</S>
        <S ID="S-37306">Section 2 describes the proposed method by leveraging statistical machine translation to improve question retrieval via matrix factorization.</S>
        <S ID="S-37307">Section 3 presents the experimental results.</S>
        <S ID="S-37308">In section 4, we conclude with ideas for future research.</S>
      </P>
      <P>
        <S ID="S-37309">4 http://translate.google.com/translate t</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Our Approach</HEADER>
      <P>
        <S ID="S-37534"></S>
      </P>
      <DIV DEPTH="1">
        <HEADER>2.1 Problem Statement</HEADER>
        <P>
          <S ID="S-37310">This paper aims to leverage statistical machine translation to enrich the question representation.</S>
          <S ID="S-37311">In order to address the word ambiguity and word mismatch problems, we expand a question by adding its translation counterparts.</S>
          <S ID="S-37312">Statistical machine translation (e.g., Google Translate) can utilize contextual information during the question translation, so it can solve the word ambiguity and word mismatch problems to some extent.</S>
        </P>
        <P>
          <S ID="S-37313">Let L = {l 1 , l 2 , .</S>
          <S ID="S-37314">.</S>
          <S ID="S-37315">.</S>
          <S ID="S-37316">, l P } denote the language set, where P is the number of languages considered in the paper, l 1 denotes the original language (e.g., English) while l 2 to l P are the foreign languages.</S>
          <S ID="S-37317">Let D 1 = {d (1) 1 , d(1) 2 , .</S>
          <S ID="S-37318">.</S>
          <S ID="S-37319">.</S>
          <S ID="S-37320">, d(1) N } be the set of historical question collection in original language, where N is the number of historical questions in D 1 with vocabulary size M 1 .</S>
          <S ID="S-37321">Now we first translate each original historical question from language l 1 into other languages l p (p &#8712; [2, P ]) by Google Translate.</S>
          <S ID="S-37322">Thus, we can obtain D 2 , .</S>
          <S ID="S-37323">.</S>
          <S ID="S-37324">.</S>
          <S ID="S-37325">, D P in different languages, and M p is the vocabulary size of D p .</S>
          <S ID="S-37326">A question d (p) i in D p is simply represented as a M p dimensional vector</S>
        </P>
        <P>
          <S ID="S-37327">, in which each entry is calculated by tf-idf.</S>
          <S ID="S-37328">The N historical questions in D p are then represented in a M p &#215; N term-question matrix D p = d (p) i</S>
        </P>
        <P>
          <S ID="S-37329">2 , .</S>
          <S ID="S-37330">.</S>
          <S ID="S-37331">.</S>
          <S ID="S-37332">, d(p) N</S>
        </P>
        <P>
          <S ID="S-37333">}, in which each row corresponds to a term and each column corresponds to a question.</S>
        </P>
        <P>
          <S ID="S-37334">Intuitively, we can enrich the original question representation by adding the translated words {d (p) 1 , d(p)</S>
        </P>
        <P>
          <S ID="S-37335">from language l 2 to l P , the original vocabulary size is increased from M 1 to &#8721; P</S>
        </P>
        <P>
          <S ID="S-37336">p=1 M p.</S>
        </P>
        <P>
          <S ID="S-37337">Thus, the term-question matrix becomes D = {D 1 , D 2 , .</S>
          <S ID="S-37338">.</S>
          <S ID="S-37339">.</S>
          <S ID="S-37340">, D P } and D &#8712; R (&#8721; P</S>
        </P>
        <P>
          <S ID="S-37341">p=1 Mp)&#215;N .</S>
        </P>
        <P>
          <S ID="S-37342">However, there are two problems with this enrichment: (1) enriching the original questions with the translated words from other languages makes the question representation even more sparse; (2) statistical machine translation may introduce noise.</S>
          <S ID="S-37343">5 To solve these two problems, we propose to leverage statistical machine translation to improve question retrieval via matrix factorization.</S>
          <S ID="S-37344">Figure 1 presents the framework of our proposed method, where q i represents a queried question, and q i is a vector representation of q i .</S>
        </P>
        <P>
          <S ID="S-37345">5 Statistical machine translation quality is far from satisfactory in real applications.</S>
        </P>
        <P>
          <S ID="S-37346">Historical Question Collection Representation</S>
        </P>
        <P>
          <S ID="S-37347">Query Representation</S>
        </P>
        <P>
          <S ID="S-37348">&#65533;</S>
        </P>
        <P>
          <S ID="S-37349">&#65533;</S>
        </P>
        <P>
          <S ID="S-37350">&#8230;&#8230;</S>
        </P>
        <P>
          <S ID="S-37351">&#8230;&#8230;</S>
        </P>
        <P>
          <S ID="S-37352">&#8230;&#8230;</S>
        </P>
        <P>
          <S ID="S-37353">&#8230;&#8230;</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.2 Model Formulation</HEADER>
        <P>
          <S ID="S-37354">To tackle the data sparseness of question representation with the translated words, we hope to find two or more lower dimensional matrices whose product provides a good approximate to the original one via matrix factorization.</S>
          <S ID="S-37355">Previous studies have shown that there is psychological and physiological evidence for parts-based representation in the human brain (Wachsmuth et al., 1994).</S>
          <S ID="S-37356">The non-negative matrix factorization (NMF) is proposed to learn the parts of objects like text documents (<REF ID="R-10" RPTR="14">Lee and Seung, 2001</REF>).</S>
          <S ID="S-37357">NMF aims to find two non-negative matrices whose product provides a good approximation to the original matrix and has been shown to be superior to SVD in document clustering (<REF ID="R-21" RPTR="25">Xu et al., 2003</REF>; <REF ID="R-17" RPTR="20">Tang et al., 2012</REF>).</S>
        </P>
        <P>
          <S ID="S-37358">In this paper, NMF is used to induce the reduced representation V p of D p , D p is independent on {D 1 , D 2 , .</S>
          <S ID="S-37359">.</S>
          <S ID="S-37360">.</S>
          <S ID="S-37361">, D p&#8722;1 , D p+1 , .</S>
          <S ID="S-37362">.</S>
          <S ID="S-37363">.</S>
          <S ID="S-37364">, D P }.</S>
          <S ID="S-37365">When ignoring the coupling between V p , it can be solved by minimizing the objective function as follows:</S>
        </P>
        <P>
          <S ID="S-37366">O 1 (U p , V p ) = min &#8741;D p &#8722; U p V p &#8741; 2 F (1)</S>
        </P>
        <P>
          <S ID="S-37367">U p &#8805;0,V p &#8805;0</S>
        </P>
        <P>
          <S ID="S-37368">where &#8741; &#183; &#8741; F denotes Frobenius norm of a matrix.</S>
          <S ID="S-37369">Matrices U p &#8712; R M p&#215;K and V p &#8712; R K&#215;N are the reduced representation for terms and questions in the K dimensional space, respectively.</S>
        </P>
        <P>
          <S ID="S-37370">To reduce the noise introduced by statistical machine translation, we assume that V p from language D p (p &#8712; [2, P ]) should be close to V 1</S>
        </P>
        <P>
          <S ID="S-37371">from the original language D 1 .</S>
          <S ID="S-37372">Based on this assumption, we minimize the distance between V p (p &#8712; [2, P ]) and V 1 as follows:</S>
        </P>
        <P>
          <S ID="S-37373">O 2 (V p ) = min</S>
        </P>
        <P>
          <S ID="S-37374">V p &#8805;0 p=2</S>
        </P>
        <P>
          <S ID="S-37375">P&#8721; &#8741;V p &#8722; V 1 &#8741; 2 F (2)</S>
        </P>
        <P>
          <S ID="S-37376">Combining equations (1) and (2), we get the following objective function:</S>
        </P>
        <P>
          <S ID="S-37377">O(U 1 , .</S>
          <S ID="S-37378">.</S>
          <S ID="S-37379">.</S>
          <S ID="S-37380">, U P ; V 1 , .</S>
          <S ID="S-37381">.</S>
          <S ID="S-37382">.</S>
          <S ID="S-37383">, V P ) (3)</S>
        </P>
        <P>
          <S ID="S-37384">P&#8721; P&#8721; = &#8741;D p &#8722; U p V p &#8741; 2 F + &#955; p &#8741;V p &#8722; V 1 &#8741; 2 F</S>
        </P>
        <P>
          <S ID="S-37385">p=1 p=2</S>
        </P>
        <P>
          <S ID="S-37386">where parameter &#955; p (p &#8712; [2, P ]) is used to adjust the relative importance of these two components.</S>
          <S ID="S-37387">If we set a small value for &#955; p , the objective function behaves like the traditional NMF and the importance of data sparseness is emphasized; while a big value of &#955; p indicates V p should be very closed to V 1 , and equation (3) aims to remove the noise introduced by statistical machine translation.</S>
          <S ID="S-37388">By solving the optimization problem in equation (4), we can get the reduced representation of terms and questions.</S>
        </P>
        <P>
          <S ID="S-37389">min O(U 1 , .</S>
          <S ID="S-37390">.</S>
          <S ID="S-37391">.</S>
          <S ID="S-37392">, U P ; V 1 , .</S>
          <S ID="S-37393">.</S>
          <S ID="S-37394">.</S>
          <S ID="S-37395">, V P ) (4)</S>
        </P>
        <P>
          <S ID="S-37396">subject to : U p &#8805; 0, V p &#8805; 0, p &#8712; [1, P ]</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.3 Optimization</HEADER>
        <P>
          <S ID="S-37397">The objective function O defined in equation (4) performs data sparseness and noise removing simultaneously.</S>
          <S ID="S-37398">There are 2P coupling components in O, and O is not convex in both U and V together.</S>
          <S ID="S-37399">Therefore it is unrealistic to expect an algorithm to find the global minima.</S>
          <S ID="S-37400">In the following, we introduce an iterative algorithm which can achieve local minima.</S>
          <S ID="S-37401">In our optimization framework, we optimize the objective function in equation (4) by alternatively minimizing each component when the remaining 2P &#8722; 1 components are fixed.</S>
          <S ID="S-37402">This procedure is summarized in Algorithm 1.</S>
        </P>
        <P>
          <S ID="S-37403">2.3.1 Update of Matrix U p</S>
        </P>
        <P>
          <S ID="S-37404">Holding V 1 , .</S>
          <S ID="S-37405">.</S>
          <S ID="S-37406">.</S>
          <S ID="S-37407">, V P and U 1 , .</S>
          <S ID="S-37408">.</S>
          <S ID="S-37409">.</S>
          <S ID="S-37410">, U p&#8722;1 , U p+1 , .</S>
          <S ID="S-37411">.</S>
          <S ID="S-37412">.</S>
          <S ID="S-37413">, U P fixed, the update of U p amounts to the following optimization problem:</S>
        </P>
        <P>
          <S ID="S-37414">min &#8741;D p &#8722; U p V p &#8741; 2 F (5)</S>
        </P>
        <P>
          <S ID="S-37415">U p&#8805;0</S>
        </P>
        <P>
          <S ID="S-37416">Algorithm 1 Optimization framework</S>
        </P>
        <P>
          <S ID="S-37417">Algorithm 2 Update U p</S>
        </P>
        <P>
          <S ID="S-37418">Input: D p &#8712; R Mp&#215;N , V p &#8712; R K&#215;N 1: for i = 1 : M p do 2: &#363; (p)&#8727;</S>
        </P>
        <P>
          <S ID="S-37419">i</S>
        </P>
        <P>
          <S ID="S-37420">3: end for 4: return U p = (V p V T p ) &#8722;1 V p&#175;d(p) i</S>
        </P>
        <P>
          <S ID="S-37421">Let &#175;d (p) i = (d (p) i1 , .</S>
          <S ID="S-37422">.</S>
          <S ID="S-37423">.</S>
          <S ID="S-37424">, d(p) iK )T and &#363; (p) i = (u (p) i1 , .</S>
          <S ID="S-37425">.</S>
          <S ID="S-37426">.</S>
          <S ID="S-37427">, u(p) iK )T be the column vectors whose entries are those of the i th row of D p and U p respectively.</S>
          <S ID="S-37428">Thus, the optimization of equation (5) can be decomposed into M p optimization problems that can be solved independently, with each corresponding to one row of U p :</S>
        </P>
        <P>
          <S ID="S-37429">min</S>
        </P>
        <P>
          <S ID="S-37430">&#363; (p) i</S>
        </P>
        <P>
          <S ID="S-37431">&#8805;0</S>
        </P>
        <P>
          <S ID="S-37432">&#8741;&#175;d (p) i</S>
        </P>
        <P>
          <S ID="S-37433">&#8722; V T p &#363; (p) i &#8741; 2 2 (6)</S>
        </P>
        <P>
          <S ID="S-37434">for i = 1, .</S>
          <S ID="S-37435">.</S>
          <S ID="S-37436">.</S>
          <S ID="S-37437">, M p .</S>
          <S ID="S-37438">Equation (6) is a standard least squares problems in statistics and the solution is:</S>
        </P>
        <P>
          <S ID="S-37439">&#363; (p)&#8727; i</S>
        </P>
        <P>
          <S ID="S-37440">= (V p V T p ) &#8722;1 V p&#175;d(p) i (7)</S>
        </P>
        <P>
          <S ID="S-37441">Algorithm 2 shows the procedure.</S>
        </P>
        <P>
          <S ID="S-37442">2.3.2 Update of Matrix V p</S>
        </P>
        <P>
          <S ID="S-37443">Holding U 1 , .</S>
          <S ID="S-37444">.</S>
          <S ID="S-37445">.</S>
          <S ID="S-37446">, U P and V 1 , .</S>
          <S ID="S-37447">.</S>
          <S ID="S-37448">.</S>
          <S ID="S-37449">, V p&#8722;1 , V p+1 , .</S>
          <S ID="S-37450">.</S>
          <S ID="S-37451">.</S>
          <S ID="S-37452">, V P fixed, the update of V p amounts to the optimization problem divided into two categories.</S>
        </P>
        <P>
          <S ID="S-37453">if p &#8712; [2, P ], the objective function can be written as:</S>
        </P>
        <P>
          <S ID="S-37454">min &#8741;D p &#8722; U p V p &#8741; 2 F + &#955; p &#8741;V p &#8722; V 1 &#8741; 2 F (8)</S>
        </P>
        <P>
          <S ID="S-37455">V p &#8805;0</S>
        </P>
        <P>
          <S ID="S-37456">if p = 1, the objective function can be written as:</S>
        </P>
        <P>
          <S ID="S-37457">min &#8741;D p &#8722; U p V p &#8741; 2 F + &#955; p &#8741;V p &#8741; 2 F (9)</S>
        </P>
        <P>
          <S ID="S-37458">V p&#8805;0</S>
        </P>
        <P>
          <S ID="S-37459">Let d (p) j be the j th column vector of D p , and</S>
        </P>
        <P>
          <S ID="S-37460">v (p) j</S>
        </P>
        <P>
          <S ID="S-37461">be the j th column vector of V p , respectively.</S>
          <S ID="S-37462">Thus, equation (8) can be rewritten as:</S>
        </P>
        <P>
          <S ID="S-37463">N&#8721; N&#8721; min &#8741;d (p)</S>
        </P>
        <P>
          <S ID="S-37464">{v (p) j</S>
        </P>
        <P>
          <S ID="S-37465">&#8722;U p v (p) j &#8741; 2 2+ &#955; p &#8741;v (p) j &#8722;v (1) j</S>
        </P>
        <P>
          <S ID="S-37466">&#8741; 2 2</S>
        </P>
        <P>
          <S ID="S-37467">j &#8805;0} j=1 j=1</S>
        </P>
        <P>
          <S ID="S-37468">(10) which can be decomposed into N optimization problems that can be solved independently, with each corresponding to one column of V p :</S>
        </P>
        <P>
          <S ID="S-37469">min</S>
        </P>
        <P>
          <S ID="S-37470">v (p) j</S>
        </P>
        <P>
          <S ID="S-37471">&#8805;0</S>
        </P>
        <P>
          <S ID="S-37472">&#8741;d (p) j</S>
        </P>
        <P>
          <S ID="S-37473">&#8722;U p v (p) j &#8741; 2 2 +&#955; p &#8741;v (p) j &#8722;v (1) j &#8741; 2 2 (11)</S>
        </P>
        <P>
          <S ID="S-37474">for j = 1, .</S>
          <S ID="S-37475">.</S>
          <S ID="S-37476">.</S>
          <S ID="S-37477">, N.</S>
        </P>
        <P>
          <S ID="S-37478">Equation (12) is a least square problem with L 2 norm regularization.</S>
          <S ID="S-37479">Now we rewrite the objective function in equation (12) as</S>
        </P>
        <P>
          <S ID="S-37480">L(v (p) j ) = &#8741;d (p) j &#8722; U p v (p) j &#8741; 2 2 + &#955; p &#8741;v p j &#8722; v(1) j &#8741; 2 2 (12) where L(v (1) j ) is convex, and hence has a unique solution.</S>
          <S ID="S-37481">Taking derivatives, we obtain:</S>
        </P>
        <P>
          <S ID="S-37482">&#8706;L(v (p) j )</S>
        </P>
        <P>
          <S ID="S-37483">&#8706;v (p) j = &#8722;2U T p (d (p) j &#8722;U p v (p) )+2&#955; p (v (p)</S>
        </P>
        <P>
          <S ID="S-37484">j j</S>
        </P>
        <P>
          <S ID="S-37485">&#8722;v (1) j )</S>
        </P>
        <P>
          <S ID="S-37486">(13) Forcing the partial derivative to be zero leads to</S>
        </P>
        <P>
          <S ID="S-37487">v (p)&#8727; j</S>
        </P>
        <P>
          <S ID="S-37488">= (U T p U p + &#955; p I) &#8722;1 (U T p d (p) j + &#955; p v (1) j ) (14) where p &#8712; [2, P ] denotes the foreign language representation.</S>
        </P>
        <P>
          <S ID="S-37489">Similarly, the solution of equation (9) is:</S>
        </P>
        <P>
          <S ID="S-37490">v (p)&#8727; j</S>
        </P>
        <P>
          <S ID="S-37491">= (U T p U p + &#955; p I) &#8722;1 U T p d (p) j (15)</S>
        </P>
        <P>
          <S ID="S-37492">where p = 1 denotes the original language representation.</S>
        </P>
        <P>
          <S ID="S-37493">Algorithm 3 shows the procedure.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.4 Time Complexity Analysis</HEADER>
        <P>
          <S ID="S-37494">In this subsection, we discuss the time complexity of our proposed method.</S>
          <S ID="S-37495">The optimization &#363; (p) i</S>
        </P>
        <P>
          <S ID="S-37496">using Algorithm 2 should calculate V p Vp T</S>
        </P>
        <P>
          <S ID="S-37497">and V p&#175;d(p) i , which takes O(NK 2 + NK) operations.</S>
          <S ID="S-37498">Therefore, the optimization U p takes O(NK 2 + M p NK) operations.</S>
          <S ID="S-37499">Similarly, the time complexity of optimization V i using Algorithm 3 is O(M p K 2 + M p NK).</S>
          <S ID="S-37500">Another time complexity is the iteration times T used in Algorithm 1 and the total number of</S>
        </P>
        <P>
          <S ID="S-37501">Algorithm 3 Update V p</S>
        </P>
        <P>
          <S ID="S-37502">Input: D p &#8712; R Mp&#215;N , U p &#8712; R M p&#215;K 1: &#931; &#8592; (U T p U p + &#955; p I) &#8722;1 2: &#934; &#8592; U T p D p 3: if p = 1 then 4: for j = 1 : N do 5: v (p) j &#8592; &#931;&#981; j , &#981; j is the j th column of &#934;</S>
        </P>
        <P>
          <S ID="S-37503">6: end for 7: end if 8: return V 1 9: if p &#8712; [2, P ] then 10: for j = 1 : N do 11: v (p) j &#8592; &#931;(&#981; j + &#955; p v (1) j )</S>
        </P>
        <P>
          <S ID="S-37504">12: end for 13: end if 14: return V p</S>
        </P>
        <P>
          <S ID="S-37505">languages P , the overall time complexity of our proposed method is:</S>
        </P>
        <P>
          <S ID="S-37506">P&#8721; T &#215; O(NK 2 + M p K 2 + 2M p NK) (16)</S>
        </P>
        <P>
          <S ID="S-37507">p=1</S>
        </P>
        <P>
          <S ID="S-37508">For each language D p , the size of vocabulary M p is almost constant as the number of questions increases.</S>
          <S ID="S-37509">Besides, K &#8810; min(M p , N), theoretically, the computational time is almost linear with the number of questions N and the number of languages P considered in the paper.</S>
          <S ID="S-37510">Thus, the proposed method can be easily adapted to the largescale information retrieval task.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.5 Relevance Ranking</HEADER>
        <P>
          <S ID="S-37511">The advantage of incorporating statistical machine translation in relevance ranking is to reduce &#8220;word ambiguity&#8221; and &#8220;word mismatch&#8221; problems.</S>
          <S ID="S-37512">To do so, given a queried question q and a historical question d from Yahoo!</S>
          <S ID="S-37513">Answers, we first translate q and d into other foreign languages (e.g., Chinese, French etc.) and get the corresponding translated representation q i and d i (i &#8712; [2, P ]), where P is the number of languages considered in the paper.</S>
          <S ID="S-37514">For queried question q = q 1 , we represent it in the reduced space:</S>
        </P>
        <P>
          <S ID="S-37515">v q1 = arg min</S>
        </P>
        <P>
          <S ID="S-37516">v&#8805;0 &#8741;q 1 &#8722; U 1 v&#8741; 2 2 + &#955; 1 &#8741;v&#8741; 2 2 (17)</S>
        </P>
        <P>
          <S ID="S-37517">where vector q 1 is the tf-idf representation of queried question q 1 in the term space.</S>
          <S ID="S-37518">Similarly, for historical question d = d 1 (and its tf-idf representation d 1 in the term space) we represent it in the reduced space as v d1 .</S>
        </P>
        <P>
          <S ID="S-37519">The relevance score between the queried question q 1 and the historical question d 1 in the reduced space is, then, calculated as the cosine similarity between v q1 and v d1 :</S>
        </P>
        <P>
          <S ID="S-37520">s(q 1 , d 1 ) = &lt; v q 1 , v d1 &gt; &#8741;v q1 &#8741; 2 &#183; &#8741;v d1 &#8741; 2 (18)</S>
        </P>
        <P>
          <S ID="S-37521">For translated representation q i (i &#8712; [2, P ]), we also represent it in the reduced space:</S>
        </P>
        <P>
          <S ID="S-37522">v qi = arg min</S>
        </P>
        <P>
          <S ID="S-37523">v&#8805;0 &#8741;q i &#8722;U i v&#8741; 2 2 +&#955; i &#8741;v&#8722;v q1 &#8741; 2 2 (19)</S>
        </P>
        <P>
          <S ID="S-37524">where vector q i is the tf-idf representation of q i in the term space.</S>
          <S ID="S-37525">Similarly, for translated representation d i (and its tf-idf representation d i in the term space) we also represent it in the reduced space as v di .</S>
          <S ID="S-37526">The relevance score s(q i , d i ) between q i and d i in the reduced space can be calculated as the cosine similarity between v qi and v di .</S>
        </P>
        <P>
          <S ID="S-37527">Finally, we consider learning a relevance function of the following general, linear form:</S>
        </P>
        <P>
          <S ID="S-37528">Score(q, d) = &#952; T &#183; &#934;(q, d) (20)</S>
        </P>
        <P>
          <S ID="S-37529">where feature vector &#934;(q, d) = (s V SM (q, d), s(q 1 , d 1 ), s(q 2 , d 2 ), .</S>
          <S ID="S-37530">.</S>
          <S ID="S-37531">.</S>
          <S ID="S-37532">, s(q P , d P )), and &#952; is the corresponding weight vector, we optimize this parameter for our evaluation metrics directly using the Powell Search algorithm (Paul et al., 1992) via cross-validation.</S>
          <S ID="S-37533">s V SM (q, d) is the relevance score in the term space and can be calculated using Vector Space Model (VSM).</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Experiments</HEADER>
      <P>
        <S ID="S-37621"></S>
      </P>
      <DIV DEPTH="1">
        <HEADER>3.1 Data Set and Evaluation Metrics</HEADER>
        <P>
          <S ID="S-37535">We collect the data set from Yahoo!</S>
          <S ID="S-37536">Answers and use the getByCategory function provided in Yahoo!</S>
          <S ID="S-37537">Answers API 6 to obtain CQA threads from the Yahoo!</S>
          <S ID="S-37538">site.</S>
          <S ID="S-37539">More specifically, we utilize the resolved questions and the resulting question repository that we use for question retrieval contains 2,288,607 questions.</S>
          <S ID="S-37540">Each resolved question consists of four parts: &#8220;question title&#8221;, &#8220;question description&#8221;, &#8220;question answers&#8221; and &#8220;question category&#8221;.</S>
          <S ID="S-37541">For question retrieval, we only use the &#8220;question title&#8221; part.</S>
          <S ID="S-37542">It is assumed that question title already provides enough semantic information for understanding the users&#8217; information needs (<REF ID="R-06" RPTR="11">Duan et al., 2008</REF>).</S>
          <S ID="S-37543">There are 26 categories</S>
        </P>
        <P>
          <S ID="S-37544">6 http://developer.yahoo.com/answers</S>
        </P>
        <P>
          <S ID="S-37545">at the first level and 1,262 categories at the leaf level.</S>
          <S ID="S-37546">Each question belongs to a unique leaf category.</S>
          <S ID="S-37547">Table 2 shows the distribution across firstlevel categories of the questions in the archives.</S>
        </P>
        <P>
          <S ID="S-37548">We use the same test set in previous work (<REF ID="R-04" RPTR="4">Cao et al., 2009</REF>; <REF ID="R-05" RPTR="7">Cao et al., 2010</REF>).</S>
          <S ID="S-37549">This set contains 252 queried questions and can be freely downloaded for research communities.</S>
          <S ID="S-37550">7</S>
        </P>
        <P>
          <S ID="S-37551">The original language of the above data set is English (l 1 ) and then they are translated into four other languages (Chinese (l 2 ), French (l 3 ), German (l 4 ), Italian (l 5 )), thus the number of language considered is P = 5) by using the state-of-the-art translation tool &#8722;&#8722;Google Translate.</S>
        </P>
        <P>
          <S ID="S-37552">Evaluation Metrics: We evaluate the performance of question retrieval using the following metrics: Mean Average Precision (MAP) and Precision@N (P@N).</S>
          <S ID="S-37553">MAP rewards methods that return relevant questions early and also rewards correct ranking of the results.</S>
          <S ID="S-37554">P@N reports the fraction of the top-N questions retrieved that are relevant.</S>
          <S ID="S-37555">We perform a significant test, i.e., a t- test with a default significant level of 0.05.</S>
        </P>
        <P>
          <S ID="S-37556">We tune the parameters on a small development set of 50 questions.</S>
          <S ID="S-37557">This development set is also extracted from Yahoo!</S>
          <S ID="S-37558">Answers, and it is not included in the test set.</S>
          <S ID="S-37559">For parameter K, we do an experiment on the development set to determine the optimal values among 50, 100, 150, &#183; &#183; &#183; , 300 in terms of MAP.</S>
          <S ID="S-37560">Finally, we set K = 100 in the experiments empirically as this setting yields the best performance.</S>
          <S ID="S-37561">For parameter &#955; 1 , we set &#955; 1 = 1 empirically, while for parameter &#955; i (i &#8712; [2, P ]),</S>
        </P>
        <P>
          <S ID="S-37562">&#8721; we set &#955; i = 0.25 empirically and ensure that</S>
        </P>
        <P>
          <S ID="S-37563">i &#955; i = 1.</S>
        </P>
        <P>
          <S ID="S-37564">7 http://homepages.inf.ed.ac.uk/gcong/qa/</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.2 Question Retrieval Results</HEADER>
        <P>
          <S ID="S-37565">Table 3 presents the main retrieval performance.</S>
          <S ID="S-37566">Row 1 and row 2 are two baseline systems, which model the relevance score using VSM (<REF ID="R-05" RPTR="8">Cao et al., 2010</REF>) and language model (LM) (<REF ID="R-23" RPTR="30">Zhai and Lafferty, 2001</REF>; <REF ID="R-05" RPTR="9">Cao et al., 2010</REF>) in the term space.</S>
          <S ID="S-37567">Row 3 and row 6 are monolingual translation models to address the word mismatch problem and obtain the state-of-the-art performance in previous work.</S>
          <S ID="S-37568">Row 3 is the word-based translation model (<REF ID="R-12" RPTR="18">Jeon et al., 2005</REF>), and row 4 is the wordbased translation language model, which linearly combines the word-based translation model and language model into a unified framework (<REF ID="R-22" RPTR="28">Xue et al., 2008</REF>).</S>
          <S ID="S-37569">Row 5 is the phrase-based translation model, which translates a sequence of words as whole (Zhou et al., 2011).</S>
          <S ID="S-37570">Row 6 is the entitybased translation model, which extends the wordbased translation model and explores strategies to learn the translation probabilities between words and the concepts using the CQA archives and a popular entity catalog (Singh, 2012).</S>
          <S ID="S-37571">Row 7 is the bilingual translation model, which translates the English questions from Yahoo!</S>
          <S ID="S-37572">Answers into Chinese questions using Google Translate and expands the English words with the translated Chinese words (<REF ID="R-25" RPTR="32">Zhou et al., 2012</REF>).</S>
          <S ID="S-37573">For these previous work, we use the same parameter settings in the original papers.</S>
          <S ID="S-37574">Row 8 and row 9 are our proposed method, which leverages statistical machine translation to improve question retrieval via matrix factorization.</S>
          <S ID="S-37575">In row 8, we only consider two languages (English and Chinese) and translate English questions into Chinese using Google Translate in order to compare with <REF ID="R-25" RPTR="35">Zhou et al. (2012)</REF>.</S>
          <S ID="S-37576">In row 9, we translate English questions into other four languages.</S>
          <S ID="S-37577">There are some clear trends in the result of Table 3:</S>
        </P>
        <P>
          <S ID="S-37578">(1) Monolingual translation models significantly outperform the VSM and LM (row 1 and row 2 vs. row 3, row 4, row 5 and row 6).</S>
        </P>
        <P>
          <S ID="S-37579">(2) Taking advantage of potentially rich semantic information drawn from other languages via statistical machine translation, question retrieval performance can be significantly improved (row 3, row 4, row 5 and row 6 vs. row 7, row 8 and row 9, all these comparisons are statistically significant at p &lt; 0.05).</S>
        </P>
        <P>
          <S ID="S-37580">(3) Our proposed method (leveraging statistical machine translation via matrix factorization, SMT + MF) significantly outperforms the bilingual translation model of <REF ID="R-25" RPTR="36">Zhou et al. (2012)</REF> (row 7 vs. row 8, the comparison is statistically significant at p &lt; 0.05).</S>
          <S ID="S-37581">The reason is that matrix factorization used in the paper can effectively solve the data sparseness and noise introduced by the machine translator simultaneously.</S>
        </P>
        <P>
          <S ID="S-37582">(4) When considering more languages, question retrieval performance can be further improved (row 8 vs. row 9).</S>
        </P>
        <P>
          <S ID="S-37583">Note that <REF ID="R-19" RPTR="21">Wang et al. (2009)</REF> also addressed the word mismatch problem for question retrieval by using syntactic tree matching.</S>
          <S ID="S-37584">We do not compare with <REF ID="R-19" RPTR="22">Wang et al. (2009)</REF> in Table 3 because previous work (<REF ID="R-11" RPTR="15">Ming et al., 2010</REF>) demonstrated that word-based translation language model (<REF ID="R-22" RPTR="29">Xue et al., 2008</REF>) obtained the superior performance than the syntactic tree matching (<REF ID="R-19" RPTR="24">Wang et al., 2009</REF>).</S>
          <S ID="S-37585">Besides, some other studies attempt to improve question retrieval with category information (<REF ID="R-04" RPTR="5">Cao et al., 2009</REF>; <REF ID="R-05" RPTR="10">Cao et al., 2010</REF>), label ranking (Li et al., 2011) or world knowledge (<REF ID="R-25" RPTR="33">Zhou et al., 2012</REF>).</S>
          <S ID="S-37586">However, their methods are orthogonal to ours, and we suspect that combining the category information or label ranking into our proposed method might get even better performance.</S>
          <S ID="S-37587">We leave it for future research.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.3 Impact of the Matrix Factorization</HEADER>
        <P>
          <S ID="S-37588">Our proposed method (SMT + MF) can effectively solve the data sparseness and noise via matrix factorization.</S>
          <S ID="S-37589">To further investigate the impact of the matrix factorization, one intuitive way is to expand the original questions with the translated words from other four languages, without considering the data sparseness and noise introduced by machine translator.</S>
          <S ID="S-37590">We compare our SMT + MF with this intuitive enriching method (SMT + IEM).</S>
          <S ID="S-37591">Besides, we also employ our proposed matrix factorization to the original question representation (VSM + MF).</S>
          <S ID="S-37592">Table 4 shows the comparison.</S>
        </P>
        <P>
          <S ID="S-37593"># Methods MAP P@10</S>
        </P>
        <P>
          <S ID="S-37594">(1) Our proposed matrix factorization can significantly improve the performance of question retrieval (row 1 vs. row2; row3 vs. row4, the improvements are statistically significant at p &lt; 0.05).</S>
          <S ID="S-37595">The results indicate that our proposed matrix factorization can effectively address the issues of data spareness and noise introduced by statistical machine translation.</S>
        </P>
        <P>
          <S ID="S-37596">(2) Compared to the relative improvements of row 3 and row 4, the relative improvements of row 1 and row 2 is much larger.</S>
          <S ID="S-37597">The reason may be that although matrix factorization can be used to reduce dimension, it may impair the meaningful terms.</S>
        </P>
        <P>
          <S ID="S-37598">(3) Compared to VSM, the performance of SMT + IEM is significantly improved (row 1 vs. row 3), which supports the motivation that the word ambiguity and word mismatch problems could be partially addressed by Google Translate.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.4 Impact of the Translation Language</HEADER>
        <P>
          <S ID="S-37599">One of the success of this paper is to take advantage of potentially rich semantic information drawn from other languages to solve the word ambiguity and word mismatch problems.</S>
          <S ID="S-37600">So we construct a dummy translator (DT) that translates an English word to itself.</S>
          <S ID="S-37601">Thus, through this translation, we do not add any semantic information into the original questions.</S>
          <S ID="S-37602">The comparison is presented in Table 5.</S>
          <S ID="S-37603">Row 1 (DT + MF) represents integrating two copies of English questions with our proposed matrix factorization.</S>
          <S ID="S-37604">From Table 5, we have several different findings:</S>
        </P>
        <P>
          <S ID="S-37605">(1) Taking advantage of potentially rich semantic information drawn from other languages can significantly improve the performance of question retrieval (row 1 vs. row 2, row 3, row 4 and row 5, the improvements relative to DT + MF are statistically significant at p &lt; 0.05).</S>
        </P>
        <P>
          <S ID="S-37606">(2) Different languages contribute unevenly for question retrieval (e.g., row 2 vs. row 3).</S>
          <S ID="S-37607">The reason may be that the improvements of leveraging different other languages depend on the quality of machine translation.</S>
          <S ID="S-37608">For example, row 3</S>
        </P>
        <P>
          <S ID="S-37609">Method Translation MAP Dict 0.468</S>
        </P>
        <P>
          <S ID="S-37610">SMT + MF (P = 2, l 1, l 2) GTrans 0.527</S>
        </P>
        <P>
          <S ID="S-37611">is better than row 2 because the translation quality of English-French is much better than English- Chinese.</S>
        </P>
        <P>
          <S ID="S-37612">(3) Using much more languages does not seem to produce significantly better performance (row 6 and row 7 vs. row 8).</S>
          <S ID="S-37613">The reason may be that inconsistency between different languages may exist due to statistical machine translation.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.5 Impact of the Contextual Information</HEADER>
        <P>
          <S ID="S-37614">In this paper, we translate the English questions into other four languages using Google Translate (GTrans), which takes into account contextual information during translation.</S>
          <S ID="S-37615">If we translate a question word by word, it discards the contextual information.</S>
          <S ID="S-37616">We would expect that such a translation would not be able to solve the word ambiguity problem.</S>
          <S ID="S-37617">To investigate the impact of contextual information for question retrieval, we only consider two languages and translate English questions into Chinese using an English to Chinese lexicon (Dict) in StarDict 8 .</S>
          <S ID="S-37618">Table 6 shows the experimental results, we can see that the performance is degraded when the contextual information is not considered for the translation of questions.</S>
          <S ID="S-37619">The reason is that GTrans is context-dependent and thus produces different translated Chinese words depending on the context of an English word.</S>
          <S ID="S-37620">Therefore, the word ambiguity problem can be solved during the English-Chinese translation.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 Conclusions and Future Work</HEADER>
      <P>
        <S ID="S-37622">In this paper, we propose to employ statistical machine translation to improve question retrieval and</S>
      </P>
      <P>
        <S ID="S-37623">859 enrich the question representation with the translated words from other languages via matrix factorization.</S>
        <S ID="S-37624">Experiments conducted on a real CQA data show some promising findings: (1) the proposed method significantly outperforms the previous work for question retrieval; (2) the proposed matrix factorization can significantly improve the performance of question retrieval, no matter whether considering the translation languages or not; (3) considering more languages can further improve the performance but it does not seem to produce significantly better performance; (4) different languages contribute unevenly for question retrieval; (5) our proposed method can be easily adapted to the large-scale information retrieval task.</S>
        <S ID="S-37625">As future work, we plan to incorporate the question structure (e.g., question topic and question focus (<REF ID="R-06" RPTR="12">Duan et al., 2008</REF>)) into the question representation for question retrieval.</S>
        <S ID="S-37626">We also want to further investigate the use of the proposed method for other kinds of data set, such as categorized questions from forum sites and FAQ sites.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS>
    <P>
      <S ID="S-37627">We thank the anonymous reviewers for their insightful comments.</S>
      <S ID="S-37628">We also thank Dr. Gao Cong for providing the data set and Dr. Li Cai for some discussion.</S>
    </P>
  </ACKNOWLEDGMENTS>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>L Adamic</RAUTHOR>
      <REFTITLE>Knowledge sharing and yahoo answers: everyone knows and something.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>A Berger</RAUTHOR>
      <REFTITLE>Bridging the lexical chasm: statistical approach to answer-finding.</REFTITLE>
      <DATE>2000</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>D Bernhard</RAUTHOR>
      <REFTITLE>Combining lexical semantic resources with question &amp; answer archives for translation-based answer finding.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>P F Brown</RAUTHOR>
      <REFTITLE>The mathematics of statistical machine translation: parameter estimation.</REFTITLE>
      <DATE>1993</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>X Cao</RAUTHOR>
      <REFTITLE>The use of categorization information in language models for question retrieval.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>X Cao</RAUTHOR>
      <REFTITLE>A generalized framework of exploring category information for question retrieval in community question answer archives.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>H Duan</RAUTHOR>
      <REFTITLE>Searching questions by identifying questions topics and question focus.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>C L Lawson</RAUTHOR>
      <REFTITLE>Solving least squares problems.</REFTITLE>
      <DATE>1974</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>J -T Lee</RAUTHOR>
      <REFTITLE>Bridging lexical gaps between queries and questions on large online Q&amp;A collections with compact translation models.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>W Wang</RAUTHOR>
      <REFTITLE>Improving question retrieval in community question answering with label ranking.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>D D Lee</RAUTHOR>
      <REFTITLE>Algorithms for non-negative matrix factorization.</REFTITLE>
      <DATE>2001</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>Z Ming</RAUTHOR>
      <REFTITLE>Prototype hierarchy based clustering for the categorization and navigation of web collections.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>J Jeon</RAUTHOR>
      <REFTITLE>Finding similar questions in large question and answer archives.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>C Paige</RAUTHOR>
      <REFTITLE>LSQR: an algorithm for sparse linear equations and sparse least squares.</REFTITLE>
      <DATE>1982</DATE>
    </REFERENCE>
    <REFERENCE ID="14">
      <RAUTHOR>W H Press</RAUTHOR>
      <REFTITLE>Numerical Recipes In C.</REFTITLE>
      <DATE>1992</DATE>
    </REFERENCE>
    <REFERENCE ID="15">
      <RAUTHOR>S Riezler</RAUTHOR>
      <REFTITLE>Statistical machine translation for query expansion in answer retrieval.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="16">
      <RAUTHOR>A Singh</RAUTHOR>
      <REFTITLE>Entity based q&amp;a retrieval.</REFTITLE>
      <DATE>2012</DATE>
    </REFERENCE>
    <REFERENCE ID="17">
      <RAUTHOR>J Tang</RAUTHOR>
      <REFTITLE>Enriching short text representation in microblog for clustering.</REFTITLE>
      <DATE>2012</DATE>
    </REFERENCE>
    <REFERENCE ID="18">
      <RAUTHOR>M W Oram Wachsmuth</RAUTHOR>
      <REFTITLE>Recognition of objects and their component parts: responses of single units in the temporal cortex of teh macaque.</REFTITLE>
      <DATE>1994</DATE>
    </REFERENCE>
    <REFERENCE ID="19">
      <RAUTHOR>K Wang</RAUTHOR>
      <REFTITLE>A syntactic tree matching approach to find similar questions in community-based qa services.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="20">
      <RAUTHOR>B Wang</RAUTHOR>
      <REFTITLE>Modeling semantic relevance for question-answer pairs in web social communities.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="21">
      <RAUTHOR>W Xu</RAUTHOR>
      <REFTITLE>Document clustering based on non-negative matrix factorization.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="22">
      <RAUTHOR>X Xue</RAUTHOR>
      <REFTITLE>Retrieval models for question and answer archives.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="23">
      <RAUTHOR>C Zhai</RAUTHOR>
      <REFTITLE>A study of smooth methods for language models applied to ad hoc information retrieval.</REFTITLE>
      <DATE>2001</DATE>
    </REFERENCE>
    <REFERENCE ID="24">
      <RAUTHOR>G Zhou</RAUTHOR>
      <REFTITLE>Phrasebased translation model for question retrieval in community question answer archives.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="25">
      <RAUTHOR>G Zhou</RAUTHOR>
      <REFTITLE>Exploiting bilingual translation for question retrieval in communitybased question answering.</REFTITLE>
      <DATE>2012</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
