<document>
  <filename>P11-1065</filename>
  <authors/>
  <title>None</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>1 Introduction</title>
        <text>Recent advances in Statistical Machine Translation (SMT) are widely centred around two concepts: (a) hierarchical translation processes, frequently employing Synchronous Context Free Grammars (SCFGs) and (b) transduction or synchronous rewrite processes over a linguistic syntactic tree. SCFGs in the form of the Inversion-Transduction Grammar (ITG) were first introduced by (Wu, 1997) as a formalism to recursively describe the translation process. The Hiero system (Chiang, 2005) 642
utilised an ITG-flavour which focused on hierarchical phrase-pairs to capture context-driven translation and reordering patterns with &#8216;gaps&#8217;, offering competitive performance particularly for language pairs with extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training (Wang et al., 2010), or by moving from linguistically motivated synchronous grammars to systems where linguistic plausibility of the translation is assessed through additional features in a phrase-based system (Venugopal et al., 2009; Chiang et al., 2009), obscuring the impact of higher level syntactic processes.
While it is assumed that linguistic structure does correlate with some translation phenomena, in this
work we do not employ it as the backbone of translation. In place of linguistically constrained translation imposing syntactic parse structure, we opt for linguistically motivated translation. We learn latent hierarchical structure, taking advantage of linguistic annotations but shaped and trained for translation.
We start by labelling each phrase-pair span in the word-aligned training data with multiple linguistically motivated categories, offering multi-grained abstractions from its lexical content. These phrasepair label charts are the input of our learning algorithm, which extracts the linguistically motivated rules and estimates the probabilities for a stochastic SCFG, without arbitrary constraints such as phrase or span sizes. Estimating such grammars under a Maximum Likelihood criterion is known to be plagued by strong overfitting leading to degenerate estimates (DeNero et al., 2006). In contrast, our learning objective not only avoids overfitting the training data but, most importantly, learns joint stochastic synchronous grammars which directly aim at generalisation towards yet unseen instances. By advancing from structures which mimic linguistic syntax, to learning linguistically aware latent recursive structures targeting translation, we achieve significant improvements in translation quality for 4 different language pairs in comparison with a strong hierarchical translation baseline. Our key contributions are presented in the following sections. Section 2 discusses the weak independence assumptions of SCFGs and introduces a joint translation model which addresses these issues and separates hierarchical translation structure from phrase-pair emission. In section 3 we consider a chart over phrase-pair spans filled with sourcelanguage linguistically motivated labels. We show how we can employ this crucial input to extract and train a hierarchical translation structure model with millions of rules. Section 4 demonstrates decoding with the model by constraining derivations to linguistic hints of the source sentence and presents our empirical results. We close with a discussion of related work and our conclusions.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Recent advances in Statistical Machine Translation (SMT) are widely centred around two concepts: (a) hierarchical translation processes, frequently employing Synchronous Context Free Grammars (SCFGs) and (b) transduction or synchronous rewrite processes over a linguistic syntactic tree.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>SCFGs in the form of the Inversion-Transduction Grammar (ITG) were first introduced by (Wu, 1997) as a formalism to recursively describe the translation process.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The Hiero system (Chiang, 2005) 642</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>utilised an ITG-flavour which focused on hierarchical phrase-pairs to capture context-driven translation and reordering patterns with &#8216;gaps&#8217;, offering competitive performance particularly for language pairs with extensive reordering.</text>
              <doc_id>3</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level.</text>
              <doc_id>4</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight.</text>
              <doc_id>5</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010).</text>
              <doc_id>6</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006).</text>
              <doc_id>7</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems.</text>
              <doc_id>8</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation.</text>
              <doc_id>9</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training (Wang et al., 2010), or by moving from linguistically motivated synchronous grammars to systems where linguistic plausibility of the translation is assessed through additional features in a phrase-based system (Venugopal et al., 2009; Chiang et al., 2009), obscuring the impact of higher level syntactic processes.</text>
              <doc_id>10</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>While it is assumed that linguistic structure does correlate with some translation phenomena, in this</text>
              <doc_id>11</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>work we do not employ it as the backbone of translation.</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In place of linguistically constrained translation imposing syntactic parse structure, we opt for linguistically motivated translation.</text>
              <doc_id>13</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We learn latent hierarchical structure, taking advantage of linguistic annotations but shaped and trained for translation.</text>
              <doc_id>14</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We start by labelling each phrase-pair span in the word-aligned training data with multiple linguistically motivated categories, offering multi-grained abstractions from its lexical content.</text>
              <doc_id>15</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>These phrasepair label charts are the input of our learning algorithm, which extracts the linguistically motivated rules and estimates the probabilities for a stochastic SCFG, without arbitrary constraints such as phrase or span sizes.</text>
              <doc_id>16</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Estimating such grammars under a Maximum Likelihood criterion is known to be plagued by strong overfitting leading to degenerate estimates (DeNero et al., 2006).</text>
              <doc_id>17</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In contrast, our learning objective not only avoids overfitting the training data but, most importantly, learns joint stochastic synchronous grammars which directly aim at generalisation towards yet unseen instances.</text>
              <doc_id>18</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>By advancing from structures which mimic linguistic syntax, to learning linguistically aware latent recursive structures targeting translation, we achieve significant improvements in translation quality for 4 different language pairs in comparison with a strong hierarchical translation baseline.</text>
              <doc_id>19</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Our key contributions are presented in the following sections.</text>
              <doc_id>20</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Section 2 discusses the weak independence assumptions of SCFGs and introduces a joint translation model which addresses these issues and separates hierarchical translation structure from phrase-pair emission.</text>
              <doc_id>21</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In section 3 we consider a chart over phrase-pair spans filled with sourcelanguage linguistically motivated labels.</text>
              <doc_id>22</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>We show how we can employ this crucial input to extract and train a hierarchical translation structure model with millions of rules.</text>
              <doc_id>23</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Section 4 demonstrates decoding with the model by constraining derivations to linguistic hints of the source sentence and presents our empirical results.</text>
              <doc_id>24</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>We close with a discussion of related work and our conclusions.</text>
              <doc_id>25</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>2 Joint Translation Model</title>
        <text>Our model is based on a probabilistic Synchronous CFG (Wu, 1997; Chiang, 2005). SCFGs define a 643 SBAR &#8594; [WHNP SBAR\WHNP]
SBAR\WHNP &#8594; &#12296;VP/NP L NP R &#12297; (b)
NP R &#8594; [NP PP]
WHNP &#8594; WHNP P
WHNP P &#8594; which / der
VP/NP L &#8594; VP/NP L P
VP/NP L P &#8594; is / ist
NP R &#8594; NP R P
NP R P &#8594; the solution / die L&#246;sung
NP &#8594; NP P
NP P &#8594; the solution / die L&#246;sung
PP &#8594; PP P
PP P &#8594; to the problem / f&#252;r das Problem
language over string pairs, which are generated beginning from a start symbol S and recursively expanding pairs of linked non-terminals across the two strings using the grammar&#8217;s rule set. By crossing the links between the non-terminals of the two sides reordering phenomena are captured. We employ binary SCFGs, i.e. grammars with a maximum of two non-terminals on the right-hand side. Also, for this work we only used grammars with either purely lexical or purely abstract rules involving one or two nonterminal pairs. An example can be seen in Figure 1, using an ITG-style notation and assuming the same non-terminal labels for both sides.
We utilise probabilistic SCFGs, where each rule is assigned a conditional probability of expanding the left-hand side symbol with the rule&#8217;s right-hand side. Phrase-pairs are emitted jointly and the overall probabilistic SCFG is a joint model over parallel strings.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Our model is based on a probabilistic Synchronous CFG (Wu, 1997; Chiang, 2005).</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>SCFGs define a 643 SBAR &#8594; [WHNP SBAR\WHNP]</text>
              <doc_id>27</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>SBAR\WHNP &#8594; &#12296;VP/NP L NP R &#12297; (b)</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>NP R &#8594; [NP PP]</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>WHNP &#8594; WHNP P</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>WHNP P &#8594; which / der</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>VP/NP L &#8594; VP/NP L P</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>VP/NP L P &#8594; is / ist</text>
              <doc_id>33</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>NP R &#8594; NP R P</text>
              <doc_id>34</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>NP R P &#8594; the solution / die L&#246;sung</text>
              <doc_id>35</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>NP &#8594; NP P</text>
              <doc_id>36</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>NP P &#8594; the solution / die L&#246;sung</text>
              <doc_id>37</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>PP &#8594; PP P</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>PP P &#8594; to the problem / f&#252;r das Problem</text>
              <doc_id>39</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>language over string pairs, which are generated beginning from a start symbol S and recursively expanding pairs of linked non-terminals across the two strings using the grammar&#8217;s rule set.</text>
              <doc_id>40</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>By crossing the links between the non-terminals of the two sides reordering phenomena are captured.</text>
              <doc_id>41</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We employ binary SCFGs, i.e. grammars with a maximum of two non-terminals on the right-hand side.</text>
              <doc_id>42</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Also, for this work we only used grammars with either purely lexical or purely abstract rules involving one or two nonterminal pairs.</text>
              <doc_id>43</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>An example can be seen in Figure 1, using an ITG-style notation and assuming the same non-terminal labels for both sides.</text>
              <doc_id>44</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We utilise probabilistic SCFGs, where each rule is assigned a conditional probability of expanding the left-hand side symbol with the rule&#8217;s right-hand side.</text>
              <doc_id>45</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Phrase-pairs are emitted jointly and the overall probabilistic SCFG is a joint model over parallel strings.</text>
              <doc_id>46</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 SCFG Reordering Weaknesses</title>
            <text>An interesting feature of all probabilistic SCFGs (i.e. not only binary ones), which has received surprisingly little attention, is that the reordering pat- (a)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
(j)
(k)
(l)
(m)
tern between the non-terminal pairs (or in the case of ITGs the choice between monotone and swap expansion) are not conditioned on any other part of a derivation. The result is that, the reordering pattern with the highest probability will always be preferred (e.g. in the Viterbi derivation) over the rest, irrespective of lexical or abstract context. As an example, a probabilistic SCFG will always assign a higher probability to derivations swapping or monotonically translating nouns and adjectives between English and French, only depending on which of the two rules NP &#8594; [NN JJ], NP &#8594; &#12296;NN JJ&#12297; has a higher probability. The rest of the (sometimes thousands of) rule-specific features usually added to SCFG translation models do not directly help either, leaving reordering decisions disconnected from the rest of the derivation.
While in a decoder this is somehow mitigated by the use of a language model, we believe that the weakness of straightforward applications of SCFGs to model reordering structure at the sentence level misses a chance to learn this crucial part of the translation process during grammar induction. As (Mylonakis and Sima&#8217;an, 2010) note, &#8216;plain&#8217; SCFGs seem to perform worse than the grammars described next, mainly due to wrong long-range reordering decisions for which the language model can hardly help.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>An interesting feature of all probabilistic SCFGs (i.e. not only binary ones), which has received surprisingly little attention, is that the reordering pat- (a)</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(c)</text>
                  <doc_id>48</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(d)</text>
                  <doc_id>49</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(e)</text>
                  <doc_id>50</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(f)</text>
                  <doc_id>51</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(g)</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(h)</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(i)</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(j)</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(k)</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(l)</text>
                  <doc_id>57</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(m)</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>tern between the non-terminal pairs (or in the case of ITGs the choice between monotone and swap expansion) are not conditioned on any other part of a derivation.</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The result is that, the reordering pattern with the highest probability will always be preferred (e.g. in the Viterbi derivation) over the rest, irrespective of lexical or abstract context.</text>
                  <doc_id>60</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As an example, a probabilistic SCFG will always assign a higher probability to derivations swapping or monotonically translating nouns and adjectives between English and French, only depending on which of the two rules NP &#8594; [NN JJ], NP &#8594; &#12296;NN JJ&#12297; has a higher probability.</text>
                  <doc_id>61</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The rest of the (sometimes thousands of) rule-specific features usually added to SCFG translation models do not directly help either, leaving reordering decisions disconnected from the rest of the derivation.</text>
                  <doc_id>62</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>While in a decoder this is somehow mitigated by the use of a language model, we believe that the weakness of straightforward applications of SCFGs to model reordering structure at the sentence level misses a chance to learn this crucial part of the translation process during grammar induction.</text>
                  <doc_id>63</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As (Mylonakis and Sima&#8217;an, 2010) note, &#8216;plain&#8217; SCFGs seem to perform worse than the grammars described next, mainly due to wrong long-range reordering decisions for which the language model can hardly help.</text>
                  <doc_id>64</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Hierarchical Reordering SCFG</title>
            <text>We address the weaknesses mentioned above by relying on an SCFG grammar design that is similar to the &#8216;Lexicalised Reordering&#8217; grammar of (Mylonakis and Sima&#8217;an, 2010). As in the rules of Figure 1, we separate non-terminals according to the reordering patterns in which they participate. Nonterminals such as B L , C R take part only in swapping right-hand sides &#12296;B L C R &#12297; (with B L swapping from the source side&#8217;s left to the target side&#8217;s right, C R swapping in the opposite direction), while non-terminals such as B, C take part solely in monotone right-hand side expansions [B C]. These nonterminal categories can appear also on the left-hand side of a rule, as in rule (c) of Figure 1.
In contrast with (Mylonakis and Sima&#8217;an, 2010), monotone and swapping non-terminals do not emit phrase-pairs themselves. Rather, each non-terminal NT is expanded to a dedicated phrase-pair emit- 644 A &#8594; [B C] A &#8594; &#12296;B L C R &#12297;
A L &#8594; [B C] A L &#8594; &#12296;B L C R &#12297;
A R &#8594; [B C] A R &#8594; &#12296;B L C R &#12297;
A &#8594; A P
A L &#8594; A L P
A R &#8594; A R P
A P &#8594; &#945; / &#946;
A L P &#8594; &#945; / &#946;
A R P &#8594; &#945; / &#946;
ting non-terminal NT P , which generates all phrasepairs for it and nothing more. In this way, the preference of non-terminals to either expand towards a (long) phrase-pair or be further analysed recursively is explicitly modelled. Furthermore, this set of pre-terminals allows us to separate the higher order translation structure from the process that emits phrase-pairs, a feature we employ next.
In (Mylonakis and Sima&#8217;an, 2010) this grammar design mainly contributed to model lexical reordering preferences. While we retain this function, for the rich linguistically-motivated grammars used in this work this design effectively propagates reordering preferences above and below the current rule application (e.g. Figure 1, rules (a)-(c)), allowing to learn and apply complex reordering patterns. The different types of grammar rules are summarised in abstract form in Figure 2. We will subsequently refer to this grammar structure as Hierarchical Reordering SCFG (HR-SCFG).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We address the weaknesses mentioned above by relying on an SCFG grammar design that is similar to the &#8216;Lexicalised Reordering&#8217; grammar of (Mylonakis and Sima&#8217;an, 2010).</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As in the rules of Figure 1, we separate non-terminals according to the reordering patterns in which they participate.</text>
                  <doc_id>66</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Nonterminals such as B L , C R take part only in swapping right-hand sides &#12296;B L C R &#12297; (with B L swapping from the source side&#8217;s left to the target side&#8217;s right, C R swapping in the opposite direction), while non-terminals such as B, C take part solely in monotone right-hand side expansions [B C].</text>
                  <doc_id>67</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>These nonterminal categories can appear also on the left-hand side of a rule, as in rule (c) of Figure 1.</text>
                  <doc_id>68</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In contrast with (Mylonakis and Sima&#8217;an, 2010), monotone and swapping non-terminals do not emit phrase-pairs themselves.</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Rather, each non-terminal NT is expanded to a dedicated phrase-pair emit- 644 A &#8594; [B C] A &#8594; &#12296;B L C R &#12297;</text>
                  <doc_id>70</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A L &#8594; [B C] A L &#8594; &#12296;B L C R &#12297;</text>
                  <doc_id>71</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A R &#8594; [B C] A R &#8594; &#12296;B L C R &#12297;</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A &#8594; A P</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A L &#8594; A L P</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A R &#8594; A R P</text>
                  <doc_id>75</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A P &#8594; &#945; / &#946;</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A L P &#8594; &#945; / &#946;</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A R P &#8594; &#945; / &#946;</text>
                  <doc_id>78</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>ting non-terminal NT P , which generates all phrasepairs for it and nothing more.</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this way, the preference of non-terminals to either expand towards a (long) phrase-pair or be further analysed recursively is explicitly modelled.</text>
                  <doc_id>80</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Furthermore, this set of pre-terminals allows us to separate the higher order translation structure from the process that emits phrase-pairs, a feature we employ next.</text>
                  <doc_id>81</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In (Mylonakis and Sima&#8217;an, 2010) this grammar design mainly contributed to model lexical reordering preferences.</text>
                  <doc_id>82</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>While we retain this function, for the rich linguistically-motivated grammars used in this work this design effectively propagates reordering preferences above and below the current rule application (e.g.</text>
                  <doc_id>83</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 1, rules (a)-(c)), allowing to learn and apply complex reordering patterns.</text>
                  <doc_id>84</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The different types of grammar rules are summarised in abstract form in Figure 2.</text>
                  <doc_id>85</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We will subsequently refer to this grammar structure as Hierarchical Reordering SCFG (HR-SCFG).</text>
                  <doc_id>86</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>2.3 Generative Model</title>
            <text>We arrive at a probabilistic SCFG model which jointly generates source e and target f strings, by augmenting each grammar rule with a probability, summing up to one for every left-hand side. The probability of a derivation D of tuple &#12296;e, f&#12297; beginning from start symbol S is equal to the product of the probabilities of the rules used to recursively generate it. We separate the structural part of the derivation D, down to the pre-terminals NT P , from the phraseemission part. The grammar rules pertaining to the
structural part and their associated probabilities define a model p(&#963;) over the latent variable &#963; determining the recursive, reordering and phrase-pair segmenting structure of translation, as in Figure 4. Given &#963;, the phrase-pair emission part merely generates the phrase-pairs utilising distributions from every NT P to the phrase-pairs that it covers, thereby defining a model over all sentence-pairs generated given each translation structure. The probabilities of a derivation and of a sentence-pair are then as follows:
p(D) =p(&#963;)p(e, f |&#963;) (1)
p(e, f) = &#8721; p(D) (2)
D:D &#8727; &#8658;&#12296;e,f&#12297;
By splitting the joint model in a hierarchical structure model and a lexical emission one we facilitate estimating the two models separately. The following section discusses this.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We arrive at a probabilistic SCFG model which jointly generates source e and target f strings, by augmenting each grammar rule with a probability, summing up to one for every left-hand side.</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The probability of a derivation D of tuple &#12296;e, f&#12297; beginning from start symbol S is equal to the product of the probabilities of the rules used to recursively generate it.</text>
                  <doc_id>88</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We separate the structural part of the derivation D, down to the pre-terminals NT P , from the phraseemission part.</text>
                  <doc_id>89</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The grammar rules pertaining to the</text>
                  <doc_id>90</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>structural part and their associated probabilities define a model p(&#963;) over the latent variable &#963; determining the recursive, reordering and phrase-pair segmenting structure of translation, as in Figure 4.</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Given &#963;, the phrase-pair emission part merely generates the phrase-pairs utilising distributions from every NT P to the phrase-pairs that it covers, thereby defining a model over all sentence-pairs generated given each translation structure.</text>
                  <doc_id>92</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The probabilities of a derivation and of a sentence-pair are then as follows:</text>
                  <doc_id>93</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(D) =p(&#963;)p(e, f |&#963;) (1)</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(e, f) = &#8721; p(D) (2)</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>D:D &#8727; &#8658;&#12296;e,f&#12297;</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>By splitting the joint model in a hierarchical structure model and a lexical emission one we facilitate estimating the two models separately.</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The following section discusses this.</text>
                  <doc_id>98</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>2</index>
        <title>3 Learning Translation Structure</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>99</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Phrase-Pair Label Chart</title>
            <text>The input to our learning algorithm is a wordaligned parallel corpus. We consider as phrasepair spans those that obey the word-alignment constraints of (Koehn et al., 2003). For every training sentence-pair, we also input a chart containing one or more labels for every synchronous span, such as that of Figure 3. Each label describes different properties of the phrase pair (syntactic, semantic etc.), possibly in relation to its context, or supplying varying levels of abstraction (phrase-pair, determiner with noun, noun-phrase, sentence etc.). We aim to induce a recursive translation structure explaining the joint generation of the source and target 645
sentence taking advantage of these phrase-pair span labels. For this work we employ the linguistically motivated labels of (Zollmann and Venugopal, 2006), albeit for the source language. Given a parse of the source sentence, each span is assigned the following kind of labels:
Phrase-Pair label
Constituent
All phrase-pairs are assigned the X
Source phrase is a constituent A
Concatenation of Constituents Source phrase labelled A+B as a concatenation of constituents A and B, similarly for 3 constituents.
Partial Constituents Categorial grammar (Bar- Hillel, 1953) inspired labels A/B, A\B, indicating a partial constituent A missing constituent B right or left respectively.
An important point is that we assign all applicable labels to every span. In this way, each label set captures the features of the source side&#8217;s parse-tree without being bounded by the actual parse structure, as well as provides a coarse to fine-grained view of the source phrase.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The input to our learning algorithm is a wordaligned parallel corpus.</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We consider as phrasepair spans those that obey the word-alignment constraints of (Koehn et al., 2003).</text>
                  <doc_id>101</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For every training sentence-pair, we also input a chart containing one or more labels for every synchronous span, such as that of Figure 3.</text>
                  <doc_id>102</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Each label describes different properties of the phrase pair (syntactic, semantic etc.), possibly in relation to its context, or supplying varying levels of abstraction (phrase-pair, determiner with noun, noun-phrase, sentence etc.).</text>
                  <doc_id>103</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We aim to induce a recursive translation structure explaining the joint generation of the source and target 645</text>
                  <doc_id>104</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>sentence taking advantage of these phrase-pair span labels.</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For this work we employ the linguistically motivated labels of (Zollmann and Venugopal, 2006), albeit for the source language.</text>
                  <doc_id>106</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Given a parse of the source sentence, each span is assigned the following kind of labels:</text>
                  <doc_id>107</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Phrase-Pair label</text>
                  <doc_id>108</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Constituent</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>All phrase-pairs are assigned the X</text>
                  <doc_id>110</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Source phrase is a constituent A</text>
                  <doc_id>111</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Concatenation of Constituents Source phrase labelled A+B as a concatenation of constituents A and B, similarly for 3 constituents.</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Partial Constituents Categorial grammar (Bar- Hillel, 1953) inspired labels A/B, A\B, indicating a partial constituent A missing constituent B right or left respectively.</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>An important point is that we assign all applicable labels to every span.</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this way, each label set captures the features of the source side&#8217;s parse-tree without being bounded by the actual parse structure, as well as provides a coarse to fine-grained view of the source phrase.</text>
                  <doc_id>115</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Grammar Extraction</title>
            <text>From every word-aligned sentence-pair and its label chart, we extract SCFG rules as those of Figure 2. Binary rules are extracted from adjoining synchronous spans up to the whole sentence-pair level, with the non-terminals of both left and right-hand side derived from the label names plus their reordering function (monotone, left/right swapping) in the span examined. A single unary rule per non-terminal NT generates the phrase-pair emitting NT P . Unary rules NT P &#8594; &#945; / &#946; generating the phrase-pair are created for all the labels covering it.
While we label the phrase-pairs similarly to (Zollmann and Venugopal, 2006), the extracted grammar is rather different. We do not employ rules that are grounded to lexical context (&#8216;gap&#8217; rules), relying instead on the reordering-aware non-terminal set and related unary and binary rules. The result is a grammar which can both capture a rich array of translation phenomena based on linguistic and lexical grounds and explicitly model the balance between
WHNP
WHNP P
which der SBAR
&lt; SBAR\WHNP &gt;
VP/NP L
VP/NP L P
is ist NP
NP P
the solution die L&#246;sung NP R
PP
PP P
to the problem f&#252;r das Problem
memorising long phrase-pairs and generalising over yet unseen ones, as shown in the next example.
The derivation in Figure 4 illustrates some of the formalism&#8217;s features. A preference to reorder based on lexical content is applied for is / ist. Noun phrase NP R is recursively constructed with a preference to constitute the right branch of an order swapping nonterminal expansion. This is matched with VP/NP L which reorders in the opposite direction. The labels VP/NP and SBAR\WHNP allow linguistic syntax context to influence the lexical and reordering translation choices. Crucially, all these lexical, attachment and reordering preferences (as encoded in the model&#8217;s rules and probabilities) must be matched together to arrive at the analysis in Figure 4.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>From every word-aligned sentence-pair and its label chart, we extract SCFG rules as those of Figure 2.</text>
                  <doc_id>116</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Binary rules are extracted from adjoining synchronous spans up to the whole sentence-pair level, with the non-terminals of both left and right-hand side derived from the label names plus their reordering function (monotone, left/right swapping) in the span examined.</text>
                  <doc_id>117</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>A single unary rule per non-terminal NT generates the phrase-pair emitting NT P .</text>
                  <doc_id>118</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Unary rules NT P &#8594; &#945; / &#946; generating the phrase-pair are created for all the labels covering it.</text>
                  <doc_id>119</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>While we label the phrase-pairs similarly to (Zollmann and Venugopal, 2006), the extracted grammar is rather different.</text>
                  <doc_id>120</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We do not employ rules that are grounded to lexical context (&#8216;gap&#8217; rules), relying instead on the reordering-aware non-terminal set and related unary and binary rules.</text>
                  <doc_id>121</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The result is a grammar which can both capture a rich array of translation phenomena based on linguistic and lexical grounds and explicitly model the balance between</text>
                  <doc_id>122</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>WHNP</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>WHNP P</text>
                  <doc_id>124</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>which der SBAR</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&lt; SBAR\WHNP &gt;</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VP/NP L</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VP/NP L P</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>is ist NP</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NP P</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the solution die L&#246;sung NP R</text>
                  <doc_id>131</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>PP</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>PP P</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>to the problem f&#252;r das Problem</text>
                  <doc_id>134</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>memorising long phrase-pairs and generalising over yet unseen ones, as shown in the next example.</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The derivation in Figure 4 illustrates some of the formalism&#8217;s features.</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A preference to reorder based on lexical content is applied for is / ist.</text>
                  <doc_id>137</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Noun phrase NP R is recursively constructed with a preference to constitute the right branch of an order swapping nonterminal expansion.</text>
                  <doc_id>138</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This is matched with VP/NP L which reorders in the opposite direction.</text>
                  <doc_id>139</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The labels VP/NP and SBAR\WHNP allow linguistic syntax context to influence the lexical and reordering translation choices.</text>
                  <doc_id>140</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Crucially, all these lexical, attachment and reordering preferences (as encoded in the model&#8217;s rules and probabilities) must be matched together to arrive at the analysis in Figure 4.</text>
                  <doc_id>141</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Parameter Estimation</title>
            <text>We estimate the parameters for the phrase-emission model p(e, f |&#963;) using Relative Frequency Estimation (RFE) on the label charts induced for the training sentence-pairs, after the labels have been augmented by the reordering indications. In the RFE estimate, every rule NT P &#8594; &#945; / &#946; receives a probability in proportion with the times that &#945; / &#946; was covered by the NT label.
On the other hand, estimating the parameters under Maximum-Likelihood Estimation (MLE) for the latent translation structure model p(&#963;) is bound to overfit towards memorising whole sentence-pairs as discussed in (Mylonakis and Sima&#8217;an, 2010), with the resulting grammar estimate not being able to 646
generalise past the training data. However, apart from overfitting towards long phrase-pairs, a grammar with millions of structural rules is also liable to overfit towards degenerate latent structures which, while fitting the training data well, have limited applicability to unseen sentences.
We avoid both pitfalls by estimating the grammar probabilities with the Cross-Validating Expectation- Maximization algorithm (CV-EM) (Mylonakis and Sima&#8217;an, 2008; Mylonakis and Sima&#8217;an, 2010). CV- EM is a cross-validating instance of the well known EM algorithm (Dempster et al., 1977). It works iteratively on a partition of the training data, climbing the likelihood of the training data while crossvalidating the latent variable values, considering for every training data point only those which can be produced by models built from the rest of the data excluding the current part. As a result, the estimation process simulates maximising future data likelihood, using the training data to directly aim towards strong generalisation of the estimate.
For our probabilistic SCFG-based translation structure variable &#963;, implementing CV-EM boils down to a synchronous version of the Inside-Outside algorithm, modified to enforce the CV criterion. In this way we arrive at cross-validated ML estimate of the &#963; parameters while keeping the phrase-emission parameters of p(e, f |&#963;) fixed. The CV-criterion, apart from avoiding overfitting, results in discarding the structural rules which are only found in a single part of the training corpus, leading to a more compact grammar while still retaining millions of structural rules that are more hopeful to generalise.
Unravelling the joint generative process, by modelling latent hierarchical structure separately from phrase-pair emission, allows us to concentrate our inference efforts towards the hidden, higher-level translation mechanism.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We estimate the parameters for the phrase-emission model p(e, f |&#963;) using Relative Frequency Estimation (RFE) on the label charts induced for the training sentence-pairs, after the labels have been augmented by the reordering indications.</text>
                  <doc_id>142</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the RFE estimate, every rule NT P &#8594; &#945; / &#946; receives a probability in proportion with the times that &#945; / &#946; was covered by the NT label.</text>
                  <doc_id>143</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>On the other hand, estimating the parameters under Maximum-Likelihood Estimation (MLE) for the latent translation structure model p(&#963;) is bound to overfit towards memorising whole sentence-pairs as discussed in (Mylonakis and Sima&#8217;an, 2010), with the resulting grammar estimate not being able to 646</text>
                  <doc_id>144</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>generalise past the training data.</text>
                  <doc_id>145</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, apart from overfitting towards long phrase-pairs, a grammar with millions of structural rules is also liable to overfit towards degenerate latent structures which, while fitting the training data well, have limited applicability to unseen sentences.</text>
                  <doc_id>146</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We avoid both pitfalls by estimating the grammar probabilities with the Cross-Validating Expectation- Maximization algorithm (CV-EM) (Mylonakis and Sima&#8217;an, 2008; Mylonakis and Sima&#8217;an, 2010).</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>CV- EM is a cross-validating instance of the well known EM algorithm (Dempster et al., 1977).</text>
                  <doc_id>148</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It works iteratively on a partition of the training data, climbing the likelihood of the training data while crossvalidating the latent variable values, considering for every training data point only those which can be produced by models built from the rest of the data excluding the current part.</text>
                  <doc_id>149</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>As a result, the estimation process simulates maximising future data likelihood, using the training data to directly aim towards strong generalisation of the estimate.</text>
                  <doc_id>150</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For our probabilistic SCFG-based translation structure variable &#963;, implementing CV-EM boils down to a synchronous version of the Inside-Outside algorithm, modified to enforce the CV criterion.</text>
                  <doc_id>151</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this way we arrive at cross-validated ML estimate of the &#963; parameters while keeping the phrase-emission parameters of p(e, f |&#963;) fixed.</text>
                  <doc_id>152</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The CV-criterion, apart from avoiding overfitting, results in discarding the structural rules which are only found in a single part of the training corpus, leading to a more compact grammar while still retaining millions of structural rules that are more hopeful to generalise.</text>
                  <doc_id>153</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Unravelling the joint generative process, by modelling latent hierarchical structure separately from phrase-pair emission, allows us to concentrate our inference efforts towards the hidden, higher-level translation mechanism.</text>
                  <doc_id>154</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>4 Experiments</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>155</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Decoding Model</title>
            <text>The induced joint translation model can be used to recover arg max e p(e|f), as it is equal to arg max e p(e, f). We employ the induced probabilistic HR-SCFG G as the backbone of a log-linear, feature based translation model, with the derivation probability p(D) under the grammar estimate being
one of the features. This is augmented with a small number n of additional smoothing features &#966; i for derivation rules r: (a) conditional phrase translation probabilities, (b) lexical phrase translation probabilities, (c) word generation penalty, and (d) a count of swapping reordering operations. Features (a), (b) and (c) are applicable to phrase-pair emission rules and features for both translation directions are used, while (d) is only triggered by structural rules.
These extra features assess translation quality past the synchronous grammar derivation and learning general reordering or word emission preferences for the language pair. As an example, while our probabilistic HR-SCFG maintains a separate joint phrase-pair emission distribution per non-terminal, the smoothing features (a) above assess the conditional translation of surface phrases irrespective of any notion of recursive translation structure.
The final feature is the language model score for the target sentence, mounting up to the following model used at decoding time, with the feature weights &#955; trained by Minimum Error Rate Training (MERT) (Och, 2003) on a development corpus.
p(D &#8727; &#8658; &#12296;e, f&#12297;) &#8733; p(e) &#955; lm p G (D) &#955; G</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The induced joint translation model can be used to recover arg max e p(e|f), as it is equal to arg max e p(e, f).</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We employ the induced probabilistic HR-SCFG G as the backbone of a log-linear, feature based translation model, with the derivation probability p(D) under the grammar estimate being</text>
                  <doc_id>157</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>one of the features.</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This is augmented with a small number n of additional smoothing features &#966; i for derivation rules r: (a) conditional phrase translation probabilities, (b) lexical phrase translation probabilities, (c) word generation penalty, and (d) a count of swapping reordering operations.</text>
                  <doc_id>159</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Features (a), (b) and (c) are applicable to phrase-pair emission rules and features for both translation directions are used, while (d) is only triggered by structural rules.</text>
                  <doc_id>160</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>These extra features assess translation quality past the synchronous grammar derivation and learning general reordering or word emission preferences for the language pair.</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As an example, while our probabilistic HR-SCFG maintains a separate joint phrase-pair emission distribution per non-terminal, the smoothing features (a) above assess the conditional translation of surface phrases irrespective of any notion of recursive translation structure.</text>
                  <doc_id>162</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The final feature is the language model score for the target sentence, mounting up to the following model used at decoding time, with the feature weights &#955; trained by Minimum Error Rate Training (MERT) (Och, 2003) on a development corpus.</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(D &#8727; &#8658; &#12296;e, f&#12297;) &#8733; p(e) &#955; lm p G (D) &#955; G</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Decoding Modifications</title>
            <text>n&#8719; &#8719; &#966; i (r) &#955; i
i=1 r&#8712;D
We use a customised version of the Joshua SCFG decoder (Li et al., 2009) to translate, with the following modifications:
Source Labels Constraints As for this work the phrase-pair labels used to extract the grammar are based on the linguistic analysis of the source side, we can construct the label chart for every input sentence from its parse. We subsequently use it to consider only derivations with synchronous spans which are covered by non-terminals matching one of the labels for those spans. This applies both for the nonterminals covering phrase-pairs as well as the higher level parts of the derivation.
In this manner we not only constrain the translation hypotheses resulting in faster decoding time, but, more importantly, we may ground the hypotheses more closely to the available linguistic information of the source sentence. This is of particular interest as we move up the derivation tree, where 647
an initial wrong choice below could propagate towards hypotheses wildly diverging from the input sentence&#8217;s linguistic annotation.
Per Non-Terminal Pruning The decoder uses a combination of beam and cube-pruning (Huang and Chiang, 2007). As our grammar uses non-terminals in the hundreds of thousands, it is important not to prune away prematurely non-terminals covering smaller spans and to leave more options to be considered as we move up the derivation tree.
For this, for every cell in the decoder&#8217;s chart, we keep a separate bin per non-terminal and prune together hypotheses leading to the same non-terminal covering a cell. This allows full derivations to be found for all input sentences, as well as avoids aggressive pruning at an early stage. Given the source label constraint discussed above, this does not increase running times or memory demands considerably as we allow only up to a few tens of nonterminals per span.
Expected Counts Rule Pruning To compact the hierarchical structure part of the grammar prior to decoding, we prune rules that fail to accumulate 10 &#8722;8 expected counts during the last CV-EM iteration. For English to German, this brings the structural rules from 15M down to 1.2M. Note that we do not prune the phrase-pair emitting rules. Overall, we consider this a much more informed pruning criterion than those based on probability values (that are not comparable across left-hand sides) or righthand side counts (frequent symbols need many more expansions than a highly specialised one).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>n&#8719; &#8719; &#966; i (r) &#955; i</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1 r&#8712;D</text>
                  <doc_id>166</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We use a customised version of the Joshua SCFG decoder (Li et al., 2009) to translate, with the following modifications:</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Source Labels Constraints As for this work the phrase-pair labels used to extract the grammar are based on the linguistic analysis of the source side, we can construct the label chart for every input sentence from its parse.</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We subsequently use it to consider only derivations with synchronous spans which are covered by non-terminals matching one of the labels for those spans.</text>
                  <doc_id>169</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This applies both for the nonterminals covering phrase-pairs as well as the higher level parts of the derivation.</text>
                  <doc_id>170</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In this manner we not only constrain the translation hypotheses resulting in faster decoding time, but, more importantly, we may ground the hypotheses more closely to the available linguistic information of the source sentence.</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This is of particular interest as we move up the derivation tree, where 647</text>
                  <doc_id>172</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>an initial wrong choice below could propagate towards hypotheses wildly diverging from the input sentence&#8217;s linguistic annotation.</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Per Non-Terminal Pruning The decoder uses a combination of beam and cube-pruning (Huang and Chiang, 2007).</text>
                  <doc_id>174</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As our grammar uses non-terminals in the hundreds of thousands, it is important not to prune away prematurely non-terminals covering smaller spans and to leave more options to be considered as we move up the derivation tree.</text>
                  <doc_id>175</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For this, for every cell in the decoder&#8217;s chart, we keep a separate bin per non-terminal and prune together hypotheses leading to the same non-terminal covering a cell.</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This allows full derivations to be found for all input sentences, as well as avoids aggressive pruning at an early stage.</text>
                  <doc_id>177</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Given the source label constraint discussed above, this does not increase running times or memory demands considerably as we allow only up to a few tens of nonterminals per span.</text>
                  <doc_id>178</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Expected Counts Rule Pruning To compact the hierarchical structure part of the grammar prior to decoding, we prune rules that fail to accumulate 10 &#8722;8 expected counts during the last CV-EM iteration.</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For English to German, this brings the structural rules from 15M down to 1.2M.</text>
                  <doc_id>180</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Note that we do not prune the phrase-pair emitting rules.</text>
                  <doc_id>181</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Overall, we consider this a much more informed pruning criterion than those based on probability values (that are not comparable across left-hand sides) or righthand side counts (frequent symbols need many more expansions than a highly specialised one).</text>
                  <doc_id>182</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>4.3 Experimental Setting &amp; Baseline</title>
            <text>We evaluate our method on four different language pairs with English as the source language and French, German, Dutch and Chinese as target. The data for the first three language pairs are derived from parliament proceedings sourced from the Europarl corpus (Koehn, 2005), with WMT- 07 development and test data for French and German. The data for the English to Chinese task is composed of parliament proceedings and news articles. For all language pairs we employ 200K and 400K sentence pairs for training, 2K for development and 2K for testing (single reference per source sentence). Both the baseline and our method decode
Training French German Dutch Chinese English to set size BLEU NIST BLEU NIST BLEU NIST BLEU NIST josh-base 29.20 7.2123 18.65 5.8047 21.97 6.2469 22.34 6.5540
200K lts 29.43 7.2611** 19.10** 5.8714** 22.31* 6.2903* 23.67** 6.6595**
400K
with a 3-gram language model smoothed with modified Knesser-Ney discounting (Chen and Goodman, 1998), trained on around 1M sentences per target language. The parses of the source sentences employed by our system during training and decoding are created with the Charniak parser (Charniak, 2000). We compare against a state-of-the-art hierarchical translation (Chiang, 2005) baseline, based on the Joshua translation system under the default training and decoding settings (josh-base). Apart of evaluating against a state-of-the-art system, especially on the English-Chinese language pair, the comparison has an added interesting aspect. The heuristically trained baseline takes advantage of &#8216;gap rules&#8217; to reorder based on lexical context cues, but makes very limited use of the hierarchical structure above the lexical surface. In contrast, our method induces a grammar with no such rules, relying on lexical content and the strength of a higher level translation structure instead.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We evaluate our method on four different language pairs with English as the source language and French, German, Dutch and Chinese as target.</text>
                  <doc_id>183</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The data for the first three language pairs are derived from parliament proceedings sourced from the Europarl corpus (Koehn, 2005), with WMT- 07 development and test data for French and German.</text>
                  <doc_id>184</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The data for the English to Chinese task is composed of parliament proceedings and news articles.</text>
                  <doc_id>185</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For all language pairs we employ 200K and 400K sentence pairs for training, 2K for development and 2K for testing (single reference per source sentence).</text>
                  <doc_id>186</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Both the baseline and our method decode</text>
                  <doc_id>187</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Training French German Dutch Chinese English to set size BLEU NIST BLEU NIST BLEU NIST BLEU NIST josh-base 29.20 7.2123 18.65 5.8047 21.97 6.2469 22.34 6.5540</text>
                  <doc_id>188</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>200K lts 29.43 7.2611** 19.10** 5.8714** 22.31* 6.2903* 23.67** 6.6595**</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>400K</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>with a 3-gram language model smoothed with modified Knesser-Ney discounting (Chen and Goodman, 1998), trained on around 1M sentences per target language.</text>
                  <doc_id>191</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The parses of the source sentences employed by our system during training and decoding are created with the Charniak parser (Charniak, 2000).</text>
                  <doc_id>192</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We compare against a state-of-the-art hierarchical translation (Chiang, 2005) baseline, based on the Joshua translation system under the default training and decoding settings (josh-base).</text>
                  <doc_id>193</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Apart of evaluating against a state-of-the-art system, especially on the English-Chinese language pair, the comparison has an added interesting aspect.</text>
                  <doc_id>194</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The heuristically trained baseline takes advantage of &#8216;gap rules&#8217; to reorder based on lexical context cues, but makes very limited use of the hierarchical structure above the lexical surface.</text>
                  <doc_id>195</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast, our method induces a grammar with no such rules, relying on lexical content and the strength of a higher level translation structure instead.</text>
                  <doc_id>196</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>4.4 Training &amp; Decoding Details</title>
            <text>To train our Latent Translation Structure (LTS) system, we used the following settings. CV-EM crossvalidated on a 10-part partition of the training data and performed 10 iterations. The structural rule probabilities were initialised to uniform per lefthand side.
The decoder does not employ any &#8216;glue grammar&#8217; as is usual with hierarchical translation systems to limit reordering up to a certain cut-off length. Instead, we rely on our LTS grammar to reorder and construct the translation output up to the full sentence length.
In summary, our system&#8217;s experimental pipeline is as follows. All input sentences are parsed and label charts are created from these parses. The Hierarchi- 648
cal Reordering SCFG is extracted and its parameters are estimated employing CV-EM. The structural rules of the estimate are pruned according to their expected counts and smoothing features are added to all rules. We train the feature weights under MERT and decode with the resulting log-linear model.
The overall training and decoding setup is appealing also regarding computational demands. On an 8-core 2.3GHz system, training on 200K sentencepairs demands 4.5 hours while decoding runs on 25 sentences per minute.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>To train our Latent Translation Structure (LTS) system, we used the following settings.</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>CV-EM crossvalidated on a 10-part partition of the training data and performed 10 iterations.</text>
                  <doc_id>198</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The structural rule probabilities were initialised to uniform per lefthand side.</text>
                  <doc_id>199</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The decoder does not employ any &#8216;glue grammar&#8217; as is usual with hierarchical translation systems to limit reordering up to a certain cut-off length.</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Instead, we rely on our LTS grammar to reorder and construct the translation output up to the full sentence length.</text>
                  <doc_id>201</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In summary, our system&#8217;s experimental pipeline is as follows.</text>
                  <doc_id>202</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>All input sentences are parsed and label charts are created from these parses.</text>
                  <doc_id>203</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The Hierarchi- 648</text>
                  <doc_id>204</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>cal Reordering SCFG is extracted and its parameters are estimated employing CV-EM.</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The structural rules of the estimate are pruned according to their expected counts and smoothing features are added to all rules.</text>
                  <doc_id>206</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We train the feature weights under MERT and decode with the resulting log-linear model.</text>
                  <doc_id>207</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The overall training and decoding setup is appealing also regarding computational demands.</text>
                  <doc_id>208</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>On an 8-core 2.3GHz system, training on 200K sentencepairs demands 4.5 hours while decoding runs on 25 sentences per minute.</text>
                  <doc_id>209</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>4</index>
            <title>4.5 Results</title>
            <text>Table 1 presents the results for the baseline and our method for the 4 language pairs, for training sets of both 200K and 400K sentence pairs. Our system (lts) outperforms the baseline for all 4 language pairs for both BLEU and NIST scores, by a margin which scales up to +1.92 BLEU points for English to Chinese translation when training on the 400K set. In addition, increasing the size of the training data from 200K to 400K sentence pairs widens the performance margin between the baseline and our system, in some cases considerably. All but one of the performance improvements are found to be statistically significant (Koehn, 2004) at the 95% confidence level, most of them also at the 99% level.
We selected an array of target languages of increasing reordering complexity with English as source. Examining the results across the target languages, LTS performance gains increase the more challenging the sentence structure of the target language is in relation to the source&#8217;s, highlighted when translating to Chinese. Even for Dutch and German, which pose additional challenges such as compound words and morphology which we do not explicitly treat in the current system, LTS still delivers significant improvements in performance. Additionally,
(a)
(b)
the robustness of our system is exemplified by delivering significant performance increases for all language pairs. For the English to Chinese translation task, we performed further experiments along two axes. We first investigate the contribution of the linguistic annotations, by comparing our complete system (lts) with an otherwise identical implementation (lts-nolabels) which does not employ any linguistically motivated labels. The latter system then uses a labels chart as that of Figure 3, which however labels all phrase-pair spans solely with the generic X label. The results in Table 2(a) indicate that a large part of the performance improvement can be attributed to the use of the linguistic annotations extracted from the source parse trees, indicating the potential of the LTS system to take advantage of such additional annotations to deliver better translations. The second additional experiment relates to the impact of employing a stronger language model during decoding, which may increase performance but slows down decoding speed. Notably, as can be seen in Table 2(b), switching to a 4-gram LM results in performance gains for both the baseline and our system and while the margin between the two systems decreases, our system continues to deliver a considerable and significant improvement in translation BLEU scores.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Table 1 presents the results for the baseline and our method for the 4 language pairs, for training sets of both 200K and 400K sentence pairs.</text>
                  <doc_id>210</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Our system (lts) outperforms the baseline for all 4 language pairs for both BLEU and NIST scores, by a margin which scales up to +1.92 BLEU points for English to Chinese translation when training on the 400K set.</text>
                  <doc_id>211</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, increasing the size of the training data from 200K to 400K sentence pairs widens the performance margin between the baseline and our system, in some cases considerably.</text>
                  <doc_id>212</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>All but one of the performance improvements are found to be statistically significant (Koehn, 2004) at the 95% confidence level, most of them also at the 99% level.</text>
                  <doc_id>213</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We selected an array of target languages of increasing reordering complexity with English as source.</text>
                  <doc_id>214</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Examining the results across the target languages, LTS performance gains increase the more challenging the sentence structure of the target language is in relation to the source&#8217;s, highlighted when translating to Chinese.</text>
                  <doc_id>215</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Even for Dutch and German, which pose additional challenges such as compound words and morphology which we do not explicitly treat in the current system, LTS still delivers significant improvements in performance.</text>
                  <doc_id>216</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Additionally,</text>
                  <doc_id>217</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(a)</text>
                  <doc_id>218</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(b)</text>
                  <doc_id>219</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the robustness of our system is exemplified by delivering significant performance increases for all language pairs.</text>
                  <doc_id>220</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For the English to Chinese translation task, we performed further experiments along two axes.</text>
                  <doc_id>221</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We first investigate the contribution of the linguistic annotations, by comparing our complete system (lts) with an otherwise identical implementation (lts-nolabels) which does not employ any linguistically motivated labels.</text>
                  <doc_id>222</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The latter system then uses a labels chart as that of Figure 3, which however labels all phrase-pair spans solely with the generic X label.</text>
                  <doc_id>223</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The results in Table 2(a) indicate that a large part of the performance improvement can be attributed to the use of the linguistic annotations extracted from the source parse trees, indicating the potential of the LTS system to take advantage of such additional annotations to deliver better translations.</text>
                  <doc_id>224</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The second additional experiment relates to the impact of employing a stronger language model during decoding, which may increase performance but slows down decoding speed.</text>
                  <doc_id>225</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Notably, as can be seen in Table 2(b), switching to a 4-gram LM results in performance gains for both the baseline and our system and while the margin between the two systems decreases, our system continues to deliver a considerable and significant improvement in translation BLEU scores.</text>
                  <doc_id>226</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>5 Related Work</title>
        <text>In this work, we focus on the combination of learning latent structure with syntax and linguistic annotations, exploring the crossroads of machine 649
learning, linguistic syntax and machine translation. Training a joint probability model was first discussed in (Marcu and Wong, 2002). We show that a translation system based on such a joint model can perform competitively in comparison with conditional probability models, when it is augmented with a rich latent hierarchical structure trained adequately to avoid overfitting.
Earlier approaches for linguistic syntax-based translation such as (Yamada and Knight, 2001; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006) focus on memorising and reusing parts of the structure of the source and/or target parse trees and constraining decoding by the input parse tree. In contrast to this approach, we choose to employ linguistic annotations in the form of unambiguous synchronous span labels, while discovering ambiguous translation structure taking advantage of them.
Later work (Marton and Resnik, 2008; Venugopal et al., 2009; Chiang et al., 2009) takes a more flexible approach, influencing translation output using linguistically motivated features, or features based on source-side linguistically-guided latent syntactic categories (Huang et al., 2010). A feature-based approach and ours are not mutually exclusive, as we also employ a limited set of features next to our trained model during decoding. We find augmenting our system with a more extensive feature set an interesting research direction for the future.
An array of recent work (Chiang, 2010; Zhang et al., 2008; Liu et al., 2009) sets off to utilise source and target syntax for translation. While for this work we constrain ourselves to source language syntax annotations, our method can be directly applied to employ labels taking advantage of linguistic annotations from both sides of translation. The decoding constraints of section 4.2 can then still be applied on the source part of hybrid source-target labels.
For the experiments in this paper we employ a label set similar to the non-terminals set of (Zollmann and Venugopal, 2006). However, the synchronous grammars we learn share few similarities with those that they heuristically extract. The HR-SCFG we adopt allows capturing more complex reordering phenomena and, in contrast to both (Chiang, 2005; Zollmann and Venugopal, 2006), is not exposed to the issues highlighted in section 2.1. Nevertheless, our results underline the capacity of linguistic anno-
tations similar to those of (Zollmann and Venugopal, 2006) as part of latent translation variables. Most of the aforementioned work does concentrate on learning hierarchical, linguistically motivated translation models. Cohn and Blunsom (2009) sample rules of the form proposed in (Galley et al., 2004) from a Bayesian model, employing Dirichlet Process priors favouring smaller rules to avoid overfitting. Their grammar is however also based on the target parse-tree structure, with their system surpassing a weak baseline by a small margin. In contrast to the Bayesian approach which imposes external priors to lead estimation away from degenerate solutions, we take a data-driven approach to arrive to estimates which generalise well. The rich linguistically motivated latent variable learnt by our method delivers translation performance that compares favourably to a state-of-the-art system. Mylonakis and Sima&#8217;an (2010) also employ the CV-EM algorithm to estimate the parameters of an SCFG, albeit a much simpler one based on a handful of non-terminals. In this work we employ some of their grammar design principles for an immensely more complex grammar with millions of hierarchical latent structure rules and show how such grammar can be learnt and applied taking advantage of source language linguistic annotations.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this work, we focus on the combination of learning latent structure with syntax and linguistic annotations, exploring the crossroads of machine 649</text>
              <doc_id>227</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>learning, linguistic syntax and machine translation.</text>
              <doc_id>228</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Training a joint probability model was first discussed in (Marcu and Wong, 2002).</text>
              <doc_id>229</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We show that a translation system based on such a joint model can perform competitively in comparison with conditional probability models, when it is augmented with a rich latent hierarchical structure trained adequately to avoid overfitting.</text>
              <doc_id>230</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Earlier approaches for linguistic syntax-based translation such as (Yamada and Knight, 2001; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006) focus on memorising and reusing parts of the structure of the source and/or target parse trees and constraining decoding by the input parse tree.</text>
              <doc_id>231</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In contrast to this approach, we choose to employ linguistic annotations in the form of unambiguous synchronous span labels, while discovering ambiguous translation structure taking advantage of them.</text>
              <doc_id>232</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Later work (Marton and Resnik, 2008; Venugopal et al., 2009; Chiang et al., 2009) takes a more flexible approach, influencing translation output using linguistically motivated features, or features based on source-side linguistically-guided latent syntactic categories (Huang et al., 2010).</text>
              <doc_id>233</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A feature-based approach and ours are not mutually exclusive, as we also employ a limited set of features next to our trained model during decoding.</text>
              <doc_id>234</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We find augmenting our system with a more extensive feature set an interesting research direction for the future.</text>
              <doc_id>235</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>An array of recent work (Chiang, 2010; Zhang et al., 2008; Liu et al., 2009) sets off to utilise source and target syntax for translation.</text>
              <doc_id>236</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>While for this work we constrain ourselves to source language syntax annotations, our method can be directly applied to employ labels taking advantage of linguistic annotations from both sides of translation.</text>
              <doc_id>237</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The decoding constraints of section 4.2 can then still be applied on the source part of hybrid source-target labels.</text>
              <doc_id>238</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For the experiments in this paper we employ a label set similar to the non-terminals set of (Zollmann and Venugopal, 2006).</text>
              <doc_id>239</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, the synchronous grammars we learn share few similarities with those that they heuristically extract.</text>
              <doc_id>240</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The HR-SCFG we adopt allows capturing more complex reordering phenomena and, in contrast to both (Chiang, 2005; Zollmann and Venugopal, 2006), is not exposed to the issues highlighted in section 2.1.</text>
              <doc_id>241</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Nevertheless, our results underline the capacity of linguistic anno-</text>
              <doc_id>242</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tations similar to those of (Zollmann and Venugopal, 2006) as part of latent translation variables.</text>
              <doc_id>243</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Most of the aforementioned work does concentrate on learning hierarchical, linguistically motivated translation models.</text>
              <doc_id>244</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Cohn and Blunsom (2009) sample rules of the form proposed in (Galley et al., 2004) from a Bayesian model, employing Dirichlet Process priors favouring smaller rules to avoid overfitting.</text>
              <doc_id>245</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Their grammar is however also based on the target parse-tree structure, with their system surpassing a weak baseline by a small margin.</text>
              <doc_id>246</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In contrast to the Bayesian approach which imposes external priors to lead estimation away from degenerate solutions, we take a data-driven approach to arrive to estimates which generalise well.</text>
              <doc_id>247</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The rich linguistically motivated latent variable learnt by our method delivers translation performance that compares favourably to a state-of-the-art system.</text>
              <doc_id>248</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Mylonakis and Sima&#8217;an (2010) also employ the CV-EM algorithm to estimate the parameters of an SCFG, albeit a much simpler one based on a handful of non-terminals.</text>
              <doc_id>249</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In this work we employ some of their grammar design principles for an immensely more complex grammar with millions of hierarchical latent structure rules and show how such grammar can be learnt and applied taking advantage of source language linguistic annotations.</text>
              <doc_id>250</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>6 Conclusions</title>
        <text>In this work we contribute a method to learn and apply latent hierarchical translation structure. To this end, we take advantage of source-language linguistic annotations to motivate instead of constrain the translation process. An input chart over phrasepair spans, with each cell filled with multiple linguistically motivated labels, is coupled with the HR- SCFG design to arrive at a rich synchronous grammar with millions of structural rules and the capacity to capture complex linguistically conditioned translation phenomena. We address overfitting issues by cross-validating climbing the likelihood of the training data and propose solutions to increase the efficiency and accuracy of decoding. An interesting aspect of our work is delivering competitive performance for difficult language pairs such as English-Chinese with a joint probability generative model and an SCFG without &#8216;gap rules&#8217;. 650
Instead of employing hierarchical phrase-pairs, we invest in learning the higher-order hierarchical synchronous structure behind translation, up to the full sentence length. While these choices and the related results challenge current MT research trends, they are not mutually exclusive with them. Future work directions include investigating the impact of hierarchical phrases for our models as well as any gains from additional features in the log-linear decoding model.
Smoothing the HR-SCFG grammar estimates could prove a possible source of further performance improvements. Learning translation and reordering behaviour with respect to linguistic cues is facilitated in our approach by keeping separate phrase-pair emission distributions per emitting nonterminal and reordering pattern, while the employment of the generic X non-terminals already allows backing off to more coarse-grained rules. Nevertheless, we still believe that further smoothing of these sparse distributions, e.g. by interpolating them with less sparse ones, could in the future lead to an additional increase in translation quality.
Finally, we discuss in this work how our method can already utilise hundreds of thousands of phrasepair labels and millions of structural rules. A further promising direction is broadening this set with labels taking advantage of both source and targetlanguage linguistic annotation or categories exploring additional phrase-pair properties past the parse trees such as semantic annotations.
Acknowledgments
Both authors are supported by a VIDI grant (nr. 639.022.604) from The Netherlands Organization for Scientific Research (NWO). The authors would like to thank Maxim Khalilov for helping with experimental data and Andreas Zollmann and the anonymous reviewers for their valuable comments.
References
Yehoshua Bar-Hillel. 1953. A quasi-arithmetical notation for syntactic description. Language, 29(1):47&#8211;58. Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the North American Association for Computational Linguistics (HLT/NAACL), Seattle, Washington, USA, April.
Stanley Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University, August. David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 218&#8211;226, Boulder, Colorado, June. Association for Computational Linguistics. David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of ACL 2005, pages 263&#8211;270. David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1443&#8211;1452, Uppsala, Sweden, July. Association for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model of syntax-directed tree to string grammar induction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 352&#8211;361, Singapore, August. Association for Computational Linguistics. A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1&#8211;38. John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006. Why generative phrase models underperform surface heuristics. In Proceedings on the Workshop on Statistical Machine Translation, pages 31&#8211;38, New York City. Association for Computational Linguistics. Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What&#8217;s in a translation rule? In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 273&#8211;280, Boston, Massachusetts, USA, May. Association for Computational Linguistics. Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 961&#8211; 968, Sydney, Australia, July. Association for Computational Linguistics. Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144&#8211;151, 651
Prague, Czech Republic, June. Association for Computational Linguistics. Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA), Boston, MA, USA. Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions.
machine translation evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388&#8211;395, Barcelona, Spain, July. Association for Computational Linguistics. Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In MT Summit 2005. Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 135&#8211;139, Athens, Greece, March. Association for Computational Linguistics. Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 609&#8211;616, Sydney, Australia, July. Association for Computational Linguistics. Yang Liu, Yajuan L&#252;, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 558&#8211;566, Suntec, Singapore, August. Association for Computational Linguistics. Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine translation. In Proceedings of Empirical methods in natural language processing, pages 133&#8211;139. Association for Computational Linguistics. Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation. In Proceedings of ACL-08: HLT, pages 1003&#8211;1011,
Columbus, Ohio, June. Association for Computational
Linguistics. Markos Mylonakis and Khalil Sima&#8217;an. 2008. Phrase
translation probabilities with ITG priors and smoothing as learning objective. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 630&#8211;639, Honolulu, USA, October. Markos Mylonakis and Khalil Sima&#8217;an. 2010. Learning probabilistic synchronous CFGs for phrase-based translation. In Fourteenth Conference on Computational Natural Language Learning, Uppsala, Sweden, July.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160&#8211;167, Sapporo, Japan, July. Association for Computational Linguistics.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt. In Proceedings of 43rd Annual Meeting of the Association for Computational Linguistics, Ann Arbor, Michigan, USA, June. Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: Softening syntactic constraints to improve statistical machine translation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 236&#8211;244, Boulder, Colorado, June. Association for Computational Linguistics. Wei Wang, Jonathan May, Kevin Knight, and Daniel Marcu. 2010. Re-structuring, re-labeling, and realigning for syntax-based machine translation. Computational Linguistics, 36(2):247&#8211;277. Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377&#8211;403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 523&#8211;530, Toulouse, France, July. Association for Computational Linguistics.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In Proceedings of ACL-08: HLT, pages 559&#8211;567, Columbus, Ohio, June. Association for Computational Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings on the Workshop on Statistical Machine 652 Translation, pages 138&#8211;141, New York City, June. Association for Computational Linguistics.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this work we contribute a method to learn and apply latent hierarchical translation structure.</text>
              <doc_id>251</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To this end, we take advantage of source-language linguistic annotations to motivate instead of constrain the translation process.</text>
              <doc_id>252</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>An input chart over phrasepair spans, with each cell filled with multiple linguistically motivated labels, is coupled with the HR- SCFG design to arrive at a rich synchronous grammar with millions of structural rules and the capacity to capture complex linguistically conditioned translation phenomena.</text>
              <doc_id>253</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We address overfitting issues by cross-validating climbing the likelihood of the training data and propose solutions to increase the efficiency and accuracy of decoding.</text>
              <doc_id>254</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>An interesting aspect of our work is delivering competitive performance for difficult language pairs such as English-Chinese with a joint probability generative model and an SCFG without &#8216;gap rules&#8217;.</text>
              <doc_id>255</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>650</text>
              <doc_id>256</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Instead of employing hierarchical phrase-pairs, we invest in learning the higher-order hierarchical synchronous structure behind translation, up to the full sentence length.</text>
              <doc_id>257</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>While these choices and the related results challenge current MT research trends, they are not mutually exclusive with them.</text>
              <doc_id>258</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Future work directions include investigating the impact of hierarchical phrases for our models as well as any gains from additional features in the log-linear decoding model.</text>
              <doc_id>259</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Smoothing the HR-SCFG grammar estimates could prove a possible source of further performance improvements.</text>
              <doc_id>260</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Learning translation and reordering behaviour with respect to linguistic cues is facilitated in our approach by keeping separate phrase-pair emission distributions per emitting nonterminal and reordering pattern, while the employment of the generic X non-terminals already allows backing off to more coarse-grained rules.</text>
              <doc_id>261</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Nevertheless, we still believe that further smoothing of these sparse distributions, e.g. by interpolating them with less sparse ones, could in the future lead to an additional increase in translation quality.</text>
              <doc_id>262</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Finally, we discuss in this work how our method can already utilise hundreds of thousands of phrasepair labels and millions of structural rules.</text>
              <doc_id>263</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A further promising direction is broadening this set with labels taking advantage of both source and targetlanguage linguistic annotation or categories exploring additional phrase-pair properties past the parse trees such as semantic annotations.</text>
              <doc_id>264</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Acknowledgments</text>
              <doc_id>265</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Both authors are supported by a VIDI grant (nr.</text>
              <doc_id>266</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>639.022.604) from The Netherlands Organization for Scientific Research (NWO).</text>
              <doc_id>267</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The authors would like to thank Maxim Khalilov for helping with experimental data and Andreas Zollmann and the anonymous reviewers for their valuable comments.</text>
              <doc_id>268</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>References</text>
              <doc_id>269</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Yehoshua Bar-Hillel.</text>
              <doc_id>270</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1953.</text>
              <doc_id>271</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A quasi-arithmetical notation for syntactic description.</text>
              <doc_id>272</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Language, 29(1):47&#8211;58.</text>
              <doc_id>273</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Eugene Charniak.</text>
              <doc_id>274</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2000.</text>
              <doc_id>275</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>A maximum-entropy-inspired</text>
              <doc_id>276</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>parser.</text>
              <doc_id>277</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the North American Association for Computational Linguistics (HLT/NAACL), Seattle, Washington, USA, April.</text>
              <doc_id>278</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Stanley Chen and Joshua Goodman.</text>
              <doc_id>279</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1998.</text>
              <doc_id>280</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>An empirical study of smoothing techniques for language modeling.</text>
              <doc_id>281</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Technical Report TR-10-98, Harvard University, August.</text>
              <doc_id>282</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>David Chiang, Kevin Knight, and Wei Wang.</text>
              <doc_id>283</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>284</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>11,001 new features for statistical machine translation.</text>
              <doc_id>285</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 218&#8211;226, Boulder, Colorado, June.</text>
              <doc_id>286</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>287</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>David Chiang.</text>
              <doc_id>288</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>289</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>A hierarchical phrase-based model</text>
              <doc_id>290</doc_id>
              <sec_id>11</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>for statistical machine translation.</text>
              <doc_id>291</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL 2005, pages 263&#8211;270.</text>
              <doc_id>292</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>David Chiang.</text>
              <doc_id>293</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>294</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Learning to translate with source</text>
              <doc_id>295</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and target syntax.</text>
              <doc_id>296</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1443&#8211;1452, Uppsala, Sweden, July.</text>
              <doc_id>297</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>298</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Trevor Cohn and Phil Blunsom.</text>
              <doc_id>299</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>300</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A Bayesian model of syntax-directed tree to string grammar induction.</text>
              <doc_id>301</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 352&#8211;361, Singapore, August.</text>
              <doc_id>302</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>303</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>A.P.</text>
              <doc_id>304</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Dempster, N.M.</text>
              <doc_id>305</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Laird, and D.B.</text>
              <doc_id>306</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Rubin.</text>
              <doc_id>307</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>1977.</text>
              <doc_id>308</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Maximum likelihood from incomplete data via the em algorithm.</text>
              <doc_id>309</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>Journal of the Royal Statistical Society, Series B, 39(1):1&#8211;38.</text>
              <doc_id>310</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>John DeNero, Dan Gillick, James Zhang, and Dan Klein.</text>
              <doc_id>311</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>312</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>Why generative phrase models underperform surface heuristics.</text>
              <doc_id>313</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings on the Workshop on Statistical Machine Translation, pages 31&#8211;38, New York City.</text>
              <doc_id>314</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>315</doc_id>
              <sec_id>16</sec_id>
            </sentence>
            <sentence>
              <text>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu.</text>
              <doc_id>316</doc_id>
              <sec_id>17</sec_id>
            </sentence>
            <sentence>
              <text>2004.</text>
              <doc_id>317</doc_id>
              <sec_id>18</sec_id>
            </sentence>
            <sentence>
              <text>What&#8217;s in a translation rule?</text>
              <doc_id>318</doc_id>
              <sec_id>19</sec_id>
            </sentence>
            <sentence>
              <text>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 273&#8211;280, Boston, Massachusetts, USA, May.</text>
              <doc_id>319</doc_id>
              <sec_id>20</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>320</doc_id>
              <sec_id>21</sec_id>
            </sentence>
            <sentence>
              <text>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel</text>
              <doc_id>321</doc_id>
              <sec_id>22</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.</text>
              <doc_id>322</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>323</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Scalable inference and training of context-rich syntactic translation models.</text>
              <doc_id>324</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 961&#8211; 968, Sydney, Australia, July.</text>
              <doc_id>325</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>326</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Liang Huang and David Chiang.</text>
              <doc_id>327</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>328</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Forest rescoring:</text>
              <doc_id>329</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Faster decoding with integrated language models.</text>
              <doc_id>330</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144&#8211;151, 651</text>
              <doc_id>331</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Prague, Czech Republic, June.</text>
              <doc_id>332</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>333</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Liang Huang, Kevin Knight, and Aravind Joshi.</text>
              <doc_id>334</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>335</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Statistical syntax-directed translation with extended domain of locality.</text>
              <doc_id>336</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA), Boston, MA, USA.</text>
              <doc_id>337</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.</text>
              <doc_id>338</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2010.</text>
              <doc_id>339</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions.</text>
              <doc_id>340</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>machine translation evaluation.</text>
              <doc_id>341</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388&#8211;395, Barcelona, Spain, July.</text>
              <doc_id>342</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>343</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Philipp Koehn.</text>
              <doc_id>344</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>345</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Europarl: A Parallel Corpus for</text>
              <doc_id>346</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Statistical Machine Translation.</text>
              <doc_id>347</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In MT Summit 2005.</text>
              <doc_id>348</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev</text>
              <doc_id>349</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan.</text>
              <doc_id>350</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>351</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Joshua: An open source toolkit for parsing-based machine translation.</text>
              <doc_id>352</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 135&#8211;139, Athens, Greece, March.</text>
              <doc_id>353</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>354</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Yang Liu, Qun Liu, and Shouxun Lin.</text>
              <doc_id>355</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>356</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Tree-tostring alignment template for statistical machine translation.</text>
              <doc_id>357</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 609&#8211;616, Sydney, Australia, July.</text>
              <doc_id>358</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>359</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Yang Liu, Yajuan L&#252;, and Qun Liu.</text>
              <doc_id>360</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>361</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Improving</text>
              <doc_id>362</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tree-to-tree translation with packed forests.</text>
              <doc_id>363</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 558&#8211;566, Suntec, Singapore, August.</text>
              <doc_id>364</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>365</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Daniel Marcu and William Wong.</text>
              <doc_id>366</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>367</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>A phrase-based,</text>
              <doc_id>368</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>joint probability model for statistical machine translation.</text>
              <doc_id>369</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of Empirical methods in natural language processing, pages 133&#8211;139.</text>
              <doc_id>370</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>371</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Yuval Marton and Philip Resnik.</text>
              <doc_id>372</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>373</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Soft syntactic</text>
              <doc_id>374</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>constraints for hierarchical phrased-based translation.</text>
              <doc_id>375</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL-08: HLT, pages 1003&#8211;1011,</text>
              <doc_id>376</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Columbus, Ohio, June.</text>
              <doc_id>377</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational</text>
              <doc_id>378</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Linguistics.</text>
              <doc_id>379</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Markos Mylonakis and Khalil Sima&#8217;an.</text>
              <doc_id>380</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>381</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Phrase</text>
              <doc_id>382</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>translation probabilities with ITG priors and smoothing as learning objective.</text>
              <doc_id>383</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 630&#8211;639, Honolulu, USA, October.</text>
              <doc_id>384</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Markos Mylonakis and Khalil Sima&#8217;an.</text>
              <doc_id>385</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>386</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Learning probabilistic synchronous CFGs for phrase-based translation.</text>
              <doc_id>387</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In Fourteenth Conference on Computational Natural Language Learning, Uppsala, Sweden, July.</text>
              <doc_id>388</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Franz Josef Och.</text>
              <doc_id>389</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>390</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Minimum error rate training in statistical machine translation.</text>
              <doc_id>391</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160&#8211;167, Sapporo, Japan, July.</text>
              <doc_id>392</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>393</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Chris Quirk, Arul Menezes, and Colin Cherry.</text>
              <doc_id>394</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>395</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Dependency treelet translation: Syntactically informed phrasal smt.</text>
              <doc_id>396</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of 43rd Annual Meeting of the Association for Computational Linguistics, Ann Arbor, Michigan, USA, June.</text>
              <doc_id>397</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Ashish Venugopal, Andreas Zollmann, Noah A. Smith,</text>
              <doc_id>398</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and Stephan Vogel.</text>
              <doc_id>399</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>400</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Preference grammars: Softening syntactic constraints to improve statistical machine translation.</text>
              <doc_id>401</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 236&#8211;244, Boulder, Colorado, June.</text>
              <doc_id>402</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>403</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Wei Wang, Jonathan May, Kevin Knight, and Daniel Marcu.</text>
              <doc_id>404</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>405</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Re-structuring, re-labeling, and realigning for syntax-based machine translation.</text>
              <doc_id>406</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 36(2):247&#8211;277.</text>
              <doc_id>407</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Dekai Wu.</text>
              <doc_id>408</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>1997.</text>
              <doc_id>409</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</text>
              <doc_id>410</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 23(3):377&#8211;403.</text>
              <doc_id>411</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Kenji Yamada and Kevin Knight.</text>
              <doc_id>412</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2001.</text>
              <doc_id>413</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A syntax-based statistical translation model.</text>
              <doc_id>414</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 523&#8211;530, Toulouse, France, July.</text>
              <doc_id>415</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>416</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li.</text>
              <doc_id>417</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>418</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A tree sequence alignment-based tree-to-tree translation model.</text>
              <doc_id>419</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL-08: HLT, pages 559&#8211;567, Columbus, Ohio, June.</text>
              <doc_id>420</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>421</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Andreas Zollmann and Ashish Venugopal.</text>
              <doc_id>422</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>423</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Syntax augmented machine translation via chart parsing.</text>
              <doc_id>424</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings on the Workshop on Statistical Machine 652 Translation, pages 138&#8211;141, New York City, June.</text>
              <doc_id>425</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>426</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Experimental results for training sets of 200K and 400K sentence pairs. Statistically significant score improvements from the baseline at the 95% confidence level are labelled with a single star, at the 99% level with two.</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>josh-base</cell>
              <cell>29.58</cell>
              <cell>7.3033</cell>
              <cell>18.86</cell>
              <cell>5.8818</cell>
              <cell>22.25</cell>
              <cell>6.2949</cell>
              <cell>23.24</cell>
              <cell>6.7402</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>lts</cell>
              <cell>29.83</cell>
              <cell>7.4000**</cell>
              <cell>19.49**</cell>
              <cell>5.9374**</cell>
              <cell>22.92**</cell>
              <cell>6.3727**</cell>
              <cell>25.16**</cell>
              <cell>6.9005**</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Additional experiments for English to Chinese translation examining (a) the impact of the linguistic annotations in the LTS system (lts), when compared with an instance not employing such annotations (lts-nolabels) and (b) decoding with a 4th-order language model (-lm4). BLEU scores for 200K and 400K training sentence pairs.</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>josh-base-lm4</cell>
              <cell>23.81</cell>
              <cell>24.77</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>lts-lm4</cell>
              <cell>24.48**</cell>
              <cell>26.35**</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references/>
    <citations/>
  </content>
</document>
