<document>
  <filename>W13-2260</filename>
  <authors/>
  <title>None</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>1 Introduction</title>
        <text>In statistical machine translation (SMT), optimisation &#8212; the task of searching for an optimum translation &#8212; is performed over a high-complexity distribution defined by the intersection between a translation hypergraph and a target language model (LM). This distribution is too complex to be represented exactly and one typically resorts to approximation techniques such as beam-search (Koehn et al., 2003) and cube-pruning (Chiang, 2007), where maximisation is performed over a pruned representation of the full distribution.
Often, rather than finding a single optimum, one is really interested in obtaining a set of probabilistic samples from the distribution. This is the case for minimum error rate training (Och, 2003; Watanabe et al., 2007), minimum risk training (Smith and Eisner, 2006) and minimum risk decoding (Kumar and Byrne, 2004). Due to the additional computational challenges posed by sampling, n-best lists, a by-product of optimisation, are typically used as approximation to true probabilistic samples. A known issue with n-best lists is that they tend to be clustered around only one mode of the distribution. A more direct procedure is to attempt to directly draw samples from the underlying distribution rather than rely on n-best list approximations (Arun et al., 2009; Blunsom and Osborne, 2008).
OS &#8727; (Dymetman et al., 2012a) is a recent approach that stresses a unified view between the two types of inference, optimisation and sampling. In this view, rather than resorting to pruning in order to cope with the tractability issues, one upperbounds the complex goal distribution with a simpler &#8220;proposal&#8221; distribution for which dynamic programming is feasible. This proposal is incrementally refined to be closer to the goal until the maximum is found, or until the sampling performance exceeds a certain level.
This paper applies the OS &#8727; approach to the problem of inference in hierarchical SMT (Chiang, 2007). In a nutshell, the idea is to replace the intractable problem of intersecting a contextfree grammar with a full language model by the tractable problem of intersecting it with a simplified, optimistic version of this LM which &#8220;forgets&#8221; parts of n-gram contexts, and to incrementally add more context based on evidence of the need to do so. Evidence is gathered by optimising or sampling from the tractable proxy distribution and focussing on the most serious over-optimistic estimates relative to the goal distribution.
Our main contribution is to provide an exact optimiser/sampler for hierarchical SMT that is efficient in exploring only a small fraction of the space of n-grams involved in a full intersection. Although at this stage our experiments are limited to short sentences, they provide insights on the behavior of the technique and indicate directions towards a more efficient implementation within the same paradigm.
The paper is organized as follows: &#167;2 provides background on OS &#8727; and hierarchical translation; &#167;3 describes our approach to exact inference in SMT; in &#167;4 the experimental setup is presented and findings are discussed; &#167;5 discusses related work, and &#167;6 concludes.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In statistical machine translation (SMT), optimisation &#8212; the task of searching for an optimum translation &#8212; is performed over a high-complexity distribution defined by the intersection between a translation hypergraph and a target language model (LM).</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This distribution is too complex to be represented exactly and one typically resorts to approximation techniques such as beam-search (Koehn et al., 2003) and cube-pruning (Chiang, 2007), where maximisation is performed over a pruned representation of the full distribution.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Often, rather than finding a single optimum, one is really interested in obtaining a set of probabilistic samples from the distribution.</text>
              <doc_id>2</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This is the case for minimum error rate training (Och, 2003; Watanabe et al., 2007), minimum risk training (Smith and Eisner, 2006) and minimum risk decoding (Kumar and Byrne, 2004).</text>
              <doc_id>3</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Due to the additional computational challenges posed by sampling, n-best lists, a by-product of optimisation, are typically used as approximation to true probabilistic samples.</text>
              <doc_id>4</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>A known issue with n-best lists is that they tend to be clustered around only one mode of the distribution.</text>
              <doc_id>5</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>A more direct procedure is to attempt to directly draw samples from the underlying distribution rather than rely on n-best list approximations (Arun et al., 2009; Blunsom and Osborne, 2008).</text>
              <doc_id>6</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>OS &#8727; (Dymetman et al., 2012a) is a recent approach that stresses a unified view between the two types of inference, optimisation and sampling.</text>
              <doc_id>7</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this view, rather than resorting to pruning in order to cope with the tractability issues, one upperbounds the complex goal distribution with a simpler &#8220;proposal&#8221; distribution for which dynamic programming is feasible.</text>
              <doc_id>8</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This proposal is incrementally refined to be closer to the goal until the maximum is found, or until the sampling performance exceeds a certain level.</text>
              <doc_id>9</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>This paper applies the OS &#8727; approach to the problem of inference in hierarchical SMT (Chiang, 2007).</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In a nutshell, the idea is to replace the intractable problem of intersecting a contextfree grammar with a full language model by the tractable problem of intersecting it with a simplified, optimistic version of this LM which &#8220;forgets&#8221; parts of n-gram contexts, and to incrementally add more context based on evidence of the need to do so.</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Evidence is gathered by optimising or sampling from the tractable proxy distribution and focussing on the most serious over-optimistic estimates relative to the goal distribution.</text>
              <doc_id>12</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Our main contribution is to provide an exact optimiser/sampler for hierarchical SMT that is efficient in exploring only a small fraction of the space of n-grams involved in a full intersection.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Although at this stage our experiments are limited to short sentences, they provide insights on the behavior of the technique and indicate directions towards a more efficient implementation within the same paradigm.</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The paper is organized as follows: &#167;2 provides background on OS &#8727; and hierarchical translation; &#167;3 describes our approach to exact inference in SMT; in &#167;4 the experimental setup is presented and findings are discussed; &#167;5 discusses related work, and &#167;6 concludes.</text>
              <doc_id>15</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>2 Background</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>16</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 OS &#8727;</title>
            <text>The OS &#8727; approach (Dymetman et al., 2012a; Dymetman et al., 2012b) proposes a unified view of exact inference in sampling and optimisation, where the two modalities are seen as extremes in a continuum of inference tasks in L p spaces (Rudin, 1987), with sampling associated with the L 1 norm, and optimisation with the L &#8734; norm. The objective function p, over which inference needs to be performed, is a complex non-negative function over a discrete or continuous space X, which defines an unnormalised distribution over X. The goal is to optimise or sample relative to p &#8212; where sampling is interpreted in terms of the normalised distribution &#175;p(.) = p(.)/ &#8747; X p(x)dx.
Directly optimising or sampling from p is unfeasible; however, it is possible to define an (unnormalized) distribution q of lower complexity than p, which upper-bounds p everywhere (ie. p(x) &#8804; q(x), &#8704;x &#8712; X), and from which it is feasible to optimise or sample directly. Sampling is performed through rejection sampling: first a sample x is drawn from q, and then x is accepted or rejected with probability given by the ratio r = p(x)/q(x), which is less than 1 by construction. Accepted x&#8217;s can be shown to produce an exact sample from p (Robert and Casella, 2004). When the sample x from q is rejected, it is used as a basis for &#8220;refining&#8221; q into a slightly more complex q &#8242; , where p &#8804; q &#8242; &#8804; q is still an upper-bound to p. This &#8220;adaptive rejection sampling&#8221; technique incrementally improves the rate of acceptance, and is pursued until some rate above a given threshold is obtained, at which point one stops refining and uses the current proposal to obtain further exact samples from p.
In the case of optimisation, one finds the maximum x relative to q, and again computes the ratio r = p(x)/q(x). If this ratio equals 1, then it is easy to show that x is the actual maximum from p. 1 Otherwise we refine the proposal in a similar way to the sampling case, continuing until we find a ratio equal to 1 (or very close to 1 if we are willing to accept an approximation to the maximum). For finite spaces X, this optimisation technique is argued to be a generalisation of A &#8727; .
An application of the OS &#8727; technique to sampling/optimisation with High-Order HMM&#8217;s is described in Carter et al. (2012) and provides background for this paper. In that work, while the highorder HMM corresponds to an intractable goal distribution, it can be upper-bounded by a sequence of tractable distributions for which optimisers and samplers can be obtained through standard dynamic programming techniques.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The OS &#8727; approach (Dymetman et al., 2012a; Dymetman et al., 2012b) proposes a unified view of exact inference in sampling and optimisation, where the two modalities are seen as extremes in a continuum of inference tasks in L p spaces (Rudin, 1987), with sampling associated with the L 1 norm, and optimisation with the L &#8734; norm.</text>
                  <doc_id>17</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The objective function p, over which inference needs to be performed, is a complex non-negative function over a discrete or continuous space X, which defines an unnormalised distribution over X.</text>
                  <doc_id>18</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The goal is to optimise or sample relative to p &#8212; where sampling is interpreted in terms of the normalised distribution &#175;p(.</text>
                  <doc_id>19</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>) = p(.</text>
                  <doc_id>20</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>)/ &#8747; X p(x)dx.</text>
                  <doc_id>21</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Directly optimising or sampling from p is unfeasible; however, it is possible to define an (unnormalized) distribution q of lower complexity than p, which upper-bounds p everywhere (ie.</text>
                  <doc_id>22</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>p(x) &#8804; q(x), &#8704;x &#8712; X), and from which it is feasible to optimise or sample directly.</text>
                  <doc_id>23</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Sampling is performed through rejection sampling: first a sample x is drawn from q, and then x is accepted or rejected with probability given by the ratio r = p(x)/q(x), which is less than 1 by construction.</text>
                  <doc_id>24</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Accepted x&#8217;s can be shown to produce an exact sample from p (Robert and Casella, 2004).</text>
                  <doc_id>25</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>When the sample x from q is rejected, it is used as a basis for &#8220;refining&#8221; q into a slightly more complex q &#8242; , where p &#8804; q &#8242; &#8804; q is still an upper-bound to p.</text>
                  <doc_id>26</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This &#8220;adaptive rejection sampling&#8221; technique incrementally improves the rate of acceptance, and is pursued until some rate above a given threshold is obtained, at which point one stops refining and uses the current proposal to obtain further exact samples from p.</text>
                  <doc_id>27</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In the case of optimisation, one finds the maximum x relative to q, and again computes the ratio r = p(x)/q(x).</text>
                  <doc_id>28</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If this ratio equals 1, then it is easy to show that x is the actual maximum from p.</text>
                  <doc_id>29</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>1 Otherwise we refine the proposal in a similar way to the sampling case, continuing until we find a ratio equal to 1 (or very close to 1 if we are willing to accept an approximation to the maximum).</text>
                  <doc_id>30</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For finite spaces X, this optimisation technique is argued to be a generalisation of A &#8727; .</text>
                  <doc_id>31</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>An application of the OS &#8727; technique to sampling/optimisation with High-Order HMM&#8217;s is described in Carter et al. (2012) and provides background for this paper.</text>
                  <doc_id>32</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In that work, while the highorder HMM corresponds to an intractable goal distribution, it can be upper-bounded by a sequence of tractable distributions for which optimisers and samplers can be obtained through standard dynamic programming techniques.</text>
                  <doc_id>33</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Hierarchical Translation</title>
            <text>An abstract formulation of the decoding process for hierarchical translation models such as that of Chiang (2007) can be expressed as a sequence of three steps. In a first step, a translation model G, represented as a weighted synchronous contextfree grammar (SCFG) (Chiang, 2005), is applied to (in other words, intersected with) the source sentence f to produce a weighted context-free grammar G(f) over the target language. 2 In a second step, G(f) is intersected with a weighted finitestate automaton A representing the target language model, resulting in a weighted context-free grammar G &#8242; (f) = G(f) &#8745; A. In a final step, a dynamic programming procedure (see &#167;2.4) is applied to find the maximum derivation x in G &#8242; (f), and the sequence of leaves of yield(x) is the result translation.
While this formulation gives the general principle, already mentioned in Chiang (2007), most implementations do not exactly follow these steps or use this terminology. In practice, the closest approach to this abstract formulation is that of Dyer (2010) and the related system cdec (Dyer et al., 2010); we follow a similar approach here.
1 This is because if x &#8242; was such that p(x &#8242; ) &gt; p(x), then
q(x &#8242; ) &#8805; p(x &#8242; ) &gt; p(x) = q(x), and hence x would not be a maximum for q, a contradiction. 2 G(f) is thus a compact representation of a forest over
target sequences, and is equivalent to a hypergraph, using different terminology.
Whatever the actual implementation chosen, all approaches face a common problem: the complexity of the intersection G &#8242; (f) = G(f) &#8745; A increases rapidly with the order of the language model, and can become unwieldy for moderate-length input sentences even with a bigram model. In order to address this problem, most implementations employ variants of a technique called cube-pruning (Chiang, 2007; Huang and Chiang, 2007), where the cells constructed during the intersection process retain only a k-best list of promising candidates. This is an approximation technique, related to beam-search, which performs well in practice, but is not guaranteed to find the actual optimum.
In the approach presented here &#8212; described in detail in &#167;3 &#8212; we do not prune the search space. While we do construct the full initial grammar G(f), we proceed by incrementally intersecting it with simple automata associated with upperbounds of A, for which the intersection is tractable.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>An abstract formulation of the decoding process for hierarchical translation models such as that of Chiang (2007) can be expressed as a sequence of three steps.</text>
                  <doc_id>34</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In a first step, a translation model G, represented as a weighted synchronous contextfree grammar (SCFG) (Chiang, 2005), is applied to (in other words, intersected with) the source sentence f to produce a weighted context-free grammar G(f) over the target language.</text>
                  <doc_id>35</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>2 In a second step, G(f) is intersected with a weighted finitestate automaton A representing the target language model, resulting in a weighted context-free grammar G &#8242; (f) = G(f) &#8745; A.</text>
                  <doc_id>36</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In a final step, a dynamic programming procedure (see &#167;2.4) is applied to find the maximum derivation x in G &#8242; (f), and the sequence of leaves of yield(x) is the result translation.</text>
                  <doc_id>37</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>While this formulation gives the general principle, already mentioned in Chiang (2007), most implementations do not exactly follow these steps or use this terminology.</text>
                  <doc_id>38</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In practice, the closest approach to this abstract formulation is that of Dyer (2010) and the related system cdec (Dyer et al., 2010); we follow a similar approach here.</text>
                  <doc_id>39</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 This is because if x &#8242; was such that p(x &#8242; ) &gt; p(x), then</text>
                  <doc_id>40</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>q(x &#8242; ) &#8805; p(x &#8242; ) &gt; p(x) = q(x), and hence x would not be a maximum for q, a contradiction.</text>
                  <doc_id>41</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>2 G(f) is thus a compact representation of a forest over</text>
                  <doc_id>42</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>target sequences, and is equivalent to a hypergraph, using different terminology.</text>
                  <doc_id>43</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Whatever the actual implementation chosen, all approaches face a common problem: the complexity of the intersection G &#8242; (f) = G(f) &#8745; A increases rapidly with the order of the language model, and can become unwieldy for moderate-length input sentences even with a bigram model.</text>
                  <doc_id>44</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In order to address this problem, most implementations employ variants of a technique called cube-pruning (Chiang, 2007; Huang and Chiang, 2007), where the cells constructed during the intersection process retain only a k-best list of promising candidates.</text>
                  <doc_id>45</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This is an approximation technique, related to beam-search, which performs well in practice, but is not guaranteed to find the actual optimum.</text>
                  <doc_id>46</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In the approach presented here &#8212; described in detail in &#167;3 &#8212; we do not prune the search space.</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>While we do construct the full initial grammar G(f), we proceed by incrementally intersecting it with simple automata associated with upperbounds of A, for which the intersection is tractable.</text>
                  <doc_id>48</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>2.3 Earley Intersection</title>
            <text>In their classical paper Bar-Hillel et al. (1961) showed that the intersection of a CFG with a FSA is a CFG, and Billot and Lang (1989) were possibly the first to notice the connection of this construct with chart-parsing. In general, parsing with a CFG can be seen as a special case of intersection, with the input sequence represented as a &#8220;flat&#8221; (linear chain) automaton, and this insight allows to generalise various parsing algorithms to corresponding intersection algorithms. One such algorithm, for weighted context-free grammars and automata, inspired by the CKY parsing algorithm, is presented in Nederhof and Satta (2008). The algorithm that we are using is different; it is inspired by Earley parsing, and was introduced in chapter 2 of Dyer (2010). The advantage of Dyer&#8217;s &#8220;Earley Intersection&#8221; algorithm is that it combines top-down predictions with bottom-up completions. The algorithm thus avoids constructing many non-terminals that may be justified from the bottom-up perspective, but can never be &#8220;requested&#8221; by a top-down derivation, and would need to be pruned in a second pass. Our early experiments showed an important gain in intermediary storage and in overall time by using this Earley-based technique as opposed to a CKY-based technique.
We do not describe the Earley Intersection algorithm in detail here, but refer to Dyer (2010), which we follow closely.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In their classical paper Bar-Hillel et al. (1961) showed that the intersection of a CFG with a FSA is a CFG, and Billot and Lang (1989) were possibly the first to notice the connection of this construct with chart-parsing.</text>
                  <doc_id>49</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In general, parsing with a CFG can be seen as a special case of intersection, with the input sequence represented as a &#8220;flat&#8221; (linear chain) automaton, and this insight allows to generalise various parsing algorithms to corresponding intersection algorithms.</text>
                  <doc_id>50</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>One such algorithm, for weighted context-free grammars and automata, inspired by the CKY parsing algorithm, is presented in Nederhof and Satta (2008).</text>
                  <doc_id>51</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The algorithm that we are using is different; it is inspired by Earley parsing, and was introduced in chapter 2 of Dyer (2010).</text>
                  <doc_id>52</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The advantage of Dyer&#8217;s &#8220;Earley Intersection&#8221; algorithm is that it combines top-down predictions with bottom-up completions.</text>
                  <doc_id>53</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The algorithm thus avoids constructing many non-terminals that may be justified from the bottom-up perspective, but can never be &#8220;requested&#8221; by a top-down derivation, and would need to be pruned in a second pass.</text>
                  <doc_id>54</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Our early experiments showed an important gain in intermediary storage and in overall time by using this Earley-based technique as opposed to a CKY-based technique.</text>
                  <doc_id>55</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We do not describe the Earley Intersection algorithm in detail here, but refer to Dyer (2010), which we follow closely.</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>2.4 Optimisation and Sampling from a WCFG</title>
            <text>Optimisation in a weighted CFG (WCFG) 3 , that is, finding the maximum derivation, is well studied and involves a dynamic programming procedure that assigns in turn to each nonterminal, according to a bottom-up traversal regime, a maximum derivation along with its weight, up to the point where a maximum derivation is found for the initial nonterminal in the grammar. This can be seen as working in the max-times semiring, where the weight of a derivation is obtained through the product of the weights of its sub-derivations, and where the weight associated with a nonterminal is obtained by maximising over the different derivations rooted in that nonterminal.
The case of sampling can be handled in a very similar way, by working in the sum-times instead of the max-times semiring. Here, instead of maximising over the weights of the competing derivations rooted in the same nonterminal, one sums over these weights. By proceeding in the same bottom-up way, one ends with an accumulation of all the weights on the initial nonterminal (this can also be seen as the partition function associated with the grammar). An efficient exact sampler is then obtained by starting at the root nonterminal, randomly selecting an expansion proportionally to the weight of this expansion, and iterating in a topdown way. This process is described in more detail in section 4 of Johnson et al. (2007), for instance.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Optimisation in a weighted CFG (WCFG) 3 , that is, finding the maximum derivation, is well studied and involves a dynamic programming procedure that assigns in turn to each nonterminal, according to a bottom-up traversal regime, a maximum derivation along with its weight, up to the point where a maximum derivation is found for the initial nonterminal in the grammar.</text>
                  <doc_id>57</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This can be seen as working in the max-times semiring, where the weight of a derivation is obtained through the product of the weights of its sub-derivations, and where the weight associated with a nonterminal is obtained by maximising over the different derivations rooted in that nonterminal.</text>
                  <doc_id>58</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The case of sampling can be handled in a very similar way, by working in the sum-times instead of the max-times semiring.</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Here, instead of maximising over the weights of the competing derivations rooted in the same nonterminal, one sums over these weights.</text>
                  <doc_id>60</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>By proceeding in the same bottom-up way, one ends with an accumulation of all the weights on the initial nonterminal (this can also be seen as the partition function associated with the grammar).</text>
                  <doc_id>61</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>An efficient exact sampler is then obtained by starting at the root nonterminal, randomly selecting an expansion proportionally to the weight of this expansion, and iterating in a topdown way.</text>
                  <doc_id>62</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This process is described in more detail in section 4 of Johnson et al. (2007), for instance.</text>
                  <doc_id>63</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>2</index>
        <title>3 Approach</title>
        <text>The complexity of building the full intersection G(f) &#8745; A, when A represents a language model of order n, is related to the fact that the number of states of A grows exponentially with n, and that each nonterminal N in G(f) tends to generate in the grammar G &#8242; (f) many indexed nonterminals of the form (i, N, j), where i, j are states of A and the nonterminal (i, N, j) can be interpreted as an N connecting an i state to a j state.
In our approach, instead of explicitly constructing the full intersection G(f) &#8745; A, which, using the notation of &#167;2.1, is identified with the unnormalised goal distribution p(x), we incrementally produce a sequence of &#8220;proposal&#8221; grammars q (t) , which all upper-bound p, where q (0) = G(f) &#8745; A (0) , ..., q (t+1) = q (t) &#8745; A (t) , etc. Here A (0) is
3 Here the CFG is assumed to be acyclic, which is typically
the case in translation applications.
an optimistic, low complexity, &#8220;unigram&#8221; version of the automaton A, and each increment A (t) is a small automaton that refines q (t) relative to some specific k-gram context (i.e., sequence of k words) not yet made explicit in the previous increments, where k takes some value between 1 and n. This process produces a sequence of grammars q (t) such that q (0) (.) &#8805; q (1) (.) &#8805; q (2) (.) &#8805; ... &#8805; p(.).
In the limit &#8898; M
t=0 A(t) = A for some large M, so
that we are in principle able to reconstruct the full intersection p(.) = q (M) = G(f)&#8745;A (0) &#8745;...&#8745;A (M) in finite time. In practice our actual process stops much earlier: in optimisation, when the value of the maximum derivation x &#8727; t relative to q (t) becomes equal to its value according to the full language model, in sampling when the acceptance rate of samples from q (t) exceeds a certain threshold. The process is detailed in what follows.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The complexity of building the full intersection G(f) &#8745; A, when A represents a language model of order n, is related to the fact that the number of states of A grows exponentially with n, and that each nonterminal N in G(f) tends to generate in the grammar G &#8242; (f) many indexed nonterminals of the form (i, N, j), where i, j are states of A and the nonterminal (i, N, j) can be interpreted as an N connecting an i state to a j state.</text>
              <doc_id>64</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In our approach, instead of explicitly constructing the full intersection G(f) &#8745; A, which, using the notation of &#167;2.1, is identified with the unnormalised goal distribution p(x), we incrementally produce a sequence of &#8220;proposal&#8221; grammars q (t) , which all upper-bound p, where q (0) = G(f) &#8745; A (0) , ..., q (t+1) = q (t) &#8745; A (t) , etc.</text>
              <doc_id>65</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Here A (0) is</text>
              <doc_id>66</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3 Here the CFG is assumed to be acyclic, which is typically</text>
              <doc_id>67</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the case in translation applications.</text>
              <doc_id>68</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>an optimistic, low complexity, &#8220;unigram&#8221; version of the automaton A, and each increment A (t) is a small automaton that refines q (t) relative to some specific k-gram context (i.e., sequence of k words) not yet made explicit in the previous increments, where k takes some value between 1 and n.</text>
              <doc_id>69</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This process produces a sequence of grammars q (t) such that q (0) (.</text>
              <doc_id>70</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>) &#8805; q (1) (.</text>
              <doc_id>71</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>) &#8805; q (2) (.</text>
              <doc_id>72</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>) &#8805; ... &#8805; p(.</text>
              <doc_id>73</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>).</text>
              <doc_id>74</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the limit &#8898; M</text>
              <doc_id>75</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>t=0 A(t) = A for some large M, so</text>
              <doc_id>76</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>that we are in principle able to reconstruct the full intersection p(.</text>
              <doc_id>77</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>) = q (M) = G(f)&#8745;A (0) &#8745;...&#8745;A (M) in finite time.</text>
              <doc_id>78</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In practice our actual process stops much earlier: in optimisation, when the value of the maximum derivation x &#8727; t relative to q (t) becomes equal to its value according to the full language model, in sampling when the acceptance rate of samples from q (t) exceeds a certain threshold.</text>
              <doc_id>79</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The process is detailed in what follows.</text>
              <doc_id>80</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 OS &#8727; for Hierarchical Translation</title>
            <text>Our application of OS &#8727; to hierarchical translation is illustrated in Algorithm 1, with the two modes, optimisation and sampling, made explicit and shown side-by-side to stress the parallelism.
On line 1, we initialise the time step to 0, and for sampling we also initialise the current acceptance rate (AR) to 0. On line 2, we initialise the initial proposal grammar q (0) , where A (0) is detailed in &#167;3.2. On line 3, we start a loop: in optimisation we stop when we have found an x that is accepted, meaning that the maximum has been found; in sampling, we stop when the estimated acceptance rate (AR) of the current proposal q (t) exceeds a certain threshold (e.g. 20%) &#8212; this AR can be roughly estimated by observing how many of the last (say) one hundred samples from the proposal have been accepted, and tends to reflect the actual acceptance rate obtained by using q (t) without further refinements. On line 4, in optimisation, we compute the argmax x from the proposal, and in sampling we draw a sample x from the proposal. 4 On line 5, we compute the ratio r = p(x)/q (t) (x); by construction q (t) is an optimistic version of p, thus r &#8804; 1.
On line 6, in optimisation we accept x if the ratio is equal to 1, in which case we have found the maximum, and in sampling we accept x with probability r, which is a form of adaptive rejection sampling and guarantees that accepted sam-
4 Following the OS &#8727; approach, taking an argmax is actually
assimilated to an extreme form of sampling, with an L &#8734; space taking the place of an L 1 space.
ples form exact samples from p; see (Dymetman et al., 2012a).
If x was rejected (line 7), we then (lines 8, 9) refine q (t) into a q (t+1) such that p(.) &#8804; q (t+1) (.) &#8804; q (t) (.) everywhere. This is done by defining the incremental automaton A (t+1) on the basis of x and q (t) , as will be detailed below, and by intersecting this automaton with q (t)
Finally, on line 11, in optimisation we return the x which has been accepted, namely the maximum of p, and in sampling we return the list of already accepted x&#8217;s, which form an exact sample from p, along with the current q (t) , which can be used as a sampler to produce further exact samples with an acceptance rate performance above the predefined threshold.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Our application of OS &#8727; to hierarchical translation is illustrated in Algorithm 1, with the two modes, optimisation and sampling, made explicit and shown side-by-side to stress the parallelism.</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>On line 1, we initialise the time step to 0, and for sampling we also initialise the current acceptance rate (AR) to 0.</text>
                  <doc_id>82</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>On line 2, we initialise the initial proposal grammar q (0) , where A (0) is detailed in &#167;3.2.</text>
                  <doc_id>83</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>On line 3, we start a loop: in optimisation we stop when we have found an x that is accepted, meaning that the maximum has been found; in sampling, we stop when the estimated acceptance rate (AR) of the current proposal q (t) exceeds a certain threshold (e.g. 20%) &#8212; this AR can be roughly estimated by observing how many of the last (say) one hundred samples from the proposal have been accepted, and tends to reflect the actual acceptance rate obtained by using q (t) without further refinements.</text>
                  <doc_id>84</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>On line 4, in optimisation, we compute the argmax x from the proposal, and in sampling we draw a sample x from the proposal.</text>
                  <doc_id>85</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>4 On line 5, we compute the ratio r = p(x)/q (t) (x); by construction q (t) is an optimistic version of p, thus r &#8804; 1.</text>
                  <doc_id>86</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>On line 6, in optimisation we accept x if the ratio is equal to 1, in which case we have found the maximum, and in sampling we accept x with probability r, which is a form of adaptive rejection sampling and guarantees that accepted sam-</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4 Following the OS &#8727; approach, taking an argmax is actually</text>
                  <doc_id>88</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>assimilated to an extreme form of sampling, with an L &#8734; space taking the place of an L 1 space.</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>ples form exact samples from p; see (Dymetman et al., 2012a).</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>If x was rejected (line 7), we then (lines 8, 9) refine q (t) into a q (t+1) such that p(.</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>) &#8804; q (t+1) (.</text>
                  <doc_id>92</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>) &#8804; q (t) (.</text>
                  <doc_id>93</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>) everywhere.</text>
                  <doc_id>94</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This is done by defining the incremental automaton A (t+1) on the basis of x and q (t) , as will be detailed below, and by intersecting this automaton with q (t)</text>
                  <doc_id>95</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Finally, on line 11, in optimisation we return the x which has been accepted, namely the maximum of p, and in sampling we return the list of already accepted x&#8217;s, which form an exact sample from p, along with the current q (t) , which can be used as a sampler to produce further exact samples with an acceptance rate performance above the predefined threshold.</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Incremental refinements</title>
            <text>Initial automatonA (0) This deterministic automaton is an &#8220;optimistic&#8221; version of A which only records unigram information. A (0) has only one state q 0 , which is both initial and final. For each word a of the target language it has a transition (q 0 , a, q 0 ) whose weight is denoted by w 1 (a). This weight is called the &#8220;max-backoff unigram weight&#8221; (Carter et al., 2012) and it is defined as:
w 1 (a) &#8801; max p lm(a|h),
h
where p lm (a|h) is the conditional language model probability of a relative to the history h, and where the maximum is taken over all possible histories, that is, over all possible sequence of target words that might precede a.
Max-backoffs Following Carter et al. (2012), for any language model of finite order, the unigram max-backoff weights w 1 (a) can be precomputed in a &#8220;Max-ARPA&#8221; table, an extension of the ARPA format (Jurafsky and Martin, 2000) for the target language model, which can be precomputed on the basis of the standard ARPA table.
From the Max-ARPA table one can also directly compute the following &#8220;max-backoff weights&#8221;: w 2 (a|a &#8722;1 ), w 3 (a|a &#8722;2 a &#8722;1 ), ..., which are defined by:
w 2 (a|a &#8722;1 ) &#8801; max
h p lm(a|h, a &#8722;1 )
w 3 (a|a &#8722;2 a &#8722;1 ) &#8801; max
h p lm(a|h, a &#8722;2 a &#8722;1 )
...
where the maximum is taken over the part of the history which is not explicitely indicated.
Note that: (i) if the underlying language model is, say, a trigram model, then w 3 (a|a &#8722;2 a &#8722;1 ) is simply p lm (a|a &#8722;2 a &#8722;1 ), and similarly for an underlying model of order k in general, and (ii) w 2 (a|a &#8722;1 ) = max a&#8722;2 w 3 (a|a &#8722;2 a &#8722;1 ) and w 1 (a) = max a&#8722;1 w 2 (a|a &#8722;1 ).
Incremental automata A (t) The weight assigned to any target sentence by A (0) is larger or equal to its weight according to A. Therefore, the initial grammar q (0) = G(f) &#8745; A (0) is optimistic relative to the actual grammar p = G(f) &#8745; A: for any derivation x in p, we have p(x) &#8804; q (0) (x). We can then apply the OS &#8727; technique with q (0) . In the case of optimisation, this means that we find the maximum derivation x from q (0) . By construction, with y = yield(x), we have A (0) (y) &#8805; A(y). If the two values are equal, we have found the maximum, 5 otherwise there must be a word y i in the sequence y1 m = y for which p lm (y i |y1 i&#8722;1 ) is strictly smaller than w 1 (y i ). Let us take among such words the one for which the ratio &#945; = w 2 (y i |y i&#8722;1 )/w 1 (y i ) &#8804; 1 is the smallest, and for convenience let us rename b = y i&#8722;1 , a = y i . We then define the (deterministic) automaton A (1) as illustrated in the following figure:
else:1
b:1 0 1 b:1 else:1
Here the state 0 is both initial and final, and the state 1 is final; all edges carry a (multiplicative) weight equal to 1, except edge (1, a, 0), which carries the weight &#945;. We use the abbreviation &#8220;else&#8221; to refer to any label other than b when starting from 0, and other than b or a when starting from 1.
5 This case is very unlikely with A (0) , but helps introduce
the general case.
a:&#945;
It is easy to check that this automaton assigns to any word sequence y a weight equal to &#945; k , where k is the number of occurrences of b a in y. In particular, if y is such that y i&#8722;1 = b, y i = a, then the transition in (the deterministic automaton) A (0) &#8745; A (1) that consumes y i carries the weight &#945; w 1 (a), in other words, the weight w 2 (a|b). Thus the new proposal grammar q (1) = q (0) &#8745; A (1) has now &#8220;incorporated&#8221; knowledge of the bigram a-in-thecontext-b, at the cost of some increase in its complexity. 6
The general procedure for choosing A (t+1) follows the same pattern. We find the max derivation x in q (t) along with its yield y; if p(x) = q (t) (x), we stop and output x; otherwise we find some subsequence y i&#8722;m&#8722;1 , y i&#8722;m , ..., y i such that the knowledge of the n-gram y i&#8722;m , ..., y i has already been registered in q (t) , but not that of the n-gram y i&#8722;m&#8722;1 , y i&#8722;m , ..., y i , and we define an automaton A (t+1) which assign to a sequence a weight &#945; k , where
&#945; = w m+1(y i |y i&#8722;m&#8722;1 , y i&#8722;m , ..., y i&#8722;1 ) , w m (y i |y i&#8722;m , ..., y i&#8722;1 )
and where k is the number of occurrences of y i&#8722;m&#8722;1 , y i&#8722;m , ..., y i in the sequence. 7
We note that we have p &#8804; q (t+1) &#8804; q (t) everywhere, and also that the number of possible refinement operations is bounded, because at some point we would have expanded all contexts to their maximum order, at which point we would have reproduced p(.) on the whole space X of possible
6 Note that without further increasing q (1) &#8217;s complexity one
can incorporate knowledge about all bigrams sharing the prefix b. This is because A (1) does not need additional states to account for different continuations of the context b, all we need is to update the weights of the transitions leaving state 1 appropriately. More generally, it is not more costly to account for all n-grams prefixed by the same context of n &#8722; 1 words than it is to account for only one of them. 7 Building A (t+1) is a variant of the standard construction
for a &#8220;substring-searching&#8221; automaton (Cormen et al., 2001) and produces an automaton with n states (the order of the n- gram). This construction is omitted for the sake of space.
derivations exactly. However, we typically stop much earlier than that, without expanding contexts in the regions of X which are not promising even on optimistic assessments based on limited contexts.
Following the OS &#8727; methodology, the situation with sampling is completely parallel to that of optimisation, the only difference being that, instead of finding the maximum derivation x from q (t) (.), we draw a sample x from the distribution associated with q (t) (.), then accept it with probability given by the ratio r = p(x)/q (t) (x) &#8804; 1. In the case of a reject, we identify a subsequence y i&#8722;m&#8722;1 , y i&#8722;m , ..., y i in yield(x) as in the optimisation case, and similarly refine q (t) into q (t+1) = q (t) &#8745; A (t+1) . The acceptance rate gradually increases because q (t) comes closer and closer to p. We stop the process at a point where the current acceptance rate, estimated on the basis of, say, the last one hundred trials, exceeds a predefined threshold, perhaps 20%.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Initial automatonA (0) This deterministic automaton is an &#8220;optimistic&#8221; version of A which only records unigram information.</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A (0) has only one state q 0 , which is both initial and final.</text>
                  <doc_id>98</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For each word a of the target language it has a transition (q 0 , a, q 0 ) whose weight is denoted by w 1 (a).</text>
                  <doc_id>99</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This weight is called the &#8220;max-backoff unigram weight&#8221; (Carter et al., 2012) and it is defined as:</text>
                  <doc_id>100</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w 1 (a) &#8801; max p lm(a|h),</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>h</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where p lm (a|h) is the conditional language model probability of a relative to the history h, and where the maximum is taken over all possible histories, that is, over all possible sequence of target words that might precede a.</text>
                  <doc_id>103</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Max-backoffs Following Carter et al. (2012), for any language model of finite order, the unigram max-backoff weights w 1 (a) can be precomputed in a &#8220;Max-ARPA&#8221; table, an extension of the ARPA format (Jurafsky and Martin, 2000) for the target language model, which can be precomputed on the basis of the standard ARPA table.</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>From the Max-ARPA table one can also directly compute the following &#8220;max-backoff weights&#8221;: w 2 (a|a &#8722;1 ), w 3 (a|a &#8722;2 a &#8722;1 ), ..., which are defined by:</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w 2 (a|a &#8722;1 ) &#8801; max</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>h p lm(a|h, a &#8722;1 )</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w 3 (a|a &#8722;2 a &#8722;1 ) &#8801; max</text>
                  <doc_id>108</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>h p lm(a|h, a &#8722;2 a &#8722;1 )</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>...</text>
                  <doc_id>110</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where the maximum is taken over the part of the history which is not explicitely indicated.</text>
                  <doc_id>111</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Note that: (i) if the underlying language model is, say, a trigram model, then w 3 (a|a &#8722;2 a &#8722;1 ) is simply p lm (a|a &#8722;2 a &#8722;1 ), and similarly for an underlying model of order k in general, and (ii) w 2 (a|a &#8722;1 ) = max a&#8722;2 w 3 (a|a &#8722;2 a &#8722;1 ) and w 1 (a) = max a&#8722;1 w 2 (a|a &#8722;1 ).</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Incremental automata A (t) The weight assigned to any target sentence by A (0) is larger or equal to its weight according to A.</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, the initial grammar q (0) = G(f) &#8745; A (0) is optimistic relative to the actual grammar p = G(f) &#8745; A: for any derivation x in p, we have p(x) &#8804; q (0) (x).</text>
                  <doc_id>114</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We can then apply the OS &#8727; technique with q (0) .</text>
                  <doc_id>115</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In the case of optimisation, this means that we find the maximum derivation x from q (0) .</text>
                  <doc_id>116</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>By construction, with y = yield(x), we have A (0) (y) &#8805; A(y).</text>
                  <doc_id>117</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>If the two values are equal, we have found the maximum, 5 otherwise there must be a word y i in the sequence y1 m = y for which p lm (y i |y1 i&#8722;1 ) is strictly smaller than w 1 (y i ).</text>
                  <doc_id>118</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Let us take among such words the one for which the ratio &#945; = w 2 (y i |y i&#8722;1 )/w 1 (y i ) &#8804; 1 is the smallest, and for convenience let us rename b = y i&#8722;1 , a = y i .</text>
                  <doc_id>119</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>We then define the (deterministic) automaton A (1) as illustrated in the following figure:</text>
                  <doc_id>120</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>else:1</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>b:1 0 1 b:1 else:1</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Here the state 0 is both initial and final, and the state 1 is final; all edges carry a (multiplicative) weight equal to 1, except edge (1, a, 0), which carries the weight &#945;.</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We use the abbreviation &#8220;else&#8221; to refer to any label other than b when starting from 0, and other than b or a when starting from 1.</text>
                  <doc_id>124</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5 This case is very unlikely with A (0) , but helps introduce</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the general case.</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>a:&#945;</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It is easy to check that this automaton assigns to any word sequence y a weight equal to &#945; k , where k is the number of occurrences of b a in y.</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In particular, if y is such that y i&#8722;1 = b, y i = a, then the transition in (the deterministic automaton) A (0) &#8745; A (1) that consumes y i carries the weight &#945; w 1 (a), in other words, the weight w 2 (a|b).</text>
                  <doc_id>129</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Thus the new proposal grammar q (1) = q (0) &#8745; A (1) has now &#8220;incorporated&#8221; knowledge of the bigram a-in-thecontext-b, at the cost of some increase in its complexity.</text>
                  <doc_id>130</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>6</text>
                  <doc_id>131</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The general procedure for choosing A (t+1) follows the same pattern.</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We find the max derivation x in q (t) along with its yield y; if p(x) = q (t) (x), we stop and output x; otherwise we find some subsequence y i&#8722;m&#8722;1 , y i&#8722;m , ..., y i such that the knowledge of the n-gram y i&#8722;m , ..., y i has already been registered in q (t) , but not that of the n-gram y i&#8722;m&#8722;1 , y i&#8722;m , ..., y i , and we define an automaton A (t+1) which assign to a sequence a weight &#945; k , where</text>
                  <doc_id>133</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#945; = w m+1(y i |y i&#8722;m&#8722;1 , y i&#8722;m , ..., y i&#8722;1 ) , w m (y i |y i&#8722;m , ..., y i&#8722;1 )</text>
                  <doc_id>134</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and where k is the number of occurrences of y i&#8722;m&#8722;1 , y i&#8722;m , ..., y i in the sequence.</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>7</text>
                  <doc_id>136</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We note that we have p &#8804; q (t+1) &#8804; q (t) everywhere, and also that the number of possible refinement operations is bounded, because at some point we would have expanded all contexts to their maximum order, at which point we would have reproduced p(.</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>) on the whole space X of possible</text>
                  <doc_id>138</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>6 Note that without further increasing q (1) &#8217;s complexity one</text>
                  <doc_id>139</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>can incorporate knowledge about all bigrams sharing the prefix b.</text>
                  <doc_id>140</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This is because A (1) does not need additional states to account for different continuations of the context b, all we need is to update the weights of the transitions leaving state 1 appropriately.</text>
                  <doc_id>141</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>More generally, it is not more costly to account for all n-grams prefixed by the same context of n &#8722; 1 words than it is to account for only one of them.</text>
                  <doc_id>142</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>7 Building A (t+1) is a variant of the standard construction</text>
                  <doc_id>143</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>for a &#8220;substring-searching&#8221; automaton (Cormen et al., 2001) and produces an automaton with n states (the order of the n- gram).</text>
                  <doc_id>144</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This construction is omitted for the sake of space.</text>
                  <doc_id>145</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>derivations exactly.</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, we typically stop much earlier than that, without expanding contexts in the regions of X which are not promising even on optimistic assessments based on limited contexts.</text>
                  <doc_id>147</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Following the OS &#8727; methodology, the situation with sampling is completely parallel to that of optimisation, the only difference being that, instead of finding the maximum derivation x from q (t) (.</text>
                  <doc_id>148</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>), we draw a sample x from the distribution associated with q (t) (.</text>
                  <doc_id>149</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>), then accept it with probability given by the ratio r = p(x)/q (t) (x) &#8804; 1.</text>
                  <doc_id>150</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In the case of a reject, we identify a subsequence y i&#8722;m&#8722;1 , y i&#8722;m , ..., y i in yield(x) as in the optimisation case, and similarly refine q (t) into q (t+1) = q (t) &#8745; A (t+1) .</text>
                  <doc_id>151</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The acceptance rate gradually increases because q (t) comes closer and closer to p.</text>
                  <doc_id>152</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We stop the process at a point where the current acceptance rate, estimated on the basis of, say, the last one hundred trials, exceeds a predefined threshold, perhaps 20%.</text>
                  <doc_id>153</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Illustration</title>
            <text>In this section, we present a small running example of our approach. Consider the lowercased German source sentence: eine letzte beobachtung .
Table 1 shows the translation associated with the optimum derivation from each proposal q (i) . The n-gram whose cost, if extended by one word to the left, would be increased by the largest factor is underlined. The extended context selected for refinement is highlighted in bold.
Consider the very first iteration (i = 0), at which point only unigram costs have been incorporated. The sequence &lt;s&gt; one last observation . &lt;/s&gt; represents the translation associated to the best derivation x in q (0) . We proceed by choosing from it one sequence to be the base for a refinement that will lower q (0) bringing it closer to p. Amongst all possible one-word (to the left) extensions, extending the unigram &#8216;one&#8217; to the bigram &#8216;&lt;s&gt; one&#8217; is the operation that lowers q (0) (x) the most. It might be helpful to understand it as the bigram &#8216;&lt;s&gt; one&#8217; being associated to the largest LM gap observed in x. Therefore the context &#8216;&lt;s&gt;&#8217; is selected for refinement, which means that an automaton A (1) is designed to down-weight derivations compatible with bigrams prefixed by &#8216;&lt;s&gt;&#8217;. The proposal q (0) is intersected with A (1) producing q (1) . We proceed like this iteratively, always selecting a context not yet accounted for until q (i) (x) = p(x) for the best derivation (13 th iteration in our example), when the true optimum is found with a certificate of optimality.
Score (Q, P, B) ; Delta (C, M) ; #states (R)
&#8722;2 &#8722;1 0 1 2 3
Q P B C M R
MC R
Q P Best Current gap Minimum gap Refinement
R R R R
C M
MC R
R
C MC M C M C M C M C M M C M C M C M C M C
Q Q Q Q
Q Q Q Q PB PB PB PB Q Q Q Q Q PB B PB PB PB PB QP B PB B PB P P
0 2 4 6 8 10 12
R
Iteration (i)
Figure 1 displays the progression of Q (score of the best derivation) and P (that derivation&#8217;s true score). As guaranteed by construction, Q is always above P . B represents the score of the best derivation so far according to the true scoring function, that is, B is a lower-bound on the true optimum 8 . The optimal solution is achieved when P = Q.
Curve B in Figure 1 shows that the best scoring solution was found quite early in the search (i = 3). However, optimality could only be proven 10 iterations later. Another way of stating the convergence criterion Q = P is observing a zero gap (in the log domain) between Q and P (see curve C &#8211; current gap), or a zero gap between Q and B (see curve M &#8211; minimum gap). Observe how M drops quickly from 1 to nearly 0, followed by a long tail where M
8 This observation allows for error-safe pruning in optimisation: if x is a lower-bound on the true optimum, derivations in q (i) that score lower than p(x) could be safely removed. We have left that possibility for future work.
R
R
R
R
R
R
decreases much slower. Note that if we were willing to accept an approximate solution, we could already stop the search if B remained unchanged for a predetermined number of iterations or if changes in B were smaller than some threshold, at the cost of giving up on the optimality certificate.
Finally, curve R shows the number of states in the automaton A (i) that refines the proposal at iteration i. Note how lower order n-grams (2-grams in fact) are responsible for the largest drop in the first iterations and higher-order n-grams (in fact 3- grams) are refined later in the long tail.
Figure 2 illustrates the progression of the sampler for the same German sentence. At each iteration a batch of 500 samples is drawn from q (i) . The rejected samples in the batch are used to collect statistics about overoptimistic n-grams and to heuristically choose one context to be refined for the next iteration, similar to the optimisation mode. We start with a low acceptance rate which grows up to 30% after 15 different contexts were incorporated. Note how the L 1 norm of q (its partition function) decreases after each refinement, that is, q is gradually brought closer to p, resulting in the increased number of exact samples and better acceptance rate.
Note that, starting from iteration one, all refinements here correspond to 2-grams (i.e. one-word contexts). This can be explained by the fact that, in sampling, lower-order refinements are those that mostly increase acceptance rate (rationale: highorder n-grams are compatible with fewer grammar rules).
9 10 0 1000 0.1 0.2 0.3 1.0 1.5 2.0
&#9679;
&#9679; &#9679; &#9679;
&#9679;
&#9679;
&#9679;
&#9679;
&#9679;
&#9679;
&#9679;
&#9679;
&#9679;
&#9679;
&#9679;
&#9679;
&#9679;
&#9679;
&#9679;
&#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679;
exact
accrate refinement
&#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679;
0 5 10
&#9679;
&#9679;
L1
&#9679;
&#9679;
Iteration (i)
&#9679; &#9679; &#9679; &#9679; &#9679;
&#9679; &#9679; &#9679; &#9679; &#9679;
&#9679;
&#9679;</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In this section, we present a small running example of our approach.</text>
                  <doc_id>154</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Consider the lowercased German source sentence: eine letzte beobachtung .</text>
                  <doc_id>155</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 1 shows the translation associated with the optimum derivation from each proposal q (i) .</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The n-gram whose cost, if extended by one word to the left, would be increased by the largest factor is underlined.</text>
                  <doc_id>157</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The extended context selected for refinement is highlighted in bold.</text>
                  <doc_id>158</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Consider the very first iteration (i = 0), at which point only unigram costs have been incorporated.</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The sequence &lt;s&gt; one last observation .</text>
                  <doc_id>160</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>&lt;/s&gt; represents the translation associated to the best derivation x in q (0) .</text>
                  <doc_id>161</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We proceed by choosing from it one sequence to be the base for a refinement that will lower q (0) bringing it closer to p. Amongst all possible one-word (to the left) extensions, extending the unigram &#8216;one&#8217; to the bigram &#8216;&lt;s&gt; one&#8217; is the operation that lowers q (0) (x) the most.</text>
                  <doc_id>162</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>It might be helpful to understand it as the bigram &#8216;&lt;s&gt; one&#8217; being associated to the largest LM gap observed in x.</text>
                  <doc_id>163</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore the context &#8216;&lt;s&gt;&#8217; is selected for refinement, which means that an automaton A (1) is designed to down-weight derivations compatible with bigrams prefixed by &#8216;&lt;s&gt;&#8217;.</text>
                  <doc_id>164</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The proposal q (0) is intersected with A (1) producing q (1) .</text>
                  <doc_id>165</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>We proceed like this iteratively, always selecting a context not yet accounted for until q (i) (x) = p(x) for the best derivation (13 th iteration in our example), when the true optimum is found with a certificate of optimality.</text>
                  <doc_id>166</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Score (Q, P, B) ; Delta (C, M) ; #states (R)</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8722;2 &#8722;1 0 1 2 3</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Q P B C M R</text>
                  <doc_id>169</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>MC R</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Q P Best Current gap Minimum gap Refinement</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R R R R</text>
                  <doc_id>172</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>C M</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>MC R</text>
                  <doc_id>174</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>C MC M C M C M C M C M M C M C M C M C M C</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Q Q Q Q</text>
                  <doc_id>177</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Q Q Q Q PB PB PB PB Q Q Q Q Q PB B PB PB PB PB QP B PB B PB P P</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0 2 4 6 8 10 12</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Iteration (i)</text>
                  <doc_id>181</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 1 displays the progression of Q (score of the best derivation) and P (that derivation&#8217;s true score).</text>
                  <doc_id>182</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As guaranteed by construction, Q is always above P .</text>
                  <doc_id>183</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>B represents the score of the best derivation so far according to the true scoring function, that is, B is a lower-bound on the true optimum 8 .</text>
                  <doc_id>184</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The optimal solution is achieved when P = Q.</text>
                  <doc_id>185</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Curve B in Figure 1 shows that the best scoring solution was found quite early in the search (i = 3).</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, optimality could only be proven 10 iterations later.</text>
                  <doc_id>187</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Another way of stating the convergence criterion Q = P is observing a zero gap (in the log domain) between Q and P (see curve C &#8211; current gap), or a zero gap between Q and B (see curve M &#8211; minimum gap).</text>
                  <doc_id>188</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Observe how M drops quickly from 1 to nearly 0, followed by a long tail where M</text>
                  <doc_id>189</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>8 This observation allows for error-safe pruning in optimisation: if x is a lower-bound on the true optimum, derivations in q (i) that score lower than p(x) could be safely removed.</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We have left that possibility for future work.</text>
                  <doc_id>191</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R</text>
                  <doc_id>192</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R</text>
                  <doc_id>193</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R</text>
                  <doc_id>194</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R</text>
                  <doc_id>196</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>decreases much slower.</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Note that if we were willing to accept an approximate solution, we could already stop the search if B remained unchanged for a predetermined number of iterations or if changes in B were smaller than some threshold, at the cost of giving up on the optimality certificate.</text>
                  <doc_id>199</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Finally, curve R shows the number of states in the automaton A (i) that refines the proposal at iteration i.</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Note how lower order n-grams (2-grams in fact) are responsible for the largest drop in the first iterations and higher-order n-grams (in fact 3- grams) are refined later in the long tail.</text>
                  <doc_id>201</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 2 illustrates the progression of the sampler for the same German sentence.</text>
                  <doc_id>202</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>At each iteration a batch of 500 samples is drawn from q (i) .</text>
                  <doc_id>203</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The rejected samples in the batch are used to collect statistics about overoptimistic n-grams and to heuristically choose one context to be refined for the next iteration, similar to the optimisation mode.</text>
                  <doc_id>204</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We start with a low acceptance rate which grows up to 30% after 15 different contexts were incorporated.</text>
                  <doc_id>205</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Note how the L 1 norm of q (its partition function) decreases after each refinement, that is, q is gradually brought closer to p, resulting in the increased number of exact samples and better acceptance rate.</text>
                  <doc_id>206</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Note that, starting from iteration one, all refinements here correspond to 2-grams (i.e. one-word contexts).</text>
                  <doc_id>207</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This can be explained by the fact that, in sampling, lower-order refinements are those that mostly increase acceptance rate (rationale: highorder n-grams are compatible with fewer grammar rules).</text>
                  <doc_id>208</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>9 10 0 1000 0.1 0.2 0.3 1.0 1.5 2.0</text>
                  <doc_id>209</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>210</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679; &#9679; &#9679;</text>
                  <doc_id>211</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>212</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>213</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>214</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>215</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>216</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>218</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>219</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>220</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>221</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>222</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>223</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>224</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>225</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>226</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679;</text>
                  <doc_id>227</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>exact</text>
                  <doc_id>228</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>accrate refinement</text>
                  <doc_id>229</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679; &#9679;</text>
                  <doc_id>230</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0 5 10</text>
                  <doc_id>231</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>232</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>233</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L1</text>
                  <doc_id>234</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>235</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>236</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Iteration (i)</text>
                  <doc_id>237</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679; &#9679; &#9679; &#9679; &#9679;</text>
                  <doc_id>238</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679; &#9679; &#9679; &#9679; &#9679;</text>
                  <doc_id>239</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>240</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9679;</text>
                  <doc_id>241</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>4 Experiments</title>
        <text>We used the Moses toolkit (Koehn et al., 2007) to extract a SCFG following Chiang (2005) from the 6 th version of the Europarl collection (Koehn, 2005) (German-English portion). We trained language models using lmplz (Heafield et al., 2013) and interpolated the models trained on the English monolingual data made available by the WMT (Callison-Burch et al., 2012) (i.e. Europarl, newscommentaries, news-2012 and commoncrawl). Tuning was performed via MERT using newstest2010 as development set; test sentences were extracted from newstest2011. Finally, we restricted our SCFGs to having at most 10 target productions for a given source production. Figure 3 shows some properties of the initial grammar G(f) as a function of the input sentence length (the quantities are averages over 20 sentences for each class of input length). The number of unigrams grows linearly with the input length, while the number of unique bigrams compatible with strings generated by G(f) appears to grow quadratically 9 and the size of the grammar in number of rules appears to be cubic &#8212; a consequence of having up to two nonterminals on the right-hand side of a rule.
Figure 4 shows the number of refinement operations until convergence in optimisation and sampling, as well as the total duration, as a function of the input length. 10 The plots will be discussed in detail below.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We used the Moses toolkit (Koehn et al., 2007) to extract a SCFG following Chiang (2005) from the 6 th version of the Europarl collection (Koehn, 2005) (German-English portion).</text>
              <doc_id>242</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We trained language models using lmplz (Heafield et al., 2013) and interpolated the models trained on the English monolingual data made available by the WMT (Callison-Burch et al., 2012) (i.e. Europarl, newscommentaries, news-2012 and commoncrawl).</text>
              <doc_id>243</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Tuning was performed via MERT using newstest2010 as development set; test sentences were extracted from newstest2011.</text>
              <doc_id>244</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we restricted our SCFGs to having at most 10 target productions for a given source production.</text>
              <doc_id>245</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Figure 3 shows some properties of the initial grammar G(f) as a function of the input sentence length (the quantities are averages over 20 sentences for each class of input length).</text>
              <doc_id>246</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The number of unigrams grows linearly with the input length, while the number of unique bigrams compatible with strings generated by G(f) appears to grow quadratically 9 and the size of the grammar in number of rules appears to be cubic &#8212; a consequence of having up to two nonterminals on the right-hand side of a rule.</text>
              <doc_id>247</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Figure 4 shows the number of refinement operations until convergence in optimisation and sampling, as well as the total duration, as a function of the input length.</text>
              <doc_id>248</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>10 The plots will be discussed in detail below.</text>
              <doc_id>249</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Optimisation</title>
            <text>In optimisation (Figures 4a and 4b), the number of refinements up to convergence appears to be linear with the input length, while the total duration grows much quicker. These findings are further discussed in what follows.
Table 2 shows some important quantities regarding optimisation with OS &#8727; using a 4-gram LM. The first column shows how many sentences we are considering, the second column shows the sentence length, the third column m is the average number of refinements up to convergence. Column |A| refers to the refinement type, which is the number of states in the automaton A, that is, the order of
9 The number of unique bigrams is an estimate obtained by
combining the terminals at the boundary of nonterminals that may be adjacent in a derivation. 10 The current implementation faces timeouts depending on
the length of the input sentence and the order of the language model, explaining why certain curves are interrupted earlier than others in Figure 4.
2 4 6 8 10
Input length
(a) Unigrams in G(f)
2 4 6 8 10
Input length
(b) Bigrams compatible with G(f)
2 4 6 8 10
Input length
(c) Number of rules in G(f)
|R f | |R 0 |
the n-grams being re-weighted (e.g. |A| = 2 when refining bigrams sharing a one-word context). Column count refers to the average number of refinements that are due to each refinement type. Finally, the last column compares the number of rules in the final proposal to that of the initial one.
The first positive result concerns how much context OS &#8727; needs to take into account for finding the optimum derivation. Table 2 (column m) shows that OS &#8727; explores a very reduced space of n-gram contexts up to convergence. To illustrate that, consider the last row in Table 2 (sentences with 6 words). On average, convergence requires incorporating only about 103 contexts of variable order, of which 55 are bigram (2-word) contexts (remember that |A| = 3 when accounting for a 2-word context). According to Figure 3b, in sentences with 6 words, about 2,000 bigrams are compatible with strings generated by G(f). This means that only 2.75% of these bigrams (55 out of 2,000) need to be explicitly accounted for, illustrating how wasteful a full intersection would be.
A problem, however, is that the time until convergence grows quickly with the length of the input (Figure 4b). This can be explained as follows. At each iteration the grammar is refined to account for n-grams sharing a context of (n &#8722; 1) words. That
|R f | |R 0 |
operation typically results in a larger grammar: most rules are preserved, some rules are deleted, but more importantly, some rules are added to account for the portion of the current grammar that involves the selected n-grams. Enlarging the grammar at each iteration means that successive refinements become incrementally slower. The histogram of refinement types of Table 2 highlights how efficient OS &#8727; is w.r.t. the space of n-grams it needs to explore before convergence. The problem is clearly not the number of refinements, but rather the relation between the growth of the grammar and the successive intersections. Controlling for this growth and optimising the intersection as to partially reuse previously computed charts may be the key for a more generally tractable solution.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In optimisation (Figures 4a and 4b), the number of refinements up to convergence appears to be linear with the input length, while the total duration grows much quicker.</text>
                  <doc_id>250</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These findings are further discussed in what follows.</text>
                  <doc_id>251</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 2 shows some important quantities regarding optimisation with OS &#8727; using a 4-gram LM.</text>
                  <doc_id>252</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The first column shows how many sentences we are considering, the second column shows the sentence length, the third column m is the average number of refinements up to convergence.</text>
                  <doc_id>253</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Column |A| refers to the refinement type, which is the number of states in the automaton A, that is, the order of</text>
                  <doc_id>254</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>9 The number of unique bigrams is an estimate obtained by</text>
                  <doc_id>255</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>combining the terminals at the boundary of nonterminals that may be adjacent in a derivation.</text>
                  <doc_id>256</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>10 The current implementation faces timeouts depending on</text>
                  <doc_id>257</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the length of the input sentence and the order of the language model, explaining why certain curves are interrupted earlier than others in Figure 4.</text>
                  <doc_id>258</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 4 6 8 10</text>
                  <doc_id>259</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Input length</text>
                  <doc_id>260</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(a) Unigrams in G(f)</text>
                  <doc_id>261</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 4 6 8 10</text>
                  <doc_id>262</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Input length</text>
                  <doc_id>263</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(b) Bigrams compatible with G(f)</text>
                  <doc_id>264</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 4 6 8 10</text>
                  <doc_id>265</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Input length</text>
                  <doc_id>266</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(c) Number of rules in G(f)</text>
                  <doc_id>267</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>|R f | |R 0 |</text>
                  <doc_id>268</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the n-grams being re-weighted (e.g. |A| = 2 when refining bigrams sharing a one-word context).</text>
                  <doc_id>269</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Column count refers to the average number of refinements that are due to each refinement type.</text>
                  <doc_id>270</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, the last column compares the number of rules in the final proposal to that of the initial one.</text>
                  <doc_id>271</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The first positive result concerns how much context OS &#8727; needs to take into account for finding the optimum derivation.</text>
                  <doc_id>272</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Table 2 (column m) shows that OS &#8727; explores a very reduced space of n-gram contexts up to convergence.</text>
                  <doc_id>273</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>To illustrate that, consider the last row in Table 2 (sentences with 6 words).</text>
                  <doc_id>274</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>On average, convergence requires incorporating only about 103 contexts of variable order, of which 55 are bigram (2-word) contexts (remember that |A| = 3 when accounting for a 2-word context).</text>
                  <doc_id>275</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>According to Figure 3b, in sentences with 6 words, about 2,000 bigrams are compatible with strings generated by G(f).</text>
                  <doc_id>276</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This means that only 2.75% of these bigrams (55 out of 2,000) need to be explicitly accounted for, illustrating how wasteful a full intersection would be.</text>
                  <doc_id>277</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A problem, however, is that the time until convergence grows quickly with the length of the input (Figure 4b).</text>
                  <doc_id>278</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This can be explained as follows.</text>
                  <doc_id>279</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>At each iteration the grammar is refined to account for n-grams sharing a context of (n &#8722; 1) words.</text>
                  <doc_id>280</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>That</text>
                  <doc_id>281</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>|R f | |R 0 |</text>
                  <doc_id>282</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>operation typically results in a larger grammar: most rules are preserved, some rules are deleted, but more importantly, some rules are added to account for the portion of the current grammar that involves the selected n-grams.</text>
                  <doc_id>283</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Enlarging the grammar at each iteration means that successive refinements become incrementally slower.</text>
                  <doc_id>284</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The histogram of refinement types of Table 2 highlights how efficient OS &#8727; is w.r.t.</text>
                  <doc_id>285</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>the space of n-grams it needs to explore before convergence.</text>
                  <doc_id>286</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The problem is clearly not the number of refinements, but rather the relation between the growth of the grammar and the successive intersections.</text>
                  <doc_id>287</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Controlling for this growth and optimising the intersection as to partially reuse previously computed charts may be the key for a more generally tractable solution.</text>
                  <doc_id>288</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Sampling</title>
            <text>Figure 4c shows that sampling is more economical than optimisation in that it explicitly incorporates even fewer contexts. Note how OS &#8727; converges to acceptance rates from 1% to 10% in much fewer iterations than are necessary to find an optimum 11 . Although the convergence in sampling is
11 Currently we use MERT to train the model&#8217;s weight vector &#8212; which is normalised by its L 1 norm in the Moses implementation. While optimisation is not sensitive to the scale of the weights, in sampling the scale determines how flat or
2 4 6 8 10
Input length
(a) Optimisation: number of refinements.
2 4 6 8 10
Input length
(b) Optimisation: time for convergence.
faster than in optimisation, the total duration is still an issue (Figure 4b).
Table 3 shows the same quantities as Table 2, but now for sampling. It is worth highlighting that even though we are using an upper-bound over a 4-gram LM (and aiming at a 5% acceptance rate), very few contexts are selected for refinement, most of them lower-order ones (one-word contexts &#8212; rows with |A| = 2). Observe that an improved acceptance rate always leads to faster acquisition of exact samples after we stop refining our proxy distribution. However, Figure 4d shows for example that moving from 5% to 10% acceptance rate using a 4-gram LM (curves X and Y) is time-consuming. Thus there is a trade-off between how much time one spends improving the acceptance rate and how many exact samples one intends do draw. Figure 5 shows the average time to draw batches between
peaked the distribution is. Arun et al. (2010) experiment with scaling MERT-trained weights as to maximise BLEU on heldout data, as well as with MBR training. A more adequate training algorithm along similar lines is reserved for future work.
Time (s)
200 500 1000 2000 5000 10000
1 2
5% AR 10% AR
2 2 2 2
1 1 1 1
1e+00 1e+02 1e+04 1e+06
Samples
one and one million samples from two exact samplers that were refined up to 5% and 10% acceptance rate respectively. The sampler at 5% AR (which is faster to obtain) turns out to be more efficient if we aim at producing less than 10K samples.
Finally, note that samples are independently
1 2 1
drawn from the final proposal, making the approach an appealing candidate to parallelism in order to increase the effective acceptance rate.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Figure 4c shows that sampling is more economical than optimisation in that it explicitly incorporates even fewer contexts.</text>
                  <doc_id>289</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Note how OS &#8727; converges to acceptance rates from 1% to 10% in much fewer iterations than are necessary to find an optimum 11 .</text>
                  <doc_id>290</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Although the convergence in sampling is</text>
                  <doc_id>291</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>11 Currently we use MERT to train the model&#8217;s weight vector &#8212; which is normalised by its L 1 norm in the Moses implementation.</text>
                  <doc_id>292</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>While optimisation is not sensitive to the scale of the weights, in sampling the scale determines how flat or</text>
                  <doc_id>293</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 4 6 8 10</text>
                  <doc_id>294</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Input length</text>
                  <doc_id>295</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(a) Optimisation: number of refinements.</text>
                  <doc_id>296</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 4 6 8 10</text>
                  <doc_id>297</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Input length</text>
                  <doc_id>298</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(b) Optimisation: time for convergence.</text>
                  <doc_id>299</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>faster than in optimisation, the total duration is still an issue (Figure 4b).</text>
                  <doc_id>300</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 3 shows the same quantities as Table 2, but now for sampling.</text>
                  <doc_id>301</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It is worth highlighting that even though we are using an upper-bound over a 4-gram LM (and aiming at a 5% acceptance rate), very few contexts are selected for refinement, most of them lower-order ones (one-word contexts &#8212; rows with |A| = 2).</text>
                  <doc_id>302</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Observe that an improved acceptance rate always leads to faster acquisition of exact samples after we stop refining our proxy distribution.</text>
                  <doc_id>303</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>However, Figure 4d shows for example that moving from 5% to 10% acceptance rate using a 4-gram LM (curves X and Y) is time-consuming.</text>
                  <doc_id>304</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Thus there is a trade-off between how much time one spends improving the acceptance rate and how many exact samples one intends do draw.</text>
                  <doc_id>305</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 5 shows the average time to draw batches between</text>
                  <doc_id>306</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>peaked the distribution is.</text>
                  <doc_id>307</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Arun et al. (2010) experiment with scaling MERT-trained weights as to maximise BLEU on heldout data, as well as with MBR training.</text>
                  <doc_id>308</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>A more adequate training algorithm along similar lines is reserved for future work.</text>
                  <doc_id>309</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Time (s)</text>
                  <doc_id>310</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>200 500 1000 2000 5000 10000</text>
                  <doc_id>311</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 2</text>
                  <doc_id>312</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5% AR 10% AR</text>
                  <doc_id>313</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 2 2 2</text>
                  <doc_id>314</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 1 1 1</text>
                  <doc_id>315</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1e+00 1e+02 1e+04 1e+06</text>
                  <doc_id>316</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Samples</text>
                  <doc_id>317</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>one and one million samples from two exact samplers that were refined up to 5% and 10% acceptance rate respectively.</text>
                  <doc_id>318</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The sampler at 5% AR (which is faster to obtain) turns out to be more efficient if we aim at producing less than 10K samples.</text>
                  <doc_id>319</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Finally, note that samples are independently</text>
                  <doc_id>320</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 2 1</text>
                  <doc_id>321</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>drawn from the final proposal, making the approach an appealing candidate to parallelism in order to increase the effective acceptance rate.</text>
                  <doc_id>322</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>5 Related Work</title>
        <text>Rush and Collins (2011) do not consider sampling, but they address exact decoding for hierarchical translation. They use a Dual Decomposition approach (a special case of Lagrangian Relaxation), where the target CFG (hypergraph in their terminology) component and the target language model component &#8220;trade-off&#8221; their weights so as to ensure agreement on what each component believes to be the maximum. In many cases, this technique is able to detect the actual true maximum derivation. When this is not the case, they use a finite-statebased intersection mechanism to &#8220;tighten&#8221; the first component so that some constraints not satisfied by the current solution are enforced, and iterate until the true maximum is found or a time-out is met, which results in a high proportion of finding the true maximum. Arun et al. (2009, 2010) address the question of sampling in a standard phrase-based translation model (Koehn et al., 2003). Contrarily to our use of rejection sampling (a Monte-Carlo method), they use a Gibbs sampler (a Markov-Chain Monte- Carlo (MCMC) method). Samples are obtained by iteratively re-sampling groups of well-designed variables in such a way that (i) the sampler does not tend to be trapped locally by high correlations between conditioning and conditioned variables, and (ii) the combinatorial space of possibilities for the next step is small enough so that conditional probabilities can be computed explicitly. By contrast to our exact approach, the samples obtained by Gibbs sampling are not independent, but form a Markov chain that only converges to the target distribution in the limit, with convergence properties difficult to assess. Also by contrast to us, these papers do not address the question of finding the maximum derivation directly, but only through finding a maximum among the derivations sampled so far, which in principle can be quite different.
Blunsom and Osborne (2008) address probabilistic inference, this time, as we do, in the context of hierarchical translation, where sampling is used both for the purposes of decoding and training the model. When decoding in the presence of a language model, an approximate sampling procedure is performed in two stages. First, cube-pruning is employed to construct a WCFG which generates a subset of all the possible derivations that would correspond to a full intersection with the target language model. In a second step this grammar is sampled through the same dynamic programming procedure that we have described in &#167;2.4. By contrast to our approach, the paper does not attempt to perform exact inference. However it does not only address the question of decoding, but also that of training the model, which requires, in addition to sampling, an estimate of the model&#8217;s partition function. In common with Arun et al. (2010), the authors stress the fact that a sampler of derivations is also a sampler of translations as strings, while a maximiser over derivations cannot be used to find the maximum translation string.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Rush and Collins (2011) do not consider sampling, but they address exact decoding for hierarchical translation.</text>
              <doc_id>323</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>They use a Dual Decomposition approach (a special case of Lagrangian Relaxation), where the target CFG (hypergraph in their terminology) component and the target language model component &#8220;trade-off&#8221; their weights so as to ensure agreement on what each component believes to be the maximum.</text>
              <doc_id>324</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In many cases, this technique is able to detect the actual true maximum derivation.</text>
              <doc_id>325</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>When this is not the case, they use a finite-statebased intersection mechanism to &#8220;tighten&#8221; the first component so that some constraints not satisfied by the current solution are enforced, and iterate until the true maximum is found or a time-out is met, which results in a high proportion of finding the true maximum.</text>
              <doc_id>326</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Arun et al. (2009, 2010) address the question of sampling in a standard phrase-based translation model (Koehn et al., 2003).</text>
              <doc_id>327</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Contrarily to our use of rejection sampling (a Monte-Carlo method), they use a Gibbs sampler (a Markov-Chain Monte- Carlo (MCMC) method).</text>
              <doc_id>328</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Samples are obtained by iteratively re-sampling groups of well-designed variables in such a way that (i) the sampler does not tend to be trapped locally by high correlations between conditioning and conditioned variables, and (ii) the combinatorial space of possibilities for the next step is small enough so that conditional probabilities can be computed explicitly.</text>
              <doc_id>329</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>By contrast to our exact approach, the samples obtained by Gibbs sampling are not independent, but form a Markov chain that only converges to the target distribution in the limit, with convergence properties difficult to assess.</text>
              <doc_id>330</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Also by contrast to us, these papers do not address the question of finding the maximum derivation directly, but only through finding a maximum among the derivations sampled so far, which in principle can be quite different.</text>
              <doc_id>331</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Blunsom and Osborne (2008) address probabilistic inference, this time, as we do, in the context of hierarchical translation, where sampling is used both for the purposes of decoding and training the model.</text>
              <doc_id>332</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>When decoding in the presence of a language model, an approximate sampling procedure is performed in two stages.</text>
              <doc_id>333</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>First, cube-pruning is employed to construct a WCFG which generates a subset of all the possible derivations that would correspond to a full intersection with the target language model.</text>
              <doc_id>334</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In a second step this grammar is sampled through the same dynamic programming procedure that we have described in &#167;2.4.</text>
              <doc_id>335</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>By contrast to our approach, the paper does not attempt to perform exact inference.</text>
              <doc_id>336</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>However it does not only address the question of decoding, but also that of training the model, which requires, in addition to sampling, an estimate of the model&#8217;s partition function.</text>
              <doc_id>337</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In common with Arun et al. (2010), the authors stress the fact that a sampler of derivations is also a sampler of translations as strings, while a maximiser over derivations cannot be used to find the maximum translation string.</text>
              <doc_id>338</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>6 Conclusions</title>
        <text>The approach we have presented is, to our knowledge, the first one to address the problem of exact sampling for hierarchical translation and to do that in a framework that also handles exact optimisation. Our experiments show that only a fraction of the language model n-grams need to be incorporated in the target grammar in order to perform exact inference in this approach. However, in the current implementation, we experience timeouts for sentences of even moderate length. We are working on improving this situation along three dimensions: (i) our implementation of the Earley Intersection rebuilds a grammar from scratch at each intersection, while it could capitalise on the charts built during the previous steps; (ii) the unigramlevel max-backoffs are not as tight as they could be if one took into account more precisely the set of contexts in which each word can appear relative to the grammar; (iii) most importantly, while our refinements are &#8220;local&#8221; in the sense of addressing one n-gram context at a time, they still affect a large portion of the rules in the current grammar, even rules that have very low probability of being ever sampled by this grammar; by preventing refinement of such rules during the intersection process, we may be able to make the intersection more local and to produce much smaller grammars, without losing the exactness properties of the approach.
Acknowledgements
The first author wishes to thank the PASCAL-2 Visit to Industry programme for partially funding his visit to Xerox Research Centre Europe last Fall, which initiated this collaboration.
References
Abhishek Arun, Chris Dyer, Barry Haddow, Phil Blunsom, Adam Lopez, and Philipp Koehn. 2009. Monte carlo inference and maximization for phrase-based translation. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning, CoNLL &#8217;09, pages 102&#8211;110, Stroudsburg, PA, USA. Association for Computational Linguistics.
Abhishek Arun, Barry Haddow, Philipp Koehn, Adam Lopez, Chris Dyer, and Phil Blunsom. 2010. Monte carlo techniques for phrase-based translation. Machine Translation, 24(2):103&#8211;121, June.
Yehoshua Bar-Hillel, Micha A. Perles, and Eli Shamir. 1961. On formal properties of simple phrase structure grammars. Zeitschrift f&#252;r Phonetik, Sprachwissenschaft und Kommunikationsforschung, (14):143&#8211; 172.
Sylvie Billot and Bernard Lang. 1989. The structure of shared forests in ambiguous parsing. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, pages 143&#8211;151, Vancouver, British Columbia, Canada, June. Association for Computational Linguistics.
Phil Blunsom and Miles Osborne. 2008. Probabilistic inference for machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &#8217;08, pages 215&#8211; 223, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10&#8211; 51, Montr&#233;al, Canada, June. Association for Computational Linguistics.
Simon Carter, Marc Dymetman, and Guillaume Bouchard. 2012. Exact Sampling and Decoding in High-Order Hidden Markov Models. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1125&#8211;1134, Jeju Island, Korea, July. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &#8217;05, pages 263&#8211; 270, Stroudsburg, PA, USA. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, 33:201&#8211;228.
Thomas H. Cormen, Clifford Stein, Ronald L. Rivest, and Charles E. Leiserson. 2001. Introduction to Algorithms. McGraw-Hill Higher Education, 2nd edition.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitkevitch, Phil Blunsom, and Philip Resnik. 2010. cdec: a decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of the ACL 2010 System Demonstrations, ACLDemos &#8217;10, pages 7&#8211;12, Stroudsburg, PA, USA. Association for Computational Linguistics.
Christopher Dyer. 2010. A Formal Model of Ambiguity and its Applications in Machine Translation. Ph.D. thesis, University of Maryland.
M. Dymetman, G. Bouchard, and S. Carter. 2012a. The OS* Algorithm: a Joint Approach to Exact Optimization and Sampling. ArXiv e-prints, July.
Marc Dymetman, Guillaume Bouchard, and Simon Carter. 2012b. Optimization and sampling for nlp from a unified viewpoint. In Proceedings of the First International Workshop on Optimization Techniques for Human Language Technology, pages 79&#8211; 94, Mumbai, India, December. The COLING 2012 Organizing Committee.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable modified Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, August.
Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144&#8211;151, Prague, Czech Republic, June. Association for Computational Linguistics.
Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 139&#8211;146, Rochester, New York, April. Association for Computational Linguistics.
Daniel Jurafsky and James H. Martin. 2000. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition (Prentice Hall Series in Artificial Intelligence). Prentice Hall, 1 edition.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL &#8217;03, pages 48&#8211;54, Stroudsburg, PA, USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond&#345;ej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL &#8217;07, pages 177&#8211;180, Stroudsburg, PA, USA. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit, pages 79&#8211;86.
Shankar Kumar and William Byrne. 2004. Minimum Bayes Risk Decoding for Statistical Machine Translation. In Joint Conference of Human Language Technologies and the North American chapter of the Association for Computational Linguistics (HLT-NAACL 2004).
Mark-Jan Nederhof and Giorgio Satta. 2008. Probabilistic parsing. In M. Dolores Jimnez-Lpez G. Bel- Enguix and C. Martn-Vide, editors, New Developments in Formal Languages and Applications, Studies in Computational Intelligence, volume 113, pages 229&#8211;258. Springer.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, volume 1 of ACL &#8217;03, pages 160&#8211; 167, Stroudsburg, PA, USA. Association for Computational Linguistics.
Christian P. Robert and George Casella. 2004. Monte Carlo Statistical Methods (Springer Texts in Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA.
Alexander M. Rush and Michael Collins. 2011. Exact decoding of syntactic translation models through lagrangian relaxation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT &#8217;11, pages 72&#8211;82, Stroudsburg, PA, USA. Association for Computational Linguistics.
David A. Smith and Jason Eisner. 2006. Minimum risk annealing for training log-linear models. In Proceedings of the COLING/ACL on Main conference poster sessions, COLING-ACL &#8217;06, pages 787&#8211;794, Stroudsburg, PA, USA. Association for Computational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP- CoNLL), pages 764&#8211;773, Prague, Czech Republic, June. Association for Computational Linguistics.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The approach we have presented is, to our knowledge, the first one to address the problem of exact sampling for hierarchical translation and to do that in a framework that also handles exact optimisation.</text>
              <doc_id>339</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our experiments show that only a fraction of the language model n-grams need to be incorporated in the target grammar in order to perform exact inference in this approach.</text>
              <doc_id>340</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, in the current implementation, we experience timeouts for sentences of even moderate length.</text>
              <doc_id>341</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We are working on improving this situation along three dimensions: (i) our implementation of the Earley Intersection rebuilds a grammar from scratch at each intersection, while it could capitalise on the charts built during the previous steps; (ii) the unigramlevel max-backoffs are not as tight as they could be if one took into account more precisely the set of contexts in which each word can appear relative to the grammar; (iii) most importantly, while our refinements are &#8220;local&#8221; in the sense of addressing one n-gram context at a time, they still affect a large portion of the rules in the current grammar, even rules that have very low probability of being ever sampled by this grammar; by preventing refinement of such rules during the intersection process, we may be able to make the intersection more local and to produce much smaller grammars, without losing the exactness properties of the approach.</text>
              <doc_id>342</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Acknowledgements</text>
              <doc_id>343</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The first author wishes to thank the PASCAL-2 Visit to Industry programme for partially funding his visit to Xerox Research Centre Europe last Fall, which initiated this collaboration.</text>
              <doc_id>344</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>References</text>
              <doc_id>345</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Abhishek Arun, Chris Dyer, Barry Haddow, Phil Blunsom, Adam Lopez, and Philipp Koehn.</text>
              <doc_id>346</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>347</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Monte carlo inference and maximization for phrase-based translation.</text>
              <doc_id>348</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning, CoNLL &#8217;09, pages 102&#8211;110, Stroudsburg, PA, USA.</text>
              <doc_id>349</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>350</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Abhishek Arun, Barry Haddow, Philipp Koehn, Adam Lopez, Chris Dyer, and Phil Blunsom.</text>
              <doc_id>351</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>352</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Monte carlo techniques for phrase-based translation.</text>
              <doc_id>353</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Machine Translation, 24(2):103&#8211;121, June.</text>
              <doc_id>354</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Yehoshua Bar-Hillel, Micha A. Perles, and Eli Shamir.</text>
              <doc_id>355</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1961.</text>
              <doc_id>356</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>On formal properties of simple phrase structure grammars.</text>
              <doc_id>357</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Zeitschrift f&#252;r Phonetik, Sprachwissenschaft und Kommunikationsforschung, (14):143&#8211; 172.</text>
              <doc_id>358</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Sylvie Billot and Bernard Lang.</text>
              <doc_id>359</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1989.</text>
              <doc_id>360</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The structure of shared forests in ambiguous parsing.</text>
              <doc_id>361</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, pages 143&#8211;151, Vancouver, British Columbia, Canada, June.</text>
              <doc_id>362</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>363</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Phil Blunsom and Miles Osborne.</text>
              <doc_id>364</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>365</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Probabilistic inference for machine translation.</text>
              <doc_id>366</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &#8217;08, pages 215&#8211; 223, Stroudsburg, PA, USA.</text>
              <doc_id>367</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>368</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia.</text>
              <doc_id>369</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2012.</text>
              <doc_id>370</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Findings of the 2012 workshop on statistical machine translation.</text>
              <doc_id>371</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10&#8211; 51, Montr&#233;al, Canada, June.</text>
              <doc_id>372</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>373</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Simon Carter, Marc Dymetman, and Guillaume Bouchard.</text>
              <doc_id>374</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2012.</text>
              <doc_id>375</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Exact Sampling and Decoding in High-Order Hidden Markov Models.</text>
              <doc_id>376</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1125&#8211;1134, Jeju Island, Korea, July.</text>
              <doc_id>377</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>378</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>David Chiang.</text>
              <doc_id>379</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>380</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A hierarchical phrase-based model for statistical machine translation.</text>
              <doc_id>381</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &#8217;05, pages 263&#8211; 270, Stroudsburg, PA, USA.</text>
              <doc_id>382</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>383</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>David Chiang.</text>
              <doc_id>384</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>385</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Hierarchical Phrase-Based Translation.</text>
              <doc_id>386</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 33:201&#8211;228.</text>
              <doc_id>387</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Thomas H. Cormen, Clifford Stein, Ronald L. Rivest, and Charles E. Leiserson.</text>
              <doc_id>388</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2001.</text>
              <doc_id>389</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Introduction to Algorithms.</text>
              <doc_id>390</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>McGraw-Hill Higher Education, 2nd edition.</text>
              <doc_id>391</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitkevitch, Phil Blunsom, and Philip Resnik.</text>
              <doc_id>392</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2010. cdec: a decoder, alignment, and learning framework for finite-state and context-free translation models.</text>
              <doc_id>393</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the ACL 2010 System Demonstrations, ACLDemos &#8217;10, pages 7&#8211;12, Stroudsburg, PA, USA.</text>
              <doc_id>394</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>395</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Christopher Dyer.</text>
              <doc_id>396</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>397</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A Formal Model of Ambiguity and its Applications in Machine Translation.</text>
              <doc_id>398</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Ph.D.</text>
              <doc_id>399</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>thesis, University of Maryland.</text>
              <doc_id>400</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>M. Dymetman, G. Bouchard, and S. Carter.</text>
              <doc_id>401</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2012a.</text>
              <doc_id>402</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The OS* Algorithm: a Joint Approach to Exact Optimization and Sampling.</text>
              <doc_id>403</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>ArXiv e-prints, July.</text>
              <doc_id>404</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Marc Dymetman, Guillaume Bouchard, and Simon Carter.</text>
              <doc_id>405</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2012b.</text>
              <doc_id>406</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Optimization and sampling for nlp from a unified viewpoint.</text>
              <doc_id>407</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the First International Workshop on Optimization Techniques for Human Language Technology, pages 79&#8211; 94, Mumbai, India, December.</text>
              <doc_id>408</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The COLING 2012 Organizing Committee.</text>
              <doc_id>409</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn.</text>
              <doc_id>410</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2013.</text>
              <doc_id>411</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Scalable modified Kneser-Ney language model estimation.</text>
              <doc_id>412</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, August.</text>
              <doc_id>413</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Liang Huang and David Chiang.</text>
              <doc_id>414</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>415</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Forest rescoring: Faster decoding with integrated language models.</text>
              <doc_id>416</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144&#8211;151, Prague, Czech Republic, June.</text>
              <doc_id>417</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>418</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Mark Johnson, Thomas Griffiths, and Sharon Goldwater.</text>
              <doc_id>419</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>420</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Bayesian inference for PCFGs via Markov chain Monte Carlo.</text>
              <doc_id>421</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 139&#8211;146, Rochester, New York, April.</text>
              <doc_id>422</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>423</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Daniel Jurafsky and James H. Martin.</text>
              <doc_id>424</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2000.</text>
              <doc_id>425</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition (Prentice Hall Series in Artificial Intelligence).</text>
              <doc_id>426</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Prentice Hall, 1 edition.</text>
              <doc_id>427</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Philipp Koehn, Franz Josef Och, and Daniel Marcu.</text>
              <doc_id>428</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>429</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Statistical phrase-based translation.</text>
              <doc_id>430</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL &#8217;03, pages 48&#8211;54, Stroudsburg, PA, USA.</text>
              <doc_id>431</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>432</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond&#345;ej Bojar, Alexandra Constantin, and Evan Herbst.</text>
              <doc_id>433</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>434</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Moses: open</text>
              <doc_id>435</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>source toolkit for statistical machine translation.</text>
              <doc_id>436</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL &#8217;07, pages 177&#8211;180, Stroudsburg, PA, USA.</text>
              <doc_id>437</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>438</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Philipp Koehn.</text>
              <doc_id>439</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>440</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Europarl: A parallel corpus for statistical machine translation.</text>
              <doc_id>441</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of Machine Translation Summit, pages 79&#8211;86.</text>
              <doc_id>442</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Shankar Kumar and William Byrne.</text>
              <doc_id>443</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2004.</text>
              <doc_id>444</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Minimum Bayes Risk Decoding for Statistical Machine Translation.</text>
              <doc_id>445</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Joint Conference of Human Language Technologies and the North American chapter of the Association for Computational Linguistics (HLT-NAACL 2004).</text>
              <doc_id>446</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Mark-Jan Nederhof and Giorgio Satta.</text>
              <doc_id>447</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>448</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Probabilistic parsing.</text>
              <doc_id>449</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In M. Dolores Jimnez-Lpez G. Bel- Enguix and C. Martn-Vide, editors, New Developments in Formal Languages and Applications, Studies in Computational Intelligence, volume 113, pages 229&#8211;258.</text>
              <doc_id>450</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Springer.</text>
              <doc_id>451</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Franz Josef Och.</text>
              <doc_id>452</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>453</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Minimum error rate training in statistical machine translation.</text>
              <doc_id>454</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, volume 1 of ACL &#8217;03, pages 160&#8211; 167, Stroudsburg, PA, USA.</text>
              <doc_id>455</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>456</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Christian P. Robert and George Casella.</text>
              <doc_id>457</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2004.</text>
              <doc_id>458</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Monte Carlo Statistical Methods (Springer Texts in Statistics).</text>
              <doc_id>459</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Springer-Verlag New York, Inc., Secaucus, NJ, USA.</text>
              <doc_id>460</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Alexander M. Rush and Michael Collins.</text>
              <doc_id>461</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2011.</text>
              <doc_id>462</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Exact decoding of syntactic translation models through lagrangian relaxation.</text>
              <doc_id>463</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT &#8217;11, pages 72&#8211;82, Stroudsburg, PA, USA.</text>
              <doc_id>464</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>465</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>David A. Smith and Jason Eisner.</text>
              <doc_id>466</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>467</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Minimum risk annealing for training log-linear models.</text>
              <doc_id>468</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the COLING/ACL on Main conference poster sessions, COLING-ACL &#8217;06, pages 787&#8211;794, Stroudsburg, PA, USA.</text>
              <doc_id>469</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>470</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki.</text>
              <doc_id>471</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>472</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Online large-margin training for statistical machine translation.</text>
              <doc_id>473</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP- CoNLL), pages 764&#8211;773, Prague, Czech Republic, June.</text>
              <doc_id>474</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>475</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Optimisation steps showing the iteration (i), the number of rules in the grammar and the translation associated to the optimum derivation.</caption>
        <reference_text></reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>i</cell>
              <cell>Rules</cell>
              <cell>Optimum</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>0</cell>
              <cell>311</cell>
              <cell>&lt;s&gt; one last observation . &lt;/s&gt;</cell>
            </row>
            <row>
              <cell>1</cell>
              <cell>454</cell>
              <cell>&lt;s&gt; one last observation . &lt;/s&gt;</cell>
            </row>
            <row>
              <cell>2</cell>
              <cell>628</cell>
              <cell>&lt;s&gt; one last observation . &lt;/s&gt;</cell>
            </row>
            <row>
              <cell>3</cell>
              <cell>839</cell>
              <cell>&lt;s&gt; one final observation . &lt;/s&gt;</cell>
            </row>
            <row>
              <cell>4</cell>
              <cell>1212</cell>
              <cell>&lt;s&gt; one final observation . &lt;/s&gt;
...</cell>
            </row>
            <row>
              <cell>12</cell>
              <cell>3000</cell>
              <cell>&lt;s&gt; a final observation . &lt;/s&gt;</cell>
            </row>
            <row>
              <cell>13</cell>
              <cell>3128</cell>
              <cell>&lt;s&gt; one final observation . &lt;/s&gt;</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Optimisation with a 4-gram LM.</caption>
        <reference_text>In PAGE 7: ... These findings are further discussed in what follows.  Table2  shows some important quantities regard- ing optimisation with OS? using a 4-gram LM. The first column shows how many sentences we are considering, the second column shows the sentence length, the third column m is the average num- ber of refinements up to convergence....  In PAGE 8: ... The first positive result concerns how much con- text OS? needs to take into account for finding the optimum derivation.  Table2  (column m) shows that OS? explores a very reduced space of n-gram contexts up to convergence. To illustrate that, con- sider the last row in Table 2 (sentences with 6 words)....  In PAGE 8: ... Table 2 (column m) shows that OS? explores a very reduced space of n-gram contexts up to convergence. To illustrate that, con- sider the last row in  Table2  (sentences with 6 words). On average, convergence requires incorpo- rating only about 103 contexts of variable order, of which 55 are bigram (2-word) contexts (remember that |A| = 3 when accounting for a 2-word con- text)....  In PAGE 9: ... faster than in optimisation, the total duration is still an issue (Figure 4b). Table 3 shows the same quantities as  Table2 , but now for sampling. It is worth highlighting that even though we are using an upper-bound over a 4-gram LM (and aiming at a 5% acceptance rate), very few contexts are selected for refinement, most of them lower-order ones (one-word contexts ? rows with |A| = 2)....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>S</cell>
              <cell>Length</cell>
              <cell>m</cell>
              <cell>|A|</cell>
              <cell>count</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>9</cell>
              <cell>4</cell>
              <cell>45.0</cell>
              <cell>2</cell>
              <cell>20.3</cell>
              <cell>74.6 &#177; 53.9</cell>
            </row>
            <row>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell>3</cell>
              <cell>19.2</cell>
              <cell></cell>
            </row>
            <row>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell>4</cell>
              <cell>5.4</cell>
              <cell></cell>
            </row>
            <row>
              <cell>10</cell>
              <cell>5</cell>
              <cell>62.3</cell>
              <cell>2</cell>
              <cell>21.9</cell>
              <cell>145.4 &#177; 162.6</cell>
            </row>
            <row>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell>3</cell>
              <cell>32.9</cell>
              <cell></cell>
            </row>
            <row>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell>4</cell>
              <cell>7.5</cell>
              <cell></cell>
            </row>
            <row>
              <cell>9</cell>
              <cell>6</cell>
              <cell>102.8</cell>
              <cell>2</cell>
              <cell>34.7</cell>
              <cell>535.8 &#177; 480.0</cell>
            </row>
            <row>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell>3</cell>
              <cell>54.9</cell>
              <cell></cell>
            </row>
            <row>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell>4</cell>
              <cell>13.2</cell>
              <cell></cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Sampling with a 4-gram LM and reaching a 5% acceptance rate.</caption>
        <reference_text>In PAGE 9: ... faster than in optimisation, the total duration is still an issue (Figure 4b).  Table3  shows the same quantities as Table 2, but now for sampling. It is worth highlighting that even though we are using an upper-bound over a 4-gram LM (and aiming at a 5% acceptance rate), very few contexts are selected for refinement, most of them lower-order ones (one-word contexts ? rows with |A| = 2)....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>S</cell>
              <cell>Input</cell>
              <cell>m</cell>
              <cell>|A|</cell>
              <cell>count</cell>
              <cell>|Rf|</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>|R0|</cell>
            </row>
            <row>
              <cell>10</cell>
              <cell>5</cell>
              <cell>1.0</cell>
              <cell>2</cell>
              <cell>1.0</cell>
              <cell>1.9 ? 1.0</cell>
            </row>
            <row>
              <cell>10</cell>
              <cell>6</cell>
              <cell>6.6</cell>
              <cell>2</cell>
              <cell>6.3</cell>
              <cell>17.6 ? 13.6</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>3</cell>
              <cell>0.3</cell>
            </row>
            <row>
              <cell>10</cell>
              <cell>7</cell>
              <cell>14.5</cell>
              <cell>2</cell>
              <cell>12.9</cell>
              <cell>93.8 ? 68.9</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>3</cell>
              <cell>1.5</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>4</cell>
              <cell>0.1</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references/>
    <citations/>
  </content>
</document>
