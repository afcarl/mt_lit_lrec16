<PAPER>
  <FILENO/>
  <TITLE>Contextual Modeling for Meeting Translation Using Unsupervised Word Sense Disambiguation</TITLE>
  <AUTHORS>
    <AUTHOR>Yang Mei</AUTHOR>
  </AUTHORS>
  <ABSTRACT>
    <A-S ID="S-1524">In this paper we investigate the challenges of applying statistical machine translation to meeting conversations, with a particular view towards analyzing the importance of modeling contextual factors such as the larger discourse context and topic/domain information on translation performance.</A-S>
    <A-S ID="S-1525">We describe the collection of a small corpus of parallel meeting data, the development of a statistical machine translation system in the absence of genre-matched training data, and we present a quantitative analysis of translation errors resulting from the lack of contextual modeling inherent in standard statistical machine translation systems.</A-S>
    <A-S ID="S-1526">Finally, we demonstrate how the largest source of translation errors (lack of topic/domain knowledge) can be addressed by applying documentlevel, unsupervised word sense disambiguation, resulting in performance improvements over the baseline system.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-1527">Although statistical machine translation (SMT) has made great progress over the last decade, most SMT research has focused on the translation of structured input data, such as newswire text or parliamentary proceedings.</S>
        <S ID="S-1528">Spoken language translation has mostly concentrated on twoperson dialogues, such as travel expressions or patient-provider interactions in the medical domain.</S>
        <S ID="S-1529">Recently, more advanced spoken-language data has been addressed, such as speeches (<REF ID="R-18" RPTR="16">St&#252;ker et al., 2007</REF>), lectures (<REF ID="R-20" RPTR="17">Waibel and F&#252;gen, 2008</REF>), and broadcast conversations (<REF ID="R-24" RPTR="20">Zheng et al., 2008</REF>).</S>
        <S ID="S-1530">Problems for machine translation in these genres include the nature of spontaneous speech input (e.g. disfluencies, incomplete sentences, etc.) and the lack of high-quality training data.</S>
        <S ID="S-1531">Data that match the desired type of spoken-language interaction in topic, domain, and, most importantly, in style, can only be obtained by transcribing and translating conversations, which is a costly and time-consuming process.</S>
        <S ID="S-1532">Finally, many spokenlanguage interactions, especially those involving more than two speakers, rely heavily on the participants&#8217; shared contextual knowledge about the domain and topic of the discourse, relationships between speakers, objects in the real-world environment, past interactions, etc.</S>
        <S ID="S-1533">These are typically not modelled in standard SMT systems.</S>
      </P>
      <P>
        <S ID="S-1534">The problem of speech disfluencies has been addressed by disfluency removal techniques that are applied prior to translation (Rao et al., 2007; <REF ID="R-21" RPTR="18">Wang et al., 2010</REF>).</S>
        <S ID="S-1535">Training data sparsity has been addressed by adding data from out-of-domain resources (e.g. (<REF ID="R-13" RPTR="11">Matusov et al., 2004</REF>; <REF ID="R-08" RPTR="7">Hildebrandt et al., 2005</REF>; <REF ID="R-22" RPTR="19">Wu et al., 2008</REF>)), exploiting comparable rather than parallel corpora (<REF ID="R-15" RPTR="13">Munteanu and Marcu, 2005</REF>), or paraphrasing techniques (<REF ID="R-01" RPTR="2">Callison-Burch et al., 2006</REF>).</S>
        <S ID="S-1536">The lack of contextual modeling, by contrast, has so far not been investigated in depth, although it is a generally recognized problem in machine translation.</S>
        <S ID="S-1537">Early attempts at modeling contextual information in machine translation include (<REF ID="R-14" RPTR="12">Mima et al., 1998</REF>), where information about the role, rank and gender of speakers and listeners was utilized in a transfer-based spoken-language translation system for travel dialogs.</S>
        <S ID="S-1538">In (<REF ID="R-11" RPTR="10">Kumar et al., 2008</REF>)</S>
      </P>
      <P>
        <S ID="S-1539">statistically predicted dialog acts were used in a phrase-based SMT system for three different dialog tasks and were shown to improve performance.</S>
        <S ID="S-1540">Recently, contextual source-language features have been incorporated into translation models to predict translation phrases for traveling domain tasks (Stroppa et al., 2007; <REF ID="R-07" RPTR="6">Haque et al., 2009</REF>).</S>
        <S ID="S-1541">However, we are not aware of any work addressing contextual modeling for statistical translation of spoken meeting-style interactions, not least due to the lack of a relevant corpus.</S>
      </P>
      <P>
        <S ID="S-1542">The first goal of this study is to provide a quantitative analysis of the impact of the lack of contextual modeling on translation performance.</S>
        <S ID="S-1543">To this end we have collected a small corpus of parallel multi-party meeting data.</S>
        <S ID="S-1544">A baseline SMT system was trained for this corpus from freely available data resources, and contextual translation errors were manually analyzed with respect to the type of knowledge sources required to resolve them.</S>
        <S ID="S-1545">Our analysis shows that the largest error category consists of word sense disambiguation errors resulting from a lack of topic/domain modeling.</S>
        <S ID="S-1546">In the second part of this study we therefore present a statistical way of incorporating such knowledge by using a graph-based unsupervised word sense disambiguation algorithm at a global (i.e. document) level.</S>
        <S ID="S-1547">Our evaluation on real-world meeting data shows that this technique improves the translation performance slightly but consistently with respect to position-independent word error rate (PER).</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Data</HEADER>
      <P>
        <S ID="S-1573"></S>
      </P>
      <DIV DEPTH="1">
        <HEADER>2.1 Parallel Conversational Data</HEADER>
        <P>
          <S ID="S-1548">For our investigations we used a subset of the AMI corpus (McCowan, 2005), which is a collection of multi-party meetings consisting of approximately 100 hours of multimodal data (audio and video recordings, slide images, data captured from digital whiteboards, etc.) with a variety of existing annotations (audio transcriptions, topic segmentations, summaries, etc.).</S>
          <S ID="S-1549">Meetings were recorded in English and fall into two broad types: scenario meetings, where participants were asked to act out roles in a pre-defined scenario, and nonscenario meetings where participants were not restricted by role assignments.</S>
          <S ID="S-1550">In the first case, the scenario was a project meeting about the development of a new TV remote control; participant roles were project manager, industrial designer, marketing expert, etc.</S>
          <S ID="S-1551">The non-scenario meetings are about the move of an academic lab to a new location on campus.</S>
          <S ID="S-1552">The number of participants is four.</S>
          <S ID="S-1553">For our study we selected 10 meetings (5 scenario meetings and 5 non-scenario meetings) and had their audio transcriptions translated into German (our chosen target language) by two native speakers each.</S>
          <S ID="S-1554">Translators were able to simultaneously read the audio transcription of the meeting, view the video, and listen to the audio, when creating the translation.</S>
          <S ID="S-1555">The translation guidelines were designed to obtain translations that match the source text as closely as possible in terms of style &#8211; for example, translators were asked to maintain the same level of colloquial as opposed to formal language, and to generally ensure that the translation was pragmatically adequate.</S>
          <S ID="S-1556">Obvious errors in the source text (e.g. errors made by non-native English speakers among the meeting participants) were not rendered by equivalent errors in the German translation but were corrected prior to translation.</S>
          <S ID="S-1557">The final translations were reviewed for accuracy and the data were filtered semi-automatically by eliminating incomplete sentences, false starts, fillers, repetitions, etc.</S>
          <S ID="S-1558">Although these would certainly pose problems in a real-world application of spoken language translation, the goal of this study is not to analyze the impact of speech-specific phenomena on translation performance (which, as discussed in Section 1, has been addressed before) but to assess the impact of contextual information such as discourse and knowledge of the real-world surroundings.</S>
          <S ID="S-1559">Finally, single-word utterances such as yeah, oh, no, sure, etc. were downsampled since they are trivial to translate and were very frequent in the corpus; their inclusion would therefore bias the development and tuning of the MT system towards these short utterances at the expense of longer, more informative utterances.</S>
        </P>
        <P>
          <S ID="S-1560">Table 1 shows the word counts of the translated meetings after the preprocessing steps described above.</S>
          <S ID="S-1561">As an indicator of inter-translator</S>
        </P>
        <P>
          <S ID="S-1562">ID type # utter.</S>
          <S ID="S-1563"># word S-BLEU</S>
        </P>
        <P>
          <S ID="S-1564">agreement we computed the symmetric BLEU (S-BLEU) scores on the reference translations (i.e. using one translation as the reference and the other as the hypothesis, then switching them and averaging the results).</S>
          <S ID="S-1565">As we can see, scores are fairly low overall, indicating large variation in the translations.</S>
          <S ID="S-1566">This is due to (a) the nature of conversational speech, and (b) the linguistic properties of the target language.</S>
          <S ID="S-1567">Conversational data contain a fair amount of colloquialisms, referential expressions, etc. that can be translated in a variety of ways.</S>
          <S ID="S-1568">Additionally, German as the target language permits many variations in word order that convey slight differences in emphasis, which is turn is dependent on the translators&#8217; interpretation of the source sentence.</S>
          <S ID="S-1569">German also has rich inflectional morphology that varies along with the choice of words and word order (e.g. verbal morphology depends on which subject is chosen).</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.2 SMT System Training Data</HEADER>
        <P>
          <S ID="S-1570">Since transcription and translation of multiparty spoken conversations is extremely timeconsuming and costly, it is unlikely that parallel conversational data will ever be produced on a sufficiently large scale for a variety of different meeting types, topics, and target languages.</S>
          <S ID="S-1571">In order to mimic this situation we trained an initial English- German SMT system on freely available out-ofdomain data resources.</S>
          <S ID="S-1572">We considered the following parallel corpora: news text (de-news 1 , 1.5M words), EU parliamentary proceedings (Europarl (<REF ID="R-10" RPTR="9">Koehn, 2005</REF>), 24M words) and EU legal documents (JRC Acquis 2 , 35M words), as well as two generic English-German machine-readable dictionaries 3 , 4 (672k and 140k entries, respectively).</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Translation Systems</HEADER>
      <P>
        <S ID="S-1574">We trained a standard statistical phrase-based English-German translation system from the resources described above using Moses (<REF ID="R-09" RPTR="8">Hoang and Koehn, 2008</REF>).</S>
        <S ID="S-1575">Individual language models were trained for each data source and were then linearly interpolated with weights optimized on the development set.</S>
        <S ID="S-1576">Similarly, individual phrase tables were trained and were then combined into a single table.</S>
        <S ID="S-1577">Binary indicator features were added for each phrase pair, indicating which data source it was extracted from.</S>
        <S ID="S-1578">Duplicated phrase pairs were merged into a single entry by averaging their scores (geometric mean) over all duplicated entries.</S>
        <S ID="S-1579">The weights for binary indicator features were optimized along with all other standard features on the development set.</S>
        <S ID="S-1580">Our previous experience showed that this method worked better than the two built-in features in Moses for handling multiple translation tables.</S>
        <S ID="S-1581">We found that the JRC corpus obtained very small weights; it was therefore omitted from further system development.</S>
        <S ID="S-1582">Table 2 reports results from six different systems: the first (System 1) is a system that only uses the parallel corpora but not the external dictionaries listed in Section 2.2.</S>
        <S ID="S-1583">System 2 additionally uses the external dictionaries.</S>
        <S ID="S-1584">All systems use two meetings (IB4002 and IS1008b) as a development set for tuning model parameters and five meetings for testing (IB4003- 5,IS1008c,TS3005a).</S>
        <S ID="S-1585">For comparison we also trained a version of the system where a small indomain data set (meetings ES2008a, IB4001, and IS1008a) was added to the training data (System 3).</S>
        <S ID="S-1586">Finally, we also compared our performance against Google Translate, which is a state-of-theart statistical MT system with unconstrained ac-</S>
      </P>
      <P>
        <S ID="S-1587">1 www.iccs.inf.ed.ac.uk/&#732;pkoehn/publications/de-news 2 http://wt.jrc.it/lt/Acquis/ 3 http://www.dict.cc 4 http://www-user.tu-chemnitz.de/&#732;fri/ding</S>
      </P>
      <P>
        <S ID="S-1588">System description Dev set Eval set OOV (%) Trans.</S>
        <S ID="S-1589">Scores OOV (%) Trans.</S>
        <S ID="S-1590">Scores EN DE BLEU PER EN DE BLEU PER</S>
      </P>
      <P>
        <S ID="S-1591">cess to the web as training data (System 6).</S>
        <S ID="S-1592">As expected, translation performance is fairly poor compared to the performance generally obtained on more structured genres.</S>
        <S ID="S-1593">The use of external dictionaries helps primarily in reducing PER scores while BLEU scores are only improved noticeably by adding in-domain data.</S>
        <S ID="S-1594">System 6 shows a more even performance across dev and eval sets than our trained system, which may reflect some degree of overtuning of our systems to the relatively small development set (about 7K words).</S>
        <S ID="S-1595">However, the PER scores of System 6 are significantly worse compared to our in-house systems.</S>
      </P>
      <P>
        <S ID="S-1596">In order to assess the impact of adding web data specifically collected to match our meeting corpus we queried a web portal 5 that searches a range of English-German bilingual web resources and returns parallel text in response to queries in either English or German.</S>
        <S ID="S-1597">As queries we used English phrases from our development and evaluation sets that (a) did not already have phrasal translations in our phrase tables, (b) had a minimum length of four words, and (c) occurred at least twice in the test data.</S>
        <S ID="S-1598">In those cases where the search engine returned results with an exact match on the English side, we word-aligned the resulting parallel text (about 600k words) by training the word alignment together with the news text corpus.</S>
        <S ID="S-1599">We then extracted new phrase pairs (about 3k) from the aligned data.</S>
        <S ID="S-1600">The phrasal scores assigned to</S>
      </P>
      <P>
        <S ID="S-1601">5 http://www.linguee.com</S>
      </P>
      <P>
        <S ID="S-1602">the new phrase pairs were set to 1; the lexical scores were computed from a word lexicon trained over both the baseline data resources and the parallel web data.</S>
        <S ID="S-1603">However, results (Row 5 in Table 2) show that performance hardly improved, indicating the difficulty in finding matching data sources for conversational speech.</S>
      </P>
      <P>
        <S ID="S-1604">Table 2 also shows the impact of different data resources on the percentages of unknown word types (OOV) for both the source and target languages.</S>
        <S ID="S-1605">The use of external dictionaries gave the largest reduction of OOV rates (System 1 vs. System 2 and System 3 vs. System 4), followed by the use of in-domain data (System 1 vs. System 3 and System 2 vs. System 4).</S>
        <S ID="S-1606">Since they were retrieved by multi-word query phrases, adding the web data did not lead to significant reduction on the OOV rates (System 4 vs. System 5).</S>
      </P>
      <P>
        <S ID="S-1607">Finally, we also explored a hierarchical phrasebased system as an alternative baseline system.</S>
        <S ID="S-1608">The system was trained using the Joshua toolkit (Li et al., 2009) with the same word alignments and language models as were used in the standard phrase-based baseline system (System 4).</S>
        <S ID="S-1609">After extracting the phrasal (rule) tables for each data source, they were combined into a single phrasal (rule) table using the same combination approach as for the basic phrase-based system.</S>
        <S ID="S-1610">However, the translation results (BLEU/PER of 24.0/46.6 (dev) and 20.8/47.6 (eval), respectively) did not show any improvement over the basic phrasebased system.</S>
      </P>
      <P>
        <S ID="S-1611">4 Analysis of Baseline Translations: Effect of Contextual Information</S>
      </P>
      <P>
        <S ID="S-1612">The output from System 5 was analyzed manually in order to assess the importance of modeling contextual information.</S>
        <S ID="S-1613">Our goal was not to determine how translation of meeting style data can be improved in general &#8211; better translations could certainly be generated by better syntactic modeling, addressing morphological variation in German, and generally improving phrasal coverage, in particular for sentences involving colloquial expressions.</S>
        <S ID="S-1614">However, these are fairly general problems of SMT that have been studied previously.</S>
        <S ID="S-1615">Instead, our goal was to determine the relative importance of modeling different contextual factors, such as discourse-level information or knowledge of the real-world environment, which have not been studied extensively.</S>
      </P>
      <P>
        <S ID="S-1616">We considered three types of contextual information: discourse coherence information (in particular anaphoric relations), knowledge of the topic or domain, and real-world/multimodal information.</S>
        <S ID="S-1617">Anaphoric relations affect the translation of referring expressions in cases where the source and target languages make different grammatical distinctions.</S>
        <S ID="S-1618">For example, German makes more morphological distinctions in noun phrases than English.</S>
        <S ID="S-1619">In order to correctly translate an expression like &#8220;the red one&#8221; the grammatical features of the target language expression for the referent need to be known.</S>
        <S ID="S-1620">This is only possible if a sufficiently large context is taken into account during translation and if the reference is resolved correctly.</S>
        <S ID="S-1621">Knowledge of the topic or domain is relevant for correctly translating content words and is closely related to the problem of word sense disambiguation.</S>
        <S ID="S-1622">In our current setup, topic/domain knowledge could be particularly helpful because in-domain training data is lacking and many word translations are obtained from generic dictionaries that do not assign probabilities to competing translations.</S>
        <S ID="S-1623">Finally, knowledge of the realworld environment, such as objects in the room, other speakers present, etc. determines translation choices.</S>
        <S ID="S-1624">If a speaker utters the expression &#8220;that one&#8221; while pointing to an object, the correct translation might depend on the grammatical features Error type % (dev) % (eval)</S>
      </P>
      <P>
        <S ID="S-1625">of the linguistic expression for that object; e.g. in German, the translation could be &#8220;die da&#8221;, &#8220;der da&#8221; or &#8220;das da&#8221;.</S>
        <S ID="S-1626">Since the participants in our meeting corpus use slides and supporting documents we expect to see some effect of such exophoric references to external objects.</S>
      </P>
      <P>
        <S ID="S-1627">In order to quantify the influence of contextual information we manually analyzed the 1-best output of System 5, identified those translation errors that require knowledge of the topic/domain, larger discourse, or external environment for their resolution, classified them into different categories, and computed their relative frequencies.</S>
        <S ID="S-1628">We then corrected these errors in the translation output to match at least one of the human references, in order to assess the maximum possible improvement in standard performance scores that could be obtained from contextual modeling.</S>
        <S ID="S-1629">The results are shown in Tables 3 and 4.</S>
        <S ID="S-1630">We observe that out of all errors that can be related to the lack of contextual knowledge, word sense confusions are by far the most frequent.</S>
        <S ID="S-1631">A smaller percentage of errors is caused by anaphoric expressions.</S>
        <S ID="S-1632">Contrary to our expectations, we did not find a strong impact of exophoric references; however, there is one crucial exception where real-world knowledge does play an important role.</S>
        <S ID="S-1633">This is the correct translation of the addressee you.</S>
        <S ID="S-1634">In English, this form is used for the second person singular, second person plural, and the generic interpretation (as in &#8220;one&#8221;, or &#8220;people&#8221;).</S>
        <S ID="S-1635">German has three distinct forms for these cases and, additionally, formal and informal versions of the second-person pronouns.</S>
        <S ID="S-1636">The required formal/informal pronouns can only be determined by prior knowledge of the relationships among the meeting participants.</S>
        <S ID="S-1637">However, the singular-plural-generic distinction can potentially be resolved by multimodal informa-</S>
      </P>
      <P>
        <S ID="S-1638">tion such as gaze, head turns, body movements, or hand gestures of the current speaker.</S>
        <S ID="S-1639">Since these errors affect mostly single words as opposed to larger phrases, the impact of the corrections on BLEU/PER scores is not large.</S>
        <S ID="S-1640">However, for practical applications (e.g. information extraction or human browsing of meeting translations) the correct translation of content words and referring expressions would be very important.</S>
        <S ID="S-1641">In the remainder of the paper we therefore describe initial experiments designed to address the most important source of contextual errors, viz.</S>
        <S ID="S-1642">word sense confusions.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Resolving Word Sense Disambiguation Errors</HEADER>
      <P>
        <S ID="S-1692">The problem of word sense disambiguation (WSD) in MT has received a fair amount of attention before.</S>
        <S ID="S-1693">Initial experiments designed at integrating a WSD component into an MT system (<REF ID="R-02" RPTR="3">Carpuat and Wu, 2005</REF>) did not meet with success; however, WSD was subsequently demonstrated to be successful in data-matched conditions (<REF ID="R-03" RPTR="4">Carpuat and Wu, 2007</REF>; <REF ID="R-04" RPTR="5">Chan et al., 2007</REF>).</S>
        <S ID="S-1694">The approach pursued by these latter approaches is to train a supervised word sense classifier on different phrase translation options provided by the phrase table of an initial baseline system (i.e. the task is to separate different phrase senses rather than word senses).</S>
        <S ID="S-1695">The input features to the classifier consist of word features obtained from the immediate context of the phrase in questions, i.e. from the same sentence or from the two or three preceding sentences.</S>
        <S ID="S-1696">The classifier is usually trained only for those phrases that are sufficiently frequent in the training data.</S>
      </P>
      <P>
        <S ID="S-1697">By contrast, our problem is quite different.</S>
        <S ID="S-1698">First, many of the translation errors caused by choosing the wrong word sense relate to words obtained from an external dictionary that do not occur in the parallel training data; there is also little in-domain training data available in general.</S>
        <S ID="S-1699">For these reasons, training a supervised WSD module is not an option without collecting additional data.</S>
        <S ID="S-1700">Second, the relevant information for resolving a word sense distinction is often not located in the immediately surrounding context but it is either at a more distant location in the discourse, or it is part of the participants&#8217; background knowledge.</S>
        <S ID="S-1701">For example, in many meetings the opening remarks refer to slides and an overhead projector.</S>
        <S ID="S-1702">It is likely that subsequent mentioning of slide later on during the conversation also refer to overhead slides (rather than e.g. slide in the sense of &#8220;playground equipment&#8221;), though the contextual features that could be used to identify this word sense are not located in the immediately preceding sentences.</S>
        <S ID="S-1703">Thus, in contrast to supervised, local phrase sense disambiguation employed in previous work, we propose to utilize unsupervised, global word sense disambiguation, in order to obtain better modeling of the topic and domain knowledge that is implicitly present in meeting conversations.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>5.1 Unsupervised Word Sense Disambiguation</HEADER>
        <P>
          <S ID="S-1643">Unsupervised WSD algorithms have been proposed previously (e.g. (<REF ID="R-16" RPTR="14">Navigli and Lapata, 2007</REF>; Cheng et al., 2009)).</S>
          <S ID="S-1644">The general idea is to exploit measures of word similarity or relatedness to jointly tag all words in a text with their correct sense.</S>
          <S ID="S-1645">We adopted the graph-based WSD method proposed in (<REF ID="R-17" RPTR="15">Sinha and Mihalcea, 2007</REF>), which represents all word senses in a text as nodes in an undirected graph G = (V, E).</S>
          <S ID="S-1646">Pairs of nodes are linked by edges weighted by scores indicating the similarity or relatedness of the words associated with the nodes.</S>
          <S ID="S-1647">Given such a graph, the likelihood of each node is derived by the PageRank algorithm (<REF ID="R-00" RPTR="0">Brin and Page, 1998</REF>), which measures the relative importance of each node to the entire graph by considering the amount of &#8220;votes&#8221; the node receives from its neighboring nodes.</S>
          <S ID="S-1648">The PageRank algorithm was originally designed for directed graphs, but can be easily extended to an undirected graph.</S>
          <S ID="S-1649">Let P R(v i ) denote the PageRank score of v i .</S>
          <S ID="S-1650">The PageRank algorithm itera-</S>
        </P>
        <P>
          <S ID="S-1651">tively updates this score as follows:</S>
        </P>
        <P>
          <S ID="S-1652">P R(v i ) = (1 &#8722; d) + d &#8721; w ij P R(v j ) &#8721;</S>
        </P>
        <P>
          <S ID="S-1653">(v i ,v j )&#8712;E k w kj</S>
        </P>
        <P>
          <S ID="S-1654">where w ij is the similarity weight of the undirected edge (v i , v j ) and d is a damping factor, which is typically set to 0.85 (<REF ID="R-00" RPTR="1">Brin and Page, 1998</REF>).</S>
          <S ID="S-1655">The outcome of the PageRank algorithm is numerical weighting of each node in the graph.</S>
          <S ID="S-1656">The sense with the highest score for each word identifies its most likely word sense.</S>
          <S ID="S-1657">For our purposes, we modified the procedure as follows.</S>
          <S ID="S-1658">Given a document (meeting transcription), we first identify all content words in the source document.</S>
          <S ID="S-1659">The graph is then built over all target-language translation candidates, i.e. each node represents a word translation.</S>
          <S ID="S-1660">Edges are then established between all pairs of nodes for which a word similarity measure can be obtained.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>5.2 Word Similarity Measures</HEADER>
        <P>
          <S ID="S-1661">We follow (Zesch et al., 2008a) in computing the semantic similarity of German words by exploiting the Wikipedia and Wiktionary databases.</S>
          <S ID="S-1662">We use the publicly available toolkits JWPL and JWKTL (Zesch et al., 2008b) to retrieve relevant articles in Wikipedia and entries in Wiktionary for each German word &#8211; these include the first paragraphs of Wikipedia articles entitled by the German word, the content of Wiktionary entries of the word itself as well as of closely related words (hypernyms, hyponyms, synonyms, etc.).</S>
          <S ID="S-1663">We then concatenate all retrieved material for each word to construct a pseudo-gloss.</S>
          <S ID="S-1664">We then lowercase and lemmatize the pseudo-glosses (using the lemmatizer available in the TextGrid package 6 ), exclude function words by applying a simple stop-word list, and compute a word similarity measure for a given pair of words by counting the number of common words in their glosses.</S>
        </P>
        <P>
          <S ID="S-1665">We need to point out that one drawback in this approach is the low coverage of German content words in the Wikipedia and Wiktionary databases.</S>
          <S ID="S-1666">Although the English edition contains millions of entries, the German edition of Wikipedia and Wiktionary is much smaller &#8211; the coverage of all content words in our task ranges between 53% and</S>
        </P>
        <P>
          <S ID="S-1667">6 http://www.textgrid.de/en/beta.html</S>
        </P>
        <P>
          <S ID="S-1668">56%, depending on the meeting, which leads to graphs with roughly 3K to 5K nodes and 8M to 13M edges.</S>
          <S ID="S-1669">Words that are not covered mostly include rare words, technical terms, and compound words.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>5.3 Experiments and Results</HEADER>
        <P>
          <S ID="S-1670">For each meeting, the derived PageRank scores were converted into a positive valued feature, referred to as the WSD feature, by normalization and exponentiation: { } P R(w g )</S>
        </P>
        <P>
          <S ID="S-1671">f W SD (w g |w e ) = exp &#8721;</S>
        </P>
        <P>
          <S ID="S-1672">w g &#8712;H(w e ) P R(w g)</S>
        </P>
        <P>
          <S ID="S-1673">where P R(w g ) is the PageRank score for the German word w g and H(w e ) is the set of all translation candidates for the English word w e .</S>
          <S ID="S-1674">Since they are not modeled in the graph-based method, multi-words phrases and words that are not found in the Wikipedia or Wiktionary databases will receive the default value 1 for their WSD feature.</S>
          <S ID="S-1675">The WSD feature was then integrated into the phrase table to perform translation.</S>
          <S ID="S-1676">The new system was optimized as before.</S>
        </P>
        <P>
          <S ID="S-1677">It should be emphasized that the standard measures of BLEU and PER give an inadequate impression of translation quality, in particular because of the large variation among the reference translations, as discussed in Section 4.</S>
          <S ID="S-1678">In many cases, better word sense disambiguation does not result in better BLEU scores (since higher gram matches are not affected) or even PER scores because although a feasible translation has been found it does not match any words in the reference translations.</S>
          <S ID="S-1679">The best way of evaluating the effect of WSD is to obtain human judgments &#8211; however, since translation hypotheses change with every change to the system, our original error annotation described in Section 4 cannot be re-used, and time and resource constraints prevented us from using manual evaluations at every step during system development.</S>
          <S ID="S-1680">In order to loosen the restrictions imposed by having only two reference translations, we utilized a German thesaurus 7 to automatically extend the content words in the references with synonyms.</S>
          <S ID="S-1681">This can be seen as an automated way of</S>
        </P>
        <P>
          <S ID="S-1682">7 http://www.openthesaurus.de</S>
        </P>
        <P>
          <S ID="S-1683">No WSD With WSD BLEU (%) PER XPER BLEU (%) PER XPER</S>
        </P>
        <P>
          <S ID="S-1684">approximating the larger space of feasible translations that could be obtained by producing additional human references.</S>
          <S ID="S-1685">Note that the thesaurus provided synonyms for only roughly 50% of all content words in the dev and eval set.</S>
          <S ID="S-1686">For each of them, on average three synonyms are found in the thesaurus.</S>
          <S ID="S-1687">We use these extended references to recompute the PER score as an indicator of correct word selection.</S>
          <S ID="S-1688">All results (BLEU, PER and extended PER (or XPER)) are shown in Table 5.</S>
          <S ID="S-1689">As expected, BLEU is not affected but WSD improves the PER and XPER slightly but consistently.</S>
          <S ID="S-1690">Note that this is despite the fact that only roughly half of all content words received disambiguation scores.</S>
        </P>
        <P>
          <S ID="S-1691">Finally, we provide a concrete example of translation improvements, with improved words highlighted: Source: on the balcony there&#8217;s that terrace there&#8217;s no place inside the building Translation, no WSD: auf dem balkon es ist das absatz es gibt keinen platz innerhalb des geb&#228;udes Translation, with WSD: auf dem balkon es ist das terrasse es gibt keinen platz geb&#228;udeintern References: auf dem balkon / auf dem balkon da gibt es die terrasse / da ist die terrasse es gibt keinen platz im geb&#228;ude / es gibt keinen platz innen im geb&#228;ude</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>6 Summary and Conclusions</HEADER>
      <P>
        <S ID="S-1704">We have presented a study on statistical translation of meeting data that makes the following contributions: to our knowledge it presents the first quantitative analysis of contextual factors in the statistical translation of multi-party spoken meetings.</S>
        <S ID="S-1705">This analysis showed that the largest impact could be obtained in the area of word sense disambiguation using topic and domain knowledge, followed by multimodal information to resolve addressees of you.</S>
        <S ID="S-1706">Contrary to our expectations, further knowledge of the real-world environment (such as objects in the room) did not show an effect on translation performance.</S>
        <S ID="S-1707">Second, it demonstrates the application of unsupervised, global WSD to SMT, whereas previous work has focused on supervised, local WSD.</S>
        <S ID="S-1708">Third, it explores definitions derived from collaborative Wiki sources (rather than WordNet or existing dictionaries) for use in machine translation.</S>
        <S ID="S-1709">We demonstrated small but consistent improvements even though word coverage was incomplete.</S>
        <S ID="S-1710">Future work will be directed at improving word coverage for the WSD algorithm, investigating alternative word similarity measures, and exploring the combination of global and local WSD techniques.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS>
    <P>
      <S ID="S-1711">This work was funded by the National Science Foundation under Grant IIS-0840461 and by a grant from the University of Washington&#8217;s Provost Office.</S>
      <S ID="S-1712">Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding organizations.</S>
    </P>
  </ACKNOWLEDGMENTS>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>S Brin</RAUTHOR>
      <REFTITLE>The Anatomy of a LargeScale Hypertextual Web Search Engine&#8221;.</REFTITLE>
      <DATE>1998</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>C Callison-Burch</RAUTHOR>
      <REFTITLE>Improved Statistical Machine Translation Using Paraphrases&#8221;.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>M Carpuat</RAUTHOR>
      <REFTITLE>Word sense disambiguation vs. statistical machine translation&#8221;.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>M Carpuat</RAUTHOR>
      <REFTITLE>Improving statistical machine translation using word sense disambiguation&#8221;.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>Y S Chan</RAUTHOR>
      <REFTITLE>Word sense disambiguation improves statistical machine translation&#8221;.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>P Chen</RAUTHOR>
      <REFTITLE>A fully unsupervised word sense disambiguation method using dependency knowledge&#8221;.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>E Gabrilovich</RAUTHOR>
      <REFTITLE>Computing semantic relatedness using Wikipedia-based explicit semantic analysis&#8221;.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>R Haque</RAUTHOR>
      <REFTITLE>Using supertags as source language context in SMT&#8221;.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>A S Hildebrandt</RAUTHOR>
      <REFTITLE>Adaptation of the Translation Model for Statistical Machine Translation using Information Retrieval&#8221;.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>H Hoang</RAUTHOR>
      <REFTITLE>Design of the Moses decoder for statistical machine translation&#8221;.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>P Koehn</RAUTHOR>
      <REFTITLE>Europarl: a parallel corpus for statistical machine translation&#8221;.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>V Kumar</RAUTHOR>
      <REFTITLE>Enriching spoken language translation with dialog acts&#8221;.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>Z Li</RAUTHOR>
      <REFTITLE>Joshua: An Open Source Toolkit for Parsing-based Machine Translation&#8221;.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>E Matusov</RAUTHOR>
      <REFTITLE>Statistical Machine Translation of Spontaneous Speech with Scarce Resources&#8221;.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="14">
      <RAUTHOR>H Mima</RAUTHOR>
      <REFTITLE>Improving Performance of Transfer-Driven Machine Translation with Extra-Linguistic Information from Context, Situation and Environment&#8221;.</REFTITLE>
      <DATE>1998</DATE>
    </REFERENCE>
    <REFERENCE ID="15">
      <RAUTHOR>D S Munteanu</RAUTHOR>
      <REFTITLE>Improving machine translation performance by exploiting nonparallel corpora&#8221;. Computational Linguistics.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="16">
      <RAUTHOR>R Navigli</RAUTHOR>
      <REFTITLE>Graph Connectivity Measures for Unsupervised Word Sense Disambiguation&#8221;,</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="17">
      <RAUTHOR>R Sinha</RAUTHOR>
      <REFTITLE>Unsupervised Graph-based Word Sense Disambiguation Using Measures of Word Semantic Similarity&#8221;,</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="18">
      <RAUTHOR>S St&#252;ker</RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="19">
      <RAUTHOR>&#8220;The ISL</RAUTHOR>
      <REFTITLE>English Speech Transcription System for European Parliament Speeches&#8221;.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="20">
      <RAUTHOR>A Waibel</RAUTHOR>
      <REFTITLE>Spoken Language Translation &#8211; Enabling cross-lingual human-human communication&#8221;.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="21">
      <RAUTHOR>W Wang</RAUTHOR>
      <REFTITLE>Automatic disfluency removal for improving spoken language translation&#8221;.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="22">
      <RAUTHOR>IEEE Signal Processing Magazine H Wu</RAUTHOR>
      <REFTITLE>Domain adaptation for statistical machine translation with domain dictionary and monolingual</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="23">
      <RAUTHOR>T Zesch</RAUTHOR>
      <REFTITLE>Christof M&#252;ler and Iryna Gurevych.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="24">
      <RAUTHOR>J Zheng</RAUTHOR>
      <REFTITLE>Development of SRI&#8217;s translation systems for broadcast news and broadcast conversations&#8221;.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
