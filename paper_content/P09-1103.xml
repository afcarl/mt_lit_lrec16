<document>
  <filename>P09-1103</filename>
  <authors/>
  <title>A non-contiguous Tree Sequence Alignment-based Model for Statistical Machine Translation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>The tree sequence based translation model allows the violation of syntactic boundaries in a rule to capture non-syntactic phrases, where a tree sequence is a contiguous sequence of subtrees. This paper goes further to present a translation model based on non-contiguous tree sequence alignment, where a non-contiguous tree sequence is a sequence of sub-trees and gaps. Compared with the contiguous tree sequencebased model, the proposed model can well handle non-contiguous phrases with any large gaps by means of non-contiguous tree sequence alignment. An algorithm targeting the noncontiguous constituent decoding is also proposed. Experimental results on the NIST MT-05 Chinese-English translation task show that the proposed model statistically significantly outperforms the baseline systems.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The tree sequence based translation model allows the violation of syntactic boundaries in a rule to capture non-syntactic phrases, where a tree sequence is a contiguous sequence of subtrees.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This paper goes further to present a translation model based on non-contiguous tree sequence alignment, where a non-contiguous tree sequence is a sequence of sub-trees and gaps.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Compared with the contiguous tree sequencebased model, the proposed model can well handle non-contiguous phrases with any large gaps by means of non-contiguous tree sequence alignment.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>An algorithm targeting the noncontiguous constituent decoding is also proposed.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Experimental results on the NIST MT-05 Chinese-English translation task show that the proposed model statistically significantly outperforms the baseline systems.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs 1 , and find them useless via real syntax-based translation systems. However, Wellington et al. (2006) statistically report that discontinuities are very useful for translational equivalence analysis using binary branching structures under word alignment and parse tree constraints. Bod (2007) also finds that discontinues phrasal rules make significant improvement in linguistically motivated STSG-based translation model. The above observations are conflicting to each other. In our opinion, the non-contiguous phrasal rules themselves may not play a trivial role, as reported in Zhang et al. (2008a). We believe that the effectiveness of non-contiguous phrasal rules highly depends on how to extract and utilize them.
To verify the above assumption, suppose there is only one tree pair in the training data with its alignment information illustrated as Fig. 1(a) 2 . A test sentence is given in Fig. 1(b): the source sentence with its syntactic tree structure as the upper tree and the expected target output with its syntactic structure as the lower tree. In the tree sequence alignment based model, in addition to the entire tree pair, it is capable to acquire the contiguous tree sequence pairs: TSP (1~4) 3 in Fig. 1. By means of the rules derived from these contiguous tree sequence pairs, it is easy to translate the contiguous phrase &#8220; /he &#161;&#163;&#162; /show up &#164; /&#8217;s&#8221;. As for the non-contiguous phrase &#8220;&#165; /at, ***, &#166;&#168;&#167; /time&#8221;, the only related rule is r 1 derived from TSP4 and the entire tree pair. However, the source side of r 1 does not match the source tree structure of the test sentence. Therefore, we can only partially translate the illustrated test sentence with this training sample.
1 A tree sequence pair in this context is a kind of translational
equivalence comprised of a pair of tree sequences. 2 We illustrate the rule extraction with an example from the
tree-to-tree translation model based on tree sequence alignment (Zhang et al, 2008a) without losing of generality to most syntactic tree based models. 3 We only list the contiguous tree sequence pairs with one
single sub-tree in both sides without losing of generality.

&#169; &#165; &#169; &#165;
VV AS PN
VP
NP
CP
IP
VV
DEC NN
&#161; &#166;&#168;&#167; &#162;&#164;&#163;
VV PN
VP
NP
CP
IP
VV
DEC NN
&#161; &#166;&#167; &#162;&#163;
TSP1: PN( )  PRP(he)
TSP2: VV( )  VP(VBZ(shows),RP(up))
TSP3: IP(PN( ),VV( ))  S((PRP(he), VP(VBZ(shows), RP(up))))
TSP4: CP(IP(PN( ),VV( )),DEC( ))  S((PRP(he), VP(VBZ(shows), RP(up))))
TSP5: VV( ), *** ,NN( )  WRB(when)
r 1 : VP(VV( ),AS( ),NP(CP[0],NN( )))  SBAR(WRB(when),S[0])
S
VP
S
VP
r 2 : VV( ), *** ,NN( )  WRB(when)
SBAR
(a)
SBAR
(b)
As discussed above, the problem lies in that the non-contiguous phrases derived from the contiguous tree sequence pairs demand greater reliance on the context. Consequently, when applying those rules to unseen data, it may suffer from the data sparseness problem. The expressiveness of the model also slacks due to their weak ability of generalization.
To address this issue, we propose a syntactic translation model based on non-contiguous tree sequence alignment. This model extracts the translation rules not only from the contiguous tree sequence pairs but also from the non-contiguous tree sequence pairs where a non-contiguous tree sequence is a sequence of sub-trees and gaps. With the help of the non-contiguous tree sequence, the proposed model can well capture the noncontiguous phrases in avoidance of the constraints of large applicability of context and enhance the non-contiguous constituent modeling. As for the above example, the proposed model enables the non-contiguous tree sequence pair indexed as TSP5 in Fig. 1 and is allowed to further derive r 2 from TSP5. By means of r 2 and the same processing to the contiguous phrase &#8220; /he &#161; &#162; /show up &#164; /&#8217;s&#8221; as the contiguous tree sequence based model, we can successfully translate the entire source sentence in Fig. 1(b).
We define a synchronous grammar, named Synchronous non-contiguous Tree Sequence Substitution Grammar (SncTSSG), extended from synchronous tree substitution grammar (STSG: Chiang, 2006) to illustrate our model. The proposed synchronous grammar is able to cover the previous proposed grammar based on tree (STSG, Eisner, 2003; Zhang et al, 2007) and tree sequence (STSSG, Zhang et al, 2008a) alignment. Besides, we modify the traditional parsing based decoding algorithm for syntax-based SMT to facilitate the non-contiguous constituent decoding for our model.
To the best of our knowledge, this is the first attempt to acquire the translation rules with rich syntactic structures from the non-contiguous Translational Equivalences (non-contiguous tree sequence pairs in this context).
The rest of this paper is organized as follows: Section 2 presents a formal definition of our model with detailed parameterization. Sections 3 and 4 elaborate the extraction of the non-contiguous tree sequence pairs and the decoding algorithm respectively. The experiments we conduct to assess the effectiveness of the proposed method are reported in Section 5. We finally conclude this work in Section 6.
2 Non-Contiguous Tree sequence Alignment-based Model
In this section, we give a formal definition of SncTSSG and accordingly we propose the alignment based translation model. The details of probabilistic parameterization are elaborated based on the log-linear framework.
x and are source and target nonterminal alphabets (linguistically syntactic tags, i.e. NP, VP) respectively; as well as the non-terminal to denote a gap,
r
x R is a production rule set consisting of rules derived from corresponding contiguous or non-contiguous tree sequence pairs, where a rule is a pair of contiguous or noncontiguous tree sequence with alignment relation between leaf nodes across the tree sequence pair.
A non-contiguous tree sequence translation rule R can be further defined as a triple
, where:
can represent any syntactic or nonsyntactic tree sequences, and
x
x
x
is a non-contiguous source tree
sequence, covering the span set in , where which means each subspan has nonzero width and which means there is a non-zero gap between each pair of consecutive intervals. A gap of interval [ ] is denoted as , and
is a non-contiguous target tree
sequence, covering the span set in , where which means each subspan has non-zero width and which means there is a non-zero gap between each pair of consecutive intervals. A gap of interval [ ] is denoted as , and
where and
In SncTSSG, the leaf nodes in a non-contiguous tree sequence rule can be either non-terminal symbols (grammar tags) or terminal symbols (lexical words) and the non-terminal symbols with the same index which are subsumed simultaneously are not required to be contiguous. Fig. 4 shows two examples of non-contiguous tree sequence rules (&#8220;non-contiguous rule&#8221; for short in the following context) derived from the noncontiguous tree sequence pair (in Fig. 3) which is extracted from the bilingual tree pair in Fig. 2. Between them, ncTSr1 is a tree rule with internal nodes non-contiguously subsumed from a contiguous tree sequence pair (dashed in Fig. 2) while ncTSr2 is a non-contiguous rule with a contiguous source side and a non-contiguous target side. Obviously, the non-contiguous tree sequence rule ncTSr2 is more flexible by neglecting the context among the gaps of the tree sequence pair while capturing all aligned counterparts with the corresponding syntactic structure information. We ,
expect these properties can well address the issues of non-contiguous phrase modeling.
2.2 SncTSSG based Translation Model
Given the source and target sentence and , as well as the corresponding parse trees  and , our approach directly approximates the posterior probability based on the log-linear framework:
&#8225;&#353;&#8217;
In this model, the feature function h m is loglinearly combined by the corresponding parameter
(Och and Ney, 2002). The following features are utilized in our model:
1) The bi-phrasal translation probabilities
2) The bi-lexical translation probabilities
3) The target language model
4) The # of words in the target sentence
5) The # of rules utilized
6) The average tree depth in the source side of the rules adopted
7) The # of non-contiguous rules utilized
8) The # of reordering times caused by the utilization of the non-contiguous rules
Feature 1~6 can be applied to either STSSG or SncTSSG based models, while the last two targets SncTSSG only.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based.</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation.</text>
              <doc_id>6</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases.</text>
              <doc_id>7</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling.</text>
              <doc_id>8</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides.</text>
              <doc_id>9</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007).</text>
              <doc_id>10</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Among them, Zhang et al. (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs 1 , and find them useless via real syntax-based translation systems.</text>
              <doc_id>11</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>However, Wellington et al. (2006) statistically report that discontinuities are very useful for translational equivalence analysis using binary branching structures under word alignment and parse tree constraints.</text>
              <doc_id>12</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Bod (2007) also finds that discontinues phrasal rules make significant improvement in linguistically motivated STSG-based translation model.</text>
              <doc_id>13</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>The above observations are conflicting to each other.</text>
              <doc_id>14</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>In our opinion, the non-contiguous phrasal rules themselves may not play a trivial role, as reported in Zhang et al. (2008a).</text>
              <doc_id>15</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>We believe that the effectiveness of non-contiguous phrasal rules highly depends on how to extract and utilize them.</text>
              <doc_id>16</doc_id>
              <sec_id>11</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To verify the above assumption, suppose there is only one tree pair in the training data with its alignment information illustrated as Fig.</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1(a) 2 .</text>
              <doc_id>18</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A test sentence is given in Fig.</text>
              <doc_id>19</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>1(b): the source sentence with its syntactic tree structure as the upper tree and the expected target output with its syntactic structure as the lower tree.</text>
              <doc_id>20</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In the tree sequence alignment based model, in addition to the entire tree pair, it is capable to acquire the contiguous tree sequence pairs: TSP (1~4) 3 in Fig.</text>
              <doc_id>21</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>1.</text>
              <doc_id>22</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>By means of the rules derived from these contiguous tree sequence pairs, it is easy to translate the contiguous phrase &#8220; /he &#161;&#163;&#162; /show up &#164; /&#8217;s&#8221;.</text>
              <doc_id>23</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>As for the non-contiguous phrase &#8220;&#165; /at, ***, &#166;&#168;&#167; /time&#8221;, the only related rule is r 1 derived from TSP4 and the entire tree pair.</text>
              <doc_id>24</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>However, the source side of r 1 does not match the source tree structure of the test sentence.</text>
              <doc_id>25</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, we can only partially translate the illustrated test sentence with this training sample.</text>
              <doc_id>26</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 A tree sequence pair in this context is a kind of translational</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>equivalence comprised of a pair of tree sequences.</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2 We illustrate the rule extraction with an example from the</text>
              <doc_id>29</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tree-to-tree translation model based on tree sequence alignment (Zhang et al, 2008a) without losing of generality to most syntactic tree based models.</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>3 We only list the contiguous tree sequence pairs with one</text>
              <doc_id>31</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>single sub-tree in both sides without losing of generality.</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>33</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#169; &#165; &#169; &#165;</text>
              <doc_id>34</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>VV AS PN</text>
              <doc_id>35</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>VP</text>
              <doc_id>36</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>NP</text>
              <doc_id>37</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>CP</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>IP</text>
              <doc_id>39</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>VV</text>
              <doc_id>40</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>DEC NN</text>
              <doc_id>41</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#161; &#166;&#168;&#167; &#162;&#164;&#163;</text>
              <doc_id>42</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>VV PN</text>
              <doc_id>43</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>VP</text>
              <doc_id>44</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>NP</text>
              <doc_id>45</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>CP</text>
              <doc_id>46</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>IP</text>
              <doc_id>47</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>VV</text>
              <doc_id>48</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>DEC NN</text>
              <doc_id>49</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#161; &#166;&#167; &#162;&#163;</text>
              <doc_id>50</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>TSP1: PN( )  PRP(he)</text>
              <doc_id>51</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>TSP2: VV( )  VP(VBZ(shows),RP(up))</text>
              <doc_id>52</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>TSP3: IP(PN( ),VV( ))  S((PRP(he), VP(VBZ(shows), RP(up))))</text>
              <doc_id>53</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>TSP4: CP(IP(PN( ),VV( )),DEC( ))  S((PRP(he), VP(VBZ(shows), RP(up))))</text>
              <doc_id>54</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>TSP5: VV( ), *** ,NN( )  WRB(when)</text>
              <doc_id>55</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>r 1 : VP(VV( ),AS( ),NP(CP[0],NN( )))  SBAR(WRB(when),S[0])</text>
              <doc_id>56</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S</text>
              <doc_id>57</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>VP</text>
              <doc_id>58</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S</text>
              <doc_id>59</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>VP</text>
              <doc_id>60</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>r 2 : VV( ), *** ,NN( )  WRB(when)</text>
              <doc_id>61</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>SBAR</text>
              <doc_id>62</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(a)</text>
              <doc_id>63</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>SBAR</text>
              <doc_id>64</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(b)</text>
              <doc_id>65</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>As discussed above, the problem lies in that the non-contiguous phrases derived from the contiguous tree sequence pairs demand greater reliance on the context.</text>
              <doc_id>66</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Consequently, when applying those rules to unseen data, it may suffer from the data sparseness problem.</text>
              <doc_id>67</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The expressiveness of the model also slacks due to their weak ability of generalization.</text>
              <doc_id>68</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To address this issue, we propose a syntactic translation model based on non-contiguous tree sequence alignment.</text>
              <doc_id>69</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This model extracts the translation rules not only from the contiguous tree sequence pairs but also from the non-contiguous tree sequence pairs where a non-contiguous tree sequence is a sequence of sub-trees and gaps.</text>
              <doc_id>70</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>With the help of the non-contiguous tree sequence, the proposed model can well capture the noncontiguous phrases in avoidance of the constraints of large applicability of context and enhance the non-contiguous constituent modeling.</text>
              <doc_id>71</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As for the above example, the proposed model enables the non-contiguous tree sequence pair indexed as TSP5 in Fig.</text>
              <doc_id>72</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>1 and is allowed to further derive r 2 from TSP5.</text>
              <doc_id>73</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>By means of r 2 and the same processing to the contiguous phrase &#8220; /he &#161; &#162; /show up &#164; /&#8217;s&#8221; as the contiguous tree sequence based model, we can successfully translate the entire source sentence in Fig.</text>
              <doc_id>74</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>1(b).</text>
              <doc_id>75</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We define a synchronous grammar, named Synchronous non-contiguous Tree Sequence Substitution Grammar (SncTSSG), extended from synchronous tree substitution grammar (STSG: Chiang, 2006) to illustrate our model.</text>
              <doc_id>76</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The proposed synchronous grammar is able to cover the previous proposed grammar based on tree (STSG, Eisner, 2003; Zhang et al, 2007) and tree sequence (STSSG, Zhang et al, 2008a) alignment.</text>
              <doc_id>77</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Besides, we modify the traditional parsing based decoding algorithm for syntax-based SMT to facilitate the non-contiguous constituent decoding for our model.</text>
              <doc_id>78</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To the best of our knowledge, this is the first attempt to acquire the translation rules with rich syntactic structures from the non-contiguous Translational Equivalences (non-contiguous tree sequence pairs in this context).</text>
              <doc_id>79</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The rest of this paper is organized as follows: Section 2 presents a formal definition of our model with detailed parameterization.</text>
              <doc_id>80</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Sections 3 and 4 elaborate the extraction of the non-contiguous tree sequence pairs and the decoding algorithm respectively.</text>
              <doc_id>81</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The experiments we conduct to assess the effectiveness of the proposed method are reported in Section 5.</text>
              <doc_id>82</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We finally conclude this work in Section 6.</text>
              <doc_id>83</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2 Non-Contiguous Tree sequence Alignment-based Model</text>
              <doc_id>84</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this section, we give a formal definition of SncTSSG and accordingly we propose the alignment based translation model.</text>
              <doc_id>85</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The details of probabilistic parameterization are elaborated based on the log-linear framework.</text>
              <doc_id>86</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>x and are source and target nonterminal alphabets (linguistically syntactic tags, i.e. NP, VP) respectively; as well as the non-terminal to denote a gap,</text>
              <doc_id>87</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>r</text>
              <doc_id>88</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>x R is a production rule set consisting of rules derived from corresponding contiguous or non-contiguous tree sequence pairs, where a rule is a pair of contiguous or noncontiguous tree sequence with alignment relation between leaf nodes across the tree sequence pair.</text>
              <doc_id>89</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A non-contiguous tree sequence translation rule R can be further defined as a triple</text>
              <doc_id>90</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>, where:</text>
              <doc_id>91</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>can represent any syntactic or nonsyntactic tree sequences, and</text>
              <doc_id>92</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>x</text>
              <doc_id>93</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>x</text>
              <doc_id>94</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>x</text>
              <doc_id>95</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>is a non-contiguous source tree</text>
              <doc_id>96</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>sequence, covering the span set in , where which means each subspan has nonzero width and which means there is a non-zero gap between each pair of consecutive intervals.</text>
              <doc_id>97</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A gap of interval [ ] is denoted as , and</text>
              <doc_id>98</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>is a non-contiguous target tree</text>
              <doc_id>99</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>sequence, covering the span set in , where which means each subspan has non-zero width and which means there is a non-zero gap between each pair of consecutive intervals.</text>
              <doc_id>100</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A gap of interval [ ] is denoted as , and</text>
              <doc_id>101</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where and</text>
              <doc_id>102</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In SncTSSG, the leaf nodes in a non-contiguous tree sequence rule can be either non-terminal symbols (grammar tags) or terminal symbols (lexical words) and the non-terminal symbols with the same index which are subsumed simultaneously are not required to be contiguous.</text>
              <doc_id>103</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Fig.</text>
              <doc_id>104</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>4 shows two examples of non-contiguous tree sequence rules (&#8220;non-contiguous rule&#8221; for short in the following context) derived from the noncontiguous tree sequence pair (in Fig.</text>
              <doc_id>105</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>3) which is extracted from the bilingual tree pair in Fig.</text>
              <doc_id>106</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>2.</text>
              <doc_id>107</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Between them, ncTSr1 is a tree rule with internal nodes non-contiguously subsumed from a contiguous tree sequence pair (dashed in Fig.</text>
              <doc_id>108</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>2) while ncTSr2 is a non-contiguous rule with a contiguous source side and a non-contiguous target side.</text>
              <doc_id>109</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Obviously, the non-contiguous tree sequence rule ncTSr2 is more flexible by neglecting the context among the gaps of the tree sequence pair while capturing all aligned counterparts with the corresponding syntactic structure information.</text>
              <doc_id>110</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>We ,</text>
              <doc_id>111</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>expect these properties can well address the issues of non-contiguous phrase modeling.</text>
              <doc_id>112</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.2 SncTSSG based Translation Model</text>
              <doc_id>113</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Given the source and target sentence and , as well as the corresponding parse trees  and , our approach directly approximates the posterior probability based on the log-linear framework:</text>
              <doc_id>114</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8225;&#353;&#8217;</text>
              <doc_id>115</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this model, the feature function h m is loglinearly combined by the corresponding parameter</text>
              <doc_id>116</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(Och and Ney, 2002).</text>
              <doc_id>117</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The following features are utilized in our model:</text>
              <doc_id>118</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1) The bi-phrasal translation probabilities</text>
              <doc_id>119</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2) The bi-lexical translation probabilities</text>
              <doc_id>120</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3) The target language model</text>
              <doc_id>121</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>4) The # of words in the target sentence</text>
              <doc_id>122</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>5) The # of rules utilized</text>
              <doc_id>123</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>6) The average tree depth in the source side of the rules adopted</text>
              <doc_id>124</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>7) The # of non-contiguous rules utilized</text>
              <doc_id>125</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>8) The # of reordering times caused by the utilization of the non-contiguous rules</text>
              <doc_id>126</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Feature 1~6 can be applied to either STSSG or SncTSSG based models, while the last two targets SncTSSG only.</text>
              <doc_id>127</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>3 Tree Sequence Pair Extraction</title>
        <text>Algorithm 1: Tree Sequence Pair Extraction Input: source tree and target tree Output: the set of tree sequence pairs Data structure: p[j 1 , j 2 ] to store tree sequence pairs covering source span[j 1 , j 2 ] 1: foreach source span [j 1 , j 2 ], do 2: find a target span [i 1 ,i 2 ] with minimal length covering all the target words aligned to [j 1 , j 2 ] 3: if all the target words in [i 1 ,i 2 ] are aligned with
source words only in [j 1 , j 2 ], then 4: Pair each source tree sequence covering [j 1 , j 2 ] with those in target covering [i 1 ,i 2 ] as a contiguous tree sequence pair 5: Insert them into p[j 1 , j 2 ] 6: else 7: create sub-span set s([i 1 ,i 2 ]) to cover all the target words aligned to [j 1 , j 2 ] 8: Pair each source tree sequence covering [j 1 , j 2 ] with each target tree sequence covering s([i 1 ,i 2 ]) as a non-contiguous tree sequence pair 9: Insert them into p[j 1 , j 2 ] 10: end if 11:end do 12: foreach target span [i 1 ,i 2 ], do 13: find a source span [j 1 , j 2 ] with minimal length covering all the source words aligned to [i 1 ,i 2 ] 14: if any source word in [j 1 , j 2 ] is aligned with target words outside [i 1 ,i 2 ], then 15: create sub-span set s([j 1 , j 2 ]) to cover all the source words aligned to [i 1 ,i 2 ] 16: Pair each source tree sequence covering s([j 1 , j 2 ]) with each target tree sequence covering [i 1 ,i 2 ] as a non-contiguous tree sequence pair 17: Insert them into p[j 1 , j 2 ] 18: end if 19: end do
In training, other than the contiguous tree sequence pairs, we extract the non-contiguous ones as well. Nevertheless, compared with the contiguous tree sequence pairs, the non-contiguous ones suffer more from the tree sequence pair redundancy problem that one non-contiguous tree sequence pair can be comprised of two or more unrelated and nonadjacent contiguous ones. To model the contiguous phrases, this problem is actually trivial, since the contiguous phrases stay adjacently and share the related syntactic constraints; however, as for non-contiguous phrase modeling, the cohesion of syntactically and semantically unrelated tree sequence pairs is more likely to generate noisy rules which do not benefit at all. In order to minimize the number of redundant tree sequence pairs, we limit the # of gaps of non-contiguous tree sequence pairs to be 0 in either source or target side. In other words, we only allow one side to be noncontiguous (either source or target side) to partially reserve its syntactic and semantic cohesion 4 . We further design a two-phase algorithm to extract the tree sequence pairs as described in Algorithm 1.
For the first phase (line 1-11), we extract the contiguous tree sequence pairs (line 3-5) and the non-contiguous ones with contiguous tree sequence in the source side (line 6-9). In the second phase (line 12-19), the ones with contiguous tree sequence in the target side and non-contiguous tree sequence on the source side are extracted.
4 Wellington et al. (2006) also reports that allowing gaps in
one side only is enough to eliminate the hierarchical alignment failure with word alignment and one side parse tree constraints. This is a particular case of our definition of non-contiguous tree sequence pair since a non-contiguous tree sequence can be considered to overcome the structural constraint by neglecting the structural information in the gaps.
The extracted tree sequence pairs are then utilized to derive the translation rules. In fact, both the contiguous and non-contiguous tree sequence pairs themselves are applicable translation rules; we denote these rules as Initial rules. By means of the Initial rules, we derive the Abstract rules similarly as in Zhang et al. (2008a).
Additionally, we develop a few constraints to limit the number of Abstract rules. The depth of a tree in a rule is no greater than h. The number of non-terminals as leaf nodes is no greater than c. The tree number is no greater than d. Besides, the number of lexical words at leaf nodes in an Initial rule is no greater than l. The maximal number of gaps for a non-contiguous rule is no greater than .</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Algorithm 1: Tree Sequence Pair Extraction Input: source tree and target tree Output: the set of tree sequence pairs Data structure: p[j 1 , j 2 ] to store tree sequence pairs covering source span[j 1 , j 2 ] 1: foreach source span [j 1 , j 2 ], do 2: find a target span [i 1 ,i 2 ] with minimal length covering all the target words aligned to [j 1 , j 2 ] 3: if all the target words in [i 1 ,i 2 ] are aligned with</text>
              <doc_id>128</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>source words only in [j 1 , j 2 ], then 4: Pair each source tree sequence covering [j 1 , j 2 ] with those in target covering [i 1 ,i 2 ] as a contiguous tree sequence pair 5: Insert them into p[j 1 , j 2 ] 6: else 7: create sub-span set s([i 1 ,i 2 ]) to cover all the target words aligned to [j 1 , j 2 ] 8: Pair each source tree sequence covering [j 1 , j 2 ] with each target tree sequence covering s([i 1 ,i 2 ]) as a non-contiguous tree sequence pair 9: Insert them into p[j 1 , j 2 ] 10: end if 11:end do 12: foreach target span [i 1 ,i 2 ], do 13: find a source span [j 1 , j 2 ] with minimal length covering all the source words aligned to [i 1 ,i 2 ] 14: if any source word in [j 1 , j 2 ] is aligned with target words outside [i 1 ,i 2 ], then 15: create sub-span set s([j 1 , j 2 ]) to cover all the source words aligned to [i 1 ,i 2 ] 16: Pair each source tree sequence covering s([j 1 , j 2 ]) with each target tree sequence covering [i 1 ,i 2 ] as a non-contiguous tree sequence pair 17: Insert them into p[j 1 , j 2 ] 18: end if 19: end do</text>
              <doc_id>129</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In training, other than the contiguous tree sequence pairs, we extract the non-contiguous ones as well.</text>
              <doc_id>130</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Nevertheless, compared with the contiguous tree sequence pairs, the non-contiguous ones suffer more from the tree sequence pair redundancy problem that one non-contiguous tree sequence pair can be comprised of two or more unrelated and nonadjacent contiguous ones.</text>
              <doc_id>131</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>To model the contiguous phrases, this problem is actually trivial, since the contiguous phrases stay adjacently and share the related syntactic constraints; however, as for non-contiguous phrase modeling, the cohesion of syntactically and semantically unrelated tree sequence pairs is more likely to generate noisy rules which do not benefit at all.</text>
              <doc_id>132</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In order to minimize the number of redundant tree sequence pairs, we limit the # of gaps of non-contiguous tree sequence pairs to be 0 in either source or target side.</text>
              <doc_id>133</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In other words, we only allow one side to be noncontiguous (either source or target side) to partially reserve its syntactic and semantic cohesion 4 .</text>
              <doc_id>134</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We further design a two-phase algorithm to extract the tree sequence pairs as described in Algorithm 1.</text>
              <doc_id>135</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For the first phase (line 1-11), we extract the contiguous tree sequence pairs (line 3-5) and the non-contiguous ones with contiguous tree sequence in the source side (line 6-9).</text>
              <doc_id>136</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In the second phase (line 12-19), the ones with contiguous tree sequence in the target side and non-contiguous tree sequence on the source side are extracted.</text>
              <doc_id>137</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>4 Wellington et al. (2006) also reports that allowing gaps in</text>
              <doc_id>138</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>one side only is enough to eliminate the hierarchical alignment failure with word alignment and one side parse tree constraints.</text>
              <doc_id>139</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This is a particular case of our definition of non-contiguous tree sequence pair since a non-contiguous tree sequence can be considered to overcome the structural constraint by neglecting the structural information in the gaps.</text>
              <doc_id>140</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The extracted tree sequence pairs are then utilized to derive the translation rules.</text>
              <doc_id>141</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In fact, both the contiguous and non-contiguous tree sequence pairs themselves are applicable translation rules; we denote these rules as Initial rules.</text>
              <doc_id>142</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>By means of the Initial rules, we derive the Abstract rules similarly as in Zhang et al. (2008a).</text>
              <doc_id>143</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Additionally, we develop a few constraints to limit the number of Abstract rules.</text>
              <doc_id>144</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The depth of a tree in a rule is no greater than h.</text>
              <doc_id>145</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The number of non-terminals as leaf nodes is no greater than c.</text>
              <doc_id>146</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The tree number is no greater than d. Besides, the number of lexical words at leaf nodes in an Initial rule is no greater than l.</text>
              <doc_id>147</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The maximal number of gaps for a non-contiguous rule is no greater than .</text>
              <doc_id>148</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>4 The Pisces decoder</title>
        <text>We implement our decoder Pisces by simulating the span based CYK parser constrained by the rules of SncTSSG. The decoder translates each span iteratively in a bottom up manner which guarantees that when translating a source span, any of its sub-spans is already translated.
For each source span [j 1 , j 2 ], we perform a threephase decoding process. In the first phase, the source side contiguous translation rules are utilized as described in Algorithm 2. When translating using a source side contiguous rule, the target tree sequence of the rule whether contiguous or noncontiguous is directly considered as a candidate translation for this span (line 3), if the rule is an Initial rule; otherwise, the non-terminal leaf nodes are replaced with the corresponding sub-spans&#8217; translations (line 5).
In the second phase, the source side noncontiguous rules 5 for [j 1 , j 2 ] are processed. As for
5 A source side non-contiguous translation rules which cover a
list of n non-contiguous spans s([ , ], i=1,&#8230;,n) is considered to cover the source span [j 1 , j 2 ] if and only if = j 1 and
= j 2 .
Algorithm 2: Contiguous rule processing Data structure: h[j 1 , j 2 ]to store translations covering source span[j 1 , j 2 ] 1: foreach rule r contiguous in source span [j 1 , j 2 ], do 2: if r is an Initial rule, then 3: insert r into h[j 1 , j 2 ] 4: else //Abstract rule 5: generate translations by replacing the nonterminal leaf nodes of r with their corresponding spans&#8217; translation 6: insert the new translation into h[j 1 , j 2 ] 7: end if 8: end do
the ones with non-terminal leaf nodes, the replacement with corresponding spans&#8217; translations is initially performed in the same way as with the contiguous rules in the first phase. After that, an operation specified for the source side noncontiguous rules named &#8220;Source gap insertion&#8221; is performed. As illustrated in Fig. 5, to use the noncontiguous rule r 1 , which covers the source span set ([0,0], [4,4]), the target portion &#8220;IN(in)&#8221; is first attained, then the translations to the gap span [1,3] is acquired from the previous steps and is inserted either to the right or to the left of &#8220;IN(in)&#8221;. The insertion is rather cohesion based but leaves a gap
&lt;***&gt; for further &#8220;Target tree sequence reordering&#8221;
in the next phase if necessary.
In the third phase, we carry out the other noncontiguous rule specific operation named &#8220;Target tree sequence reordering&#8221;. Algorithm 3 gives an overview of this operation. For each source span, we first binarize the span into the left one and the right one. The translation hypothesis for this span is generated by firstly inserting the candidate translations of the right span to each gap in the ones of the left span respectively (line 2-9) and then repeating in the alternative direction (line10-17). The gaps for the insertion of the tree sequences in the target side are generated from either the inherit-
ance of the target side non-contiguous tree sequence pairs or the production of the previous operations of &#8220;Source gap insertion&#8221;. Therefore, the insertion for target gaps helps search for a better order of the non-contiguous constituents in the target side. On the other hand, the non-contiguous tree sequences with rich syntactic information are reordered, nevertheless, without much consideration of the constraints of the syntactic structure. Consequently, this distortional operation, like phrase-based models, is much more flexible in the order of the target constituents than the traditional syntax-based models which are limited by the syntactic structure. As a result, &#8220;Target tree sequence reordering&#8221; enhances the reordering ability of the model.
To speed up the decoder, we use several thresholds to limit the searching space for each span. The maximal number of the rules in a source span is no greater than . The maximal number of translation candidates for a source span is no greater than . On the other hand, to simplify the computation of language model, we only compute for source side contiguous translational hypothesis, while neglecting gaps in the target side if any.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We implement our decoder Pisces by simulating the span based CYK parser constrained by the rules of SncTSSG.</text>
              <doc_id>149</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The decoder translates each span iteratively in a bottom up manner which guarantees that when translating a source span, any of its sub-spans is already translated.</text>
              <doc_id>150</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For each source span [j 1 , j 2 ], we perform a threephase decoding process.</text>
              <doc_id>151</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In the first phase, the source side contiguous translation rules are utilized as described in Algorithm 2.</text>
              <doc_id>152</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>When translating using a source side contiguous rule, the target tree sequence of the rule whether contiguous or noncontiguous is directly considered as a candidate translation for this span (line 3), if the rule is an Initial rule; otherwise, the non-terminal leaf nodes are replaced with the corresponding sub-spans&#8217; translations (line 5).</text>
              <doc_id>153</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the second phase, the source side noncontiguous rules 5 for [j 1 , j 2 ] are processed.</text>
              <doc_id>154</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As for</text>
              <doc_id>155</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>5 A source side non-contiguous translation rules which cover a</text>
              <doc_id>156</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>list of n non-contiguous spans s([ , ], i=1,&#8230;,n) is considered to cover the source span [j 1 , j 2 ] if and only if = j 1 and</text>
              <doc_id>157</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>= j 2 .</text>
              <doc_id>158</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Algorithm 2: Contiguous rule processing Data structure: h[j 1 , j 2 ]to store translations covering source span[j 1 , j 2 ] 1: foreach rule r contiguous in source span [j 1 , j 2 ], do 2: if r is an Initial rule, then 3: insert r into h[j 1 , j 2 ] 4: else //Abstract rule 5: generate translations by replacing the nonterminal leaf nodes of r with their corresponding spans&#8217; translation 6: insert the new translation into h[j 1 , j 2 ] 7: end if 8: end do</text>
              <doc_id>159</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the ones with non-terminal leaf nodes, the replacement with corresponding spans&#8217; translations is initially performed in the same way as with the contiguous rules in the first phase.</text>
              <doc_id>160</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>After that, an operation specified for the source side noncontiguous rules named &#8220;Source gap insertion&#8221; is performed.</text>
              <doc_id>161</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>As illustrated in Fig.</text>
              <doc_id>162</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>5, to use the noncontiguous rule r 1 , which covers the source span set ([0,0], [4,4]), the target portion &#8220;IN(in)&#8221; is first attained, then the translations to the gap span [1,3] is acquired from the previous steps and is inserted either to the right or to the left of &#8220;IN(in)&#8221;.</text>
              <doc_id>163</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The insertion is rather cohesion based but leaves a gap</text>
              <doc_id>164</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&lt;***&gt; for further &#8220;Target tree sequence reordering&#8221;</text>
              <doc_id>165</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>in the next phase if necessary.</text>
              <doc_id>166</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the third phase, we carry out the other noncontiguous rule specific operation named &#8220;Target tree sequence reordering&#8221;.</text>
              <doc_id>167</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Algorithm 3 gives an overview of this operation.</text>
              <doc_id>168</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For each source span, we first binarize the span into the left one and the right one.</text>
              <doc_id>169</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The translation hypothesis for this span is generated by firstly inserting the candidate translations of the right span to each gap in the ones of the left span respectively (line 2-9) and then repeating in the alternative direction (line10-17).</text>
              <doc_id>170</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The gaps for the insertion of the tree sequences in the target side are generated from either the inherit-</text>
              <doc_id>171</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>ance of the target side non-contiguous tree sequence pairs or the production of the previous operations of &#8220;Source gap insertion&#8221;.</text>
              <doc_id>172</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, the insertion for target gaps helps search for a better order of the non-contiguous constituents in the target side.</text>
              <doc_id>173</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>On the other hand, the non-contiguous tree sequences with rich syntactic information are reordered, nevertheless, without much consideration of the constraints of the syntactic structure.</text>
              <doc_id>174</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Consequently, this distortional operation, like phrase-based models, is much more flexible in the order of the target constituents than the traditional syntax-based models which are limited by the syntactic structure.</text>
              <doc_id>175</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>As a result, &#8220;Target tree sequence reordering&#8221; enhances the reordering ability of the model.</text>
              <doc_id>176</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To speed up the decoder, we use several thresholds to limit the searching space for each span.</text>
              <doc_id>177</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The maximal number of the rules in a source span is no greater than .</text>
              <doc_id>178</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The maximal number of translation candidates for a source span is no greater than .</text>
              <doc_id>179</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>On the other hand, to simplify the computation of language model, we only compute for source side contiguous translational hypothesis, while neglecting gaps in the target side if any.</text>
              <doc_id>180</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>5 Experiments</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>181</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Experimental Settings</title>
            <text>In the experiments, we train the translation model on FBIS corpus (7.2M (Chinese) + 9.2M (English) words) and train a 4-gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002). We use these sentences with less than 50 characters from the NIST MT-2002 test set as the development set and the NIST MT-2005 test set as our test set. We use the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test set. The evaluation metric is casesensitive BLEU-4 (Papineni et al., 2002). We base on the m-to-n word alignments dumped by GI- ZA++ to extract the tree sequence pairs. For the MER training, we modify Koehn&#8217;s version (Koehn, 2004). We use Zhang et al&#8217;s implementation (Zhang et al, 2004) for 95% confidence intervals significant test.
We compare the SncTSSG based model against two baseline models: the phrase-based and the STSSG-based models. For the phrase-based model, we use Moses (Koehn et al, 2007) with its default settings; for the STSSG and SncTSSG based models we use our decoder Pisces by setting the following parameters: , , , , , . Additionally, for STSSG we set , and for SncTSSG, we set .</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In the experiments, we train the translation model on FBIS corpus (7.2M (Chinese) + 9.2M (English) words) and train a 4-gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002).</text>
                  <doc_id>182</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We use these sentences with less than 50 characters from the NIST MT-2002 test set as the development set and the NIST MT-2005 test set as our test set.</text>
                  <doc_id>183</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We use the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test set.</text>
                  <doc_id>184</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The evaluation metric is casesensitive BLEU-4 (Papineni et al., 2002).</text>
                  <doc_id>185</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We base on the m-to-n word alignments dumped by GI- ZA++ to extract the tree sequence pairs.</text>
                  <doc_id>186</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For the MER training, we modify Koehn&#8217;s version (Koehn, 2004).</text>
                  <doc_id>187</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>We use Zhang et al&#8217;s implementation (Zhang et al, 2004) for 95% confidence intervals significant test.</text>
                  <doc_id>188</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We compare the SncTSSG based model against two baseline models: the phrase-based and the STSSG-based models.</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For the phrase-based model, we use Moses (Koehn et al, 2007) with its default settings; for the STSSG and SncTSSG based models we use our decoder Pisces by setting the following parameters: , , , , , .</text>
                  <doc_id>190</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Additionally, for STSSG we set , and for SncTSSG, we set .</text>
                  <doc_id>191</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Experimental Results</title>
            <text>Table 1 compares the performance of different models across the two systems. The proposed SncTSSG based model significantly outperforms (p &lt; 0.05) the two baseline models. Since the SncTSSG based model covers the STSSG based model in its modeling ability and obtains a superset in rules, the improvement empirically verifies the effectiveness of the additional non-contiguous rules.
Pisces
Table 2 measures the contribution of different combination of rules. cR refers to the rules derived from contiguous tree sequence pairs (i.e., all STSSG rules); ncPR refers to non-contiguous phrasal rules derived from contiguous tree sequence pairs with at least one non-terminal leaf node between two lexicalized leaf nodes (i.e., all non-contiguous rules in STSSG defined as in Zhang et al. (2008a)); srcncR refers to source side non-contiguous rules (SncTSSG rules only, not STSSG rules); tgtncR refers to target side noncontiguous rules (SncTSSG rules only, not STSSG rules) and src&amp;tgtncR refers non-contiguous rules
with gaps in either side (srcncR+ tgtncR). The last three kinds of rules are all derived from noncontiguous tree sequence pairs.
1) From Exp 1 and 2 in Table 2, we find that non-contiguous phrasal rules (ncPR) derived from contiguous tree sequence pairs make little impact on the translation performance which is consistent with the discovery of Zhang et al. (2008a). However, if we append the non-contiguous phrasal rules derived from non-contiguous tree sequence pairs, no matter whether non-contiguous in source or in target, the performance statistically significantly (p &lt; 0.05) improves (as presented in Exp 2~5), which validates our prediction that the noncontiguous rules derived from non-contiguous tree sequence pairs contribute more to the performance than those acquired from contiguous tree sequence pairs.
2) Not only that, after comparing Exp 6,7,8 against Exp 3,4,5 respectively, we find that the ability of rules derived from non-contiguous tree sequence pairs generally covers that of the rules derived from the contiguous tree sequence pairs, due to the slight change in BLEU score.
3) The further comparison of the noncontiguous rules from non-contiguous spans in Exp. 6&amp;7 as well as Exp 3&amp;4, shows that noncontiguity in the target side in Chinese-English translation task is not so useful as that in the source side when constructing the non-contiguous phrasal rules. This also validates the findings in Wellington et al. (2006) that varying the gaps on the English side (the target side in this context) seldom reduce the hierarchical alignment failures.
Table 3 explores the contribution of the noncontiguous translational equivalence to phrasebased models (all the rules in Table 3 has no grammar tags, but a gap &lt;***&gt; is allowed in the last three rows). tgtncBP refers to the bilingual phrases with gaps in the target side; srcncBP refers to the bilingual phrases with gaps in the source side; src&amp;tgtncBP refers to the bilingual phrases with gaps in either side.
1) As presented in Table 3, the effectiveness of the bilingual phrases derived from noncontiguous tree sequence pairs is clearly indicated. Models adopting both tgtncBP and srcncBP significantly (p &lt; 0.05) outperform the model adopting cBP only.
2) Pisces underperforms Moses when utilizing cBPs only, since Pisces can only perform monotonic search with cBPs.
3) The bilingual phrase model with both tgtncBP and srcncBP even outperforms Moses. Compared with Moses, we only utilize plain features in Pisces for the bilingual phrase model (Feature 1~5 for all phrases and additional 7, 8 only for non-contiguous bilingual phrases as stated in Section 2.2; None of the complex reordering features or distortion features are employed by Pisces while Moses uses them), which suggests the effectiveness of the non-contiguous rules and the advantages of the proposed decoding algorithm.
Table 4 studies the impact on performance when setting different maximal gaps allowed for either side in a tree sequence pair (parameter ) and the relation with the quantity of rule set.
Significant improvement is achieved when allowing at least one gap on either side compared with when only allowing contiguous tree sequence pairs. However, the further increment of gaps does not benefit much. The result exhibits the accordance with the growing amplitude of the rule set filtered for the test set, in which the rule size increases more slowly as the maximal number of gaps increments. As a result, this slow increase against the increment of gaps can be probably attributed to the small augmentation of the effective

Output &amp; References
non-contiguous rules.
In order to facilitate a better intuition to the ability of the SncTSSG based model against the STSSG based model, we present in Table 5, two translation outputs produced by both models.
In the first example, GIZA++ wrongly aligns the idiom word &#8220; &#162;&#161;&#164;&#163;&#166;&#165; /confront at court&#8221; to a noncontiguous phrase &#8220;confront other countries at court*** leisurely manner&#8221; in training, in which only the first constituent &#8220;confront other countries at court&#8221; is reasonable, indicated from the key rules of SncTSSG leant from the training set. The STSSG or any contiguous translational equivalence based model is unable to attain the corresponding target output for this idiom word via the non-contiguous word alignment and consider it as an out-of-vocabulary (OOV). On the contrary, the SncTSSG based model can capture the noncontiguous tree sequence pair consistent with the word alignment and further provide a reasonable target translation. It suggests that SncTSSG can easily capture the non-contiguous translational candidates while STSSG cannot. Besides, SncTSSG is less sensitive to the error of word alignment when extracting the translation candidates than the contiguous translational equivalence based models.
In the second example, &#8220;&#167; /in &#168;&#169; /recent &#164; /&#8217;s  /survey  /middle&#8221; is correctly translated into &#8220;in
the recent surveys&#8221; by both the STSSG and SncTSSG based models. This suggests that the short non-contiguous phrase &#8220;&#167; /in ***  /middle&#8221; is well handled by both models. Nevertheless, as for the one with a larger gap, &#8220; /will ***  /continue&#8221; is correctly translated and well reordering into &#8220;will continue&#8221; by SncTSSG but failed by
STSSG. Although the STSSG is theoretically able to capture this phrase from the contiguous tree sequence pair, the richer context in the gap as in this example, the more difficult STSSG can correctly translate the non-contiguous phrases. This exhibits the flexibility of SncTSSG to the rich context among the non-contiguous constituents.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Table 1 compares the performance of different models across the two systems.</text>
                  <doc_id>192</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The proposed SncTSSG based model significantly outperforms (p &lt; 0.05) the two baseline models.</text>
                  <doc_id>193</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Since the SncTSSG based model covers the STSSG based model in its modeling ability and obtains a superset in rules, the improvement empirically verifies the effectiveness of the additional non-contiguous rules.</text>
                  <doc_id>194</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Pisces</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 2 measures the contribution of different combination of rules.</text>
                  <doc_id>196</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>cR refers to the rules derived from contiguous tree sequence pairs (i.e., all STSSG rules); ncPR refers to non-contiguous phrasal rules derived from contiguous tree sequence pairs with at least one non-terminal leaf node between two lexicalized leaf nodes (i.e., all non-contiguous rules in STSSG defined as in Zhang et al. (2008a)); srcncR refers to source side non-contiguous rules (SncTSSG rules only, not STSSG rules); tgtncR refers to target side noncontiguous rules (SncTSSG rules only, not STSSG rules) and src&amp;tgtncR refers non-contiguous rules</text>
                  <doc_id>197</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>with gaps in either side (srcncR+ tgtncR).</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The last three kinds of rules are all derived from noncontiguous tree sequence pairs.</text>
                  <doc_id>199</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1) From Exp 1 and 2 in Table 2, we find that non-contiguous phrasal rules (ncPR) derived from contiguous tree sequence pairs make little impact on the translation performance which is consistent with the discovery of Zhang et al. (2008a).</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, if we append the non-contiguous phrasal rules derived from non-contiguous tree sequence pairs, no matter whether non-contiguous in source or in target, the performance statistically significantly (p &lt; 0.05) improves (as presented in Exp 2~5), which validates our prediction that the noncontiguous rules derived from non-contiguous tree sequence pairs contribute more to the performance than those acquired from contiguous tree sequence pairs.</text>
                  <doc_id>201</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2) Not only that, after comparing Exp 6,7,8 against Exp 3,4,5 respectively, we find that the ability of rules derived from non-contiguous tree sequence pairs generally covers that of the rules derived from the contiguous tree sequence pairs, due to the slight change in BLEU score.</text>
                  <doc_id>202</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3) The further comparison of the noncontiguous rules from non-contiguous spans in Exp.</text>
                  <doc_id>203</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>6&amp;7 as well as Exp 3&amp;4, shows that noncontiguity in the target side in Chinese-English translation task is not so useful as that in the source side when constructing the non-contiguous phrasal rules.</text>
                  <doc_id>204</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This also validates the findings in Wellington et al. (2006) that varying the gaps on the English side (the target side in this context) seldom reduce the hierarchical alignment failures.</text>
                  <doc_id>205</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 3 explores the contribution of the noncontiguous translational equivalence to phrasebased models (all the rules in Table 3 has no grammar tags, but a gap &lt;***&gt; is allowed in the last three rows).</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>tgtncBP refers to the bilingual phrases with gaps in the target side; srcncBP refers to the bilingual phrases with gaps in the source side; src&amp;tgtncBP refers to the bilingual phrases with gaps in either side.</text>
                  <doc_id>207</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1) As presented in Table 3, the effectiveness of the bilingual phrases derived from noncontiguous tree sequence pairs is clearly indicated.</text>
                  <doc_id>208</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Models adopting both tgtncBP and srcncBP significantly (p &lt; 0.05) outperform the model adopting cBP only.</text>
                  <doc_id>209</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2) Pisces underperforms Moses when utilizing cBPs only, since Pisces can only perform monotonic search with cBPs.</text>
                  <doc_id>210</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3) The bilingual phrase model with both tgtncBP and srcncBP even outperforms Moses.</text>
                  <doc_id>211</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Compared with Moses, we only utilize plain features in Pisces for the bilingual phrase model (Feature 1~5 for all phrases and additional 7, 8 only for non-contiguous bilingual phrases as stated in Section 2.2; None of the complex reordering features or distortion features are employed by Pisces while Moses uses them), which suggests the effectiveness of the non-contiguous rules and the advantages of the proposed decoding algorithm.</text>
                  <doc_id>212</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 4 studies the impact on performance when setting different maximal gaps allowed for either side in a tree sequence pair (parameter ) and the relation with the quantity of rule set.</text>
                  <doc_id>213</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Significant improvement is achieved when allowing at least one gap on either side compared with when only allowing contiguous tree sequence pairs.</text>
                  <doc_id>214</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, the further increment of gaps does not benefit much.</text>
                  <doc_id>215</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The result exhibits the accordance with the growing amplitude of the rule set filtered for the test set, in which the rule size increases more slowly as the maximal number of gaps increments.</text>
                  <doc_id>216</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>As a result, this slow increase against the increment of gaps can be probably attributed to the small augmentation of the effective</text>
                  <doc_id>217</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text></text>
                  <doc_id>218</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Output &amp; References</text>
                  <doc_id>219</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>non-contiguous rules.</text>
                  <doc_id>220</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In order to facilitate a better intuition to the ability of the SncTSSG based model against the STSSG based model, we present in Table 5, two translation outputs produced by both models.</text>
                  <doc_id>221</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In the first example, GIZA++ wrongly aligns the idiom word &#8220; &#162;&#161;&#164;&#163;&#166;&#165; /confront at court&#8221; to a noncontiguous phrase &#8220;confront other countries at court*** leisurely manner&#8221; in training, in which only the first constituent &#8220;confront other countries at court&#8221; is reasonable, indicated from the key rules of SncTSSG leant from the training set.</text>
                  <doc_id>222</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The STSSG or any contiguous translational equivalence based model is unable to attain the corresponding target output for this idiom word via the non-contiguous word alignment and consider it as an out-of-vocabulary (OOV).</text>
                  <doc_id>223</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>On the contrary, the SncTSSG based model can capture the noncontiguous tree sequence pair consistent with the word alignment and further provide a reasonable target translation.</text>
                  <doc_id>224</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>It suggests that SncTSSG can easily capture the non-contiguous translational candidates while STSSG cannot.</text>
                  <doc_id>225</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Besides, SncTSSG is less sensitive to the error of word alignment when extracting the translation candidates than the contiguous translational equivalence based models.</text>
                  <doc_id>226</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In the second example, &#8220;&#167; /in &#168;&#169; /recent &#164; /&#8217;s  /survey  /middle&#8221; is correctly translated into &#8220;in</text>
                  <doc_id>227</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the recent surveys&#8221; by both the STSSG and SncTSSG based models.</text>
                  <doc_id>228</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This suggests that the short non-contiguous phrase &#8220;&#167; /in ***  /middle&#8221; is well handled by both models.</text>
                  <doc_id>229</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Nevertheless, as for the one with a larger gap, &#8220; /will ***  /continue&#8221; is correctly translated and well reordering into &#8220;will continue&#8221; by SncTSSG but failed by</text>
                  <doc_id>230</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>STSSG.</text>
                  <doc_id>231</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Although the STSSG is theoretically able to capture this phrase from the contiguous tree sequence pair, the richer context in the gap as in this example, the more difficult STSSG can correctly translate the non-contiguous phrases.</text>
                  <doc_id>232</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This exhibits the flexibility of SncTSSG to the rich context among the non-contiguous constituents.</text>
                  <doc_id>233</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>6 Conclusions and Future Work</title>
        <text>In this paper, we present a non-contiguous tree sequence alignment model based on SncTSSG to enhance the ability of non-contiguous phrase modeling and the reordering caused by non-contiguous constituents with large gaps. A three-phase decoding algorithm is developed to facilitate the usage of non-contiguous translational equivalences (tree sequence pairs in this work) which provides much flexibility for the reordering of the non-contiguous constituents with rich syntactic structural information. The experimental results show that our model outperforms the baseline models and verify the effectiveness of non-contiguous translational equivalences to non-contiguous phrase modeling in both syntax-based and phrase-based systems. We also find that in Chinese-English translation task, gaps are more effective in Chinese side than in the English side. Although the characteristic of more sensitiveness to word alignment error enables SncTSSG to capture the additional non-contiguous language phenomenon, it also induces many redundant noncontiguous rules. Therefore, further work of our studies includes the optimization of the large rule set of the SncTSSG based model.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper, we present a non-contiguous tree sequence alignment model based on SncTSSG to enhance the ability of non-contiguous phrase modeling and the reordering caused by non-contiguous constituents with large gaps.</text>
              <doc_id>234</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A three-phase decoding algorithm is developed to facilitate the usage of non-contiguous translational equivalences (tree sequence pairs in this work) which provides much flexibility for the reordering of the non-contiguous constituents with rich syntactic structural information.</text>
              <doc_id>235</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The experimental results show that our model outperforms the baseline models and verify the effectiveness of non-contiguous translational equivalences to non-contiguous phrase modeling in both syntax-based and phrase-based systems.</text>
              <doc_id>236</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We also find that in Chinese-English translation task, gaps are more effective in Chinese side than in the English side.</text>
              <doc_id>237</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Although the characteristic of more sensitiveness to word alignment error enables SncTSSG to capture the additional non-contiguous language phenomenon, it also induces many redundant noncontiguous rules.</text>
              <doc_id>238</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, further work of our studies includes the optimization of the large rule set of the SncTSSG based model.</text>
              <doc_id>239</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Translation results of different models (cBP refers to contiguous bilingual phrases without syntactic structural information, as used in Moses)</caption>
        <reference_text></reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>STSSG</cell>
              <cell>25.92</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>SncTSSG</cell>
              <cell>26.53</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Performance of different rule combination</caption>
        <reference_text>In PAGE 6: ...92  SncTSSG 26.53  Table 1: Translation results of different models (cBP  refers to contiguous bilingual phrases without syntactic  structural information, as used in Moses)     Table2  measures the contribution of different  combination of rules. cR refers to the rules derived  from contiguous tree sequence pairs (i....  In PAGE 7: ...three kinds of rules are all derived from non- contiguous tree sequence pairs.  1)  From Exp 1 and 2 in  Table2 , we find that  non-contiguous phrasal rules (ncPR) derived from  contiguous tree sequence pairs make little impact  on the translation performance which is consistent  with the discovery of Zhang et al. (2008a)....</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>1 cR (STSSG)</cell>
              <cell>1 cR (STSSG)</cell>
              <cell>25.92</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>2</cell>
              <cell>cR w/o ncPR</cell>
              <cell>25.87</cell>
            </row>
            <row>
              <cell>3</cell>
              <cell>cR w/o ncPR + tgtncR</cell>
              <cell>26.14</cell>
            </row>
            <row>
              <cell>4</cell>
              <cell>cR w/o ncPR + srcncR</cell>
              <cell>26.50</cell>
            </row>
            <row>
              <cell>5</cell>
              <cell>cR w/o ncPR + src amp</cell>
              <cell>tgtncR</cell>
              <cell>26.51</cell>
            </row>
            <row>
              <cell>6</cell>
              <cell>cR + tgtncR</cell>
              <cell>26.11</cell>
            </row>
            <row>
              <cell>7</cell>
              <cell>cR + srcncR</cell>
              <cell>26.56</cell>
            </row>
            <row>
              <cell>8 cR+src amp</cell>
              <cell>tgtncR(SncTSSG)  26.53</cell>
              <cell>8 cR+src amp</cell>
              <cell>tgtncR(SncTSSG)  26.53</cell>
              <cell>8 cR+src amp</cell>
              <cell>tgtncR(SncTSSG)  26.53</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Performance of bilingual phrasal rules</caption>
        <reference_text>In PAGE 7: ... (2006) that varying the gaps on the Eng- lish side (the target side in this context) seldom  reduce the hierarchical alignment failures.    Table3  explores the contribution of the non- contiguous translational equivalence to phrase- based models (all the rules in Table 3 has no  grammar tags, but a gap  lt;*** gt; is allowed in the  last three rows). tgtncBP refers to the bilingual  phrases with gaps in the target side; srcncBP refers  to the bilingual phrases with gaps in the source  side;  src amp;tgtncBP refers to the bilingual phrases  with gaps in either side....  In PAGE 7: ... (2006) that varying the gaps on the Eng- lish side (the target side in this context) seldom  reduce the hierarchical alignment failures.   Table 3 explores the contribution of the non- contiguous translational equivalence to phrase- based models (all the rules in  Table3  has no  grammar tags, but a gap  lt;*** gt; is allowed in the  last three rows). tgtncBP refers to the bilingual  phrases with gaps in the target side; srcncBP refers  to the bilingual phrases with gaps in the source  side;  src amp;tgtncBP refers to the bilingual phrases  with gaps in either side....  In PAGE 7: ...Table 3: Performance of bilingual phrasal rules  1)  As presented in  Table3 , the effectiveness  of the bilingual phrases derived from non- contiguous tree sequence pairs is clearly indicated.  Models adopting both tgtncBP and srcncBP sig- nificantly (p  lt; 0....</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Rule Set</cell>
              <cell>BLEU</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Moses</cell>
              <cell>cBP</cell>
              <cell>23.86</cell>
            </row>
            <row>
              <cell></cell>
              <cell>cBP</cell>
              <cell>22.63</cell>
            </row>
            <row>
              <cell>Pisces</cell>
              <cell>cBP + tgtncBP 23.74
cBP + srcncBP 23.93</cell>
            </row>
            <row>
              <cell>cBP + src&amp;tgtncBP</cell>
              <cell>24.24</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TET</source>
        <caption>Table 4: Performance and rule size changing with different maximal number of gaps</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Max gaps allowed</cell>
              <cell>Rule #</cell>
              <cell>BLEU</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>source</cell>
              <cell>target</cell>
              <cell></cell>
              <cell></cell>
            </row>
            <row>
              <cell>0</cell>
              <cell>0</cell>
              <cell>1,661,045</cell>
              <cell>25.92</cell>
            </row>
            <row>
              <cell>1</cell>
              <cell>1</cell>
              <cell>+841,263</cell>
              <cell>26.53</cell>
            </row>
            <row>
              <cell>2</cell>
              <cell>2</cell>
              <cell>+447,161</cell>
              <cell>26.55</cell>
            </row>
            <row>
              <cell>3</cell>
              <cell>3</cell>
              <cell>+17,782</cell>
              <cell>26.56</cell>
            </row>
            <row>
              <cell></cell>
              <cell>&#8217;</cell>
              <cell>+8,223</cell>
              <cell>26.57</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TET</source>
        <caption>Table 5: Sample translations (tokens in italic match the reference provided)</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row/>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Reference after only five years the two confronted each other at court</cell>
            </row>
            <row>
              <cell>STSSG</cell>
              <cell>only in the five years , the two candidates would</cell>
            </row>
            <row>
              <cell>SncTSSG</cell>
              <cell>the two people can confront other countries at court leisurely manner only in the five years</cell>
            </row>
            <row>
              <cell>key rules VV( )! VB(confront)NP(JJ(other),NNS(countries))IN(at) NN(court) JJ(leisurely)NN(manner)
"# /Euro /&#8217;s %'&amp; /substantial () /appreciation /will /in ,- /recent /&#8217;s ./ /survey /middle 12 /continue Source /for 34 /economy 56 /confidence 78 /produce 9': /impact</cell>
            </row>
            <row>
              <cell>Reference substantial appreciation of the euro will continue to impact the economic confidence in the recent surveys</cell>
            </row>
            <row>
              <cell>STSSG
substantial appreciation of the euro has continued to have an impact on confidence in the economy , in the recent
surveys will</cell>
            </row>
            <row>
              <cell>SncTSSG</cell>
              <cell>substantial appreciation of the euro will continue in the recent surveys have an impact on economic confidence</cell>
            </row>
            <row>
              <cell>AD(* ) VV(12 ) VP(MD(will),VB(continue))
key rules
P(+ )LC(0 ) IN(in)</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Rens Bod</author>
        </authors>
        <title>Unsupervised Syntax-Based Machine Translation: The Contribution of Discontinuous Phrases.</title>
        <publication>None</publication>
        <pages>51--56</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>An Introduction to Synchronous Grammars. Tutorial on</title>
        <publication>ACL-06 Yuan Ding</publication>
        <pages>05--541</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Michel Galley</author>
          <author>J Graehl</author>
          <author>K Knight</author>
          <author>D Marcu</author>
          <author>S DeNeefe</author>
          <author>W Wang</author>
          <author>I Thayer</author>
        </authors>
        <title>Scalable Inference and training of context-rich syntactic translation models.</title>
        <publication>None</publication>
        <pages>06--961</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Mary Hearne</author>
          <author>Andy Way</author>
        </authors>
        <title>Seeing the wood for the trees: data-oriented translation.</title>
        <publication>MT Summit IX,</publication>
        <pages>165--172</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Dan Klein</author>
          <author>Christopher D Manning</author>
        </authors>
        <title>None</title>
        <publication>Accurate Unlexicalized Parsing. ACL-03.</publication>
        <pages>423--430</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz J Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Chris Callison-Burch Birch</author>
          <author>Marcello Federico</author>
          <author>Nicola Bertoldi</author>
          <author>Brooke Cowan</author>
          <author>Wade Shen</author>
        </authors>
        <title>Statistical phrase-based translation. HLT-NAACL03. 127-133 Philipp Koehn, Hieu Hoang,</title>
        <publication>None</publication>
        <pages>77--180</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Yang Liu</author>
          <author>Qun Liu</author>
          <author>Shouxun Lin</author>
        </authors>
        <title>Tree-toString Alignment Template for Statistical Machine Translation.</title>
        <publication>None</publication>
        <pages>609--616</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Daniel Marcu</author>
          <author>William Wong</author>
        </authors>
        <title>A phrasebased, joint probability model for statistical machine translation. EMNLP-02,</title>
        <publication>None</publication>
        <pages>133--139</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Franz J Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>The alignment template approach to statistical machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Computational Linguistics</author>
        </authors>
        <title>30(4):417-449 Kishore Papineni, Salim Roukos, ToddWard and WeiJing Zhu.</title>
        <publication>None</publication>
        <pages>02--311</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Chris Quirk</author>
          <author>Arul Menezes</author>
          <author>Colin Cherry</author>
        </authors>
        <title>Dependency treelet translation: syntactically informed phrasal SMT.</title>
        <publication>None</publication>
        <pages>05--271</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>S Shieber</author>
        </authors>
        <title>Synchronous grammars as tree transducers.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>13</id>
        <authors/>
        <title>SRILM - an extensible language modeling toolkit.</title>
        <publication>In Proceedings of the Seventh International Workshop on Tree Adjoining Grammar and Related Formalisms Andreas Stolcke.</publication>
        <pages>02--901</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Benjamin Wellington</author>
          <author>Sonjia Waxmonsky</author>
          <author>I Dan Melamed</author>
        </authors>
        <title>Empirical Lower Bounds on the Complexity of Translational Equivalence.</title>
        <publication>ACL-06. 977-984 Kenji Yamada</publication>
        <pages>07--535</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Min Zhang</author>
          <author>Hongfei Jiang</author>
          <author>AiTi Aw</author>
          <author>Haizhou Li</author>
          <author>Chew Lim Tan</author>
          <author>Sheng Li</author>
        </authors>
        <title>A tree sequence alignment-based tree-to-tree translation model.</title>
        <publication>None</publication>
        <pages>08--559</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Min Zhang</author>
          <author>Hongfei Jiang</author>
          <author>Haizhou Li</author>
        </authors>
        <title>Aiti Aw, Sheng Li.</title>
        <publication>2008b. Grammar Comparison Study for Translational Equivalence Modeling and Statistical Machine Translation. COLING-08.</publication>
        <pages>1097--1104</pages>
        <date>None</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Bod, 2007</string>
        <sentence_id>27590</sentence_id>
        <char_offset>174</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Chiang, 2006</string>
        <sentence_id>27658</sentence_id>
        <char_offset>171</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Galley et al, 2006</string>
        <sentence_id>27592</sentence_id>
        <char_offset>118</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>3</reference_id>
        <string>Hearne and Way, 2003</string>
        <sentence_id>27590</sentence_id>
        <char_offset>208</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>4</reference_id>
        <string>Klein and Manning, 2003</string>
        <sentence_id>27765</sentence_id>
        <char_offset>28</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>5</reference_id>
        <string>Koehn et al, 2003</string>
        <sentence_id>27588</sentence_id>
        <char_offset>63</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>7</reference_id>
        <string>Liu et al, 2006</string>
        <sentence_id>27590</sentence_id>
        <char_offset>185</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>7</reference_id>
        <string>Liu et al, 2006</string>
        <sentence_id>27592</sentence_id>
        <char_offset>190</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>8</reference_id>
        <string>Marcu and Wong, 2002</string>
        <sentence_id>27588</sentence_id>
        <char_offset>41</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>9</reference_id>
        <string>Och and Ney, 2004</string>
        <sentence_id>27588</sentence_id>
        <char_offset>82</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>11</reference_id>
        <string>Quirk et al, 2005</string>
        <sentence_id>27590</sentence_id>
        <char_offset>129</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>14</reference_id>
        <string>Wellington et al. (2006)</string>
        <sentence_id>27594</sentence_id>
        <char_offset>9</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>14</reference_id>
        <string>Wellington et al. (2006)</string>
        <sentence_id>27720</sentence_id>
        <char_offset>2</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>14</reference_id>
        <string>Wellington et al. (2006)</string>
        <sentence_id>27786</sentence_id>
        <char_offset>36</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>15</reference_id>
        <string>Zhang et al. (2008</string>
        <sentence_id>27593</sentence_id>
        <char_offset>12</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>15</reference_id>
        <string>Zhang et al. (2008</string>
        <sentence_id>27597</sentence_id>
        <char_offset>104</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>15</reference_id>
        <string>Zhang et al. (2008</string>
        <sentence_id>27725</sentence_id>
        <char_offset>76</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>15</reference_id>
        <string>Zhang et al. (2008</string>
        <sentence_id>27778</sentence_id>
        <char_offset>311</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>15</reference_id>
        <string>Zhang et al. (2008</string>
        <sentence_id>27781</sentence_id>
        <char_offset>217</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>15</reference_id>
        <string>Zhang et al, 2008</string>
        <sentence_id>27612</sentence_id>
        <char_offset>65</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>15</reference_id>
        <string>Zhang et al, 2008</string>
        <sentence_id>27659</sentence_id>
        <char_offset>160</char_offset>
      </citation>
    </citations>
  </content>
</document>
