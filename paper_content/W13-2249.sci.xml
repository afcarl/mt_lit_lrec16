<PAPER>
  <FILENO/>
  <TITLE>DCU-Symantec at the WMT 2013 Quality Estimation Shared Task</TITLE>
  <AUTHORS/>
  <ABSTRACT>
    <A-S ID="S-54907">We describe the two systems submitted by the DCU-Symantec team to Task 1.1.</A-S>
    <A-S ID="S-54908">of the WMT 2013 Shared Task on Quality Estimation for Machine Translation.</A-S>
    <A-S ID="S-54909">Task 1.1 involve estimating postediting effort for English-Spanish translation pairs in the news domain.</A-S>
    <A-S ID="S-54910">The two systems use a wide variety of features, of which the most effective are the word-alignment, n-gram frequency, language model, POS-tag-based and pseudoreferences ones.</A-S>
    <A-S ID="S-54911">Both systems perform at a similarly high level in the two tasks of scoring and ranking translations, although there is some evidence that the systems are over-fitting to the training data.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-54912">The WMT 2013 Quality Estimation Shared Task involve both sentence-level and word-level quality estimation (QE).</S>
        <S ID="S-54913">The sentence-level task consist of three subtasks: scoring and ranking translations with regard to post-editing effort (Task 1.1), selecting among several translations produced by multiple MT systems for the same source sentence (Task 1.2), and predicting post-editing time (Task 1.3).</S>
        <S ID="S-54914">The DCU-Symantec team enter two systems to Task 1.1.</S>
        <S ID="S-54915">Given a set of source English news sentences and their Spanish translations, the goals are to predict the HTER score of each translation and to produce a ranking based on HTER for the set of translations.</S>
        <S ID="S-54916">A set of 2,254 sentence pairs are provided for training.</S>
        <S ID="S-54917">On the ranking task, our system DCU-SYMC alltypes is second placed out of thirteen systems and our system DCU-SYMC combine is ranked fifth, according to the Delta Average metric.</S>
        <S ID="S-54918">According to the Spearman rank correlation, our systems are the joint-highest systems.</S>
        <S ID="S-54919">In the scoring task, the DCU-SYMC alltypes system is placed sixth out of seventeen systems according to Mean Absolute Error (MAE) and third according to Root Mean Squared Error (RMSE).</S>
        <S ID="S-54920">The DCU-SYMC combine system is placed fifth according to MAE and second according to RMSE.</S>
      </P>
      <P>
        <S ID="S-54921">In this system description paper, we describe the features, the learning methods used, the results for the two submitted systems and some other systems we experiment with.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Features</HEADER>
      <P>
        <S ID="S-54922">Our starting point for the WMT13 QE shared task was the feature set used in the system we submitted to the WMT12 QE task (<REF ID="R-05" RPTR="5">Rubino et al., 2012</REF>).</S>
        <S ID="S-54923">This feature set, comprising 308 features in total, extended the 17 baseline features provided by the task organisers to include 6 additional surface features, 6 additional language model features, 17 additional features derived from the MT system components and the n-best lists, 138 features obtained by part-of-speech tagging and parsing the source sentences and 95 obtained by part-of-speech tagging the target sentences, 21 topic model features, 2 features produced by a grammar checker 1 and 6 pseudo-source (or backtranslation) features.</S>
        <S ID="S-54924">We made the following modifications to this 2012 feature set:</S>
      </P>
      <P>
        <S ID="S-54925">&#8226; The pseudo-source (or back-translation) features were removed, as they did not contribute useful information to our system last year.</S>
      </P>
      <P>
        <S ID="S-54926">&#8226; The language model and n-gram frequency feature sets were extended in order to cover 1 to 5 gram sequences, as well as source and target ratios for these feature values.</S>
      </P>
      <P>
        <S ID="S-54927">&#8226; The word-alignment feature set was also extended by considering several thresholds</S>
      </P>
      <P>
        <S ID="S-54928">1 http://www.languagetool.org/</S>
      </P>
      <P>
        <S ID="S-54929">when counting the number of target words</S>
      </P>
      <P>
        <S ID="S-54930">aligned with source words.</S>
        <S ID="S-54931">&#8226; We extracted 8 additional features from the decoder log file, including the number of discarded hypotheses, the total number of translation options and the number of nodes in the decoding graph.</S>
        <S ID="S-54932">&#8226; The set of topic model features was reduced in order to keep only those that were shown to be effective on three quality estimation datasets (the details can be found in (Rubino et al. (to appear), 2013)).</S>
        <S ID="S-54933">These features encode the difference between source and target topic distributions according to several distance/divergence metrics.</S>
        <S ID="S-54934">&#8226; Following <REF ID="R-09" RPTR="9">Soricut et al. (2012)</REF>, we employed pseudo-reference features.</S>
        <S ID="S-54935">The source sentences were translated with three different MT systems: an in-house phrase-based SMT system built using Moses (<REF ID="R-02" RPTR="2">Koehn et al., 2007</REF>) and trained on the parallel data provided by the organisers, the rule-based system Systran 2 and the online, publicly available, Bing Translator 3 .</S>
        <S ID="S-54936">The obtained translations are compared to the target sentences using sentence-level BLEU (<REF ID="R-04" RPTR="4">Papineni et al., 2002</REF>), TER (<REF ID="R-08" RPTR="8">Snover et al., 2006</REF>) and the Levenshtein distance (<REF ID="R-03" RPTR="3">Levenshtein, 1966</REF>).</S>
        <S ID="S-54937">&#8226; Also following <REF ID="R-09" RPTR="10">Soricut et al. (2012)</REF>, oneto-one word-alignments, with and without Part-Of-Speech (POS) agreement, were included as features.</S>
        <S ID="S-54938">Using the alignment information provided by the decoder, we POS tagged the source and target sentences with TreeTagger (<REF ID="R-07" RPTR="7">Schmidt, 1994</REF>) and the publicly available pre-trained models for English and Spanish.</S>
        <S ID="S-54939">We mapped the tagsets of both languages by simplifying the initial tags and obtain a reduced set of 8 tags.</S>
        <S ID="S-54940">We applied that simplification on the tagged sentences before checking for POS agreement.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Machine Learning</HEADER>
      <P>
        <S ID="S-54996">In this section, we describe the learning algorithms and feature selection used in our experiments, leading to the two submitted systems for the shared task.</S>
      </P>
      <P>
        <S ID="S-54997">2 Systran Enterprise Server version 6 3 http://www.bing.com/translator</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>3.1 Primary Learning Method</HEADER>
        <P>
          <S ID="S-54941">To estimate the post-editing effort of translated sentences, we rely on regression models built using the Support Vector Machine (SVM) algorithm for regression &#603;-SVR, implemented in the LIB- SVM toolkit (<REF ID="R-00" RPTR="0">Chang and Lin, 2011</REF>).</S>
          <S ID="S-54942">To build our final regression models, we optimise SVM hyper-parameters (C, &#947; and &#603;) using a grid-search method with 5-fold cross-validation for each parameter triplet.</S>
          <S ID="S-54943">The parameters leading to the best MAE, RMSE and Pearson&#8217;s correlation coefficient (r) are kept to build the model.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.2 Feature Selection on Feature Types</HEADER>
        <P>
          <S ID="S-54944">In order to reduce the feature and obtain more compact models, we apply feature selection on each of our 15 feature types.</S>
          <S ID="S-54945">Examples of feature types are language model features or topic model features.</S>
          <S ID="S-54946">For each feature type, we apply a feature subset evaluation method based on the wrapper paradigm and using the best-first search algorithm to explore the feature space.</S>
          <S ID="S-54947">The M5P (Wang and Witten, 1997) regression tree algorithm implemented in the Weka toolkit (<REF ID="R-01" RPTR="1">Hall et al., 2009</REF>) is used with default parameters to train and evaluate a regression model for each feature subset obtained with best-first search.</S>
          <S ID="S-54948">A 10-fold crossvalidation is performed for each subset and we keep the features leading to the best RMSE.</S>
          <S ID="S-54949">We use M5P regression trees instead of &#603;-SVR because grid-search with the latter is too computationally expensive to be applied so many times.</S>
          <S ID="S-54950">Using feature selection in this way, we obtain 15 reduced feature sets that we combine to form the DCU-SYMC alltypes system, containing 102 features detailed in Table 1.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.3 Feature Binarisation</HEADER>
        <P>
          <S ID="S-54951">In order to aid the SVM learner, we also experiment with binarising our feature set, i.e. converting our features with various feature value ranges into features whose values are either 1 or 0.</S>
          <S ID="S-54952">Again, we employ regression tree learning.</S>
          <S ID="S-54953">We train regression trees with M5P and M5P-R 4 (implemented in the Weka toolkit) and create a binary feature for each regression rule found in the trees (ignoring the leaf nodes).</S>
          <S ID="S-54954">For example, a binary feature indicating whether the Bing TER score is less than or equal to 55.685 is derived from the</S>
        </P>
        <P>
          <S ID="S-54955">4 We experiment with J48 decision trees as well, but this</S>
        </P>
        <P>
          <S ID="S-54956">method did not outperform regression tree methods.</S>
        </P>
        <P>
          <S ID="S-54957">Backward LM Source 1-gram perplexity.</S>
          <S ID="S-54958">Source &amp; target 1-grams perplexity ratio.</S>
          <S ID="S-54959">Source &amp; target 3-grams and 4-gram perplexity ratio.</S>
          <S ID="S-54960">Target Syntax Frequency of tags: ADV, FS, DM, VLinf, VMinf, semicolon, VLger, NC, PDEL, VEfin, CC, CCNEG, PPx, ART, SYM, CODE, PREP, SE and number of ambiguous tags Frequency of least frequent POS 3-gram observed in a corpus.</S>
          <S ID="S-54961">Frequency of least frequent POS 4-gram and 6-gram with sentence padding (start and end of sentence tags) observed in a corpus.</S>
          <S ID="S-54962">Source Syntax Features from three probabilistic parsers.</S>
          <S ID="S-54963">(<REF ID="R-05" RPTR="6">Rubino et al., 2012</REF>).</S>
          <S ID="S-54964">Frequency of least frequent POS 2-gram, 4-gram and 9-gram with sentence padding observed in a corpus.</S>
          <S ID="S-54965">Number of analyses found and number of words, using a Lexical Functional Grammar of English as described in Rubino et al. (2012).</S>
          <S ID="S-54966">LM Source unigram perplexity.</S>
          <S ID="S-54967">Target 3-gram and 4-gram perplexity with sentence padding.</S>
          <S ID="S-54968">Source &amp; target 1-gram and 5-gram perplexity ratio.</S>
          <S ID="S-54969">Source &amp; target unigram log-probability.</S>
          <S ID="S-54970">Decoder Component scores during decoding.</S>
          <S ID="S-54971">Number of phrases in the best translation.</S>
          <S ID="S-54972">Number of translation options.</S>
          <S ID="S-54973">N-gram Frequency Target 2-gram in second and third frequency quartiles.</S>
          <S ID="S-54974">Target 3-gram and 5-gram in low frequency quartiles.</S>
          <S ID="S-54975">Number of target 1-gram seen in a corpus.</S>
          <S ID="S-54976">Source &amp; target 1-grams in highest and second highest frequency quartile.</S>
          <S ID="S-54977">One-to-One Word-Alignment Count of O2O word alignment, weighted by target sentence length.</S>
          <S ID="S-54978">Count of O2O word alignment with POS agreement, weighted by count of O2O, by source length, by target length.</S>
          <S ID="S-54979">Pseudo-Reference Moses translation TER score.</S>
          <S ID="S-54980">Bing translation number of words and TER score.</S>
          <S ID="S-54981">Systran sBLEU, number of substitutions and TER score.</S>
          <S ID="S-54982">Surface Source number of punctuation marks and average words occurrence in source sentence.</S>
          <S ID="S-54983">Target number of punctuation marks, uppercased letters and binary value if the last character of the sentence is a punctuation mark.</S>
          <S ID="S-54984">Ratio of source and target sentence lengths, average word length and number of punctuation marks over sentence lengths.</S>
          <S ID="S-54985">Topic Model Cosine distance between source and target topic distributions.</S>
          <S ID="S-54986">Jensen-Shannon divergence between source and target topic distributions.</S>
          <S ID="S-54987">Word Alignment Averaged number of source words aligned per target words with p(s|t) thresholds: 1.0, 0.75, 0.5, 0.25, 0.01 Averaged number of source words aligned per target words with p(s|t) = 0.01 weighted by target words frequency Averaged number of target words aligned per source word with p(t|s) = 0.01 weighted by source words frequency Ratio of source and target averaged aligned words with thresholds: 1.0 and 0.1, and with threshold: 0.75, 0.5, 0.25 weighted by words frequency</S>
        </P>
        <P>
          <S ID="S-54988">regression rule Bing TER score &#8804; 55.685.</S>
        </P>
        <P>
          <S ID="S-54989">The primary motivation for using regression tree learning in this way was to provide a quick and convenient method for binarising our feature set.</S>
          <S ID="S-54990">However, we can also perform feature selection using this method by experimenting with various minimum leaf sizes (Weka parameter M).</S>
          <S ID="S-54991">We plot the performance of the M5P and M5P-R (optimising towards correlation) over the parameter M and select the best three values of M.</S>
          <S ID="S-54992">To experiment with the effect of smaller and larger feature sets, we further include parameters of M that (a) lead to an approximately 50% bigger feature set and (b) to an approximately 50% smaller feature set.</S>
          <S ID="S-54993">Our DCU-SYMC combine system was built by combining the DCU-SYMC alltypes feature set, reduced using the best-first M5P wrapper approach as described in subsection 3.2, with a binarised set produced using an M5P regression tree with a minimum of 50 nodes per leaf.</S>
          <S ID="S-54994">This latter configuration, containing 34 features detailed in Table 2, was selected according to the evaluation scores obtained during cross-validation on the training set using &#603;-SVR, as described in the next section.</S>
          <S ID="S-54995">Finally, we run a greedy backward feature selection algorithm wrapping &#603;-SVR on both DCU-SYMC alltypes and DCU-SYMC combine in order to optimise our feature sets for the SVR learning algorithm, removing 6 and 2 features respectively.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 System Evaluation and Results</HEADER>
      <P>
        <S ID="S-54998">In this section, we present the results obtained with &#603;-SVR during 5-fold cross-validation on the training set and the final results obtained on the test set.</S>
        <S ID="S-54999">We selected two systems to submit amongst the different configurations based on MAE, RMSE and r.</S>
        <S ID="S-55000">As several systems reach the same performance according to these metrics, we use the number of support vectors (noted SV) as an indicator of training data over-fitting.</S>
        <S ID="S-55001">We report the results obtained with some of our systems in Table 3.</S>
      </P>
      <P>
        <S ID="S-55002">The results show that the submitted systems DCU-SYMC alltypes and DCU-SYMC combine lead to the best scores on crossvalidation, but they do not outperform the system combining the 15 feature types without feature selection (15 types).</S>
        <S ID="S-55003">This system reaches the best scores on the test set compared to all our systems built on reduced feature sets.</S>
        <S ID="S-55004">This indicates that we over-fit and fail to generalise from the training data.</S>
        <S ID="S-55005">Amongst the systems built using reduced feature sets, the M5P-R M80 system, based on the tree binarisation approach using M5P-R, yields the best results on the test set on 3 out of 4 official metrics.</S>
        <S ID="S-55006">These results indicate that this system, trained on 16 features only, tends to estimate HTER scores more accurately on the unseen test data.</S>
        <S ID="S-55007">The results of the two systems based on the M5P-R binarisation method are the best compared to all the other systems presented in this Section.</S>
        <S ID="S-55008">This feature binarisation and selection method leads to robust systems with few features: 31 and 16 for M5P-R M50 and M5P-R M80 respectively.</S>
        <S ID="S-55009">Even though these systems do not lead to the best results, they outperform the two submitted systems on one metric used to evaluate the</S>
      </P>
      <P>
        <S ID="S-55010">scoring task and two metrics to evaluate the ranking task.</S>
      </P>
      <P>
        <S ID="S-55011">On the systems built using reduced feature sets, we observe a difference of approximately 0.03pt absolute between the MAE and RMSE scores obtained during cross-validation and those on the test set.</S>
        <S ID="S-55012">Such a difference can be related to training data over-fitting, even though the feature sets obtained with the tree binarisation methods are small.</S>
        <S ID="S-55013">For instance, the system M5P M130 is trained on 4 features only, but the difference between cross-validation and test MAE scores is similar to the other systems.</S>
        <S ID="S-55014">We see on the final results that our feature selection methods is an over-fitting factor: by selecting the features which explain well the training set, the final model tends to generalise less.</S>
        <S ID="S-55015">The selected features are suited for the specificities of the training data, but are less accurate at predicting values on the unseen test set.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Discussion</HEADER>
      <P>
        <S ID="S-55016">Training data over-fitting is clearly shown by the results presented in Table 3, indicated by the performance drop between results obtained during cross-validation and the ones obtained on the test set.</S>
        <S ID="S-55017">While this drop may be related to data overfitting, it may also be related to the use of different machine learning methods for feature selection (M5P and M5P-R) and for building the final regression models (&#603;-SVR).</S>
        <S ID="S-55018">In order to verify this aspect, we build two regression models using M5P, based on the feature sets alltypes and combine.</S>
        <S ID="S-55019">Results are presented in Table 4 and show that, for the alltypes feature set, the RMSE, DeltaAvg and Spearman scores are improved using M5P compared to SVM.</S>
        <S ID="S-55020">For the combine feature set, the scoring results (MAE and RMSE) are better using SVM, while the ranking results are similar for both machine learning methods.</S>
        <S ID="S-55021">The performance drop between the results on the training data (or a development set) and the test data was also observed by the highest ranked participants in the WMT12 QE shared task.</S>
        <S ID="S-55022">To compare our system without feature selection to the winner of the previous shared task, we evaluate the 15 types system in Table 3 using the WMT12 QE dataset.</S>
        <S ID="S-55023">The results are presented in Table 5.</S>
        <S ID="S-55024">We can see that similar MAEs are obtained with our feature set and the WMT12 QE winner, whereas our system gets a higher RMSE (+0.01).</S>
        <S ID="S-55025">For the ranking scores, our system is worse using the DeltaAvg metric while it is better on Spearman coefficient.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>6 Conclusion</HEADER>
      <P>
        <S ID="S-55026">We presented in this paper our experiments for the WMT13 Quality Estimation shared task.</S>
        <S ID="S-55027">Our approach is based on the extraction of a large initial feature set, followed by two feature selection methods.</S>
        <S ID="S-55028">The first one is a wrapper approach using M5P and a best-first search algorithm, while the second one is a feature binarisation approach using M5P and M5P-R.</S>
        <S ID="S-55029">The final regression models were built using &#603;-SVR and we selected two systems to submit based on cross-validation results.</S>
        <S ID="S-55030">We observed that our system reaching the best scores on the test set was not a system trained on a reduced feature set and it did not yield the best cross-validation results.</S>
        <S ID="S-55031">This system was trained using 442 features, which are the combination of 15 different feature types.</S>
        <S ID="S-55032">Amongst the systems built on reduced sets, the best results are obtained</S>
      </P>
      <P>
        <S ID="S-55033">QE highest ranked team, in the Likert score prediction task.</S>
        <S ID="S-55034">using the feature binarisation approach M5P-R 80, which contains 16 features selected from our initial set of features.</S>
        <S ID="S-55035">The tree-based feature binarisation is a fast and flexible method which allows us to vary the number of features by optimising the leaf size and leads to acceptable results with a few selected features.</S>
        <S ID="S-55036">Future work involves a deeper analysis of the over-fitting effect and an investigation of other methods in order to outperform the non-reduced feature set.</S>
        <S ID="S-55037">We are also interested in finding a robust way to optimise the leaf size parameter for our tree-based feature binarisation method, without using cross-validation on the training set with an SVM algorithm.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS>
    <P>
      <S ID="S-55038"></S>
    </P>
  </ACKNOWLEDGMENTS>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>Chih-Chung Chang</RAUTHOR>
      <REFTITLE>LIBSVM: A Library for Support Vector Machines.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>Mark Hall</RAUTHOR>
      <REFTITLE>The WEKA Data Mining Software: an Update.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Moses: Open source toolkit for statistical machine translation. In</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>Vladimir I Levenshtein</RAUTHOR>
      <REFTITLE>Binary codes capable of correcting deletions, insertions and reversals.</REFTITLE>
      <DATE>1966</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>Kishore Papineni</RAUTHOR>
      <REFTITLE>Bleu: a method for automatic evaluation of machine translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>Raphael Rubino</RAUTHOR>
      <REFTITLE>DCU-Symantec Submission for the WMT 2012 Quality Estimation Task.</REFTITLE>
      <DATE>2012</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>Raphael Rubino</RAUTHOR>
      <REFTITLE>(to appear).</REFTITLE>
      <DATE>2013</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>Helmut Schmidt</RAUTHOR>
      <REFTITLE>Probabilistic part-of-speech tagging using decision trees.</REFTITLE>
      <DATE>1994</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>Matthew Snover</RAUTHOR>
      <REFTITLE>A study of translation edit rate with targeted human annotation.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>Radu Soricut</RAUTHOR>
      <REFTITLE>The SDL Language Weaver Systems in the WMT12 Quality Estimation Shared Task.</REFTITLE>
      <DATE>2012</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
