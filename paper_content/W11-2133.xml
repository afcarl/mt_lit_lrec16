<document>
  <filename>W11-2133</filename>
  <authors>
    <author>Nick Ruiz</author>
    <author>Marcello Federico</author>
  </authors>
  <title>Topic Adaptation for Lecture Translation through Bilingual Latent Semantic Models</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>This work presents a simplified approach to bilingual topic modeling for language model adaptation by combining text in the source and target language into very short documents and performing Probabilistic Latent Semantic Analysis (PLSA) during model training. During inference, documents containing only the source language can be used to infer a full topic-word distribution on all words in the target language&#8217;s vocabulary, from which we perform Minimum Discrimination Information (MDI) adaptation on a background language model (LM). We apply our approach on the English-French IWSLT 2010 TED Talk exercise, and report a 15% reduction in perplexity and relative BLEU and NIST improvements of 3% and 2.4%, respectively over a baseline only using a 5-gram background LM over the entire translation task. Our topic modeling approach is simpler to construct than its counterparts.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This work presents a simplified approach to bilingual topic modeling for language model adaptation by combining text in the source and target language into very short documents and performing Probabilistic Latent Semantic Analysis (PLSA) during model training.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>During inference, documents containing only the source language can be used to infer a full topic-word distribution on all words in the target language&#8217;s vocabulary, from which we perform Minimum Discrimination Information (MDI) adaptation on a background language model (LM).</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We apply our approach on the English-French IWSLT 2010 TED Talk exercise, and report a 15% reduction in perplexity and relative BLEU and NIST improvements of 3% and 2.4%, respectively over a baseline only using a 5-gram background LM over the entire translation task.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Our topic modeling approach is simpler to construct than its counterparts.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Adaptation is usually applied to reduce the performance drop of Statistical Machine Translation (SMT) systems when translating documents that deviate from training and tuning conditions. In this paper, we focus primarily on language model (LM) adaptation. In SMT, LMs are used to promote fluent translations. As probabilistic models of sequences of words, language models guide the selection and ordering of phrases in translation. With respect to
&#8727; This work was carried out during an internship period at
Fondazione Bruno Kessler.
LM training, LM adaptation for SMT tries to improve an existing LM by using smaller amounts of texts. When adaptation data represents the translation task domain one generally refers to domain adaptation, while when they just represent the content of the single document to be translated one typically refers to topic adaptation.
We propose a cross-language topic adaptation method, enabling the adaptation of a LM based on the topic distribution of the source document during translation. We train a latent semantic topic model on a collection of bilingual documents, in which each document contains both the source and target language. During inference, a latent topic distribution of words across both the source and target languages is inferred from a source document to be translated. After inference, we remove all source language words from the topic-word distributions and construct a unigram language model which is used to adapt our background LM via Minimum Discrimination Information (MDI) estimation (Federico, 1999, 2002; Kneser et al., 1997). We organize the paper as follows: In Section 2, we discuss relevant previous work. In Section 3, we review topic modeling. In Section 4, we review MDI adaptation. In Section 5, we describe our new bilingual topic modeling based adaptation technique. In Section 6, we report adaptation experiments, followed by conclusions and future work in Section 7.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Adaptation is usually applied to reduce the performance drop of Statistical Machine Translation (SMT) systems when translating documents that deviate from training and tuning conditions.</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, we focus primarily on language model (LM) adaptation.</text>
              <doc_id>5</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In SMT, LMs are used to promote fluent translations.</text>
              <doc_id>6</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As probabilistic models of sequences of words, language models guide the selection and ordering of phrases in translation.</text>
              <doc_id>7</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>With respect to</text>
              <doc_id>8</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8727; This work was carried out during an internship period at</text>
              <doc_id>9</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Fondazione Bruno Kessler.</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>LM training, LM adaptation for SMT tries to improve an existing LM by using smaller amounts of texts.</text>
              <doc_id>11</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>When adaptation data represents the translation task domain one generally refers to domain adaptation, while when they just represent the content of the single document to be translated one typically refers to topic adaptation.</text>
              <doc_id>12</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We propose a cross-language topic adaptation method, enabling the adaptation of a LM based on the topic distribution of the source document during translation.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We train a latent semantic topic model on a collection of bilingual documents, in which each document contains both the source and target language.</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>During inference, a latent topic distribution of words across both the source and target languages is inferred from a source document to be translated.</text>
              <doc_id>15</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>After inference, we remove all source language words from the topic-word distributions and construct a unigram language model which is used to adapt our background LM via Minimum Discrimination Information (MDI) estimation (Federico, 1999, 2002; Kneser et al., 1997).</text>
              <doc_id>16</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We organize the paper as follows: In Section 2, we discuss relevant previous work.</text>
              <doc_id>17</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In Section 3, we review topic modeling.</text>
              <doc_id>18</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In Section 4, we review MDI adaptation.</text>
              <doc_id>19</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Section 5, we describe our new bilingual topic modeling based adaptation technique.</text>
              <doc_id>20</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>In Section 6, we report adaptation experiments, followed by conclusions and future work in Section 7.</text>
              <doc_id>21</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Previous work</title>
        <text>Zhao et al. (2004) construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolin-
gual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model. In Sethy et al. (2006), domain-specific language models are obtained by including only the sentences that are similar to the ones in the target domain via a relative entropy based criterion.
Researchers such as Foster and Kuhn (2007) and Koehn and Schroeder (2007) have investigated mixture model approaches to adaptation. Foster and Kuhn (2007) use a mixture model approach that involves splitting a training corpus into different components, training separate models on each component, and applying mixture weights as a function of the distances of each component to the source text. Koehn and Schroeder (2007) learn mixture weights for language models trained with in-domain and outof-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models. Although the application of mixture models yields significant results, the number of mixture weights to learn grows linearly with the number of independent language models applied.
Most works focus on monolingual language model adaptation in the context of automatic speech recognition. Federico (2002) combines Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999) for topic modeling with the minimum discrimination information (MDI) estimation criterion for speech recognition and notes an improvement in terms of perplexity and word error rate (WER). Latent Dirichlet Allocation (LDA) techniques have been proposed as an alternative to PLSA to construct purely generative models. LDA techniques include variational Bayes (Blei et al., 2003) and HMM-LDA (Hsu and Glass, 2006). Recently, bilingual approaches to topic modeling have also been proposed. A Hidden Markov Bilingual Topic AdMixture (HM-BiTAM) model is proposed by Zhao and Xing (2008), which constructs a generative model in which words from a target language are sampled from a mixture of topics drawn from a Dirichlet distribution. Foreign words are sampled via alignment links from a first-order Markov process and a topic specific translation lexicon. While HM-BiTAM has been used for bilingual topic extraction and topic-specific lexicon mapping in the context of SMT, Zhao and Xing (2008) note that HM-BiTAM can generate unigram language models for both the source and target language and thus can be used for language model adaptation through MDI in a similar manner as outlined in Federico (2002). Another bilingual LSA approach is proposed by Tam et al. (2007), which consists of two hierarchical LDA models, constructed from parallel document corpora. A one-to-one correspondence between LDA models is enforced by learning the hyperparameters of the variational Dirichlet posteriors in one LDA model and bootstrapping the second model by fixing the hyperparameters. The technique is based on the assumption that the topic distributions of the source and target documents are identical. It is shown by Tam et al. (2007) that the bilingual LSA framework is also capable of adapting the translation model. Their work is extended in Tam and Schultz (2009) by constructing parallel document clusters formed by monolingual documents using M parallel seed documents.
Additionally, Gong et al. (2010) propose translation model adaptation via a monolingual LDA training. A monolingual LDA model is trained from either the source or target side of the training corpus and each phrase pair is assigned a phrase-topic distribution based on:
&#710; M j i = wj k &#183; M j i &#8721; m , (1)
k=1 wj k
where M j is the topic distribution of document j and w k is the number of occurrences of phrase pair X k in document j.
Mimno et al. (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles). Documents are grouped into tuples w = (w 1 , ..., w L ) for each language l = 1, ..., L. Each document w l in tuple w is assumed to have the same topic distribution, drawn from an asymmetric Dirichlet prior. Tuple-specific topic distributions are learned using LDA with distinct topic-word concentration parameters &#946; l . Mimno et al. (2009) show that PLTM sufficiently aligns topics in parallel corpora.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Zhao et al. (2004) construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolin-</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>gual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model.</text>
              <doc_id>23</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Sethy et al. (2006), domain-specific language models are obtained by including only the sentences that are similar to the ones in the target domain via a relative entropy based criterion.</text>
              <doc_id>24</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Researchers such as Foster and Kuhn (2007) and Koehn and Schroeder (2007) have investigated mixture model approaches to adaptation.</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Foster and Kuhn (2007) use a mixture model approach that involves splitting a training corpus into different components, training separate models on each component, and applying mixture weights as a function of the distances of each component to the source text.</text>
              <doc_id>26</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Koehn and Schroeder (2007) learn mixture weights for language models trained with in-domain and outof-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models.</text>
              <doc_id>27</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Although the application of mixture models yields significant results, the number of mixture weights to learn grows linearly with the number of independent language models applied.</text>
              <doc_id>28</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Most works focus on monolingual language model adaptation in the context of automatic speech recognition.</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Federico (2002) combines Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999) for topic modeling with the minimum discrimination information (MDI) estimation criterion for speech recognition and notes an improvement in terms of perplexity and word error rate (WER).</text>
              <doc_id>30</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Latent Dirichlet Allocation (LDA) techniques have been proposed as an alternative to PLSA to construct purely generative models.</text>
              <doc_id>31</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>LDA techniques include variational Bayes (Blei et al., 2003) and HMM-LDA (Hsu and Glass, 2006).</text>
              <doc_id>32</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Recently, bilingual approaches to topic modeling have also been proposed.</text>
              <doc_id>33</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>A Hidden Markov Bilingual Topic AdMixture (HM-BiTAM) model is proposed by Zhao and Xing (2008), which constructs a generative model in which words from a target language are sampled from a mixture of topics drawn from a Dirichlet distribution.</text>
              <doc_id>34</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Foreign words are sampled via alignment links from a first-order Markov process and a topic specific translation lexicon.</text>
              <doc_id>35</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>While HM-BiTAM has been used for bilingual topic extraction and topic-specific lexicon mapping in the context of SMT, Zhao and Xing (2008) note that HM-BiTAM can generate unigram language models for both the source and target language and thus can be used for language model adaptation through MDI in a similar manner as outlined in Federico (2002).</text>
              <doc_id>36</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Another bilingual LSA approach is proposed by Tam et al. (2007), which consists of two hierarchical LDA models, constructed from parallel document corpora.</text>
              <doc_id>37</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>A one-to-one correspondence between LDA models is enforced by learning the hyperparameters of the variational Dirichlet posteriors in one LDA model and bootstrapping the second model by fixing the hyperparameters.</text>
              <doc_id>38</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>The technique is based on the assumption that the topic distributions of the source and target documents are identical.</text>
              <doc_id>39</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>It is shown by Tam et al. (2007) that the bilingual LSA framework is also capable of adapting the translation model.</text>
              <doc_id>40</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Their work is extended in Tam and Schultz (2009) by constructing parallel document clusters formed by monolingual documents using M parallel seed documents.</text>
              <doc_id>41</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Additionally, Gong et al. (2010) propose translation model adaptation via a monolingual LDA training.</text>
              <doc_id>42</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A monolingual LDA model is trained from either the source or target side of the training corpus and each phrase pair is assigned a phrase-topic distribution based on:</text>
              <doc_id>43</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#710; M j i = wj k &#183; M j i &#8721; m , (1)</text>
              <doc_id>44</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>k=1 wj k</text>
              <doc_id>45</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where M j is the topic distribution of document j and w k is the number of occurrences of phrase pair X k in document j.</text>
              <doc_id>46</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Mimno et al. (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles).</text>
              <doc_id>47</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Documents are grouped into tuples w = (w 1 , ..., w L ) for each language l = 1, ..., L.</text>
              <doc_id>48</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Each document w l in tuple w is assumed to have the same topic distribution, drawn from an asymmetric Dirichlet prior.</text>
              <doc_id>49</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Tuple-specific topic distributions are learned using LDA with distinct topic-word concentration parameters &#946; l .</text>
              <doc_id>50</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Mimno et al. (2009) show that PLTM sufficiently aligns topics in parallel corpora.</text>
              <doc_id>51</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Topic Modeling</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>52</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 PLSA</title>
            <text>The original idea of LSA is to map documents to a latent semantic space, which reduces the dimensionality by means of singular value decomposition (Deerwester et al., 1990). A word-document matrix A is decomposed by the formula A = U&#931;V t , where U and V are orthogonal matrices with unit-length columns and &#931; is a diagonal matrix containing the singular values of A. LSA approximates &#931; by casting all but the largest k singular values in &#931; to zero.
PLSA is a statistical model based on the likelihood principle that incorporates mixing proportions of latent class variables (or topics) for each observation. In the context of topic modeling, the latent class variables z &#8712; Z = {z 1 , ..., z k } correspond to topics, from which we can derive probabilistic distributions of words w &#8712; W = {w 1 , ..., w m } in a document d &#8712; D = {d 1 , ..., d n } with k &lt;&lt; n. Thus, the goal is to learn P (z | d) and P (w|z) by maximizing the log-likelihood function:
L(W, D) = &#8721; d&#8712;D &#8721;
n(w, d) log P (w | d), (2)
w&#8712;W
where n(w, d) is the term frequency of w in d. Using Bayes&#8217; formula, the conditional probability P (w | d) is defined as:
P (w | d) = &#8721; z&#8712;Z P (w | z)P (z | d). (3)
Using the Expectation Maximization (EM) algorithm (Dempster et al., 1977), we estimate the parameters P (z|d) and P (w|z) via an iterative process that alternates two steps: (i) an expectation step (E) in which posterior probabilities are computed for each latent topic z; and (ii) a maximization (M) step, in which the parameters are updated for the posterior probabilities computed in the previous E-step. Details of how to efficiently implement the re-estimation formulas can be found in Federico (2002).
Iterating the E- and M-steps will lead to a convergence that approximates the maximum likelihood equation in (2).
A document-topic distribution &#710;&#952; can be inferred on a new document d &#8242; by maximizing the following equation: &#8721;
&#710;&#952; = arg max n(w, d &#8242; ) log &#8721; P (w | z)&#952; z,d &#8242;,
&#952; w z
(4) where &#952; z,d &#8242; = P (z | d &#8242; ). (4) can be maximized by performing Expectation Maximization on document d &#8242; by keeping fixed the word-topic distributions already estimated on the training data. Consequently, a word-document distribution can be inferred by applying the mixture model (3) (see Federico, 2002 for details).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The original idea of LSA is to map documents to a latent semantic space, which reduces the dimensionality by means of singular value decomposition (Deerwester et al., 1990).</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A word-document matrix A is decomposed by the formula A = U&#931;V t , where U and V are orthogonal matrices with unit-length columns and &#931; is a diagonal matrix containing the singular values of A. LSA approximates &#931; by casting all but the largest k singular values in &#931; to zero.</text>
                  <doc_id>54</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>PLSA is a statistical model based on the likelihood principle that incorporates mixing proportions of latent class variables (or topics) for each observation.</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the context of topic modeling, the latent class variables z &#8712; Z = {z 1 , ..., z k } correspond to topics, from which we can derive probabilistic distributions of words w &#8712; W = {w 1 , ..., w m } in a document d &#8712; D = {d 1 , ..., d n } with k &lt;&lt; n.</text>
                  <doc_id>56</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Thus, the goal is to learn P (z | d) and P (w|z) by maximizing the log-likelihood function:</text>
                  <doc_id>57</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(W, D) = &#8721; d&#8712;D &#8721;</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n(w, d) log P (w | d), (2)</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w&#8712;W</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where n(w, d) is the term frequency of w in d.</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Using Bayes&#8217; formula, the conditional probability P (w | d) is defined as:</text>
                  <doc_id>62</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (w | d) = &#8721; z&#8712;Z P (w | z)P (z | d).</text>
                  <doc_id>63</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(3)</text>
                  <doc_id>64</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Using the Expectation Maximization (EM) algorithm (Dempster et al., 1977), we estimate the parameters P (z|d) and P (w|z) via an iterative process that alternates two steps: (i) an expectation step (E) in which posterior probabilities are computed for each latent topic z; and (ii) a maximization (M) step, in which the parameters are updated for the posterior probabilities computed in the previous E-step.</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Details of how to efficiently implement the re-estimation formulas can be found in Federico (2002).</text>
                  <doc_id>66</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Iterating the E- and M-steps will lead to a convergence that approximates the maximum likelihood equation in (2).</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A document-topic distribution &#710;&#952; can be inferred on a new document d &#8242; by maximizing the following equation: &#8721;</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#710;&#952; = arg max n(w, d &#8242; ) log &#8721; P (w | z)&#952; z,d &#8242;,</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#952; w z</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(4) where &#952; z,d &#8242; = P (z | d &#8242; ).</text>
                  <doc_id>71</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(4) can be maximized by performing Expectation Maximization on document d &#8242; by keeping fixed the word-topic distributions already estimated on the training data.</text>
                  <doc_id>72</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Consequently, a word-document distribution can be inferred by applying the mixture model (3) (see Federico, 2002 for details).</text>
                  <doc_id>73</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 MDI Adaptation</title>
        <text>An n-gram language model approximates the probability of a sequence of words in a text W T 1 = w 1 , ..., w T drawn from a vocabulary V by the following equation:
P (W T 1 ) = T&#8719;
P (w i |h i ), (5)
i=1
where h i = w i&#8722;n+1 , ..., w i&#8722;1 is the history of n &#8722; 1 words preceding w i . Given a training corpus B, we can compute the probability of a n-gram from a smoothed model via interpolation as:
P B (w|h) = f &#8727; B(w|h) + &#955; B (h)P B (w|h &#8242; ), (6)
where fB &#8727; (w|h) is the discounted frequency of sequence hw, h &#8242; is the lower order history, where |h|&#8722;1 = |h &#8242; |, and &#955; B (h) is the zero-frequency probability of h, defined as:
&#955; B (h) = 1.0 &#8722; &#8721; w&#8712;V f &#8727; B(w|h).
Federico (1999) has shown that MDI Adaptation is useful to adapt a background language model with a small adaptation text sample A, by assuming to have only sufficient statistics on unigrams. Thus, we can reliably estimate &#710;P A (w) constraints on the marginal distribution of an adapted language model P A (h, w) which minimizes the Kullback- Leibler distance from B, i.e.:
P A (&#183;) = arg min
Q(&#183;)
&#8721; Q(h, w) Q(h, w) log P
hw&#8712;V n B (h, w) . (7)
The joint distribution in (7) can be computed using Generalized Iterative Scaling (Darroch and Ratcliff, 1972). Under the unigram constraints, the GIS algorithm reduces to the closed form:
where
P A (h, w) = P B (h, w)&#945;(w), (8)
&#945;(w) = &#710;P A (w) P B (w) . (9) In order to estimate the conditional distribution of the adapted LM, we rewrite (8) and simplify the equation to:
P A (w|h) = P B (w|h)&#945;(w)
&#8721;&#373;&#8712;V P B(&#373;|h)&#945;(&#373;) . (10)
The adaptation model can be improved by smoothing the scaling factor in (9) by an exponential term &#947; (Kneser et al., 1997): ( ) &#947; &#710;PA (w)
&#945;(w) = , (11) P B (w)
where 0 &lt; &#947; &#8804; 1. Empirically, &#947; values less than one decrease the effect of the adaptation ratio to reduce the bias.
As outlined in Federico (2002), the adapted language model can also be written in an interpolation form:
fA(w|h) &#8727; = f B &#8727; (w|h)&#945;(w) , (12) z(h)
&#955; A (h) = &#955; B(h)z(h &#8242; ) , (13) z(h)
z(h) = ( &#8721;
w:N B (h,w)&gt;0
f &#8727; B(w|h)&#945;(w)) + &#955; B (h)z(h &#8242; ),
(14)
which permits to efficiently compute the normalization term for high order n-grams recursively and by just summing over observed n-grams. The recursion ends with the following initial values for the empty history &#603;:
MDI adaptation is one of the adaptation methods provided by the IRSTLM toolkit and was applied as explained in the following section.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>An n-gram language model approximates the probability of a sequence of words in a text W T 1 = w 1 , ..., w T drawn from a vocabulary V by the following equation:</text>
              <doc_id>74</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (W T 1 ) = T&#8719;</text>
              <doc_id>75</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (w i |h i ), (5)</text>
              <doc_id>76</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1</text>
              <doc_id>77</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where h i = w i&#8722;n+1 , ..., w i&#8722;1 is the history of n &#8722; 1 words preceding w i .</text>
              <doc_id>78</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Given a training corpus B, we can compute the probability of a n-gram from a smoothed model via interpolation as:</text>
              <doc_id>79</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P B (w|h) = f &#8727; B(w|h) + &#955; B (h)P B (w|h &#8242; ), (6)</text>
              <doc_id>80</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where fB &#8727; (w|h) is the discounted frequency of sequence hw, h &#8242; is the lower order history, where |h|&#8722;1 = |h &#8242; |, and &#955; B (h) is the zero-frequency probability of h, defined as:</text>
              <doc_id>81</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#955; B (h) = 1.0 &#8722; &#8721; w&#8712;V f &#8727; B(w|h).</text>
              <doc_id>82</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Federico (1999) has shown that MDI Adaptation is useful to adapt a background language model with a small adaptation text sample A, by assuming to have only sufficient statistics on unigrams.</text>
              <doc_id>83</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Thus, we can reliably estimate &#710;P A (w) constraints on the marginal distribution of an adapted language model P A (h, w) which minimizes the Kullback- Leibler distance from B, i.e.:</text>
              <doc_id>84</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P A (&#183;) = arg min</text>
              <doc_id>85</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Q(&#183;)</text>
              <doc_id>86</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8721; Q(h, w) Q(h, w) log P</text>
              <doc_id>87</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>hw&#8712;V n B (h, w) .</text>
              <doc_id>88</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>(7)</text>
              <doc_id>89</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The joint distribution in (7) can be computed using Generalized Iterative Scaling (Darroch and Ratcliff, 1972).</text>
              <doc_id>90</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Under the unigram constraints, the GIS algorithm reduces to the closed form:</text>
              <doc_id>91</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where</text>
              <doc_id>92</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P A (h, w) = P B (h, w)&#945;(w), (8)</text>
              <doc_id>93</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#945;(w) = &#710;P A (w) P B (w) .</text>
              <doc_id>94</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>(9) In order to estimate the conditional distribution of the adapted LM, we rewrite (8) and simplify the equation to:</text>
              <doc_id>95</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P A (w|h) = P B (w|h)&#945;(w)</text>
              <doc_id>96</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8721;&#373;&#8712;V P B(&#373;|h)&#945;(&#373;) .</text>
              <doc_id>97</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>(10)</text>
              <doc_id>98</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The adaptation model can be improved by smoothing the scaling factor in (9) by an exponential term &#947; (Kneser et al., 1997): ( ) &#947; &#710;PA (w)</text>
              <doc_id>99</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#945;(w) = , (11) P B (w)</text>
              <doc_id>100</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where 0 &lt; &#947; &#8804; 1.</text>
              <doc_id>101</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Empirically, &#947; values less than one decrease the effect of the adaptation ratio to reduce the bias.</text>
              <doc_id>102</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>As outlined in Federico (2002), the adapted language model can also be written in an interpolation form:</text>
              <doc_id>103</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>fA(w|h) &#8727; = f B &#8727; (w|h)&#945;(w) , (12) z(h)</text>
              <doc_id>104</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#955; A (h) = &#955; B(h)z(h &#8242; ) , (13) z(h)</text>
              <doc_id>105</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>z(h) = ( &#8721;</text>
              <doc_id>106</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>w:N B (h,w)&gt;0</text>
              <doc_id>107</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>f &#8727; B(w|h)&#945;(w)) + &#955; B (h)z(h &#8242; ),</text>
              <doc_id>108</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(14)</text>
              <doc_id>109</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>which permits to efficiently compute the normalization term for high order n-grams recursively and by just summing over observed n-grams.</text>
              <doc_id>110</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The recursion ends with the following initial values for the empty history &#603;:</text>
              <doc_id>111</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>MDI adaptation is one of the adaptation methods provided by the IRSTLM toolkit and was applied as explained in the following section.</text>
              <doc_id>112</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>5 Bilingual Latent Semantic Models</title>
        <text>Similar to the treatment of documents in HM- BiTAM (Zhao and Xing, 2008), we combine parallel texts into a document-pair (E, F) containing n parallel sentence pairs (e i , f i ), 1 &lt; i &#8804; n, corresponding to the source and target languages, respectively. Based on the assumption that the topics in a parallel text share the same semantic meanings across languages, the topics are sampled from the same topicdocument distribution. We make the additional assumption that stop-words and punctuation, although having high word frequencies in documents, will generally have a uniform topic distribution across documents; therefore, it is not necessary to remove them prior to model training, as they will not adversely affect the overall topic distribution in each document. In order to ensure the uniqueness between word tokens between languages, we annotate E with special characters. We perform PLSA training, as described in Section 3.1 and receive wordtopic distributions P (w|z), w &#8712; V E &#8746; V F Given an untranslated text &#202;, we split &#202; into a sequence of documents D. For each document d i &#8712; D, we infer a full word-document distribution by learning &#710;&#952; via (4). Via (3), we can generate the full word-document distribution P (w | d) for w &#8712; V F .
We then convert the word-document probabilities into pseudo-counts via a scaling function:
n(w | d) =
P (w | d) max w &#8242; P (w &#8242; &#183; &#8710;, (17) | d)
where &#8710; is a scaling factor to raise the probability ratios above 1. Since our goal is to generate a unigram language model on the target language for adaptation, we remove the source words generated in (17) prior to building the language model.
From our newly generated unigram language model, we perform MDI adaptation on the background LM to yield an adapted LM for translating the source document used for the PLSA inference step.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Similar to the treatment of documents in HM- BiTAM (Zhao and Xing, 2008), we combine parallel texts into a document-pair (E, F) containing n parallel sentence pairs (e i , f i ), 1 &lt; i &#8804; n, corresponding to the source and target languages, respectively.</text>
              <doc_id>113</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Based on the assumption that the topics in a parallel text share the same semantic meanings across languages, the topics are sampled from the same topicdocument distribution.</text>
              <doc_id>114</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We make the additional assumption that stop-words and punctuation, although having high word frequencies in documents, will generally have a uniform topic distribution across documents; therefore, it is not necessary to remove them prior to model training, as they will not adversely affect the overall topic distribution in each document.</text>
              <doc_id>115</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In order to ensure the uniqueness between word tokens between languages, we annotate E with special characters.</text>
              <doc_id>116</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We perform PLSA training, as described in Section 3.1 and receive wordtopic distributions P (w|z), w &#8712; V E &#8746; V F Given an untranslated text &#202;, we split &#202; into a sequence of documents D.</text>
              <doc_id>117</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>For each document d i &#8712; D, we infer a full word-document distribution by learning &#710;&#952; via (4).</text>
              <doc_id>118</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Via (3), we can generate the full word-document distribution P (w | d) for w &#8712; V F .</text>
              <doc_id>119</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We then convert the word-document probabilities into pseudo-counts via a scaling function:</text>
              <doc_id>120</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>n(w | d) =</text>
              <doc_id>121</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (w | d) max w &#8242; P (w &#8242; &#183; &#8710;, (17) | d)</text>
              <doc_id>122</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where &#8710; is a scaling factor to raise the probability ratios above 1.</text>
              <doc_id>123</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Since our goal is to generate a unigram language model on the target language for adaptation, we remove the source words generated in (17) prior to building the language model.</text>
              <doc_id>124</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>From our newly generated unigram language model, we perform MDI adaptation on the background LM to yield an adapted LM for translating the source document used for the PLSA inference step.</text>
              <doc_id>125</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>6 Experiments</title>
        <text>Our experiments were done using the TED Talks collection, used in the IWSLT 2010 evaluation task 1 .
1 http://iwslt2010.fbk.eu/
In IWSLT 2010, the challenge was to translate talks from the TED website 2 from English to French. The talks include a variety of topics, including photography and pyschology and thus do not adhere to a single genre. All talks were given in English and were manually transcribed and translated into French. The TED training data consists of 329 parallel talk transcripts with approximately 84k sentences. The TED test data consists of transcriptions created via 1-best ASR outputs from the KIT Quaero Evaluation System. It consists of 758 sentences and 27,432 and 27,307 English and French words, respectively. The TED talk data is segmented at the clause level, rather than at the level of sentences.
Our SMT systems are built upon the Moses opensource SMT toolkit (Koehn et al., 2007) 3 . The translation and lexicalized reordering models have been trained on parallel data. One 5-gram background LM was constructed from the French side of the TED training data (740k words), smoothed with the improved Kneser-Ney technique (Chen and Goodman, 1999) and computed with the IRSTLM toolkit (Federico et al., 2008). The weights of the log-linear interpolation model were optimized via minimum error rate training (MERT) (Och, 2003) on the TED development set, using 200 best translations at each tuning iteration. This paper investigates the effects of language model adaptation via bilingual latent semantic modeling on the TED background LM against a baseline model that uses only the TED LM.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Our experiments were done using the TED Talks collection, used in the IWSLT 2010 evaluation task 1 .</text>
              <doc_id>126</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 http://iwslt2010.fbk.eu/</text>
              <doc_id>127</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In IWSLT 2010, the challenge was to translate talks from the TED website 2 from English to French.</text>
              <doc_id>128</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The talks include a variety of topics, including photography and pyschology and thus do not adhere to a single genre.</text>
              <doc_id>129</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>All talks were given in English and were manually transcribed and translated into French.</text>
              <doc_id>130</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The TED training data consists of 329 parallel talk transcripts with approximately 84k sentences.</text>
              <doc_id>131</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The TED test data consists of transcriptions created via 1-best ASR outputs from the KIT Quaero Evaluation System.</text>
              <doc_id>132</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>It consists of 758 sentences and 27,432 and 27,307 English and French words, respectively.</text>
              <doc_id>133</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The TED talk data is segmented at the clause level, rather than at the level of sentences.</text>
              <doc_id>134</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Our SMT systems are built upon the Moses opensource SMT toolkit (Koehn et al., 2007) 3 .</text>
              <doc_id>135</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The translation and lexicalized reordering models have been trained on parallel data.</text>
              <doc_id>136</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>One 5-gram background LM was constructed from the French side of the TED training data (740k words), smoothed with the improved Kneser-Ney technique (Chen and Goodman, 1999) and computed with the IRSTLM toolkit (Federico et al., 2008).</text>
              <doc_id>137</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The weights of the log-linear interpolation model were optimized via minimum error rate training (MERT) (Och, 2003) on the TED development set, using 200 best translations at each tuning iteration.</text>
              <doc_id>138</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This paper investigates the effects of language model adaptation via bilingual latent semantic modeling on the TED background LM against a baseline model that uses only the TED LM.</text>
              <doc_id>139</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>6.1 Bilingual Latent Semantic Model</title>
            <text>Using the technique outlined in Section 5, we construct bilingual documents by splitting the parallel TED training corpus into 41,847 documents of 5 lines each. While each individual TED lecture could be used as a document, our experimental goal is to simulate near-time translation of speeches; thus, we prefer to construct small documents to simulate topic modeling on a spoken language scenario in which the length of a talk is not known a priori. We annotate the English source text for removal after inference. Figure 1 contains a sample document constructed for PLSA training. (In fact, we distin-
2 http://www.ted.com/talks/ 3 http://www.statmt.org/moses/
robert lang is a pioneer of the newest kind of origami &#8211; using math and engineering principles to fold mind-blowingly
intricate designs that are beautiful and , sometimes , very
useful . my talk is &#8221; flapping birds and space telescopes .
&#8221; and you would think that should have nothing to do with
one another , but i hope by the end of these 18 minutes
, you &#8217;ll see a little bit of a relation . robert lang est un
pionnier des nouvelles techniques d&#8217; origami - bas&#233;es sur
des principes math&#233;matiques et d&#8217; ing&#233;nierie permettant de
cr&#233;er des mod&#232;les complexes et &#233;poustouflants , qui sont
beaux et parfois , tr&#232;s utiles . ma conf&#233;rence s&#8217; intitule &#8221;
oiseaux en papier et t&#233;lescopes spatiaux &#8221; . et vous pensez
probablement que les uns et les autres n&#8217; ont rien en commun , mais j&#8217; esp&#232;re qu&#8217; &#224; l&#8217; issue de ces 18 minutes , vous
comprendrez ce qui les relie .
guish English words from French words by attaching to the former a special suffix.) By using our inhouse implementation, training of the PLSA model on the bilingual collection converged after 20 EM iterations.
Using our PLSA model, we run inference on each of the 476 test documents from the TED lectures, constructed by splitting the test set into 5-line documents. Since our goal is to translate and evaluate the test set, we construct monolingual (English) documents. Figure 2 provides an example of a document to be inferred. We collect the bilingual unigram pseudocounts after 10 iterations of inference and remove the English words. The TED lecture data is transcribed by clauses, rather than full sentences, so we do not add sentence splitting tags before training our unigram language models.
As a result of PLSA inference, the probabilities of target words increase with respect to the background language model. Table 1 demonstrates this phenomenon by outlining several of the top ranked words that have similar semantic meaning to nonstop words on the source side. In every case, the probability P A (w) increases fairly substantially with respect to the P B (w). As a result, we expect that the adapted language model will favor both fluent and semantically correct translations as the adaptation is suggesting better lexical choices of words.
we didn &#8217;t have money , so we had a cheap , little ad , but we
wanted college students for a study of prison life . 75 people volunteered , took personality tests . we did interviews .
picked two dozen : the most normal , the most healthy .</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Using the technique outlined in Section 5, we construct bilingual documents by splitting the parallel TED training corpus into 41,847 documents of 5 lines each.</text>
                  <doc_id>140</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>While each individual TED lecture could be used as a document, our experimental goal is to simulate near-time translation of speeches; thus, we prefer to construct small documents to simulate topic modeling on a spoken language scenario in which the length of a talk is not known a priori.</text>
                  <doc_id>141</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We annotate the English source text for removal after inference.</text>
                  <doc_id>142</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 1 contains a sample document constructed for PLSA training.</text>
                  <doc_id>143</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>(In fact, we distin-</text>
                  <doc_id>144</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 http://www.ted.com/talks/ 3 http://www.statmt.org/moses/</text>
                  <doc_id>145</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>robert lang is a pioneer of the newest kind of origami &#8211; using math and engineering principles to fold mind-blowingly</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>intricate designs that are beautiful and , sometimes , very</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>useful .</text>
                  <doc_id>148</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>my talk is &#8221; flapping birds and space telescopes .</text>
                  <doc_id>149</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8221; and you would think that should have nothing to do with</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>one another , but i hope by the end of these 18 minutes</text>
                  <doc_id>151</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>, you &#8217;ll see a little bit of a relation .</text>
                  <doc_id>152</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>robert lang est un</text>
                  <doc_id>153</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>pionnier des nouvelles techniques d&#8217; origami - bas&#233;es sur</text>
                  <doc_id>154</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>des principes math&#233;matiques et d&#8217; ing&#233;nierie permettant de</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>cr&#233;er des mod&#232;les complexes et &#233;poustouflants , qui sont</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>beaux et parfois , tr&#232;s utiles .</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>ma conf&#233;rence s&#8217; intitule &#8221;</text>
                  <doc_id>158</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>oiseaux en papier et t&#233;lescopes spatiaux &#8221; .</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>et vous pensez</text>
                  <doc_id>160</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>probablement que les uns et les autres n&#8217; ont rien en commun , mais j&#8217; esp&#232;re qu&#8217; &#224; l&#8217; issue de ces 18 minutes , vous</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>comprendrez ce qui les relie .</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>guish English words from French words by attaching to the former a special suffix.</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>) By using our inhouse implementation, training of the PLSA model on the bilingual collection converged after 20 EM iterations.</text>
                  <doc_id>164</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Using our PLSA model, we run inference on each of the 476 test documents from the TED lectures, constructed by splitting the test set into 5-line documents.</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Since our goal is to translate and evaluate the test set, we construct monolingual (English) documents.</text>
                  <doc_id>166</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 2 provides an example of a document to be inferred.</text>
                  <doc_id>167</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We collect the bilingual unigram pseudocounts after 10 iterations of inference and remove the English words.</text>
                  <doc_id>168</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The TED lecture data is transcribed by clauses, rather than full sentences, so we do not add sentence splitting tags before training our unigram language models.</text>
                  <doc_id>169</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As a result of PLSA inference, the probabilities of target words increase with respect to the background language model.</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Table 1 demonstrates this phenomenon by outlining several of the top ranked words that have similar semantic meaning to nonstop words on the source side.</text>
                  <doc_id>171</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In every case, the probability P A (w) increases fairly substantially with respect to the P B (w).</text>
                  <doc_id>172</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>As a result, we expect that the adapted language model will favor both fluent and semantically correct translations as the adaptation is suggesting better lexical choices of words.</text>
                  <doc_id>173</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>we didn &#8217;t have money , so we had a cheap , little ad , but we</text>
                  <doc_id>174</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>wanted college students for a study of prison life .</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>75 people volunteered , took personality tests .</text>
                  <doc_id>176</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>we did interviews .</text>
                  <doc_id>177</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>picked two dozen : the most normal , the most healthy .</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>6.2 MDI Adaptation</title>
            <text>We perform MDI adaptation with each of the unigram language models to update the background TED language model. We configure the adaptation rate parameter &#947; to 0.3, as recommended in Federico (2002). The baseline LM is replaced with each adapted LM, corresponding to the document to be translated. We then calculate the mean perplexity of the adapted LMs and the baseline, respectively. The perplexity scores are shown in Table 2. We observe a 15.3% relative improvement in perplexity score over the baseline.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We perform MDI adaptation with each of the unigram language models to update the background TED language model.</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We configure the adaptation rate parameter &#947; to 0.3, as recommended in Federico (2002).</text>
                  <doc_id>180</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The baseline LM is replaced with each adapted LM, corresponding to the document to be translated.</text>
                  <doc_id>181</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We then calculate the mean perplexity of the adapted LMs and the baseline, respectively.</text>
                  <doc_id>182</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The perplexity scores are shown in Table 2.</text>
                  <doc_id>183</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We observe a 15.3% relative improvement in perplexity score over the baseline.</text>
                  <doc_id>184</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>6.3 Results</title>
            <text>We perform MT experiments on the IWSLT 2010 evaluation set to compare the baseline and adapted LMs. In the evaluation, we notice a 0.85 improvement in BLEU (%), yielding a 3% improvement over the baseline. The same performance trend in NIST is observed with a 2.4% relative improvement compared to the unadapted baseline. Our PLSA and MDI-based adaptation method not only improves fluency but also improves adequacy: the topicbased adaptation approach is attempting to suggest more appropriate words based on increased unigram probabilities than that of the baseline LM. Table 3 demonstrates a large improvement in unigram selection for the adapted TED model in terms of the individual contribution to the NIST score, with diminishing effects on larger n-grams. The majority of the overall improvements are on individual word selection.
Examples of improved fluency and adequacy are shown in Figure 3. Line 285 shows an example of a translation that doesn&#8217;t provide much of an n-gram improvement, but demonstrates more fluent output, due to the deletion of the first comma and the movement of the second comma to the end of the clause. While &#8220;installation&#8221; remains an inadequate noun in this clause, the adapted model reorders the root words &#8220;rehab&#8221; and &#8220;installation&#8221; (in comparison with the baseline) and improves the grammaticality of the sentence; however, the number does not match between the determiner and the noun phrase. Line 597 demonstrates a perfect phrase translation with respect to the reference translation using semantic paraphrasing. The baseline phrase &#8220;d&#8217;origine&#8221; is transformed and attributed to the noun. Instead of translating &#8220;original&#8221; as a phrase for &#8220;home&#8221;, the adapted model captures the original meaning of the word in the translation. Line 752 demonstrates an improvement in adequacy through the replacement of the word &#8220;quelque&#8221; with &#8220;autre.&#8221; Additionally, extra words are removed.
These lexical changes result in the improvement in translation quality due to topic-based adaptation via PLSA.
(Line 285)
, j&#8217; ai eu la chance de travailler dans les installations , rehab
j&#8217; ai eu la chance de travailler dans les rehab installation ,
j&#8217; ai la chance de travailler dans un centre de d&#233;sintoxication ,
(Line 597)
d&#8217; origine , les id&#233;es qui ont de la valeur &#8211;
d&#8217; avoir des id&#233;es originales qui ont de la valeur &#8211;
d&#8217; avoir des id&#233;es originales qui ont de la valeur &#8211;
(Line 752)
un nom qui appartient &#224; quelque chose d&#8217; autre , le soleil .
un nom qui appartient &#224; autre chose , le soleil .
le nom d&#8217; une autre chose , le soleil .</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We perform MT experiments on the IWSLT 2010 evaluation set to compare the baseline and adapted LMs.</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the evaluation, we notice a 0.85 improvement in BLEU (%), yielding a 3% improvement over the baseline.</text>
                  <doc_id>186</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The same performance trend in NIST is observed with a 2.4% relative improvement compared to the unadapted baseline.</text>
                  <doc_id>187</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Our PLSA and MDI-based adaptation method not only improves fluency but also improves adequacy: the topicbased adaptation approach is attempting to suggest more appropriate words based on increased unigram probabilities than that of the baseline LM.</text>
                  <doc_id>188</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Table 3 demonstrates a large improvement in unigram selection for the adapted TED model in terms of the individual contribution to the NIST score, with diminishing effects on larger n-grams.</text>
                  <doc_id>189</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The majority of the overall improvements are on individual word selection.</text>
                  <doc_id>190</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Examples of improved fluency and adequacy are shown in Figure 3.</text>
                  <doc_id>191</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Line 285 shows an example of a translation that doesn&#8217;t provide much of an n-gram improvement, but demonstrates more fluent output, due to the deletion of the first comma and the movement of the second comma to the end of the clause.</text>
                  <doc_id>192</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>While &#8220;installation&#8221; remains an inadequate noun in this clause, the adapted model reorders the root words &#8220;rehab&#8221; and &#8220;installation&#8221; (in comparison with the baseline) and improves the grammaticality of the sentence; however, the number does not match between the determiner and the noun phrase.</text>
                  <doc_id>193</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Line 597 demonstrates a perfect phrase translation with respect to the reference translation using semantic paraphrasing.</text>
                  <doc_id>194</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The baseline phrase &#8220;d&#8217;origine&#8221; is transformed and attributed to the noun.</text>
                  <doc_id>195</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Instead of translating &#8220;original&#8221; as a phrase for &#8220;home&#8221;, the adapted model captures the original meaning of the word in the translation.</text>
                  <doc_id>196</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Line 752 demonstrates an improvement in adequacy through the replacement of the word &#8220;quelque&#8221; with &#8220;autre.&#8221; Additionally, extra words are removed.</text>
                  <doc_id>197</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>These lexical changes result in the improvement in translation quality due to topic-based adaptation via PLSA.</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(Line 285)</text>
                  <doc_id>199</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>, j&#8217; ai eu la chance de travailler dans les installations , rehab</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j&#8217; ai eu la chance de travailler dans les rehab installation ,</text>
                  <doc_id>201</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j&#8217; ai la chance de travailler dans un centre de d&#233;sintoxication ,</text>
                  <doc_id>202</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(Line 597)</text>
                  <doc_id>203</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>d&#8217; origine , les id&#233;es qui ont de la valeur &#8211;</text>
                  <doc_id>204</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>d&#8217; avoir des id&#233;es originales qui ont de la valeur &#8211;</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>d&#8217; avoir des id&#233;es originales qui ont de la valeur &#8211;</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(Line 752)</text>
                  <doc_id>207</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>un nom qui appartient &#224; quelque chose d&#8217; autre , le soleil .</text>
                  <doc_id>208</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>un nom qui appartient &#224; autre chose , le soleil .</text>
                  <doc_id>209</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>le nom d&#8217; une autre chose , le soleil .</text>
                  <doc_id>210</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>7</index>
        <title>7 Conclusions</title>
        <text>An alternative approach to bilingual topic modeling has been presented that integrates the PLSA framework with MDI adaptation that can effectively adapt a background language model when given a document in the source language. Rather than training two topic models and enforcing a one-to-one correspondence for translation, we use the assumption that parallel texts refer to the same topics and have a very similar topic distribution. Preliminary experiments show a reduction in perplexity and an overall improvement in BLEU and NIST scores on speech translation. We also note that, unlike previous works involving topic modeling, we did not remove stop words and punctuation, but rather assumed that these features would have a relatively uniform topic distribution. One downside to the MDI adaptation approach is that the computation of the normalization term z(h) is expensive and potentially prohibitive during continuous speech translation tasks. Further investigation is needed to determine if there is a suitable approximation that avoids computing probabilities across all n-grams.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>An alternative approach to bilingual topic modeling has been presented that integrates the PLSA framework with MDI adaptation that can effectively adapt a background language model when given a document in the source language.</text>
              <doc_id>211</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Rather than training two topic models and enforcing a one-to-one correspondence for translation, we use the assumption that parallel texts refer to the same topics and have a very similar topic distribution.</text>
              <doc_id>212</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Preliminary experiments show a reduction in perplexity and an overall improvement in BLEU and NIST scores on speech translation.</text>
              <doc_id>213</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We also note that, unlike previous works involving topic modeling, we did not remove stop words and punctuation, but rather assumed that these features would have a relatively uniform topic distribution.</text>
              <doc_id>214</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>One downside to the MDI adaptation approach is that the computation of the normalization term z(h) is expensive and potentially prohibitive during continuous speech translation tasks.</text>
              <doc_id>215</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Further investigation is needed to determine if there is a suitable approximation that avoids computing probabilities across all n-grams.</text>
              <doc_id>216</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>8</index>
        <title>Acknowledgments</title>
        <text>This work was supported by the T4ME network of excellence (IST-249119), funded by the DG INFSO of the European Commission through the Seventh Framework Programme. The first author received a grant under the Erasmus Mundus Language &amp; Communication Technologies programme.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This work was supported by the T4ME network of excellence (IST-249119), funded by the DG INFSO of the European Commission through the Seventh Framework Programme.</text>
              <doc_id>217</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The first author received a grant under the Erasmus Mundus Language &amp; Communication Technologies programme.</text>
              <doc_id>218</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Sample unigram probabilities of the adaptation model for document #230, compared to the baseline unigram probabilities. The French words selected are semantically related to the English words in the adapted document. The PLSA adaptation infers higher unigram probabilities for words with latent topics related to the source document.</caption>
        <reference_text></reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>Rank</cell>
              <cell>Word</cell>
              <cell>P A (w)</cell>
              <cell>P B (w)</cell>
              <cell>P A (w)/P B (w)</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>20</cell>
              <cell>gens</cell>
              <cell>8.41E-03</cell>
              <cell>4.55E-05</cell>
              <cell>184.84</cell>
            </row>
            <row>
              <cell>22</cell>
              <cell>vie</cell>
              <cell>8.30E-03</cell>
              <cell>1.09E-04</cell>
              <cell>76.15</cell>
            </row>
            <row>
              <cell>51</cell>
              <cell>prix</cell>
              <cell>2.59E-03</cell>
              <cell>8.70E-05</cell>
              <cell>29.77</cell>
            </row>
            <row>
              <cell>80</cell>
              <cell>&#233;cole</cell>
              <cell>1.70E-03</cell>
              <cell>6.13E-05</cell>
              <cell>27.73</cell>
            </row>
            <row>
              <cell>83</cell>
              <cell>argent</cell>
              <cell>1.60E-03</cell>
              <cell>3.96E-05</cell>
              <cell>40.04</cell>
            </row>
            <row>
              <cell>86</cell>
              <cell>personnes</cell>
              <cell>1.52E-03</cell>
              <cell>2.75E-04</cell>
              <cell>5.23</cell>
            </row>
            <row>
              <cell>94</cell>
              <cell>aide</cell>
              <cell>1.27E-03</cell>
              <cell>7.71E-05</cell>
              <cell>16.47</cell>
            </row>
            <row>
              <cell>98</cell>
              <cell>&#233;tudiants</cell>
              <cell>1.20E-03</cell>
              <cell>7.12E-05</cell>
              <cell>16.85</cell>
            </row>
            <row>
              <cell>119</cell>
              <cell>march&#233;</cell>
              <cell>9.22E-04</cell>
              <cell>9.10E-05</cell>
              <cell>10.13</cell>
            </row>
            <row>
              <cell>133</cell>
              <cell>&#233;tude</cell>
              <cell>7.63E-04</cell>
              <cell>4.55E-05</cell>
              <cell>16.77</cell>
            </row>
            <row>
              <cell>173</cell>
              <cell>&#233;ducation</cell>
              <cell>5.04E-04</cell>
              <cell>2.97E-05</cell>
              <cell>16.97</cell>
            </row>
            <row>
              <cell>315</cell>
              <cell>prison</cell>
              <cell>2.65E-04</cell>
              <cell>1.98E-05</cell>
              <cell>13.38</cell>
            </row>
            <row>
              <cell>323</cell>
              <cell>universit&#233;</cell>
              <cell>2.60E-04</cell>
              <cell>2.97E-05</cell>
              <cell>8.75</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Perplexity, BLEU, and NIST scores for the baseline and adapted models. The perplexity scores are averaged across each document-specific LM adaptation.</caption>
        <reference_text>In PAGE 6: ... We then calculate the mean perplexity of the adapted LMs and the baseline, respectively. The perplexity scores are shown in  Table2 . We observe a 15....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>via PLSA.</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>LM</cell>
              <cell>Perplexity</cell>
              <cell>BLEU (%)</cell>
              <cell>NIST</cell>
            </row>
            <row>
              <cell>Adapt TED</cell>
              <cell>162.44</cell>
              <cell>28.49</cell>
              <cell>6.5956</cell>
            </row>
            <row>
              <cell>Base TED</cell>
              <cell>191.76</cell>
              <cell>27.64</cell>
              <cell>6.4405</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TET</source>
        <caption>Table 3: Individual unigram NIST scores for n-grams 1-3 of the baseline and adapted models. The improvement of the adapted model over the baseline is listed below.</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>NIST</cell>
              <cell>1-gram</cell>
              <cell>2-gram</cell>
              <cell>3-gram</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Adapt TED</cell>
              <cell>4.8077</cell>
              <cell>1.3925</cell>
              <cell>0.3229</cell>
            </row>
            <row>
              <cell>Base TED</cell>
              <cell>4.6980</cell>
              <cell>1.3527</cell>
              <cell>0.3173</cell>
            </row>
            <row>
              <cell>Difference</cell>
              <cell>0.1097</cell>
              <cell>0.0398</cell>
              <cell>0.0056</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>David M Blei</author>
          <author>Andrew Ng</author>
          <author>Michael Jordan</author>
        </authors>
        <title>Latent Dirichlet Allocation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>1</id>
        <authors/>
        <title>None</title>
        <publication>None</publication>
        <pages>359--393</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>J N Darroch</author>
          <author>D Ratcliff</author>
        </authors>
        <title>Generalized iterative scaling for log-linear models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1972</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>A P Dempster</author>
          <author>N M Laird</author>
          <author>D B Rubin</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1990</date>
      </reference>
      <reference>
        <id>4</id>
        <authors/>
        <title>Maximum-likelihood from incomplete data via the EM algorithm.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1977</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Marcello Federico</author>
        </authors>
        <title>Efficient language model adaptation through MDI estimation.</title>
        <publication>In Proceedings of the 6th European Conference on Speech Communication and Technology,</publication>
        <pages>1583--1586</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Nicola Bertoldi Federico</author>
          <author>Mauro Cettolo</author>
        </authors>
        <title>None</title>
        <publication>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</publication>
        <pages>703--706</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>7</id>
        <authors/>
        <title>IRSTLM: an Open Source Toolkit for Handling Large Scale Language Models.</title>
        <publication>In Proceedings of Interspeech,</publication>
        <pages>1618--1621</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>George Foster</author>
          <author>Roland Kuhn</author>
        </authors>
        <title>Mixture-model adaptation for SMT.</title>
        <publication>In Proceedings of the Second Workshop on Statistical Machine Translation,</publication>
        <pages>128--135</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>9</id>
        <authors/>
        <title>None</title>
        <publication>In Universal Communication Symposium (IUCS), 2010 4th International,</publication>
        <pages>286--290</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Thomas Hofmann</author>
        </authors>
        <title>Probabilistic Latent Semantic Analysis.</title>
        <publication>In Proceedings of the 15th Conference on Uncertainty in AI,</publication>
        <pages>289--296</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Bo-June Hsu</author>
          <author>James Glass</author>
        </authors>
        <title>Style &amp; topic language model adaptation using HMM-LDA. In</title>
        <publication>in Proc. ACL Conf. on Empirical Methods in Natural Language Processing &#8211; EMNLP,</publication>
        <pages>373--381</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Reinhard Kneser</author>
          <author>Jochen Peters</author>
          <author>Dietrich Klakow</author>
        </authors>
        <title>Language Model Adaptation Using Dynamic Marginals.</title>
        <publication>In Proceedings of the 5th European Conference on Speech Communication and Technology,</publication>
        <pages>1971--1974</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>P Koehn</author>
          <author>H Hoang</author>
          <author>A Birch</author>
          <author>C Callison-Burch</author>
          <author>M Federico</author>
          <author>N Bertoldi</author>
          <author>B Cowan</author>
          <author>W Shen</author>
          <author>C Moran</author>
          <author>R Zens</author>
          <author>C Dyer</author>
          <author>O Bojar</author>
          <author>A Constantin</author>
          <author>E Herbst</author>
        </authors>
        <title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
        <publication>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</publication>
        <pages>177--180</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Josh Schroeder</author>
        </authors>
        <title>Experiments in Domain Adaptation for Statistical Machine Translation.</title>
        <publication>In Proceedings of the Second Workshop on Statistical Machine Translation,</publication>
        <pages>224--227</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>15</id>
        <authors/>
        <title>Polylingual Topic Models.</title>
        <publication>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum Error Rate Training in Statistical Machine Translation.</title>
        <publication>Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>160--167</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>17</id>
        <authors/>
        <title>None</title>
        <publication>None</publication>
        <pages>03--1021</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Abhinav Sethy</author>
          <author>Panayiotis Georgiou</author>
          <author>Shrikanth Narayanan</author>
        </authors>
        <title>Selecting relevant text subsets from web-data for building topic specific language models.</title>
        <publication>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</publication>
        <pages>145--148</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Yik-Cheung Tam</author>
          <author>Tanja Schultz</author>
        </authors>
        <title>Incorporating monolingual corpora into bilingual latent semantic analysis for crosslingual lm adaptation.</title>
        <publication>In Acoustics, Speech and Signal Processing,</publication>
        <pages>06--06</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>ICASSP</author>
        </authors>
        <title>doi: 10.1109/ ICASSP.2009.4960710. Yik-Cheung Tam, Ian Lane, and Tanja Schultz.</title>
        <publication>IEEE International Conference on,</publication>
        <pages>4821--4824</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>21</id>
        <authors/>
        <title>Bilingual LSA-based adaptation for statistical machine translation.</title>
        <publication>None</publication>
        <pages>21--187</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Bing Zhao</author>
          <author>Eric P Xing</author>
        </authors>
        <title>HM-BiTAM: Bilingual topic exploration, word alignment, and trans301</title>
        <publication>Advances in Neural Information Processing Systems 20,</publication>
        <pages>1689--1696</pages>
        <date>2008</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Blei et al., 2003</string>
        <sentence_id>46976</sentence_id>
        <char_offset>42</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>(1999)</string>
        <sentence_id>47027</sentence_id>
        <char_offset>9</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Darroch and Ratcliff, 1972</string>
        <sentence_id>47034</sentence_id>
        <char_offset>83</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>5</reference_id>
        <string>Federico, 1999</string>
        <sentence_id>46960</sentence_id>
        <char_offset>224</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>5</reference_id>
        <string>Federico (1999)</string>
        <sentence_id>47027</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>7</reference_id>
        <string>(2008)</string>
        <sentence_id>46978</sentence_id>
        <char_offset>88</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>7</reference_id>
        <string>(2008)</string>
        <sentence_id>46980</sentence_id>
        <char_offset>132</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>8</reference_id>
        <string>Foster and Kuhn (2007)</string>
        <sentence_id>46969</sentence_id>
        <char_offset>20</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>8</reference_id>
        <string>Foster and Kuhn (2007)</string>
        <sentence_id>46970</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>9</reference_id>
        <string>(2010)</string>
        <sentence_id>46986</sentence_id>
        <char_offset>26</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>10</reference_id>
        <string>Hofmann, 1999</string>
        <sentence_id>46974</sentence_id>
        <char_offset>72</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>11</reference_id>
        <string>Hsu and Glass, 2006</string>
        <sentence_id>46976</sentence_id>
        <char_offset>74</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>12</reference_id>
        <string>Kneser et al., 1997</string>
        <sentence_id>46960</sentence_id>
        <char_offset>246</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>12</reference_id>
        <string>Kneser et al., 1997</string>
        <sentence_id>47043</sentence_id>
        <char_offset>102</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>13</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>47150</sentence_id>
        <char_offset>65</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>14</reference_id>
        <string>Koehn and Schroeder (2007)</string>
        <sentence_id>46969</sentence_id>
        <char_offset>47</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>14</reference_id>
        <string>Koehn and Schroeder (2007)</string>
        <sentence_id>46971</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>15</reference_id>
        <string>(2009)</string>
        <sentence_id>46985</sentence_id>
        <char_offset>42</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>15</reference_id>
        <string>(2009)</string>
        <sentence_id>46991</sentence_id>
        <char_offset>13</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>15</reference_id>
        <string>(2009)</string>
        <sentence_id>46995</sentence_id>
        <char_offset>13</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>16</reference_id>
        <string>Och, 2003</string>
        <sentence_id>47153</sentence_id>
        <char_offset>105</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>18</reference_id>
        <string>Sethy et al. (2006)</string>
        <sentence_id>46968</sentence_id>
        <char_offset>3</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>19</reference_id>
        <string>Tam and Schultz (2009)</string>
        <sentence_id>46985</sentence_id>
        <char_offset>26</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>21</reference_id>
        <string>(2007)</string>
        <sentence_id>46969</sentence_id>
        <char_offset>36</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>21</reference_id>
        <string>(2007)</string>
        <sentence_id>46969</sentence_id>
        <char_offset>67</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>21</reference_id>
        <string>(2007)</string>
        <sentence_id>46970</sentence_id>
        <char_offset>16</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>21</reference_id>
        <string>(2007)</string>
        <sentence_id>46971</sentence_id>
        <char_offset>20</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>21</reference_id>
        <string>(2007)</string>
        <sentence_id>46981</sentence_id>
        <char_offset>57</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>21</reference_id>
        <string>(2007)</string>
        <sentence_id>46984</sentence_id>
        <char_offset>26</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>22</reference_id>
        <string>(2008)</string>
        <sentence_id>46978</sentence_id>
        <char_offset>88</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>22</reference_id>
        <string>(2008)</string>
        <sentence_id>46980</sentence_id>
        <char_offset>132</char_offset>
      </citation>
    </citations>
  </content>
</document>
