<document>
  <filename>W12-3126</filename>
  <authors/>
  <title>None</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>1 Introduction</title>
        <text>Statistical phrase-based machine translation (PMT) is attractive, as it requires no linguistic information beyond word-aligned parallel corpora (Zens et al., 2002; Koehn et al., 2003). Unfortunately, this linguistic agnosticism leaves phrase-based systems with no precise characterization of the word order relationships between languages, often leading to ungrammatical translations. Syntax could provide guidance to phrase-based systems, by steering them towards reorderings that reflect the structural relationships between languages, but using syntax to guide a phrase-based system is problematic. Phrasebased systems build the result incrementally from the beginning of the target string to the end, and the intermediate strings need not constitute complete traditional syntactic constituents. It is difficult to reconcile traditional recursive syntactic processing with this regime, because not all intermediate strings considered by the decoder would even have a syntactic category to assess. As a result, most phrase-based decoders control reordering using simple distancebased distortion models, which penalize all reordering equally, and lexicalized reordering models (Tillmann, 2004; Axelrod et al., 2005), which probabilistically score various reordering configurations conditioned on specific lexical translations. While undoubtedly better than nothing, these models perform poorly when languages diverge considerably in sentence structure. Distance-based distortion models are too coarse-grained to distinguish correct from incorrect reordering, while lexical reordering models suffer from data sparsity and fail to capture more general patterns. We argue that finding a way to label translation phrases with syntactic labels will abstract over the observed reordering configurations thereby address both all three deficiencies of granularity, data sparsity and lack of generality.
The present work presents a novel syntactic analogue of the lexicalized reordering model that uses multiword syntactic labels to capture the general reordering patterns between two languages with very different word order. We accomplish this by using Combinatory Categorial Grammar, or CCG (Steed-
man, 2000), a word-centered syntax that allows a great deal of flexibility in how sentence analyses are formed. Syntactic derivations in CCG are massively spuriously ambiguous, i.e., there are many ways to derive the same semantic analysis of a sentence, similar to how a mathematical equation can be reduced by canceling out variables in different orders. Despite its name, spurious ambiguity is a benefit to us, as it provides many different labelled bracketings for the same dependency graph of the same sentence, thereby increasing the chance that any substring of that sentence will have a syntactic label. Our approach exploits this property of CCG to derive multiword CCG syntactic labels for target translation strings in a phrase table, thus providing a firmer basis on which to collect syntactic reordering statistics. In particular:
&#8226; We show how CCG can derive constituent labels for target-side phrase-table entries that are often lamented as &#8220;non-constituents&#8221; or as &#8220;crossing a phrase boundary&#8221;.
&#8226; Our CCG categories are not limited to singleword supertags. Rather, as these labels are drawn from CCG parse charts, they can span multiple words. Further, the labels are tailored specifically to each translation constituent&#8217;s boundaries (Section 2.1). As a consequence, &#8776;70% of phrase table entries receive a single syntactic label (Section 5), largely removing the terminological inconsistency of calling lexical translation constituents &#8220;phrases&#8221;. Now, more of them actually are syntactic phrases.
&#8226; We use these labels to train a target-language bidirectional reordering model over CCG syntactic sequences (Section 3), which, when added to the baseline system, is found to be superior to systems that use both lexicalized reordering models and supertag reordering models (Section 5).
With only minor modifications, we incorporate these enhancements into a state-of-the-art PMT decoder (Koehn et al., 2007), achieving significant improvements over two competitive baselines in an Urdu- English translation task (Sections 5). This language pair was chosen to highlight the promise of this approach for languages with considerable, but syntactically governed, word-order differences to one another. Finally, in a small discussion we provide qualitative evidence that the improvements in automatic metric scores correspond to real gains in target language fluency.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Statistical phrase-based machine translation (PMT) is attractive, as it requires no linguistic information beyond word-aligned parallel corpora (Zens et al., 2002; Koehn et al., 2003).</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Unfortunately, this linguistic agnosticism leaves phrase-based systems with no precise characterization of the word order relationships between languages, often leading to ungrammatical translations.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Syntax could provide guidance to phrase-based systems, by steering them towards reorderings that reflect the structural relationships between languages, but using syntax to guide a phrase-based system is problematic.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Phrasebased systems build the result incrementally from the beginning of the target string to the end, and the intermediate strings need not constitute complete traditional syntactic constituents.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>It is difficult to reconcile traditional recursive syntactic processing with this regime, because not all intermediate strings considered by the decoder would even have a syntactic category to assess.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>As a result, most phrase-based decoders control reordering using simple distancebased distortion models, which penalize all reordering equally, and lexicalized reordering models (Tillmann, 2004; Axelrod et al., 2005), which probabilistically score various reordering configurations conditioned on specific lexical translations.</text>
              <doc_id>5</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>While undoubtedly better than nothing, these models perform poorly when languages diverge considerably in sentence structure.</text>
              <doc_id>6</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Distance-based distortion models are too coarse-grained to distinguish correct from incorrect reordering, while lexical reordering models suffer from data sparsity and fail to capture more general patterns.</text>
              <doc_id>7</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>We argue that finding a way to label translation phrases with syntactic labels will abstract over the observed reordering configurations thereby address both all three deficiencies of granularity, data sparsity and lack of generality.</text>
              <doc_id>8</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The present work presents a novel syntactic analogue of the lexicalized reordering model that uses multiword syntactic labels to capture the general reordering patterns between two languages with very different word order.</text>
              <doc_id>9</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We accomplish this by using Combinatory Categorial Grammar, or CCG (Steed-</text>
              <doc_id>10</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>man, 2000), a word-centered syntax that allows a great deal of flexibility in how sentence analyses are formed.</text>
              <doc_id>11</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Syntactic derivations in CCG are massively spuriously ambiguous, i.e., there are many ways to derive the same semantic analysis of a sentence, similar to how a mathematical equation can be reduced by canceling out variables in different orders.</text>
              <doc_id>12</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Despite its name, spurious ambiguity is a benefit to us, as it provides many different labelled bracketings for the same dependency graph of the same sentence, thereby increasing the chance that any substring of that sentence will have a syntactic label.</text>
              <doc_id>13</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Our approach exploits this property of CCG to derive multiword CCG syntactic labels for target translation strings in a phrase table, thus providing a firmer basis on which to collect syntactic reordering statistics.</text>
              <doc_id>14</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In particular:</text>
              <doc_id>15</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; We show how CCG can derive constituent labels for target-side phrase-table entries that are often lamented as &#8220;non-constituents&#8221; or as &#8220;crossing a phrase boundary&#8221;.</text>
              <doc_id>16</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; Our CCG categories are not limited to singleword supertags.</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Rather, as these labels are drawn from CCG parse charts, they can span multiple words.</text>
              <doc_id>18</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Further, the labels are tailored specifically to each translation constituent&#8217;s boundaries (Section 2.1).</text>
              <doc_id>19</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As a consequence, &#8776;70% of phrase table entries receive a single syntactic label (Section 5), largely removing the terminological inconsistency of calling lexical translation constituents &#8220;phrases&#8221;.</text>
              <doc_id>20</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Now, more of them actually are syntactic phrases.</text>
              <doc_id>21</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; We use these labels to train a target-language bidirectional reordering model over CCG syntactic sequences (Section 3), which, when added to the baseline system, is found to be superior to systems that use both lexicalized reordering models and supertag reordering models (Section 5).</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>With only minor modifications, we incorporate these enhancements into a state-of-the-art PMT decoder (Koehn et al., 2007), achieving significant improvements over two competitive baselines in an Urdu- English translation task (Sections 5).</text>
              <doc_id>23</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This language pair was chosen to highlight the promise of this approach for languages with considerable, but syntactically governed, word-order differences to one another.</text>
              <doc_id>24</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Finally, in a small discussion we provide qualitative evidence that the improvements in automatic metric scores correspond to real gains in target language fluency.</text>
              <doc_id>25</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>2 Syntax, Constituency and Phrase-based MT</title>
        <text>Consider the following German-English PMT phrase pair that we have extracted from a parallel European parliamentary transcript: 1
Ich hoffe, da&#223; &#8660; I hope that
Neither word string is a well-formed constituent in traditional theories of syntax. But tradition is at odds with the intuition that that such &#8220;non-constituent&#8221; sequences are still well-formed substrings, governed by rules of how they can be combined with other word strings &#8212; e.g., declarative sentence translation rules like es m&#246;glich sein wird &#8660; it will be possible can grammatically extend each, but a noun phrase rule cannot. As Figure 1 illustrates, putative non-constituent word sequences abound in phrase-based MT. Here a translation &#8220;phrase&#8221; is simply any contiguous word string that is consistent with a word alignment (a relation between source and target words), usually produced by a language-independent alignment procedure (Zens et al., 2002). The figure also highlights the need for linguistic syntax in controlling how translations are assembled; the successful translation is merely one among many possible reorderings, many of which (despite their ungrammaticality) might score well on a word n-gram model. But rather than changing the word alignments or PMT &#8220;phrase&#8221; boundaries to fit a syntactic theory, we choose to use a flexible syntax which can produce a wider range of bracketings to accommodate the results of alignment-derived translations. To this end, we use Combinatory Categorial Grammar, or CCG, (Steedman, 2000). To understand how CCG allows this, we illustrate its use with some simple examples.
1 Throughout this paper, the term &#8220;PMT phrase&#8221; refers to an
unbroken sequence of words used by a PMT system, whereas &#8220;phrase&#8221; (without context) refers to a syntactic constituent.
Wiederaufnahme der Situngsperiode Ich hoffe , da&#223; es m&#246;glich sein wird
Resumption of the session I hope that it will be possible
Ich hoffe, da&#223; das den Weg f&#252;r eine baldige Wiederaufnahme der Debatte ebnen wird
I hope that this will pave the way for an early resumption of the debate
2.1 CCG, Spurious Ambiguity and PMT: Turning &#8220;Phrases&#8221; into Phrases
CCG is a derivational syntax, where words are assigned a lexical category 2 and sentence structures are then recursively built using a small set of deductive rule schemata known as combinators (Steedman, 2000). Lexical syntactic categories can be richly structured in CCG, indicating how words can combine. A syntactic category of the form X/Y, e.g., states that a category of type X can be formed if combined with a Y to its right &#8212; i.e., a function from rightward Ys to X. This can be accomplished with the forward function application combinator (&gt;), 3 which is written in derivational form as follows: 4
X/Y
X This derivation of the symbol X is known as the normal-form derivation (Steedman, 2000), since it uses function application whenever possible. But CCG has the ability to construct the same result by using a different, non-normal-form sequence of combinatory inferences. For example, by using the backward type-raising combinator (T &lt; ) and then backward function application (&lt;), we can arrive at the same result:
2 When represented by a strings, lexical categories are called
supertags. 3 CCG actually respects the rule-to-rule hypothesis (Bach,
l976), where, for every syntactic term built, there is a corresponding semantic term, but, for simplicity of exposition, we focus only on syntax here. 4 The reader will notice that CCG derivations are in fact
trees, but that they &#8220;grow&#8221; in the direction opposite to how parse trees are often depicted in NLP.
Y
&gt;
X/Y Y
T &lt;
X\(X/Y)
&lt;
X This derivation shows how the argument Y to the functional type X/Y 5 can &#8220;raise&#8221; its type to become a function that consumes that functional type, X\(X/Y), only to produce same result as before, namely X. This property of CCG is often referred to as &#8220;spurious ambiguity&#8221;, because there are many ways of reaching the same result as the canonical, normal-form derivation.
Despite the name, this property is useful for our purposes. Considering the target translation in Figure 1, we then observe in Figure 2 how CCG can derive not only a bracketing similar to a more traditional Penn Treebank-style parse, but also a nonnormal-form variant that gives us a single category for the English translation string I hope that &#8212; namely the category S[dcl]/S[dcl] (a declarative sentence lacking a declarative sentence complement to its right).
We use this fact about CCG to label a wider range of PMT phrases with genuine syntactic constituent labels. First we parse the English sentences in our training data with the C&amp;C parser, a state-of-the-art, treebank-trained CCG parser (Clark and Curran, 2007), producing normal-form CCG derivations. We then enumerate all non-normalform derivations that result in the same top-level symbol, packing all derivations (normal-form and non-normal-form) into a parse chart (see Figure 4).
5 Also referred to as a functor.
UR.-EN.
SINGLE-LABEL COVERAGE 69%
AVE. EN. PHRASE LEN.
AVE. CCG LABEL SPAN
2.8 wds 2.3 wds
AVE. CCG LABS/ENTRY 1.4
For the English string of each phrase table entry, we inspect the chart for the English-side sentence that it came from and extract a list of labels as in Figure 3. For each span, this procedure either (lines 5&#8211;9) finds the topmost single label, only using typeraised categories when no others exist, 6 or (lines 10&#8211; 19) recursively and greedily finds the longest spanning labels from left to right, if no single label exists. The degenerate case is the single-word level (supertags). In this way we find single labels for 69% of the English-side phrase training instances. Table 1 gives more details.
6 Type-raisings are almost always possible, and will always
be closer to the top-level symbol. Many type-raisings, however, are superfluous &#8211; i.e., produce no novel bracketings. Therefore we only use type-raised symbols to derive a label for a span of words when necessary.
S[dcl] S[dcl]\NP
(S[dcl]\NP)/(S[b]\NP)
S[em]/(S[b]\NP)
S[dcl]/(S[b]\NP)
0 I 1 hope 2 that 3 it 4 will 5 rain 6
0 Ich 1 hoffe 2 , 3 da&#223; 4 es 5 regnen 6 wird 7
3 Reordering Models: from Words to Supertags to Parses
In phrase-based MT systems, the standard reordering model that controls the order in which the source string is translated is the lexicalized reordering model (Tillmann, 2004; Axelrod et al., 2005). In its simplest form, a lexicalized reordering model estimates, for each translation phrase pair (f i...j , e k...l ) (where the indices sit &#8220;in-between&#8221; words, as in Figure 4), the probability of p(O | f i...j , e k...l ), where
O &#8712; {MONO, SWAP, DISCONTINUOUS} (abbreviated M, S and D) is the orientation of the phrase pair (f i...j , e k...l ) w.r.t. the previously translated source phrase f u...v . If v = i, then O = M; if u = j, then
O = S; otherwise O = D. This model, known as
a unidirectional MSD lexicalized reordering model, can also be enriched with statistics over orientations to the next source phrase translated (i.e., it can be a bidirectional model), as well as with more finegrained distinctions in the third class D (i.e., whether it is D LEFT or D RIGHT ). All models in the present work are bidirectional MSD models.
During decoding, orientations are predicted based on previously translated (or following) phrases in the decoder&#8217;s search state, but, when extracting orientation statistics, there are many different possible phrasal segmentations of both strings. A simple solution, known as word-based extraction, is to look for neighboring alignment points that support the various orientations. In Figure 4, e.g., a wordbased extraction regime would count the phrase hoffe &#8660; hope as being in orientation D w.r.t. to what follows, because its rightmost index, 2, is discontiguous with the next aligned source point, (3,4). Another approach, known as phrase-based extraction aims to remedy this situation by conditioning the extraction of orientations on translation phrases consistent with the alignment. In Figure 4 there is a translation phrase that follows the phrase in question &#8212; viz., , da&#223; &#8660; that &#8212; and an orientation of M is therefore tallied.
Regardless of the method of extraction, lexicalized reordering model statistics rely on exact word-string pairs, (f, e), which can lead problems with data sparsity. Moreover, even given ample data, cross-phrasal reordering generalizations will be missed. E.g., the fact that regnen &#8660; rain has orientation S w.r.t. the previous phrase pair does not support the fact that other infinitival German verbs should also behave similarly in relative clausal environments.
To remedy this we might substitute abstract symbols for each word in e, and train a syntactic bidirectional MSD reordering model. For this we use CCG supertags (cf. the single-word labels in the parse
chart in Figure 4), which are richly structured parts of speech that describe their potential to combine with other words (cf. Section 2.1). Given the same phrase from Figure 4, we can estimate the probability of orientation S, given regnen &#8660; S[b]\NP . A further level of abstraction is to use CCG parse charts packed with all derivations. The phrase da&#223; es &#8660; that it can therefore be abstracted to
da&#223; es &#8660; S[em]/(S[dcl]\NP) (a &#8220;that&#8221; clause lacking a verb phrase to the right). Except in cases of high ambiguity, the source phrase effectively encodes the target phrase, meaning that these extensions will suffer from data sparsity similarly to the baseline lexicalized model. We therefore omit the source phrase in our syntactic reordering models, estimating probability distributions p(O|LAB(e)) where LAB(e) is the syntactic label sequence derived from the chart (or supertagged string, as the case may be) using the algorithm in Figure 3. 7 Orientations are determined using the phrase-based extraction regime described in (Tillmann, 2004), but statistics are tallied only for the syntactic label sequence of the target string. More precisely, for phrase pair (f i...j , e k...l ), if a phrase (f a...i , e b...k ) exists in the alignment grid, an orientation of M is assigned to LAB(e k...l ) . Otherwise, if a phrase (f j...p , e l...m ) exists in the alignment grid, an orientation of S is assigned. In all other cases, an orientation of D is assigned. Using these statistics, we deploy target-side reordering models, as described below.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Consider the following German-English PMT phrase pair that we have extracted from a parallel European parliamentary transcript: 1</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Ich hoffe, da&#223; &#8660; I hope that</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Neither word string is a well-formed constituent in traditional theories of syntax.</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>But tradition is at odds with the intuition that that such &#8220;non-constituent&#8221; sequences are still well-formed substrings, governed by rules of how they can be combined with other word strings &#8212; e.g., declarative sentence translation rules like es m&#246;glich sein wird &#8660; it will be possible can grammatically extend each, but a noun phrase rule cannot.</text>
              <doc_id>29</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>As Figure 1 illustrates, putative non-constituent word sequences abound in phrase-based MT.</text>
              <doc_id>30</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Here a translation &#8220;phrase&#8221; is simply any contiguous word string that is consistent with a word alignment (a relation between source and target words), usually produced by a language-independent alignment procedure (Zens et al., 2002).</text>
              <doc_id>31</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The figure also highlights the need for linguistic syntax in controlling how translations are assembled; the successful translation is merely one among many possible reorderings, many of which (despite their ungrammaticality) might score well on a word n-gram model.</text>
              <doc_id>32</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>But rather than changing the word alignments or PMT &#8220;phrase&#8221; boundaries to fit a syntactic theory, we choose to use a flexible syntax which can produce a wider range of bracketings to accommodate the results of alignment-derived translations.</text>
              <doc_id>33</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>To this end, we use Combinatory Categorial Grammar, or CCG, (Steedman, 2000).</text>
              <doc_id>34</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>To understand how CCG allows this, we illustrate its use with some simple examples.</text>
              <doc_id>35</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 Throughout this paper, the term &#8220;PMT phrase&#8221; refers to an</text>
              <doc_id>36</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>unbroken sequence of words used by a PMT system, whereas &#8220;phrase&#8221; (without context) refers to a syntactic constituent.</text>
              <doc_id>37</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Wiederaufnahme der Situngsperiode Ich hoffe , da&#223; es m&#246;glich sein wird</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Resumption of the session I hope that it will be possible</text>
              <doc_id>39</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Ich hoffe, da&#223; das den Weg f&#252;r eine baldige Wiederaufnahme der Debatte ebnen wird</text>
              <doc_id>40</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>I hope that this will pave the way for an early resumption of the debate</text>
              <doc_id>41</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.1 CCG, Spurious Ambiguity and PMT: Turning &#8220;Phrases&#8221; into Phrases</text>
              <doc_id>42</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>CCG is a derivational syntax, where words are assigned a lexical category 2 and sentence structures are then recursively built using a small set of deductive rule schemata known as combinators (Steedman, 2000).</text>
              <doc_id>43</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Lexical syntactic categories can be richly structured in CCG, indicating how words can combine.</text>
              <doc_id>44</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A syntactic category of the form X/Y, e.g., states that a category of type X can be formed if combined with a Y to its right &#8212; i.e., a function from rightward Ys to X.</text>
              <doc_id>45</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This can be accomplished with the forward function application combinator (&gt;), 3 which is written in derivational form as follows: 4</text>
              <doc_id>46</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>X/Y</text>
              <doc_id>47</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>X This derivation of the symbol X is known as the normal-form derivation (Steedman, 2000), since it uses function application whenever possible.</text>
              <doc_id>48</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>But CCG has the ability to construct the same result by using a different, non-normal-form sequence of combinatory inferences.</text>
              <doc_id>49</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For example, by using the backward type-raising combinator (T &lt; ) and then backward function application (&lt;), we can arrive at the same result:</text>
              <doc_id>50</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2 When represented by a strings, lexical categories are called</text>
              <doc_id>51</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>supertags.</text>
              <doc_id>52</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>3 CCG actually respects the rule-to-rule hypothesis (Bach,</text>
              <doc_id>53</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>l976), where, for every syntactic term built, there is a corresponding semantic term, but, for simplicity of exposition, we focus only on syntax here.</text>
              <doc_id>54</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>4 The reader will notice that CCG derivations are in fact</text>
              <doc_id>55</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>trees, but that they &#8220;grow&#8221; in the direction opposite to how parse trees are often depicted in NLP.</text>
              <doc_id>56</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Y</text>
              <doc_id>57</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&gt;</text>
              <doc_id>58</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>X/Y Y</text>
              <doc_id>59</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>T &lt;</text>
              <doc_id>60</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>X\(X/Y)</text>
              <doc_id>61</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&lt;</text>
              <doc_id>62</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>X This derivation shows how the argument Y to the functional type X/Y 5 can &#8220;raise&#8221; its type to become a function that consumes that functional type, X\(X/Y), only to produce same result as before, namely X.</text>
              <doc_id>63</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This property of CCG is often referred to as &#8220;spurious ambiguity&#8221;, because there are many ways of reaching the same result as the canonical, normal-form derivation.</text>
              <doc_id>64</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Despite the name, this property is useful for our purposes.</text>
              <doc_id>65</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Considering the target translation in Figure 1, we then observe in Figure 2 how CCG can derive not only a bracketing similar to a more traditional Penn Treebank-style parse, but also a nonnormal-form variant that gives us a single category for the English translation string I hope that &#8212; namely the category S[dcl]/S[dcl] (a declarative sentence lacking a declarative sentence complement to its right).</text>
              <doc_id>66</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We use this fact about CCG to label a wider range of PMT phrases with genuine syntactic constituent labels.</text>
              <doc_id>67</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>First we parse the English sentences in our training data with the C&amp;C parser, a state-of-the-art, treebank-trained CCG parser (Clark and Curran, 2007), producing normal-form CCG derivations.</text>
              <doc_id>68</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We then enumerate all non-normalform derivations that result in the same top-level symbol, packing all derivations (normal-form and non-normal-form) into a parse chart (see Figure 4).</text>
              <doc_id>69</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>5 Also referred to as a functor.</text>
              <doc_id>70</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>UR.-EN.</text>
              <doc_id>71</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>SINGLE-LABEL COVERAGE 69%</text>
              <doc_id>72</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>AVE.</text>
              <doc_id>73</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>EN.</text>
              <doc_id>74</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>PHRASE LEN.</text>
              <doc_id>75</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>AVE.</text>
              <doc_id>76</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>CCG LABEL SPAN</text>
              <doc_id>77</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.8 wds 2.3 wds</text>
              <doc_id>78</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>AVE.</text>
              <doc_id>79</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>CCG LABS/ENTRY 1.4</text>
              <doc_id>80</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For the English string of each phrase table entry, we inspect the chart for the English-side sentence that it came from and extract a list of labels as in Figure 3.</text>
              <doc_id>81</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For each span, this procedure either (lines 5&#8211;9) finds the topmost single label, only using typeraised categories when no others exist, 6 or (lines 10&#8211; 19) recursively and greedily finds the longest spanning labels from left to right, if no single label exists.</text>
              <doc_id>82</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The degenerate case is the single-word level (supertags).</text>
              <doc_id>83</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In this way we find single labels for 69% of the English-side phrase training instances.</text>
              <doc_id>84</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Table 1 gives more details.</text>
              <doc_id>85</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>6 Type-raisings are almost always possible, and will always</text>
              <doc_id>86</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>be closer to the top-level symbol.</text>
              <doc_id>87</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Many type-raisings, however, are superfluous &#8211; i.e., produce no novel bracketings.</text>
              <doc_id>88</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Therefore we only use type-raised symbols to derive a label for a span of words when necessary.</text>
              <doc_id>89</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S[dcl] S[dcl]\NP</text>
              <doc_id>90</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(S[dcl]\NP)/(S[b]\NP)</text>
              <doc_id>91</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S[em]/(S[b]\NP)</text>
              <doc_id>92</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S[dcl]/(S[b]\NP)</text>
              <doc_id>93</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0 I 1 hope 2 that 3 it 4 will 5 rain 6</text>
              <doc_id>94</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0 Ich 1 hoffe 2 , 3 da&#223; 4 es 5 regnen 6 wird 7</text>
              <doc_id>95</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3 Reordering Models: from Words to Supertags to Parses</text>
              <doc_id>96</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In phrase-based MT systems, the standard reordering model that controls the order in which the source string is translated is the lexicalized reordering model (Tillmann, 2004; Axelrod et al., 2005).</text>
              <doc_id>97</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In its simplest form, a lexicalized reordering model estimates, for each translation phrase pair (f i...j , e k...l ) (where the indices sit &#8220;in-between&#8221; words, as in Figure 4), the probability of p(O | f i...j , e k...l ), where</text>
              <doc_id>98</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>O &#8712; {MONO, SWAP, DISCONTINUOUS} (abbreviated M, S and D) is the orientation of the phrase pair (f i...j , e k...l ) w.r.t.</text>
              <doc_id>99</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>the previously translated source phrase f u...v .</text>
              <doc_id>100</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>If v = i, then O = M; if u = j, then</text>
              <doc_id>101</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>O = S; otherwise O = D.</text>
              <doc_id>102</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This model, known as</text>
              <doc_id>103</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>a unidirectional MSD lexicalized reordering model, can also be enriched with statistics over orientations to the next source phrase translated (i.e., it can be a bidirectional model), as well as with more finegrained distinctions in the third class D (i.e., whether it is D LEFT or D RIGHT ).</text>
              <doc_id>104</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>All models in the present work are bidirectional MSD models.</text>
              <doc_id>105</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>During decoding, orientations are predicted based on previously translated (or following) phrases in the decoder&#8217;s search state, but, when extracting orientation statistics, there are many different possible phrasal segmentations of both strings.</text>
              <doc_id>106</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A simple solution, known as word-based extraction, is to look for neighboring alignment points that support the various orientations.</text>
              <doc_id>107</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In Figure 4, e.g., a wordbased extraction regime would count the phrase hoffe &#8660; hope as being in orientation D w.r.t.</text>
              <doc_id>108</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>to what follows, because its rightmost index, 2, is discontiguous with the next aligned source point, (3,4).</text>
              <doc_id>109</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Another approach, known as phrase-based extraction aims to remedy this situation by conditioning the extraction of orientations on translation phrases consistent with the alignment.</text>
              <doc_id>110</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In Figure 4 there is a translation phrase that follows the phrase in question &#8212; viz., , da&#223; &#8660; that &#8212; and an orientation of M is therefore tallied.</text>
              <doc_id>111</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Regardless of the method of extraction, lexicalized reordering model statistics rely on exact word-string pairs, (f, e), which can lead problems with data sparsity.</text>
              <doc_id>112</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, even given ample data, cross-phrasal reordering generalizations will be missed.</text>
              <doc_id>113</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>E.g., the fact that regnen &#8660; rain has orientation S w.r.t.</text>
              <doc_id>114</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>the previous phrase pair does not support the fact that other infinitival German verbs should also behave similarly in relative clausal environments.</text>
              <doc_id>115</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To remedy this we might substitute abstract symbols for each word in e, and train a syntactic bidirectional MSD reordering model.</text>
              <doc_id>116</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For this we use CCG supertags (cf. the single-word labels in the parse</text>
              <doc_id>117</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>chart in Figure 4), which are richly structured parts of speech that describe their potential to combine with other words (cf. Section 2.1).</text>
              <doc_id>118</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Given the same phrase from Figure 4, we can estimate the probability of orientation S, given regnen &#8660; S[b]\NP .</text>
              <doc_id>119</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A further level of abstraction is to use CCG parse charts packed with all derivations.</text>
              <doc_id>120</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The phrase da&#223; es &#8660; that it can therefore be abstracted to</text>
              <doc_id>121</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>da&#223; es &#8660; S[em]/(S[dcl]\NP) (a &#8220;that&#8221; clause lacking a verb phrase to the right).</text>
              <doc_id>122</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Except in cases of high ambiguity, the source phrase effectively encodes the target phrase, meaning that these extensions will suffer from data sparsity similarly to the baseline lexicalized model.</text>
              <doc_id>123</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We therefore omit the source phrase in our syntactic reordering models, estimating probability distributions p(O|LAB(e)) where LAB(e) is the syntactic label sequence derived from the chart (or supertagged string, as the case may be) using the algorithm in Figure 3.</text>
              <doc_id>124</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>7 Orientations are determined using the phrase-based extraction regime described in (Tillmann, 2004), but statistics are tallied only for the syntactic label sequence of the target string.</text>
              <doc_id>125</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>More precisely, for phrase pair (f i...j , e k...l ), if a phrase (f a...i , e b...k ) exists in the alignment grid, an orientation of M is assigned to LAB(e k...l ) .</text>
              <doc_id>126</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Otherwise, if a phrase (f j...p , e l...m ) exists in the alignment grid, an orientation of S is assigned.</text>
              <doc_id>127</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In all other cases, an orientation of D is assigned.</text>
              <doc_id>128</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Using these statistics, we deploy target-side reordering models, as described below.</text>
              <doc_id>129</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>4 Related Work</title>
        <text>As noted, lexicalized reordering models can be trained and configured in many different ways. In addition to the standard word-based extraction (Axelrod et al., 2005) and phrase-based extraction (Tillmann, 2004) cases, more recent work has explored using dynamic programming to extract and later score orientations based on hierarchical configurations of phrases consistent with an alignment (Galley and Manning, 2008). This means that the reordering model can be conditioned on an unbounded amount of context and can capture the fact that
7 Note that a tagged string can be viewed as a very impoverished parse chart, and so the algorithm defined in Figure 3 can be applied to the supertagging case as well.
many translations are monotonic w.r.t. the previously translated block, but are mistakenly identified as having orientation S or D. Su and colleagues (2010) observe that the space of phrase pairs consistent with an alignment can be viewed in its entirety, as a graph of phrases, thereby collecting reordering statistics w.r.t. the entire space of surrounding phrases. Ling and colleagues (2011) extend this approach by weighting orientation counts with multiple scored alignments. All of these more sophisticated reordering extraction approaches are compatible with the current approach, and could be straightforwardly applied to our labelled target-side word strings. Syntax-driven reordering approaches in phrasebased MT abound, but, perhaps due to the incompatibility of phrase table entries and traditional syntactic constituency, most research has avoided using recursive target-side syntax during decoding. Tillmann (2008) presents an algorithm that reorders using part-of-speech based permutation patterns during the decoding process. Others have side-stepped the issue by restructuring the source language before decoding to resemble the target language using syntactic rules, either automatically extracted (Xia and McCord, 2004), or hand-crafted (Collins et al., 2005; Wang et al., 2007; Xu and Seneff, 2008).
The flexibility of CCG syntax is also gaining recognition as a useful tool for constraining statistical MT decoders. Hassan (2009) describes an incremental CCG parsing language model, although his model does not beat a supertag factored PMT approach. Almaghout and colleagues (2010) also use a CCG chart to improve translation, augmenting SCFG rules by consulting the multiple derivations in the parse chart of Clark and Curran&#8217;s (2007) CCG parser. We note two key differences to our use of spurious ambiguity. First, they use a chart packed with multiple dependency analyses, unlike our spuriously ambiguous reworkings of the parser&#8217;s single-best analysis. Second, the C&amp;C parser restrains type-raising to a small number of possibilities, thereby blocking many non-normal-form derivations that we do not.
Two SCFG approaches that employ categorial syntax that resembles CCG are the syntaxaugmented MT (SAMT) system described in (Venugopal et al., 2007), and the target dependency lan-
guage model of of (Shen et al., 2008). (Venugopal et al., 2007) uses a Penn Treebank-trained CFG parser to label target strings and then reworks the CFG parse trees, if needed,x to account for non-traditional constituents. This ondemand reworking process, however, is bounded by tree depth, and sometimes produces conjoined categories, rather than consistently produce the functional &#8220;slash&#8221; categories that a full CCG would &#8212; e.g., a subject + transitive verb string might sometimes be labelled NP + V and other times S/NP . The approach in (Shen et al., 2010) uses a simple categorial grammar with only a single atomic symbol &#8212; i.e., every functional category has the form C\X or C/X, where X is either C or another slash category C\X or C/X. In contrast to these two approaches, the CCG parser we use is trained on a CCG treebank that is the result of a carefully engineered Penn Treebank-to-CCG conversion (Hockenmaier and Steedman, 2007) and we impose no limits on deriving categorial functional categories (X/Y). We view our reworking of CCG charts as a potentially useful extension to such approaches.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>As noted, lexicalized reordering models can be trained and configured in many different ways.</text>
              <doc_id>130</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In addition to the standard word-based extraction (Axelrod et al., 2005) and phrase-based extraction (Tillmann, 2004) cases, more recent work has explored using dynamic programming to extract and later score orientations based on hierarchical configurations of phrases consistent with an alignment (Galley and Manning, 2008).</text>
              <doc_id>131</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This means that the reordering model can be conditioned on an unbounded amount of context and can capture the fact that</text>
              <doc_id>132</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>7 Note that a tagged string can be viewed as a very impoverished parse chart, and so the algorithm defined in Figure 3 can be applied to the supertagging case as well.</text>
              <doc_id>133</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>many translations are monotonic w.r.t.</text>
              <doc_id>134</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>the previously translated block, but are mistakenly identified as having orientation S or D. Su and colleagues (2010) observe that the space of phrase pairs consistent with an alignment can be viewed in its entirety, as a graph of phrases, thereby collecting reordering statistics w.r.t.</text>
              <doc_id>135</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>the entire space of surrounding phrases.</text>
              <doc_id>136</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Ling and colleagues (2011) extend this approach by weighting orientation counts with multiple scored alignments.</text>
              <doc_id>137</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>All of these more sophisticated reordering extraction approaches are compatible with the current approach, and could be straightforwardly applied to our labelled target-side word strings.</text>
              <doc_id>138</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Syntax-driven reordering approaches in phrasebased MT abound, but, perhaps due to the incompatibility of phrase table entries and traditional syntactic constituency, most research has avoided using recursive target-side syntax during decoding.</text>
              <doc_id>139</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Tillmann (2008) presents an algorithm that reorders using part-of-speech based permutation patterns during the decoding process.</text>
              <doc_id>140</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Others have side-stepped the issue by restructuring the source language before decoding to resemble the target language using syntactic rules, either automatically extracted (Xia and McCord, 2004), or hand-crafted (Collins et al., 2005; Wang et al., 2007; Xu and Seneff, 2008).</text>
              <doc_id>141</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The flexibility of CCG syntax is also gaining recognition as a useful tool for constraining statistical MT decoders.</text>
              <doc_id>142</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Hassan (2009) describes an incremental CCG parsing language model, although his model does not beat a supertag factored PMT approach.</text>
              <doc_id>143</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Almaghout and colleagues (2010) also use a CCG chart to improve translation, augmenting SCFG rules by consulting the multiple derivations in the parse chart of Clark and Curran&#8217;s (2007) CCG parser.</text>
              <doc_id>144</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We note two key differences to our use of spurious ambiguity.</text>
              <doc_id>145</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>First, they use a chart packed with multiple dependency analyses, unlike our spuriously ambiguous reworkings of the parser&#8217;s single-best analysis.</text>
              <doc_id>146</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Second, the C&amp;C parser restrains type-raising to a small number of possibilities, thereby blocking many non-normal-form derivations that we do not.</text>
              <doc_id>147</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Two SCFG approaches that employ categorial syntax that resembles CCG are the syntaxaugmented MT (SAMT) system described in (Venugopal et al., 2007), and the target dependency lan-</text>
              <doc_id>148</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>guage model of of (Shen et al., 2008).</text>
              <doc_id>149</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>(Venugopal et al., 2007) uses a Penn Treebank-trained CFG parser to label target strings and then reworks the CFG parse trees, if needed,x to account for non-traditional constituents.</text>
              <doc_id>150</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This ondemand reworking process, however, is bounded by tree depth, and sometimes produces conjoined categories, rather than consistently produce the functional &#8220;slash&#8221; categories that a full CCG would &#8212; e.g., a subject + transitive verb string might sometimes be labelled NP + V and other times S/NP .</text>
              <doc_id>151</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The approach in (Shen et al., 2010) uses a simple categorial grammar with only a single atomic symbol &#8212; i.e., every functional category has the form C\X or C/X, where X is either C or another slash category C\X or C/X.</text>
              <doc_id>152</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In contrast to these two approaches, the CCG parser we use is trained on a CCG treebank that is the result of a carefully engineered Penn Treebank-to-CCG conversion (Hockenmaier and Steedman, 2007) and we impose no limits on deriving categorial functional categories (X/Y).</text>
              <doc_id>153</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We view our reworking of CCG charts as a potentially useful extension to such approaches.</text>
              <doc_id>154</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>5 Experimental Results</title>
        <text>We empirically validate our technique by translating from Urdu into English. Urdu has a canonical word order of SOV &#8212; subject, object(s), verb &#8212; whereas English has SVO, leading to indefinitely long distances between corresponding verbs and objects. This language pair is therefore a strong test case for a reordering model.
For decoding we use Moses (Koehn et al., 2007), a state-of-the-art PMT decoder, with IRST LM (Federico and Cettolo, 2007) for language model inference. For Urdu-English parallel data, we use the OpenMT 2008 training set which consists of 88 thousand sentence-level translations and a translation dictionary of &#8776;114 thousand word and phrase translations. We use half of the OpenMT 2008 Urdu- English evaluation data for development and perform development testing on the other half. Both halves are &#8776;900 sentences long and were balanced to contain approximately the same number of tokens. Our blind test set is the entire OpenMT 2009 Urdu-English evaluation set. All evaluation sets had 4 reference translations for each tuning or testing instance. All system component weights were tuned using minimum error-rate training (Och, 2003), with three tuning runs for each condition. The data was normalized, tokenized and the English sentences were lowercased, 8 As a baseline, we train a standard phrase-based system with a bidirectional MSD lexicalized reordering model using word-based extraction. Our CCG-augmented reordering system has all of the model components of the baseline, as well as a bidirectional orientation reordering model over targetside multiword syntactic labels. To directly test the effect of using CCG parse charts &#8212; as opposed to simply using a CCG supertagger &#8212; we also added a CCG supertag bidirectional MSD reordering model to the baseline set-up. All systems were tuned and tested with distortion limit of 15 words, and test runs were performed with and without 200-best minimum Bayes&#8217; risk (MBR) hypothesis selection (Kumar and Byrne, 2004). To acquire CCG labels for our English parallel data, we use the C&amp;C CCG toolkit of Clark and Curran (2007). We build CCG parse charts by reworking the normal-form derivations from the C&amp;C parser in all spuriously ambiguous ways, as described in Section 2.1. For supertags, we tag with the C&amp;C supertagger. Rather than training separate phrase tables for our CCG systems, however, we instead decorate the baseline phrase tables with CCG multiword labels or supertags. To smooth over parsing and tagging errors, we only use those labels whose relative frequency (rf) is sufficiently high w.r.t. the most frequent label for that phrase pair
LAB* [f&#8660;e] . More precisely, for each phrase pair, we
use the set of labels: 9
{LAB [f&#8660;e] |rf(LAB [f&#8660;e] ) &#8805; &#946; &#183; rf(LAB* [f&#8660;e] )}
This is reminiscent of the &#946;-best tagging approach of (Clark and Curran, 2004), but performed in a batch process when creating the syntactic phrase tables (both supertag and CCG chart-derived). We set
8 N.B. We use Penn Treebank III-compatible tokenization for
English and a specially designed tokenization script for Urdu, cf. (Baker et al., 2010), Appendix C 9 Recalling that &#8776;31% of the time, a phrase pair might have
a list of labels, rather than a single label, the word &#8216;label&#8217; here refers to a single token that can be the concatenation of multiple symbols.
&#946; = 0.5 in all of our CCG experiments.
To minimize disruption to the Moses decoder (which only supports single-word labels in phrasebased mode), we project multiword labels across the words they label as single-word factors with bookkeeping characters, similar to the &#8220;microtag&#8221; annotations of asynchronous factored translation models (Cettolo et al., 2008). We modified to the decoder to reassemble the multiple single-word factors into a single label before querying the reordering model. As an example, we might have the phrase pair le v&#233;lo rouge &#8660; the|NP( red|NP+ bike|NP) . Before querying the reordering model, the factor sequence NP( NP+ NP) is collapsed into the single, multiword label &#8216;NP&#8217; by the rule schema X( . . . X+ . . . X) &#8594; X.
We train a language model using all of the WMT 2011 NEWSCRAWL, NEWSCOMENTARY and EU-
ROPARL monolingual data, 10 tokenized and lowercased as above, but de-duplicated to address the redundancy of the Web-crawled portion of that data set. We also train a separate language model on the English portion of the Urdu-English parallel corpus (minus the dictionary entries), and interpolate the two models by optimizing perplexity on our tuning set.
Table 2 lists our results, where we see significant improvement over both of our baselines, lexicalized reordering (LR) and supertag reordering plus lexicalized reordering (ST+LR). To test the effects of the lexicalized reordering model itself, we also evaluate a system with no lexicalized reordering model
10 http://www.statmt.org/wmt11/
translation-task.html
(only a distance-based distortion model). This last system (a system which almost always prefers not to reorder) is considerably worse than all other systems, demonstrating the need for non-monotonic reordering configurations when accounting for the Urdu-English data.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We empirically validate our technique by translating from Urdu into English.</text>
              <doc_id>155</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Urdu has a canonical word order of SOV &#8212; subject, object(s), verb &#8212; whereas English has SVO, leading to indefinitely long distances between corresponding verbs and objects.</text>
              <doc_id>156</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This language pair is therefore a strong test case for a reordering model.</text>
              <doc_id>157</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For decoding we use Moses (Koehn et al., 2007), a state-of-the-art PMT decoder, with IRST LM (Federico and Cettolo, 2007) for language model inference.</text>
              <doc_id>158</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For Urdu-English parallel data, we use the OpenMT 2008 training set which consists of 88 thousand sentence-level translations and a translation dictionary of &#8776;114 thousand word and phrase translations.</text>
              <doc_id>159</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We use half of the OpenMT 2008 Urdu- English evaluation data for development and perform development testing on the other half.</text>
              <doc_id>160</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Both halves are &#8776;900 sentences long and were balanced to contain approximately the same number of tokens.</text>
              <doc_id>161</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Our blind test set is the entire OpenMT 2009 Urdu-English evaluation set.</text>
              <doc_id>162</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>All evaluation sets had 4 reference translations for each tuning or testing instance.</text>
              <doc_id>163</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>All system component weights were tuned using minimum error-rate training (Och, 2003), with three tuning runs for each condition.</text>
              <doc_id>164</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The data was normalized, tokenized and the English sentences were lowercased, 8 As a baseline, we train a standard phrase-based system with a bidirectional MSD lexicalized reordering model using word-based extraction.</text>
              <doc_id>165</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Our CCG-augmented reordering system has all of the model components of the baseline, as well as a bidirectional orientation reordering model over targetside multiword syntactic labels.</text>
              <doc_id>166</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>To directly test the effect of using CCG parse charts &#8212; as opposed to simply using a CCG supertagger &#8212; we also added a CCG supertag bidirectional MSD reordering model to the baseline set-up.</text>
              <doc_id>167</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>All systems were tuned and tested with distortion limit of 15 words, and test runs were performed with and without 200-best minimum Bayes&#8217; risk (MBR) hypothesis selection (Kumar and Byrne, 2004).</text>
              <doc_id>168</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>To acquire CCG labels for our English parallel data, we use the C&amp;C CCG toolkit of Clark and Curran (2007).</text>
              <doc_id>169</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>We build CCG parse charts by reworking the normal-form derivations from the C&amp;C parser in all spuriously ambiguous ways, as described in Section 2.1.</text>
              <doc_id>170</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>For supertags, we tag with the C&amp;C supertagger.</text>
              <doc_id>171</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>Rather than training separate phrase tables for our CCG systems, however, we instead decorate the baseline phrase tables with CCG multiword labels or supertags.</text>
              <doc_id>172</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>To smooth over parsing and tagging errors, we only use those labels whose relative frequency (rf) is sufficiently high w.r.t.</text>
              <doc_id>173</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>the most frequent label for that phrase pair</text>
              <doc_id>174</doc_id>
              <sec_id>16</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>LAB* [f&#8660;e] .</text>
              <doc_id>175</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>More precisely, for each phrase pair, we</text>
              <doc_id>176</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>use the set of labels: 9</text>
              <doc_id>177</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{LAB [f&#8660;e] |rf(LAB [f&#8660;e] ) &#8805; &#946; &#183; rf(LAB* [f&#8660;e] )}</text>
              <doc_id>178</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>This is reminiscent of the &#946;-best tagging approach of (Clark and Curran, 2004), but performed in a batch process when creating the syntactic phrase tables (both supertag and CCG chart-derived).</text>
              <doc_id>179</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We set</text>
              <doc_id>180</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>8 N.B.</text>
              <doc_id>181</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We use Penn Treebank III-compatible tokenization for</text>
              <doc_id>182</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>English and a specially designed tokenization script for Urdu, cf. (Baker et al., 2010), Appendix C 9 Recalling that &#8776;31% of the time, a phrase pair might have</text>
              <doc_id>183</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>a list of labels, rather than a single label, the word &#8216;label&#8217; here refers to a single token that can be the concatenation of multiple symbols.</text>
              <doc_id>184</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#946; = 0.5 in all of our CCG experiments.</text>
              <doc_id>185</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To minimize disruption to the Moses decoder (which only supports single-word labels in phrasebased mode), we project multiword labels across the words they label as single-word factors with bookkeeping characters, similar to the &#8220;microtag&#8221; annotations of asynchronous factored translation models (Cettolo et al., 2008).</text>
              <doc_id>186</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We modified to the decoder to reassemble the multiple single-word factors into a single label before querying the reordering model.</text>
              <doc_id>187</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>As an example, we might have the phrase pair le v&#233;lo rouge &#8660; the|NP( red|NP+ bike|NP) .</text>
              <doc_id>188</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Before querying the reordering model, the factor sequence NP( NP+ NP) is collapsed into the single, multiword label &#8216;NP&#8217; by the rule schema X( .</text>
              <doc_id>189</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>190</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>191</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>X+ .</text>
              <doc_id>192</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>193</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>194</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>X) &#8594; X.</text>
              <doc_id>195</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We train a language model using all of the WMT 2011 NEWSCRAWL, NEWSCOMENTARY and EU-</text>
              <doc_id>196</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>ROPARL monolingual data, 10 tokenized and lowercased as above, but de-duplicated to address the redundancy of the Web-crawled portion of that data set.</text>
              <doc_id>197</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We also train a separate language model on the English portion of the Urdu-English parallel corpus (minus the dictionary entries), and interpolate the two models by optimizing perplexity on our tuning set.</text>
              <doc_id>198</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Table 2 lists our results, where we see significant improvement over both of our baselines, lexicalized reordering (LR) and supertag reordering plus lexicalized reordering (ST+LR).</text>
              <doc_id>199</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To test the effects of the lexicalized reordering model itself, we also evaluate a system with no lexicalized reordering model</text>
              <doc_id>200</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>10 http://www.statmt.org/wmt11/</text>
              <doc_id>201</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>translation-task.html</text>
              <doc_id>202</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(only a distance-based distortion model).</text>
              <doc_id>203</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This last system (a system which almost always prefers not to reorder) is considerably worse than all other systems, demonstrating the need for non-monotonic reordering configurations when accounting for the Urdu-English data.</text>
              <doc_id>204</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>6 Analysis and Discussion</title>
        <text>Our CCG system (CCG+LR) outperforms both baseline systems (LR and ST+LR) in a majority of metrics in both MBR and non-MBR conditions. We see that, even though MBR decoding closes the performance gap somewhat, our system continues to match or outperform (if sometimes insignificantly) in all areas. Note that the CCG+LR non-MBR configuration outperforms both LR and ST+LR in MBR and non-MBR decoding conditions in its ME-
TEOR score on the NIST-09 test set. We note also
that, in the NIST-09 test case, the CCG+LR system&#8217;s poorer performance is perhaps due to a mismatch in hypothesis length, which could be harming its scores, particularly the BLEU brevity penalty.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Our CCG system (CCG+LR) outperforms both baseline systems (LR and ST+LR) in a majority of metrics in both MBR and non-MBR conditions.</text>
              <doc_id>205</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We see that, even though MBR decoding closes the performance gap somewhat, our system continues to match or outperform (if sometimes insignificantly) in all areas.</text>
              <doc_id>206</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Note that the CCG+LR non-MBR configuration outperforms both LR and ST+LR in MBR and non-MBR decoding conditions in its ME-</text>
              <doc_id>207</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>TEOR score on the NIST-09 test set.</text>
              <doc_id>208</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We note also</text>
              <doc_id>209</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>that, in the NIST-09 test case, the CCG+LR system&#8217;s poorer performance is perhaps due to a mismatch in hypothesis length, which could be harming its scores, particularly the BLEU brevity penalty.</text>
              <doc_id>210</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>6.1 Poor Performance of CCG Supertag Model</title>
            <text>We have no firm explanation for the poor performance of the CCG supertag model (ST- LR), but it is important to note that the supertag reordering model does not unify statistics across phrases of different lengths, as the CCG chart-derived model does. E.g., the phrase pair den Weg f&#252;r eine &#8660; the way for an will query the CCG chart-derived reordering model with the same symbol as the phrase pair den Weg f&#252;r eine baldige &#8660; the way for an early
twenty-seven year old abdullah britain blasts in hatcheries planning accused of is .
CCG+LR: twenty-seven year old abdullah is accused of planning hatcheries blasts in britain .
now musharraf resignation give should .
CCG+LR: now musharraf should give resignation .
&#8212; viz., NP/N. The CCG supertag model, however, will have two distinct label sequences for these phrases &#8212; viz., NP/N N (NP\NP)/NP NP/N and NP/N N (NP\NP)/NP NP/N N/N, resp. &#8212; both of which could be reduced to the single label, NP/N, using CCG&#8217;s syntactic combinators. The supertag system does not have the means of relating the reordering patterns of strings of symbols such as this. 11 Such data fragmentation may be leading to decreased performance, which would indicate the use of recursive CCG syntax.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We have no firm explanation for the poor performance of the CCG supertag model (ST- LR), but it is important to note that the supertag reordering model does not unify statistics across phrases of different lengths, as the CCG chart-derived model does.</text>
                  <doc_id>211</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>E.g., the phrase pair den Weg f&#252;r eine &#8660; the way for an will query the CCG chart-derived reordering model with the same symbol as the phrase pair den Weg f&#252;r eine baldige &#8660; the way for an early</text>
                  <doc_id>212</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>twenty-seven year old abdullah britain blasts in hatcheries planning accused of is .</text>
                  <doc_id>213</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>CCG+LR: twenty-seven year old abdullah is accused of planning hatcheries blasts in britain .</text>
                  <doc_id>214</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>now musharraf resignation give should .</text>
                  <doc_id>215</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>CCG+LR: now musharraf should give resignation .</text>
                  <doc_id>216</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8212; viz., NP/N.</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The CCG supertag model, however, will have two distinct label sequences for these phrases &#8212; viz., NP/N N (NP\NP)/NP NP/N and NP/N N (NP\NP)/NP NP/N N/N, resp.</text>
                  <doc_id>218</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>&#8212; both of which could be reduced to the single label, NP/N, using CCG&#8217;s syntactic combinators.</text>
                  <doc_id>219</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The supertag system does not have the means of relating the reordering patterns of strings of symbols such as this.</text>
                  <doc_id>220</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>11 Such data fragmentation may be leading to decreased performance, which would indicate the use of recursive CCG syntax.</text>
                  <doc_id>221</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>6.2 Qualitative Improvements</title>
            <text>In addition to improved metric scores, we noted real qualitative improvements in some examples, as Figure 5 shows. These examples demonstrate the ability of the reordering model to navigate the massive, structure-governed reorderings needed to approximate the correct answer with the phrase inventory it is given.
11 Its reordering table has more than twice as many entries as
that of the chart-derived model.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In addition to improved metric scores, we noted real qualitative improvements in some examples, as Figure 5 shows.</text>
                  <doc_id>222</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These examples demonstrate the ability of the reordering model to navigate the massive, structure-governed reorderings needed to approximate the correct answer with the phrase inventory it is given.</text>
                  <doc_id>223</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>11 Its reordering table has more than twice as many entries as</text>
                  <doc_id>224</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>that of the chart-derived model.</text>
                  <doc_id>225</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>6.3 Comparison to the State of the Art</title>
            <text>To our knowledge, the state of the art in Urdu- English translation using the OpenMT data is listed in the NIST OpenMT 2009 evaluation results (http://www.itl.nist.gov/iad/ mig/tests/mt/2009/ResultsRelease/ currentUrdu.html). This evaluation accepted only single system outputs, and used cased references. Therefore we had to choose a single system output and recase its text. For system selection, we picked the tuned system that performed best on the development test set. For recasing, we trained a lowercased-to-cased monolingual phrase-based &#8220;translation model&#8221; with no reordering and a cased language model, similar to what is described in (Baker et al., 2010). The training text is simply the non-dictionary portion of the Urdu-English parallel corpus, with its lowercased version as the source and the original cased text as the target, both halves tokenized as above. We tuned on a similar version of the English half of our tuning
references. The lowercased output of our system is fed to this model and the first token of each casing &#8220;translation&#8221; is capitalized (if not already). The official metric of the NIST 2009 evaluation is BLEU (as implemented in the NIST-distributed mteval-v13a.pl script). 12 The best-performing system in the constrained data evaluation scored 0.312 w.r.t. the cased references, with the second and third place systems scoring 0.2395 and 0.2322, respectively. 13 Our best performing MERT-tuned system (as determined on the devtest data) scores 0.2734 on the test set, putting it between the top two systems. For comparison, our devtest-best baseline LR system scores 0.2683 on the test set.
While is generally not useful to test experimental manipulations based on a single tuning run (Clark et al., 2011) and with different monolingual language modelling data, we note these figures simply to situate our results within the state of the art.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>To our knowledge, the state of the art in Urdu- English translation using the OpenMT data is listed in the NIST OpenMT 2009 evaluation results (http://www.itl.nist.gov/iad/ mig/tests/mt/2009/ResultsRelease/ currentUrdu.html).</text>
                  <doc_id>226</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This evaluation accepted only single system outputs, and used cased references.</text>
                  <doc_id>227</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore we had to choose a single system output and recase its text.</text>
                  <doc_id>228</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For system selection, we picked the tuned system that performed best on the development test set.</text>
                  <doc_id>229</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For recasing, we trained a lowercased-to-cased monolingual phrase-based &#8220;translation model&#8221; with no reordering and a cased language model, similar to what is described in (Baker et al., 2010).</text>
                  <doc_id>230</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The training text is simply the non-dictionary portion of the Urdu-English parallel corpus, with its lowercased version as the source and the original cased text as the target, both halves tokenized as above.</text>
                  <doc_id>231</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>We tuned on a similar version of the English half of our tuning</text>
                  <doc_id>232</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>references.</text>
                  <doc_id>233</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The lowercased output of our system is fed to this model and the first token of each casing &#8220;translation&#8221; is capitalized (if not already).</text>
                  <doc_id>234</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The official metric of the NIST 2009 evaluation is BLEU (as implemented in the NIST-distributed mteval-v13a.pl script).</text>
                  <doc_id>235</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>12 The best-performing system in the constrained data evaluation scored 0.312 w.r.t.</text>
                  <doc_id>236</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>the cased references, with the second and third place systems scoring 0.2395 and 0.2322, respectively.</text>
                  <doc_id>237</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>13 Our best performing MERT-tuned system (as determined on the devtest data) scores 0.2734 on the test set, putting it between the top two systems.</text>
                  <doc_id>238</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>For comparison, our devtest-best baseline LR system scores 0.2683 on the test set.</text>
                  <doc_id>239</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>While is generally not useful to test experimental manipulations based on a single tuning run (Clark et al., 2011) and with different monolingual language modelling data, we note these figures simply to situate our results within the state of the art.</text>
                  <doc_id>240</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>7 Conclusion</title>
        <text>We have argued for the use of CCG in phrasebased translation, due to its flexibility in providing a wealth of different bracketings that better accommodate lexical translation strings. We have also presented a novel method for using CCG constituent labels in a syntactic reordering model where the syntactic labels span multiple words, do not cross translation constituent boundaries and are tailored specifically to each translation constituent. The result is a significant improvement in Urdu-English (SOV &#8594; SVO) translation scores over two baselines: a traditional phrase-based baseline with a lexicalized reordering model and a phrase-based baseline with an additional supertag reordering model. Moreover, we have provided qualitative examples that confirm the improvements in automatic metrics. In future work we would like explore whether further improvements can be gained by using more sophisticated reordering models, such as reordering graphs (Su et al., 2010) and hierarchical reordering models (Galley and Manning, 2008) both for our word-based and syntactic reordering models. Further, as in prior work (Zollmann et al., 2006; Shen
12 ftp://jaguar.ncsl.nist.gov/mt/
resources/mteval-v13a-20091001.tar.gz. 13 We exclude combination entries that are combinations of
multiple systems with different algorithmic approaches.
et al., 2010; Almaghout et al., 2010), our categorial labels could also be used to derive CCG-augmented SCFG rules, both lexicalized and unlexicalized, cf. (Zhao and Al-onaizan, 2008) &#8212; the latter being the SCFG analogue of our current model.
Acknowledgments
The authors would like to thank Chong Min Lee, Aoife Cahill and Nitin Madnani at ETS for taking the time to read earlier drafts of this (and closely related) work. Their comments and suggestions made this a better paper. We would also like to thank the anonymous reviewers for their very helpful feedback. The views expressed in this paper do not necessarily reflect those of The Ohio State University or of Educational Testing Service.
References
Hala Almaghout, Jie Jiang, and Andy Way. 2010. CCG Augmented Hierarchical Phrase-based Machine Translation. In Proceedings of the 7th International Workshop on Spoken Language Translation, Paris, France. Amittai Axelrod, Ra Birch Mayne, Chris Callison-Burch,
Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT-05), Pittsburgh, PA, USA. Emmon Bach. l976. An Extension of Classical Transformational Grammar. In Proceedings of the 1976 Conference on Problems of Linguistic Metatheory, pages 183&#8211;224, East Lansing, MI, USA. Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith, Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine, Mike Kayser, Lori Levin, Justin Martineau, Jim Mayeld, Scott Miller, Aaron Phillips, Andrew Philpot, Christine Piatko, Lane Schwartz, and David Zajic. 2010. Semantically informed machine translation. Technical Report 002, Johns Hopkins University, Baltimore, MD, Human Language Technology Center of Excellence. Mauro Cettolo, Marcello Federico, Daniele Pighin, and
Nicola Bertoldi. 2008. Shallow-syntax Phrase-based
Translation: Joint versus Factored String-to-chunk Models. In Proceedings of AMTA 2008, Honolulu, HI, USA. Stephen Clark and James R. Curran. 2004. The Importance of Supertagging for Wide-Coverage CCG Parsing. In Proceedings of the 20th International Con-
ference on Computational Linguistics (COLING-04),
Geneva, Switzerland. Stephen Clark and James R. Curran. 2007. Wide- Coverage Efficient Statistical Parsing with CCG and Log-Linear Models. Computational Linguistics, 33(4):493&#8211;552.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better Hypothesis Testing for Machine Translation: Controlling for Optimizer Instability. In Proceedings of the Meeting of the Association for Computational Linguistics (ACL-11), Portland, OR, USA. Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause Restructuring for Statistical Machine Translation. In Proceedings of the Association for Computational Linguistics (ACL-05), Ann Arbor, MI, USA. Marcello Federico and Mauro Cettolo. 2007. Efficient
Handling of n-gram Language Models for Statistical Machine Translation. In Proceedings of Association for Computational Linguistics, Prague, The Czech Republic. Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reordering Model. In Proceedings of EMNLP-08.
Hany Hassan. 2009. Lexical Syntax for Statistical Machine Translation. Ph.D. thesis, Dublin City University, Dublin, Ireland. Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank. Computational Linguistics, 33(3):355&#8211;396. Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical Phrase-based Translation. In Proceedings of NAACL-HLT, pages 48&#8211;54, Edmonton, Alberta, CA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proceedings of the Association for Computational Linguistics, Companion Volume Proceedings of the Demo and Poster Sessions, Prague, Czech Republic, June. Shankar Kumar and William Byrne. 2004. Minimum
Bayes-Risk Decoding for Statistical Machine Translation. In Proceedings of HLT-NAACL. Wang Ling, Jo ao Gra&#231;a, David Martins de Matos, Isabel Trancoso, and Alan Black. 2011. Discriminative Phrase-based Lexicalized Reordering Models using Weighted Reordering Graphs. In Proceedings of the 5 th International Joint Conference on Natural Language Processing.
Franz Joseph Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41 st Annual Meeting of the Association for Computational Linguistics, pages 160&#8211;167, Sapporo, Japan. Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
New String-to-dependency Machine Translation Algorithm with a Target Dependency Language Model. In Proceedings of the Joint Meeting of the Association for Computational Linguistics and Human Language Technologies (ACL-08:HLT), Columbus, OH, USA. Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-Dependency Statistical Machine Translation. Computational Linguistics, 36(4):649&#8211;671. Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA. Jinsong Su, Yang Liu, Yajuan L&#252;, Haitao Mi, and Qun
Liu. 2010. Learning Lexicalized Reordering Models from Reordering Graphs. In Proceedings of the ACL 2010; Short Papers. Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Proceedings of HLT-NAACL 2004: Short Papers, HLT- NAACL-Short &#8217;04. Christoph Tillmann. 2008. A Rule-Driven Dynamic Programming Decoder for Statistical MT. In Proceedings of the Second Workshop on Syntax and Structure in Statistical Translation (SSST-08). Ashish Venugopal, Andreas Zollmann, and Stephan Vogel. 2007. An Efficient Two-pass Approach to Synchronous-CFG Driven Statistical MT. In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL-07), Rochester, NY. Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese Syntactic Reordering for Statistical Machine
Translation. In Proceedings of EMNLP/CoNLL-07,
Prague, The Czech Republic. Fei Xia and Michael McCord. 2004. Improving a Statistical MT System with Automatically Learned Rewrite Patterns. In Proceedings of International Conference on Computational Linguistics (COLING-04), Geneva, Switzerland. Yushi Xu and Stephanie Seneff. 2008. Two-stage Translation: A Combined Linguistic and Statistical Machine
Translation Framework. In Proceedings of the 8 th Conference of the Association for Machine Translation in the Americas (AMTA-08), Waikiki, Honolulu, HI, USA. Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-Based Statistical Machine Translation. In M. Jarke, J. Koehler, and G. Lakemeyer, editors, KI- 2002: Advances in Artificial Intelligence, Proceedings of the 25 th Annual German Conference on AI, (KI- 2002), pages 18&#8211;32. Springer Verlag, Aachen, Germany.
Bing Zhao and Yaser Al-onaizan. 2008. Generalizing Local and Non-Local Word-Reordering Patterns for Syntax-Based Machine Translation. In Proceedings of The Conference on Empirical Methods in Natural Language Processig (EMNLP-08). Andreas Zollmann, Ashish Venugopal, Stephan Vogel,
and Alex Waibel. 2006. The CMU-UKA Syntax Augmented Machine Translation System for IWSLT-06. In Proceedings of International Workshop on Spoken Language Translation (IWSLT-06), Kyoto, Japan.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have argued for the use of CCG in phrasebased translation, due to its flexibility in providing a wealth of different bracketings that better accommodate lexical translation strings.</text>
              <doc_id>241</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We have also presented a novel method for using CCG constituent labels in a syntactic reordering model where the syntactic labels span multiple words, do not cross translation constituent boundaries and are tailored specifically to each translation constituent.</text>
              <doc_id>242</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The result is a significant improvement in Urdu-English (SOV &#8594; SVO) translation scores over two baselines: a traditional phrase-based baseline with a lexicalized reordering model and a phrase-based baseline with an additional supertag reordering model.</text>
              <doc_id>243</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, we have provided qualitative examples that confirm the improvements in automatic metrics.</text>
              <doc_id>244</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In future work we would like explore whether further improvements can be gained by using more sophisticated reordering models, such as reordering graphs (Su et al., 2010) and hierarchical reordering models (Galley and Manning, 2008) both for our word-based and syntactic reordering models.</text>
              <doc_id>245</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Further, as in prior work (Zollmann et al., 2006; Shen</text>
              <doc_id>246</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>12 ftp://jaguar.ncsl.nist.gov/mt/</text>
              <doc_id>247</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>resources/mteval-v13a-20091001.tar.gz.</text>
              <doc_id>248</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>13 We exclude combination entries that are combinations of</text>
              <doc_id>249</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>multiple systems with different algorithmic approaches.</text>
              <doc_id>250</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>et al., 2010; Almaghout et al., 2010), our categorial labels could also be used to derive CCG-augmented SCFG rules, both lexicalized and unlexicalized, cf. (Zhao and Al-onaizan, 2008) &#8212; the latter being the SCFG analogue of our current model.</text>
              <doc_id>251</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Acknowledgments</text>
              <doc_id>252</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The authors would like to thank Chong Min Lee, Aoife Cahill and Nitin Madnani at ETS for taking the time to read earlier drafts of this (and closely related) work.</text>
              <doc_id>253</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Their comments and suggestions made this a better paper.</text>
              <doc_id>254</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We would also like to thank the anonymous reviewers for their very helpful feedback.</text>
              <doc_id>255</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The views expressed in this paper do not necessarily reflect those of The Ohio State University or of Educational Testing Service.</text>
              <doc_id>256</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>References</text>
              <doc_id>257</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Hala Almaghout, Jie Jiang, and Andy Way.</text>
              <doc_id>258</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>259</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>CCG Augmented Hierarchical Phrase-based Machine Translation.</text>
              <doc_id>260</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 7th International Workshop on Spoken Language Translation, Paris, France.</text>
              <doc_id>261</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Amittai Axelrod, Ra Birch Mayne, Chris Callison-Burch,</text>
              <doc_id>262</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Miles Osborne, and David Talbot.</text>
              <doc_id>263</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>264</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation.</text>
              <doc_id>265</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT-05), Pittsburgh, PA, USA.</text>
              <doc_id>266</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Emmon Bach.</text>
              <doc_id>267</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>l976.</text>
              <doc_id>268</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>An Extension of Classical Transformational Grammar.</text>
              <doc_id>269</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 1976 Conference on Problems of Linguistic Metatheory, pages 183&#8211;224, East Lansing, MI, USA.</text>
              <doc_id>270</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf</text>
              <doc_id>271</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Brown, Chris Callison-Burch, Glen Coppersmith, Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine, Mike Kayser, Lori Levin, Justin Martineau, Jim Mayeld, Scott Miller, Aaron Phillips, Andrew Philpot, Christine Piatko, Lane Schwartz, and David Zajic.</text>
              <doc_id>272</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>273</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Semantically informed machine translation.</text>
              <doc_id>274</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Technical Report 002, Johns Hopkins University, Baltimore, MD, Human Language Technology Center of Excellence.</text>
              <doc_id>275</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Mauro Cettolo, Marcello Federico, Daniele Pighin, and</text>
              <doc_id>276</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Nicola Bertoldi.</text>
              <doc_id>277</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>278</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Shallow-syntax Phrase-based</text>
              <doc_id>279</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Translation: Joint versus Factored String-to-chunk Models.</text>
              <doc_id>280</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of AMTA 2008, Honolulu, HI, USA.</text>
              <doc_id>281</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Stephen Clark and James R. Curran.</text>
              <doc_id>282</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2004.</text>
              <doc_id>283</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The Importance of Supertagging for Wide-Coverage CCG Parsing.</text>
              <doc_id>284</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 20th International Con-</text>
              <doc_id>285</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>ference on Computational Linguistics (COLING-04),</text>
              <doc_id>286</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Geneva, Switzerland.</text>
              <doc_id>287</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Stephen Clark and James R. Curran.</text>
              <doc_id>288</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>289</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Wide- Coverage Efficient Statistical Parsing with CCG and Log-Linear Models.</text>
              <doc_id>290</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 33(4):493&#8211;552.</text>
              <doc_id>291</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith.</text>
              <doc_id>292</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2011.</text>
              <doc_id>293</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Better Hypothesis Testing for Machine Translation: Controlling for Optimizer Instability.</text>
              <doc_id>294</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Meeting of the Association for Computational Linguistics (ACL-11), Portland, OR, USA.</text>
              <doc_id>295</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Michael Collins, Philipp Koehn, and Ivona Kucerova.</text>
              <doc_id>296</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>297</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Clause Restructuring for Statistical Machine Translation.</text>
              <doc_id>298</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Association for Computational Linguistics (ACL-05), Ann Arbor, MI, USA.</text>
              <doc_id>299</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Marcello Federico and Mauro Cettolo.</text>
              <doc_id>300</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>301</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Efficient</text>
              <doc_id>302</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Handling of n-gram Language Models for Statistical Machine Translation.</text>
              <doc_id>303</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of Association for Computational Linguistics, Prague, The Czech Republic.</text>
              <doc_id>304</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Michel Galley and Christopher D. Manning.</text>
              <doc_id>305</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>306</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>A</text>
              <doc_id>307</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Simple and Effective Hierarchical Phrase Reordering Model.</text>
              <doc_id>308</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of EMNLP-08.</text>
              <doc_id>309</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Hany Hassan.</text>
              <doc_id>310</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>311</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Lexical Syntax for Statistical Machine Translation.</text>
              <doc_id>312</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Ph.D.</text>
              <doc_id>313</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>thesis, Dublin City University, Dublin, Ireland.</text>
              <doc_id>314</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Julia Hockenmaier and Mark Steedman.</text>
              <doc_id>315</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>316</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank.</text>
              <doc_id>317</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 33(3):355&#8211;396.</text>
              <doc_id>318</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Philipp Koehn, Franz Joseph Och, and Daniel Marcu.</text>
              <doc_id>319</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>320</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>Statistical Phrase-based Translation.</text>
              <doc_id>321</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of NAACL-HLT, pages 48&#8211;54, Edmonton, Alberta, CA.</text>
              <doc_id>322</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst.</text>
              <doc_id>323</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>324</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Moses: Open Source</text>
              <doc_id>325</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Toolkit for Statistical Machine Translation.</text>
              <doc_id>326</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Association for Computational Linguistics, Companion Volume Proceedings of the Demo and Poster Sessions, Prague, Czech Republic, June.</text>
              <doc_id>327</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Shankar Kumar and William Byrne.</text>
              <doc_id>328</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2004.</text>
              <doc_id>329</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Minimum</text>
              <doc_id>330</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Bayes-Risk Decoding for Statistical Machine Translation.</text>
              <doc_id>331</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of HLT-NAACL.</text>
              <doc_id>332</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Wang Ling, Jo ao Gra&#231;a, David Martins de Matos, Isabel Trancoso, and Alan Black.</text>
              <doc_id>333</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2011.</text>
              <doc_id>334</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Discriminative Phrase-based Lexicalized Reordering Models using Weighted Reordering Graphs.</text>
              <doc_id>335</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 5 th International Joint Conference on Natural Language Processing.</text>
              <doc_id>336</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Franz Joseph Och.</text>
              <doc_id>337</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>338</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Minimum Error Rate Training in Statistical Machine Translation.</text>
              <doc_id>339</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 41 st Annual Meeting of the Association for Computational Linguistics, pages 160&#8211;167, Sapporo, Japan.</text>
              <doc_id>340</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Libin Shen, Jinxi Xu, and Ralph Weischedel.</text>
              <doc_id>341</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>342</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>A</text>
              <doc_id>343</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>New String-to-dependency Machine Translation Algorithm with a Target Dependency Language Model.</text>
              <doc_id>344</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Joint Meeting of the Association for Computational Linguistics and Human Language Technologies (ACL-08:HLT), Columbus, OH, USA.</text>
              <doc_id>345</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Libin Shen, Jinxi Xu, and Ralph Weischedel.</text>
              <doc_id>346</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>347</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>String-to-Dependency Statistical Machine Translation.</text>
              <doc_id>348</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 36(4):649&#8211;671.</text>
              <doc_id>349</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Mark Steedman.</text>
              <doc_id>350</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2000.</text>
              <doc_id>351</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The Syntactic Process.</text>
              <doc_id>352</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>MIT</text>
              <doc_id>353</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Press, Cambridge, MA, USA.</text>
              <doc_id>354</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Jinsong Su, Yang Liu, Yajuan L&#252;, Haitao Mi, and Qun</text>
              <doc_id>355</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Liu.</text>
              <doc_id>356</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>357</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Learning Lexicalized Reordering Models from Reordering Graphs.</text>
              <doc_id>358</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the ACL 2010; Short Papers.</text>
              <doc_id>359</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Christoph Tillmann.</text>
              <doc_id>360</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2004.</text>
              <doc_id>361</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>A Unigram Orientation</text>
              <doc_id>362</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Model for Statistical Machine Translation.</text>
              <doc_id>363</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of HLT-NAACL 2004: Short Papers, HLT- NAACL-Short &#8217;04.</text>
              <doc_id>364</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Christoph Tillmann.</text>
              <doc_id>365</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>366</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>A Rule-Driven Dynamic Programming Decoder for Statistical MT.</text>
              <doc_id>367</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Second Workshop on Syntax and Structure in Statistical Translation (SSST-08).</text>
              <doc_id>368</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Ashish Venugopal, Andreas Zollmann, and Stephan Vogel.</text>
              <doc_id>369</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>370</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>An Efficient Two-pass Approach to Synchronous-CFG Driven Statistical MT.</text>
              <doc_id>371</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL-07), Rochester, NY.</text>
              <doc_id>372</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Chao Wang, Michael Collins, and Philipp Koehn.</text>
              <doc_id>373</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>374</doc_id>
              <sec_id>11</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Chinese Syntactic Reordering for Statistical Machine</text>
              <doc_id>375</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Translation.</text>
              <doc_id>376</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of EMNLP/CoNLL-07,</text>
              <doc_id>377</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Prague, The Czech Republic.</text>
              <doc_id>378</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Fei Xia and Michael McCord.</text>
              <doc_id>379</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>2004.</text>
              <doc_id>380</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Improving a Statistical MT System with Automatically Learned Rewrite Patterns.</text>
              <doc_id>381</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of International Conference on Computational Linguistics (COLING-04), Geneva, Switzerland.</text>
              <doc_id>382</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Yushi Xu and Stephanie Seneff.</text>
              <doc_id>383</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>384</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Two-stage Translation: A Combined Linguistic and Statistical Machine</text>
              <doc_id>385</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Translation Framework.</text>
              <doc_id>386</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 8 th Conference of the Association for Machine Translation in the Americas (AMTA-08), Waikiki, Honolulu, HI, USA.</text>
              <doc_id>387</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Richard Zens, Franz Josef Och, and Hermann Ney.</text>
              <doc_id>388</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2002.</text>
              <doc_id>389</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Phrase-Based Statistical Machine Translation.</text>
              <doc_id>390</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In M. Jarke, J. Koehler, and G. Lakemeyer, editors, KI- 2002: Advances in Artificial Intelligence, Proceedings of the 25 th Annual German Conference on AI, (KI- 2002), pages 18&#8211;32.</text>
              <doc_id>391</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Springer Verlag, Aachen, Germany.</text>
              <doc_id>392</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Bing Zhao and Yaser Al-onaizan.</text>
              <doc_id>393</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>394</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Generalizing Local and Non-Local Word-Reordering Patterns for Syntax-Based Machine Translation.</text>
              <doc_id>395</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of The Conference on Empirical Methods in Natural Language Processig (EMNLP-08).</text>
              <doc_id>396</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Andreas Zollmann, Ashish Venugopal, Stephan Vogel,</text>
              <doc_id>397</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and Alex Waibel.</text>
              <doc_id>398</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>399</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The CMU-UKA Syntax Augmented Machine Translation System for IWSLT-06.</text>
              <doc_id>400</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of International Workshop on Spoken Language Translation (IWSLT-06), Kyoto, Japan.</text>
              <doc_id>401</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Training data statistics (top to bottom): (1) % of sin- gle CCG labels spanning entire English translation phrases, (2) average length of English translation phrase, (3) average CCG label span and (4) average CCG labels per English translation phrase. (Maximum translation phrase length is 7 words.)</caption>
        <reference_text>In PAGE 4: ... In this way we find single labels for 69% of the English-side phrase training instances.  Table1  gives more details. 6Type-raisings are almost always possible, and will always be closer to the top-level symbol....</reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>None</cell>
              <cell>UR.-EN.</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>SINGLE-LABEL COVERAGE</cell>
              <cell>69%</cell>
            </row>
            <row>
              <cell>AVE. EN. PHRASE LEN.</cell>
              <cell>2.8 wds</cell>
            </row>
            <row>
              <cell>AVE. CCG LABEL SPAN</cell>
              <cell>2.3 wds</cell>
            </row>
            <row>
              <cell>AVE. CCG LABS/ENTRY</cell>
              <cell>1.4</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Case-insensitive BLEU-4, METEOR, TER and hypothesis/reference length ratio (LENGTH) for a lexicalized reordering baseline (LR), a system with only a distance-based distortion model (NO-LR), a system with an additional CCG supertag reordering model (ST+LR) and our system with an additional CCG chart-derived reordering model (CCG+LR). Systems were run with (left of slash) and without (right of slash) 200-best-list MBR hypothesis selection. All boldfaced results were found to be significantly better than the baseline at &#8805; the 95% confidence level using method described in (Clark et al., 2011) with 3 separate MERT tuning runs for each system. Non-boldfaced numbers are statistically indistinguishable from (or worse than) the baseline.</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>DEVTEST (NIST-08) (MBR/NON-MBR)</cell>
              <cell>NIST-09 TEST (MBR/NON-MBR)</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell></cell>
              <cell>BLEU-4</cell>
              <cell>METEOR</cell>
              <cell>TER</cell>
              <cell>LENGTH</cell>
              <cell>BLEU-4</cell>
              <cell>METEOR</cell>
              <cell>TER</cell>
              <cell>LENGTH</cell>
            </row>
            <row>
              <cell>LR</cell>
              <cell>25.3/24.7</cell>
              <cell>28.3/28.2</cell>
              <cell>64.2/64.4</cell>
              <cell>98.2/97.6</cell>
              <cell>29.1/28.8</cell>
              <cell>30.0/28.8</cell>
              <cell>60.0/60.1</cell>
              <cell>98.2/97.8</cell>
            </row>
            <row>
              <cell>NO-LR</cell>
              <cell>22.5/22.1</cell>
              <cell>27.5/27.3</cell>
              <cell>66.3/66.3</cell>
              <cell>97.6/97.1</cell>
              <cell>26.2/25.8</cell>
              <cell>29.2/29.1</cell>
              <cell>61.9/62.0</cell>
              <cell>97.1/96.6</cell>
            </row>
            <row>
              <cell>ST+LR</cell>
              <cell>24.5/24.2</cell>
              <cell>28.4/28.3</cell>
              <cell>64.6/64.5</cell>
              <cell>97.9/97.3</cell>
              <cell>28.5/28.2</cell>
              <cell>30.0/30.0</cell>
              <cell>60.3/60.2</cell>
              <cell>97.9/97.3</cell>
            </row>
            <row>
              <cell>CCG+LR</cell>
              <cell>25.6/25.2</cell>
              <cell>28.7/28.5</cell>
              <cell>64.3/64.5</cell>
              <cell>98.7/98.1</cell>
              <cell>29.1/29.2</cell>
              <cell>30.1/30.2</cell>
              <cell>59.5/59.8</cell>
              <cell>97.4/97.9</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references/>
    <citations/>
  </content>
</document>
