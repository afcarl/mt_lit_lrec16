<document>
  <filename>N09-1025</filename>
  <authors>
    <author>David Chiang</author>
    <author>Kevin Knight</author>
  </authors>
  <title>11,001 New Features for Statistical Machine Translation &#8727;</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrasebased translation system and our syntax-based translation system. On a large-scale Chinese- English translation task, we obtain statistically significant improvements of +1.5 BLEU and +1.1 BLEU, respectively. We analyze the impact of the new features and the performance of the learning algorithm.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrasebased translation system and our syntax-based translation system.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>On a large-scale Chinese- English translation task, we obtain statistically significant improvements of +1.5 BLEU and +1.1 BLEU, respectively.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We analyze the impact of the new features and the performance of the learning algorithm.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>What linguistic features can improve statistical machine translation (MT)? This is a fundamental question for the discipline, particularly as it pertains to improving the best systems we have. Further:
&#8226; Do syntax-based translation systems have unique and effective levers to pull when designing new features?
&#8226; Can large numbers of feature weights be learned efficiently and stably on modest amounts of data?
In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntaxbased MT system&#8212;already the highest-scoring single system in the NIST 2008 Chinese-English common-data track&#8212;by +1.1 BLEU. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 BLEU improvement.
&#8727; This research was supported in part by DARPA contract
HR0011-06-C-0022 under subcontract to BBN Technologies.
Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntaxbased models have over other types of models.
The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>What linguistic features can improve statistical machine translation (MT)?</text>
              <doc_id>3</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This is a fundamental question for the discipline, particularly as it pertains to improving the best systems we have.</text>
              <doc_id>4</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Further:</text>
              <doc_id>5</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; Do syntax-based translation systems have unique and effective levers to pull when designing new features?</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; Can large numbers of feature weights be learned efficiently and stably on modest amounts of data?</text>
              <doc_id>7</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this paper, we address these questions by experimenting with a large number of new features.</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We add more than 250 features to improve a syntaxbased MT system&#8212;already the highest-scoring single system in the NIST 2008 Chinese-English common-data track&#8212;by +1.1 BLEU.</text>
              <doc_id>9</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 BLEU improvement.</text>
              <doc_id>10</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8727; This research was supported in part by DARPA contract</text>
              <doc_id>11</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>HR0011-06-C-0022 under subcontract to BBN Technologies.</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Thus they widen the advantage that syntaxbased models have over other types of models.</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003).</text>
              <doc_id>15</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.</text>
              <doc_id>16</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Related Work</title>
        <text>The work of Och et al (2004) is perhaps the bestknown study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders.
A third difficulty with Och et al.&#8217;s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many train-
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218&#8211;226, Boulder, Colorado, June 2009. c&#9675;2009 Association for Computational Linguistics
ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The work of Och et al (2004) is perhaps the bestknown study of new features and their impact on translation quality.</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, it had a few shortcomings.</text>
              <doc_id>18</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008).</text>
              <doc_id>19</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for.</text>
              <doc_id>20</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>By contrast, we incorporate features directly into hierarchical and syntaxbased decoders.</text>
              <doc_id>21</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A third difficulty with Och et al.&#8217;s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets.</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many train-</text>
              <doc_id>23</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218&#8211;226, Boulder, Colorado, June 2009. c&#9675;2009 Association for Computational Linguistics</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>ing examples, and to train discriminatively, we need to search through all possible translations of each training example.</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset.</text>
              <doc_id>26</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We follow this approach here.</text>
              <doc_id>27</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Systems Used</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Hiero</title>
            <text>Hiero (Chiang, 2005) is a hierarchical, string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example:
X &#8594; X 1 de X 2 , X 2 of X 1
As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Hiero (Chiang, 2005) is a hierarchical, string-tostring translation system.</text>
                  <doc_id>29</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example:</text>
                  <doc_id>30</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>X &#8594; X 1 de X 2 , X 2 of X 1</text>
                  <doc_id>31</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997).</text>
                  <doc_id>32</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The baseline model includes 12 features whose weights are optimized using MERT.</text>
                  <doc_id>33</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models.</text>
                  <doc_id>34</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This grammar can be parsed efficiently using cube pruning (Chiang, 2007).</text>
                  <doc_id>35</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Syntax-based system</title>
            <text>Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese. We represent the translation model as a tree transducer (Knight and Graehl, 2005). It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like:
NP-C(x 0 :NPB PP(IN(of x 1 :NPB)) &#8596; x 1 de x 0
Though this rule can be used in either direction, here we use it right-to-left (Chinese to English). We follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules, and in collecting larger rules composed of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007). We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates:
P(rule) =
count(rule) count(LHS-root(rule))
When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up with the joint probability P(e, c).
The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25. For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words. All features are linearly combined and their weights are optimized using MERT.
For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences.
We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Our syntax-based system transforms source Chinese strings into target English syntax trees.</text>
                  <doc_id>36</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese.</text>
                  <doc_id>37</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We represent the translation model as a tree transducer (Knight and Graehl, 2005).</text>
                  <doc_id>38</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed.</text>
                  <doc_id>39</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like:</text>
                  <doc_id>40</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NP-C(x 0 :NPB PP(IN(of x 1 :NPB)) &#8596; x 1 de x 0</text>
                  <doc_id>41</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Though this rule can be used in either direction, here we use it right-to-left (Chinese to English).</text>
                  <doc_id>42</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules, and in collecting larger rules composed of minimal rules.</text>
                  <doc_id>43</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007).</text>
                  <doc_id>44</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates:</text>
                  <doc_id>45</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P(rule) =</text>
                  <doc_id>46</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>count(rule) count(LHS-root(rule))</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up with the joint probability P(e, c).</text>
                  <doc_id>48</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25.</text>
                  <doc_id>49</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words.</text>
                  <doc_id>50</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>All features are linearly combined and their weights are optimized using MERT.</text>
                  <doc_id>51</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006).</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences.</text>
                  <doc_id>53</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We include two other techniques in our baseline.</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006).</text>
                  <doc_id>55</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 MIRA training</title>
            <text>We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).
Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly:
&#8226; Select a batch of input sentences f 1 , . . . , f m and decode each f i to obtain a forest of translations.
&#8226; For each i, select from the forest a set of hypothesis translations e i1 , . . . , e in , which are the
10-best translations according to each of:
h(e) &#183; w
BLEU(e) + h(e) &#183; w
&#8722;BLEU(e) + h(e) &#183; w
&#8226; For each i, select an oracle translation:
(1) e &#8727; = arg max (BLEU(e) + h(e) &#183; w) (2)
e
Let &#8710;h i j = h(e &#8727; i ) &#8722; h(e i j).
&#8226; For each e i j , compute the loss
l i j = BLEU(e &#8727; i ) &#8722; BLEU(e i j) (3)
&#8226; Update w to the value of w &#8242; that minimizes:
1 m&#8721;
2 &#8214;w&#8242; &#8722; w&#8214; 2 + C max (l i j &#8722; &#8710;h i j &#183; w &#8242; ) (4)
1&#8804; j&#8804;n i=1
where C = 0.01. This minimization is performed by a variant of sequential minimal optimization (Platt, 1998).
Following Chiang et al. (2008), we calculate the sentence BLEU scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together.
Since the interface between the trainer and the decoder is fairly simple&#8212;for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update&#8212;it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008).</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w.</text>
                  <doc_id>57</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Then, repeatedly:</text>
                  <doc_id>58</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Select a batch of input sentences f 1 , .</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>60</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>61</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>, f m and decode each f i to obtain a forest of translations.</text>
                  <doc_id>62</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; For each i, select from the forest a set of hypothesis translations e i1 , .</text>
                  <doc_id>63</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>64</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>65</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>, e in , which are the</text>
                  <doc_id>66</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>10-best translations according to each of:</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>h(e) &#183; w</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BLEU(e) + h(e) &#183; w</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8722;BLEU(e) + h(e) &#183; w</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; For each i, select an oracle translation:</text>
                  <doc_id>71</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(1) e &#8727; = arg max (BLEU(e) + h(e) &#183; w) (2)</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Let &#8710;h i j = h(e &#8727; i ) &#8722; h(e i j).</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; For each e i j , compute the loss</text>
                  <doc_id>75</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>l i j = BLEU(e &#8727; i ) &#8722; BLEU(e i j) (3)</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Update w to the value of w &#8242; that minimizes:</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 m&#8721;</text>
                  <doc_id>78</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 &#8214;w&#8242; &#8722; w&#8214; 2 + C max (l i j &#8722; &#8710;h i j &#183; w &#8242; ) (4)</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1&#8804; j&#8804;n i=1</text>
                  <doc_id>80</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where C = 0.01.</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This minimization is performed by a variant of sequential minimal optimization (Platt, 1998).</text>
                  <doc_id>82</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Following Chiang et al. (2008), we calculate the sentence BLEU scores in (1), (2), and (3) in the context of some previous 1-best translations.</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together.</text>
                  <doc_id>84</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Since the interface between the trainer and the decoder is fairly simple&#8212;for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update&#8212;it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder.</text>
                  <doc_id>85</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Features</title>
        <text>In this section, we describe the new features introduced on top of our baseline systems.
Discount features Both of our systems calculate several features based on observed counts of rules in the training data. Though the syntax-based system uses Good-Turing discounting when computing the P(e, c) feature, we find, as noted above, that it uses quite a few one-count rules, suggesting that their probabilities have been overestimated. We can directly attack this problem by adding features count i that reward or punish rules seen i times, or features count [i, j] for rules seen between i and j times.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this section, we describe the new features introduced on top of our baseline systems.</text>
              <doc_id>86</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Discount features Both of our systems calculate several features based on observed counts of rules in the training data.</text>
              <doc_id>87</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Though the syntax-based system uses Good-Turing discounting when computing the P(e, c) feature, we find, as noted above, that it uses quite a few one-count rules, suggesting that their probabilities have been overestimated.</text>
              <doc_id>88</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We can directly attack this problem by adding features count i that reward or punish rules seen i times, or features count [i, j] for rules seen between i and j times.</text>
              <doc_id>89</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Target-side features</title>
            <text>String-to-tree MT offers some unique levers to pull, in terms of target-side features. Because the system outputs English trees, we can analyze output trees on the tuning set and design new features to encourage the decoder to produce more grammatical trees.
Rule overlap features While individual rules observed in decoder output are often quite reasonable, two adjacent rules can create problems. For example, a rule that has a variable of type IN (preposition) needs another rule rooted with IN to fill the position. If the second rule supplies the wrong preposition, a bad translation results. The IN node here is an overlap point between rules. Considering that certain nonterminal symbols may be more reliable overlap points than others, we create a binary feature for each nonterminal. A rule like:
IN(at) &#8596; zai
will have feature rule-root-IN set to 1 and all other rule-root features set to 0. Our rule root features range over the original (non-split) nonterminal set; we have 105 in total. Even though the rule root features are locally attached to individual rules&#8212;and therefore cause no additional problems for the decoder search&#8212;they are aimed at problematic rule/rule interactions.
Bad single-level rewrites Sometimes the decoder uses questionable rules, for example:
PP(x 0 :VBN x 1 :NP-C) &#8596; x 0 x 1
This rule is learned from 62 cases in our training data, where the VBN is almost always the word given. However, the decoder misuses this rule with other VBNs. So we can add a feature that penalizes any rule in which a PP dominates a VBN and NP-C. The feature class bad-rewrite comprises penalties for the following configurations based on our analysis of the tuning set:
PP &#8594; VBN NP-C
PP-BAR &#8594; NP-C IN
VP &#8594; NP-C PP
CONJP &#8594; RB IN
Node count features It is possible that the decoder creates English trees with too many or too few nodes of a particular syntactic category. For example, there may be an tendency to generate too many determiners or past-tense verbs. We therefore add a count feature for each of the 109 (non-split) English nonterminal symbols. For a rule like
NPB(NNP(us) NNP(president) x 0 :NNP)
&#8596; meiguo zongtong x 0
the feature node-count-NPB gets value 1, nodecount-NNP gets value 2, and all others get 0.
Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side. Sample syntaxbased insertion rules are:
NPB(DT(the) x 0 :NN) &#8596; x 0
S(x 0 :NP-C VP(VBZ(is) x 1 :VP-C)) &#8596; x 0 x 1
We notice that our decoder, however, frequently fails to insert words like is and are, which often have no equivalent in the Chinese source. We also notice that the-insertion rules sometimes have a good effect, as in the translation &#8220;in the bloom of youth,&#8221; but other times have a bad effect, as in &#8220;people seek areas of the conspiracy.&#8221;
Each time the decoder uses (or fails to use) an insertion rule, it incurs some risk. There is no guarantee that the interaction of the rule probabilities and the language model provides the best way to manage this risk. We therefore provide MIRA with a feature for each of the most common English words appearing in insertion rules, e.g., insert-the and insert-is. There are 35 such features.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>String-to-tree MT offers some unique levers to pull, in terms of target-side features.</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Because the system outputs English trees, we can analyze output trees on the tuning set and design new features to encourage the decoder to produce more grammatical trees.</text>
                  <doc_id>91</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Rule overlap features While individual rules observed in decoder output are often quite reasonable, two adjacent rules can create problems.</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For example, a rule that has a variable of type IN (preposition) needs another rule rooted with IN to fill the position.</text>
                  <doc_id>93</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>If the second rule supplies the wrong preposition, a bad translation results.</text>
                  <doc_id>94</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The IN node here is an overlap point between rules.</text>
                  <doc_id>95</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Considering that certain nonterminal symbols may be more reliable overlap points than others, we create a binary feature for each nonterminal.</text>
                  <doc_id>96</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>A rule like:</text>
                  <doc_id>97</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>IN(at) &#8596; zai</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>will have feature rule-root-IN set to 1 and all other rule-root features set to 0.</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Our rule root features range over the original (non-split) nonterminal set; we have 105 in total.</text>
                  <doc_id>100</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Even though the rule root features are locally attached to individual rules&#8212;and therefore cause no additional problems for the decoder search&#8212;they are aimed at problematic rule/rule interactions.</text>
                  <doc_id>101</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Bad single-level rewrites Sometimes the decoder uses questionable rules, for example:</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>PP(x 0 :VBN x 1 :NP-C) &#8596; x 0 x 1</text>
                  <doc_id>103</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>This rule is learned from 62 cases in our training data, where the VBN is almost always the word given.</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, the decoder misuses this rule with other VBNs.</text>
                  <doc_id>105</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>So we can add a feature that penalizes any rule in which a PP dominates a VBN and NP-C.</text>
                  <doc_id>106</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The feature class bad-rewrite comprises penalties for the following configurations based on our analysis of the tuning set:</text>
                  <doc_id>107</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>PP &#8594; VBN NP-C</text>
                  <doc_id>108</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>PP-BAR &#8594; NP-C IN</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VP &#8594; NP-C PP</text>
                  <doc_id>110</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>CONJP &#8594; RB IN</text>
                  <doc_id>111</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Node count features It is possible that the decoder creates English trees with too many or too few nodes of a particular syntactic category.</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For example, there may be an tendency to generate too many determiners or past-tense verbs.</text>
                  <doc_id>113</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We therefore add a count feature for each of the 109 (non-split) English nonterminal symbols.</text>
                  <doc_id>114</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For a rule like</text>
                  <doc_id>115</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NPB(NNP(us) NNP(president) x 0 :NNP)</text>
                  <doc_id>116</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8596; meiguo zongtong x 0</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the feature node-count-NPB gets value 1, nodecount-NNP gets value 2, and all others get 0.</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side.</text>
                  <doc_id>119</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Sample syntaxbased insertion rules are:</text>
                  <doc_id>120</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NPB(DT(the) x 0 :NN) &#8596; x 0</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>S(x 0 :NP-C VP(VBZ(is) x 1 :VP-C)) &#8596; x 0 x 1</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We notice that our decoder, however, frequently fails to insert words like is and are, which often have no equivalent in the Chinese source.</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We also notice that the-insertion rules sometimes have a good effect, as in the translation &#8220;in the bloom of youth,&#8221; but other times have a bad effect, as in &#8220;people seek areas of the conspiracy.&#8221;</text>
                  <doc_id>124</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Each time the decoder uses (or fails to use) an insertion rule, it incurs some risk.</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>There is no guarantee that the interaction of the rule probabilities and the language model provides the best way to manage this risk.</text>
                  <doc_id>126</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We therefore provide MIRA with a feature for each of the most common English words appearing in insertion rules, e.g., insert-the and insert-is.</text>
                  <doc_id>127</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>There are 35 such features.</text>
                  <doc_id>128</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Source-side features</title>
            <text>We now turn to features that make use of source-side context. Although these features capture dependencies that cross boundaries between rules, they are still local in the sense that no new states need to be added to the decoder. This is because the entire source sentence, being fixed, is always available to every feature.
Soft syntactic constraints Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008). In brief, these features use the output of an independent syntactic parser on the source sentence, rewarding decoder constituents that match syntactic constituents and punishing decoder constituents that cross syntactic constituents. We use separatelytunable features for each syntactic category.
Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system&#8217;s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance reorderings from short-distance reorderings. To remedy this problem, Chiang et al. (2008) introduce a structural distortion model, which we include in our experiment. Our syntax-based baseline includes the generative version of this model already.
Word context During rule extraction, we retain word alignments from the training data in the extracted rules. (If a rule is observed with more than one set of word alignments, we keep only the most frequent one.) We then define, for each triple ( f, e, f +1 ), a feature that counts the number of times that f is aligned to e and f +1 occurs to the right of f ; and similarly for triples ( f, e, f &#8722;1 ) with f &#8722;1 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token &lt;unk&gt;. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We now turn to features that make use of source-side context.</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Although these features capture dependencies that cross boundaries between rules, they are still local in the sense that no new states need to be added to the decoder.</text>
                  <doc_id>130</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This is because the entire source sentence, being fixed, is always available to every feature.</text>
                  <doc_id>131</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Soft syntactic constraints Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008).</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In brief, these features use the output of an independent syntactic parser on the source sentence, rewarding decoder constituents that match syntactic constituents and punishing decoder constituents that cross syntactic constituents.</text>
                  <doc_id>133</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We use separatelytunable features for each syntactic category.</text>
                  <doc_id>134</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system&#8217;s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance reorderings from short-distance reorderings.</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To remedy this problem, Chiang et al. (2008) introduce a structural distortion model, which we include in our experiment.</text>
                  <doc_id>136</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Our syntax-based baseline includes the generative version of this model already.</text>
                  <doc_id>137</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Word context During rule extraction, we retain word alignments from the training data in the extracted rules.</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(If a rule is observed with more than one set of word alignments, we keep only the most frequent one.</text>
                  <doc_id>139</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>) We then define, for each triple ( f, e, f +1 ), a feature that counts the number of times that f is aligned to e and f +1 occurs to the right of f ; and similarly for triples ( f, e, f &#8722;1 ) with f &#8722;1 occurring to the left of f .</text>
                  <doc_id>140</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token &lt;unk&gt;.</text>
                  <doc_id>141</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model.</text>
                  <doc_id>142</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Experiments</title>
        <text>For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For
the syntax-based system, we ran a reimplementation of the Collins parser (Collins, 1997) on the English half of the bitext to produce parse trees, then restructured and relabeled them as described in Section 3.2. Syntax-based rule extraction was performed on a 65 million word subset of the training data. For Hiero, rules with up to two nonterminals were extracted from a 38 million word subset and phrasal rules were extracted from the remainder of the training data.
We trained three 5-gram language models: one on the English half of the bitext, used by both systems, one on one billion words of English, used by the syntax-based system, and one on two billion words of English, used by Hiero. Modified Kneser-Ney smoothing (Chen and Goodman, 1998) was applied to all language models. The language models are represented using randomized data structures similar to those of Talbot et al. (2007).
Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets.
We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system. We chose a stopping iteration based on the BLEU score on the tuning set, and used the averaged feature weights from all iter-
ations of all learners to decode the test set. The results (Table 1) show significant improvements in both systems (p &lt; 0.01) over already very strong MERT baselines. Adding the source-side and discount features to Hiero yields a +1.5 BLEU improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 BLEU improvement. The results also show that for Hiero, the various classes of features contributed roughly equally; for the syntax-based system, we see that two of the feature classes make small contributions but time constraints unfortunately did not permit isolated testing of all feature classes.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>For our experiments, we used a 260 million word Chinese/English bitext.</text>
              <doc_id>143</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments.</text>
              <doc_id>144</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For</text>
              <doc_id>145</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the syntax-based system, we ran a reimplementation of the Collins parser (Collins, 1997) on the English half of the bitext to produce parse trees, then restructured and relabeled them as described in Section 3.2.</text>
              <doc_id>146</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Syntax-based rule extraction was performed on a 65 million word subset of the training data.</text>
              <doc_id>147</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For Hiero, rules with up to two nonterminals were extracted from a 38 million word subset and phrasal rules were extracted from the remainder of the training data.</text>
              <doc_id>148</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We trained three 5-gram language models: one on the English half of the bitext, used by both systems, one on one billion words of English, used by the syntax-based system, and one on two billion words of English, used by Hiero.</text>
              <doc_id>149</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Modified Kneser-Ney smoothing (Chen and Goodman, 1998) was applied to all language models.</text>
              <doc_id>150</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The language models are represented using randomized data structures similar to those of Talbot et al. (2007).</text>
              <doc_id>151</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level).</text>
              <doc_id>152</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets.</text>
              <doc_id>153</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both.</text>
              <doc_id>154</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system.</text>
              <doc_id>155</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We chose a stopping iteration based on the BLEU score on the tuning set, and used the averaged feature weights from all iter-</text>
              <doc_id>156</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>ations of all learners to decode the test set.</text>
              <doc_id>157</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The results (Table 1) show significant improvements in both systems (p &lt; 0.01) over already very strong MERT baselines.</text>
              <doc_id>158</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Adding the source-side and discount features to Hiero yields a +1.5 BLEU improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 BLEU improvement.</text>
              <doc_id>159</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The results also show that for Hiero, the various classes of features contributed roughly equally; for the syntax-based system, we see that two of the feature classes make small contributions but time constraints unfortunately did not permit isolated testing of all feature classes.</text>
              <doc_id>160</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>6 Analysis</title>
        <text>How did the various new features improve the translation quality of our two systems? We begin by examining the discount features. For these features, we used slightly different schemes for the two systems, shown in Table 2 with their learned feature weights. We see in both cases that one-count rules are strongly penalized, as expected.
.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>How did the various new features improve the translation quality of our two systems?</text>
              <doc_id>161</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We begin by examining the discount features.</text>
              <doc_id>162</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For these features, we used slightly different schemes for the two systems, shown in Table 2 with their learned feature weights.</text>
              <doc_id>163</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We see in both cases that one-count rules are strongly penalized, as expected.</text>
              <doc_id>164</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>.</text>
              <doc_id>165</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>6.1 Syntax features</title>
            <text>Table 3 shows word-insertion feature weights. The system rewards insertion of forms of be; examples 1&#8211;3 in Figure 1 show typical improved translations that result. Among determiners, inserting a is rewarded, while inserting the is punished. This seems to be because the is often part of a fixed phrase, such as the White House, and therefore comes naturally as part of larger phrasal rules. Inserting the outside these fixed phrases is a risk that the generative model is too inclined to take. We also note that the system learns to punish unmotivated insertions of commas and periods, which get into our grammar via quirks in the MT training data. Table 4 shows weights for rule-overlap features. MIRA punishes the case where rules overlap with an IN (preposition) node. This makes sense: if a rule has a variable that can be filled by any English preposition, there is a risk that an incorrect preposition will fill it. On the other hand, splitting at a period is a safe bet, and frees the model to use rules that dig deeper into NP and VP trees when constructing a top-level S. Table 5 shows weights for generated English nonterminals: SBAR-C nodes are rewarded and commas are punished.
The combined effect of all weights is subtle. To interpret them further, it helps to look at gross changes in the system&#8217;s behavior. For example, a major error in the baseline system is to move &#8220;X said&#8221; or &#8220;X asked&#8221; from the beginning of the Chinese input to the middle or end of the English trans- .
.
.
.
.
lation. The error occurs with many speaking verbs, and each time, we trace it to a different rule. The problematic rules can even be non-lexical, e.g.:
S(x 0 :NP-C x 1 :VP x 2 :, x 3 :NP-C x 4 :VP x 5 :.)
&#8596; x 3 x 4 x 2 x 0 x 1 x 5
It is therefore difficult to come up with a straightforward feature to address the problem. However, when we apply MIRA with the features already listed, these translation errors all disappear, as demonstrated by examples 4&#8211;5 in Figure 1. Why does this happen? It turns out that in translation hypotheses that move &#8220;X said&#8221; or &#8220;X asked&#8221; away from the beginning of the sentence, more commas appear, and fewer S-C and SBAR-C nodes appear. Therefore, the new features work to discourage these hypotheses. Example 6 shows additionally that commas next to speaking verbs are now correctly deleted.
Examples 7&#8211;8 in Figure 1 show other kinds of unanticipated improvements. We do not have space for a fuller analysis, but we note that the specific effects we describe above account for only part of the overall BLEU improvement.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Table 3 shows word-insertion feature weights.</text>
                  <doc_id>166</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The system rewards insertion of forms of be; examples 1&#8211;3 in Figure 1 show typical improved translations that result.</text>
                  <doc_id>167</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Among determiners, inserting a is rewarded, while inserting the is punished.</text>
                  <doc_id>168</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This seems to be because the is often part of a fixed phrase, such as the White House, and therefore comes naturally as part of larger phrasal rules.</text>
                  <doc_id>169</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Inserting the outside these fixed phrases is a risk that the generative model is too inclined to take.</text>
                  <doc_id>170</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We also note that the system learns to punish unmotivated insertions of commas and periods, which get into our grammar via quirks in the MT training data.</text>
                  <doc_id>171</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Table 4 shows weights for rule-overlap features.</text>
                  <doc_id>172</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>MIRA punishes the case where rules overlap with an IN (preposition) node.</text>
                  <doc_id>173</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>This makes sense: if a rule has a variable that can be filled by any English preposition, there is a risk that an incorrect preposition will fill it.</text>
                  <doc_id>174</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>On the other hand, splitting at a period is a safe bet, and frees the model to use rules that dig deeper into NP and VP trees when constructing a top-level S.</text>
                  <doc_id>175</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>Table 5 shows weights for generated English nonterminals: SBAR-C nodes are rewarded and commas are punished.</text>
                  <doc_id>176</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The combined effect of all weights is subtle.</text>
                  <doc_id>177</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To interpret them further, it helps to look at gross changes in the system&#8217;s behavior.</text>
                  <doc_id>178</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For example, a major error in the baseline system is to move &#8220;X said&#8221; or &#8220;X asked&#8221; from the beginning of the Chinese input to the middle or end of the English trans- .</text>
                  <doc_id>179</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>.</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>.</text>
                  <doc_id>181</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>.</text>
                  <doc_id>182</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>.</text>
                  <doc_id>183</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>lation.</text>
                  <doc_id>184</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The error occurs with many speaking verbs, and each time, we trace it to a different rule.</text>
                  <doc_id>185</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The problematic rules can even be non-lexical, e.g.:</text>
                  <doc_id>186</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>S(x 0 :NP-C x 1 :VP x 2 :, x 3 :NP-C x 4 :VP x 5 :.</text>
                  <doc_id>187</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>)</text>
                  <doc_id>188</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8596; x 3 x 4 x 2 x 0 x 1 x 5</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It is therefore difficult to come up with a straightforward feature to address the problem.</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, when we apply MIRA with the features already listed, these translation errors all disappear, as demonstrated by examples 4&#8211;5 in Figure 1.</text>
                  <doc_id>191</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Why does this happen?</text>
                  <doc_id>192</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>It turns out that in translation hypotheses that move &#8220;X said&#8221; or &#8220;X asked&#8221; away from the beginning of the sentence, more commas appear, and fewer S-C and SBAR-C nodes appear.</text>
                  <doc_id>193</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, the new features work to discourage these hypotheses.</text>
                  <doc_id>194</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Example 6 shows additionally that commas next to speaking verbs are now correctly deleted.</text>
                  <doc_id>195</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Examples 7&#8211;8 in Figure 1 show other kinds of unanticipated improvements.</text>
                  <doc_id>196</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We do not have space for a fuller analysis, but we note that the specific effects we describe above account for only part of the overall BLEU improvement.</text>
                  <doc_id>197</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>6.2 Word context features</title>
            <text>In Table 6 are shown feature weights learned for the word-context features. A surprising number of the highest-weighted features have to do with translations of dates and bylines. Many of the penalties seem to discourage spurious insertion or deletion of frequent words (for, &#8217;s, said, parentheses, and quotes). Finally, we note that several of the features (the third- and eighth-ranked reward and twelfthranked penalty) shape the translation of shuo &#8216;said&#8217;, preferring translations with an overt complementizer that and without a comma. Thus these features work together to attack a frequent problem that our targetsyntax features also addressed.
Figure 2 shows the performance of Hiero with all of its features on the tuning and test sets over time. The scores on the tuning set rise rapidly, and the scores on the test set also rise, but much more slowly, and there appears to be slight degradation after the 18th pass through the tuning data. This seems in line with the finding of Watanabe et al. (2007) that with on the order of 10,000 features, overfitting is possible, but we can still improve accuracy on new data.
BLEU
38.5
37.5
36.5
35.5
Tune Test
35 0 5 10 15 20 25 Epoch
Early stopping would have given +0.2 BLEU over the results reported in Table 1. 1</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In Table 6 are shown feature weights learned for the word-context features.</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A surprising number of the highest-weighted features have to do with translations of dates and bylines.</text>
                  <doc_id>199</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Many of the penalties seem to discourage spurious insertion or deletion of frequent words (for, &#8217;s, said, parentheses, and quotes).</text>
                  <doc_id>200</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, we note that several of the features (the third- and eighth-ranked reward and twelfthranked penalty) shape the translation of shuo &#8216;said&#8217;, preferring translations with an overt complementizer that and without a comma.</text>
                  <doc_id>201</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Thus these features work together to attack a frequent problem that our targetsyntax features also addressed.</text>
                  <doc_id>202</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 2 shows the performance of Hiero with all of its features on the tuning and test sets over time.</text>
                  <doc_id>203</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The scores on the tuning set rise rapidly, and the scores on the test set also rise, but much more slowly, and there appears to be slight degradation after the 18th pass through the tuning data.</text>
                  <doc_id>204</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This seems in line with the finding of Watanabe et al. (2007) that with on the order of 10,000 features, overfitting is possible, but we can still improve accuracy on new data.</text>
                  <doc_id>205</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BLEU</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>38.5</text>
                  <doc_id>207</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>37.5</text>
                  <doc_id>208</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>36.5</text>
                  <doc_id>209</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>35.5</text>
                  <doc_id>210</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Tune Test</text>
                  <doc_id>211</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>35 0 5 10 15 20 25 Epoch</text>
                  <doc_id>212</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Early stopping would have given +0.2 BLEU over the results reported in Table 1.</text>
                  <doc_id>213</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>1</text>
                  <doc_id>214</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>7</index>
        <title>7 Conclusion</title>
        <text>We have described a variety of features for statistical machine translation and applied them to syntaxbased and hierarchical systems. We saw that these features, discriminatively trained using MIRA, led to significant improvements, and took a closer look at the results to see how the new features qualitatively improved translation quality. We draw three conclusions from this study. First, we have shown that these new features can improve the performance even of top-scoring MT systems. Second, these results add to a growing body of evidence that MIRA is preferable to MERT for discriminative training. When training over 10,000 features on a modest amount of data, we, like Watanabe et al. (2007), did observe overfitting, yet saw improvements on new data. Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work. 1 It was this iteration, in fact, which was used to derive the combined feature count used in the title of this paper. 224 1 MERT: the united states pending israeli clarification on golan settlement plan MIRA: the united states is waiting for israeli clarification on golan settlement plan
2 MERT: . . . the average life expectancy of only 18 months , canada &#8217;s minority goverment will . . . MIRA: . . . the average life expectancy of canada&#8217;s previous minority government is only 18 months . . . 3 MERT: . . . since un inspectors expelled by north korea . . . MIRA: . . . since un inspectors were expelled by north korea . . . 4 MERT: another thing is . . . , " he said , " obviously , the first thing we need to do . . . . MIRA: he said : " obviously , the first thing we need to do . . . , and another thing is . . . . " 5 MERT: the actual timing . . . reopened in january , yoon said . MIRA: yoon said the issue of the timing . . . 6 MERT: . . . us - led coalition forces , said today that the crash . . . MIRA: . . . us - led coalition forces said today that a us military . . . 7 MERT: . . . and others will feel the danger . MIRA: . . . and others will not feel the danger . 8 MERT: in residential or public activities within 200 meters of the region , . . .</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have described a variety of features for statistical machine translation and applied them to syntaxbased and hierarchical systems.</text>
              <doc_id>215</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We saw that these features, discriminatively trained using MIRA, led to significant improvements, and took a closer look at the results to see how the new features qualitatively improved translation quality.</text>
              <doc_id>216</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We draw three conclusions from this study.</text>
              <doc_id>217</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>First, we have shown that these new features can improve the performance even of top-scoring MT systems.</text>
              <doc_id>218</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Second, these results add to a growing body of evidence that MIRA is preferable to MERT for discriminative training.</text>
              <doc_id>219</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>When training over 10,000 features on a modest amount of data, we, like Watanabe et al. (2007), did observe overfitting, yet saw improvements on new data.</text>
              <doc_id>220</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work.</text>
              <doc_id>221</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>1 It was this iteration, in fact, which was used to derive the combined feature count used in the title of this paper.</text>
              <doc_id>222</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>224 1 MERT: the united states pending israeli clarification on golan settlement plan MIRA: the united states is waiting for israeli clarification on golan settlement plan</text>
              <doc_id>223</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2 MERT: .</text>
              <doc_id>224</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>225</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>226</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>the average life expectancy of only 18 months , canada &#8217;s minority goverment will .</text>
              <doc_id>227</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>228</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>229</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>MIRA: .</text>
              <doc_id>230</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>231</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>232</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>the average life expectancy of canada&#8217;s previous minority government is only 18 months .</text>
              <doc_id>233</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>234</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>235</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>3 MERT: .</text>
              <doc_id>236</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>237</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>238</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>since un inspectors expelled by north korea .</text>
              <doc_id>239</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>240</doc_id>
              <sec_id>16</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>241</doc_id>
              <sec_id>17</sec_id>
            </sentence>
            <sentence>
              <text>MIRA: .</text>
              <doc_id>242</doc_id>
              <sec_id>18</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>243</doc_id>
              <sec_id>19</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>244</doc_id>
              <sec_id>20</sec_id>
            </sentence>
            <sentence>
              <text>since un inspectors were expelled by north korea .</text>
              <doc_id>245</doc_id>
              <sec_id>21</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>246</doc_id>
              <sec_id>22</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>247</doc_id>
              <sec_id>23</sec_id>
            </sentence>
            <sentence>
              <text>4 MERT: another thing is .</text>
              <doc_id>248</doc_id>
              <sec_id>24</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>249</doc_id>
              <sec_id>25</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>250</doc_id>
              <sec_id>26</sec_id>
            </sentence>
            <sentence>
              <text>, " he said , " obviously , the first thing we need to do .</text>
              <doc_id>251</doc_id>
              <sec_id>27</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>252</doc_id>
              <sec_id>28</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>253</doc_id>
              <sec_id>29</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>254</doc_id>
              <sec_id>30</sec_id>
            </sentence>
            <sentence>
              <text>MIRA: he said : " obviously , the first thing we need to do .</text>
              <doc_id>255</doc_id>
              <sec_id>31</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>256</doc_id>
              <sec_id>32</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>257</doc_id>
              <sec_id>33</sec_id>
            </sentence>
            <sentence>
              <text>, and another thing is .</text>
              <doc_id>258</doc_id>
              <sec_id>34</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>259</doc_id>
              <sec_id>35</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>260</doc_id>
              <sec_id>36</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>261</doc_id>
              <sec_id>37</sec_id>
            </sentence>
            <sentence>
              <text>" 5 MERT: the actual timing .</text>
              <doc_id>262</doc_id>
              <sec_id>38</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>263</doc_id>
              <sec_id>39</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>264</doc_id>
              <sec_id>40</sec_id>
            </sentence>
            <sentence>
              <text>reopened in january , yoon said .</text>
              <doc_id>265</doc_id>
              <sec_id>41</sec_id>
            </sentence>
            <sentence>
              <text>MIRA: yoon said the issue of the timing .</text>
              <doc_id>266</doc_id>
              <sec_id>42</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>267</doc_id>
              <sec_id>43</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>268</doc_id>
              <sec_id>44</sec_id>
            </sentence>
            <sentence>
              <text>6 MERT: .</text>
              <doc_id>269</doc_id>
              <sec_id>45</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>270</doc_id>
              <sec_id>46</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>271</doc_id>
              <sec_id>47</sec_id>
            </sentence>
            <sentence>
              <text>us - led coalition forces , said today that the crash .</text>
              <doc_id>272</doc_id>
              <sec_id>48</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>273</doc_id>
              <sec_id>49</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>274</doc_id>
              <sec_id>50</sec_id>
            </sentence>
            <sentence>
              <text>MIRA: .</text>
              <doc_id>275</doc_id>
              <sec_id>51</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>276</doc_id>
              <sec_id>52</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>277</doc_id>
              <sec_id>53</sec_id>
            </sentence>
            <sentence>
              <text>us - led coalition forces said today that a us military .</text>
              <doc_id>278</doc_id>
              <sec_id>54</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>279</doc_id>
              <sec_id>55</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>280</doc_id>
              <sec_id>56</sec_id>
            </sentence>
            <sentence>
              <text>7 MERT: .</text>
              <doc_id>281</doc_id>
              <sec_id>57</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>282</doc_id>
              <sec_id>58</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>283</doc_id>
              <sec_id>59</sec_id>
            </sentence>
            <sentence>
              <text>and others will feel the danger .</text>
              <doc_id>284</doc_id>
              <sec_id>60</sec_id>
            </sentence>
            <sentence>
              <text>MIRA: .</text>
              <doc_id>285</doc_id>
              <sec_id>61</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>286</doc_id>
              <sec_id>62</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>287</doc_id>
              <sec_id>63</sec_id>
            </sentence>
            <sentence>
              <text>and others will not feel the danger .</text>
              <doc_id>288</doc_id>
              <sec_id>64</sec_id>
            </sentence>
            <sentence>
              <text>8 MERT: in residential or public activities within 200 meters of the region , .</text>
              <doc_id>289</doc_id>
              <sec_id>65</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>290</doc_id>
              <sec_id>66</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>291</doc_id>
              <sec_id>67</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM BLEU scores. &#8727; or &#8727;&#8727; = significantly better than MERT baseline (p &lt; 0.05 or 0.01, respectively).</caption>
        <reference_text>In PAGE 5: ... ations of all learners to decode the test set. The results ( Table1 ) show significant improve- ments in both systems (p  lt; 0:01) over already very strong MERT baselines. Adding the source-side and discount features to Hiero yields a +1....</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Training</cell>
              <cell>Features</cell>
              <cell>#</cell>
              <cell>Tune</cell>
              <cell>Test</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Hiero</cell>
              <cell>MERT</cell>
              <cell>baseline</cell>
              <cell>11</cell>
              <cell>35.4</cell>
              <cell>36.1</cell>
            </row>
            <row>
              <cell></cell>
              <cell>MIRA</cell>
              <cell>syntax, distortion</cell>
              <cell>56</cell>
              <cell>35.9</cell>
              <cell>36.9 &#8727;</cell>
            </row>
            <row>
              <cell></cell>
              <cell></cell>
              <cell>syntax, distortion, discount</cell>
              <cell>61</cell>
              <cell>36.6</cell>
              <cell>37.3 &#8727;&#8727;</cell>
            </row>
            <row>
              <cell></cell>
              <cell></cell>
              <cell>all source-side, discount</cell>
              <cell>10990</cell>
              <cell>38.4</cell>
              <cell>37.6 &#8727;&#8727;</cell>
            </row>
            <row>
              <cell>Syntax</cell>
              <cell>MERT</cell>
              <cell>baseline</cell>
              <cell>25</cell>
              <cell>38.6</cell>
              <cell>39.5</cell>
            </row>
            <row>
              <cell></cell>
              <cell>MIRA</cell>
              <cell>baseline</cell>
              <cell>25</cell>
              <cell>38.5</cell>
              <cell>39.8 &#8727;</cell>
            </row>
            <row>
              <cell></cell>
              <cell></cell>
              <cell>overlap</cell>
              <cell>132</cell>
              <cell>38.7</cell>
              <cell>39.9 &#8727;</cell>
            </row>
            <row>
              <cell></cell>
              <cell></cell>
              <cell>node count</cell>
              <cell>136</cell>
              <cell>38.7</cell>
              <cell>40.0 &#8727;&#8727;</cell>
            </row>
            <row>
              <cell></cell>
              <cell></cell>
              <cell>all target-side, discount</cell>
              <cell>283</cell>
              <cell>39.6</cell>
              <cell>40.6 &#8727;&#8727;</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Weights learned for discount features. Negative weights indicate bonuses; positive weights indicate penalties.</caption>
        <reference_text>In PAGE 5: ... 6 Analysis How did the various new features improve the trans- lation quality of our two systems? We begin by ex- amining the discount features. For these features, we used slightly di erent schemes for the two sys- tems, shown in  Table2  with their learned feature weights. We see in both cases that one-count rules are strongly penalized, as expected....</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>Syntax-based</cell>
              <cell>Syntax-based#@#@Hiero</cell>
              <cell>Hiero</cell>
              <cell>Hiero</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>count</cell>
              <cell>weight</cell>
              <cell>count</cell>
              <cell>weight</cell>
            </row>
            <row>
              <cell>1</cell>
              <cell>+1:28#@#@+1.28</cell>
              <cell>1</cell>
              <cell>+2:23#@#@+2.23</cell>
            </row>
            <row>
              <cell>2</cell>
              <cell>+0:35#@#@+0.35</cell>
              <cell>2</cell>
              <cell>+0:77#@#@+0.77</cell>
            </row>
            <row>
              <cell>3?5#@#@3&#8211;5</cell>
              <cell>0:73#@#@&#8722;0.73</cell>
              <cell>3</cell>
              <cell>+0:54#@#@+0.54</cell>
            </row>
            <row>
              <cell>6?10#@#@6&#8211;10</cell>
              <cell>0:64#@#@&#8722;0.64</cell>
              <cell>4</cell>
              <cell>+0:29#@#@+0.29</cell>
            </row>
            <row>
              <cell></cell>
              <cell></cell>
              <cell>5+</cell>
              <cell>0:02#@#@&#8722;0.02</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Weights learned for inserting target English words with rules that lack Chinese words.</caption>
        <reference_text>In PAGE 6: ... 6.1 Syntax features  Table3  shows word-insertion feature weights. The system rewards insertion of forms of be; examples 1?3 in Figure 1 show typical improved translations that result....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Reward</cell>
              <cell>Reward</cell>
              <cell>Penalty</cell>
              <cell>Penalty</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>0:42</cell>
              <cell>a</cell>
              <cell>+0:67</cell>
              <cell>of</cell>
            </row>
            <row>
              <cell>0:13</cell>
              <cell>are</cell>
              <cell>+0:56</cell>
              <cell>the</cell>
            </row>
            <row>
              <cell>0:09</cell>
              <cell>at</cell>
              <cell>+0:47</cell>
              <cell>comma</cell>
            </row>
            <row>
              <cell>0:09</cell>
              <cell>on</cell>
              <cell>+0:13</cell>
              <cell>period</cell>
            </row>
            <row>
              <cell>0:05</cell>
              <cell>was</cell>
              <cell>+0:11</cell>
              <cell>in</cell>
            </row>
            <row>
              <cell>0:05</cell>
              <cell>from</cell>
              <cell>+0:08</cell>
              <cell>for</cell>
            </row>
            <row>
              <cell>0:04</cell>
              <cell>?s</cell>
              <cell>+0:06</cell>
              <cell>to</cell>
            </row>
            <row>
              <cell>0:04</cell>
              <cell>by</cell>
              <cell>+0:05</cell>
              <cell>will</cell>
            </row>
            <row>
              <cell>0:04</cell>
              <cell>is</cell>
              <cell>+0:04</cell>
              <cell>and</cell>
            </row>
            <row>
              <cell>0:03</cell>
              <cell>it</cell>
              <cell>+0:02</cell>
              <cell>as</cell>
            </row>
            <row>
              <cell>0:03</cell>
              <cell>its</cell>
              <cell>+0:02</cell>
              <cell>have</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>: :</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>:</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: Weights learned for employing rules whose English sides are rooted at particular syntactic categories.</caption>
        <reference_text>In PAGE 6: ... We also note that the system learns to punish unmotivated insertions of commas and periods, which get into our grammar via quirks in the MT training data.  Table4  shows weights for rule-overlap features. MIRA punishes the case where rules overlap with an IN (preposition) node....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>None</cell>
              <cell>Bonus</cell>
              <cell>None</cell>
              <cell>Penalty</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>0:50</cell>
              <cell>period</cell>
              <cell>+0:93</cell>
              <cell>IN</cell>
            </row>
            <row>
              <cell>0:39</cell>
              <cell>VP-C</cell>
              <cell>+0:57</cell>
              <cell>NNP</cell>
            </row>
            <row>
              <cell>0:36</cell>
              <cell>VB</cell>
              <cell>+0:44</cell>
              <cell>NN</cell>
            </row>
            <row>
              <cell>0:31</cell>
              <cell>SG-C</cell>
              <cell>+0:41</cell>
              <cell>DT</cell>
            </row>
            <row>
              <cell>0:30</cell>
              <cell>MD</cell>
              <cell>+0:34</cell>
              <cell>JJ</cell>
            </row>
            <row>
              <cell>0:26</cell>
              <cell>VBG</cell>
              <cell>+0:24</cell>
              <cell>right double quote</cell>
            </row>
            <row>
              <cell>0:25</cell>
              <cell>ADJP</cell>
              <cell>+0:20</cell>
              <cell>VBZ</cell>
            </row>
            <row>
              <cell>0:22</cell>
              <cell>-LRB-</cell>
              <cell>+0:19</cell>
              <cell>NP</cell>
            </row>
            <row>
              <cell>0:21</cell>
              <cell>VP-BAR</cell>
              <cell>+0:16</cell>
              <cell>TO</cell>
            </row>
            <row>
              <cell>0:20</cell>
              <cell>NPB-BAR</cell>
              <cell>+0:15</cell>
              <cell>ADJP-BAR</cell>
            </row>
            <row>
              <cell>0:16</cell>
              <cell>FRAG</cell>
              <cell>+0:14</cell>
              <cell>PRN-BAR</cell>
            </row>
            <row>
              <cell>0:16</cell>
              <cell>PRN</cell>
              <cell>+0:14</cell>
              <cell>NML</cell>
            </row>
            <row>
              <cell>0:15</cell>
              <cell>NPB</cell>
              <cell>+0:13</cell>
              <cell>comma</cell>
            </row>
            <row>
              <cell>0:13</cell>
              <cell>RB</cell>
              <cell>+0:12</cell>
              <cell>VBD</cell>
            </row>
            <row>
              <cell>0:12</cell>
              <cell>SBAR-C</cell>
              <cell>+0:12</cell>
              <cell>NNPS</cell>
            </row>
            <row>
              <cell>0:12</cell>
              <cell>VP-C-BAR</cell>
              <cell>+0:12</cell>
              <cell>PRP</cell>
            </row>
            <row>
              <cell>0:11</cell>
              <cell>-RRB-</cell>
              <cell>+0:11</cell>
              <cell>SG</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>: :</cell>
              <cell>None</cell>
              <cell>: :</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>:</cell>
              <cell>None</cell>
              <cell>:</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TableSeer</source>
        <caption>Table 5: Weights learned for generating syntactic nodes of various types anywhere in the English translation.</caption>
        <reference_text>In PAGE 6: ... On the other hand, splitting at a pe- riod is a safe bet, and frees the model to use rules that dig deeper into NP and VP trees when constructing a top-level S.  Table5  shows weights for generated English nonterminals: SBAR-C nodes are rewarded and commas are punished. The combined e ect of all weights is subtle....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>None</cell>
              <cell>Bonus</cell>
              <cell>None</cell>
              <cell>Penalty</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>0:73</cell>
              <cell>SBAR-C</cell>
              <cell>+1:30</cell>
              <cell>comma</cell>
            </row>
            <row>
              <cell>0:54</cell>
              <cell>VBZ</cell>
              <cell>+0:80</cell>
              <cell>DT</cell>
            </row>
            <row>
              <cell>0:54</cell>
              <cell>IN</cell>
              <cell>+0:58</cell>
              <cell>PP</cell>
            </row>
            <row>
              <cell>0:52</cell>
              <cell>NN</cell>
              <cell>+0:44</cell>
              <cell>TO</cell>
            </row>
            <row>
              <cell>0:51</cell>
              <cell>PP-C</cell>
              <cell>+0:33</cell>
              <cell>NNP</cell>
            </row>
            <row>
              <cell>0:47</cell>
              <cell>right double quote</cell>
              <cell>+0:30</cell>
              <cell>NNS</cell>
            </row>
            <row>
              <cell>0:39</cell>
              <cell>ADJP</cell>
              <cell>+0:30</cell>
              <cell>NML</cell>
            </row>
            <row>
              <cell>0:34</cell>
              <cell>POS</cell>
              <cell>+0:22</cell>
              <cell>CD</cell>
            </row>
            <row>
              <cell>0:31</cell>
              <cell>ADVP</cell>
              <cell>+0:18</cell>
              <cell>PRN</cell>
            </row>
            <row>
              <cell>0:30</cell>
              <cell>RP</cell>
              <cell>+0:16</cell>
              <cell>SYM</cell>
            </row>
            <row>
              <cell>0:29</cell>
              <cell>PRT</cell>
              <cell>+0:15</cell>
              <cell>ADJP-BAR</cell>
            </row>
            <row>
              <cell>0:27</cell>
              <cell>SG-C</cell>
              <cell>+0:15</cell>
              <cell>NP</cell>
            </row>
            <row>
              <cell>0:22</cell>
              <cell>S-C</cell>
              <cell>+0:15</cell>
              <cell>MD</cell>
            </row>
            <row>
              <cell>0:21</cell>
              <cell>NNPS</cell>
              <cell>+0:15</cell>
              <cell>HYPH</cell>
            </row>
            <row>
              <cell>0:21</cell>
              <cell>VP-BAR</cell>
              <cell>+0:14</cell>
              <cell>PRN-BAR</cell>
            </row>
            <row>
              <cell>0:20</cell>
              <cell>PRP</cell>
              <cell>+0:14</cell>
              <cell>NP-C</cell>
            </row>
            <row>
              <cell>0:20</cell>
              <cell>NPB-BAR</cell>
              <cell>+0:11</cell>
              <cell>ADJP-C</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>: :</cell>
              <cell>None</cell>
              <cell>: :</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>:</cell>
              <cell>None</cell>
              <cell>:</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>6</id>
        <source>TableSeer</source>
        <caption>Table 6: Weights learned for word-context features, which fire when English word e is generated aligned to Chinese word f , with Chinese word f &#8722;1 to the left or f +1 to the right. Glosses for Chinese words are not part of features.</caption>
        <reference_text>In PAGE 7: ... 6.2 Word context features In  Table6  are shown feature weights learned for the word-context features. A surprising number of the highest-weighted features have to do with transla- tions of dates and bylines....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>None</cell>
              <cell>f</cell>
              <cell>Bonus   e</cell>
              <cell>context</cell>
              <cell>None</cell>
              <cell>Penalty   f</cell>
              <cell>e</cell>
              <cell>context</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>1:19</cell>
              <cell>lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>f 1 = ri ?day?</cell>
              <cell>+1:12</cell>
              <cell>lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>)</cell>
              <cell>f+1 =  lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
            </row>
            <row>
              <cell>1:01</cell>
              <cell>lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>f 1 = (</cell>
              <cell>+0:83</cell>
              <cell>jiang ?shall?</cell>
              <cell>be</cell>
              <cell>f+1 =  lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
            </row>
            <row>
              <cell>0:84</cell>
              <cell>,</cell>
              <cell>that</cell>
              <cell>f 1 = shuo ?say?</cell>
              <cell>+0:83</cell>
              <cell>zhengfu ?government?</cell>
              <cell>the</cell>
              <cell>f 1 =  lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
            </row>
            <row>
              <cell>0:82</cell>
              <cell>yue ?month?</cell>
              <cell>lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>f+1 =  lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>+0:73</cell>
              <cell>lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>)</cell>
              <cell>f 1 =  lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
            </row>
            <row>
              <cell>0:78</cell>
              <cell>quot</cell>
              <cell>None</cell>
              <cell>quot</cell>
              <cell>None</cell>
              <cell>f 1 =  lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>+0:73</cell>
              <cell>lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>(</cell>
              <cell>f+1 =  lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
            </row>
            <row>
              <cell>0:76</cell>
              <cell>quot</cell>
              <cell>None</cell>
              <cell>quot</cell>
              <cell>None</cell>
              <cell>f+1 =  lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>+0:72</cell>
              <cell>lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>)</cell>
              <cell>f 1 = ri ?day?</cell>
            </row>
            <row>
              <cell>0:66</cell>
              <cell>lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>f+1 = nian ?year?</cell>
              <cell>+0:70</cell>
              <cell>lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>(</cell>
              <cell>f 1 = ri ?day?</cell>
            </row>
            <row>
              <cell>0:65</cell>
              <cell>,</cell>
              <cell>that</cell>
              <cell>f+1 =  lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>+0:69</cell>
              <cell>lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>(</cell>
              <cell>f 1 =  lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>: :</cell>
              <cell>None</cell>
              <cell>+0:66</cell>
              <cell>lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>for</cell>
              <cell>f 1 =  lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>:</cell>
              <cell>None</cell>
              <cell>+0:66</cell>
              <cell>lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>?s</cell>
              <cell>f 1 = ,</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>+0:65</cell>
              <cell>lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>f 1 =  lt</cell>
              <cell>unk gt</cell>
              <cell>None</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>+0:60</cell>
              <cell>,</cell>
              <cell>,</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>: :</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>:</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Phil Blunsom</author>
          <author>Trevor Cohn</author>
          <author>Miles Osborne</author>
        </authors>
        <title>A discriminative latent variable model for statistical machine translation.</title>
        <publication>In Proc. ACL-08: HLT.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Pietra</author>
          <author>Robert L Mercer</author>
        </authors>
        <title>The mathematics of statistical machine translation: Parameter estimation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1993</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Yee Seng Chan</author>
          <author>Hwee Tou Ng</author>
          <author>David Chiang</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>3</id>
        <authors/>
        <title>Word sense disambiguation improves statistical machine translation.</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Stanley F Chen</author>
          <author>Joshua T Goodman</author>
        </authors>
        <title>An empirical study of smoothing techniques for language modeling.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>David Chiang</author>
          <author>Yuval Marton</author>
          <author>Philip Resnik</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>6</id>
        <authors/>
        <title>Online large-margin training of syntactic and structural translation features.</title>
        <publication>In Proc. EMNLP</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>A hierarchical phrase-based model for statistical machine translation.</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>Hierarchical phrase-based translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Michael Collins</author>
        </authors>
        <title>Three generative, lexicalized models for statistical parsing.</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Koby Crammer</author>
          <author>Ofer Dekel</author>
          <author>Joseph Keshet</author>
          <author>Shai ShalevShwartz</author>
          <author>Yoram Singer</author>
        </authors>
        <title>Online passiveaggressive algorithms.</title>
        <publication>None</publication>
        <pages>7--551</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Steve DeNeefe</author>
          <author>Kevin Knight</author>
          <author>Wei Wang</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>What can syntax-based MT learn from phrase-based MT? In</title>
        <publication>Proc. EMNLP-CoNLL-2007.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Victoria Fossum</author>
          <author>Kevin Knight</author>
          <author>Steven Abney</author>
        </authors>
        <title>Using syntax to improve word alignment for syntaxbased statistical machine translation.</title>
        <publication>In Proc. Third Workshop on Statistical Machine Translation.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Michel Galley</author>
          <author>Mark Hopkins</author>
          <author>Kevin Knight</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>What&#8217;s in a translation rule?</title>
        <publication>In Proc. HLT-NAACL 2004,</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Michel Galley</author>
          <author>Jonathan Graehl</author>
          <author>Kevin Knight</author>
          <author>Daniel Marcu</author>
          <author>Steve DeNeefe</author>
          <author>Wei Wang</author>
          <author>Ignacio Thayer</author>
        </authors>
        <title>Scalable inference and training of context-rich syntactic models.</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Liang Huang</author>
        </authors>
        <title>Forest reranking: Discriminative parsing with non-local features.</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Kevin Knight</author>
          <author>Jonathan Graehl</author>
        </authors>
        <title>An overview of probabilistic tree transducers for natural language processing.</title>
        <publication>In Proceedings of the Sixth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing).</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Yoong Keok Lee</author>
          <author>Hwee Tou Ng</author>
        </authors>
        <title>An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation.</title>
        <publication>In Proc. EMNLP</publication>
        <pages>41--48</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Percy Liang</author>
          <author>Alexandre Bouchard-C&#244;t&#233;</author>
          <author>Dan Klein</author>
          <author>Ben Taskar</author>
        </authors>
        <title>An end-to-end discriminative approach to machine translation.</title>
        <publication>In Proc. COLING-ACL</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Wolfgang Macherey</author>
          <author>Franz Josef Och</author>
          <author>Ignacio Thayer</author>
          <author>Jakob Uskoreit</author>
        </authors>
        <title>Lattice-based minimum error rate training for statistical machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>20</id>
        <authors/>
        <title>None</title>
        <publication>In Proc. EMNLP</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Yuval Marton</author>
          <author>Philip Resnik</author>
        </authors>
        <title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
        <publication>In Proc. ACL-08: HLT.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Discriminative training and maximum entropy models for statistical machine translation.</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Daniel Gildea</author>
        </authors>
        <title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>24</id>
        <authors/>
        <title>None</title>
        <publication>In Proc. HLT-NAACL</publication>
        <pages>161--168</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>Slav Petrov</author>
          <author>Leon Barrett</author>
          <author>Romain Thibaux</author>
          <author>Dan Klein</author>
        </authors>
        <title>Learning accurate, compact, and interpretable tree annotation.</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>27</id>
        <authors>
          <author>John C Platt</author>
        </authors>
        <title>Fast training of support vector machines using sequential minimal optimization.</title>
        <publication>Advances in Kernel Methods: Support Vector Learning,</publication>
        <pages>195--208</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>28</id>
        <authors>
          <author>David Talbot</author>
          <author>Miles Osborne</author>
        </authors>
        <title>Randomised language modelling for statistical machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>29</id>
        <authors/>
        <title>None</title>
        <publication>In Proc. ACL</publication>
        <pages>512--519</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>30</id>
        <authors>
          <author>Christoph Tillmann</author>
          <author>Tong Zhang</author>
        </authors>
        <title>A discriminative global training algorithm for statistical MT.</title>
        <publication>In Proc. COLING-ACL</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>31</id>
        <authors>
          <author>Joseph Turian</author>
          <author>Benjamin Wellington</author>
          <author>I Dan Melamed</author>
        </authors>
        <title>Scalable discriminative learning for natural language parsing and translation.</title>
        <publication>In Proc. NIPS</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>32</id>
        <authors>
          <author>Wei Wang</author>
          <author>Kevin Knight</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Binarizing syntax trees to improve syntax-based machine translation accuracy.</title>
        <publication>In Proc. EMNLP-CoNLL</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>33</id>
        <authors>
          <author>Taro Watanabe</author>
          <author>Jun Suzuki</author>
          <author>Hajime Tsukuda</author>
          <author>Hideki Isozaki</author>
        </authors>
        <title>Online large-margin training for statistical machine translation.</title>
        <publication>In Proc. EMNLP</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>34</id>
        <authors>
          <author>Dekai Wu</author>
        </authors>
        <title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
        <publication>None</publication>
        <pages>23--377</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>35</id>
        <authors>
          <author>Kenji Yamada</author>
          <author>Kevin Knight</author>
        </authors>
        <title>A decoder for syntax-based statistical MT.</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Blunsom et al., 2008</string>
        <sentence_id>22564</sentence_id>
        <char_offset>135</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>2</reference_id>
        <string>Chan et al. (2007)</string>
        <sentence_id>22679</sentence_id>
        <char_offset>249</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>3</reference_id>
        <string>(2007)</string>
        <sentence_id>22679</sentence_id>
        <char_offset>72</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>3</reference_id>
        <string>(2007)</string>
        <sentence_id>22679</sentence_id>
        <char_offset>261</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>3</reference_id>
        <string>(2007)</string>
        <sentence_id>22692</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>3</reference_id>
        <string>(2007)</string>
        <sentence_id>22741</sentence_id>
        <char_offset>55</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>3</reference_id>
        <string>(2007)</string>
        <sentence_id>22761</sentence_id>
        <char_offset>88</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>4</reference_id>
        <string>Chen and Goodman, 1998</string>
        <sentence_id>22691</sentence_id>
        <char_offset>31</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>5</reference_id>
        <string>Chiang et al. (2008)</string>
        <sentence_id>22623</sentence_id>
        <char_offset>10</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>5</reference_id>
        <string>Chiang et al. (2008)</string>
        <sentence_id>22673</sentence_id>
        <char_offset>24</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>5</reference_id>
        <string>Chiang et al., 2008</string>
        <sentence_id>22557</sentence_id>
        <char_offset>70</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>5</reference_id>
        <string>Chiang et al., 2008</string>
        <sentence_id>22567</sentence_id>
        <char_offset>49</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>5</reference_id>
        <string>Chiang et al., 2008</string>
        <sentence_id>22596</sentence_id>
        <char_offset>174</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>6</reference_id>
        <string>(2008)</string>
        <sentence_id>22623</sentence_id>
        <char_offset>24</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>6</reference_id>
        <string>(2008)</string>
        <sentence_id>22669</sentence_id>
        <char_offset>194</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>6</reference_id>
        <string>(2008)</string>
        <sentence_id>22673</sentence_id>
        <char_offset>38</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>7</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>22551</sentence_id>
        <char_offset>48</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>7</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>22569</sentence_id>
        <char_offset>7</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>8</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>22575</sentence_id>
        <char_offset>59</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>9</reference_id>
        <string>Collins, 1997</string>
        <sentence_id>22687</sentence_id>
        <char_offset>74</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>10</reference_id>
        <string>Crammer et al., 2006</string>
        <sentence_id>22556</sentence_id>
        <char_offset>75</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>10</reference_id>
        <string>Crammer et al., 2006</string>
        <sentence_id>22596</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>11</reference_id>
        <string>DeNeefe et al., 2007</string>
        <sentence_id>22584</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>12</reference_id>
        <string>Fossum et al., 2008</string>
        <sentence_id>22685</sentence_id>
        <char_offset>113</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>13</reference_id>
        <string>Galley et al., 2004</string>
        <sentence_id>22580</sentence_id>
        <char_offset>74</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>14</reference_id>
        <string>Galley et al. (2006)</string>
        <sentence_id>22583</sentence_id>
        <char_offset>10</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>15</reference_id>
        <string>Huang, 2008</string>
        <sentence_id>22560</sentence_id>
        <char_offset>118</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>16</reference_id>
        <string>Knight and Graehl, 2005</string>
        <sentence_id>22578</sentence_id>
        <char_offset>57</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>17</reference_id>
        <string>Lee and Ng (2002)</string>
        <sentence_id>22679</sentence_id>
        <char_offset>173</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>18</reference_id>
        <string>Liang et al., 2006</string>
        <sentence_id>22564</sentence_id>
        <char_offset>94</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>19</reference_id>
        <string>Macherey et al., 2008</string>
        <sentence_id>22564</sentence_id>
        <char_offset>157</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>20</reference_id>
        <string>(2008)</string>
        <sentence_id>22623</sentence_id>
        <char_offset>24</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>20</reference_id>
        <string>(2008)</string>
        <sentence_id>22669</sentence_id>
        <char_offset>194</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>20</reference_id>
        <string>(2008)</string>
        <sentence_id>22673</sentence_id>
        <char_offset>38</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>21</reference_id>
        <string>Marton and Resnik (2008)</string>
        <sentence_id>22669</sentence_id>
        <char_offset>176</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>22</reference_id>
        <string>Och and Ney, 2002</string>
        <sentence_id>22596</sentence_id>
        <char_offset>57</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>24</reference_id>
        <string>(2004)</string>
        <sentence_id>22558</sentence_id>
        <char_offset>22</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>25</reference_id>
        <string>Och, 2003</string>
        <sentence_id>22556</sentence_id>
        <char_offset>168</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>26</reference_id>
        <string>Petrov et al. (2006)</string>
        <sentence_id>22595</sentence_id>
        <char_offset>329</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>26</reference_id>
        <string>Petrov et al., 2006</string>
        <sentence_id>22694</sentence_id>
        <char_offset>66</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>27</reference_id>
        <string>Platt, 1998</string>
        <sentence_id>22622</sentence_id>
        <char_offset>80</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>29</reference_id>
        <string>(2007)</string>
        <sentence_id>22679</sentence_id>
        <char_offset>72</char_offset>
      </citation>
      <citation>
        <id>42</id>
        <reference_id>29</reference_id>
        <string>(2007)</string>
        <sentence_id>22679</sentence_id>
        <char_offset>261</char_offset>
      </citation>
      <citation>
        <id>43</id>
        <reference_id>29</reference_id>
        <string>(2007)</string>
        <sentence_id>22692</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>44</id>
        <reference_id>29</reference_id>
        <string>(2007)</string>
        <sentence_id>22741</sentence_id>
        <char_offset>55</char_offset>
      </citation>
      <citation>
        <id>45</id>
        <reference_id>29</reference_id>
        <string>(2007)</string>
        <sentence_id>22761</sentence_id>
        <char_offset>88</char_offset>
      </citation>
      <citation>
        <id>46</id>
        <reference_id>30</reference_id>
        <string>Tillmann and Zhang, 2006</string>
        <sentence_id>22564</sentence_id>
        <char_offset>68</char_offset>
      </citation>
      <citation>
        <id>47</id>
        <reference_id>31</reference_id>
        <string>Turian et al., 2007</string>
        <sentence_id>22564</sentence_id>
        <char_offset>114</char_offset>
      </citation>
      <citation>
        <id>48</id>
        <reference_id>32</reference_id>
        <string>Wang et al., 2007</string>
        <sentence_id>22595</sentence_id>
        <char_offset>112</char_offset>
      </citation>
      <citation>
        <id>49</id>
        <reference_id>33</reference_id>
        <string>Watanabe et al. (2007)</string>
        <sentence_id>22679</sentence_id>
        <char_offset>56</char_offset>
      </citation>
      <citation>
        <id>50</id>
        <reference_id>33</reference_id>
        <string>Watanabe et al. (2007)</string>
        <sentence_id>22741</sentence_id>
        <char_offset>39</char_offset>
      </citation>
      <citation>
        <id>51</id>
        <reference_id>33</reference_id>
        <string>Watanabe et al. (2007)</string>
        <sentence_id>22761</sentence_id>
        <char_offset>72</char_offset>
      </citation>
      <citation>
        <id>52</id>
        <reference_id>33</reference_id>
        <string>Watanabe et al., 2007</string>
        <sentence_id>22557</sentence_id>
        <char_offset>47</char_offset>
      </citation>
      <citation>
        <id>53</id>
        <reference_id>33</reference_id>
        <string>Watanabe et al., 2007</string>
        <sentence_id>22567</sentence_id>
        <char_offset>26</char_offset>
      </citation>
      <citation>
        <id>54</id>
        <reference_id>33</reference_id>
        <string>Watanabe et al., 2007</string>
        <sentence_id>22596</sentence_id>
        <char_offset>151</char_offset>
      </citation>
      <citation>
        <id>55</id>
        <reference_id>34</reference_id>
        <string>Wu, 1997</string>
        <sentence_id>22572</sentence_id>
        <char_offset>113</char_offset>
      </citation>
      <citation>
        <id>56</id>
        <reference_id>35</reference_id>
        <string>Yamada and Knight, 2002</string>
        <sentence_id>22593</sentence_id>
        <char_offset>32</char_offset>
      </citation>
    </citations>
  </content>
</document>
