<document>
  <filename>W11-1004</filename>
  <authors/>
  <title>Incorporating Source-Language Paraphrases into Phrase-Based SMT with Confusion Networks</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>To increase the model coverage, sourcelanguage paraphrases have been utilized to boost SMT system performance. Previous work showed that word lattices constructed from paraphrases are able to reduce out-ofvocabulary words and to express inputs in different ways for better translation quality. However, such a word-lattice-based method suffers from two problems: 1) path duplications in word lattices decrease the capacities for potential paraphrases; 2) lattice decoding in SMT dramatically increases the search space and results in poor time efficiency. Therefore, in this paper, we adopt word confusion networks as the input structure to carry source-language paraphrase information. Similar to previous work, we use word lattices to build word confusion networks for merging of duplicated paths and faster decoding. Experiments are carried out on small-, medium- and large-scale English&#8211; Chinese translation tasks, and we show that compared with the word-lattice-based method, the decoding time on three tasks is reduced significantly (up to 79%) while comparable translation quality is obtained on the largescale task.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>To increase the model coverage, sourcelanguage paraphrases have been utilized to boost SMT system performance.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Previous work showed that word lattices constructed from paraphrases are able to reduce out-ofvocabulary words and to express inputs in different ways for better translation quality.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, such a word-lattice-based method suffers from two problems: 1) path duplications in word lattices decrease the capacities for potential paraphrases; 2) lattice decoding in SMT dramatically increases the search space and results in poor time efficiency.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, in this paper, we adopt word confusion networks as the input structure to carry source-language paraphrase information.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Similar to previous work, we use word lattices to build word confusion networks for merging of duplicated paths and faster decoding.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Experiments are carried out on small-, medium- and large-scale English&#8211; Chinese translation tasks, and we show that compared with the word-lattice-based method, the decoding time on three tasks is reduced significantly (up to 79%) while comparable translation quality is obtained on the largescale task.</text>
              <doc_id>5</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>With the rapid development of large-scale parallel corpus, research on data-driven SMT has made good progress to the real world applications. Currently, for a typical automatic translation task, the SMT system searches and exactly matches the input sentences with the phrases or rules in the models. Obvi- 31
ously, if the following two conditions could be satisfied, namely:
&#8226; the words in the parallel corpus are highly aligned so that the phrase alignment can be performed well;
&#8226; the coverage of the input sentence by the parallel corpus is high;
then the &#8220;exact phrase match&#8221; translation method could bring a good translation.
However, for some language pairs, it is not easy to obtain a huge amount of parallel data, so it is not that easy to satisfy these two conditions. To alleviate this problem, paraphrase-enriched SMT systems have been proposed to show the effectiveness of incorporating paraphrase information. In terms of the position at which paraphrases are incorporated in the MT-pipeline, previous work can be organized into three different categories:
&#8226; Translation model augmentation with paraphrases (Callison-Burch et al., 2006; Marton et al., 2009). Here the focus is on the translation of unknown source words or phrases in the input sentences by enriching the translation table with paraphrases.
&#8226; Training corpus augmentation with paraphrases (Bond et al., 2008; Nakov, 2008a; Nakov, 2008b). Paraphrases are incorporated into the MT systems by expanding the training data.
&#8226; Word-lattice-based method with paraphrases (Du et al., 2010; Onishi et al.,
2010). Instead of augmenting the translation table, source-language paraphrases are constructed to enrich the inputs to the SMT system. Another directly related work is to use word lattices to deal with multi-source translation (Schroeder et al., 2009), in which paraphrases are actually generated from the alignments of difference source sentences.
Comparing these three methods, the word-latticebased method has the least overheads because:
&#8226; The translation model augmentation method has to re-run the whole MT pipeline once the inputs are changed, while the word-latticebased method only need to transform the new input sentences into word lattices.
&#8226; The training corpus augmentation method requires corpus-scale expansion, which drastically increases the computational complexity on large corpora, while the word-lattice-based method only deals with the development set and test set.
In (Du et al., 2010; Onishi et al., 2010), it is also observed that the word-lattice-based method performed better than the translation model augmentation method on different scales and two different language pairs in several translation tasks. Thus they concluded that the word-lattice-based method is preferable for this task. However, there are still some drawbacks for the word-lattice-based method:
&#8226; In the lattice construction processing, duplicated paths are created and fed into SMT decoders. This decreases the paraphrase capacity in the word lattices. Note that we use the phrase &#8220;paraphrase capacity&#8221; to represent the amount of paraphrases that are actually built into the word lattices. As presented in (Du et al., 2010), only a limited number of paraphrases are allowed to be used while others are pruned during the construction process, so duplicate paths actually decrease the number of paraphrases that contribute to the translation quality.
&#8226; The lattice decoding in SMT decoder have a very high computational complexity which 32 makes the system less feasible in real time application.
Therefore, in this paper, we use confusion networks (CNs) instead of word lattices to carry paraphrase information in the inputs for SMT decoders. CNs are constructed from the aforementioned word lattices, while duplicate paths are merged to increase paraphrase capacity (e.g. by admitting more nonduplicate paraphrases without increasing the input size). Furthermore, much less computational complexity is required to perform CN decoding instead of lattice decoding in the SMT decoder. We carried out experiments on small-, medium- and largescale English&#8211;Chinese translation tasks to compare against a baseline PBSMT system, the translation model augmentation of (Callison-Burch et al., 2006) method and the word-lattice-based method of (Du et al., 2010) to show the effectiveness of our novel approach.
The motivation of this work is to use CN as the compromise between speed and quality, which comes from previous studies in speech recognition and speech translation: in (Hakkani-T&#252;r et al., 2005), word lattices are transformed into CNs to obtain compact representations of multiple aligned ASR hypotheses in speech understanding; in (Bertoldi et al., 2008), CNs are also adopted instead of word lattices as the source-side inputs for speech translation systems. The main contribution of this paper is to show that this compromise also works for SMT systems incorporating sourcelanguage paraphrases in the inputs. Regarding the use of paraphrases SMT system, there are still other two categories of work that are related to this paper:
&#8226; Using paraphrases to improve system optimization (Madnani et al., 2007). With an English&#8211; English MT system, this work utilises paraphrases to reduce the number of manually translated references that are needed in the parameter tuning process of SMT, while preserved a similar translation quality.
&#8226; Using paraphrases to smooth translation models (Kuhn et al., 2010; Max, 2010). Either cluster-based or example-based methods are
proposed to obtain better estimation on phrase translation probabilities with paraphrases.
The rest of this paper is organized as follows: In section 2, we present an overview of the wordlattice-based method and its drawbacks. Section 3 proposes the CN-based method, including the building process and its application on paraphrases in SMT. Section 4 presents the experiments and results of the proposed method as well as discussions. Conclusions and future work are then given in Section 5.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>With the rapid development of large-scale parallel corpus, research on data-driven SMT has made good progress to the real world applications.</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Currently, for a typical automatic translation task, the SMT system searches and exactly matches the input sentences with the phrases or rules in the models.</text>
              <doc_id>7</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Obvi- 31</text>
              <doc_id>8</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>ously, if the following two conditions could be satisfied, namely:</text>
              <doc_id>9</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; the words in the parallel corpus are highly aligned so that the phrase alignment can be performed well;</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; the coverage of the input sentence by the parallel corpus is high;</text>
              <doc_id>11</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>then the &#8220;exact phrase match&#8221; translation method could bring a good translation.</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>However, for some language pairs, it is not easy to obtain a huge amount of parallel data, so it is not that easy to satisfy these two conditions.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To alleviate this problem, paraphrase-enriched SMT systems have been proposed to show the effectiveness of incorporating paraphrase information.</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In terms of the position at which paraphrases are incorporated in the MT-pipeline, previous work can be organized into three different categories:</text>
              <doc_id>15</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; Translation model augmentation with paraphrases (Callison-Burch et al., 2006; Marton et al., 2009).</text>
              <doc_id>16</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Here the focus is on the translation of unknown source words or phrases in the input sentences by enriching the translation table with paraphrases.</text>
              <doc_id>17</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; Training corpus augmentation with paraphrases (Bond et al., 2008; Nakov, 2008a; Nakov, 2008b).</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Paraphrases are incorporated into the MT systems by expanding the training data.</text>
              <doc_id>19</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; Word-lattice-based method with paraphrases (Du et al., 2010; Onishi et al.,</text>
              <doc_id>20</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2010).</text>
              <doc_id>21</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Instead of augmenting the translation table, source-language paraphrases are constructed to enrich the inputs to the SMT system.</text>
              <doc_id>22</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Another directly related work is to use word lattices to deal with multi-source translation (Schroeder et al., 2009), in which paraphrases are actually generated from the alignments of difference source sentences.</text>
              <doc_id>23</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Comparing these three methods, the word-latticebased method has the least overheads because:</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; The translation model augmentation method has to re-run the whole MT pipeline once the inputs are changed, while the word-latticebased method only need to transform the new input sentences into word lattices.</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; The training corpus augmentation method requires corpus-scale expansion, which drastically increases the computational complexity on large corpora, while the word-lattice-based method only deals with the development set and test set.</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In (Du et al., 2010; Onishi et al., 2010), it is also observed that the word-lattice-based method performed better than the translation model augmentation method on different scales and two different language pairs in several translation tasks.</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Thus they concluded that the word-lattice-based method is preferable for this task.</text>
              <doc_id>28</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, there are still some drawbacks for the word-lattice-based method:</text>
              <doc_id>29</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; In the lattice construction processing, duplicated paths are created and fed into SMT decoders.</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This decreases the paraphrase capacity in the word lattices.</text>
              <doc_id>31</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Note that we use the phrase &#8220;paraphrase capacity&#8221; to represent the amount of paraphrases that are actually built into the word lattices.</text>
              <doc_id>32</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As presented in (Du et al., 2010), only a limited number of paraphrases are allowed to be used while others are pruned during the construction process, so duplicate paths actually decrease the number of paraphrases that contribute to the translation quality.</text>
              <doc_id>33</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; The lattice decoding in SMT decoder have a very high computational complexity which 32 makes the system less feasible in real time application.</text>
              <doc_id>34</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Therefore, in this paper, we use confusion networks (CNs) instead of word lattices to carry paraphrase information in the inputs for SMT decoders.</text>
              <doc_id>35</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>CNs are constructed from the aforementioned word lattices, while duplicate paths are merged to increase paraphrase capacity (e.g. by admitting more nonduplicate paraphrases without increasing the input size).</text>
              <doc_id>36</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Furthermore, much less computational complexity is required to perform CN decoding instead of lattice decoding in the SMT decoder.</text>
              <doc_id>37</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We carried out experiments on small-, medium- and largescale English&#8211;Chinese translation tasks to compare against a baseline PBSMT system, the translation model augmentation of (Callison-Burch et al., 2006) method and the word-lattice-based method of (Du et al., 2010) to show the effectiveness of our novel approach.</text>
              <doc_id>38</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The motivation of this work is to use CN as the compromise between speed and quality, which comes from previous studies in speech recognition and speech translation: in (Hakkani-T&#252;r et al., 2005), word lattices are transformed into CNs to obtain compact representations of multiple aligned ASR hypotheses in speech understanding; in (Bertoldi et al., 2008), CNs are also adopted instead of word lattices as the source-side inputs for speech translation systems.</text>
              <doc_id>39</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The main contribution of this paper is to show that this compromise also works for SMT systems incorporating sourcelanguage paraphrases in the inputs.</text>
              <doc_id>40</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Regarding the use of paraphrases SMT system, there are still other two categories of work that are related to this paper:</text>
              <doc_id>41</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; Using paraphrases to improve system optimization (Madnani et al., 2007).</text>
              <doc_id>42</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>With an English&#8211; English MT system, this work utilises paraphrases to reduce the number of manually translated references that are needed in the parameter tuning process of SMT, while preserved a similar translation quality.</text>
              <doc_id>43</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; Using paraphrases to smooth translation models (Kuhn et al., 2010; Max, 2010).</text>
              <doc_id>44</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Either cluster-based or example-based methods are</text>
              <doc_id>45</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>proposed to obtain better estimation on phrase translation probabilities with paraphrases.</text>
              <doc_id>46</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The rest of this paper is organized as follows: In section 2, we present an overview of the wordlattice-based method and its drawbacks.</text>
              <doc_id>47</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Section 3 proposes the CN-based method, including the building process and its application on paraphrases in SMT.</text>
              <doc_id>48</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Section 4 presents the experiments and results of the proposed method as well as discussions.</text>
              <doc_id>49</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Conclusions and future work are then given in Section 5.</text>
              <doc_id>50</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Word-lattice-based method</title>
        <text>Compared with translation model augmentation with paraphrases (Callison-Burch et al., 2006), word-lattice-based paraphrasing for PBSMT is introduced in (Du et al., 2010). A brief overview of this method is given in this section.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Compared with translation model augmentation with paraphrases (Callison-Burch et al., 2006), word-lattice-based paraphrasing for PBSMT is introduced in (Du et al., 2010).</text>
              <doc_id>51</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A brief overview of this method is given in this section.</text>
              <doc_id>52</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Lattice construction from paraphrases</title>
            <text>The first step of the word-lattice-based method is to generate paraphrases from parallel corpus. The algorithm in (Bannard and Callison-Burch, 2005) is used for this purpose by pivoting through phrases in the source- and the target- languages: for each source phrase, all occurrences of its target phrases are found, and all the corresponding source phrases of these target phrases are considered as the potential paraphrases of the original source phrase (Callison- Burch et al., 2006). A paraphrase probability p(e 2 |e 1 ) is defined to reflect the similarities between two phrases, as in (1):
p(e 2 |e 1 ) = &#8721; f p(f|e 1 )p(e 2 |f) (1)
where the probability p(f|e 1 ) is the probability that the original source phrase e 1 translates as a particular phrase f on the target side, and p(e 2 |f) is the probability that the candidate paraphrase e 2 translates as the source phrase. Here p(e 2 |f) and p(f|e 1 ) are defined as the translation probabilities estimated using maximum likelihood by counting the observations of alignments between phrases e and f in the 33 ...
...
q 1 q 2 &#8230; q m
... w x-1 w x w x+1 ... w y w y+1
w x-1
q 1 q 2 ... q m
w x w x+1 ... w y w y+1 ...
parallel corpus, as in (2) and (3):
p(e 2 |f) &#8776; count(e 2, f) &#8721;
e 2
count(e 2 , f) p(f|e 1 ) &#8776; &#8721; count(f, e 1)
f count(f, e 1)
(2)
(3)
The second step is to transform input sentences in the development and test sets into word lattices with paraphrases extracted in the first step. As illustrated in Figure 1, given a sequence of words {w 1 , . . . , w N } as the input, for each of the paraphrase pairs found in the source sentence (e.g. p i = {q 1 , . . . , q m } for {w x , . . . , w y }), add in extra nodes and edges to make sure those phrases coming from paraphrases share the same start nodes and end nodes with that of the original ones. Subsequently the following empirical methods are used to assign weights on paraphrases edges:
&#8226; Edges originating from the input sentences are assigned weight 1.
&#8226; The first edges for each of the paraphrases are calculated as in (4):
w(e 1 p i ) = 1 k + i
(1 &lt;= i &lt;= k) (4)
where 1 stands for the first edge of paraphrase p i , and i is the probability rank of p i among those paraphrases sharing with a same start node, while k is a predefined constant as a trade-off parameter for efficiency and performance, which is related to the paraphrase capacity.
&#8226; The rest of the edges corresponding to the paraphrases are assigned weight 1.
The last step is to modify the MT pipeline to tune and evaluate the SMT system with word lattice inputs, as is described in (Du et al., 2010; Onishi et al., 2010).
For further discussion, a real example of the generated word lattice is illustrated in Figure 2. In the word lattice, double-line circled nodes and solid lined edges come from originated from the original sentence, while others are generated from paraphrases. Word, weight and ranking of each edge are displayed in the figure. By adopting such an input structure, the diversity of the input sentences is increased to provide more flexible translation options during the decoding process, which has been shown to improve translation performance (Du et al., 2010).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The first step of the word-lattice-based method is to generate paraphrases from parallel corpus.</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The algorithm in (Bannard and Callison-Burch, 2005) is used for this purpose by pivoting through phrases in the source- and the target- languages: for each source phrase, all occurrences of its target phrases are found, and all the corresponding source phrases of these target phrases are considered as the potential paraphrases of the original source phrase (Callison- Burch et al., 2006).</text>
                  <doc_id>54</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>A paraphrase probability p(e 2 |e 1 ) is defined to reflect the similarities between two phrases, as in (1):</text>
                  <doc_id>55</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(e 2 |e 1 ) = &#8721; f p(f|e 1 )p(e 2 |f) (1)</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where the probability p(f|e 1 ) is the probability that the original source phrase e 1 translates as a particular phrase f on the target side, and p(e 2 |f) is the probability that the candidate paraphrase e 2 translates as the source phrase.</text>
                  <doc_id>57</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Here p(e 2 |f) and p(f|e 1 ) are defined as the translation probabilities estimated using maximum likelihood by counting the observations of alignments between phrases e and f in the 33 ...</text>
                  <doc_id>58</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>...</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>q 1 q 2 &#8230; q m</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>... w x-1 w x w x+1 ... w y w y+1</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w x-1</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>q 1 q 2 ... q m</text>
                  <doc_id>63</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w x w x+1 ... w y w y+1 ...</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>parallel corpus, as in (2) and (3):</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(e 2 |f) &#8776; count(e 2, f) &#8721;</text>
                  <doc_id>66</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e 2</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>count(e 2 , f) p(f|e 1 ) &#8776; &#8721; count(f, e 1)</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f count(f, e 1)</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(2)</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(3)</text>
                  <doc_id>71</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The second step is to transform input sentences in the development and test sets into word lattices with paraphrases extracted in the first step.</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As illustrated in Figure 1, given a sequence of words {w 1 , .</text>
                  <doc_id>73</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>74</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>75</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>, w N } as the input, for each of the paraphrase pairs found in the source sentence (e.g. p i = {q 1 , .</text>
                  <doc_id>76</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>77</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>78</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>, q m } for {w x , .</text>
                  <doc_id>79</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>80</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>81</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>, w y }), add in extra nodes and edges to make sure those phrases coming from paraphrases share the same start nodes and end nodes with that of the original ones.</text>
                  <doc_id>82</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>Subsequently the following empirical methods are used to assign weights on paraphrases edges:</text>
                  <doc_id>83</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Edges originating from the input sentences are assigned weight 1.</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; The first edges for each of the paraphrases are calculated as in (4):</text>
                  <doc_id>85</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w(e 1 p i ) = 1 k + i</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(1 &lt;= i &lt;= k) (4)</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where 1 stands for the first edge of paraphrase p i , and i is the probability rank of p i among those paraphrases sharing with a same start node, while k is a predefined constant as a trade-off parameter for efficiency and performance, which is related to the paraphrase capacity.</text>
                  <doc_id>88</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; The rest of the edges corresponding to the paraphrases are assigned weight 1.</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The last step is to modify the MT pipeline to tune and evaluate the SMT system with word lattice inputs, as is described in (Du et al., 2010; Onishi et al., 2010).</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For further discussion, a real example of the generated word lattice is illustrated in Figure 2.</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the word lattice, double-line circled nodes and solid lined edges come from originated from the original sentence, while others are generated from paraphrases.</text>
                  <doc_id>92</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Word, weight and ranking of each edge are displayed in the figure.</text>
                  <doc_id>93</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>By adopting such an input structure, the diversity of the input sentences is increased to provide more flexible translation options during the decoding process, which has been shown to improve translation performance (Du et al., 2010).</text>
                  <doc_id>94</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Path duplication and decoding efficiency</title>
            <text>As can be seen in Figure 2, the construction process in the previous steps tends to generate duplicate paths in the word lattices. For example, there are two paths from node 6 to node 11 with the same words &#8220;secretary of state&#8221; but different edge probabilities (the path via node 27 and 28 has the probability 1/12, while the path via node 26 and 9 has the probability 1/99). This is because the aforementioned straightforward construction process does not track path duplications from different spans on the source side. Since the number of admitted paraphrases is restricted by parameter k in formula (4), the path duplication will decrease the paraphrase capacity to a certain extend.
Moreover, state of the art PBSMT decoders (e.g. Moses (Koehn et al., 2007)) have a much higher computational complexity for lattice structures than for sentences. Thus even though only the test sentences need to be transformed into word lattices, decoding time is still too slow for real-time applications.
Motivated by transforming ASR word-graphs into CNs (Bertoldi et al., 2008), we adopt CN as the trade-off between efficiency and quality. We aim to merge duplicate paths in the word lattices to increase paraphrase capacity, and to speed up the decoding process via CN decoding. Details of the proposed method are presented in the following section. 34</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>As can be seen in Figure 2, the construction process in the previous steps tends to generate duplicate paths in the word lattices.</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For example, there are two paths from node 6 to node 11 with the same words &#8220;secretary of state&#8221; but different edge probabilities (the path via node 27 and 28 has the probability 1/12, while the path via node 26 and 9 has the probability 1/99).</text>
                  <doc_id>96</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This is because the aforementioned straightforward construction process does not track path duplications from different spans on the source side.</text>
                  <doc_id>97</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Since the number of admitted paraphrases is restricted by parameter k in formula (4), the path duplication will decrease the paraphrase capacity to a certain extend.</text>
                  <doc_id>98</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Moreover, state of the art PBSMT decoders (e.g. Moses (Koehn et al., 2007)) have a much higher computational complexity for lattice structures than for sentences.</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Thus even though only the test sentences need to be transformed into word lattices, decoding time is still too slow for real-time applications.</text>
                  <doc_id>100</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Motivated by transforming ASR word-graphs into CNs (Bertoldi et al., 2008), we adopt CN as the trade-off between efficiency and quality.</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We aim to merge duplicate paths in the word lattices to increase paraphrase capacity, and to speed up the decoding process via CN decoding.</text>
                  <doc_id>102</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Details of the proposed method are presented in the following section.</text>
                  <doc_id>103</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>34</text>
                  <doc_id>104</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Confusion-network-based method</title>
        <text>CNs are weighted direct graphs where each path from the start node to the end node goes through all the other nodes. Each edge is labelled with a word and a probability (or weight). Although it is commonly required to normalize the probability of edges between two consecutive nodes to sum up to one, from the point of view of the decoder, this is not a strict constraint as long as any score is provided (similar to the weights on the word lattices in the last section, and we prefer to call it &#8220;weight&#8221; in this case).
The benefits of using CNs are:
1. the ability to represent the original word lattice with a highly compact structure;
2. all hypotheses in the word lattice are totally ordered, so that the decoding algorithm is mostly retained except for the collection of translation options and the handeling of &#603; edges (Bertoldi et al., 2008), which requires much less computational resources than the lattice decoding.
The rest of this section details the construction process of the CNs and the application in paraphraseenriched SMT.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>CNs are weighted direct graphs where each path from the start node to the end node goes through all the other nodes.</text>
              <doc_id>105</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each edge is labelled with a word and a probability (or weight).</text>
              <doc_id>106</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Although it is commonly required to normalize the probability of edges between two consecutive nodes to sum up to one, from the point of view of the decoder, this is not a strict constraint as long as any score is provided (similar to the weights on the word lattices in the last section, and we prefer to call it &#8220;weight&#8221; in this case).</text>
              <doc_id>107</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The benefits of using CNs are:</text>
              <doc_id>108</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1. the ability to represent the original word lattice with a highly compact structure;</text>
              <doc_id>109</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2. all hypotheses in the word lattice are totally ordered, so that the decoding algorithm is mostly retained except for the collection of translation options and the handeling of &#603; edges (Bertoldi et al., 2008), which requires much less computational resources than the lattice decoding.</text>
              <doc_id>110</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The rest of this section details the construction process of the CNs and the application in paraphraseenriched SMT.</text>
              <doc_id>111</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Confusion Network building</title>
            <text>We build our CN from the aforementioned word lattices. Previous studies provide several methods to do this. (Mangu et al., 2000) propose a method to cluster lattice words on the similarity of pronunciations and frequency of occurrence, and then to create CNs using cluster orders. Although this method has a computational complexity of O(n 3 ), the SRILM toolkit (Stolcke, 2002) provides a modified algorithm which runs much faster than the original version. In (Hakkani-T&#252;r et al., 2005), a pivot algorithm is proposed to form CNs by normalizing the topology of the input lattices.
In this paper, we use the modified method of (Mangu et al., 2000) provided by the SRILM toolkit to convert word lattices into CNs. Moreover, we aim to obtain CNs with the following guidelines:
&#8226; Cluster the lattice words only by topological orders and edge weights without considering word similarity. The objective is to reduce the
impact of path duplications in the building process, since duplicate words will bias the importance of paths.
&#8226; Assign edge weights by the ranking of paraphrase probabilities, rather than by posterior probabilities from the modified method of (Mangu et al., 2000). This is similar to that given in formula (4). The reason for this is to reduce the impact of path duplications on the calculation of weights.
Thus, we modified the construction process as follows:
1. For each of the input word lattices, replace word texts with unique identifiers (to make the lattice alignment uncorrelated to the word similarity, since in this case, all words in the lattice are different from each other).
2. Evenly distribute edge weights for each of the lattices by modifying formula (4) as in (5):
w(e j p i ) = 1
M i
&#8730; (k + i) (1 &lt;= i &lt;= k) (5)
where 1 &lt;= j &lt;= M i , given e j p i is the j th edge of paraphrase p i , and M i is the number of words in p i . This is to avoid large weights on the paraphrase edges for lattice alignments.
3. Transform the weighted word lattices into CNs with the SRILM toolkit, and the paraphrase ranking information is carried on the edges.
4. Replace the word texts in step 1, and then for each column of the CN, merge edges with same words by keeping those with the highest ranking (a smaller number indicates a higher ranking, and edges from the original sentences will always have the highest ranking). Note that to assign ranking for each &#603; edge which does not appear in the word lattice, we use the ranking of non-original edges (in the same column) which have the closest posterior probability to it. (Assign ranking 1 if failed to find a such edge).
5. Reassign the edge weights: 1) edges from original sentences are assigned with weight 1; 2) edges from paraphrases are assigned with an
empirical method as in (6):
w(e cn p i ) = 1 k + i (1 &lt;= i &lt;= k) (6)
where e cn p i are edges corresponding with paraphrase p i , and i is the probability rank of p i in formula (4), while k is also defined in formula (4).
A real example of a constructed CN is depicted in Figure 3, which is correspondent with the word lattice in Figure 2. Unlike the word lattices, all the nodes in the CN are generated from the original sentence, while solid lined edges come from the original sentence, and dotted lined edges correspond to paraphrases.
As in shown in the Figures, duplicate paths in the word lattices have been merged into CN edges by step 4. For example, the two occurrences of &#8220;secretary of state&#8221; in the word lattices (one path from node 6 to 11 via 27 and 28, and one path from node 6 to 11 via 26 and 9 in the word lattice) are merged to keep the highest-ranked path in the CN (note there is one &#603; edge between node 9 and 10 to accomplish the merging operation). Furthermore, each edge in the CN is assigned a weight by formula (6). This weight assignment procedure penalizes paths from paraphrases according to the paraphrase probabilities, in a similar manner to the aforementioned word-lattice-based method.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We build our CN from the aforementioned word lattices.</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Previous studies provide several methods to do this.</text>
                  <doc_id>113</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>(Mangu et al., 2000) propose a method to cluster lattice words on the similarity of pronunciations and frequency of occurrence, and then to create CNs using cluster orders.</text>
                  <doc_id>114</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Although this method has a computational complexity of O(n 3 ), the SRILM toolkit (Stolcke, 2002) provides a modified algorithm which runs much faster than the original version.</text>
                  <doc_id>115</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In (Hakkani-T&#252;r et al., 2005), a pivot algorithm is proposed to form CNs by normalizing the topology of the input lattices.</text>
                  <doc_id>116</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In this paper, we use the modified method of (Mangu et al., 2000) provided by the SRILM toolkit to convert word lattices into CNs.</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Moreover, we aim to obtain CNs with the following guidelines:</text>
                  <doc_id>118</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Cluster the lattice words only by topological orders and edge weights without considering word similarity.</text>
                  <doc_id>119</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The objective is to reduce the</text>
                  <doc_id>120</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>impact of path duplications in the building process, since duplicate words will bias the importance of paths.</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Assign edge weights by the ranking of paraphrase probabilities, rather than by posterior probabilities from the modified method of (Mangu et al., 2000).</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This is similar to that given in formula (4).</text>
                  <doc_id>123</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The reason for this is to reduce the impact of path duplications on the calculation of weights.</text>
                  <doc_id>124</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Thus, we modified the construction process as follows:</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1.</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For each of the input word lattices, replace word texts with unique identifiers (to make the lattice alignment uncorrelated to the word similarity, since in this case, all words in the lattice are different from each other).</text>
                  <doc_id>127</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Evenly distribute edge weights for each of the lattices by modifying formula (4) as in (5):</text>
                  <doc_id>129</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w(e j p i ) = 1</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>M i</text>
                  <doc_id>131</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8730; (k + i) (1 &lt;= i &lt;= k) (5)</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where 1 &lt;= j &lt;= M i , given e j p i is the j th edge of paraphrase p i , and M i is the number of words in p i .</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This is to avoid large weights on the paraphrase edges for lattice alignments.</text>
                  <doc_id>134</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3.</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Transform the weighted word lattices into CNs with the SRILM toolkit, and the paraphrase ranking information is carried on the edges.</text>
                  <doc_id>136</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Replace the word texts in step 1, and then for each column of the CN, merge edges with same words by keeping those with the highest ranking (a smaller number indicates a higher ranking, and edges from the original sentences will always have the highest ranking).</text>
                  <doc_id>138</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Note that to assign ranking for each &#603; edge which does not appear in the word lattice, we use the ranking of non-original edges (in the same column) which have the closest posterior probability to it.</text>
                  <doc_id>139</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>(Assign ranking 1 if failed to find a such edge).</text>
                  <doc_id>140</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5.</text>
                  <doc_id>141</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Reassign the edge weights: 1) edges from original sentences are assigned with weight 1; 2) edges from paraphrases are assigned with an</text>
                  <doc_id>142</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>empirical method as in (6):</text>
                  <doc_id>143</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w(e cn p i ) = 1 k + i (1 &lt;= i &lt;= k) (6)</text>
                  <doc_id>144</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where e cn p i are edges corresponding with paraphrase p i , and i is the probability rank of p i in formula (4), while k is also defined in formula (4).</text>
                  <doc_id>145</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A real example of a constructed CN is depicted in Figure 3, which is correspondent with the word lattice in Figure 2.</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Unlike the word lattices, all the nodes in the CN are generated from the original sentence, while solid lined edges come from the original sentence, and dotted lined edges correspond to paraphrases.</text>
                  <doc_id>147</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As in shown in the Figures, duplicate paths in the word lattices have been merged into CN edges by step 4.</text>
                  <doc_id>148</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For example, the two occurrences of &#8220;secretary of state&#8221; in the word lattices (one path from node 6 to 11 via 27 and 28, and one path from node 6 to 11 via 26 and 9 in the word lattice) are merged to keep the highest-ranked path in the CN (note there is one &#603; edge between node 9 and 10 to accomplish the merging operation).</text>
                  <doc_id>149</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Furthermore, each edge in the CN is assigned a weight by formula (6).</text>
                  <doc_id>150</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This weight assignment procedure penalizes paths from paraphrases according to the paraphrase probabilities, in a similar manner to the aforementioned word-lattice-based method.</text>
                  <doc_id>151</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Modified MT pipeline</title>
            <text>By transforming word lattices into CNs, duplicate paths are merged. Furthermore the new features on the edges are introduced by formula (6), which is then tuned on the development set using MERT (Och, 2003) in the log-linear model (Och and Ney, 2002). Since the SMT decoders are able to perform CN decoding (Bertoldi et al., 2008) in an efficient multi-stack decoding way, decoding time is drastically reduced compared to lattice decoding.
The training steps are then modified as follows: 1) Extract phrase table, reordering table, and build target-side language models from parallel and monolingual corpora respectively for the PBSMT model; 2) Transform source sentences in the development set into word lattices, and then transform them into CNs using the method proposed in Section 3.1; 3) Tune the PBSMT model on the CNs via 36
the development set. Note that the overhead of the evaluation steps are: transform each test set sentence into a word lattice, and also transform them into a CN, then feed them into the SMT decoder to obtain decoding results.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>By transforming word lattices into CNs, duplicate paths are merged.</text>
                  <doc_id>152</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Furthermore the new features on the edges are introduced by formula (6), which is then tuned on the development set using MERT (Och, 2003) in the log-linear model (Och and Ney, 2002).</text>
                  <doc_id>153</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Since the SMT decoders are able to perform CN decoding (Bertoldi et al., 2008) in an efficient multi-stack decoding way, decoding time is drastically reduced compared to lattice decoding.</text>
                  <doc_id>154</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The training steps are then modified as follows: 1) Extract phrase table, reordering table, and build target-side language models from parallel and monolingual corpora respectively for the PBSMT model; 2) Transform source sentences in the development set into word lattices, and then transform them into CNs using the method proposed in Section 3.1; 3) Tune the PBSMT model on the CNs via 36</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the development set.</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Note that the overhead of the evaluation steps are: transform each test set sentence into a word lattice, and also transform them into a CN, then feed them into the SMT decoder to obtain decoding results.</text>
                  <doc_id>157</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Experiments</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>158</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Experimental setup</title>
            <text>Experiments were carried out on three English&#8211; Chinese translation tasks. The training corpora comprise 20K, 200K and 2.1 million sentence pairs, where the former two corpora are derived from FBIS corpus 1 which is sentence-aligned by Champollion aligner (Ma, 2006), the latter corpus comes from HK parallel corpus, 2 ISI parallel corpus, 3 other news data and parallel dictionaries from LDC.
The development set and the test set for the 20K and 200K corpora are randomly selected from the FBIS corpus, each of which contains 1,200 sentences, with one reference. For the 2.1 million corpus, the NIST 2005 Chinese&#8211;English current set (1,082 sentences) with one reference is used as the development set, and NIST 2003 English&#8211;Chinese current set (1,859 sentences) with four references is used as the test set.
Three baseline systems are built for comparison: Moses PBSMT baseline system (Koehn et al., 2007), a realization of the translation model augmentation system described in (Callison-Burch et al., 2006) (named &#8220;Para-Sub&#8221; hereafter), and the word-lattice based system proposed in (Du et al., 2010). Word alignments on the parallel corpus are performed using GIZA++ (Och and Ney, 2003) with the &#8220;grow-diag-final&#8221; refinement. Maximum phrase length is set to 10 words and the parameters in the log-linear model are tuned by MERT (Och, 2003). All the language models are 5-gram built with the SRILM toolkit (Stolcke, 2002) on the monolingual part of the parallel corpora.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Experiments were carried out on three English&#8211; Chinese translation tasks.</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The training corpora comprise 20K, 200K and 2.1 million sentence pairs, where the former two corpora are derived from FBIS corpus 1 which is sentence-aligned by Champollion aligner (Ma, 2006), the latter corpus comes from HK parallel corpus, 2 ISI parallel corpus, 3 other news data and parallel dictionaries from LDC.</text>
                  <doc_id>160</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The development set and the test set for the 20K and 200K corpora are randomly selected from the FBIS corpus, each of which contains 1,200 sentences, with one reference.</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For the 2.1 million corpus, the NIST 2005 Chinese&#8211;English current set (1,082 sentences) with one reference is used as the development set, and NIST 2003 English&#8211;Chinese current set (1,859 sentences) with four references is used as the test set.</text>
                  <doc_id>162</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Three baseline systems are built for comparison: Moses PBSMT baseline system (Koehn et al., 2007), a realization of the translation model augmentation system described in (Callison-Burch et al., 2006) (named &#8220;Para-Sub&#8221; hereafter), and the word-lattice based system proposed in (Du et al., 2010).</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Word alignments on the parallel corpus are performed using GIZA++ (Och and Ney, 2003) with the &#8220;grow-diag-final&#8221; refinement.</text>
                  <doc_id>164</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Maximum phrase length is set to 10 words and the parameters in the log-linear model are tuned by MERT (Och, 2003).</text>
                  <doc_id>165</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>All the language models are 5-gram built with the SRILM toolkit (Stolcke, 2002) on the monolingual part of the parallel corpora.</text>
                  <doc_id>166</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Paraphrase acquisition</title>
            <text>The paraphrases data for all paraphrase-enriched system is derived from the &#8220;Paraphrase Phrase Ta-
ble&#8221; 4 of TER-Plus (Snover et al., 2009). Furthermore, the following two steps are taken to filter out noise paraphrases as described in (Du et al., 2010):
1. Filter out paraphrases with probabilities lower than 0.01.
2. Filter out paraphrases which are not observed in the phrase table. This objective is to guarantee that no extra out-of-vocabulary words are introduced into the paraphrase systems.
The filtered paraphrase table is then used to generate word lattices and CNs.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The paraphrases data for all paraphrase-enriched system is derived from the &#8220;Paraphrase Phrase Ta-</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>ble&#8221; 4 of TER-Plus (Snover et al., 2009).</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Furthermore, the following two steps are taken to filter out noise paraphrases as described in (Du et al., 2010):</text>
                  <doc_id>169</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1.</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Filter out paraphrases with probabilities lower than 0.01.</text>
                  <doc_id>171</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.</text>
                  <doc_id>172</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Filter out paraphrases which are not observed in the phrase table.</text>
                  <doc_id>173</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This objective is to guarantee that no extra out-of-vocabulary words are introduced into the paraphrase systems.</text>
                  <doc_id>174</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The filtered paraphrase table is then used to generate word lattices and CNs.</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>4.3 Experimental results</title>
            <text>The results are reported in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Table 1 compares the performance of four systems on three translation tasks. As can be observed from the Table, for 20K and 200K corpora, the word-lattice-based system accomplished the best results. For the 20K corpus, the CN outperformed the baseline PBSMT by 0.31 absolute (2.15% relative) BLEU points and 1.5 absolute (1.99% relative) TER points. For the 200K corpus, it still outperformed the &#8220;Para-Sub&#8221; by 0.06 absolute (0.26% relative) BLEU points and 0.15 absolute (0.23% relative) TER points. Note that for the 2.1M corpus, although CN underperformed the best word lattice by an insignificant amount (0.06 absolute, 0.41%
4 http://www.umiacs.umd.edu/&#732;snover/terp/
downloads/terp-pt.v1.tgz
relative) in terms of BLEU points, it has the best performance in terms of TER points (0.22 absolute, 0.3% relative than word lattice). Furthermore, the CN outperformed &#8220;Para-Sub&#8221; by 0.36 absolute (2.55% relative) BLEU points and 1.37 absolute (1.84% relative) TER points, and also beat the baseline PBSMT system by 0.45 absolute (3.21% relative) BLEU points and 1.82 absolute (2.43% relative) TER points. The paired 95% confidence interval of significant test (Zhang and Vogel, 2004) between the &#8220;Lattice&#8221; and &#8220;CN&#8221; system is [-0.19, +0.38], which also suggests that the two system has a comparable performance in terms of BLEU.
In Table 2, decoding time on test sets is reported to compare the computational efficiency of the baseline PBSMT, word-lattice-based and CNbased methods. Note that word lattice construction time and CN building time (including word lattice construction and conversion from word lattices into CNs with the SRILM toolkit (Stolcke, 2002)) are counted in the decoding time and illustrated in the table within parentheses respectively. Although both word-lattice-based and CN-based methods require longer decoding times than the baseline PB- SMT system, it is observed that compared with the word lattices, CNs reduced the decoding time significantly on three tasks, namely 52.06% for the 20K model, 75.75% for the 200K model and 78.88% for the 2.1M model. It is also worth noting that the &#8220;Para-Sub&#8221; system has a similar decoding time with baseline PBSMT since only the translation table is modified.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The results are reported in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores.</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Table 1 compares the performance of four systems on three translation tasks.</text>
                  <doc_id>177</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As can be observed from the Table, for 20K and 200K corpora, the word-lattice-based system accomplished the best results.</text>
                  <doc_id>178</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For the 20K corpus, the CN outperformed the baseline PBSMT by 0.31 absolute (2.15% relative) BLEU points and 1.5 absolute (1.99% relative) TER points.</text>
                  <doc_id>179</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For the 200K corpus, it still outperformed the &#8220;Para-Sub&#8221; by 0.06 absolute (0.26% relative) BLEU points and 0.15 absolute (0.23% relative) TER points.</text>
                  <doc_id>180</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Note that for the 2.1M corpus, although CN underperformed the best word lattice by an insignificant amount (0.06 absolute, 0.41%</text>
                  <doc_id>181</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4 http://www.umiacs.umd.edu/&#732;snover/terp/</text>
                  <doc_id>182</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>downloads/terp-pt.v1.tgz</text>
                  <doc_id>183</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>relative) in terms of BLEU points, it has the best performance in terms of TER points (0.22 absolute, 0.3% relative than word lattice).</text>
                  <doc_id>184</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Furthermore, the CN outperformed &#8220;Para-Sub&#8221; by 0.36 absolute (2.55% relative) BLEU points and 1.37 absolute (1.84% relative) TER points, and also beat the baseline PBSMT system by 0.45 absolute (3.21% relative) BLEU points and 1.82 absolute (2.43% relative) TER points.</text>
                  <doc_id>185</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The paired 95% confidence interval of significant test (Zhang and Vogel, 2004) between the &#8220;Lattice&#8221; and &#8220;CN&#8221; system is [-0.19, +0.38], which also suggests that the two system has a comparable performance in terms of BLEU.</text>
                  <doc_id>186</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In Table 2, decoding time on test sets is reported to compare the computational efficiency of the baseline PBSMT, word-lattice-based and CNbased methods.</text>
                  <doc_id>187</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Note that word lattice construction time and CN building time (including word lattice construction and conversion from word lattices into CNs with the SRILM toolkit (Stolcke, 2002)) are counted in the decoding time and illustrated in the table within parentheses respectively.</text>
                  <doc_id>188</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Although both word-lattice-based and CN-based methods require longer decoding times than the baseline PB- SMT system, it is observed that compared with the word lattices, CNs reduced the decoding time significantly on three tasks, namely 52.06% for the 20K model, 75.75% for the 200K model and 78.88% for the 2.1M model.</text>
                  <doc_id>189</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>It is also worth noting that the &#8220;Para-Sub&#8221; system has a similar decoding time with baseline PBSMT since only the translation table is modified.</text>
                  <doc_id>190</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>4.4 Discussion</title>
            <text>From the performance and decoding time reported in the last section, it is obvious that on large scale corpora, the CN-based method significantly reduced the computational complexity while preserved the system performance of the best lattice-based method. Thus it makes the paraphrase-enriched SMT system more applicable to real-world applications. On the other hand, for small- and medium-scale data, CNs can be used as a compromise between speed and quality, since decoding time is much less than word lattices, and compared with the &#8220;Para-Sub&#8221; system, the only overhead is the transforming of the input sentences.
It is also interesting that the relative performance of the CNs increases gradually with the size of the training corpus, which indicates that it is more suitable for models built from large scale data. Considering the decoding time, it is preferable to use CNs instead of word lattices for such translation tasks. However, for the small- and medium-scale data, the CN system is not competitive even compared with the baseline. In this case it suggests that, on these two tasks, the coverage issue is not solved by incorporating paraphrases with the CN structure. It might because of the ambiguity that introduced by CNs harms the decoder to choose the appropriate source words from paraphrases. On the other hand, this ambiguity could be decreased with translation models trained on a large corpus, which provides enough observations for the decoders to favour para- 38
phrases.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>From the performance and decoding time reported in the last section, it is obvious that on large scale corpora, the CN-based method significantly reduced the computational complexity while preserved the system performance of the best lattice-based method.</text>
                  <doc_id>191</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Thus it makes the paraphrase-enriched SMT system more applicable to real-world applications.</text>
                  <doc_id>192</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>On the other hand, for small- and medium-scale data, CNs can be used as a compromise between speed and quality, since decoding time is much less than word lattices, and compared with the &#8220;Para-Sub&#8221; system, the only overhead is the transforming of the input sentences.</text>
                  <doc_id>193</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It is also interesting that the relative performance of the CNs increases gradually with the size of the training corpus, which indicates that it is more suitable for models built from large scale data.</text>
                  <doc_id>194</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Considering the decoding time, it is preferable to use CNs instead of word lattices for such translation tasks.</text>
                  <doc_id>195</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, for the small- and medium-scale data, the CN system is not competitive even compared with the baseline.</text>
                  <doc_id>196</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In this case it suggests that, on these two tasks, the coverage issue is not solved by incorporating paraphrases with the CN structure.</text>
                  <doc_id>197</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>It might because of the ambiguity that introduced by CNs harms the decoder to choose the appropriate source words from paraphrases.</text>
                  <doc_id>198</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>On the other hand, this ambiguity could be decreased with translation models trained on a large corpus, which provides enough observations for the decoders to favour para- 38</text>
                  <doc_id>199</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>phrases.</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Conclusion and future work</title>
        <text>In this paper, CNs are used instead of word lattices to incorporate paraphrases into SMT. Transformation from word lattices into CNs is used to merge path duplications, and decoding time is drastically reduced with CN decoding. Experiments are carried out on small-, medium- and large-scale English&#8211; Chinese translation tasks and confirm that compared with word lattices, it is much more computationally efficient to use CNs, while no loss of performance is observed on the large-scale task. In the future, we plan to apply more features such as source-side language models and phrase length (Onishi et al., 2010) on the CNs to obtain better system performance. Furthermore, we will carry out this work on other language pairs to show the effectiveness of paraphrases in SMT systems. We will also investigate the reason for its lower performance on the small- and medium-scale corpora, as well as the impact of the paraphrase filtering procedure on translation quality.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper, CNs are used instead of word lattices to incorporate paraphrases into SMT.</text>
              <doc_id>201</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Transformation from word lattices into CNs is used to merge path duplications, and decoding time is drastically reduced with CN decoding.</text>
              <doc_id>202</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Experiments are carried out on small-, medium- and large-scale English&#8211; Chinese translation tasks and confirm that compared with word lattices, it is much more computationally efficient to use CNs, while no loss of performance is observed on the large-scale task.</text>
              <doc_id>203</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In the future, we plan to apply more features such as source-side language models and phrase length (Onishi et al., 2010) on the CNs to obtain better system performance.</text>
              <doc_id>204</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Furthermore, we will carry out this work on other language pairs to show the effectiveness of paraphrases in SMT systems.</text>
              <doc_id>205</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We will also investigate the reason for its lower performance on the small- and medium-scale corpora, as well as the impact of the paraphrase filtering procedure on translation quality.</text>
              <doc_id>206</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>Acknowledgments</title>
        <text>This research is supported by the Science Foundation Ireland (Grant 07/CE/I1142) as part of the Centre for Next Generation Localisation (www.cngl.ie) at Dublin City University. Thanks to the reviewers for their invaluable comments and suggestions.
References
Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In 43rd Annual meeting of the Association for Computational Linguistics, Ann Arbor, MI, pages 597&#8211;604. Nicola Bertoldi, Richard Zens, Marcello Federico, and
Wade Shen 2008. Efficient Speech Translation Through Confusion Network Decoding. In IEEE Transactions on Audio, Speech, and Language Processing, 16(8), pages 1696&#8211;1705. Francis Bond, Eric Nichols, Darren Scott Appling and
Michael Paul. 2008. Improving Statistical Machine Translation by Paraphrasing the Training Data. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), Hawaii, pages 150&#8211; 157.
Chris Callison-Burch, Philipp Koehn and Miles Osborne. 2006. Improved Statistical Machine Translation Using Paraphrases. In Proceedings of the Human Language Technology conference - North American chapter of the Association for Computational Linguistics (HLT- NAACL), NY, pages 17&#8211;24.
Jinhua Du, Jie Jiang and Andy Way. 2010. Facilitating Translation Using Source Language Paraphrase Lattices. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Cambridge, MA, pages 420&#8211;429.
Dilek Hakkani-T&#252;r, Fr&#233;d&#233;ric B&#233;chet, Giuseppe Riccardi and Gokhan Tur. 2005. Beyond ASR 1-best: Using word confusion networks in spoken language understanding. In Computer Speech and Language (2005): 20(4), pages 495&#8211;514.
Philipp Koehn, Hieu Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, Wade Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In ACL 2007: demo and poster sessions, Prague, Czech Republic, pages 177&#8211;180. Roland Kuhn, Boxing Chen, George Foster and Evan
Stratford. 2010. Phrase Clustering for Smoothing TM Probabilities - or, How to Extract Paraphrases from Phrase Tables. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), Beijing, China, pages 608&#8211;616.
Xiaoyi Ma. 2006. Champollion: A Robust Parallel Text
Sentence Aligner. LREC 2006: Fifth International Conference on Language Resources and Evaluation, Genova, Italy, pages 489&#8211;492. Nitin Madnani, Necip Fazil Ayan, Philip Resnik and Bonnie J. Dorr. 2007. Using Paraphrases for Parameter Tuning in Statistical Machine Translation. In Proceed- 39
ings of the Second Workshop on Statistical Machine
Translation, Prague, Czech Republic, pages 120&#8211;127. Lidia Mangu, Eric Brill and Andreas Stolcke. 2000.
Finding Consensus in Speech Recognition: Word Error Minimization and Other Applications of Confusion Networks. In Computer Speech and Language 14 (4), pages 373&#8211;400. Yuval Marton, Chris Callison-Burch and Philip Resnik.
2009. Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Singapore, pages 381&#8211;390. Aur&#233;lien Max. 2010. Example-Based Paraphrasing
for Improved Phrase-Based Statistical Machine Translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, Cambridge, MA, pages 656&#8211;666. Preslav Nakov. 2008a. Improved Statistical Machine Translation Using Monolingual Paraphrases In Proceedings of the European Conference on Artificial Intelligence (ECAI), Patras, Greece, pages 338&#8211;342. Preslav Nakov. 2008b. Improving English-Spanish statistical machine translation: experiments in domain adaptation, sentence paraphrasing, tokenization, and recasing. In Proceedings of ACL-08:HLT. Third Workshop on Statistical Machine Translation, Columbus, Ohio, USA, pages 147&#8211;150. Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 160&#8211;167. Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, PA, pages 295&#8211;302. Franz Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1), pages 19&#8211;51. Takashi Onishi, Masao Utiyama and Eiichiro, Sumita.
2010. Paraphrase Lattice for Statistical Machine Translation. In Proceedings of the ACL 2010 Conference Short Papers, Uppsala, Sweden, pages 1&#8211;5. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: A Method For Automatic Evaluation of Machine Translation. ACL-2002: 40th Annual meeting of the Association for Computational Linguistics, pp.311-318, Philadelphia, PA. Josh Schroeder, Trevor Cohn and Philipp Koehn. 2009.
Word Lattices for Multi-Source Translation. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), Athens, Greece, pages 719&#8211;727.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, Cambridge, pages 223&#8211;231. Matthew Snover, Nitin Madnani, Bonnie J.Dorr and
Richard Schwartz. 2009. Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric. In Proceedings of the Fourth Workshop on Statistical Machine Translation, Athens, Greece, pages 259&#8211;268. Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing (ICSLP), Denver, Colorado, pages 901&#8211;904. Ying Zhang and Stephan Vogel. 2004. Measuring confidence intervals for the machine translation evaluation metrics. In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI). pages 85&#8211;94.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This research is supported by the Science Foundation Ireland (Grant 07/CE/I1142) as part of the Centre for Next Generation Localisation (www.cngl.ie) at Dublin City University.</text>
              <doc_id>207</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Thanks to the reviewers for their invaluable comments and suggestions.</text>
              <doc_id>208</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>References</text>
              <doc_id>209</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Colin Bannard and Chris Callison-Burch.</text>
              <doc_id>210</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>211</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Paraphrasing with bilingual parallel corpora.</text>
              <doc_id>212</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In 43rd Annual meeting of the Association for Computational Linguistics, Ann Arbor, MI, pages 597&#8211;604.</text>
              <doc_id>213</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Nicola Bertoldi, Richard Zens, Marcello Federico, and</text>
              <doc_id>214</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Wade Shen 2008.</text>
              <doc_id>215</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Efficient Speech Translation Through Confusion Network Decoding.</text>
              <doc_id>216</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In IEEE Transactions on Audio, Speech, and Language Processing, 16(8), pages 1696&#8211;1705.</text>
              <doc_id>217</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Francis Bond, Eric Nichols, Darren Scott Appling and</text>
              <doc_id>218</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Michael Paul.</text>
              <doc_id>219</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>220</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Improving Statistical Machine Translation by Paraphrasing the Training Data.</text>
              <doc_id>221</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), Hawaii, pages 150&#8211; 157.</text>
              <doc_id>222</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Chris Callison-Burch, Philipp Koehn and Miles Osborne.</text>
              <doc_id>223</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>224</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Improved Statistical Machine Translation Using Paraphrases.</text>
              <doc_id>225</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Human Language Technology conference - North American chapter of the Association for Computational Linguistics (HLT- NAACL), NY, pages 17&#8211;24.</text>
              <doc_id>226</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Jinhua Du, Jie Jiang and Andy Way.</text>
              <doc_id>227</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>228</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Facilitating Translation Using Source Language Paraphrase Lattices.</text>
              <doc_id>229</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Cambridge, MA, pages 420&#8211;429.</text>
              <doc_id>230</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Dilek Hakkani-T&#252;r, Fr&#233;d&#233;ric B&#233;chet, Giuseppe Riccardi and Gokhan Tur.</text>
              <doc_id>231</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>232</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Beyond ASR 1-best: Using word confusion networks in spoken language understanding.</text>
              <doc_id>233</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Computer Speech and Language (2005): 20(4), pages 495&#8211;514.</text>
              <doc_id>234</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Philipp Koehn, Hieu Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, Wade Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin and Evan Herbst.</text>
              <doc_id>235</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>236</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Moses: Open Source Toolkit for Statistical Machine Translation.</text>
              <doc_id>237</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In ACL 2007: demo and poster sessions, Prague, Czech Republic, pages 177&#8211;180.</text>
              <doc_id>238</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Roland Kuhn, Boxing Chen, George Foster and Evan</text>
              <doc_id>239</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Stratford.</text>
              <doc_id>240</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>241</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Phrase Clustering for Smoothing TM Probabilities - or, How to Extract Paraphrases from Phrase Tables.</text>
              <doc_id>242</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), Beijing, China, pages 608&#8211;616.</text>
              <doc_id>243</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Xiaoyi Ma.</text>
              <doc_id>244</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>245</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Champollion: A Robust Parallel Text</text>
              <doc_id>246</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Sentence Aligner.</text>
              <doc_id>247</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>LREC 2006: Fifth International Conference on Language Resources and Evaluation, Genova, Italy, pages 489&#8211;492.</text>
              <doc_id>248</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Nitin Madnani, Necip Fazil Ayan, Philip Resnik and Bonnie J. Dorr.</text>
              <doc_id>249</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>250</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Using Paraphrases for Parameter Tuning in Statistical Machine Translation.</text>
              <doc_id>251</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In Proceed- 39</text>
              <doc_id>252</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>ings of the Second Workshop on Statistical Machine</text>
              <doc_id>253</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Translation, Prague, Czech Republic, pages 120&#8211;127.</text>
              <doc_id>254</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Lidia Mangu, Eric Brill and Andreas Stolcke.</text>
              <doc_id>255</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>2000.</text>
              <doc_id>256</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Finding Consensus in Speech Recognition: Word Error Minimization and Other Applications of Confusion Networks.</text>
              <doc_id>257</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Computer Speech and Language 14 (4), pages 373&#8211;400.</text>
              <doc_id>258</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Yuval Marton, Chris Callison-Burch and Philip Resnik.</text>
              <doc_id>259</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2009.</text>
              <doc_id>260</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases.</text>
              <doc_id>261</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Singapore, pages 381&#8211;390.</text>
              <doc_id>262</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Aur&#233;lien Max.</text>
              <doc_id>263</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>264</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Example-Based Paraphrasing</text>
              <doc_id>265</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>for Improved Phrase-Based Statistical Machine Translation.</text>
              <doc_id>266</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, Cambridge, MA, pages 656&#8211;666.</text>
              <doc_id>267</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Preslav Nakov.</text>
              <doc_id>268</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2008a.</text>
              <doc_id>269</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Improved Statistical Machine Translation Using Monolingual Paraphrases In Proceedings of the European Conference on Artificial Intelligence (ECAI), Patras, Greece, pages 338&#8211;342.</text>
              <doc_id>270</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Preslav Nakov.</text>
              <doc_id>271</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>2008b.</text>
              <doc_id>272</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Improving English-Spanish statistical machine translation: experiments in domain adaptation, sentence paraphrasing, tokenization, and recasing.</text>
              <doc_id>273</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL-08:HLT.</text>
              <doc_id>274</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Third Workshop on Statistical Machine Translation, Columbus, Ohio, USA, pages 147&#8211;150.</text>
              <doc_id>275</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Franz Josef Och.</text>
              <doc_id>276</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>277</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Minimum Error Rate Training in</text>
              <doc_id>278</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Statistical Machine Translation.</text>
              <doc_id>279</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 160&#8211;167.</text>
              <doc_id>280</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Franz Josef Och and Hermann Ney.</text>
              <doc_id>281</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>282</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Discriminative training and maximum entropy models for statistical machine translation.</text>
              <doc_id>283</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, PA, pages 295&#8211;302.</text>
              <doc_id>284</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Franz Och and Hermann Ney.</text>
              <doc_id>285</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>286</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>A systematic comparison of various statistical alignment models.</text>
              <doc_id>287</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 29(1), pages 19&#8211;51.</text>
              <doc_id>288</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Takashi Onishi, Masao Utiyama and Eiichiro, Sumita.</text>
              <doc_id>289</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2010.</text>
              <doc_id>290</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Paraphrase Lattice for Statistical Machine Translation.</text>
              <doc_id>291</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the ACL 2010 Conference Short Papers, Uppsala, Sweden, pages 1&#8211;5.</text>
              <doc_id>292</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu.</text>
              <doc_id>293</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>294</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>BLEU: A Method For Automatic Evaluation of Machine Translation.</text>
              <doc_id>295</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>ACL-2002: 40th Annual meeting of the Association for Computational Linguistics, pp.311-318, Philadelphia, PA.</text>
              <doc_id>296</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Josh Schroeder, Trevor Cohn and Philipp Koehn.</text>
              <doc_id>297</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>298</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Word Lattices for Multi-Source Translation.</text>
              <doc_id>299</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), Athens, Greece, pages 719&#8211;727.</text>
              <doc_id>300</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla and John Makhoul.</text>
              <doc_id>301</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>302</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A study of translation edit rate with targeted human annotation.</text>
              <doc_id>303</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, Cambridge, pages 223&#8211;231.</text>
              <doc_id>304</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Matthew Snover, Nitin Madnani, Bonnie J.Dorr and</text>
              <doc_id>305</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Richard Schwartz.</text>
              <doc_id>306</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>307</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Fluency, adequacy, or HTER?</text>
              <doc_id>308</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Exploring different human judgments with a tunable MT metric.</text>
              <doc_id>309</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Fourth Workshop on Statistical Machine Translation, Athens, Greece, pages 259&#8211;268.</text>
              <doc_id>310</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Andreas Stolcke.</text>
              <doc_id>311</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>312</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>SRILM - an extensible language</text>
              <doc_id>313</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>modeling toolkit.</text>
              <doc_id>314</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the International Conference on Spoken Language Processing (ICSLP), Denver, Colorado, pages 901&#8211;904.</text>
              <doc_id>315</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Ying Zhang and Stephan Vogel.</text>
              <doc_id>316</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2004.</text>
              <doc_id>317</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Measuring confidence intervals for the machine translation evaluation metrics.</text>
              <doc_id>318</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI).</text>
              <doc_id>319</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>pages 85&#8211;94.</text>
              <doc_id>320</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Comparison on PBSMT, &#8220;Para-Sub&#8221;, word-lattice and CN-based methods.</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>20K</cell>
              <cell>200K</cell>
              <cell>2.1M</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System</cell>
              <cell>BLEU</cell>
              <cell>TER</cell>
              <cell>BLEU</cell>
              <cell>TER</cell>
              <cell>BLEU</cell>
              <cell>TER</cell>
            </row>
            <row>
              <cell>Baseline PBSMT</cell>
              <cell>14.42</cell>
              <cell>75.30</cell>
              <cell>23.60</cell>
              <cell>63.65</cell>
              <cell>14.04</cell>
              <cell>74.88</cell>
            </row>
            <row>
              <cell>Para-Sub</cell>
              <cell>14.78</cell>
              <cell>73.75</cell>
              <cell>23.41</cell>
              <cell>63.84</cell>
              <cell>14.13</cell>
              <cell>74.43</cell>
            </row>
            <row>
              <cell>Word-lattice-based</cell>
              <cell>15.44</cell>
              <cell>73.06</cell>
              <cell>25.20</cell>
              <cell>62.37</cell>
              <cell>14.55</cell>
              <cell>73.28</cell>
            </row>
            <row>
              <cell>CN-based</cell>
              <cell>14.73</cell>
              <cell>73.8</cell>
              <cell>23.47</cell>
              <cell>63.69</cell>
              <cell>14.49</cell>
              <cell>73.06</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 1: Comparison on PBSMT,  Para-Sub , word-lattice and CN-based methods.#@#@Table 2: Decoding time comparison of PBSMT, word-lattice (&#8220;Lattice&#8221;) and CN-based (&#8220;CN&#8221;) methods.</caption>
        <reference_text>In PAGE 7: ..., 2006) scores.  Table1  compares the performance of four sys- tems on three translation tasks. As can be observed from the Table, for 20K and 200K corpora, the word-lattice-based system accomplished the best re- sults....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>System</cell>
              <cell>FBIS testset (1,200 inputs)</cell>
              <cell>FBIS testset (1,200 inputs)</cell>
              <cell>NIST testset (1,859 inputs)</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>20K model</cell>
              <cell>200K model</cell>
              <cell>2.1M model</cell>
            </row>
            <row>
              <cell>Baseline</cell>
              <cell>21 min</cell>
              <cell>41 min</cell>
              <cell>37 min</cell>
            </row>
            <row>
              <cell>Lattice</cell>
              <cell>102 min (+ 15 sec)</cell>
              <cell>398 min (+ 20 sec)</cell>
              <cell>559 min (+ 21 sec)</cell>
            </row>
            <row>
              <cell>CN</cell>
              <cell>48 min (+ 61 sec)</cell>
              <cell>95 min (+ 96 sec)</cell>
              <cell>116 min (+ 129 sec)</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Colin Bannard</author>
          <author>Chris Callison-Burch</author>
        </authors>
        <title>Paraphrasing with bilingual parallel corpora.</title>
        <publication>In 43rd Annual meeting of the Association for Computational Linguistics,</publication>
        <pages>597--604</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Nicola Bertoldi</author>
          <author>Richard Zens</author>
          <author>Marcello Federico</author>
          <author>Wade Shen</author>
        </authors>
        <title>Efficient Speech Translation Through Confusion Network Decoding.</title>
        <publication>In IEEE Transactions on Audio, Speech, and Language Processing,</publication>
        <pages>1696--1705</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Francis Bond</author>
          <author>Eric Nichols</author>
          <author>Darren Scott Appling</author>
          <author>Michael Paul</author>
        </authors>
        <title>Improving Statistical Machine Translation by Paraphrasing the Training Data.</title>
        <publication>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), Hawaii,</publication>
        <pages>150--157</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Chris Callison-Burch</author>
          <author>Philipp Koehn</author>
          <author>Miles Osborne</author>
        </authors>
        <title>Improved Statistical Machine Translation Using Paraphrases.</title>
        <publication>In Proceedings of the Human Language Technology conference - North American chapter of the Association for Computational Linguistics (HLTNAACL), NY,</publication>
        <pages>17--24</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Jinhua Du</author>
          <author>Jie Jiang</author>
          <author>Andy Way</author>
        </authors>
        <title>Facilitating Translation Using Source Language Paraphrase Lattices.</title>
        <publication>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</publication>
        <pages>420--429</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Dilek Hakkani-T&#252;r</author>
          <author>Fr&#233;d&#233;ric B&#233;chet</author>
          <author>Giuseppe Riccardi</author>
          <author>Gokhan Tur</author>
        </authors>
        <title>Beyond ASR 1-best: Using word confusion networks in spoken language understanding.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>6</id>
        <authors/>
        <title>None</title>
        <publication>In Computer Speech and Language</publication>
        <pages>495--514</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Hieu Hoang</author>
          <author>A Birch</author>
          <author>C Callison-Burch</author>
          <author>M Federico</author>
          <author>N Bertoldi</author>
          <author>B Cowan</author>
          <author>Wade Shen</author>
          <author>C</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>R Zens Moran</author>
          <author>C Dyer</author>
          <author>O Bojar</author>
          <author>A Constantin</author>
          <author>Evan Herbst</author>
        </authors>
        <title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
        <publication>In ACL</publication>
        <pages>177--180</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Roland Kuhn</author>
          <author>Boxing Chen</author>
          <author>George Foster</author>
          <author>Evan Stratford</author>
        </authors>
        <title>Phrase Clustering for Smoothing TM Probabilities - or, How to Extract Paraphrases from Phrase Tables.</title>
        <publication>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),</publication>
        <pages>608--616</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Xiaoyi Ma</author>
        </authors>
        <title>Champollion: A Robust Parallel Text Sentence Aligner. LREC</title>
        <publication>Fifth International Conference on Language Resources and Evaluation,</publication>
        <pages>489--492</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Nitin Madnani</author>
          <author>Necip Fazil Ayan</author>
          <author>Philip Resnik</author>
          <author>Bonnie J Dorr</author>
        </authors>
        <title>Using Paraphrases for Parameter Tuning in Statistical Machine Translation.</title>
        <publication>In Proceed39 ings of the Second Workshop on Statistical Machine Translation,</publication>
        <pages>120--127</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Lidia Mangu</author>
        </authors>
        <title>Eric Brill and Andreas Stolcke.</title>
        <publication>None</publication>
        <pages>373--400</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Yuval Marton</author>
          <author>Chris Callison-Burch</author>
          <author>Philip Resnik</author>
        </authors>
        <title>Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases.</title>
        <publication>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Singapore,</publication>
        <pages>381--390</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Aur&#233;lien Max</author>
        </authors>
        <title>Example-Based Paraphrasing for Improved Phrase-Based Statistical Machine Translation.</title>
        <publication>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>656--666</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Preslav Nakov</author>
        </authors>
        <title>Improved Statistical Machine Translation Using Monolingual Paraphrases</title>
        <publication>In Proceedings of the European Conference on Artificial Intelligence (ECAI), Patras, Greece,</publication>
        <pages>338--342</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Preslav Nakov</author>
        </authors>
        <title>Improving English-Spanish statistical machine translation: experiments in domain adaptation, sentence paraphrasing, tokenization, and recasing.</title>
        <publication>In Proceedings of ACL-08:HLT. Third Workshop on Statistical Machine Translation,</publication>
        <pages>147--150</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum Error Rate Training in Statistical Machine Translation.</title>
        <publication>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</publication>
        <pages>160--167</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Discriminative training and maximum entropy models for statistical machine translation.</title>
        <publication>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</publication>
        <pages>295--302</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Franz Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A systematic comparison of various statistical alignment models.</title>
        <publication>None</publication>
        <pages>pages</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Takashi Onishi</author>
          <author>Masao Utiyama</author>
          <author>Sumita Eiichiro</author>
        </authors>
        <title>Paraphrase Lattice for Statistical Machine Translation.</title>
        <publication>In Proceedings of the ACL 2010 Conference Short Papers,</publication>
        <pages>1--5</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>BLEU: A Method For Automatic Evaluation</title>
        <publication>of Machine Translation. ACL-2002: 40th Annual meeting of the Association for Computational Linguistics,</publication>
        <pages>311--318</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Josh Schroeder</author>
          <author>Trevor Cohn</author>
          <author>Philipp Koehn</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Bonnie Dorr Snover</author>
          <author>Richard Schwartz</author>
          <author>Linnea Micciulla</author>
          <author>John Makhoul</author>
        </authors>
        <title>Word Lattices for Multi-Source Translation.</title>
        <publication>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009),</publication>
        <pages>719--727</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>Matthew Snover</author>
          <author>Nitin Madnani</author>
          <author>Bonnie J Dorr</author>
          <author>Richard Schwartz</author>
        </authors>
        <title>Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric.</title>
        <publication>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</publication>
        <pages>259--268</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Andreas Stolcke</author>
        </authors>
        <title>SRILM - an extensible language modeling toolkit.</title>
        <publication>In Proceedings of the International Conference on Spoken Language Processing (ICSLP),</publication>
        <pages>901--904</pages>
        <date>2002</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Bannard and Callison-Burch, 2005</string>
        <sentence_id>44850</sentence_id>
        <char_offset>18</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Bertoldi et al., 2008</string>
        <sentence_id>44837</sentence_id>
        <char_offset>334</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>Bertoldi et al., 2008</string>
        <sentence_id>44897</sentence_id>
        <char_offset>52</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>1</reference_id>
        <string>Bertoldi et al., 2008</string>
        <sentence_id>44954</sentence_id>
        <char_offset>188</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>1</reference_id>
        <string>Bertoldi et al., 2008</string>
        <sentence_id>44945</sentence_id>
        <char_offset>56</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>2</reference_id>
        <string>Bond et al., 2008</string>
        <sentence_id>44816</sentence_id>
        <char_offset>49</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>3</reference_id>
        <string>Callison-Burch et al., 2006</string>
        <sentence_id>44814</sentence_id>
        <char_offset>51</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>3</reference_id>
        <string>Callison-Burch et al., 2006</string>
        <sentence_id>44836</sentence_id>
        <char_offset>178</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>3</reference_id>
        <string>Callison-Burch et al., 2006</string>
        <sentence_id>44901</sentence_id>
        <char_offset>63</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>3</reference_id>
        <string>Callison-Burch et al., 2006</string>
        <sentence_id>44960</sentence_id>
        <char_offset>172</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>4</reference_id>
        <string>Du et al., 2010</string>
        <sentence_id>44818</sentence_id>
        <char_offset>46</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>4</reference_id>
        <string>Du et al., 2010</string>
        <sentence_id>44825</sentence_id>
        <char_offset>4</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>4</reference_id>
        <string>Du et al., 2010</string>
        <sentence_id>44831</sentence_id>
        <char_offset>17</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>4</reference_id>
        <string>Du et al., 2010</string>
        <sentence_id>44836</sentence_id>
        <char_offset>252</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>4</reference_id>
        <string>Du et al., 2010</string>
        <sentence_id>44901</sentence_id>
        <char_offset>153</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>4</reference_id>
        <string>Du et al., 2010</string>
        <sentence_id>44886</sentence_id>
        <char_offset>125</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>4</reference_id>
        <string>Du et al., 2010</string>
        <sentence_id>44890</sentence_id>
        <char_offset>218</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>4</reference_id>
        <string>Du et al., 2010</string>
        <sentence_id>44960</sentence_id>
        <char_offset>278</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>4</reference_id>
        <string>Du et al., 2010</string>
        <sentence_id>44966</sentence_id>
        <char_offset>96</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>5</reference_id>
        <string>Hakkani-T&#252;r et al., 2005</string>
        <sentence_id>44837</sentence_id>
        <char_offset>170</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>5</reference_id>
        <string>Hakkani-T&#252;r et al., 2005</string>
        <sentence_id>44907</sentence_id>
        <char_offset>4</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>9</reference_id>
        <string>Kuhn et al., 2010</string>
        <sentence_id>44842</sentence_id>
        <char_offset>50</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>10</reference_id>
        <string>Ma, 2006</string>
        <sentence_id>44957</sentence_id>
        <char_offset>182</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>11</reference_id>
        <string>Madnani et al., 2007</string>
        <sentence_id>44840</sentence_id>
        <char_offset>52</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>13</reference_id>
        <string>Marton et al., 2009</string>
        <sentence_id>44814</sentence_id>
        <char_offset>80</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>14</reference_id>
        <string>Max, 2010</string>
        <sentence_id>44842</sentence_id>
        <char_offset>69</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>15</reference_id>
        <string>Nakov, 2008</string>
        <sentence_id>44816</sentence_id>
        <char_offset>68</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>15</reference_id>
        <string>Nakov, 2008</string>
        <sentence_id>44816</sentence_id>
        <char_offset>82</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>16</reference_id>
        <string>Nakov, 2008</string>
        <sentence_id>44816</sentence_id>
        <char_offset>68</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>16</reference_id>
        <string>Nakov, 2008</string>
        <sentence_id>44816</sentence_id>
        <char_offset>82</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>17</reference_id>
        <string>Och, 2003</string>
        <sentence_id>44944</sentence_id>
        <char_offset>128</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>17</reference_id>
        <string>Och, 2003</string>
        <sentence_id>44962</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>18</reference_id>
        <string>Och and Ney, 2002</string>
        <sentence_id>44944</sentence_id>
        <char_offset>164</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>19</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>44961</sentence_id>
        <char_offset>67</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>20</reference_id>
        <string>Onishi et al., 2010</string>
        <sentence_id>44825</sentence_id>
        <char_offset>21</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>20</reference_id>
        <string>Onishi et al., 2010</string>
        <sentence_id>44886</sentence_id>
        <char_offset>142</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>20</reference_id>
        <string>Onishi et al., 2010</string>
        <sentence_id>45002</sentence_id>
        <char_offset>101</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>21</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>44973</sentence_id>
        <char_offset>34</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>22</reference_id>
        <string>Schroeder et al., 2009</string>
        <sentence_id>44821</sentence_id>
        <char_offset>93</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>23</reference_id>
        <string>Snover et al., 2006</string>
        <sentence_id>44973</sentence_id>
        <char_offset>66</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>24</reference_id>
        <string>Snover et al., 2009</string>
        <sentence_id>44965</sentence_id>
        <char_offset>20</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>25</reference_id>
        <string>Stolcke, 2002</string>
        <sentence_id>44906</sentence_id>
        <char_offset>83</char_offset>
      </citation>
      <citation>
        <id>42</id>
        <reference_id>25</reference_id>
        <string>Stolcke, 2002</string>
        <sentence_id>44963</sentence_id>
        <char_offset>65</char_offset>
      </citation>
      <citation>
        <id>43</id>
        <reference_id>25</reference_id>
        <string>Stolcke, 2002</string>
        <sentence_id>44985</sentence_id>
        <char_offset>166</char_offset>
      </citation>
    </citations>
  </content>
</document>
