<PAPER>
  <FILENO/>
  <TITLE>Incremental Adaptation of Speech-to-Speech Translation</TITLE>
  <AUTHORS>
    <AUTHOR>Nguyen Bach</AUTHOR>
    <AUTHOR>Roger Hsiao</AUTHOR>
    <AUTHOR>Matthias Eck</AUTHOR>
    <AUTHOR>Paisarn Charoenpornsawat</AUTHOR>
    <AUTHOR>Stephan Vogel</AUTHOR>
    <AUTHOR>Tanja Schultz</AUTHOR>
    <AUTHOR>Ian Lane</AUTHOR>
    <AUTHOR>Alex Waibel</AUTHOR>
  </AUTHORS>
  <ABSTRACT>
    <A-S ID="S-23154">In building practical two-way speech-to-speech translation systems the end user will always wish to use the system in an environment different from the original training data.</A-S>
    <A-S ID="S-23155">As with all speech systems, it is important to allow the system to adapt to the actual usage situations.</A-S>
    <A-S ID="S-23156">This paper investigates how a speech-to-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two.</A-S>
    <A-S ID="S-23157">The platform is the CMU Iraqi-English portable two-way speechto-speech system as developed under the DARPA TransTac program.</A-S>
    <A-S ID="S-23158">We show how machine translation, speech recognition and overall system performance can be improved on day 2 after adapting from day 1 in both a supervised and unsupervised way.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-23159">As speech-to-speech translation systems move from the laboratory into field deployment, we quickly see that mismatch in training data with field use can degrade the performance of the system.</S>
        <S ID="S-23160">Retraining based on field usage is a common technique used in all speech systems to improve performance.</S>
        <S ID="S-23161">In the case of speech-to-speech translation we would particularly like to be able to adapt the system based on its usage automatically without having to ship data back to the laboratory for retraining.</S>
        <S ID="S-23162">This paper investigates the scenario of a two-day event.</S>
        <S ID="S-23163">We wish to improve the system for the second day based on the data collected on the first day.</S>
        <S ID="S-23164">Our system is designed for eyes-free use and hence provides no graphical user interface.</S>
        <S ID="S-23165">This allows the user to concentrate on his surrounding environment during an operation.</S>
        <S ID="S-23166">The system only provides audio control and feedback.</S>
        <S ID="S-23167">Additionally the system operates on a push-totalk method.</S>
        <S ID="S-23168">Previously the system (<REF ID="R-04" RPTR="4">Hsiao et al., 2006</REF>; <REF ID="R-00" RPTR="0">Bach et al., 2007</REF>) needed 2 buttons to operate, one for the English speaker and the other one for the Iraqi speaker.</S>
      </P>
      <P>
        <S ID="S-23169">Wiicontroller Mic&amp;Light Loudspeaker</S>
      </P>
      <P>
        <S ID="S-23170">To make the system easier and faster to use, we propose to use a single button which can be controlled by the English speaker.</S>
        <S ID="S-23171">We mounted a microphone and a Wii remote controller together as shown in 1.</S>
        <S ID="S-23172">Since the Wii controller has an accelerometer which can be used to detect the orientation of the controller, this feature can be applied to identify who is speaking.</S>
        <S ID="S-23173">When the English speaker points towards himself, the system will switch to English-Iraqi translation.</S>
        <S ID="S-23174">However, when the Wii is pointed towards somebody else, the system will switch to Iraqi-English translation.</S>
        <S ID="S-23175">In addition, we attach a light on the Wii controller providing visual feedback.</S>
        <S ID="S-23176">This can inform an Iraqi speaker when to start speaking.</S>
        <S ID="S-23177">The overall system is composed of five major components: two automatic speech recognition (ASR) systems, a bidirectional statistical machine translation (SMT) system and two text-to-speech (TTS) systems.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Data Scenario</HEADER>
      <P>
        <S ID="S-23178">The standard data that is available for the TransTac project was collected by recording human interpreter mediated dialogs between war fighters and Iraqi native speakers in various scenarios.</S>
        <S ID="S-23179">The dialog partners were aware that the data was being collected for training machine based translation devices, but would often talk directly to the human interpreter rather than pretending it was an automatic device.</S>
        <S ID="S-23180">This means that the dialog</S>
      </P>
      <P>
        <S ID="S-23181">partners soon ignored the recording equipment and used a mostly natural language, using informal pronunciation and longer sentences with more disfluencies than we find in machine mediated translation dialogs.</S>
      </P>
      <P>
        <S ID="S-23182">Most users mismatch their language when they communicate using an automatic speech-to-speech translation system.</S>
        <S ID="S-23183">They often switch to a clearer pronunciation and use shorter and simpler sentences with less disfluency.</S>
        <S ID="S-23184">This change could have a significant impact on speech recognition and machine translation performance if a system was originally trained on data from the interpreter mediated dialogs.</S>
        <S ID="S-23185">For this reason, additional data was collected during the TransTac meeting in June of 2008.</S>
        <S ID="S-23186">This data was collected with dialog partners using the speech-to-speech translation systems from 4 developer participants in the TransTac program.</S>
        <S ID="S-23187">The dialog partners were given a description of the specific scenario in form of a rough script and had to speak their sentences into the translation systems.</S>
        <S ID="S-23188">The dialog partners were not asked to actually react to the potentially incorrect translations but just followed the script, ignoring the output of the translation system.</S>
        <S ID="S-23189">This has the effect that the dialog partners are no longer talking to a human interpreter, but to a machine, pressing push-to-talk buttons etc. and will change their speech patterns accordingly.</S>
      </P>
      <P>
        <S ID="S-23190">The data was collected over two days, with around 2 hours of actual speech per day.</S>
        <S ID="S-23191">This data was transcribed and translated, resulting in 864 and 824 utterance pairs on day 1 and 2, respectively.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 ASR LM Adaptation</HEADER>
      <P>
        <S ID="S-23192">This section describes the Iraqi ASR system and how we perform LM adaptation on the day 1 data to improve ASR performance on day 2.</S>
        <S ID="S-23193">The CMU Iraqi ASR system is trained with around 350 hours of audio data collected under the TransTac program.</S>
        <S ID="S-23194">The acoustic model is speaker independent but incremental unsupervised MLLR adaptation is performed to improve recognition.</S>
        <S ID="S-23195">The acoustic model has 6000 codebooks and each codebook has at most 64 Gaussian mixtures determined by merge-andsplit training.</S>
        <S ID="S-23196">Semi-tied covariance and boosted MMI discriminative training is performed to improve the model (<REF ID="R-06" RPTR="6">Povey et al., 2009</REF>).</S>
        <S ID="S-23197">The features for the acoustic model is the standard 39-dimension MFCC and we concatenate adjacent 15 frames and perform LDA to reduce the dimension to 42 for the final feature vectors.</S>
        <S ID="S-23198">The language model of the ASR system is a trigram LM trained on the audio transcripts with around three million words with Kneser-Ney smoothing (<REF ID="R-07" RPTR="7">Stolcke, 2002</REF>).</S>
      </P>
      <P>
        <S ID="S-23199">To perform LM adaptation for the ASR system, we use the ASR hypotheses from day 1 to build a LM.</S>
        <S ID="S-23200">This LM is then interpolated with the original trigram LM to produce an adapted LM for day 2.</S>
        <S ID="S-23201">We also evaluate the effect of having transcribers provide accurate transcription references for day 1 data, and see how it may improve the performance on day 2.</S>
        <S ID="S-23202">We compare unigram, bigram and trigram LMs for adaptation.</S>
        <S ID="S-23203">Since the amount of day 1 data is much smaller than the whole training set and we do not assume transcription of day 1 is always available, the interpolation weight is chosen of be 0.9 for the original trigram LM and 0.1 for the new LM built from the day 1 data.</S>
        <S ID="S-23204">The WER of baseline ASR system on day 1 is 32.0%.</S>
      </P>
      <P>
        <S ID="S-23205">The results in Table 1 show that the ASR benefits from LM adaptation.</S>
        <S ID="S-23206">Adapting day 1 data can slightly improve the performance of day 2.</S>
        <S ID="S-23207">The improvement is larger when day 1 transcript is available which is expected.</S>
        <S ID="S-23208">The result also shows that the unigram LM is the most robust model for adaptation as it works reasonably well when transcripts are not available, whereas bigram and trigram LM are more sensitive to the ASR errors made on day 1.</S>
      </P>
      <P>
        <S ID="S-23209">Table 2 shows the impact of ASR adaptation on the performance of the translation system in BLEU (<REF ID="R-05" RPTR="5">Papineni et al., 2002</REF>).</S>
        <S ID="S-23210">In these experiments we only performed adaptation on ASR and still using the baseline SMT component.</S>
        <S ID="S-23211">There is no obvious difference between unsupervised and supervised ASR adaptation on performance of SMT on day 2.</S>
        <S ID="S-23212">However, we can see that the difference in WER on day 2 of unsupervised and supervised ASR adaptation is relatively small.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 SMT Adaptation</HEADER>
      <P>
        <S ID="S-23213">The Iraqi-English SMT system is trained with around 650K sentence pairs collected under the TransTac program.</S>
        <S ID="S-23214">We used PESA phrase extraction (<REF ID="R-09" RPTR="9">Vogel, 2005</REF>) and a suffix array language model (Zhang and <REF ID="R-09" RPTR="10">Vogel, 2005</REF>).</S>
        <S ID="S-23215">To adapt SMT components one approach is to optimize LM interpolation weights by minimizing perplexity of the 1-best translation output (<REF ID="R-01" RPTR="1">Bulyko et al., 2007</REF>).</S>
        <S ID="S-23216">Related work including (<REF ID="R-03" RPTR="3">Eck et al., 2004</REF>) attempts to use information retrieval to select training sentences similar to those in the test set.</S>
        <S ID="S-23217">To adapt the SMT components we use a domain-specific LM on top of the background</S>
      </P>
      <P>
        <S ID="S-23218">language models.</S>
        <S ID="S-23219">This approach is similar to the work in (<REF ID="R-02" RPTR="2">Chen et al., 2008</REF>).</S>
        <S ID="S-23220">sThe adaptation framework is 1) create a domain-specific LM via an n-best list of day 1 machine translation hypothesis, or day 1 translation references; 2) re-tune the translation system on day 1 via minimum error rate training (MERT) (<REF ID="R-08" RPTR="8">Venugopal and Vogel, 2005</REF><REF ID="R-09" RPTR="11">Vogel, 2005</REF>).</S>
      </P>
      <P>
        <S ID="S-23221">Use Day 1 Day 2</S>
      </P>
      <P>
        <S ID="S-23222">Baseline 29.39 27.41</S>
      </P>
      <P>
        <S ID="S-23223">The first question we would like to address is whether our adaptation obtains improvements via an unsupervised manner.</S>
        <S ID="S-23224">We take day 1 baseline ASR hypothesis and use the baseline SMT to get the MT hypothesis and a 500- best list.</S>
        <S ID="S-23225">We train a domain LM using the 500-best list and use the MT hypotheses as the reference in MERT.</S>
        <S ID="S-23226">We treat day 1 as a development set and day 2 as an unseen test set.</S>
        <S ID="S-23227">In Table 3 we compare the performance of four systems: the baseline which does not have any adaptation steps; and 3 adapted systems using unigram, bigram and trigram LMs build from 500-best MT hypotheses.</S>
      </P>
      <P>
        <S ID="S-23228">Use Day 1 Day 2</S>
      </P>
      <P>
        <S ID="S-23229">MT Ref 1gramLM MT Ref 30.53 28.35</S>
      </P>
      <P>
        <S ID="S-23230">Experimental results from unsupervised adaptation did not show consistent improvements but suggest we may obtain gains via supervised adaptation.</S>
        <S ID="S-23231">In supervised adaptation, we assume we have day 1 translation references.</S>
        <S ID="S-23232">The references are used in MERT.</S>
        <S ID="S-23233">In Table 4 we show performances of two additional systems which are the baseline system without adaptation but tuned toward day 1, and the adapted system which used day 1 translation references to train a unigram LM (1gramLM MT Ref).</S>
        <S ID="S-23234">The unigram and bigram LMs from 500-best and unigram LM from MT day 1 references perform relatively similar on day 2.</S>
        <S ID="S-23235">Using a trigram 500-best LM returned a large degradation and this LM is sensitive to the translation errors on day1</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Joint Adaptation</HEADER>
      <P>
        <S ID="S-23236">In Sections 3 and 4 we saw that individual adaptation helps ASR to reduce WER and SMT to increase BLEU</S>
      </P>
      <P>
        <S ID="S-23237">ASR SMT Day 1 Day 2</S>
      </P>
      <P>
        <S ID="S-23238">No adaptation No adaptation 29.39 27.41</S>
      </P>
      <P>
        <S ID="S-23239">score.</S>
        <S ID="S-23240">The next step in validating the adaptation framework was to check if the joint adaptation of ASR and SMT on day 1 data will lead to improvements on day 2.</S>
        <S ID="S-23241">Table 5 shows the combination of ASR and SMT adaptation methods.</S>
        <S ID="S-23242">Improvements are obtained by using both ASR and SMT adaptation.</S>
        <S ID="S-23243">Joint adaptation consistently gained more than one BLEU point improvement on day 2.</S>
        <S ID="S-23244">Our best system is unsupervised ASR adaptation via 1gramLM of ASR day 1 transcription coupled with supervised SMT adaptation via 1gramLM of day 1 translation references.</S>
        <S ID="S-23245">An interesting result is that to have a better result on day 2 our approach only requires translation references on day 1.</S>
        <S ID="S-23246">We selected 1gramLM of 500-best MT hypotheses to conduct the experiments since there is no significant difference between 1gramLM and 2gramLM on day 2 as showed in Table 3.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>6 Selective Adaptation</HEADER>
      <P>
        <S ID="S-23247">The previous results indicate that we require human translation references on day 1 data to get improved performance on day 2.</S>
        <S ID="S-23248">However, our goal is to make a better system on day 2 but try to minimize human efforts on day 1.</S>
        <S ID="S-23249">Therefore, we raise two questions: 1) Can we still obtain improvements by not using all of day 1 data?</S>
        <S ID="S-23250">and 2) Can we obtain more improvements?</S>
      </P>
      <P>
        <S ID="S-23251">To answer these questions we performed oracle experiments when we take the translation hypotheses on day 1 of the baseline SMT and compare them with translation references, then select sentences which have BLEU scores higher than a threshold.</S>
        <S ID="S-23252">The subset of day 1 sentences is used to perform supervised adaptation in a similar way showed in section 5.</S>
        <S ID="S-23253">These experiments also simulate the situation when we have a perfect confidence score for machine translation hypothesis selection.</S>
        <S ID="S-23254">Table 6 shows results when we use various portions of day 1 to perform adaptation.</S>
        <S ID="S-23255">By using day 1 sentences which have smoothed sentence BLEU scores higher than 10 or 20 we have very close performance with adaptation by using all day 1 data.</S>
        <S ID="S-23256">The results also show that by using 416 sentences which have sentence BLEU score higher than 40 on day 1, our adapted translation components outperform the baseline.</S>
        <S ID="S-23257">Performance starts degrading after 50.</S>
        <S ID="S-23258">Experimental results lead to the answer for question 1) that</S>
      </P>
      <P>
        <S ID="S-23259">EU</S>
      </P>
      <P>
        <S ID="S-23260">by using less day 1 data our adapted translation components still obtain improvements compare with the baseline, and 2) we did not see that using less data will lead us to a better performance compare with using all day 1 data.</S>
      </P>
      <P>
        <S ID="S-23261">No.</S>
        <S ID="S-23262">sents Day 1 Day 2</S>
      </P>
      <P>
        <S ID="S-23263">Baseline 29.39 27.41</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>7 Conclusions</HEADER>
      <P>
        <S ID="S-23264">This work clearly shows that improvement is possible using collected data for adaptation.</S>
        <S ID="S-23265">The overall picture is shown in Figure 2.</S>
        <S ID="S-23266">However this result is only based on one such data set, it would be useful to do such adaptation over multiple days.</S>
        <S ID="S-23267">The best results however still require producing translation references, notably ASR transcriptions do not seem to help, but may still be required in the process of generating translation references.</S>
        <S ID="S-23268">We wish to further investigate automatic adaptation based on implicit confidence scores, or even active participation of the user e.g. by marking bad utterance which could be excluded from the adaptation.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>Acknowledgments References</HEADER>
      <P>
        <S ID="S-23269">expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.</S>
        <S ID="S-23270">We would also like to thank Cepstral LLC and Mobile Technologies LLC, for support of some of the lower level software components.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS/>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>Nguyen Bach</RAUTHOR>
      <REFTITLE>Eyes-free and Hands-free Two-way Speech-to-Speech Translation System.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>Ivan Bulyko</RAUTHOR>
      <REFTITLE>Language Model Adaptation in Machine Translation from Speech.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>Boxing Chen</RAUTHOR>
      <REFTITLE>Exploiting n-best hypotheses for smt self-enhancement.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>Matthias Eck</RAUTHOR>
      <REFTITLE>Language model adaptation for statistical machine translation based on information retrieval.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>Roger Hsiao</RAUTHOR>
      <REFTITLE>Optimizing Components for Handheld Two-way Speech Translation for an English-Iraqi Arabic System. In</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>Kishore Papineni</RAUTHOR>
      <REFTITLE>BLEU: A method for automatic evaluation of machine translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>Daniel Povey</RAUTHOR>
      <REFTITLE>Boosted MMI for model and feature-space discriminative training.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>Andreas Stolcke</RAUTHOR>
      <REFTITLE>SRILM &#8211; An extensible language modeling toolkit.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>Ashish Venugopal</RAUTHOR>
      <REFTITLE>Considerations in maximum mutual information and minimum classification error training for statistical machine translation.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>Stephan Vogel</RAUTHOR>
      <REFTITLE>Pesa: Phrase pair extraction as sentence splitting.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
