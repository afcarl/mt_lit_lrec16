<document>
  <filename>P10-1049</filename>
  <authors/>
  <title>None</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>1 Introduction</title>
        <text>A phrase-based SMT system takes a source sentence and produces a translation by segmenting the sentence into phrases and translating those phrases separately (Koehn et al., 2003). The phrase translation table, which contains the bilingual phrase pairs and the corresponding translation probabilities, is one of the main components of an SMT system. The most common method for obtaining the phrase table is heuristic extraction from automatically word-aligned bilingual training data (Och et al., 1999). In this method, all phrases of the sentence pair that match constraints given by the alignment are extracted. This includes overlapping phrases. At extraction time it does not matter, whether the phrases are extracted from a highly probable phrase alignment or from an unlikely one.
Phrase model probabilities are typically defined as relative frequencies of phrases extracted from word-aligned parallel training data. The joint counts C( &#732;f, &#7869;) of the source phrase &#732;f and the target phrase &#7869; in the entire training data are normalized by the marginal counts of source and target phrase to obtain a conditional probability
p H ( &#732;f|&#7869;) = C( &#732;f, &#7869;) C(&#7869;) . (1)
The translation process is implemented as a weighted log-linear combination of several models h m (e I 1 , sK 1 , f 1 J ) including the logarithm of the phrase probability in source-to-target as well as in target-to-source direction. The phrase model is combined with a language model, word lexicon models, word and phrase penalty, and many others. (Och and Ney, 2004) The best translation &#234;&#206;1 as defined by the models then can be written as { M } &#8721;
&#234;&#206;1 = argmax &#955; m h m (e I 1, s K 1 , f1 J ) (2) I,e I 1 m=1
In this work, we propose to directly train our phrase models by applying a forced alignment procedure where we use the decoder to find a phrase alignment between source and target sentences of the training data and then updating phrase translation probabilities based on this alignment. In contrast to heuristic extraction, the proposed method provides a way of consistently training and using phrase models in translation. We use a modified version of a phrase-based decoder to perform the forced alignment. This way we ensure that all models used in training are identical to the ones used at decoding time. An illustration of the basic
idea can be seen in Figure 1. In the literature this method by itself has been shown to be problematic because it suffers from over-fitting (DeNero et al., 2006), (Liang et al., 2006). Since our initial phrases are extracted from the same training data, that we want to align, very long phrases can be found for segmentation. As these long phrases tend to occur in only a few training sentences, the EM algorithm generally overestimates their probability and neglects shorter phrases, which better generalize to unseen data and thus are more useful for translation. In order to counteract these effects, our training procedure applies leaving-one-out on the sentence level. Our results show, that this leads to a better translation quality.
Ideally, we would produce all possible segmentations and alignments during training. However, this has been shown to be infeasible for real-world data (DeNero and Klein, 2008). As training uses a modified version of the translation decoder, it is straightforward to apply pruning as in regular decoding. Additionally, we consider three ways of approximating the full search space:
1. the single-best Viterbi alignment,
2. the n-best alignments,
3. all alignments remaining in the search space after pruning.
The performance of the different approaches is measured and compared on the German-English Europarl task from the ACL 2008 Workshop on Statistical Machine Translation (WMT08). Our results show that the proposed phrase model training improves translation quality on the test set by 0.9 BLEU points over our baseline. We find that by interpolation with the heuristically extracted phrases translation performance can reach up to 1.4 BLEU improvement over the baseline on the test set.
After reviewing the related work in the following section, we give a detailed description of phrasal alignment and leaving-one-out in Section 3. Section 4 explains the estimation of phrase models. The empirical evaluation of the different approaches is done in Section 5.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>A phrase-based SMT system takes a source sentence and produces a translation by segmenting the sentence into phrases and translating those phrases separately (Koehn et al., 2003).</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The phrase translation table, which contains the bilingual phrase pairs and the corresponding translation probabilities, is one of the main components of an SMT system.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The most common method for obtaining the phrase table is heuristic extraction from automatically word-aligned bilingual training data (Och et al., 1999).</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In this method, all phrases of the sentence pair that match constraints given by the alignment are extracted.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This includes overlapping phrases.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>At extraction time it does not matter, whether the phrases are extracted from a highly probable phrase alignment or from an unlikely one.</text>
              <doc_id>5</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Phrase model probabilities are typically defined as relative frequencies of phrases extracted from word-aligned parallel training data.</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The joint counts C( &#732;f, &#7869;) of the source phrase &#732;f and the target phrase &#7869; in the entire training data are normalized by the marginal counts of source and target phrase to obtain a conditional probability</text>
              <doc_id>7</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>p H ( &#732;f|&#7869;) = C( &#732;f, &#7869;) C(&#7869;) .</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>(1)</text>
              <doc_id>9</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The translation process is implemented as a weighted log-linear combination of several models h m (e I 1 , sK 1 , f 1 J ) including the logarithm of the phrase probability in source-to-target as well as in target-to-source direction.</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The phrase model is combined with a language model, word lexicon models, word and phrase penalty, and many others.</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>(Och and Ney, 2004) The best translation &#234;&#206;1 as defined by the models then can be written as { M } &#8721;</text>
              <doc_id>12</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#234;&#206;1 = argmax &#955; m h m (e I 1, s K 1 , f1 J ) (2) I,e I 1 m=1</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this work, we propose to directly train our phrase models by applying a forced alignment procedure where we use the decoder to find a phrase alignment between source and target sentences of the training data and then updating phrase translation probabilities based on this alignment.</text>
              <doc_id>14</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In contrast to heuristic extraction, the proposed method provides a way of consistently training and using phrase models in translation.</text>
              <doc_id>15</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We use a modified version of a phrase-based decoder to perform the forced alignment.</text>
              <doc_id>16</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This way we ensure that all models used in training are identical to the ones used at decoding time.</text>
              <doc_id>17</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>An illustration of the basic</text>
              <doc_id>18</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>idea can be seen in Figure 1.</text>
              <doc_id>19</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In the literature this method by itself has been shown to be problematic because it suffers from over-fitting (DeNero et al., 2006), (Liang et al., 2006).</text>
              <doc_id>20</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Since our initial phrases are extracted from the same training data, that we want to align, very long phrases can be found for segmentation.</text>
              <doc_id>21</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As these long phrases tend to occur in only a few training sentences, the EM algorithm generally overestimates their probability and neglects shorter phrases, which better generalize to unseen data and thus are more useful for translation.</text>
              <doc_id>22</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In order to counteract these effects, our training procedure applies leaving-one-out on the sentence level.</text>
              <doc_id>23</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Our results show, that this leads to a better translation quality.</text>
              <doc_id>24</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Ideally, we would produce all possible segmentations and alignments during training.</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, this has been shown to be infeasible for real-world data (DeNero and Klein, 2008).</text>
              <doc_id>26</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>As training uses a modified version of the translation decoder, it is straightforward to apply pruning as in regular decoding.</text>
              <doc_id>27</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Additionally, we consider three ways of approximating the full search space:</text>
              <doc_id>28</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1. the single-best Viterbi alignment,</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2. the n-best alignments,</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3. all alignments remaining in the search space after pruning.</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The performance of the different approaches is measured and compared on the German-English Europarl task from the ACL 2008 Workshop on Statistical Machine Translation (WMT08).</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our results show that the proposed phrase model training improves translation quality on the test set by 0.9 BLEU points over our baseline.</text>
              <doc_id>33</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We find that by interpolation with the heuristically extracted phrases translation performance can reach up to 1.4 BLEU improvement over the baseline on the test set.</text>
              <doc_id>34</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>After reviewing the related work in the following section, we give a detailed description of phrasal alignment and leaving-one-out in Section 3.</text>
              <doc_id>35</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Section 4 explains the estimation of phrase models.</text>
              <doc_id>36</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The empirical evaluation of the different approaches is done in Section 5.</text>
              <doc_id>37</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>2 Related Work</title>
        <text>It has been pointed out in literature, that training phrase models poses some difficulties. For a generative model, (DeNero et al., 2006) gave a detailed analysis of the challenges and arising problems. They introduce a model similar to the one we propose in Section 4.2 and train it with the EM algorithm. Their results show that it can not reach a performance competitive to extracting a phrase table from word alignment by heuristics (Och et al., 1999).
Several reasons are revealed in (DeNero et al., 2006). When given a bilingual sentence pair, we can usually assume there are a number of equally correct phrase segmentations and corresponding alignments. For example, it may be possible to transform one valid segmentation into another by splitting some of its phrases into sub-phrases or by shifting phrase boundaries. This is different from word-based translation models, where a typical assumption is that each target word corresponds to only one source word. As a result of this ambiguity, different segmentations are recruited for different examples during training. That in turn leads to over-fitting which shows in overly determinized estimates of the phrase translation probabilities. In addition, (DeNero et al., 2006) found that the trained phrase table shows a highly peaked distribution in opposition to the more flat distribution resulting from heuristic extraction, leaving the decoder only few translation options at decoding time.
Our work differs from (DeNero et al., 2006) in a number of ways, addressing those problems.
To limit the effects of over-fitting, we apply the leaving-one-out and cross-validation methods in training. In addition, we do not restrict the training to phrases consistent with the word alignment, as was done in (DeNero et al., 2006). This allows us to recover from flawed word alignments.
In (Liang et al., 2006) a discriminative translation system is described. For training of the parameters for the discriminative features they propose a strategy they call bold updating. It is similar to our forced alignment training procedure described in Section 3.
For the hierarchical phrase-based approach, (Blunsom et al., 2008) present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations.
Forced alignment can also be utilized to train a phrase segmentation model, as is shown in (Shen et al., 2008). They report small but consistent improvements by incorporating this segmentation model, which works as an additional prior probability on the monolingual target phrase.
In (Ferrer and Juan, 2009), phrase models are trained by a semi-hidden Markov model. They train a conditional &#8220;inverse&#8221; phrase model of the target phrase given the source phrase. Additionally to the phrases, they model the segmentation sequence that is used to produce a phrase alignment between the source and the target sentence. They used a phrase length limit of 4 words with longer phrases not resulting in further improvements. To counteract over-fitting, they interpolate the phrase model with IBM Model 1 probabilities that are computed on the phrase level. We also include these word lexica, as they are standard components of the phrase-based system.
It is shown in (Ferrer and Juan, 2009), that Viterbi training produces almost the same results as full Baum-Welch training. They report improvements over a phrase-based model that uses an inverse phrase model and a language model. Experiments are carried out on a custom subset of the English-Spanish Europarl corpus.
Our approach is similar to the one presented in (Ferrer and Juan, 2009) in that we compare Viterbi and a training method based on the Forward- Backward algorithm. But instead of focusing on the statistical model and relaxing the translation task by using monotone translation only, we use a full and competitive translation system as starting point with reordering and all models included.
In (Marcu and Wong, 2002), a joint probability phrase model is presented. The learned phrases are restricted to the most frequent n-grams up to length 6 and all unigrams. Monolingual phrases have to occur at least 5 times to be considered in training. Smoothing is applied to the learned models so that probabilities for rare phrases are non-zero. In training, they use a greedy algorithm to produce the Viterbi phrase alignment and then apply a hill-climbing technique that modifies the Viterbi alignment by merge, move, split, and swap operations to find an alignment with a better probability in each iteration. The model shows improvements in translation quality over the singleword-based IBM Model 4 (Brown et al., 1993) on a subset of the Canadian Hansards corpus.
The joint model by (Marcu and Wong, 2002) is refined by (Birch et al., 2006) who use high-confidence word alignments to constrain the search space in training. They observe that due to several constraints and pruning steps, the trained phrase table is much smaller than the heuristically extracted one, while preserving translation quality.
The work by (DeNero et al., 2008) describes a method to train the joint model described in (Marcu and Wong, 2002) with a Gibbs sampler. They show that by applying a prior distribution over the phrase translation probabilities they can prevent over-fitting. The prior is composed of IBM1 lexical probabilities and a geometric distribution over phrase lengths which penalizes long phrases. The two approaches differ in that we apply the leaving-one-out procedure to avoid overfitting, as opposed to explicitly defining a prior distribution.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>It has been pointed out in literature, that training phrase models poses some difficulties.</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For a generative model, (DeNero et al., 2006) gave a detailed analysis of the challenges and arising problems.</text>
              <doc_id>39</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>They introduce a model similar to the one we propose in Section 4.2 and train it with the EM algorithm.</text>
              <doc_id>40</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Their results show that it can not reach a performance competitive to extracting a phrase table from word alignment by heuristics (Och et al., 1999).</text>
              <doc_id>41</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Several reasons are revealed in (DeNero et al., 2006).</text>
              <doc_id>42</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>When given a bilingual sentence pair, we can usually assume there are a number of equally correct phrase segmentations and corresponding alignments.</text>
              <doc_id>43</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For example, it may be possible to transform one valid segmentation into another by splitting some of its phrases into sub-phrases or by shifting phrase boundaries.</text>
              <doc_id>44</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This is different from word-based translation models, where a typical assumption is that each target word corresponds to only one source word.</text>
              <doc_id>45</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>As a result of this ambiguity, different segmentations are recruited for different examples during training.</text>
              <doc_id>46</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>That in turn leads to over-fitting which shows in overly determinized estimates of the phrase translation probabilities.</text>
              <doc_id>47</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In addition, (DeNero et al., 2006) found that the trained phrase table shows a highly peaked distribution in opposition to the more flat distribution resulting from heuristic extraction, leaving the decoder only few translation options at decoding time.</text>
              <doc_id>48</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Our work differs from (DeNero et al., 2006) in a number of ways, addressing those problems.</text>
              <doc_id>49</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To limit the effects of over-fitting, we apply the leaving-one-out and cross-validation methods in training.</text>
              <doc_id>50</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In addition, we do not restrict the training to phrases consistent with the word alignment, as was done in (DeNero et al., 2006).</text>
              <doc_id>51</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This allows us to recover from flawed word alignments.</text>
              <doc_id>52</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In (Liang et al., 2006) a discriminative translation system is described.</text>
              <doc_id>53</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For training of the parameters for the discriminative features they propose a strategy they call bold updating.</text>
              <doc_id>54</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>It is similar to our forced alignment training procedure described in Section 3.</text>
              <doc_id>55</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For the hierarchical phrase-based approach, (Blunsom et al., 2008) present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations.</text>
              <doc_id>56</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Forced alignment can also be utilized to train a phrase segmentation model, as is shown in (Shen et al., 2008).</text>
              <doc_id>57</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>They report small but consistent improvements by incorporating this segmentation model, which works as an additional prior probability on the monolingual target phrase.</text>
              <doc_id>58</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In (Ferrer and Juan, 2009), phrase models are trained by a semi-hidden Markov model.</text>
              <doc_id>59</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>They train a conditional &#8220;inverse&#8221; phrase model of the target phrase given the source phrase.</text>
              <doc_id>60</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Additionally to the phrases, they model the segmentation sequence that is used to produce a phrase alignment between the source and the target sentence.</text>
              <doc_id>61</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>They used a phrase length limit of 4 words with longer phrases not resulting in further improvements.</text>
              <doc_id>62</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>To counteract over-fitting, they interpolate the phrase model with IBM Model 1 probabilities that are computed on the phrase level.</text>
              <doc_id>63</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We also include these word lexica, as they are standard components of the phrase-based system.</text>
              <doc_id>64</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>It is shown in (Ferrer and Juan, 2009), that Viterbi training produces almost the same results as full Baum-Welch training.</text>
              <doc_id>65</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>They report improvements over a phrase-based model that uses an inverse phrase model and a language model.</text>
              <doc_id>66</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Experiments are carried out on a custom subset of the English-Spanish Europarl corpus.</text>
              <doc_id>67</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Our approach is similar to the one presented in (Ferrer and Juan, 2009) in that we compare Viterbi and a training method based on the Forward- Backward algorithm.</text>
              <doc_id>68</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>But instead of focusing on the statistical model and relaxing the translation task by using monotone translation only, we use a full and competitive translation system as starting point with reordering and all models included.</text>
              <doc_id>69</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In (Marcu and Wong, 2002), a joint probability phrase model is presented.</text>
              <doc_id>70</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The learned phrases are restricted to the most frequent n-grams up to length 6 and all unigrams.</text>
              <doc_id>71</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Monolingual phrases have to occur at least 5 times to be considered in training.</text>
              <doc_id>72</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Smoothing is applied to the learned models so that probabilities for rare phrases are non-zero.</text>
              <doc_id>73</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In training, they use a greedy algorithm to produce the Viterbi phrase alignment and then apply a hill-climbing technique that modifies the Viterbi alignment by merge, move, split, and swap operations to find an alignment with a better probability in each iteration.</text>
              <doc_id>74</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The model shows improvements in translation quality over the singleword-based IBM Model 4 (Brown et al., 1993) on a subset of the Canadian Hansards corpus.</text>
              <doc_id>75</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The joint model by (Marcu and Wong, 2002) is refined by (Birch et al., 2006) who use high-confidence word alignments to constrain the search space in training.</text>
              <doc_id>76</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>They observe that due to several constraints and pruning steps, the trained phrase table is much smaller than the heuristically extracted one, while preserving translation quality.</text>
              <doc_id>77</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The work by (DeNero et al., 2008) describes a method to train the joint model described in (Marcu and Wong, 2002) with a Gibbs sampler.</text>
              <doc_id>78</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>They show that by applying a prior distribution over the phrase translation probabilities they can prevent over-fitting.</text>
              <doc_id>79</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The prior is composed of IBM1 lexical probabilities and a geometric distribution over phrase lengths which penalizes long phrases.</text>
              <doc_id>80</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The two approaches differ in that we apply the leaving-one-out procedure to avoid overfitting, as opposed to explicitly defining a prior distribution.</text>
              <doc_id>81</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>3 Alignment</title>
        <text>The training process is divided into three parts. First we obtain all models needed for a normal translations system. We perform minimum error rate training with the downhill simplex algorithm (Nelder and Mead, 1965) on the development data to obtain a set of scaling factors that achieve a good BLEU score. We then use these models and scaling factors to do a forced alignment, where we compute a phrase alignment for the training data. From this alignment we then estimate new phrase models, while keeping all other models un-
changed. In this section we describe our forced alignment procedure that is the basic training procedure for the models proposed here.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The training process is divided into three parts.</text>
              <doc_id>82</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>First we obtain all models needed for a normal translations system.</text>
              <doc_id>83</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We perform minimum error rate training with the downhill simplex algorithm (Nelder and Mead, 1965) on the development data to obtain a set of scaling factors that achieve a good BLEU score.</text>
              <doc_id>84</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We then use these models and scaling factors to do a forced alignment, where we compute a phrase alignment for the training data.</text>
              <doc_id>85</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>From this alignment we then estimate new phrase models, while keeping all other models un-</text>
              <doc_id>86</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>changed.</text>
              <doc_id>87</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this section we describe our forced alignment procedure that is the basic training procedure for the models proposed here.</text>
              <doc_id>88</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Forced Alignment</title>
            <text>The idea of forced alignment is to perform a phrase segmentation and alignment of each sentence pair of the training data using the full translation system as in decoding. What we call segmentation and alignment here corresponds to the &#8220;concepts&#8221; used by (Marcu and Wong, 2002). We apply our normal phrase-based decoder on the source side of the training data and constrain the translations to the corresponding target sentences from the training data.
Given a source sentence f1 J and target sentence e I 1 , we search for the best phrase segmentation and alignment that covers both sentences. A segmentation of a sentence into K phrase is defined by
k &#8594; s k := (i k , b k , j k ), for k = 1, . . . , K
where for each segment i k is last position of kth target phrase, and (b k , j k ) are the start and end positions of the source phrase aligned to the kth target phrase. Consequently, we can modify Equation 2 to define the best segmentation of a sentence pair as: { M }
&#349; &#710;K &#8721;
1 = argmax &#955; m h m (e I 1, s K 1 , f1 J ) (3) K,s K 1 m=1
The identical models as in search are used: conditional phrase probabilities p( &#732;f k |&#7869; k ) and p(&#7869; k | &#732;f k ), within-phrase lexical probabilities, distance-based reordering model as well as word and phrase penalty. A language model is not used in this case, as the system is constrained to the given target sentence and thus the language model score has no effect on the alignment.
In addition to the phrase matching on the source sentence, we also discard all phrase translation candidates, that do not match any sequence in the given target sentence.
Sentences for which the decoder can not find an alignment are discarded for the phrase model training. In our experiments, this is the case for roughly 5% of the training sentences.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The idea of forced alignment is to perform a phrase segmentation and alignment of each sentence pair of the training data using the full translation system as in decoding.</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>What we call segmentation and alignment here corresponds to the &#8220;concepts&#8221; used by (Marcu and Wong, 2002).</text>
                  <doc_id>90</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We apply our normal phrase-based decoder on the source side of the training data and constrain the translations to the corresponding target sentences from the training data.</text>
                  <doc_id>91</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Given a source sentence f1 J and target sentence e I 1 , we search for the best phrase segmentation and alignment that covers both sentences.</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A segmentation of a sentence into K phrase is defined by</text>
                  <doc_id>93</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k &#8594; s k := (i k , b k , j k ), for k = 1, .</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>95</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>96</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>, K</text>
                  <doc_id>97</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where for each segment i k is last position of kth target phrase, and (b k , j k ) are the start and end positions of the source phrase aligned to the kth target phrase.</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Consequently, we can modify Equation 2 to define the best segmentation of a sentence pair as: { M }</text>
                  <doc_id>99</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#349; &#710;K &#8721;</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 = argmax &#955; m h m (e I 1, s K 1 , f1 J ) (3) K,s K 1 m=1</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The identical models as in search are used: conditional phrase probabilities p( &#732;f k |&#7869; k ) and p(&#7869; k | &#732;f k ), within-phrase lexical probabilities, distance-based reordering model as well as word and phrase penalty.</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A language model is not used in this case, as the system is constrained to the given target sentence and thus the language model score has no effect on the alignment.</text>
                  <doc_id>103</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In addition to the phrase matching on the source sentence, we also discard all phrase translation candidates, that do not match any sequence in the given target sentence.</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Sentences for which the decoder can not find an alignment are discarded for the phrase model training.</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In our experiments, this is the case for roughly 5% of the training sentences.</text>
                  <doc_id>106</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Leaving-one-out</title>
            <text>As was mentioned in Section 2, previous approaches found over-fitting to be a problem in phrase model training. In this section, we describe a leaving-one-out method that can improve the phrase alignment in situations, where the probability of rare phrases and alignments might be overestimated. The training data that consists of N parallel sentence pairs f n and e n for n = 1, . . . , N is used for both the initialization of the translation model p( &#732;f|&#7869;) and the phrase model training. While this way we can make full use of the available data and avoid unknown words during training, it has the drawback that it can lead to overfitting. All phrases extracted from a specific sentence pair f n , e n can be used for the alignment of this sentence pair. This includes longer phrases, which only match in very few sentences in the data. Therefore those long phrases are trained to fit only a few sentence pairs, strongly overestimating their translation probabilities and failing to generalize. In the extreme case, whole sentences will be learned as phrasal translations. The average length of the used phrases is an indicator of this kind of over-fitting, as the number of matching training sentences decreases with increasing phrase length. We can see an example in Figure 2. Without leaving-one-out the sentence is segmented into a few long phrases, which are unlikely to occur in data to be translated. Phrase boundaries seem to be unintuitive and based on some hidden structures. With leaving-one-out the phrases are shorter and therefore better suited for generalization to unseen data.
Previous attempts have dealt with the overfitting problem by limiting the maximum phrase length (DeNero et al., 2006; Marcu and Wong, 2002) and by smoothing the phrase probabilities by lexical models on the phrase level (Ferrer and Juan, 2009). However, (DeNero et al., 2006) experienced similar over-fitting with short phrases due to the fact that the same word sequence can be segmented in different ways, leading to specific segmentations being learned for specific training sentence pairs. Our results confirm these findings. To deal with this problem, instead of simple phrase length restriction, we propose to apply the leavingone-out method, which is also used for language modeling techniques (Kneser and Ney, 1995).
When using leaving-one-out, we modify the phrase translation probabilities for each sentence pair. For a training example f n , e n , we have to remove all phrases C n ( &#732;f, &#7869;) that were extracted from this sentence pair from the phrase counts that
we used to construct our phrase translation table. The same holds for the marginal counts C n (&#7869;) and C n ( &#732;f). Starting from Equation 1, the leaving-oneout phrase probability for training sentence pair n is
p l1o,n ( &#732;f|&#7869;) = C( &#732;f, &#7869;) &#8722; C n ( &#732;f, &#7869;) (4) C(&#7869;) &#8722; C n (&#7869;)
To be able to perform the re-computation in an efficient way, we store the source and target phrase marginal counts for each phrase in the phrase table. A phrase extraction is performed for each training sentence pair separately using the same word alignment as for the initialization. It is then straightforward to compute the phrase counts after leaving-one-out using the phrase probabilities and marginal counts stored in the phrase table.
While this works well for more frequent observations, singleton phrases are assigned a probability of zero. We refer to singleton phrases as phrase pairs that occur only in one sentence. For these sentences, the decoder needs the singleton phrase pairs to produce an alignment. Therefore we retain those phrases by assigning them a positive probability close to zero. We evaluated with two different strategies for this, which we call standard and length-based leaving-one-out. Standard leavingone-out assigns a fixed probability &#945; to singleton phrase pairs. This way the decoder will prefer using more frequent phrases for the alignment, but is able to resort to singletons if necessary. However, we found that with this method longer singleton phrases are preferred over shorter ones, because fewer of them are needed to produce the target sentence. In order to better generalize to unseen data, we would like to give the preference to shorter phrases. This is done by length-based leavingone-out, where singleton phrases are assigned the probability &#946; (| &#732;f|+|&#7869;|) with the source and target Table 1: Avg. source phrase lengths in forced alignment without leaving-one-out and with standard and length-based leaving-one-out.
phrase lengths | &#732;f| and |&#7869;| and fixed &#946; &lt; 1. In our experiments we set &#945; = e &#8722;20 and &#946; = e &#8722;5 . Table 1 shows the decrease in average source phrase length by application of leaving-one-out.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>As was mentioned in Section 2, previous approaches found over-fitting to be a problem in phrase model training.</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this section, we describe a leaving-one-out method that can improve the phrase alignment in situations, where the probability of rare phrases and alignments might be overestimated.</text>
                  <doc_id>108</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The training data that consists of N parallel sentence pairs f n and e n for n = 1, .</text>
                  <doc_id>109</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>110</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>111</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>, N is used for both the initialization of the translation model p( &#732;f|&#7869;) and the phrase model training.</text>
                  <doc_id>112</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>While this way we can make full use of the available data and avoid unknown words during training, it has the drawback that it can lead to overfitting.</text>
                  <doc_id>113</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>All phrases extracted from a specific sentence pair f n , e n can be used for the alignment of this sentence pair.</text>
                  <doc_id>114</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>This includes longer phrases, which only match in very few sentences in the data.</text>
                  <doc_id>115</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore those long phrases are trained to fit only a few sentence pairs, strongly overestimating their translation probabilities and failing to generalize.</text>
                  <doc_id>116</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>In the extreme case, whole sentences will be learned as phrasal translations.</text>
                  <doc_id>117</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>The average length of the used phrases is an indicator of this kind of over-fitting, as the number of matching training sentences decreases with increasing phrase length.</text>
                  <doc_id>118</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
                <sentence>
                  <text>We can see an example in Figure 2.</text>
                  <doc_id>119</doc_id>
                  <sec_id>12</sec_id>
                </sentence>
                <sentence>
                  <text>Without leaving-one-out the sentence is segmented into a few long phrases, which are unlikely to occur in data to be translated.</text>
                  <doc_id>120</doc_id>
                  <sec_id>13</sec_id>
                </sentence>
                <sentence>
                  <text>Phrase boundaries seem to be unintuitive and based on some hidden structures.</text>
                  <doc_id>121</doc_id>
                  <sec_id>14</sec_id>
                </sentence>
                <sentence>
                  <text>With leaving-one-out the phrases are shorter and therefore better suited for generalization to unseen data.</text>
                  <doc_id>122</doc_id>
                  <sec_id>15</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Previous attempts have dealt with the overfitting problem by limiting the maximum phrase length (DeNero et al., 2006; Marcu and Wong, 2002) and by smoothing the phrase probabilities by lexical models on the phrase level (Ferrer and Juan, 2009).</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, (DeNero et al., 2006) experienced similar over-fitting with short phrases due to the fact that the same word sequence can be segmented in different ways, leading to specific segmentations being learned for specific training sentence pairs.</text>
                  <doc_id>124</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Our results confirm these findings.</text>
                  <doc_id>125</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>To deal with this problem, instead of simple phrase length restriction, we propose to apply the leavingone-out method, which is also used for language modeling techniques (Kneser and Ney, 1995).</text>
                  <doc_id>126</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>When using leaving-one-out, we modify the phrase translation probabilities for each sentence pair.</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For a training example f n , e n , we have to remove all phrases C n ( &#732;f, &#7869;) that were extracted from this sentence pair from the phrase counts that</text>
                  <doc_id>128</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>we used to construct our phrase translation table.</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The same holds for the marginal counts C n (&#7869;) and C n ( &#732;f).</text>
                  <doc_id>130</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Starting from Equation 1, the leaving-oneout phrase probability for training sentence pair n is</text>
                  <doc_id>131</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p l1o,n ( &#732;f|&#7869;) = C( &#732;f, &#7869;) &#8722; C n ( &#732;f, &#7869;) (4) C(&#7869;) &#8722; C n (&#7869;)</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To be able to perform the re-computation in an efficient way, we store the source and target phrase marginal counts for each phrase in the phrase table.</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A phrase extraction is performed for each training sentence pair separately using the same word alignment as for the initialization.</text>
                  <doc_id>134</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It is then straightforward to compute the phrase counts after leaving-one-out using the phrase probabilities and marginal counts stored in the phrase table.</text>
                  <doc_id>135</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>While this works well for more frequent observations, singleton phrases are assigned a probability of zero.</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We refer to singleton phrases as phrase pairs that occur only in one sentence.</text>
                  <doc_id>137</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For these sentences, the decoder needs the singleton phrase pairs to produce an alignment.</text>
                  <doc_id>138</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore we retain those phrases by assigning them a positive probability close to zero.</text>
                  <doc_id>139</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We evaluated with two different strategies for this, which we call standard and length-based leaving-one-out.</text>
                  <doc_id>140</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Standard leavingone-out assigns a fixed probability &#945; to singleton phrase pairs.</text>
                  <doc_id>141</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>This way the decoder will prefer using more frequent phrases for the alignment, but is able to resort to singletons if necessary.</text>
                  <doc_id>142</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>However, we found that with this method longer singleton phrases are preferred over shorter ones, because fewer of them are needed to produce the target sentence.</text>
                  <doc_id>143</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>In order to better generalize to unseen data, we would like to give the preference to shorter phrases.</text>
                  <doc_id>144</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>This is done by length-based leavingone-out, where singleton phrases are assigned the probability &#946; (| &#732;f|+|&#7869;|) with the source and target Table 1: Avg.</text>
                  <doc_id>145</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>source phrase lengths in forced alignment without leaving-one-out and with standard and length-based leaving-one-out.</text>
                  <doc_id>146</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>phrase lengths | &#732;f| and |&#7869;| and fixed &#946; &lt; 1.</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In our experiments we set &#945; = e &#8722;20 and &#946; = e &#8722;5 .</text>
                  <doc_id>148</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Table 1 shows the decrease in average source phrase length by application of leaving-one-out.</text>
                  <doc_id>149</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Cross-validation</title>
            <text>For the first iteration of the phrase training, leaving-one-out can be implemented efficiently as described in Section 3.2. For higher iterations, phrase counts obtained in the previous iterations would have to be stored on disk separately for each sentence and accessed during the forced alignment process. To simplify this procedure, we propose a cross-validation strategy on larger batches of data. Instead of recomputing the phrase counts for each sentence individually, this is done for a whole batch of sentences at a time. In our experiments, we set this batch-size to 10000 sentences.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>For the first iteration of the phrase training, leaving-one-out can be implemented efficiently as described in Section 3.2.</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For higher iterations, phrase counts obtained in the previous iterations would have to be stored on disk separately for each sentence and accessed during the forced alignment process.</text>
                  <doc_id>151</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>To simplify this procedure, we propose a cross-validation strategy on larger batches of data.</text>
                  <doc_id>152</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Instead of recomputing the phrase counts for each sentence individually, this is done for a whole batch of sentences at a time.</text>
                  <doc_id>153</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In our experiments, we set this batch-size to 10000 sentences.</text>
                  <doc_id>154</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>3.4 Parallelization</title>
            <text>To cope with the runtime and memory requirements of phrase model training that was pointed out by previous work (Marcu and Wong, 2002; Birch et al., 2006), we parallelized the forced alignment by splitting the training corpus into blocks of 10k sentence pairs. From the initial phrase table, each of these blocks only loads the phrases that are required for alignment. The align-
ment and the counting of phrases are done separately for each block and then accumulated to build the updated phrase model.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>To cope with the runtime and memory requirements of phrase model training that was pointed out by previous work (Marcu and Wong, 2002; Birch et al., 2006), we parallelized the forced alignment by splitting the training corpus into blocks of 10k sentence pairs.</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>From the initial phrase table, each of these blocks only loads the phrases that are required for alignment.</text>
                  <doc_id>156</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The align-</text>
                  <doc_id>157</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>ment and the counting of phrases are done separately for each block and then accumulated to build the updated phrase model.</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>4 Phrase Model Training</title>
        <text>The produced phrase alignment can be given as a single best alignment, as the n-best alignments or as an alignment graph representing all alignments considered by the decoder. We have developed two different models for phrase translation probabilities which make use of the force-aligned training data. Additionally we consider smoothing by different kinds of interpolation of the generative model with the state-of-the-art heuristics.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The produced phrase alignment can be given as a single best alignment, as the n-best alignments or as an alignment graph representing all alignments considered by the decoder.</text>
              <doc_id>159</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We have developed two different models for phrase translation probabilities which make use of the force-aligned training data.</text>
              <doc_id>160</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Additionally we consider smoothing by different kinds of interpolation of the generative model with the state-of-the-art heuristics.</text>
              <doc_id>161</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Viterbi</title>
            <text>The simplest of our generative phrase models estimates phrase translation probabilities by their relative frequencies in the Viterbi alignment of the data, similar to the heuristic model but with counts from the phrase-aligned data produced in training rather than computed on the basis of a word alignment. The translation probability of a phrase pair ( &#732;f, &#7869;) is estimated as
p F A ( &#732;f|&#7869;) = C F A( &#732;f, &#7869;) &#8721;
C F A ( &#732;f &#8242; , &#7869;)
&#732;f &#8242;
(5)
where C F A ( &#732;f, &#7869;) is the count of the phrase pair ( &#732;f, &#7869;) in the phrase-aligned training data. This can be applied to either the Viterbi phrase alignment or an n-best list. For the simplest model, each hypothesis in the n-best list is weighted equally. We will refer to this model as the count model as we simply count the number of occurrences of a phrase pair. We also experimented with weighting the counts with the estimated likelihood of the corresponding entry in the the n-best list. The sum of the likelihoods of all entries in an n-best list is normalized to 1. We will refer to this model as the weighted count model.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The simplest of our generative phrase models estimates phrase translation probabilities by their relative frequencies in the Viterbi alignment of the data, similar to the heuristic model but with counts from the phrase-aligned data produced in training rather than computed on the basis of a word alignment.</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The translation probability of a phrase pair ( &#732;f, &#7869;) is estimated as</text>
                  <doc_id>163</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p F A ( &#732;f|&#7869;) = C F A( &#732;f, &#7869;) &#8721;</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>C F A ( &#732;f &#8242; , &#7869;)</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#732;f &#8242;</text>
                  <doc_id>166</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(5)</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where C F A ( &#732;f, &#7869;) is the count of the phrase pair ( &#732;f, &#7869;) in the phrase-aligned training data.</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This can be applied to either the Viterbi phrase alignment or an n-best list.</text>
                  <doc_id>169</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For the simplest model, each hypothesis in the n-best list is weighted equally.</text>
                  <doc_id>170</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We will refer to this model as the count model as we simply count the number of occurrences of a phrase pair.</text>
                  <doc_id>171</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We also experimented with weighting the counts with the estimated likelihood of the corresponding entry in the the n-best list.</text>
                  <doc_id>172</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The sum of the likelihoods of all entries in an n-best list is normalized to 1.</text>
                  <doc_id>173</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>We will refer to this model as the weighted count model.</text>
                  <doc_id>174</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Forward-backward</title>
            <text>Ideally, the training procedure would consider all possible alignment and segmentation hypotheses. When alternatives are weighted by their posterior probability. As discussed earlier, the run-time requirements for computing all possible alignments is prohibitive for large data tasks. However, we can approximate the space of all possible hypotheses by the search space that was used for the alignment. While this might not cover all phrase translation probabilities, it allows the search space and translation times to be feasible and still contains the most probable alignments. This search space can be represented as a graph of partial hypotheses (Ueffing et al., 2002) on which we can compute expectations using the Forward-Backward algorithm. We will refer to this alignment as the full alignment. In contrast to the method described in Section 4.1, phrases are weighted by their posterior probability in the word graph. As suggested in work on minimum Bayes-risk decoding for SMT (Tromble et al., 2008; Ehling et al., 2007), we use a global factor to scale the posterior probabilities.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Ideally, the training procedure would consider all possible alignment and segmentation hypotheses.</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>When alternatives are weighted by their posterior probability.</text>
                  <doc_id>176</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As discussed earlier, the run-time requirements for computing all possible alignments is prohibitive for large data tasks.</text>
                  <doc_id>177</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>However, we can approximate the space of all possible hypotheses by the search space that was used for the alignment.</text>
                  <doc_id>178</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>While this might not cover all phrase translation probabilities, it allows the search space and translation times to be feasible and still contains the most probable alignments.</text>
                  <doc_id>179</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This search space can be represented as a graph of partial hypotheses (Ueffing et al., 2002) on which we can compute expectations using the Forward-Backward algorithm.</text>
                  <doc_id>180</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>We will refer to this alignment as the full alignment.</text>
                  <doc_id>181</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast to the method described in Section 4.1, phrases are weighted by their posterior probability in the word graph.</text>
                  <doc_id>182</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>As suggested in work on minimum Bayes-risk decoding for SMT (Tromble et al., 2008; Ehling et al., 2007), we use a global factor to scale the posterior probabilities.</text>
                  <doc_id>183</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>4.3 Phrase Table Interpolation</title>
            <text>As (DeNero et al., 2006) have reported improvements in translation quality by interpolation of phrase tables produced by the generative and the heuristic model, we adopt this method and also report results using log-linear interpolation of the estimated model with the original model.
The log-linear interpolations p int ( &#732;f|&#7869;) of the phrase translation probabilities are estimated as
p int ( &#732;f|&#7869;) = ( &#732;f|&#7869;)) 1&#8722;&#969; ( p H ( &#183; p gen ( &#732;f|&#7869;) ) (&#969;)
(6)
where &#969; is the interpolation weight, p H the heuristically estimated phrase model and p gen the count model. The interpolation weight &#969; is adjusted on the development corpus. When interpolating phrase tables containing different sets of phrase pairs, we retain the intersection of the two.
As a generalization of the fixed interpolation of the two phrase tables we also experimented with adding the two trained phrase probabilities as additional features to the log-linear framework. This way we allow different interpolation weights for the two translation directions and can optimize them automatically along with the other feature weights. We will refer to this method as featurewise combination. Again, we retain the intersection of the two phrase tables. With good loglinear feature weights, feature-wise combination should perform at least as well as fixed interpolation. However, the results presented in Table 5
German English
show a slightly lower performance. This illustrates that a higher number of features results in a less reliable optimization of the log-linear parameters.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>As (DeNero et al., 2006) have reported improvements in translation quality by interpolation of phrase tables produced by the generative and the heuristic model, we adopt this method and also report results using log-linear interpolation of the estimated model with the original model.</text>
                  <doc_id>184</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The log-linear interpolations p int ( &#732;f|&#7869;) of the phrase translation probabilities are estimated as</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p int ( &#732;f|&#7869;) = ( &#732;f|&#7869;)) 1&#8722;&#969; ( p H ( &#183; p gen ( &#732;f|&#7869;) ) (&#969;)</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(6)</text>
                  <doc_id>187</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where &#969; is the interpolation weight, p H the heuristically estimated phrase model and p gen the count model.</text>
                  <doc_id>188</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The interpolation weight &#969; is adjusted on the development corpus.</text>
                  <doc_id>189</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>When interpolating phrase tables containing different sets of phrase pairs, we retain the intersection of the two.</text>
                  <doc_id>190</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As a generalization of the fixed interpolation of the two phrase tables we also experimented with adding the two trained phrase probabilities as additional features to the log-linear framework.</text>
                  <doc_id>191</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This way we allow different interpolation weights for the two translation directions and can optimize them automatically along with the other feature weights.</text>
                  <doc_id>192</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We will refer to this method as featurewise combination.</text>
                  <doc_id>193</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Again, we retain the intersection of the two phrase tables.</text>
                  <doc_id>194</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>With good loglinear feature weights, feature-wise combination should perform at least as well as fixed interpolation.</text>
                  <doc_id>195</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>However, the results presented in Table 5</text>
                  <doc_id>196</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>German English</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>show a slightly lower performance.</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This illustrates that a higher number of features results in a less reliable optimization of the log-linear parameters.</text>
                  <doc_id>199</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>5 Experimental Evaluation</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>200</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Experimental Setup</title>
            <text>We conducted our experiments on the German- English data published for the ACL 2008 Workshop on Statistical Machine Translation (WMT08). Statistics for the Europarl data are given in Table 2.
We are given the three data sets T RAIN, DEV and T EST . For the heuristic phrase model, we first use GIZA++ (Och and Ney, 2003) to compute the word alignment on T RAIN. Next we obtain a phrase table by extraction of phrases from the word alignment. The scaling factors of the translation models have been optimized for BLEU on the DEV data.
The phrase table obtained by heuristic extraction is also used to initialize the training. The forced alignment is run on the training data T RAIN from which we obtain the phrase alignments. Those are used to build a phrase table according to the proposed generative phrase models. Afterward, the scaling factors are trained on DEV for the new phrase table. By feeding back the new phrase table into forced alignment we can reiterate the training procedure. When training is finished the resulting phrase model is evaluated on DEV Table 3: Comparison of different training setups for the count model on DEV .
leaving-one-out max phr.len. BLEU TER baseline 6 25.7 61.1
and T EST . Additionally, we can apply smoothing by interpolation of the new phrase table with the original one estimated heuristically, retrain the scaling factors and evaluate afterwards.
The baseline system is a standard phrase-based SMT system with eight features: phrase translation and word lexicon probabilities in both translation directions, phrase penalty, word penalty, language model score and a simple distance-based reordering model. The features are combined in a log-linear way. To investigate the generative models, we replace the two phrase translation probabilities and keep the other features identical to the baseline. For the feature-wise combination the two generative phrase probabilities are added to the features, resulting in a total of 10 features. We used a 4-gram language model with modified Kneser-Ney discounting for all experiments. The metrics used for evaluation are the case-sensitive
BLEU (Papineni et al., 2002) score and the translation edit rate (TER) (Snover et al., 2006) with one reference translation.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We conducted our experiments on the German- English data published for the ACL 2008 Workshop on Statistical Machine Translation (WMT08).</text>
                  <doc_id>201</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Statistics for the Europarl data are given in Table 2.</text>
                  <doc_id>202</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We are given the three data sets T RAIN, DEV and T EST .</text>
                  <doc_id>203</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For the heuristic phrase model, we first use GIZA++ (Och and Ney, 2003) to compute the word alignment on T RAIN.</text>
                  <doc_id>204</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Next we obtain a phrase table by extraction of phrases from the word alignment.</text>
                  <doc_id>205</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The scaling factors of the translation models have been optimized for BLEU on the DEV data.</text>
                  <doc_id>206</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The phrase table obtained by heuristic extraction is also used to initialize the training.</text>
                  <doc_id>207</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The forced alignment is run on the training data T RAIN from which we obtain the phrase alignments.</text>
                  <doc_id>208</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Those are used to build a phrase table according to the proposed generative phrase models.</text>
                  <doc_id>209</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Afterward, the scaling factors are trained on DEV for the new phrase table.</text>
                  <doc_id>210</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>By feeding back the new phrase table into forced alignment we can reiterate the training procedure.</text>
                  <doc_id>211</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>When training is finished the resulting phrase model is evaluated on DEV Table 3: Comparison of different training setups for the count model on DEV .</text>
                  <doc_id>212</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>leaving-one-out max phr.len.</text>
                  <doc_id>213</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>BLEU TER baseline 6 25.7 61.1</text>
                  <doc_id>214</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and T EST .</text>
                  <doc_id>215</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Additionally, we can apply smoothing by interpolation of the new phrase table with the original one estimated heuristically, retrain the scaling factors and evaluate afterwards.</text>
                  <doc_id>216</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The baseline system is a standard phrase-based SMT system with eight features: phrase translation and word lexicon probabilities in both translation directions, phrase penalty, word penalty, language model score and a simple distance-based reordering model.</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The features are combined in a log-linear way.</text>
                  <doc_id>218</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>To investigate the generative models, we replace the two phrase translation probabilities and keep the other features identical to the baseline.</text>
                  <doc_id>219</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For the feature-wise combination the two generative phrase probabilities are added to the features, resulting in a total of 10 features.</text>
                  <doc_id>220</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We used a 4-gram language model with modified Kneser-Ney discounting for all experiments.</text>
                  <doc_id>221</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The metrics used for evaluation are the case-sensitive</text>
                  <doc_id>222</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BLEU (Papineni et al., 2002) score and the translation edit rate (TER) (Snover et al., 2006) with one reference translation.</text>
                  <doc_id>223</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Results</title>
            <text>In this section, we investigate the different aspects of the models and methods presented before. We will focus on the proposed leaving-oneout technique and show that it helps in finding good phrasal alignments on the training data that lead to improved translation models. Our final results show an improvement of 1.4 BLEU over the heuristically extracted phrase model on the test data set.
In Section 3.2 we have discussed several methods which aim to overcome the over-fitting prob-
lems described in (DeNero et al., 2006). Table 3 shows translation scores of the count model on the development data after the first training iteration for both leaving-one-out strategies we have introduced and for training without leaving-one-out with different restrictions on phrase length. We can see that by restricting the source phrase length to a maximum of 3 words, the trained model is close to the performance of the heuristic phrase model. With the application of leaving-one-out, the trained model is superior to the baseline, the length-based strategy performing slightly better than standard leaving-one-out. For these experiments the count model was estimated with a 100- best list.
The count model we describe in Section 4.1 estimates phrase translation probabilities using counts from the n-best phrase alignments. For smaller n the resulting phrase table contains fewer phrases and is more deterministic. For higher values of n more competing alignments are taken into account, resulting in a bigger phrase table and a smoother distribution. We can see in Figure 3 that translation performance improves by moving from the Viterbi alignment to n-best alignments. The variations in performance with sizes between n = 10 and n = 10000 are less than 0.2 BLEU. The maximum is reached for n = 100, which we used in all subsequent experiments. An additional benefit of the count model is the smaller phrase table size compared to the heuristic phrase extraction. This is consistent with the findings of (Birch et al., 2006). Table 4 shows the phrase table sizes for different n. With n = 100 we retain only 17% of the original phrases. Even for the full model, we Table 4: Phrase table size of the count model for different n-best list sizes, the full model and for heuristic phrase extraction.
do not retain all phrase table entries. Due to pruning in the forced alignment step, not all translation options are considered. As a result experiments can be done more rapidly and with less resources than with the heuristically extracted phrase table. Also, our experiments show that the increased performance of the count model is partly derived from the smaller phrase table size. In Table 5 we can see that the performance of the heuristic phrase model can be increased by 0.6 BLEU on T EST by filtering the phrase table to contain the same phrases as the count model and reoptimizing the log-linear model weights. The experiments on the number of different alignments taken into account were done with standard leaving-one-out.
The final results are given in Table 5. We can see that the count model outperforms the baseline by 0.8 BLEU on DEV and 0.9 BLEU on T EST after the first training iteration. The performance of the filtered baseline phrase table shows that part of that improvement derives from the smaller phrase table size. Application of crossvalidation (cv) in the first iteration yields a performance close to training with leaving-one-out (l1o), which indicates that cross-validation can be safely applied to higher training iterations as an alternative to leaving-one-out. The weighted count model clearly under-performs the simpler count model. A second iteration of the training algorithm shows nearly no changes in BLEU score, but a small improvement in TER. Here, we used the phrase table trained with leaving-one-out in the first iteration and applied cross-validation in the second iteration. Log-linear interpolation of the count model with the heuristic yields a further increase, showing an improvement of 1.3 BLEU on DEV and 1.4
BLEU on T EST over the baseline. The interpo-
lation weight is adjusted on the development set and was set to &#969; = 0.6. Integrating both models into the log-linear framework (feat. comb.) yields a BLEU score slightly lower than with fixed interpolation on both DEV and T EST . This might be attributed to deficiencies in the tuning procedure. The full model, where we extract all phrases from the search graph, weighted with their posterior probability, performs comparable to the count model with a slightly worse BLEU and a slightly better TER.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In this section, we investigate the different aspects of the models and methods presented before.</text>
                  <doc_id>224</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We will focus on the proposed leaving-oneout technique and show that it helps in finding good phrasal alignments on the training data that lead to improved translation models.</text>
                  <doc_id>225</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Our final results show an improvement of 1.4 BLEU over the heuristically extracted phrase model on the test data set.</text>
                  <doc_id>226</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In Section 3.2 we have discussed several methods which aim to overcome the over-fitting prob-</text>
                  <doc_id>227</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>lems described in (DeNero et al., 2006).</text>
                  <doc_id>228</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Table 3 shows translation scores of the count model on the development data after the first training iteration for both leaving-one-out strategies we have introduced and for training without leaving-one-out with different restrictions on phrase length.</text>
                  <doc_id>229</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We can see that by restricting the source phrase length to a maximum of 3 words, the trained model is close to the performance of the heuristic phrase model.</text>
                  <doc_id>230</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>With the application of leaving-one-out, the trained model is superior to the baseline, the length-based strategy performing slightly better than standard leaving-one-out.</text>
                  <doc_id>231</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For these experiments the count model was estimated with a 100- best list.</text>
                  <doc_id>232</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The count model we describe in Section 4.1 estimates phrase translation probabilities using counts from the n-best phrase alignments.</text>
                  <doc_id>233</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For smaller n the resulting phrase table contains fewer phrases and is more deterministic.</text>
                  <doc_id>234</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For higher values of n more competing alignments are taken into account, resulting in a bigger phrase table and a smoother distribution.</text>
                  <doc_id>235</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We can see in Figure 3 that translation performance improves by moving from the Viterbi alignment to n-best alignments.</text>
                  <doc_id>236</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The variations in performance with sizes between n = 10 and n = 10000 are less than 0.2 BLEU.</text>
                  <doc_id>237</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The maximum is reached for n = 100, which we used in all subsequent experiments.</text>
                  <doc_id>238</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>An additional benefit of the count model is the smaller phrase table size compared to the heuristic phrase extraction.</text>
                  <doc_id>239</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>This is consistent with the findings of (Birch et al., 2006).</text>
                  <doc_id>240</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>Table 4 shows the phrase table sizes for different n.</text>
                  <doc_id>241</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>With n = 100 we retain only 17% of the original phrases.</text>
                  <doc_id>242</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>Even for the full model, we Table 4: Phrase table size of the count model for different n-best list sizes, the full model and for heuristic phrase extraction.</text>
                  <doc_id>243</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>do not retain all phrase table entries.</text>
                  <doc_id>244</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Due to pruning in the forced alignment step, not all translation options are considered.</text>
                  <doc_id>245</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As a result experiments can be done more rapidly and with less resources than with the heuristically extracted phrase table.</text>
                  <doc_id>246</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Also, our experiments show that the increased performance of the count model is partly derived from the smaller phrase table size.</text>
                  <doc_id>247</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In Table 5 we can see that the performance of the heuristic phrase model can be increased by 0.6 BLEU on T EST by filtering the phrase table to contain the same phrases as the count model and reoptimizing the log-linear model weights.</text>
                  <doc_id>248</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The experiments on the number of different alignments taken into account were done with standard leaving-one-out.</text>
                  <doc_id>249</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The final results are given in Table 5.</text>
                  <doc_id>250</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We can see that the count model outperforms the baseline by 0.8 BLEU on DEV and 0.9 BLEU on T EST after the first training iteration.</text>
                  <doc_id>251</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The performance of the filtered baseline phrase table shows that part of that improvement derives from the smaller phrase table size.</text>
                  <doc_id>252</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Application of crossvalidation (cv) in the first iteration yields a performance close to training with leaving-one-out (l1o), which indicates that cross-validation can be safely applied to higher training iterations as an alternative to leaving-one-out.</text>
                  <doc_id>253</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The weighted count model clearly under-performs the simpler count model.</text>
                  <doc_id>254</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>A second iteration of the training algorithm shows nearly no changes in BLEU score, but a small improvement in TER.</text>
                  <doc_id>255</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Here, we used the phrase table trained with leaving-one-out in the first iteration and applied cross-validation in the second iteration.</text>
                  <doc_id>256</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Log-linear interpolation of the count model with the heuristic yields a further increase, showing an improvement of 1.3 BLEU on DEV and 1.4</text>
                  <doc_id>257</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BLEU on T EST over the baseline.</text>
                  <doc_id>258</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The interpo-</text>
                  <doc_id>259</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>lation weight is adjusted on the development set and was set to &#969; = 0.6.</text>
                  <doc_id>260</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Integrating both models into the log-linear framework (feat.</text>
                  <doc_id>261</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>comb.</text>
                  <doc_id>262</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>) yields a BLEU score slightly lower than with fixed interpolation on both DEV and T EST .</text>
                  <doc_id>263</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This might be attributed to deficiencies in the tuning procedure.</text>
                  <doc_id>264</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The full model, where we extract all phrases from the search graph, weighted with their posterior probability, performs comparable to the count model with a slightly worse BLEU and a slightly better TER.</text>
                  <doc_id>265</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>6 Conclusion</title>
        <text>We have shown that training phrase models can improve translation performance on a state-ofthe-art phrase-based translation model. This is achieved by training phrase translation probabilities in a way that they are consistent with their use in translation. A crucial aspect here is the use of leaving-one-out to avoid over-fitting. We have shown that the technique is superior to limiting phrase lengths and smoothing with lexical probabilities alone.
While models trained from Viterbi alignments already lead to good results, we have demonstrated that considering the 100-best alignments allows to better model the ambiguities in phrase segmentation.
The proposed techniques are shown to be superior to previous approaches that only used lexical probabilities to smooth phrase tables or imposed limits on the phrase lengths. On the WMT08 Europarl task we show improvements of 0.9 BLEU points with the trained phrase table and 1.4 BLEU points when interpolating the newly trained model with the original, heuristically extracted phrase table. In TER, improvements are 0.4 and 1.7 points.
In addition to the improved performance, the trained models are smaller leading to faster and smaller translation systems.
Acknowledgments
This work was partly realized as part of the Quaero Programme, funded by OSEO, French State agency for innovation, and also partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001-06-C-0023. Any opinions, ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reect the views of the DARPA.
References
Alexandra Birch, Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Constraining the phrase-based, joint probability statistical translation model. In smt2006, pages 154&#8211;157, Jun.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proceedings of ACL-08: HLT, pages 200&#8211;208, Columbus, Ohio, June. Association for Computational Linguistics.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263&#8211;312, June.
John DeNero and Dan Klein. 2008. The complexity of phrase alignment problems. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 25&#8211;28, Morristown, NJ, USA. Association for Computational Linguistics.
John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006. Why Generative Phrase Models Underperform Surface Heuristics. In Proceedings of the
Workshop on Statistical Machine Translation, pages 31&#8211;38, New York City, June.
John DeNero, Alexandre Buchard-C&#244;t&#233;, and Dan Klein. 2008. Sampling Alignment Structure under a Bayesian Translation Model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 314&#8211;323, Honolulu, October.
Nicola Ehling, Richard Zens, and Hermann Ney. 2007. Minimum bayes risk decoding for bleu. In ACL &#8217;07: Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 101&#8211;104, Morristown, NJ, USA. Association for Computational Linguistics.
Jes&#250;s-Andr&#233;s Ferrer and Alfons Juan. 2009. A phrasebased hidden semi-markov approach to machine translation. In Procedings of European Association for Machine Translation (EAMT), Barcelona, Spain, May. European Association for Machine Translation.
Reinhard Kneser and Hermann Ney. 1995. Improved Backing-Off for M-gram Language Modelling. In IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pages 181&#8211;184, Detroit, MI, May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, pages 48&#8211;54, Morristown, NJ, USA. Association for Computational Linguistics.
F.J. Och, C. Tillmann, and H. Ney. 1999. Improved alignment models for statistical machine translation. In Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP99), pages 20&#8211;28, University of Maryland, College Park, MD, USA, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311&#8211;318, Morristown, NJ, USA. Association for Computational Linguistics.
Wade Shen, Brian Delaney, Tim Anderson, and Ray Slyh. 2008. The MIT-LL/AFRL IWSLT-2008 MT System. In Proceedings of IWSLT 2008, pages 69&#8211; 76, Hawaii, U.S.A., October.
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proc. of AMTA, pages 223&#8211;231, Aug.
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum Bayes- Risk decoding for statistical machine translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 620&#8211;629, Honolulu, Hawaii, October. Association for Computational Linguistics.
N. Ueffing, F.J. Och, and H. Ney. 2002. Generation of word graphs in statistical machine translation. In Proc. of the Conference on Empirical Methods for Natural Language Processing, pages 156&#8211;163, Philadelphia, PA, USA, July.
Percy Liang, Alexandre Buchard-C&#244;t&#233;, Dan Klein, and Ben Taskar. 2006. An End-to-End Discriminative Approach to Machine Translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 761&#8211; 768, Sydney, Australia.
Daniel Marcu and William Wong. 2002. A phrasebased, joint probability model for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2002), July.
J.A. Nelder and R. Mead. 1965. A Simplex Method for Function Minimization. The Computer Journal), 7:308&#8211;313.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19&#8211;51, March.
Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417&#8211;449, December.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have shown that training phrase models can improve translation performance on a state-ofthe-art phrase-based translation model.</text>
              <doc_id>266</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This is achieved by training phrase translation probabilities in a way that they are consistent with their use in translation.</text>
              <doc_id>267</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A crucial aspect here is the use of leaving-one-out to avoid over-fitting.</text>
              <doc_id>268</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We have shown that the technique is superior to limiting phrase lengths and smoothing with lexical probabilities alone.</text>
              <doc_id>269</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>While models trained from Viterbi alignments already lead to good results, we have demonstrated that considering the 100-best alignments allows to better model the ambiguities in phrase segmentation.</text>
              <doc_id>270</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The proposed techniques are shown to be superior to previous approaches that only used lexical probabilities to smooth phrase tables or imposed limits on the phrase lengths.</text>
              <doc_id>271</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>On the WMT08 Europarl task we show improvements of 0.9 BLEU points with the trained phrase table and 1.4 BLEU points when interpolating the newly trained model with the original, heuristically extracted phrase table.</text>
              <doc_id>272</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In TER, improvements are 0.4 and 1.7 points.</text>
              <doc_id>273</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In addition to the improved performance, the trained models are smaller leading to faster and smaller translation systems.</text>
              <doc_id>274</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Acknowledgments</text>
              <doc_id>275</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>This work was partly realized as part of the Quaero Programme, funded by OSEO, French State agency for innovation, and also partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.</text>
              <doc_id>276</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>HR001-06-C-0023.</text>
              <doc_id>277</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Any opinions, ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reect the views of the DARPA.</text>
              <doc_id>278</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>References</text>
              <doc_id>279</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Alexandra Birch, Chris Callison-Burch, Miles Osborne, and Philipp Koehn.</text>
              <doc_id>280</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>281</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Constraining the phrase-based, joint probability statistical translation model.</text>
              <doc_id>282</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In smt2006, pages 154&#8211;157, Jun.</text>
              <doc_id>283</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Phil Blunsom, Trevor Cohn, and Miles Osborne.</text>
              <doc_id>284</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>285</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A discriminative latent variable model for statistical machine translation.</text>
              <doc_id>286</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL-08: HLT, pages 200&#8211;208, Columbus, Ohio, June.</text>
              <doc_id>287</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>288</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P. F. Brown, V.</text>
              <doc_id>289</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>J. Della Pietra, S.</text>
              <doc_id>290</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A. Della Pietra, and R.</text>
              <doc_id>291</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>L. Mercer.</text>
              <doc_id>292</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>1993.</text>
              <doc_id>293</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The mathematics of statistical machine translation: parameter estimation.</text>
              <doc_id>294</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 19(2):263&#8211;312, June.</text>
              <doc_id>295</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>John DeNero and Dan Klein.</text>
              <doc_id>296</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>297</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The complexity of phrase alignment problems.</text>
              <doc_id>298</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 25&#8211;28, Morristown, NJ, USA.</text>
              <doc_id>299</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>300</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>John DeNero, Dan Gillick, James Zhang, and Dan Klein.</text>
              <doc_id>301</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>302</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Why Generative Phrase Models Underperform Surface Heuristics.</text>
              <doc_id>303</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the</text>
              <doc_id>304</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Workshop on Statistical Machine Translation, pages 31&#8211;38, New York City, June.</text>
              <doc_id>305</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>John DeNero, Alexandre Buchard-C&#244;t&#233;, and Dan Klein.</text>
              <doc_id>306</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>307</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Sampling Alignment Structure under a Bayesian Translation Model.</text>
              <doc_id>308</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 314&#8211;323, Honolulu, October.</text>
              <doc_id>309</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Nicola Ehling, Richard Zens, and Hermann Ney.</text>
              <doc_id>310</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>311</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Minimum bayes risk decoding for bleu.</text>
              <doc_id>312</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In ACL &#8217;07: Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 101&#8211;104, Morristown, NJ, USA.</text>
              <doc_id>313</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>314</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Jes&#250;s-Andr&#233;s Ferrer and Alfons Juan.</text>
              <doc_id>315</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>316</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A phrasebased hidden semi-markov approach to machine translation.</text>
              <doc_id>317</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Procedings of European Association for Machine Translation (EAMT), Barcelona, Spain, May.</text>
              <doc_id>318</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>European Association for Machine Translation.</text>
              <doc_id>319</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Reinhard Kneser and Hermann Ney.</text>
              <doc_id>320</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1995.</text>
              <doc_id>321</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Improved Backing-Off for M-gram Language Modelling.</text>
              <doc_id>322</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In IEEE Int.</text>
              <doc_id>323</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Conf. on Acoustics, Speech and Signal Processing (ICASSP), pages 181&#8211;184, Detroit, MI, May.</text>
              <doc_id>324</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Philipp Koehn, Franz Josef Och, and Daniel Marcu.</text>
              <doc_id>325</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>326</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Statistical phrase-based translation.</text>
              <doc_id>327</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, pages 48&#8211;54, Morristown, NJ, USA.</text>
              <doc_id>328</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>329</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>F.J.</text>
              <doc_id>330</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Och, C. Tillmann, and H. Ney.</text>
              <doc_id>331</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>1999.</text>
              <doc_id>332</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Improved alignment models for statistical machine translation.</text>
              <doc_id>333</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP99), pages 20&#8211;28, University of Maryland, College Park, MD, USA, June.</text>
              <doc_id>334</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu.</text>
              <doc_id>335</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>336</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Bleu: a method for automatic evaluation of machine translation.</text>
              <doc_id>337</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311&#8211;318, Morristown, NJ, USA.</text>
              <doc_id>338</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>339</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Wade Shen, Brian Delaney, Tim Anderson, and Ray Slyh.</text>
              <doc_id>340</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>341</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The MIT-LL/AFRL IWSLT-2008 MT System.</text>
              <doc_id>342</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of IWSLT 2008, pages 69&#8211; 76, Hawaii, U.S.A., October.</text>
              <doc_id>343</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul.</text>
              <doc_id>344</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>345</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A study of translation edit rate with targeted human annotation.</text>
              <doc_id>346</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of AMTA, pages 223&#8211;231, Aug.</text>
              <doc_id>347</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey.</text>
              <doc_id>348</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>349</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Lattice Minimum Bayes- Risk decoding for statistical machine translation.</text>
              <doc_id>350</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 620&#8211;629, Honolulu, Hawaii, October.</text>
              <doc_id>351</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>352</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>N. Ueffing, F.J.</text>
              <doc_id>353</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Och, and H. Ney.</text>
              <doc_id>354</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>355</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Generation of word graphs in statistical machine translation.</text>
              <doc_id>356</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the Conference on Empirical Methods for Natural Language Processing, pages 156&#8211;163, Philadelphia, PA, USA, July.</text>
              <doc_id>357</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Percy Liang, Alexandre Buchard-C&#244;t&#233;, Dan Klein, and Ben Taskar.</text>
              <doc_id>358</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>359</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>An End-to-End Discriminative Approach to Machine Translation.</text>
              <doc_id>360</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 761&#8211; 768, Sydney, Australia.</text>
              <doc_id>361</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Daniel Marcu and William Wong.</text>
              <doc_id>362</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>363</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A phrasebased, joint probability model for statistical machine translation.</text>
              <doc_id>364</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2002), July.</text>
              <doc_id>365</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>J.A.</text>
              <doc_id>366</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Nelder and R. Mead.</text>
              <doc_id>367</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>1965.</text>
              <doc_id>368</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>A Simplex Method for Function Minimization.</text>
              <doc_id>369</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The Computer Journal), 7:308&#8211;313.</text>
              <doc_id>370</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Franz Josef Och and Hermann Ney.</text>
              <doc_id>371</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>372</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A systematic comparison of various statistical alignment models.</text>
              <doc_id>373</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 29(1):19&#8211;51, March.</text>
              <doc_id>374</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Franz Josef Och and Hermann Ney.</text>
              <doc_id>375</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2004.</text>
              <doc_id>376</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The alignment template approach to statistical machine translation.</text>
              <doc_id>377</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 30(4):417&#8211;449, December.</text>
              <doc_id>378</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Statistics for the Europarl German- English data</caption>
        <reference_text>None</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>German English</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>TRAIN</cell>
              <cell>Sentences</cell>
              <cell>1 311 815</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Run. Words</cell>
              <cell>34 398 651 36 090 085</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Vocabulary</cell>
              <cell>336 347 118 112</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Singletons</cell>
              <cell>168 686 47 507</cell>
            </row>
            <row>
              <cell>DEV</cell>
              <cell>Sentences</cell>
              <cell>2 000</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Run. Words</cell>
              <cell>55 118 58 761</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Vocabulary</cell>
              <cell>9 211 6 549</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>OOVs</cell>
              <cell>284 77</cell>
            </row>
            <row>
              <cell>TEST</cell>
              <cell>Sentences</cell>
              <cell>2 000</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Run. Words</cell>
              <cell>56 635 60 188</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Vocabulary</cell>
              <cell>9 254 6 497</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>OOVs</cell>
              <cell>266 89</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Comparison of different training setups for the count model on DEV .</caption>
        <reference_text>None</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>leaving-one-out</cell>
              <cell>max phr.len.</cell>
              <cell>BLEU</cell>
              <cell>TER</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>baseline</cell>
              <cell>6</cell>
              <cell>25.7</cell>
              <cell>61.1</cell>
            </row>
            <row>
              <cell>none</cell>
              <cell>2</cell>
              <cell>25.2</cell>
              <cell>61.3</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>3</cell>
              <cell>25.7</cell>
              <cell>61.3</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>4</cell>
              <cell>25.5</cell>
              <cell>61.4</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>5</cell>
              <cell>25.5</cell>
              <cell>61.4</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>6</cell>
              <cell>25.4</cell>
              <cell>61.7</cell>
            </row>
            <row>
              <cell>standard</cell>
              <cell>6</cell>
              <cell>26.4</cell>
              <cell>60.9</cell>
            </row>
            <row>
              <cell>length-based</cell>
              <cell>6</cell>
              <cell>26.5</cell>
              <cell>60.6</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: Phrase table size of the count model for different n-best list sizes, the full model and for heuristic phrase extraction.#@#@Table 5: Final results for the heuristic phrase table filtered to contain the same phrases as the count model (baseline filt.), the count model trained with leaving-one-out (l1o) and cross-validation (cv), the weighted count model and the full model. Further, scores for fixed log-linear interpolation of the count model trained with leaving-one-out with the heuristic as well as a feature-wise combination are shown. The results of the second training iteration are given in the bottom row.</caption>
        <reference_text>In PAGE 8: ..., 2006).  Table4  shows the phrase table sizes for different n. With n = 100 we retain only 17% of the original phrases....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>N</cell>
              <cell># phrases</cell>
              <cell>% of full table</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>1</cell>
              <cell>4.9M</cell>
              <cell>5.3</cell>
            </row>
            <row>
              <cell>10</cell>
              <cell>8.4M</cell>
              <cell>9.1</cell>
            </row>
            <row>
              <cell>100</cell>
              <cell>15.9M</cell>
              <cell>17.2</cell>
            </row>
            <row>
              <cell>1000</cell>
              <cell>27.1M</cell>
              <cell>29.2</cell>
            </row>
            <row>
              <cell>10000</cell>
              <cell>40.1M</cell>
              <cell>43.2</cell>
            </row>
            <row>
              <cell>full</cell>
              <cell>59.6M</cell>
              <cell>64.2</cell>
            </row>
            <row>
              <cell>heuristic</cell>
              <cell>92.7M</cell>
              <cell>100.0</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TableSeer</source>
        <caption>Table 5: Final results for the heuristic phrase table filtered to contain the same phrases as the count model (baseline filt.), the count model trained with leaving-one-out (l1o) and cross-validation (cv), the weighted count model and the full model. Fur- ther, scores for fixed log-linear interpolation of the count model trained with leaving-one-out with the heuristic as well as a feature-wise combination are shown. The results of the second training iteration are given in the bottom row.</caption>
        <reference_text>In PAGE 8: ... Also, our experiments show that the increased per- formance of the count model is partly derived from the smaller phrase table size. In  Table5  we can see that the performance of the heuristic phrase model can be increased by 0.6 BLEU on T EST by fil- tering the phrase table to contain the same phrases as the count model and reoptimizing the log-linear model weights....  In PAGE 8: ... The experiments on the number of different alignments taken into account were done with standard leaving-one-out. The final results are given in  Table5 . We can see that the count model outperforms the base- line by 0....</reference_text>
        <page_num>9</page_num>
        <head>
          <rows>
            <row>
              <cell>None</cell>
              <cell>DEV  BLEU</cell>
              <cell>DEV   TER</cell>
              <cell>TEST  BLEU</cell>
              <cell>TEST    TER</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>baseline</cell>
              <cell>25.7</cell>
              <cell>61.1</cell>
              <cell>26.3</cell>
              <cell>60.9</cell>
            </row>
            <row>
              <cell>baseline filt.</cell>
              <cell>26.0</cell>
              <cell>61.6</cell>
              <cell>26.9</cell>
              <cell>61.2</cell>
            </row>
            <row>
              <cell>count (l1o)</cell>
              <cell>26.5</cell>
              <cell>60.6</cell>
              <cell>27.2</cell>
              <cell>60.5</cell>
            </row>
            <row>
              <cell>count (cv)</cell>
              <cell>26.4</cell>
              <cell>60.7</cell>
              <cell>27.0</cell>
              <cell>60.7</cell>
            </row>
            <row>
              <cell>weight. count</cell>
              <cell>25.9</cell>
              <cell>61.4</cell>
              <cell>26.4</cell>
              <cell>61.3</cell>
            </row>
            <row>
              <cell>full</cell>
              <cell>26.3</cell>
              <cell>60.0</cell>
              <cell>27.0</cell>
              <cell>60.2</cell>
            </row>
            <row>
              <cell>fixed interpol.</cell>
              <cell>27.0</cell>
              <cell>59.4</cell>
              <cell>27.7</cell>
              <cell>59.2</cell>
            </row>
            <row>
              <cell>feat. comb.</cell>
              <cell>26.8</cell>
              <cell>60.1</cell>
              <cell>27.6</cell>
              <cell>59.9</cell>
            </row>
            <row>
              <cell>count, iter. 2</cell>
              <cell>26.4</cell>
              <cell>60.3</cell>
              <cell>27.2</cell>
              <cell>60.0</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references/>
    <citations/>
  </content>
</document>
