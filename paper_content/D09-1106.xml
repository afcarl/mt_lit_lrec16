<document>
  <filename>D09-1106</filename>
  <authors/>
  <title>Weighted Alignment Matrices for Statistical Machine Translation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>Current statistical machine translation systems usually extract rules from bilingual corpora annotated with 1-best alignments. They are prone to learn noisy rules due to alignment mistakes. We propose a new structure called weighted alignment matrix to encode all possible alignments for a parallel text compactly. The key idea is to assign a probability to each word pair to indicate how well they are aligned. We design new algorithms for extracting phrase pairs from weighted alignment matrices and estimating their probabilities. Our experiments on multiple language pairs show that using weighted matrices achieves consistent improvements over using n-best lists in significant less extraction time.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Current statistical machine translation systems usually extract rules from bilingual corpora annotated with 1-best alignments.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>They are prone to learn noisy rules due to alignment mistakes.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We propose a new structure called weighted alignment matrix to encode all possible alignments for a parallel text compactly.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The key idea is to assign a probability to each word pair to indicate how well they are aligned.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We design new algorithms for extracting phrase pairs from weighted alignment matrices and estimating their probabilities.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Our experiments on multiple language pairs show that using weighted matrices achieves consistent improvements over using n-best lists in significant less extraction time.</text>
              <doc_id>5</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Statistical machine translation (SMT) relies heavily on annotated bilingual corpora. Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT. Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)). Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment.
Modern alignment methods can be divided into two major categories: generative methods and discriminative methods. Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm. In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2005; Liu et al., 2005; Blunsom and Cohn, 2006)) have the freedom to define arbitrary feature functions that describe various characteristics of an alignment. They usually optimize feature weights on manually-aligned data. While discriminative methods show superior alignment accuracy in benchmarks, generative methods are still widely used to produce word alignments for large sentence-aligned corpora.
However, neither generative nor discriminative alignment methods are reliable enough to yield high quality alignments for SMT, especially for distantly-related language pairs such as Chinese- English and Arabic-English. The F-measures for Chinese-English and Arabic-English are usually around 80% (Liu et al., 2005) and 70% (Fraser and Marcu, 2007), respectively. As most current SMT systems only use 1-best alignments for extracting rules, alignment errors might impair translation quality.
Recently, several studies have shown that offering more alternatives of annotations to SMT systems will result in significant improvements, such as replacing 1-best trees with packed forests (Mi et al., 2008) and replacing 1-best word segmentations with word lattices (Dyer et al., 2008). Similarly, Venugopal et al. (2008) use n-best alignments instead of 1-best alignments for translation rule extraction. While they achieve significant improvements on the IWSLT data, extracting rules from n-best alignments might be computationally expensive.
In this paper, we propose a new structure named weighted alignment matrix to represent the alignment distribution for a sentence pair compactly. In a weighted matrix, each element that corresponds to a word pair is assigned a probability to measure the confidence of aligning the two words. Therefore, a weighted matrix is capable of using a lin-
economy
&#8217;s China
of development the
zhongguo
de jingji
fazhan
ear space to encode the probabilities of exponentially many alignments. We develop a new algorithm for extracting phrase pairs from weighted matrices and show how to estimate their relative frequencies and lexical weights. Experimental results show that using weighted matrices achieves consistent improvements in translation quality and significant reduction in extraction time over using n-best lists.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Statistical machine translation (SMT) relies heavily on annotated bilingual corpora.</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Word alignment, which indicates the correspondence between the words in a parallel text, is one of the most important annotations in SMT.</text>
              <doc_id>7</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Word-aligned corpora have been found to be an excellent source for translation-related knowledge, not only for phrase-based models (Och and Ney, 2004; Koehn et al., 2003), but also for syntax-based models (e.g., (Chiang, 2007; Galley et al., 2006; Shen et al., 2008; Liu et al., 2006)).</text>
              <doc_id>8</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Och and Ney (2003) indicate that the quality of machine translation output depends directly on the quality of initial word alignment.</text>
              <doc_id>9</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Modern alignment methods can be divided into two major categories: generative methods and discriminative methods.</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Generative methods (Brown et al., 1993; Vogel and Ney, 1996) treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization (EM) algorithm.</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In contrast, discriminative methods (e.g., (Moore et al., 2006; Taskar et al., 2005; Liu et al., 2005; Blunsom and Cohn, 2006)) have the freedom to define arbitrary feature functions that describe various characteristics of an alignment.</text>
              <doc_id>12</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>They usually optimize feature weights on manually-aligned data.</text>
              <doc_id>13</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>While discriminative methods show superior alignment accuracy in benchmarks, generative methods are still widely used to produce word alignments for large sentence-aligned corpora.</text>
              <doc_id>14</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>However, neither generative nor discriminative alignment methods are reliable enough to yield high quality alignments for SMT, especially for distantly-related language pairs such as Chinese- English and Arabic-English.</text>
              <doc_id>15</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The F-measures for Chinese-English and Arabic-English are usually around 80% (Liu et al., 2005) and 70% (Fraser and Marcu, 2007), respectively.</text>
              <doc_id>16</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>As most current SMT systems only use 1-best alignments for extracting rules, alignment errors might impair translation quality.</text>
              <doc_id>17</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Recently, several studies have shown that offering more alternatives of annotations to SMT systems will result in significant improvements, such as replacing 1-best trees with packed forests (Mi et al., 2008) and replacing 1-best word segmentations with word lattices (Dyer et al., 2008).</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Similarly, Venugopal et al. (2008) use n-best alignments instead of 1-best alignments for translation rule extraction.</text>
              <doc_id>19</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>While they achieve significant improvements on the IWSLT data, extracting rules from n-best alignments might be computationally expensive.</text>
              <doc_id>20</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this paper, we propose a new structure named weighted alignment matrix to represent the alignment distribution for a sentence pair compactly.</text>
              <doc_id>21</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In a weighted matrix, each element that corresponds to a word pair is assigned a probability to measure the confidence of aligning the two words.</text>
              <doc_id>22</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, a weighted matrix is capable of using a lin-</text>
              <doc_id>23</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>economy</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8217;s China</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>of development the</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>zhongguo</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>de jingji</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>fazhan</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>ear space to encode the probabilities of exponentially many alignments.</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We develop a new algorithm for extracting phrase pairs from weighted matrices and show how to estimate their relative frequencies and lexical weights.</text>
              <doc_id>31</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Experimental results show that using weighted matrices achieves consistent improvements in translation quality and significant reduction in extraction time over using n-best lists.</text>
              <doc_id>32</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Background</title>
        <text>Figure 1 shows an example of word alignment between a pair of Chinese and English sentences. The Chinese and English words are listed horizontally and vertically, respectively. The dark points indicate the correspondence between the words in two languages. For example, the first Chinese word &#8220;zhongguo&#8221; is aligned to the fourth English word &#8220;China&#8221;.
Formally, given a source sentence f = f1 J = f 1 , . . . , f j , . . . , f J and a target sentence e = e I 1 = e 1 , . . . , e i , . . . , e I , we define a link l = (j, i) to exist if f j and e i are translation (or part of translation) of one another. Then, an alignment a is a subset of the Cartesian product of word positions:
a &#8838; {(j, i) : j = 1, . . . , J; i = 1, . . . , I} (1)
Usually, SMT systems only use the 1-best alignments for extracting translation rules. For example, given a source phrase &#732;f and a target phrase &#7869;, the phrase pair ( &#732;f, &#7869;) is said to be consistent (Och and Ney, 2004) with the alignment if and only if: (1) there must be at least one word inside one phrase aligned to a word inside the other phrase and (2) no words inside one phrase can be aligned to a word outside the other phrase. After all phrase pairs are extracted from the training corpus, their translation probabilities can be estimated as relative frequencies (Och and Ney, 2004):
&#966;(&#7869;| &#732;f) = count( &#732;f, &#7869;) &#8721;
&#7869; &#8242; count( &#732;f, &#7869; &#8242; )
(2)
where count( &#732;f, &#7869;) indicates how often the phrase pair ( &#732;f, &#7869;) occurs in the training corpus.
Besides relative frequencies, lexical weights (Koehn et al., 2003) are widely used to estimate how well the words in &#732;f translate the words in &#7869;. To do this, one needs first to estimate a lexical translation probability distribution w(e|f) by relative frequency from the same word alignments in the training corpus:
w(e|f) =
count(f, e) &#8721;
e &#8242; count(f, e&#8242; )
(3)
Note that a special source NULL token is added to each source sentence and aligned to each unaligned target word.
As the alignment &#227; between a phrase pair ( &#732;f, &#7869;) is retained during extraction, the lexical weight can be calculated as
p w (&#7869;| &#732;f, &#227;) =
|&#7869;|
&#8719;
i=1
1 &#8721; w(ei |f j ) (4) |{j|(j, i) &#8712; &#227;}|
If there are multiple alignments &#227; for a phrase pair ( &#732;f, &#7869;), Koehn et al. (2003) choose the one with the highest lexical weight:
p w (&#7869;| &#732;f) = max
&#227;
{ p w (&#7869;| &#732;f, &#227;) } (5)
Simple and effective, relative frequencies and lexical weights have become the standard features in modern discriminative SMT systems.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Figure 1 shows an example of word alignment between a pair of Chinese and English sentences.</text>
              <doc_id>33</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The Chinese and English words are listed horizontally and vertically, respectively.</text>
              <doc_id>34</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The dark points indicate the correspondence between the words in two languages.</text>
              <doc_id>35</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>For example, the first Chinese word &#8220;zhongguo&#8221; is aligned to the fourth English word &#8220;China&#8221;.</text>
              <doc_id>36</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Formally, given a source sentence f = f1 J = f 1 , .</text>
              <doc_id>37</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>38</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>39</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>, f j , .</text>
              <doc_id>40</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>41</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>42</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>, f J and a target sentence e = e I 1 = e 1 , .</text>
              <doc_id>43</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>44</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>45</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>, e i , .</text>
              <doc_id>46</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>47</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>48</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>, e I , we define a link l = (j, i) to exist if f j and e i are translation (or part of translation) of one another.</text>
              <doc_id>49</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>Then, an alignment a is a subset of the Cartesian product of word positions:</text>
              <doc_id>50</doc_id>
              <sec_id>13</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>a &#8838; {(j, i) : j = 1, .</text>
              <doc_id>51</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>52</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>53</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>, J; i = 1, .</text>
              <doc_id>54</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>55</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>56</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>, I} (1)</text>
              <doc_id>57</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Usually, SMT systems only use the 1-best alignments for extracting translation rules.</text>
              <doc_id>58</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For example, given a source phrase &#732;f and a target phrase &#7869;, the phrase pair ( &#732;f, &#7869;) is said to be consistent (Och and Ney, 2004) with the alignment if and only if: (1) there must be at least one word inside one phrase aligned to a word inside the other phrase and (2) no words inside one phrase can be aligned to a word outside the other phrase.</text>
              <doc_id>59</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>After all phrase pairs are extracted from the training corpus, their translation probabilities can be estimated as relative frequencies (Och and Ney, 2004):</text>
              <doc_id>60</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#966;(&#7869;| &#732;f) = count( &#732;f, &#7869;) &#8721;</text>
              <doc_id>61</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#7869; &#8242; count( &#732;f, &#7869; &#8242; )</text>
              <doc_id>62</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(2)</text>
              <doc_id>63</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where count( &#732;f, &#7869;) indicates how often the phrase pair ( &#732;f, &#7869;) occurs in the training corpus.</text>
              <doc_id>64</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Besides relative frequencies, lexical weights (Koehn et al., 2003) are widely used to estimate how well the words in &#732;f translate the words in &#7869;.</text>
              <doc_id>65</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To do this, one needs first to estimate a lexical translation probability distribution w(e|f) by relative frequency from the same word alignments in the training corpus:</text>
              <doc_id>66</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>w(e|f) =</text>
              <doc_id>67</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>count(f, e) &#8721;</text>
              <doc_id>68</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e &#8242; count(f, e&#8242; )</text>
              <doc_id>69</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(3)</text>
              <doc_id>70</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Note that a special source NULL token is added to each source sentence and aligned to each unaligned target word.</text>
              <doc_id>71</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>As the alignment &#227; between a phrase pair ( &#732;f, &#7869;) is retained during extraction, the lexical weight can be calculated as</text>
              <doc_id>72</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>p w (&#7869;| &#732;f, &#227;) =</text>
              <doc_id>73</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>|&#7869;|</text>
              <doc_id>74</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8719;</text>
              <doc_id>75</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1</text>
              <doc_id>76</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 &#8721; w(ei |f j ) (4) |{j|(j, i) &#8712; &#227;}|</text>
              <doc_id>77</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>If there are multiple alignments &#227; for a phrase pair ( &#732;f, &#7869;), Koehn et al. (2003) choose the one with the highest lexical weight:</text>
              <doc_id>78</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>p w (&#7869;| &#732;f) = max</text>
              <doc_id>79</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#227;</text>
              <doc_id>80</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{ p w (&#7869;| &#732;f, &#227;) } (5)</text>
              <doc_id>81</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Simple and effective, relative frequencies and lexical weights have become the standard features in modern discriminative SMT systems.</text>
              <doc_id>82</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Weighted Alignment Matrix</title>
        <text>We believe that offering more candidate alignments to extracting translation rules might help improve translation quality. Instead of using n- best lists (Venugopal et al., 2008), we propose a new structure called weighted alignment matrix.
We use an example to illustrate our idea. Figure 2(a) and Figure 2(b) show two alignments of a Chinese-English sentence pair. We observe that some links (e.g., (1,4) corresponding to the word
economy
&#8217;s China
of development the economy
&#8217;s China
of development the economy
&#8217;s China
of development the
0 0 0 0.4 1.0 0
zhongguo
de jingji
fazhan
zhongguo
de jingji
fazhan
zhongguo
de jingji
fazhan
(a) (b) (c)
pair (&#8220;zhongguo&#8221;, &#8220;China&#8221;)) occur in both alignments, some links (e.g., (2,3) corresponding to the word pair (&#8220;de&#8221;,&#8220;of&#8221;)) occur only in one alignment, and some links (e.g., (1,1) corresponding to the word pair (&#8220;zhongguo&#8221;, &#8220;the&#8221;)) do not occur. Intuitively, we can estimate how well two words are aligned by calculating its relative frequency, which is the probability sum of alignments in which the link occurs divided by the probability sum of all possible alignments. Suppose that the probabilities of the two alignments in Figures 2(a) and 2(b) are 0.6 and 0.4, respectively. We can estimate the relative frequencies for every word pair and obtain a weighted matrix shown in Figure 2(c). Therefore, each word pair is associated with a probability to indicate how well they are aligned. For example, in Figure 2(c), we say that the word pair (&#8220;zhongguo&#8221;, &#8220;China&#8221;) is definitely aligned, (&#8220;zhongguo&#8221;, &#8220;the&#8221;) is definitely unaligned, and (&#8220;de&#8221;, &#8220;of&#8221;) has a 60% chance to get aligned.
Formally, a weighted alignment matrix m is a J &#215; I matrix, in which each element stores a link probability p m (j, i) to indicate how well f j and e i are aligned. Currently, we estimate link probabilities from an n-best list by calculating relative frequencies:
p m (j, i) = &#8721;
a&#8712;N p(a) &#215; &#948;(a, j, i) &#8721;a&#8712;N p(a) (6)
= &#8721; p(a) &#215; &#948;(a, j, i) (7)
a&#8712;N
where
&#948;(a, j, i) =
{ 1 (j, i) &#8712; a 0 otherwise (8)
Note that N is an n-best list, p(a) is the probability of an alignment a in the n-best list, &#948;(a, j, i) indicates whether a link (j, i) occurs in the alignment a or not. We assign 0 to any unseen alignment. As p(a) is usually normalized (i.e., &#8721;
a&#8712;N p(a) &#8801; 1), we remove the denominator in
Eq. (6).
Accordingly, the probability that the two words f j and e i are not aligned is
&#175;p m (j, i) = 1.0 &#8722; p m (j, i) (9)
For example, as shown in Figure 2(c), the probability for the two words &#8220;de&#8221; and &#8220;of&#8221; being aligned is 0.6 and the probability that they are not aligned is 0.4.
Intuitively, the probability of an alignment a is the product of link probabilities. If a link (j, i) occurs in a, we use p m (j, i); otherwise we use &#175;p m (j, i). Formally, given a weighted alignment matrix m, the probability of an alignment a can be calculated as
J&#8719; I&#8719; p m (a) = (p m (j, i) &#215; &#948;(a, j, i) +
j=1 i=1
&#175;p m (j, i) &#215; (1 &#8722; &#948;(a, j, i))) (10)
It proves that the sum of all alignment probabilities is always 1: &#8721; a&#8712;A p m(a) &#8801; 1, where A
is the set of all possible alignments. Therefore, a weighted alignment matrix is capable of encoding the probabilities of 2 J&#215;I alignments using only a J &#215; I space.
Note that p m (a) is not necessarily equal to p(a) because the encoding of a weighted alignment matrix changes the alignment probability distribution. For example, while the initial probability of the alignment in Figure 2(a) (i.e., p(a)) is 0.6, the probability of the same alignment encoded in the matrix shown in Figure 2(c) (i.e., p m (a)) becomes 0.1296 according to Eq. (10). It should be emphasized that a weighted matrix encodes all possible alignments rather than the input n-best list, although the link probabilities are estimated from the n-best list.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We believe that offering more candidate alignments to extracting translation rules might help improve translation quality.</text>
              <doc_id>83</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Instead of using n- best lists (Venugopal et al., 2008), we propose a new structure called weighted alignment matrix.</text>
              <doc_id>84</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We use an example to illustrate our idea.</text>
              <doc_id>85</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Figure 2(a) and Figure 2(b) show two alignments of a Chinese-English sentence pair.</text>
              <doc_id>86</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We observe that some links (e.g., (1,4) corresponding to the word</text>
              <doc_id>87</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>economy</text>
              <doc_id>88</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8217;s China</text>
              <doc_id>89</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>of development the economy</text>
              <doc_id>90</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8217;s China</text>
              <doc_id>91</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>of development the economy</text>
              <doc_id>92</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8217;s China</text>
              <doc_id>93</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>of development the</text>
              <doc_id>94</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0 0 0 0.4 1.0 0</text>
              <doc_id>95</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>zhongguo</text>
              <doc_id>96</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>de jingji</text>
              <doc_id>97</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>fazhan</text>
              <doc_id>98</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>zhongguo</text>
              <doc_id>99</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>de jingji</text>
              <doc_id>100</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>fazhan</text>
              <doc_id>101</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>zhongguo</text>
              <doc_id>102</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>de jingji</text>
              <doc_id>103</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>fazhan</text>
              <doc_id>104</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(a) (b) (c)</text>
              <doc_id>105</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>pair (&#8220;zhongguo&#8221;, &#8220;China&#8221;)) occur in both alignments, some links (e.g., (2,3) corresponding to the word pair (&#8220;de&#8221;,&#8220;of&#8221;)) occur only in one alignment, and some links (e.g., (1,1) corresponding to the word pair (&#8220;zhongguo&#8221;, &#8220;the&#8221;)) do not occur.</text>
              <doc_id>106</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Intuitively, we can estimate how well two words are aligned by calculating its relative frequency, which is the probability sum of alignments in which the link occurs divided by the probability sum of all possible alignments.</text>
              <doc_id>107</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Suppose that the probabilities of the two alignments in Figures 2(a) and 2(b) are 0.6 and 0.4, respectively.</text>
              <doc_id>108</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We can estimate the relative frequencies for every word pair and obtain a weighted matrix shown in Figure 2(c).</text>
              <doc_id>109</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, each word pair is associated with a probability to indicate how well they are aligned.</text>
              <doc_id>110</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>For example, in Figure 2(c), we say that the word pair (&#8220;zhongguo&#8221;, &#8220;China&#8221;) is definitely aligned, (&#8220;zhongguo&#8221;, &#8220;the&#8221;) is definitely unaligned, and (&#8220;de&#8221;, &#8220;of&#8221;) has a 60% chance to get aligned.</text>
              <doc_id>111</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Formally, a weighted alignment matrix m is a J &#215; I matrix, in which each element stores a link probability p m (j, i) to indicate how well f j and e i are aligned.</text>
              <doc_id>112</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Currently, we estimate link probabilities from an n-best list by calculating relative frequencies:</text>
              <doc_id>113</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>p m (j, i) = &#8721;</text>
              <doc_id>114</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>a&#8712;N p(a) &#215; &#948;(a, j, i) &#8721;a&#8712;N p(a) (6)</text>
              <doc_id>115</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>= &#8721; p(a) &#215; &#948;(a, j, i) (7)</text>
              <doc_id>116</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>a&#8712;N</text>
              <doc_id>117</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where</text>
              <doc_id>118</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#948;(a, j, i) =</text>
              <doc_id>119</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{ 1 (j, i) &#8712; a 0 otherwise (8)</text>
              <doc_id>120</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Note that N is an n-best list, p(a) is the probability of an alignment a in the n-best list, &#948;(a, j, i) indicates whether a link (j, i) occurs in the alignment a or not.</text>
              <doc_id>121</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We assign 0 to any unseen alignment.</text>
              <doc_id>122</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>As p(a) is usually normalized (i.e., &#8721;</text>
              <doc_id>123</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>a&#8712;N p(a) &#8801; 1), we remove the denominator in</text>
              <doc_id>124</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Eq.</text>
              <doc_id>125</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>(6).</text>
              <doc_id>126</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Accordingly, the probability that the two words f j and e i are not aligned is</text>
              <doc_id>127</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#175;p m (j, i) = 1.0 &#8722; p m (j, i) (9)</text>
              <doc_id>128</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For example, as shown in Figure 2(c), the probability for the two words &#8220;de&#8221; and &#8220;of&#8221; being aligned is 0.6 and the probability that they are not aligned is 0.4.</text>
              <doc_id>129</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Intuitively, the probability of an alignment a is the product of link probabilities.</text>
              <doc_id>130</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>If a link (j, i) occurs in a, we use p m (j, i); otherwise we use &#175;p m (j, i).</text>
              <doc_id>131</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Formally, given a weighted alignment matrix m, the probability of an alignment a can be calculated as</text>
              <doc_id>132</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>J&#8719; I&#8719; p m (a) = (p m (j, i) &#215; &#948;(a, j, i) +</text>
              <doc_id>133</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>j=1 i=1</text>
              <doc_id>134</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#175;p m (j, i) &#215; (1 &#8722; &#948;(a, j, i))) (10)</text>
              <doc_id>135</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>It proves that the sum of all alignment probabilities is always 1: &#8721; a&#8712;A p m(a) &#8801; 1, where A</text>
              <doc_id>136</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>is the set of all possible alignments.</text>
              <doc_id>137</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, a weighted alignment matrix is capable of encoding the probabilities of 2 J&#215;I alignments using only a J &#215; I space.</text>
              <doc_id>138</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Note that p m (a) is not necessarily equal to p(a) because the encoding of a weighted alignment matrix changes the alignment probability distribution.</text>
              <doc_id>139</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For example, while the initial probability of the alignment in Figure 2(a) (i.e., p(a)) is 0.6, the probability of the same alignment encoded in the matrix shown in Figure 2(c) (i.e., p m (a)) becomes 0.1296 according to Eq.</text>
              <doc_id>140</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>(10).</text>
              <doc_id>141</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>It should be emphasized that a weighted matrix encodes all possible alignments rather than the input n-best list, although the link probabilities are estimated from the n-best list.</text>
              <doc_id>142</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Phrase Pair Extraction</title>
        <text>In this section, we describe how to extract phrase pairs from the training corpus annotated with weighted alignment matrices (Section 4.1) and how to estimate their relative frequencies (Section 4.2) and lexical weights (Section 4.3).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this section, we describe how to extract phrase pairs from the training corpus annotated with weighted alignment matrices (Section 4.1) and how to estimate their relative frequencies (Section 4.2) and lexical weights (Section 4.3).</text>
              <doc_id>143</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Extraction Algorithm</title>
            <text>Och and Ney (2004) describe a &#8220;phrase-extract&#8221; algorithm for extracting phrase pairs from a sentence pair annotated with a 1-best alignment. Given a source phrase, they first identify the target phrase that is consistent with the alignment. Then, they expand the boundaries of the target phrase if the boundary words are unaligned.
Unfortunately, this algorithm cannot be directly used to manipulate a weighted alignment matrix, which is a compact representation of all possible alignments. The major difference is that the &#8220;tight&#8221; phrase that has both boundary words aligned is not necessarily the smallest candidate in a weighted matrix. For example, in Figure 2(a), the &#8220;tight&#8221; target phrase corresponding to the source phrase &#8220;zhongguo de&#8221; is &#8220;of China&#8221;. According to Och&#8217;s algorithm, the target phrase &#8220;China&#8221; breaks the alignment consistency and therefore is not valid candidate. However, this is not true for using the weighted matrix shown in Figure 2(c). The target phrase &#8220;China&#8221; is treated as a &#8220;potential&#8221; candidate 1 , although it might be assigned only a small fractional count (see Table 1). Therefore, we enumerate all potential phrase pairs and calculate their fractional counts for eliminating less promising candidates. Figure 3 shows the algorithm for extracting phrases from a weighted matrix. The input of the algorithm is a source sentence f1 J, a target sentence eI 1 , a weighted alignment matrix m, and a phrase length limit l (line 1). After initializing R that stores collected phrase pairs (line 2), we identify the corresponding target phrases for all possible source phrases (lines 3-5). Given a source phrase f j 2
j 1
, we find the lower and upper bounds of target positions (i.e., i l and i u ) that have positive link probabilities (lines 6-8). For example, the lower bound is 3 and the upper bound is 5 for the source phrase &#8220;zhongguo de&#8221; in Figure 2(c). Finally, we enumerate all target phrases that allow for unaligned boundary words with varying phrase lengths (lines 9-14). Note that we need to ensure that 1 &#8804; i 1 &#8804; I and 1 &#8804; i 2 &#8804; I in lines 10-11, which are omitted for simplicity.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Och and Ney (2004) describe a &#8220;phrase-extract&#8221; algorithm for extracting phrase pairs from a sentence pair annotated with a 1-best alignment.</text>
                  <doc_id>144</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Given a source phrase, they first identify the target phrase that is consistent with the alignment.</text>
                  <doc_id>145</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Then, they expand the boundaries of the target phrase if the boundary words are unaligned.</text>
                  <doc_id>146</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Unfortunately, this algorithm cannot be directly used to manipulate a weighted alignment matrix, which is a compact representation of all possible alignments.</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The major difference is that the &#8220;tight&#8221; phrase that has both boundary words aligned is not necessarily the smallest candidate in a weighted matrix.</text>
                  <doc_id>148</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For example, in Figure 2(a), the &#8220;tight&#8221; target phrase corresponding to the source phrase &#8220;zhongguo de&#8221; is &#8220;of China&#8221;.</text>
                  <doc_id>149</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>According to Och&#8217;s algorithm, the target phrase &#8220;China&#8221; breaks the alignment consistency and therefore is not valid candidate.</text>
                  <doc_id>150</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>However, this is not true for using the weighted matrix shown in Figure 2(c).</text>
                  <doc_id>151</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The target phrase &#8220;China&#8221; is treated as a &#8220;potential&#8221; candidate 1 , although it might be assigned only a small fractional count (see Table 1).</text>
                  <doc_id>152</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, we enumerate all potential phrase pairs and calculate their fractional counts for eliminating less promising candidates.</text>
                  <doc_id>153</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 3 shows the algorithm for extracting phrases from a weighted matrix.</text>
                  <doc_id>154</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>The input of the algorithm is a source sentence f1 J, a target sentence eI 1 , a weighted alignment matrix m, and a phrase length limit l (line 1).</text>
                  <doc_id>155</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>After initializing R that stores collected phrase pairs (line 2), we identify the corresponding target phrases for all possible source phrases (lines 3-5).</text>
                  <doc_id>156</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>Given a source phrase f j 2</text>
                  <doc_id>157</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j 1</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>, we find the lower and upper bounds of target positions (i.e., i l and i u ) that have positive link probabilities (lines 6-8).</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For example, the lower bound is 3 and the upper bound is 5 for the source phrase &#8220;zhongguo de&#8221; in Figure 2(c).</text>
                  <doc_id>160</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, we enumerate all target phrases that allow for unaligned boundary words with varying phrase lengths (lines 9-14).</text>
                  <doc_id>161</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Note that we need to ensure that 1 &#8804; i 1 &#8804; I and 1 &#8804; i 2 &#8804; I in lines 10-11, which are omitted for simplicity.</text>
                  <doc_id>162</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Calculating Relative Frequencies</title>
            <text>To estimate the relative frequency of a phrase pair, we need to estimate how often it occurs in the training corpus. Given an n-best list, the fractional count of a phrase pair is the probability sum of the alignments with which the phrase pair is consistent. Obviously, it is unrealistic for a weighted alignment matrix to enumerate all possible alignments explicitly to calculate fractional counts. Instead, we resort to link probabilities to calculate
1 By potential, we mean that the fractional count of a
phrase pair is positive. Section 4.2 describes how to calculate fractional counts.
economy
&#8217;s China
of development the
zhongguo
de jingji 0 0 0 0.4 1.0 0
fazhan
counts efficiently. Equivalent to explicit enumeration, we interpret the fractional count of a phrase pair as the probability that it satisfies the two alignment consistency conditions (see Section 2).
Given a phrase pair, we divide the elements of a weighted alignment matrix into three categories: (1) inside elements that fall inside the phrase pair, (2) outside elements that fall outside the phrase pair while fall in the same row or the same column, and (3) irrelevant elements that fall outside the phrase pair while fall in neither the same row nor the same column. Figure 4 shows an example. Given the phrase pair (&#8220;zhongguo de&#8221;, &#8220;of China&#8221;), we divide the matrix into three areas: inside (heavy shading), outside (light shading), and irrelevant (no shading).
To what extent a phrase pair satisfies the alignment consistency is measured by calculating inside and outside probabilities. Although there are the same terms in the parsing literature, they have different meanings here. The inside probability indicates the chance that there is at least one word inside one phrase aligned to a word inside the other phrase. The outside probability indicates the chance that no words inside one phrase are aligned to a word outside the other phrase.
Given a phrase pair (f j 2
j 1
, e i 2
i1
), we denote the inside area as in(j 1 , j 2 , i 1 , i 2 ) and the outside area as out(j 1 , j 2 , i 1 , i 2 ). Therefore, the inside probability of a phrase pair is calculated as
&#945;(j 1 , j 2 , i 1 , i 2 ) = 1 &#8722; &#8719;
(j,i)&#8712;in(j 1 ,j 2 ,i 1 ,i 2 )
&#175;p m (j, i) (11)
For example, the inside probability for (&#8220;zhongguo de&#8221;, &#8220;of China&#8221;) in Figure 4 is 1.0, which means that there always exists at least one aligned word pair inside.
Accordingly, the outside probability of a phrase pair is calculated as &#8719; &#946;(j 1 , j 2 , i 1 , i 2 ) = &#175;p m (j, i) (12)
(j,i)&#8712;out(j 1 ,j 2 ,i 1 ,i 2 )
For example, the outside probability for (&#8220;zhongguo de&#8221;, &#8220;of China&#8221;) in Figure 4 is 0.36, which means the probability that there are no aligned word pairs outside is 0.36.
Finally, we use the product of inside and outside probabilities as the fractional count of a phrase pair:
count(f j 2
j 1
, e i 2
i1
) = &#945;(j 1 , j 2 , i 1 , i 2 ) &#215;
&#946;(j 1 , j 2 , i 1 , i 2 ) (13)
Table 1 lists some candidate target phrases of the source phrase &#8220;zhongguo de&#8221; in Figure 4. We also give their inside probabilities, outside probabilities, and fractional counts. After collecting the fractional counts from the training corpus, we then use Eq. (2) to calculate relative frequencies in two translation directions.
Often, our approach extracts a large amount of phrase pairs from training corpus as we soften the alignment consistency constraint. To maintain a reasonable phrase table size, we discard any phrase pair that has a fractional count lower than a threshold t. During extraction, we first obtain a list of candidate target phrases for each source phrase, as shown in Table 1. Then, we prune the list according to the threshold t. For example, we only retain the top two candidates in Table 1 if t = 0.3. Note that we perform the pruning locally. Although it is more reasonable to prune a phrase table after accumulating all fractional counts from
training corpus, such global pruning strategy usually leads to very large disk and memory requirements.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>To estimate the relative frequency of a phrase pair, we need to estimate how often it occurs in the training corpus.</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Given an n-best list, the fractional count of a phrase pair is the probability sum of the alignments with which the phrase pair is consistent.</text>
                  <doc_id>164</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Obviously, it is unrealistic for a weighted alignment matrix to enumerate all possible alignments explicitly to calculate fractional counts.</text>
                  <doc_id>165</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Instead, we resort to link probabilities to calculate</text>
                  <doc_id>166</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 By potential, we mean that the fractional count of a</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>phrase pair is positive.</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Section 4.2 describes how to calculate fractional counts.</text>
                  <doc_id>169</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>economy</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8217;s China</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>of development the</text>
                  <doc_id>172</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>zhongguo</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>de jingji 0 0 0 0.4 1.0 0</text>
                  <doc_id>174</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>fazhan</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>counts efficiently.</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Equivalent to explicit enumeration, we interpret the fractional count of a phrase pair as the probability that it satisfies the two alignment consistency conditions (see Section 2).</text>
                  <doc_id>177</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Given a phrase pair, we divide the elements of a weighted alignment matrix into three categories: (1) inside elements that fall inside the phrase pair, (2) outside elements that fall outside the phrase pair while fall in the same row or the same column, and (3) irrelevant elements that fall outside the phrase pair while fall in neither the same row nor the same column.</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 4 shows an example.</text>
                  <doc_id>179</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Given the phrase pair (&#8220;zhongguo de&#8221;, &#8220;of China&#8221;), we divide the matrix into three areas: inside (heavy shading), outside (light shading), and irrelevant (no shading).</text>
                  <doc_id>180</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To what extent a phrase pair satisfies the alignment consistency is measured by calculating inside and outside probabilities.</text>
                  <doc_id>181</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Although there are the same terms in the parsing literature, they have different meanings here.</text>
                  <doc_id>182</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The inside probability indicates the chance that there is at least one word inside one phrase aligned to a word inside the other phrase.</text>
                  <doc_id>183</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The outside probability indicates the chance that no words inside one phrase are aligned to a word outside the other phrase.</text>
                  <doc_id>184</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Given a phrase pair (f j 2</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j 1</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>, e i 2</text>
                  <doc_id>187</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i1</text>
                  <doc_id>188</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>), we denote the inside area as in(j 1 , j 2 , i 1 , i 2 ) and the outside area as out(j 1 , j 2 , i 1 , i 2 ).</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, the inside probability of a phrase pair is calculated as</text>
                  <doc_id>190</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#945;(j 1 , j 2 , i 1 , i 2 ) = 1 &#8722; &#8719;</text>
                  <doc_id>191</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(j,i)&#8712;in(j 1 ,j 2 ,i 1 ,i 2 )</text>
                  <doc_id>192</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#175;p m (j, i) (11)</text>
                  <doc_id>193</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For example, the inside probability for (&#8220;zhongguo de&#8221;, &#8220;of China&#8221;) in Figure 4 is 1.0, which means that there always exists at least one aligned word pair inside.</text>
                  <doc_id>194</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Accordingly, the outside probability of a phrase pair is calculated as &#8719; &#946;(j 1 , j 2 , i 1 , i 2 ) = &#175;p m (j, i) (12)</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(j,i)&#8712;out(j 1 ,j 2 ,i 1 ,i 2 )</text>
                  <doc_id>196</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For example, the outside probability for (&#8220;zhongguo de&#8221;, &#8220;of China&#8221;) in Figure 4 is 0.36, which means the probability that there are no aligned word pairs outside is 0.36.</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Finally, we use the product of inside and outside probabilities as the fractional count of a phrase pair:</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>count(f j 2</text>
                  <doc_id>199</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j 1</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>, e i 2</text>
                  <doc_id>201</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i1</text>
                  <doc_id>202</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>) = &#945;(j 1 , j 2 , i 1 , i 2 ) &#215;</text>
                  <doc_id>203</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#946;(j 1 , j 2 , i 1 , i 2 ) (13)</text>
                  <doc_id>204</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 1 lists some candidate target phrases of the source phrase &#8220;zhongguo de&#8221; in Figure 4.</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We also give their inside probabilities, outside probabilities, and fractional counts.</text>
                  <doc_id>206</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>After collecting the fractional counts from the training corpus, we then use Eq.</text>
                  <doc_id>207</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>(2) to calculate relative frequencies in two translation directions.</text>
                  <doc_id>208</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Often, our approach extracts a large amount of phrase pairs from training corpus as we soften the alignment consistency constraint.</text>
                  <doc_id>209</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To maintain a reasonable phrase table size, we discard any phrase pair that has a fractional count lower than a threshold t.</text>
                  <doc_id>210</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>During extraction, we first obtain a list of candidate target phrases for each source phrase, as shown in Table 1.</text>
                  <doc_id>211</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Then, we prune the list according to the threshold t.</text>
                  <doc_id>212</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For example, we only retain the top two candidates in Table 1 if t = 0.3.</text>
                  <doc_id>213</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Note that we perform the pruning locally.</text>
                  <doc_id>214</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Although it is more reasonable to prune a phrase table after accumulating all fractional counts from</text>
                  <doc_id>215</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>training corpus, such global pruning strategy usually leads to very large disk and memory requirements.</text>
                  <doc_id>216</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>4.3 Calculating Lexical Weights</title>
            <text>Recall that we need to obtain two translation probability tables w(e|f) and w(f |e) before calculating lexical weights (see Section 2). Following Koehn et al. (2003), we estimate the two distributions by relative frequencies from the training corpus annotated with weighted alignment matrices. In other words, we still use Eq. (3) but the way of calculating fractional counts is different now.
Given a source word f j , a target word e i , and a weighted alignment matrix, the fractional count count(f j , e i ) is p m (j, i). For NULL words, the fractional counts can be calculated as I&#8719; count(f j , e 0 ) = &#175;p m (j, i) (14)
count(f 0 , e i ) =
i=1
J&#8719;
j=1
&#175;p m (j, i) (15)
i=1
&#8721;
&#8704;j:p m(j,i)&gt;0
p(e i |f 0 ) &#215;
) p(e i |f j ) &#215; p m (j, i) +
| &#732;f| &#8719;
j=1
&#175;p m (j, i)
For example, for the target word &#8220;of&#8221; in Figure 4, the sum of aligned and unaligned probabilities is 1
&#215; (p(of|de) &#215; 0.6 + p(of|fazhan) &#215; 0.4) + 2 p(of|NULL) &#215; 0.24
Note that we take link probabilities into account and calculate the probability that a target word translates a source NULL token explicitly.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Recall that we need to obtain two translation probability tables w(e|f) and w(f |e) before calculating lexical weights (see Section 2).</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Following Koehn et al. (2003), we estimate the two distributions by relative frequencies from the training corpus annotated with weighted alignment matrices.</text>
                  <doc_id>218</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In other words, we still use Eq.</text>
                  <doc_id>219</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>(3) but the way of calculating fractional counts is different now.</text>
                  <doc_id>220</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Given a source word f j , a target word e i , and a weighted alignment matrix, the fractional count count(f j , e i ) is p m (j, i).</text>
                  <doc_id>221</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For NULL words, the fractional counts can be calculated as I&#8719; count(f j , e 0 ) = &#175;p m (j, i) (14)</text>
                  <doc_id>222</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>count(f 0 , e i ) =</text>
                  <doc_id>223</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>224</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>J&#8719;</text>
                  <doc_id>225</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=1</text>
                  <doc_id>226</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#175;p m (j, i) (15)</text>
                  <doc_id>227</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>228</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;</text>
                  <doc_id>229</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8704;j:p m(j,i)&gt;0</text>
                  <doc_id>230</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(e i |f 0 ) &#215;</text>
                  <doc_id>231</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>) p(e i |f j ) &#215; p m (j, i) +</text>
                  <doc_id>232</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>| &#732;f| &#8719;</text>
                  <doc_id>233</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=1</text>
                  <doc_id>234</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#175;p m (j, i)</text>
                  <doc_id>235</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For example, for the target word &#8220;of&#8221; in Figure 4, the sum of aligned and unaligned probabilities is 1</text>
                  <doc_id>236</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#215; (p(of|de) &#215; 0.6 + p(of|fazhan) &#215; 0.4) + 2 p(of|NULL) &#215; 0.24</text>
                  <doc_id>237</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Note that we take link probabilities into account and calculate the probability that a target word translates a source NULL token explicitly.</text>
                  <doc_id>238</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Experiments</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>239</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Data Preparation</title>
            <text>We evaluated our approach on Chinese-to-English translation. We used the FBIS corpus (6.9M )
(16)
+ 8.9M words) as the training data. For language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD corpus. We used the NIST 2002 MT evaluation test set as our development set, and used the NIST 2005 test set as our test set. We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002).
To obtain weighted alignment matrices, we followed Venugopal et al. (2008) to produce n- best lists via GIZA++. We first ran GIZA++ to produce 50-best lists in two translation directions. Then, we used the refinement technique &#8220;grow-diag-final-and&#8221; (Koehn et al., 2003) to all 50 &#215; 50 bidirectional alignment pairs. Suppose that p s2t and p t2s are the probabilities of an alignment pair assigned by GIZA++, respectively. We used p s2t &#215; p t2s as the probability of the resulting symmetric alignment. As different alignment pairs might produce the same symmetric alignments, we followed Venugopal et al. (2008) to remove duplicate alignments and retain only the alignment with the highest probability. Therefore, there were 550 candidate alignments on average for each sentence pair in the training data. We obtained n-best lists by selecting the top n alignments from the 550-best lists. The probability of each alignment in the n-best list was re-estimated by re-normalization (Venugopal et al., 2008). Finally, these n-best alignments served as samples for constructing weighted alignment matrices. After extracting phrase pairs from n-best lists and weighted alignment matrices, we ran Moses (Koehn et al., 2007) to translate the development and test sets. We used the simple distance-based reordering model to remove the dependency of lexicalization on word alignments for Moses.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We evaluated our approach on Chinese-to-English translation.</text>
                  <doc_id>240</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We used the FBIS corpus (6.9M )</text>
                  <doc_id>241</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(16)</text>
                  <doc_id>242</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>+ 8.9M words) as the training data.</text>
                  <doc_id>243</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD corpus.</text>
                  <doc_id>244</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We used the NIST 2002 MT evaluation test set as our development set, and used the NIST 2005 test set as our test set.</text>
                  <doc_id>245</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002).</text>
                  <doc_id>246</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To obtain weighted alignment matrices, we followed Venugopal et al. (2008) to produce n- best lists via GIZA++.</text>
                  <doc_id>247</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We first ran GIZA++ to produce 50-best lists in two translation directions.</text>
                  <doc_id>248</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Then, we used the refinement technique &#8220;grow-diag-final-and&#8221; (Koehn et al., 2003) to all 50 &#215; 50 bidirectional alignment pairs.</text>
                  <doc_id>249</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Suppose that p s2t and p t2s are the probabilities of an alignment pair assigned by GIZA++, respectively.</text>
                  <doc_id>250</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We used p s2t &#215; p t2s as the probability of the resulting symmetric alignment.</text>
                  <doc_id>251</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>As different alignment pairs might produce the same symmetric alignments, we followed Venugopal et al. (2008) to remove duplicate alignments and retain only the alignment with the highest probability.</text>
                  <doc_id>252</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, there were 550 candidate alignments on average for each sentence pair in the training data.</text>
                  <doc_id>253</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>We obtained n-best lists by selecting the top n alignments from the 550-best lists.</text>
                  <doc_id>254</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>The probability of each alignment in the n-best list was re-estimated by re-normalization (Venugopal et al., 2008).</text>
                  <doc_id>255</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, these n-best alignments served as samples for constructing weighted alignment matrices.</text>
                  <doc_id>256</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>After extracting phrase pairs from n-best lists and weighted alignment matrices, we ran Moses (Koehn et al., 2007) to translate the development and test sets.</text>
                  <doc_id>257</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>We used the simple distance-based reordering model to remove the dependency of lexicalization on word alignments for Moses.</text>
                  <doc_id>258</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Effect of Pruning Threshold</title>
            <text>Our first experiment investigated the effect of pruning threshold on translation quality (BLEU scores on the test set) and the phrase table size (filtered for the test set), as shown in Figure 5. To save time, we extracted phrase pairs just from the first 10K sentence pairs of the FBIS corpus. We used 12 different thresholds: 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, and 0.9. Obviously, the lower the threshold is, the more phrase pairs are extracted. When t = 0.0001, the number of phrase pairs used on the test set was 460,284
and the BLEU score was 20.55. Generally, both the number of phrase pairs and the BLEU score went down with the increase of t. However, this trend did not hold within the range [0.1, 0.9]. To achieve a good tradeoff between translation quality and phrase table size, we set t = 0.01 for the following experiments.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Our first experiment investigated the effect of pruning threshold on translation quality (BLEU scores on the test set) and the phrase table size (filtered for the test set), as shown in Figure 5.</text>
                  <doc_id>259</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To save time, we extracted phrase pairs just from the first 10K sentence pairs of the FBIS corpus.</text>
                  <doc_id>260</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We used 12 different thresholds: 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, and 0.9.</text>
                  <doc_id>261</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Obviously, the lower the threshold is, the more phrase pairs are extracted.</text>
                  <doc_id>262</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>When t = 0.0001, the number of phrase pairs used on the test set was 460,284</text>
                  <doc_id>263</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and the BLEU score was 20.55.</text>
                  <doc_id>264</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Generally, both the number of phrase pairs and the BLEU score went down with the increase of t.</text>
                  <doc_id>265</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, this trend did not hold within the range [0.1, 0.9].</text>
                  <doc_id>266</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>To achieve a good tradeoff between translation quality and phrase table size, we set t = 0.01 for the following experiments.</text>
                  <doc_id>267</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>5.3 N-best lists Vs. Weighted Matrices</title>
            <text>Figure 6 shows the BLEU scores and average extraction time using n-best alignments and weighted matrices, respectively. We used the entire training data for phrase extraction. When using 1-best alignments, Moses achieved a BLEU score of 0.2826 and the average extraction time was 4.19 milliseconds per sentence pair (see point n = 1). The BLEU scores rose with the increase of n for using n-best alignments. However, the score went down slightly when n = 50. This suggests that including more noisy alignments might be harmful. These improvements over 1-best alignments are not statistically significant. This finding failed to echo the promising results reported by Venogopal et al. (2008). We think that there are two possible reasons. First, they evaluated their approach on the IWSLT data while we used the NIST data. It might be easier to obtain significant improvements on the IWSLT data in which the sentences are shorter. Second, they used the hierarchical phrase-based system while we used the phrase-based system, which might be less sensitive to word alignments because the alignments inside the phrase pairs hardly have an effect. When using weighted alignment matrices, we Figure 6: Comparison of n-best alignments and weighted alignment matrices. We use m(n) to denote the matrices that take n-best lists as samples.
obtained higher BLEU scores than using n-best lists with much less extraction time. We achieved a BLEU score of 0.2901 when using the weighted matrices estimated from 10-best lists. The absolute improvement of 0.75 over using 1-best alignments (from 0.2826 to 0.2901) is statistically significant at p &lt; 0.05 by using sign-test (Collins et al., 2005). Although the improvements over n- best lists are not always statistically significant, weighted alignment matrices maintain consistent superiority in both translation quality and extraction speed.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Figure 6 shows the BLEU scores and average extraction time using n-best alignments and weighted matrices, respectively.</text>
                  <doc_id>268</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We used the entire training data for phrase extraction.</text>
                  <doc_id>269</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>When using 1-best alignments, Moses achieved a BLEU score of 0.2826 and the average extraction time was 4.19 milliseconds per sentence pair (see point n = 1).</text>
                  <doc_id>270</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The BLEU scores rose with the increase of n for using n-best alignments.</text>
                  <doc_id>271</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>However, the score went down slightly when n = 50.</text>
                  <doc_id>272</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This suggests that including more noisy alignments might be harmful.</text>
                  <doc_id>273</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>These improvements over 1-best alignments are not statistically significant.</text>
                  <doc_id>274</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>This finding failed to echo the promising results reported by Venogopal et al. (2008).</text>
                  <doc_id>275</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>We think that there are two possible reasons.</text>
                  <doc_id>276</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>First, they evaluated their approach on the IWSLT data while we used the NIST data.</text>
                  <doc_id>277</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>It might be easier to obtain significant improvements on the IWSLT data in which the sentences are shorter.</text>
                  <doc_id>278</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>Second, they used the hierarchical phrase-based system while we used the phrase-based system, which might be less sensitive to word alignments because the alignments inside the phrase pairs hardly have an effect.</text>
                  <doc_id>279</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
                <sentence>
                  <text>When using weighted alignment matrices, we Figure 6: Comparison of n-best alignments and weighted alignment matrices.</text>
                  <doc_id>280</doc_id>
                  <sec_id>12</sec_id>
                </sentence>
                <sentence>
                  <text>We use m(n) to denote the matrices that take n-best lists as samples.</text>
                  <doc_id>281</doc_id>
                  <sec_id>13</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>obtained higher BLEU scores than using n-best lists with much less extraction time.</text>
                  <doc_id>282</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We achieved a BLEU score of 0.2901 when using the weighted matrices estimated from 10-best lists.</text>
                  <doc_id>283</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The absolute improvement of 0.75 over using 1-best alignments (from 0.2826 to 0.2901) is statistically significant at p &lt; 0.05 by using sign-test (Collins et al., 2005).</text>
                  <doc_id>284</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Although the improvements over n- best lists are not always statistically significant, weighted alignment matrices maintain consistent superiority in both translation quality and extraction speed.</text>
                  <doc_id>285</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>5.4 Comparison of Parameter Estimation</title>
            <text>In theory, the set of phrase pairs extracted from n- best alignments is the subset of the set extracted from the corresponding weighted matrices. In practice, however, this is not true because we use the pruning threshold t to maintain a reasonable table size. Even so, the phrase tables produced by n-best lists and weighted matrices still share many phrase pairs. Table 2 gives some statistics. We use m(10) to represent the weighted matrices estimated from 10-best lists. &#8220;all&#8221; denotes the full phrase table, &#8220;shared&#8221; denotes the intersection of two tables, and &#8220;non-shared&#8221; denotes the complement. Note that the probabilities of &#8220;shared&#8221; phrase pairs are different for the two approaches. We obtained 6.13M and 6.34M phrase pairs for the test set by using 10-best lists and the corresponding matrices, respectively. There were 4.58M phrase pairs included by both tables. Note that the relative frequencies and lexical weights for the same phrase
BLEU score
0.290 0.280 0.270 0.260 0.250 0.240 0.230 0.220 0.210 0.200 1-best 10-best m(10)
0 50 100 150 200 250
training corpus size (10 3 )
pairs might be different in two tables. We found that using matrices outperformed using n-best lists even with the same phrase pairs. This suggests that our methods for parameter estimation make better use of noisy data. Another interesting finding was that using the shared phrase pairs achieved almost the same results with using full phrase tables.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In theory, the set of phrase pairs extracted from n- best alignments is the subset of the set extracted from the corresponding weighted matrices.</text>
                  <doc_id>286</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In practice, however, this is not true because we use the pruning threshold t to maintain a reasonable table size.</text>
                  <doc_id>287</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Even so, the phrase tables produced by n-best lists and weighted matrices still share many phrase pairs.</text>
                  <doc_id>288</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Table 2 gives some statistics.</text>
                  <doc_id>289</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We use m(10) to represent the weighted matrices estimated from 10-best lists.</text>
                  <doc_id>290</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>&#8220;all&#8221; denotes the full phrase table, &#8220;shared&#8221; denotes the intersection of two tables, and &#8220;non-shared&#8221; denotes the complement.</text>
                  <doc_id>291</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Note that the probabilities of &#8220;shared&#8221; phrase pairs are different for the two approaches.</text>
                  <doc_id>292</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>We obtained 6.13M and 6.34M phrase pairs for the test set by using 10-best lists and the corresponding matrices, respectively.</text>
                  <doc_id>293</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>There were 4.58M phrase pairs included by both tables.</text>
                  <doc_id>294</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>Note that the relative frequencies and lexical weights for the same phrase</text>
                  <doc_id>295</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BLEU score</text>
                  <doc_id>296</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.290 0.280 0.270 0.260 0.250 0.240 0.230 0.220 0.210 0.200 1-best 10-best m(10)</text>
                  <doc_id>297</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0 50 100 150 200 250</text>
                  <doc_id>298</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>training corpus size (10 3 )</text>
                  <doc_id>299</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>pairs might be different in two tables.</text>
                  <doc_id>300</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We found that using matrices outperformed using n-best lists even with the same phrase pairs.</text>
                  <doc_id>301</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This suggests that our methods for parameter estimation make better use of noisy data.</text>
                  <doc_id>302</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Another interesting finding was that using the shared phrase pairs achieved almost the same results with using full phrase tables.</text>
                  <doc_id>303</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>4</index>
            <title>5.5 Effect of Training Corpus Size</title>
            <text>To investigate the effect of training corpus size on our approach, we extracted phrase pairs from n- best lists and weighted matrices trained on five training corpora with varying sizes: 10K, 50K, 100K, 150K, and 239K sentence pairs. As shown in Figure 7, our approach outperformed both 1- best and n-best lists consistently. More importantly, the gains seem increase when more training data are used.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>To investigate the effect of training corpus size on our approach, we extracted phrase pairs from n- best lists and weighted matrices trained on five training corpora with varying sizes: 10K, 50K, 100K, 150K, and 239K sentence pairs.</text>
                  <doc_id>304</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As shown in Figure 7, our approach outperformed both 1- best and n-best lists consistently.</text>
                  <doc_id>305</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>More importantly, the gains seem increase when more training data are used.</text>
                  <doc_id>306</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>5</index>
            <title>5.6 Results on Other Language Pairs</title>
            <text>To further examine the efficacy of the proposed approach, we scaled our experiments to large data with multiple language pairs. We used the Europarl training corpus from the WMT07 shared
task. 2 Table 3 shows the statistics of the training data. There are four languages (Spanish, French, German, and English) and six translation directions (Foreign-to-English and Englishto-Foreign). We used the &#8220;dev2006&#8221; data in the &#8220;dev&#8221; directory as the development set and the &#8220;test2006&#8221; data in the &#8220;devtest&#8221; directory as the test set. Both the development and test sets contain 2,000 sentences with single reference translations. We tokenized and lowercased all the training, development, and test data. We trained a 4-gram language model using SRI Language Modeling Toolkit on the target side of the training corpus for each task. We ran GIZA++ on the entire training data to obtain n-best alignments and weighted matrices. To save time, we just used the first 100K sentences of each aligned training corpus to extract phrase pairs.
2 http://www.statmt.org/wmt07/shared-task.html
Table 4 lists the case-insensitive BLEU scores of 1-best, 10-best, and m(10) on the Europarl data. Using weighted packed matrices continued to show advantage over using 1-best alignments on multiple language pairs. However, these improvements were very small and not significant. We attribute this to the fact that GIZA++ usually produces high quality 1-best alignments for closelyrelated European language pairs, especially when trained on millions of sentences.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>To further examine the efficacy of the proposed approach, we scaled our experiments to large data with multiple language pairs.</text>
                  <doc_id>307</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We used the Europarl training corpus from the WMT07 shared</text>
                  <doc_id>308</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>task.</text>
                  <doc_id>309</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>2 Table 3 shows the statistics of the training data.</text>
                  <doc_id>310</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>There are four languages (Spanish, French, German, and English) and six translation directions (Foreign-to-English and Englishto-Foreign).</text>
                  <doc_id>311</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We used the &#8220;dev2006&#8221; data in the &#8220;dev&#8221; directory as the development set and the &#8220;test2006&#8221; data in the &#8220;devtest&#8221; directory as the test set.</text>
                  <doc_id>312</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Both the development and test sets contain 2,000 sentences with single reference translations.</text>
                  <doc_id>313</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We tokenized and lowercased all the training, development, and test data.</text>
                  <doc_id>314</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>We trained a 4-gram language model using SRI Language Modeling Toolkit on the target side of the training corpus for each task.</text>
                  <doc_id>315</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>We ran GIZA++ on the entire training data to obtain n-best alignments and weighted matrices.</text>
                  <doc_id>316</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>To save time, we just used the first 100K sentences of each aligned training corpus to extract phrase pairs.</text>
                  <doc_id>317</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 http://www.statmt.org/wmt07/shared-task.html</text>
                  <doc_id>318</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 4 lists the case-insensitive BLEU scores of 1-best, 10-best, and m(10) on the Europarl data.</text>
                  <doc_id>319</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Using weighted packed matrices continued to show advantage over using 1-best alignments on multiple language pairs.</text>
                  <doc_id>320</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, these improvements were very small and not significant.</text>
                  <doc_id>321</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We attribute this to the fact that GIZA++ usually produces high quality 1-best alignments for closelyrelated European language pairs, especially when trained on millions of sentences.</text>
                  <doc_id>322</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>6</index>
        <title>6 Related Work</title>
        <text>Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al., 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and n-best alignments instead of 1-best alignments (Venugopal et al., 2008). We propose a compact representation of multiple word alignments that enables SMT systems to make a better use of noisy alignments.
Matusov et al. (2004) propose &#8220;cost matrices&#8221; for producing symmetric alignments. Kumar et al. (2007) describe how to use &#8220;posterior probability matrices&#8221; to improve alignment accuracy via a bridge language. Although not using the term &#8221;weighted matrices&#8221; directly, they both assign a probability to each word pair.
We follow Och and Ney (2004) to develop a new phrase extraction algorithm for weighted alignment matrices. The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al., 2003) are also adapted for the weighted matrix case.
Many researchers (e.g., (Venugopal et al., 2003; Deng et al., 2008)) observe that softening the alignment consistency constraint help improve translation quality. For example, Deng et al. (2008) define a feature named &#8220;within phrase pair consistency ratio&#8221; to measure the degree of consistency. As each link is associated with a probability in a weighted matrix, we use these probabilities to evaluate the validity of a phrase pair.
We estimate the link probabilities by calculating relative frequencies over n-best lists. Niehues and Vogel (2008) propose a discriminative approach to modeling the alignment matrix directly. The difference is that they assign a boolean value instead of a probability to each word pair.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al., 2008), word lattices instead of 1-best segmentations (Dyer et al., 2008), and n-best alignments instead of 1-best alignments (Venugopal et al., 2008).</text>
              <doc_id>323</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We propose a compact representation of multiple word alignments that enables SMT systems to make a better use of noisy alignments.</text>
              <doc_id>324</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Matusov et al. (2004) propose &#8220;cost matrices&#8221; for producing symmetric alignments.</text>
              <doc_id>325</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Kumar et al. (2007) describe how to use &#8220;posterior probability matrices&#8221; to improve alignment accuracy via a bridge language.</text>
              <doc_id>326</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Although not using the term &#8221;weighted matrices&#8221; directly, they both assign a probability to each word pair.</text>
              <doc_id>327</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We follow Och and Ney (2004) to develop a new phrase extraction algorithm for weighted alignment matrices.</text>
              <doc_id>328</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al., 2003) are also adapted for the weighted matrix case.</text>
              <doc_id>329</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Many researchers (e.g., (Venugopal et al., 2003; Deng et al., 2008)) observe that softening the alignment consistency constraint help improve translation quality.</text>
              <doc_id>330</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For example, Deng et al. (2008) define a feature named &#8220;within phrase pair consistency ratio&#8221; to measure the degree of consistency.</text>
              <doc_id>331</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>As each link is associated with a probability in a weighted matrix, we use these probabilities to evaluate the validity of a phrase pair.</text>
              <doc_id>332</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We estimate the link probabilities by calculating relative frequencies over n-best lists.</text>
              <doc_id>333</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Niehues and Vogel (2008) propose a discriminative approach to modeling the alignment matrix directly.</text>
              <doc_id>334</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The difference is that they assign a boolean value instead of a probability to each word pair.</text>
              <doc_id>335</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>7</index>
        <title>7 Conclusion and Future Work</title>
        <text>We have presented a new structure called weighted alignment matrix that encodes the alignment distribution for a sentence pair. Accordingly, we develop new methods for extracting phrase pairs and estimating their probabilities. Our experiments show that the proposed approach achieves better translation quality over using n-best lists in less extraction time. An interesting finding is that our approach performs better than the baseline even they use the same phrase pairs. Although our approach consistently outperforms using 1-best alignments for varying language pairs, the improvements are comparatively small. One possible reason is that taking n-best lists as samples sometimes might change alignment probability distributions inappropriately. A more principled solution is to directly model the weighted alignment matrices, either in a generative or a discriminative way. We believe that better estimation of alignment distributions will result in more significant improvements. Another interesting direction is applying our approach to extracting translation rules with hierarchical structures such as hierarchical phrases (Chiang, 2007) and tree-to-string rules (Galley et al., 2006; Liu et al., 2006). We expect that these syntax-based systems could benefit more from our approach.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have presented a new structure called weighted alignment matrix that encodes the alignment distribution for a sentence pair.</text>
              <doc_id>336</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Accordingly, we develop new methods for extracting phrase pairs and estimating their probabilities.</text>
              <doc_id>337</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Our experiments show that the proposed approach achieves better translation quality over using n-best lists in less extraction time.</text>
              <doc_id>338</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>An interesting finding is that our approach performs better than the baseline even they use the same phrase pairs.</text>
              <doc_id>339</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Although our approach consistently outperforms using 1-best alignments for varying language pairs, the improvements are comparatively small.</text>
              <doc_id>340</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>One possible reason is that taking n-best lists as samples sometimes might change alignment probability distributions inappropriately.</text>
              <doc_id>341</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>A more principled solution is to directly model the weighted alignment matrices, either in a generative or a discriminative way.</text>
              <doc_id>342</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>We believe that better estimation of alignment distributions will result in more significant improvements.</text>
              <doc_id>343</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Another interesting direction is applying our approach to extracting translation rules with hierarchical structures such as hierarchical phrases (Chiang, 2007) and tree-to-string rules (Galley et al., 2006; Liu et al., 2006).</text>
              <doc_id>344</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>We expect that these syntax-based systems could benefit more from our approach.</text>
              <doc_id>345</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>8</index>
        <title>Acknowledgement</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>346</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Some candidate target phrases of the source phrase &#8220;zhongguo de&#8221; in Figure 4, where &#945; is inside probability, &#946; is outside probability, and count is fractional count.</caption>
        <reference_text></reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>target phrase</cell>
              <cell>&#945;</cell>
              <cell>&#946;</cell>
              <cell>count</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>of China</cell>
              <cell>1.0</cell>
              <cell>0.36</cell>
              <cell>0.36</cell>
            </row>
            <row>
              <cell>of China &#8217;s</cell>
              <cell>1.0</cell>
              <cell>0.36</cell>
              <cell>0.36</cell>
            </row>
            <row>
              <cell>China &#8217;s</cell>
              <cell>1.0</cell>
              <cell>0.24</cell>
              <cell>0.24</cell>
            </row>
            <row>
              <cell>China</cell>
              <cell>1.0</cell>
              <cell>0.24</cell>
              <cell>0.24</cell>
            </row>
            <row>
              <cell>&#8217;s economy</cell>
              <cell>0.4</cell>
              <cell>0</cell>
              <cell>0</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Comparison of phrase tables learned from n-best lists and weighted matrices. We use m(10) to represent the weighted matrices estimated from 10-best lists. &#8220;all&#8221; denotes the full phrase table, &#8220;shared&#8221; denotes the intersection of two tables, and &#8220;non-shared&#8221; denotes the complement. Note that the probabilities of &#8220;shared&#8221; phrase pairs are different for the two approaches.</caption>
        <reference_text>In PAGE 7: ... Even so, the phrase tables produced by n-best lists and weighted matrices still share many phrase pairs.  Table2  gives some statistics. We use m(10) to represent the weighted matrices estimated from 10-best lists....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>B</cell>
              <cell>L</cell>
              <cell>E</cell>
              <cell>U</cell>
              <cell></cell>
              <cell>s</cell>
              <cell>c</cell>
              <cell>o</cell>
              <cell>r</cell>
              <cell>e</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>1-best</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>10-bestm(10)</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>0</cell>
              <cell>50</cell>
              <cell>100</cell>
              <cell>150</cell>
              <cell>200</cell>
              <cell>250</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>training corpus size (103)</cell>
              <cell>training corpus size (103)</cell>
              <cell>training corpus size (103)</cell>
              <cell>training corpus size (103)</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TET</source>
        <caption>Table 3: Statistics of the Europarl training data. &#8220;S&#8221; denotes Spanish, &#8220;E&#8221; denotes English, &#8220;F&#8221; denotes French, &#8220;G&#8221; denotes German.</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell>S&#8596;E</cell>
              <cell>F&#8596;E</cell>
              <cell>G&#8596;E</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Sentences</cell>
              <cell>1.26M</cell>
              <cell>1.29M</cell>
              <cell>1.26M</cell>
            </row>
            <row>
              <cell>Foreign words</cell>
              <cell>33.16M</cell>
              <cell>33.18M</cell>
              <cell>29.58M</cell>
            </row>
            <row>
              <cell>English words</cell>
              <cell>31.81M</cell>
              <cell>32.62M</cell>
              <cell>31.93M</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 3: Statistics of the Europarl training data.  S  denotes Spanish,  E  denotes English,  F  de- notes French,  G  denotes German.#@#@Table 4: BLEU scores (case-insensitive) on the Europarl data. &#8220;S&#8221; denotes Spanish, &#8220;E&#8221; denotes English, &#8220;F&#8221; denotes French, &#8220;G&#8221; denotes German.</caption>
        <reference_text>In PAGE 8: ... task. 2  Table3  shows the statistics of the train- ing data. There are four languages (Spanish, French, German, and English) and six transla- tion directions (Foreign-to-English and English- to-Foreign)....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell>1-best</cell>
              <cell>10-best</cell>
              <cell>m(10)</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>S?E#@#@S&#8594;E</cell>
              <cell>30.90</cell>
              <cell>30.97</cell>
              <cell>31.03</cell>
            </row>
            <row>
              <cell>E?S#@#@E&#8594;S</cell>
              <cell>31.16</cell>
              <cell>31.25</cell>
              <cell>31.34</cell>
            </row>
            <row>
              <cell>F?E#@#@F&#8594;E</cell>
              <cell>30.69</cell>
              <cell>30.76</cell>
              <cell>30.82</cell>
            </row>
            <row>
              <cell>E?F#@#@E&#8594;F</cell>
              <cell>26.42</cell>
              <cell>26.65</cell>
              <cell>26.54</cell>
            </row>
            <row>
              <cell>G?E#@#@G&#8594;E</cell>
              <cell>24.46</cell>
              <cell>24.58</cell>
              <cell>24.66</cell>
            </row>
            <row>
              <cell>E?G#@#@E&#8594;G</cell>
              <cell>18.03</cell>
              <cell>18.30</cell>
              <cell>18.20</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Phil Blunsom</author>
          <author>Trevor Cohn</author>
        </authors>
        <title>Discriminative word alignment with conditional random fields.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>F Brown</author>
          <author>Stephen A Della Pietra</author>
          <author>J Vincent</author>
        </authors>
        <title>None</title>
        <publication>In Proceedings of COLING/ACL</publication>
        <pages>65--72</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Della Pietra</author>
          <author>Robert L Mercer</author>
        </authors>
        <title>The mathematics of statistical machine translation: Parameter estimation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1993</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>Hierarchical phrase-based translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Michael Collins</author>
          <author>Philipp Koehn</author>
          <author>Ivona Ku&#728;cerov&#225;</author>
        </authors>
        <title>Clause restructuring for statistical machine translation.</title>
        <publication>In Proceedings of ACL 2005,</publication>
        <pages>531--540</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Yonggang Deng</author>
          <author>Jia Xu</author>
          <author>Yuqing Gao</author>
        </authors>
        <title>Phrase table training for precision and recall: What makes a good phrase and a good phrase pair?</title>
        <publication>In Proceedings of ACL/HLT 2008,</publication>
        <pages>81--88</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Christopher Dyer</author>
          <author>Smaranda Muresan</author>
          <author>Philip Resnik</author>
        </authors>
        <title>Generalizing word lattice translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>7</id>
        <authors/>
        <title>None</title>
        <publication>In Proceedings of ACL/HLT</publication>
        <pages>1012--1020</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Alexander Fraser</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Measuring word alignment quality for statistical machine translation.</title>
        <publication>Computational Linguistics, Squibs and Discussions,</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Michel Galley</author>
          <author>Jonathan Graehl</author>
          <author>Kevin Knight</author>
          <author>Daniel Marcu</author>
          <author>Steve DeNeefe</author>
          <author>Wei Wang</author>
          <author>Ignacio Thayer</author>
        </authors>
        <title>Scalable inference and training of context-rich syntactic translation models.</title>
        <publication>In Proceedings of COLING/ACL</publication>
        <pages>961--968</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz J Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical phrase-based translation.</title>
        <publication>In Proceedings of HLT/NAACL 2003,</publication>
        <pages>127--133</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Hieu Hoang</author>
          <author>Alexandra Birch</author>
          <author>Chris Callison-Burch</author>
          <author>Marcello Federico</author>
          <author>Nicola Bertoldi</author>
          <author>Brooke Cowan</author>
          <author>Wade Shen</author>
        </authors>
        <title>Moses: Open source toolkit for statistical machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>12</id>
        <authors/>
        <title>None</title>
        <publication>In Proceedings of ACL</publication>
        <pages>77--80</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Shankar Kumar</author>
          <author>Franz J Och</author>
          <author>Wolfgang Macherey</author>
        </authors>
        <title>Improving word alignment with bridge languages.</title>
        <publication>In Proceedings of EMNLP 2007,</publication>
        <pages>42--50</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Yang Liu</author>
          <author>Qun Liu</author>
          <author>Shouxun Lin</author>
        </authors>
        <title>Loglinear models for word alignment.</title>
        <publication>In Proceedings of ACL 2005,</publication>
        <pages>459--466</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Yang Liu</author>
          <author>Qun Liu</author>
          <author>Shouxun Lin</author>
        </authors>
        <title>Treeto-string alignment template for statistical machine translation.</title>
        <publication>In Proceedings of COLING/ACL 2006,</publication>
        <pages>609--616</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Evgeny Matusov</author>
          <author>Richard Zens</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Symmetric word alignments for statistical machine translation.</title>
        <publication>In Proceedings of COLING 2004,</publication>
        <pages>219--225</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Haitao Mi</author>
          <author>Liang Huang</author>
          <author>Qun Liu</author>
        </authors>
        <title>Forestbased translation.</title>
        <publication>In Proceedings of ACL/HLT 2008,</publication>
        <pages>192--199</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Robert C Moore</author>
          <author>Wen-tau Yih</author>
          <author>Andreas Bode</author>
        </authors>
        <title>Improved discriminative bilingual word alignment.</title>
        <publication>In Proceedings of COLING/ACL 2006,</publication>
        <pages>513--520</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Jan Niehues</author>
          <author>Stephan Vogel</author>
        </authors>
        <title>Discriminative word alignment via alignment matrix modeling.</title>
        <publication>In Proceedings of WMT-3,</publication>
        <pages>18--25</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Franz J Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A systematic comparison of various statistical alignment models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Franz J Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>The alignment template approach to statistical machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>Bleu: a method for automatic evaluation of machine translation.</title>
        <publication>In Proceedings of ACL 2002,</publication>
        <pages>311--318</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Libin Shen</author>
          <author>Jinxi Xu</author>
          <author>Ralph Weischedel</author>
        </authors>
        <title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>24</id>
        <authors/>
        <title>None</title>
        <publication>In Proceedings of ACL/HLT</publication>
        <pages>577--585</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Andreas Stolcke</author>
        </authors>
        <title>Srilm - an extension language model modeling toolkit.</title>
        <publication>In Proceedings of ICSLP 2002,</publication>
        <pages>901--904</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>Ben Taskar</author>
          <author>Simon Lacoste-Julien</author>
          <author>Dan Klein</author>
        </authors>
        <title>A discriminative matching approach to word alignment.</title>
        <publication>In Proceedings of HLT/EMNLP 2005,</publication>
        <pages>73--80</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>27</id>
        <authors>
          <author>Ashish Venugopal</author>
          <author>Stephan Vogel</author>
          <author>Alex Waibel</author>
        </authors>
        <title>Effective phrase translation extraction from alignment models.</title>
        <publication>In Proceedings of ACL 2003,</publication>
        <pages>319--326</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>28</id>
        <authors>
          <author>Ashish Venugopal</author>
          <author>Andreas Zollmann</author>
          <author>Noah A Smith</author>
          <author>Stephan Vogel</author>
        </authors>
        <title>Wider pipelines: nbest alignments and parses in mt training.</title>
        <publication>In Proceedings of AMTA</publication>
        <pages>192--201</pages>
        <date>2008</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Blunsom and Cohn, 2006</string>
        <sentence_id>6470</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>3</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>6466</sentence_id>
        <char_offset>213</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>3</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>6802</sentence_id>
        <char_offset>146</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>5</reference_id>
        <string>Deng et al., 2008</string>
        <sentence_id>6788</sentence_id>
        <char_offset>49</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>6</reference_id>
        <string>Dyer et al., 2008</string>
        <sentence_id>6476</sentence_id>
        <char_offset>269</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>6</reference_id>
        <string>Dyer et al., 2008</string>
        <sentence_id>6781</sentence_id>
        <char_offset>208</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>7</reference_id>
        <string>(2008)</string>
        <sentence_id>6477</sentence_id>
        <char_offset>28</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>7</reference_id>
        <string>(2008)</string>
        <sentence_id>6704</sentence_id>
        <char_offset>68</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>7</reference_id>
        <string>(2008)</string>
        <sentence_id>6709</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>7</reference_id>
        <string>(2008)</string>
        <sentence_id>6732</sentence_id>
        <char_offset>79</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>7</reference_id>
        <string>(2008)</string>
        <sentence_id>6789</sentence_id>
        <char_offset>25</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>7</reference_id>
        <string>(2008)</string>
        <sentence_id>6792</sentence_id>
        <char_offset>18</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>8</reference_id>
        <string>Fraser and Marcu, 2007</string>
        <sentence_id>6474</sentence_id>
        <char_offset>105</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>9</reference_id>
        <string>Galley et al., 2006</string>
        <sentence_id>6466</sentence_id>
        <char_offset>227</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>9</reference_id>
        <string>Galley et al., 2006</string>
        <sentence_id>6802</sentence_id>
        <char_offset>186</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>10</reference_id>
        <string>Koehn et al. (2003)</string>
        <sentence_id>6536</sentence_id>
        <char_offset>63</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>10</reference_id>
        <string>Koehn et al. (2003)</string>
        <sentence_id>6675</sentence_id>
        <char_offset>10</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>10</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>6466</sentence_id>
        <char_offset>151</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>10</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>6523</sentence_id>
        <char_offset>47</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>10</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>6706</sentence_id>
        <char_offset>62</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>10</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>6787</sentence_id>
        <char_offset>90</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>11</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>6714</sentence_id>
        <char_offset>95</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>12</reference_id>
        <string>(2007)</string>
        <sentence_id>6784</sentence_id>
        <char_offset>13</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>13</reference_id>
        <string>Kumar et al. (2007)</string>
        <sentence_id>6784</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>14</reference_id>
        <string>Liu et al., 2005</string>
        <sentence_id>6470</sentence_id>
        <char_offset>85</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>14</reference_id>
        <string>Liu et al., 2005</string>
        <sentence_id>6474</sentence_id>
        <char_offset>78</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>15</reference_id>
        <string>Liu et al., 2006</string>
        <sentence_id>6466</sentence_id>
        <char_offset>267</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>15</reference_id>
        <string>Liu et al., 2006</string>
        <sentence_id>6802</sentence_id>
        <char_offset>207</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>16</reference_id>
        <string>Matusov et al. (2004)</string>
        <sentence_id>6783</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>17</reference_id>
        <string>Mi et al., 2008</string>
        <sentence_id>6476</sentence_id>
        <char_offset>192</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>17</reference_id>
        <string>Mi et al., 2008</string>
        <sentence_id>6781</sentence_id>
        <char_offset>143</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>18</reference_id>
        <string>Moore et al., 2006</string>
        <sentence_id>6470</sentence_id>
        <char_offset>44</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>19</reference_id>
        <string>Niehues and Vogel (2008)</string>
        <sentence_id>6792</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>20</reference_id>
        <string>Och and Ney (2003)</string>
        <sentence_id>6467</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>21</reference_id>
        <string>Och and Ney, 2004</string>
        <sentence_id>6466</sentence_id>
        <char_offset>132</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>21</reference_id>
        <string>Och and Ney, 2004</string>
        <sentence_id>6517</sentence_id>
        <char_offset>112</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>21</reference_id>
        <string>Och and Ney, 2004</string>
        <sentence_id>6518</sentence_id>
        <char_offset>137</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>21</reference_id>
        <string>Och and Ney, 2004</string>
        <sentence_id>6787</sentence_id>
        <char_offset>50</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>21</reference_id>
        <string>Och and Ney (2004)</string>
        <sentence_id>6601</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>21</reference_id>
        <string>Och and Ney (2004)</string>
        <sentence_id>6786</sentence_id>
        <char_offset>10</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>22</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>6703</sentence_id>
        <char_offset>73</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>23</reference_id>
        <string>Shen et al., 2008</string>
        <sentence_id>6466</sentence_id>
        <char_offset>248</char_offset>
      </citation>
      <citation>
        <id>42</id>
        <reference_id>24</reference_id>
        <string>(2008)</string>
        <sentence_id>6477</sentence_id>
        <char_offset>28</char_offset>
      </citation>
      <citation>
        <id>43</id>
        <reference_id>24</reference_id>
        <string>(2008)</string>
        <sentence_id>6704</sentence_id>
        <char_offset>68</char_offset>
      </citation>
      <citation>
        <id>44</id>
        <reference_id>24</reference_id>
        <string>(2008)</string>
        <sentence_id>6709</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>45</id>
        <reference_id>24</reference_id>
        <string>(2008)</string>
        <sentence_id>6732</sentence_id>
        <char_offset>79</char_offset>
      </citation>
      <citation>
        <id>46</id>
        <reference_id>24</reference_id>
        <string>(2008)</string>
        <sentence_id>6789</sentence_id>
        <char_offset>25</char_offset>
      </citation>
      <citation>
        <id>47</id>
        <reference_id>24</reference_id>
        <string>(2008)</string>
        <sentence_id>6792</sentence_id>
        <char_offset>18</char_offset>
      </citation>
      <citation>
        <id>48</id>
        <reference_id>25</reference_id>
        <string>Stolcke, 2002</string>
        <sentence_id>6701</sentence_id>
        <char_offset>63</char_offset>
      </citation>
      <citation>
        <id>49</id>
        <reference_id>26</reference_id>
        <string>Taskar et al., 2005</string>
        <sentence_id>6470</sentence_id>
        <char_offset>64</char_offset>
      </citation>
      <citation>
        <id>50</id>
        <reference_id>27</reference_id>
        <string>Venugopal et al., 2003</string>
        <sentence_id>6788</sentence_id>
        <char_offset>25</char_offset>
      </citation>
      <citation>
        <id>51</id>
        <reference_id>28</reference_id>
        <string>Venugopal et al., 2008</string>
        <sentence_id>6542</sentence_id>
        <char_offset>32</char_offset>
      </citation>
      <citation>
        <id>52</id>
        <reference_id>28</reference_id>
        <string>Venugopal et al., 2008</string>
        <sentence_id>6712</sentence_id>
        <char_offset>91</char_offset>
      </citation>
      <citation>
        <id>53</id>
        <reference_id>28</reference_id>
        <string>Venugopal et al., 2008</string>
        <sentence_id>6781</sentence_id>
        <char_offset>280</char_offset>
      </citation>
      <citation>
        <id>54</id>
        <reference_id>28</reference_id>
        <string>Venugopal et al. (2008)</string>
        <sentence_id>6477</sentence_id>
        <char_offset>11</char_offset>
      </citation>
      <citation>
        <id>55</id>
        <reference_id>28</reference_id>
        <string>Venugopal et al. (2008)</string>
        <sentence_id>6704</sentence_id>
        <char_offset>51</char_offset>
      </citation>
      <citation>
        <id>56</id>
        <reference_id>28</reference_id>
        <string>Venugopal et al. (2008)</string>
        <sentence_id>6709</sentence_id>
        <char_offset>86</char_offset>
      </citation>
    </citations>
  </content>
</document>
