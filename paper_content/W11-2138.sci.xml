<PAPER>
  <FILENO/>
  <TITLE>Improving Translation Model by Monolingual Data &#8727;</TITLE>
  <AUTHORS>
    <AUTHOR>Ond&#345;ej Bojar</AUTHOR>
  </AUTHORS>
  <ABSTRACT>
    <A-S ID="S-47253">We use target-side monolingual data to extend the vocabulary of the translation model in statistical machine translation.</A-S>
    <A-S ID="S-47254">This method called &#8220;reverse self-training&#8221; improves the decoder&#8217;s ability to produce grammatically correct translations into languages with morphology richer than the source language esp.</A-S>
    <A-S ID="S-47255">in small-data setting.</A-S>
    <A-S ID="S-47256">We empirically evaluate the gains for several pairs of European languages and discuss some approaches of the underlying back-off techniques needed to translate unseen forms of known words.</A-S>
    <A-S ID="S-47257">We also provide a description of the systems we submitted to WMT11 Shared Task.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-47258">Like any other statistical NLP task, SMT relies on sizable language data for training.</S>
        <S ID="S-47259">However the parallel data required for MT are a very scarce resource, making it difficult to train MT systems of decent quality.</S>
        <S ID="S-47260">On the other hand, it is usually possible to obtain large amounts of monolingual data.</S>
      </P>
      <P>
        <S ID="S-47261">In this paper, we attempt to make use of the monolingual data to reduce the sparseness of surface forms, an issue typical for morphologically rich languages.</S>
        <S ID="S-47262">When MT systems translate into such languages, the limited size of parallel data often causes the situation where the output should include a word form never observed in the training data.</S>
        <S ID="S-47263">Even though the parallel data do contain the desired word</S>
      </P>
      <P>
        <S ID="S-47264">&#8727; This work has been supported by the grants EuroMatrix-</S>
      </P>
      <P>
        <S ID="S-47265">Plus (FP7-ICT-2007-3-231720 of the EU and 7E09003 of the Czech Republic), P406/10/P259, and MSM 0021620838.</S>
      </P>
      <P>
        <S ID="S-47266">in other forms, a standard phrase-based decoder has no way of using it to generate the correct translation.</S>
        <S ID="S-47267">Reverse self-training addresses this problem by incorporating the available monolingual data in the translation model.</S>
        <S ID="S-47268">This paper builds upon the idea outlined in <REF ID="R-01" RPTR="1">Bojar and Tamchyna (2011)</REF>, describing how this technique was incorporated in the WMT Shared Task and extending the experimental evaluation of reverse self-training in several directions &#8211; the examined language pairs (Section 4.2), data size (Section 4.3) and back-off techniques (Section 4.4).</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Related Work</HEADER>
      <P>
        <S ID="S-47269">The idea of using monolingual data for improving the translation model has been explored in several previous works.</S>
        <S ID="S-47270"><REF ID="R-00" RPTR="0">Bertoldi and Federico (2009)</REF> used monolingual data for adapting existing translation models to translation of data from different domains.</S>
        <S ID="S-47271">In their experiments, the most effective approach was to train a new translation model from &#8220;fake&#8221; parallel data consisting of target-side monolingual data and their machine translation into the source language by a baseline system.</S>
      </P>
      <P>
        <S ID="S-47272">Ueffing et al. (2007) used a boot-strapping technique to extend translation models using monolingual data.</S>
        <S ID="S-47273">They gradually translated additional source-side sentences and selectively incorporated them and their translations in the model.</S>
      </P>
      <P>
        <S ID="S-47274">Our technique also bears a similarity to de Gispert et al. (2005), in that we try to use a back-off for surface forms to generalize our model and produce translations with word forms never seen in the original parallel data.</S>
        <S ID="S-47275">However, instead of a rulebased approach, we take advantage of the available</S>
      </P>
      <P>
        <S ID="S-47276">data and learn these forms statistically.</S>
        <S ID="S-47277">We are therefore not limited to verbs, but our system is only able to generate surface forms observed in the target-side monolingual data.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Reverse Self-Training</HEADER>
      <P>
        <S ID="S-47278">Figure 1 illustrates the core of the method.</S>
        <S ID="S-47279">Using available parallel data, we first train an MT system to translate from the target to the source language.</S>
        <S ID="S-47280">Since we want to gather new word forms from the monolingual data, this reverse model needs the ability to translate them.</S>
        <S ID="S-47281">For that purpose we use a factored translation model (<REF ID="R-05" RPTR="4">Koehn and Hoang, 2007</REF>) with two alternative decoding paths: form&#8594;form and back-off&#8594;form.</S>
        <S ID="S-47282">We experimented with several options for the back-off (simple stemming by truncation or full lemmatization), see Section 4.4.</S>
        <S ID="S-47283">The decoder can thus use a less sparse representation of words if their exact forms are not available in the parallel data.</S>
      </P>
      <P>
        <S ID="S-47284">We use this reverse model to translate (much larger) target-side monolingual data into the source language.</S>
        <S ID="S-47285">We preserve the word alignments of the phrases as used in the decoding so we directly obtain the word alignment in the new &#8220;parallel&#8221; corpus.</S>
        <S ID="S-47286">This gives us enough information to proceed with the standard MT system training &#8211; we extract and score the phrases consistent with the constructed word alignment and create the phrase table.</S>
      </P>
      <P>
        <S ID="S-47287">We combine this enlarged translation model with a model trained on the true parallel data and use Minimum Error Rate Training (<REF ID="R-10" RPTR="9">Och, 2003</REF>) to find the balance between the two models.</S>
        <S ID="S-47288">The final model has four separate components &#8211; two language models (one trained on parallel and one on monolingual data) and the two translation models.</S>
      </P>
      <P>
        <S ID="S-47289">We do not expect the translation quality to improve simply because more data is included in training &#8211; by adding translations generated using known data, the model could gain only new combinations of known words.</S>
        <S ID="S-47290">However, by using a back-off to less sparse units (e.g. lemmas) in the factored target&#8594;source translation, we enable the decoder to produce previously unseen surface forms.</S>
        <S ID="S-47291">These translations are then included in the model, reducing the data sparseness of the target-side surface forms.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 Experiments</HEADER>
      <P>
        <S ID="S-47405">We used common tools for phrase-based translation &#8211; Moses (<REF ID="R-06" RPTR="5">Koehn et al., 2007</REF>) decoder and tools, SRILM (<REF ID="R-13" RPTR="11">Stolcke, 2002</REF>) and KenLM (<REF ID="R-04" RPTR="3">Heafield, 2011</REF>) for language modelling and GIZA++ (<REF ID="R-09" RPTR="8">Och and Ney, 2000</REF>) for word alignments.</S>
      </P>
      <P>
        <S ID="S-47406">For reverse self-training, we needed Moses to also output word alignments between source sentences and their translations.</S>
        <S ID="S-47407">As we were not able to make the existing version of this feature work, we added a new option and re-implemented this funcionality.</S>
      </P>
      <P>
        <S ID="S-47408">We rely on automatic translation quality evaluation throughout our paper, namely the wellestablished BLEU metric (<REF ID="R-11" RPTR="10">Papineni et al., 2002</REF>).</S>
        <S ID="S-47409">We estimate 95% confidence bounds for the scores as described in <REF ID="R-08" RPTR="7">Koehn (2004)</REF>.</S>
        <S ID="S-47410">We evaluated our translations on lower-cased sentences.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>4.1 Data Sources</HEADER>
        <P>
          <S ID="S-47292">Aside from the WMT 2011 Translation Task data, we also used several additional data sources for the experiments aimed at evaluating various aspects of reverse self-training.</S>
        </P>
        <P>
          <S ID="S-47293">JRC-Acquis</S>
        </P>
        <P>
          <S ID="S-47294">We used the JRC-Acquis 3.0 corpus (Steinberger et al., 2006) mainly because of the number of available languages.</S>
          <S ID="S-47295">This corpus contains a large amount</S>
        </P>
        <P>
          <S ID="S-47296">of legislative texts of the European Union.</S>
          <S ID="S-47297">The fact that all data in the corpus come from a single, very narrow domain has two effects &#8211; models trained on this corpus perform mostly very well in that domain (as documented e.g. in <REF ID="R-07" RPTR="6">Koehn et al. (2009)</REF>), but fail when translating ordinary texts such as news or fiction.</S>
          <S ID="S-47298">Sentences in this corpus also tend to be rather long (e.g. 30 words on average for English).</S>
        </P>
        <P>
          <S ID="S-47299">CzEng</S>
        </P>
        <P>
          <S ID="S-47300">CzEng 0.9 (<REF ID="R-02" RPTR="2">Bojar and &#381;abokrtsk&#253;, 2009</REF>) is a parallel richly annotated Czech-English corpus.</S>
          <S ID="S-47301">It contains roughly 8 million parallel sentences from a variety of domains, including European regulations (about 34% of tokens), fiction (15%), news (3%), technical texts (10%) and unofficial movie subtitles (27%).</S>
          <S ID="S-47302">We do not make much use of the rich annotation in this paper, however we did experiment with using Czech lemmas (included in the annotation) as the back-off factor for reverse self-training.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>4.2 Comparison Across Languages</HEADER>
        <P>
          <S ID="S-47303">In order to determine how successful our approach is across languages, we experimented with Czech, Finnish, German and Slovak as target languages.</S>
          <S ID="S-47304">All of them have a rich morphology in some sense.</S>
          <S ID="S-47305">We limited our selection of source languages to English, French and German because our method focuses on the target language anyway.</S>
          <S ID="S-47306">We did however combine the languages with respect to the richness of their vocabulary &#8211; the source language has less word forms in almost all cases.</S>
        </P>
        <P>
          <S ID="S-47307">Czech and Slovak are very close languages, sharing a large portion of vocabulary and having a very similar grammar.</S>
          <S ID="S-47308">There are many inflectional rules for verbs, nouns, adjectives, pronouns and numerals.</S>
          <S ID="S-47309">Sentence structure is exhibited by various agreement rules which often apply over long distance.</S>
          <S ID="S-47310">Most of the issues commonly associated with rich morphology are clearly observable in these languages.</S>
        </P>
        <P>
          <S ID="S-47311">German also has some inflection, albeit much less complex.</S>
          <S ID="S-47312">The main source of German vocabulary size are the compound words.</S>
          <S ID="S-47313">Finnish serves as an example of agglutinative languages well-known for the abundance of word forms.</S>
        </P>
        <P>
          <S ID="S-47314">Table 1 contains the summary of our experimental results.</S>
          <S ID="S-47315">Here, only the JRC-Acquis corpus was used for training, development and evaluation.</S>
          <S ID="S-47316">For every language pair, we extracted the first 10 percent of the parallel corpus and used them as the parallel data.</S>
          <S ID="S-47317">The last 70 percent of the same corpus were our &#8220;monolingual&#8221; data.</S>
          <S ID="S-47318">We used a separate set of 1000 sentences for the development and another 1000 for testing.</S>
        </P>
        <P>
          <S ID="S-47319">Sentence counts of the corpora are shown in the columns Corpus Size Para and Mono.</S>
          <S ID="S-47320">The table also shows the ratio between observed vocabulary size of the target and source language.</S>
          <S ID="S-47321">Except for the German&#8594;Czech language pair, the ratios are higher than 1.</S>
          <S ID="S-47322">The Baseline column contains the BLEU score of a system trained solely on the parallel data (i.e. the first 10 percent).</S>
          <S ID="S-47323">A 5-gram language model was used.</S>
          <S ID="S-47324">The &#8220;+Mono LM&#8221; scores were achieved by adding a 5-gram language model trained on the monolingual data as a separate component (its weight was determined by MERT).</S>
          <S ID="S-47325">The last column contains the scores after adding the translation model self-trained on target monolingual data.</S>
          <S ID="S-47326">This model was also added as another component and the weights associated with it were found by MERT.</S>
        </P>
        <P>
          <S ID="S-47327">For the back-off in the reverse self-training, we used a simple suffix-trimming heuristic suitable for fusional languages: cut off the last three characters of each word always keeping at least the first three characters.</S>
          <S ID="S-47328">This heuristic reduces the vocabulary size to a half for Czech and Slovak but it is much less effective for Finish and German (Table 2), as can be expected from their linguistic properties.</S>
        </P>
        <P>
          <S ID="S-47329">Gain in BLEU (absolute) Figure 2: Vocabulary ratio and BLEU score</S>
        </P>
        <P>
          <S ID="S-47330">1.2 en-de</S>
        </P>
        <P>
          <S ID="S-47331">0.8 en-cs en-fi</S>
        </P>
        <P>
          <S ID="S-47332">0.6 fr-cs fr-fi</S>
        </P>
        <P>
          <S ID="S-47333">0.4 en-sk de-cs fr-de</S>
        </P>
        <P>
          <S ID="S-47334">0.2 0.8 1 1.2 1.4 1.6 1.8 2 2.2 2.4 2.6 2.8 3</S>
        </P>
        <P>
          <S ID="S-47335">Vocabulary size ratio</S>
        </P>
        <P>
          <S ID="S-47336">We did not use any linguistic tools, such as morphological analyzers, in this set of experiments.</S>
          <S ID="S-47337">We see the main point of this section in illustrating the applicability of our technique on a wide range of languages, including languages for which such tools are not available.</S>
        </P>
        <P>
          <S ID="S-47338">We encountered problems when using MERT to balance the weights of the four model components.</S>
          <S ID="S-47339">Our model consisted of 14 features &#8211; one for each language model, five for each translation model (phrase probability and lexical weight for both directions and phrase penalty), word penalty and distortion penalty.</S>
          <S ID="S-47340">The extra 5 weights of the reversely trained translation model caused MERT to diverge in some cases.</S>
          <S ID="S-47341">Since we used the mert-moses.pl script for tuning and kept the default parameters, MERT ran for 25 iterations and stopped.</S>
          <S ID="S-47342">As a result, even though our method seemed to improve translation performance in most language pairs, several experiments contradicted this observation.</S>
          <S ID="S-47343">We simply reran the final tuning procedure in these cases and were able to achieve an improvement in BLEU as well.</S>
          <S ID="S-47344">These language pairs are marked with a &#8217;*&#8217; sign in Table 1.</S>
        </P>
        <P>
          <S ID="S-47345">A possible explanation for this behaviour of MERT is that the alternative decoding paths add a lot of possible derivations that generate the same string.</S>
          <S ID="S-47346">To validate our hypothesis we examined a diverging run of MERT for English&#8594;Czech translation with two translation models.</S>
          <S ID="S-47347">Our n-best lists contained the best 100 derivations for each trans- lated sentence from the development data.</S>
          <S ID="S-47348">On average (over all 1000 sentences and over all runs), the n-best list only contained 6.13 different translations of a sentence.</S>
          <S ID="S-47349">The result of the same calculation applied on the baseline run of MERT (which converged in 9 iterations) was 34.85 hypotheses.</S>
          <S ID="S-47350">This clear disproportion shows that MERT had much less information when optimizing our model.</S>
        </P>
        <P>
          <S ID="S-47351">Overall, reverse self-training seems helpful for translating into morphologically rich languages.</S>
          <S ID="S-47352">We achieved promising gains in BLEU, even over the baseline including a language model trained on the monolingual data.</S>
          <S ID="S-47353">The improvement ranges from roughly 0.3 (e.g. German&#8594;Czech) to over 1 point (English&#8594;German) absolute.</S>
          <S ID="S-47354">This result also indicates that suffix trimming is a quite robust heuristic, useful for a variety of language types.</S>
        </P>
        <P>
          <S ID="S-47355">Figure 2 illustrates the relationship between vocabulary size ratio of the language pair and the improvement in translation quality.</S>
          <S ID="S-47356">Although the points are distributed quite irregularly, a certain tendency towards higher gains with higher ratios is observable.</S>
          <S ID="S-47357">We assume that reverse self-training is most useful in cases where a single word form in the source language can be translated as several forms in the target language.</S>
          <S ID="S-47358">A higher ratio between vocabulary sizes suggests that these cases happen more often, thus providing more space for improvement using our method.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>4.3 Data Sizes</HEADER>
        <P>
          <S ID="S-47359">We conducted a series of English-to-Czech experiments with fixed parallel data and a varying size of monolingual data.</S>
          <S ID="S-47360">We used the CzEng corpus, 500 thousand parallel sentences and from 500 thousand up to 5 million monolingual sentences.</S>
          <S ID="S-47361">We used two separate sets of 1000 sentences from CzEng for development and evaluation.</S>
          <S ID="S-47362">Our results are summarized in Figure 3.</S>
          <S ID="S-47363">The gains in BLEU become more significant as the size of included monolingual data increases.</S>
          <S ID="S-47364">The highest improvement can be observed when the data are largest &#8211; over 3 points absolute.</S>
          <S ID="S-47365">Figure 4 shows an example of the impact on translation quality &#8211; the &#8220;Mono&#8221; data are 5 million sentences.</S>
        </P>
        <P>
          <S ID="S-47366">When evaluated from this point of view, our method can also be seen as a way of considerably improving translation quality for languages with little available parallel data.</S>
        </P>
        <P>
          <S ID="S-47367">We also experimented with varying size of parallel data (500 thousand to 5 million sentences) and its effect on reverse self-training contribution.</S>
          <S ID="S-47368">The size of monolingual data was always 5 million sentences.</S>
          <S ID="S-47369">We first measured the percentage of test data word forms covered by the training data.</S>
          <S ID="S-47370">We calculated the value for parallel data and for the combination of parallel and monolingual data.</S>
          <S ID="S-47371">For word forms that appeared only in the monolingual data, a different form of the word had to be contained in the parallel data (so that the model can learn it through the backoff heuristic) in order to be counted in.</S>
          <S ID="S-47372">The difference between the first and second value can simply be thought of as the upper-bound estimation of reverse self-training contribution.</S>
          <S ID="S-47373">Figure 5 shows the results along with BLEU scores achieved in translation experiments following this scenario.</S>
          <S ID="S-47374">Our technique has much greater effect for small parallel data sizes; the amount of newly learned word forms declines rapidly as the size grows.</S>
          <S ID="S-47375">Similarly, improvement in BLEU score decreases quickly and becomes negligible around 2 million parallel sentences.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>4.4 Back-off Techniques</HEADER>
        <P>
          <S ID="S-47376">We experimented with several options for the backoff factor in English&#8594;Czech translation.</S>
          <S ID="S-47377">Data from training section of CzEng were used, 1 million par- Figure 3: Relation between monolingual data size and gains in BLEU score</S>
        </P>
        <P>
          <S ID="S-47378">BLEU 33</S>
        </P>
        <P>
          <S ID="S-47379">Mono LM and TM Mono LM</S>
        </P>
        <P>
          <S ID="S-47380">26 0 1 2 3 4 5</S>
        </P>
        <P>
          <S ID="S-47381">Monolingual data size (millions of sentences)</S>
        </P>
        <P>
          <S ID="S-47382">allel sentences and another 5 million sentences as target-side monolingual data.</S>
          <S ID="S-47383">As in the previous section, the sizes of our development and evaluation sets were a thousand sentences.</S>
        </P>
        <P>
          <S ID="S-47384">CzEng annotation contains lexically disambiguated word lemmas, an appealing option for our purposes.</S>
          <S ID="S-47385">We also tried trimming the last 3 characters of each word, keeping at least the first 3 characters intact.</S>
          <S ID="S-47386">Stemming of each word to four characters was also evaluated (Stem-4).</S>
        </P>
        <P>
          <S ID="S-47387">Table 3 summarizes our results.</S>
          <S ID="S-47388">The last column shows the vocabulary size compared to original vocabulary size, estimated on lower-cased words.</S>
        </P>
        <P>
          <S ID="S-47389">We are not surprised by stemming performing the Coverage of surface forms (%)</S>
        </P>
        <P>
          <S ID="S-47390">worst &#8211; the equivalence classes generated by this simple heuristic are too broad.</S>
          <S ID="S-47391">Using lemmas seems optimal from the linguistic point of view, however suffix trimming outperformed this approach in our experiments.</S>
          <S ID="S-47392">We feel that finding well-performing back-off techniques for other languages merits further research.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>4.5 WMT Systems</HEADER>
        <P>
          <S ID="S-47393">We submitted systems that used reverse selftraining (cu-tamchyna) for English&#8594;Czech and English&#8594;German language pairs.</S>
        </P>
        <P>
          <S ID="S-47394">Our parallel data for German were constrained to the provided set (1.9 million sentences).</S>
          <S ID="S-47395">For Czech, we used the training sections of CzEng and the supplied WMT11 News Commentary data (7.3 million sentences in total).</S>
        </P>
        <P>
          <S ID="S-47396">In case of German, we only used the supplied monolingual data, for Czech we used a large collection of texts for language modelling (i.e. unconstrained).</S>
          <S ID="S-47397">The reverse self-training used only the constrained data &#8211; 2.3 million sentences in German and 2.2 in Czech.</S>
          <S ID="S-47398">In case of Czech, we only used the News monolingual data from 2010 and 2011 for reverse self-training &#8211; we expected that recent data from the same domain as the test set would improve translation performance the most.</S>
          <S ID="S-47399">We achieved mixed results with these systems &#8211; for translation into German, reverse self-training did not improve translation performance.</S>
          <S ID="S-47400">For Czech, we were able to achieve a small gain, even though the reversely translated data contained less sentences than the parallel data.</S>
          <S ID="S-47401">Our BLEU scores were also affected by submitting translation outputs without normalized punctuation and with a slightly different tokenization.</S>
        </P>
        <P>
          <S ID="S-47402">In this scenario, a lot of parallel data were available and we did not manage to prepare a reversely trained model from larger monolingual data.</S>
          <S ID="S-47403">Both of these factors contributed to the inconclusive results.</S>
          <S ID="S-47404">Table 4 shows case-insensitive BLEU scores as calculated in the official evaluation.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Conclusion</HEADER>
      <P>
        <S ID="S-47411">We introduced a technique for exploiting monolingual data to improve the quality of translation into morphologically rich languages.</S>
        <S ID="S-47412">We carried out experiments showing improvements in BLEU when using our method for translating into Czech, Finnish, German and Slovak with small parallel data.</S>
        <S ID="S-47413">We discussed the issues of including similar translation models as separate components in MERT.</S>
        <S ID="S-47414">We showed that gains in BLEU score increase with growing size of monolingual data.</S>
        <S ID="S-47415">On the other hand, growing parallel data size diminishes the effect of our method quite rapidly.</S>
        <S ID="S-47416">We also documented our experiments with several back-off techniques for English to Czech translation.</S>
        <S ID="S-47417">Finally, we described our primary submissions to the WMT 2011 Shared Translation Task.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS/>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>Nicola Bertoldi</RAUTHOR>
      <REFTITLE>Domain adaptation for statistical machine translation with monolingual resources.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>Ond&#345;ej Bojar</RAUTHOR>
      <REFTITLE>Forms Wanted: Training</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>Ond&#345;ej Bojar</RAUTHOR>
      <REFTITLE>CzEng 0.9: Large Parallel Treebank with Rich Annotation.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>Adri&#224; de Gispert</RAUTHOR>
      <REFTITLE>Improving statistical machine translation by classifying and generalizing inflected verb forms. In Eurospeech</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>Kenneth Heafield</RAUTHOR>
      <REFTITLE>Kenlm: Faster and smaller language model queries.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Factored Translation Models.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Moses: Open source toolkit for statistical machine translation. In ACL. The Association for Computer Linguistics.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>462 machine translation systems for europe.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Statistical significance tests for machine translation evaluation.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>Franz Josef Och</RAUTHOR>
      <REFTITLE>Improved statistical alignment models.</REFTITLE>
      <DATE>2000</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>Franz Josef Och</RAUTHOR>
      <REFTITLE>Minimum error rate training in statistical machine translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>Kishore Papineni</RAUTHOR>
      <REFTITLE>Bleu: a method for automatic evaluation of machine translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>Ralf Steinberger</RAUTHOR>
      <REFTITLE>Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis,</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>Andreas Stolcke</RAUTHOR>
      <REFTITLE>Srilm &#8212; an extensible language modeling toolkit,</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
