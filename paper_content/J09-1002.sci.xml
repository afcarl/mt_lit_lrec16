<PAPER>
  <FILENO/>
  <TITLE>Statistical Approaches to Computer-Assisted Translation</TITLE>
  <AUTHORS>
    <AUTHOR>Sergio Barrachina</AUTHOR>
    <AUTHOR>Francisco Casacuberta</AUTHOR>
    <AUTHOR>Antonio Lagarda</AUTHOR>
    <AUTHOR>RWTH Aachen Jorge Civera</AUTHOR>
  </AUTHORS>
  <ABSTRACT/>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>2. Statistical Framework</HEADER>
      <P>
        <S ID="S-20289">The statistical or pattern recognition framework constitutes a very successful framework for MT.</S>
        <S ID="S-20290">As we will see here, this framework also proves adequate for IPMT.</S>
      </P>
      <P>
        <S ID="S-20291">Assuming that we are given a sentence s in a source language, the text-to-text translation problem can be stated as finding its translation t in a target language.</S>
        <S ID="S-20292">Using statistical decision theory, the best translation is given by the equation 2</S>
      </P>
      <P>
        <S ID="S-20293">This equation is generally interpreted as follows.</S>
        <S ID="S-20294">The best translation must be a correct sentence in the target language that conveys the meaning of the source sentence.</S>
        <S ID="S-20295">The probability Pr(t) represents the well-formedness of t and it is generally called the language model probability (n-gram models are usually adopted [<REF ID="R-23" RPTR="21">Jelinek 1998</REF>]).</S>
        <S ID="S-20296">On the other hand, Pr(s|t) represents the relationship between the two sentences (the source and its translation).</S>
        <S ID="S-20297">It should be of a high value if the source is a good translation of the target and of a low value otherwise.</S>
        <S ID="S-20298">Note that the translation direction is inverted from what would be normally expected; correspondingly the models built around this equation are often called inverted translation models (<REF ID="R-07" RPTR="7">Brown et al. 1990</REF>, 1993).</S>
        <S ID="S-20299">As we will see in Section 3, these models are based on the notion of alignment.</S>
        <S ID="S-20300">It is interesting to note that if we had perfect models, the use of Equation (1) would suffice.</S>
        <S ID="S-20301">Given that we have only approximations, the use of Equation (2) allows the language model to correct deficiencies in the translation model.</S>
        <S ID="S-20302">In practice all of these models (and possibly others) are often combined into a loglinear model for Pr(t  |s) (<REF ID="R-39" RPTR="36">Och and Ney 2004</REF>):</S>
      </P>
      <P>
        <S ID="S-20303">where f i (t, s) can be a model for Pr(s|t), a model for Pr(t|s), a target language model for Pr(t), or any model that represents an important feature for the translation.</S>
        <S ID="S-20304">N is the number of models (or features) and &#955; i are the weights of the log-linear combination.</S>
        <S ID="S-20305">When using SFSTs, a different transformation can be used.</S>
        <S ID="S-20306">These transducers have an implicit target language model (which can be obtained from the finite-state transducer by dropping the source symbols of each transition (Vidal et al. 2005)).</S>
        <S ID="S-20307">Therefore, this separation is no longer needed.</S>
        <S ID="S-20308">SFSTs model joint probability distributions; therefore, Equation (1) has to be rewritten as</S>
      </P>
      <P>
        <S ID="S-20309">This is the approach followed in GIATI (<REF ID="R-10" RPTR="9">Casacuberta et al. 2004</REF><REF ID="R-15" RPTR="13">Casacuberta et al. 2004</REF>a; <REF ID="R-11" RPTR="10">Casacuberta and Vidal 2004</REF>), but other models for the joint probability can be adopted.</S>
        <S ID="S-20310">If the input is a spoken sentence, instead of a written one, the problem becomes more complex; we will not deal with this here.</S>
        <S ID="S-20311">The interested reader may consult <REF ID="R-00" RPTR="0">Amengual et al. (2000)</REF>, <REF ID="R-36" RPTR="29">Ney et al. (2000)</REF>, or Casacuberta et al. (2004a, 2004b), for instance.</S>
      </P>
      <P>
        <S ID="S-20312">Unfortunately, current models and therefore the systems which can be built from them are still far from perfect.</S>
        <S ID="S-20313">This implies that, in order to achieve good, or even acceptable, translations, manual post-editing is needed.</S>
        <S ID="S-20314">An alternative to this serial approach (first MT, then manual correction) is given by the IPMT paradigm.</S>
        <S ID="S-20315">Under this paradigm, translation is considered as an iterative process where human and computer activity</S>
      </P>
      <P>
        <S ID="S-20316">Typical example of IPMT with keyboard interaction.</S>
        <S ID="S-20317">The aim is to translate the English sentence Click OK to close the print dialog into Spanish.</S>
        <S ID="S-20318">Each step starts with a previously fixed target language prefix t p , from which the system suggests a suffix &#710;t s .</S>
        <S ID="S-20319">Then the user accepts a part of this suffix (a) and types some keystrokes (k), possibly in order to amend the remaining part of t s .</S>
        <S ID="S-20320">This produces a new prefix, composed by the prefix from the previous iteration and the accepted and typed text, (a)(k), to be used as t p in the next step.</S>
        <S ID="S-20321">The process ends when the user enters the special keystroke &#8221;#&#8221;.</S>
        <S ID="S-20322">System suggestions are printed in italics and user input in boldface typewriter font.</S>
        <S ID="S-20323">In the final translation t, text that has been typed by the user is underlined.</S>
        <S ID="S-20324">are interwoven.</S>
        <S ID="S-20325">This way, the models take into account both the input sentence and the corrections of the user.</S>
        <S ID="S-20326">As previously mentioned, this idea was originally proposed in the TransType project (Foster, Isabelle, and Plamondon 1997; Langlais, Foster, and Lapalme 2000; Langlais, Lapalme, and Loranger 2002).</S>
        <S ID="S-20327">In that project, the parts proposed by the systems were produced using a linear combination of a target language model (trigrams) and a lexicon model (so-called IBM-1 or -2) (Langlais, Lapalme, and Loranger 2002).</S>
        <S ID="S-20328">As a result, TransType allowed only single-token completions, where a token could be either a word or a short sequence of words from a predefined set of sequences.</S>
        <S ID="S-20329">This proposal was extended to complete full target sentences in the TT2 project, as discussed hereafter.</S>
        <S ID="S-20330">The approach taken in TT2 is exemplified in Figure 1.</S>
        <S ID="S-20331">Initially, the system provides a possible translation.</S>
        <S ID="S-20332">From this translation, the user marks a prefix as correct and provides, as a hint, the beginning of the rest of the translation.</S>
        <S ID="S-20333">Depending on the system or the user preferences, the hint can be the next word or some letters from it (in the figure, hints are assumed to be words and are referred to as k).</S>
        <S ID="S-20334">Let us use t p for the prefix validated by the user together with the hint.</S>
        <S ID="S-20335">The system now has to produce (predict) asuffixt s to complete the translation.</S>
        <S ID="S-20336">The cycle continues with a new validation and hint from the user until the translation is completed.</S>
        <S ID="S-20337">This justifies our choice of the term &#8220;interactive-predictive machine translation&#8221; for this approach.</S>
        <S ID="S-20338">The crucial step of the process is the production of the suffix.</S>
        <S ID="S-20339">Again, decision theory tells us to maximize the probability of the suffix given the available information.</S>
        <S ID="S-20340">That is, the best suffix will be which can be straightforwardly rewritten as</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>7 Computational Linguistics Volume 35, Number 1</HEADER>
      <P>
        <S ID="S-20341">Note that, because t p t s = t, this equation is very similar to Equation (1).</S>
        <S ID="S-20342">The main difference is that the argmax search now is performed over the set of suffixes t s that complete t p instead of complete sentences (t in Equation (1)).</S>
        <S ID="S-20343">This implies that we can use the same models if the search procedures are adequately modified (Och, Zens, and Ney 2003).</S>
        <S ID="S-20344">The situation with respect to finite-state models is similar.</S>
        <S ID="S-20345">Now, Equation (5) is rewritten as</S>
      </P>
      <P>
        <S ID="S-20346">which allows the use of the same models as in Equation (4) as long as the search procedure is changed appropriately (<REF ID="R-19" RPTR="19">Cubel et al. 2003</REF>, 2004; <REF ID="R-16" RPTR="14">Civera et al. 2004</REF><REF ID="R-17" RPTR="16">Civera et al. 2004</REF>a, 2004b).</S>
      </P>
      <P>
        <S ID="S-20347">The models used are presented in the following subsections: Section 3.1 for the conditional distribution Pr(s|t) in Equation (2) and Section 3.2 for the joint distribution Pr(s, t) in Equation (4).</S>
      </P>
      <P>
        <S ID="S-20348">The translation models which <REF ID="R-08" RPTR="8">Brown et al. (1993)</REF> introduced to deal with Pr(s|t) in Equation (2) are based on the concept of alignment between the components of a pair (s, t) (thus they are called statistical alignment models).</S>
        <S ID="S-20349">Formally, if the number of the source words in s is J and the number of target words in t is I, analignment is a function a : {1, ..., J} &#8594;{0, ..., I}.</S>
        <S ID="S-20350">The image of j by a will be denoted as a j , in which the particular case a j = 0 means that the position j in s is not aligned with any position of t.</S>
        <S ID="S-20351">By introducing the alignment as a hidden variable in Pr(s|t),</S>
      </P>
      <P>
        <S ID="S-20352">The alignment that maximizes Pr(s, a|t) is shown to be very useful in practice for training and for searching.</S>
        <S ID="S-20353">Different approaches have been proposed for modeling Pr(s, a|t) in Equation (8): Zero-order models such as model 1, model 2,andmodel 3 (Brown et al. 1993) and the firstorder models such as model 4, model 5 (Brown et al. 1993), hidden Markov model (<REF ID="R-36" RPTR="28">Ney et al. 2000</REF>), and model 6 (<REF ID="R-38" RPTR="32">Och and Ney 2003</REF>).</S>
        <S ID="S-20354">In all these models, single words are taken into account.</S>
        <S ID="S-20355">Moreover, in practice the summation operator is replaced with the maximization operator, which in turn reduces the contribution of each individual source word in generating a target word.</S>
        <S ID="S-20356">On the other hand, modeling word sequences rather than single words in both the alignment and lexicon models cause significant improvement in translation quality (Och and Ney 8 Barrachina et al.</S>
      </P>
      <P>
        <S ID="S-20357">2004).</S>
        <S ID="S-20358">In this work, we use two closely related models: ATs (<REF ID="R-39" RPTR="37">Och and Ney 2004</REF>) and PBMs (<REF ID="R-48" RPTR="43">Tom&#225;s and Casacuberta 2001</REF>; Koehn, Och, and Marcu 2003; Zens and Ney 2004).</S>
        <S ID="S-20359">Both models are based on bilingual phrases 3 (pairs of segments or word sequences) in which all words within the source-language phrase are aligned only to words of the target-language phrase and vice versa.</S>
        <S ID="S-20360">Note that at least one word in the sourcelanguage phrase must be aligned to one word of the target-language phrase, that is, there are no empty phrases similar to the empty word of the word-based models.</S>
        <S ID="S-20361">In addition, no gaps and no overlaps between phrases are allowed.</S>
        <S ID="S-20362">We introduce some notation to deal with phrases.</S>
        <S ID="S-20363">As before, s denotes a sourcelanguage sentence; &#732;s denotes a generic phrase in s,and&#732;s k the kth phrase in s. s j denotes the jth source word in s; s j&#8242; j denotes the contiguous sequence of words in s beginning at position j and ending at position j &#8242; (inclusive); obviously, if s has J words, s J 1 denotes the whole sentence s.</S>
        <S ID="S-20364">An analogous notation is used for target words, phrases, and sequences in target sentence t.</S>
        <S ID="S-20365">3.1.1 Alignment Templates.</S>
        <S ID="S-20366">The ATs are based on the bilingual phrases but they are generalized by replacing words with word classes and by storing the alignment information for each phrase pair.</S>
        <S ID="S-20367">Formally, an AT Z is a triple (S, T, &#227;), where S and T are a source class sequence and a target class sequence, respectively, and &#227; is an alignment from the set of positions in S to the set of positions in T.</S>
        <S ID="S-20368">4 Mapping of source and target words to bilingual word classes is automatically trained using the method described by <REF ID="R-37" RPTR="30">Och (1999)</REF>.</S>
        <S ID="S-20369">The method is actually an unsupervised clustering method which partitions the source and target vocabularies, so that assigning words to classes is a deterministic operation.</S>
        <S ID="S-20370">It is also possible to employ parts-of-speech or semantic categories instead of the unsupervised clustering method used here.</S>
        <S ID="S-20371">More details can be found in <REF ID="R-37" RPTR="31">Och (1999)</REF> and <REF ID="R-39" RPTR="38">Och and Ney (2004)</REF>.</S>
        <S ID="S-20372">However, it should be mentioned that the whole AT approach (and similar PBM approaches as they are now called) is independent of the word clustering concept.</S>
        <S ID="S-20373">In particular, for large training corpora, omitting the word clustering in the AT system does not much affect the translation accuracy.</S>
        <S ID="S-20374">To arrive at our translation model, we first perform a segmentation of the source and target sentences into K &#8220;blocks&#8221; d k &#8801; (i k ; b k , j k )(i k &#8712;{1, ..., I} and j k , b k &#8712;{1, ..., J} for 1 &#8804; k &#8804; K).</S>
        <S ID="S-20375">For a given sentence pair (s J 1 , tI 1 ), the kth bilingual segment (&#732;s k,&#732;t k ) is (s j k b k&#8722;1 +1 , ti k ik&#8722;1 +1 ) (<REF ID="R-38" RPTR="33">Och and Ney 2003</REF>).</S>
        <S ID="S-20376">The AT Z k = (S k , T k , &#227; k ) associated with the kth bilingual segment is: S k the sequence of word classes in &#732;s k ; T k the sequence of word classes in &#732;t k ,and&#227; k the alignment between positions in a source class sequence S and positions in a target class sequence T.</S>
        <S ID="S-20377">For translating a given source sentence s we use the following decision rule as an approximation to Equation (1):</S>
      </P>
      <P>
        <S ID="S-20378">with weights &#955; i , i = 1, &#183;&#183;&#183; , 7.</S>
        <S ID="S-20379">The weights &#955; 1 and &#955; 4 play a special role and are used to control the number I of words and number K of segments for the target sentence to be generated, respectively.</S>
        <S ID="S-20380">The log-linear combination uses the following set of models:</S>
      </P>
      <P>
        <S ID="S-20381">from frequency counts in a training corpus &#65533; p(t i |&#732;s k , &#227; k ): Single word model based on a statistical dictionary and &#227; k .As in the preceding model, the model parameters are estimated by using frequency counts &#65533; q(b k |j k&#8722;1 ) = e |b k&#8722;j k&#8722;1 +1 |: Re-ordering model using absolute j distance of the phrases.</S>
        <S ID="S-20382">As can be observed, all models are implemented as feature functions which depend on the source and the target language sentences, as well as on the two hidden variables (&#227; K 1 , bK 1 ).</S>
        <S ID="S-20383">Other feature functions can be added to this sort of model as needed.</S>
        <S ID="S-20384">For a more detailed description the reader is referred to <REF ID="R-39" RPTR="39">Och and Ney (2004)</REF>.</S>
        <S ID="S-20385">Learning alignment templates.</S>
        <S ID="S-20386">To learn the probability of applying an AT, p(Z = (S, T, &#227;)|&#732;s ), all bilingual phrases that are consistent with the segmentation are extracted from the training corpus together with the alignment within these phrases.</S>
        <S ID="S-20387">Thus, we obtain a count N(Z) of how often an AT occurred in the aligned training corpus.</S>
        <S ID="S-20388">Using the relative frequency</S>
      </P>
      <P>
        <S ID="S-20389">we estimate the probability of applying an AT Z to translate the source language phrase &#732;s, in which &#948; is Kronecker&#8217;s delta function.</S>
        <S ID="S-20390">The class function C maps words onto their 10 Barrachina et al.</S>
      </P>
      <P>
        <S ID="S-20391">classes.</S>
        <S ID="S-20392">To reduce the memory requirements, only probabilities for phrases up to a maximal length are estimated, and phrases with a probability estimate below a certain threshold are discarded.</S>
        <S ID="S-20393">The weights &#955; i in Equation (10) are usually estimated using held-out data with respect to the automatic evaluation metric employed using the downhill simplex algorithm from <REF ID="R-43" RPTR="42">Press et al. (2002)</REF>.</S>
      </P>
      <P>
        <S ID="S-20394">works: The PBM approach (<REF ID="R-48" RPTR="44">Tom&#225;s and Casacuberta 2001</REF>; <REF ID="R-35" RPTR="26">Marcu and Wong 2002</REF>; Zens, Och, and Ney 2002; <REF ID="R-49" RPTR="46">Tom&#225;s and Casacuberta 2003</REF>; Zens and Ney 2004).</S>
        <S ID="S-20395">These methods learn the probability that a sequence of contiguous words&#8212;the source phrase&#8212;(as a whole unit) in a source sentence is a translation of another sequence of contiguous words&#8212;the target phrase&#8212;(as a whole unit) in the target sentence.</S>
        <S ID="S-20396">In this case, the statistical dictionaries of single word pairs are substituted by statistical dictionaries of bilingual phrases or bilingual segments.</S>
        <S ID="S-20397">These models are simpler than ATs, because no alignments are assumed between word positions inside a bilingual segment and word classes are not used in the definition of a bilingual phrase.</S>
        <S ID="S-20398">The simplest formulation is for monotone PBMs (<REF ID="R-52" RPTR="49">Tom&#225;s and Casacuberta 2007</REF>), assuming a uniform distribution of the possible segmentations of the source and of the target sentences.</S>
        <S ID="S-20399">In this case, the approximation to Equation (1) is:</S>
      </P>
      <P>
        <S ID="S-20400">with weights &#955; i , i = 1, &#183;&#183;&#183; , 5.</S>
        <S ID="S-20401">The weights &#955; 1 and &#955; 4 play a special role and are used to control the number I of words and number K of segments for the target sentence to be generated, respectively.</S>
        <S ID="S-20402">The log-linear combination uses the following set of models:</S>
      </P>
      <P>
        <S ID="S-20403">If segment re-ordering is desired (non-monotone models), the probability of phrasealignment q can be introduced (a first-order distortion model is assumed):</S>
      </P>
      <P>
        <S ID="S-20404">with the additional model q, similar to the one used for AT.</S>
        <S ID="S-20405">Learning phrase-based alignment models.</S>
        <S ID="S-20406">The parameters of each model and the weights &#955; i in Equations (13) and (14) have to be estimated.</S>
        <S ID="S-20407">There are different approaches to estimating the parameters of each model (<REF ID="R-52" RPTR="50">Tom&#225;s and Casacuberta 2007</REF>).</S>
        <S ID="S-20408">Some of these techniques correspond to a direct learning of the parameters from a sentence-aligned corpus using a maximum likelihood approach (<REF ID="R-48" RPTR="45">Tom&#225;s and Casacuberta 2001</REF>; <REF ID="R-35" RPTR="27">Marcu and Wong 2002</REF>).</S>
        <S ID="S-20409">Other techniques are heuristics based on the previous computation of word alignments in the training corpus (Zens, Och, and Ney 2002; Koehn, Och, and Marcu 2003).</S>
        <S ID="S-20410">On the other hand, as for AT, the weights &#955; i in Equation (13) are usually optimized using held-out data.</S>
      </P>
      <P>
        <S ID="S-20411">SFSTs constitute an important framework in syntactic pattern recognition and natural language processing.</S>
        <S ID="S-20412">The simplicity of finite-state models has given rise to some concerns about their applicability to real tasks.</S>
        <S ID="S-20413">Specifically in the field of language translation, it is often argued that natural languages are so complex that these simple models are never able to cope with the required source-target mappings.</S>
        <S ID="S-20414">However, one should take into account that the complexity of the mapping between the source and target domains of a transducer is not always directly related to the complexity of the domains themselves.</S>
        <S ID="S-20415">Instead, a key factor is the degree of monotonicity or sequentiality between source and target subsequences of these domains (Casacuberta, Vidal, and Pic&#243; 2005).</S>
        <S ID="S-20416">Finite-state transducers have been shown to be adequate to handle complex mappings efficiently (<REF ID="R-04" RPTR="5">Berstel 1979</REF>) and SFSTs are closely related to monotone PBMs.</S>
        <S ID="S-20417">In Equation (4), Pr(s, t) can be modeled by an SFST T, which is defined as a tuple &#12296;&#931;, &#8710;, Q, q 0 , p, f &#12297;, where &#931; is a finite set of source symbols, &#8710; is a finite set of target symbols</S>
      </P>
      <P>
        <S ID="S-20418">the product of its transition probabilities, times the final-state probability of the last stateinthepath:</S>
      </P>
      <P>
        <S ID="S-20419">The probability of a translation pair (s, t) according to T is then defined as the sum of the probabilities of all the paths associated with (s, t):</S>
      </P>
      <P>
        <S ID="S-20420">Learning finite-state transducers.</S>
        <S ID="S-20421">There are different families of techniques to train an SFST from a parallel corpus of source&#8211;target sentences (<REF ID="R-12" RPTR="12">Casacuberta and Vidal 2007</REF>).</S>
        <S ID="S-20422">One of the techniques that has been adopted in this work is the grammatical inference and alignments for transducer inference (GIATI) technique.</S>
        <S ID="S-20423">This technique is in the category of hybrid methods which use statistical techniques to guide the SFST structure learning and simultaneously train the associated probabilities.</S>
        <S ID="S-20424">Given a finite sample of string pairs, the inference of SFSTs using the GIATI technique is performed as follows (<REF ID="R-11" RPTR="11">Casacuberta and Vidal 2004</REF>; Casacuberta, Vidal, and Pic&#243; 2005): i) Building training strings: Each training pair is transformed into a single string from an extended alphabet to obtain a new sample of strings.</S>
        <S ID="S-20425">ii) Inferring a (stochastic) regular grammar.</S>
        <S ID="S-20426">Typically, a smoothed n-gram is inferred from the sample of strings obtained in the previous step.</S>
        <S ID="S-20427">iii) Transforming the inferred regular grammar into a transducer: The symbols associated with the grammar rules are converted back into input/output symbols, thereby transforming the grammar inferred in the previous step into a transducer.</S>
        <S ID="S-20428">The transformation of a parallel corpus into a string corpus is performed using statistical alignments.</S>
        <S ID="S-20429">These alignments are obtained using the GIZA++ software (<REF ID="R-38" RPTR="34">Och and Ney 2003</REF>).</S>
      </P>
      <P>
        <S ID="S-20430">Searching is an important computational problem in SMT.</S>
        <S ID="S-20431">Algorithmic solutions developed for SMT can be adapted to the IPMT framework.</S>
        <S ID="S-20432">The main general search procedures for each model in Section 3 are presented in the following subsections, each followed by a detailed description of the necessary adaptations to the interactive framework.</S>
      </P>
      <P>
        <S ID="S-20433">In offline MT, the generation of the best translation for a given source sentence s is carried out by producing the target sentence in left-to-right order using the model of Equation (10).</S>
        <S ID="S-20434">At each step of the generation algorithm we maintain a set of active hypotheses and choose one of them for extension.</S>
        <S ID="S-20435">A word of the target language is then added to the chosen hypothesis and its costs get updated.</S>
        <S ID="S-20436">This kind of generation fits nicely into a dynamic programming (DP) framework, as hypotheses which are indistinguishable by both language and translation models (and that have covered the same source positions) can be recombined.</S>
        <S ID="S-20437">Because the DP search space grows</S>
      </P>
      <P>
        <S ID="S-20438">translation: &#8220;what did you say?&#8221;).</S>
        <S ID="S-20439">exponentially with the size of the input, standard DP search is prohibitive, and we resort to a beam-search heuristic.</S>
      </P>
      <P>
        <S ID="S-20440">4.1.1 Adaptation to the Interactive-Predictive Scenario.</S>
        <S ID="S-20441">The most important modification</S>
      </P>
      <P>
        <S ID="S-20442">is to rely on a word graph that represents possible translations of the given source sentence.</S>
        <S ID="S-20443">This word graph is generated once for each source sentence.</S>
        <S ID="S-20444">During the process of human&#8211;machine interaction the system makes use of this word graph in order to complete the prefixes accepted by the human translator.</S>
        <S ID="S-20445">In other words, after the human translator has accepted a prefix string, the system finds the best path in the word graph associated with this prefix string so that it is able to complete the target sentence.</S>
        <S ID="S-20446">Using the word graph in such a way, the system is able to interact with the human translator in a time efficient way.</S>
        <S ID="S-20447">In Och, Zens, and Ney (2003), an efficient algorithm for interactive generation using word graphs was presented.</S>
        <S ID="S-20448">A word graph is a weighted directed acyclic graph, in which each node represents a partial translation hypothesis and each edge is labeled with a word of the target sentence and is weighted according to the language and translation model scores.</S>
        <S ID="S-20449">In Ueffing, Och, and Ney (2002), the authors give a more detailed description of word graphs and show how they can be easily produced as a by-product of the search process.</S>
        <S ID="S-20450">An example of a word graph is shown in Figure 2.</S>
        <S ID="S-20451">The computational cost of this approach is much lower, as the whole search for the translation must be carried out only once, and the generated word graph can be reused for further completion requests.</S>
        <S ID="S-20452">For a fixed source sentence, if no pruning is applied in the production of the word graph, it represents all possible sequences of target words for which the posterior probability is greater than zero, according to the models used.</S>
        <S ID="S-20453">However, because of the pruning generally needed to render the problem computationally feasible, the resulting word graph only represents a subset of the possible translations.</S>
        <S ID="S-20454">Therefore, it may happen that the user sets prefixes which cannot be found in the word graph.</S>
        <S ID="S-20455">To circumvent this problem some heuristics need to be implemented.</S>
        <S ID="S-20456">First, we look for the node with minimum edit distance to the prefix except for</S>
      </P>
      <P>
        <S ID="S-20457">with a path going from the node to the final node.</S>
        <S ID="S-20458">Now, because the original word graph may not be compatible with the new information provided by the prefix, it might be impossible to find a completion in this word graph due to incompatibility with the last (partial) word in the prefix.</S>
        <S ID="S-20459">This problem can be solved to a certain degree by searching for a completion of the last word with the highest probability using only the language model.</S>
        <S ID="S-20460">This supplementary heuristic to the usual search increases the performance of the system, because some of the rejected words in the pruning process can be recovered.</S>
        <S ID="S-20461">A desirable feature of an IPMT system is the possibility of producing a list of alternative target suffixes, instead of only one.</S>
        <S ID="S-20462">This feature can be easily added by computing the n-best hypotheses.</S>
        <S ID="S-20463">Of course, these n-best hypotheses do not refer to the whole target sentence, but only to the suffixes.</S>
        <S ID="S-20464">However, the problem is that in many cases the sentence hypotheses in the n-best list differ in only one or two words.</S>
        <S ID="S-20465">Therefore, we introduce the additional requirement that the first four words of the n- best hypotheses must be different.</S>
      </P>
      <P>
        <S ID="S-20466">The generation of the best translation with PBMs is similar to the one described in the previous section.</S>
        <S ID="S-20467">Each hypothesis is composed of a prefix of the target sentence, a subset of source positions that are aligned with the positions of the prefix of the target sentence, and a score.</S>
        <S ID="S-20468">In this case, we adopted an extension of the best-first strategy where the hypotheses are stored in several sorted lists, depending on which words in the source sentence have been translated.</S>
        <S ID="S-20469">This strategy is related to the well-known multi-stackdecoding algorithm (<REF ID="R-03" RPTR="4">Berger et al. 1996</REF>; <REF ID="R-50" RPTR="47">Tom&#225;s and Casacuberta 2004</REF>).</S>
        <S ID="S-20470">In each iteration, the algorithm extends the best hypothesis from each available list.</S>
        <S ID="S-20471">While target words are always generated from left to right, there are two alternatives in the source word extraction: Monotone search, which takes the source words from left to right, and non-monotone search, which can take source words in any order.</S>
      </P>
      <P>
        <S ID="S-20472">4.2.1 Adaptation to the Interactive-Predictive Scenario.</S>
        <S ID="S-20473">Only a simple modification of this</S>
      </P>
      <P>
        <S ID="S-20474">search algorithm is necessary: If the new extended hypothesis is not compatible with the fixed target prefix, t p , then this hypothesis is not considered.</S>
        <S ID="S-20475">This compatibility is verified at the character level; therefore the user does not need to type the whole target word at the end of the target prefix.</S>
        <S ID="S-20476">In the interactive scenario, speed is a critical aspect.</S>
        <S ID="S-20477">In the PBM approach, monotone search is much faster than non-monotone search in the tasks which are considered in this work (<REF ID="R-51" RPTR="48">Tom&#225;s and Casacuberta 2006</REF>).</S>
        <S ID="S-20478">However, monotone search presents a problem for interactive operation: If a user introduces a prefix that cannot be obtained in a monotone way from the source, the search algorithm is not able to complete this prefix.</S>
        <S ID="S-20479">In order to solve this problem without losing computational efficiency, we use the following approach: Non-monotone search is used for target prefixes, whereas completions (suffixes) are generated using monotone search.</S>
        <S ID="S-20480">As for AT models, a list of target suffixes can also be produced.</S>
        <S ID="S-20481">This list can be obtained easily by keeping the n-best hypotheses in each sorted list.</S>
        <S ID="S-20482">To avoid generating very similar hypotheses in the n-best list, we apply the following procedure: Starting from the n-best list resulting from the normal search, we first add hypotheses obtained</S>
      </P>
      <P>
        <S ID="S-20483">by translating a single untranslated word from the source, along with hypotheses consisting of a single high-probability word according to the target language model; we then re-order the hypotheses, maximizing the diversity at the beginning of the suffixes, and keep only the n first hypotheses in the re-ordered list.</S>
      </P>
      <P>
        <S ID="S-20484">As discussed by <REF ID="R-42" RPTR="41">Pic&#243; and Casacuberta (2001)</REF>, the computation of Equation (4) for SFSTs under a maximum approximation (i.e., using maximization in Equation (17) instead of the sum) amounts to a conventional Viterbi search.</S>
        <S ID="S-20485">The algorithm finds the most probable path among those paths in the SFST which are compatible with the source sentence s.</S>
        <S ID="S-20486">The corresponding translation, &#732;t, is simply obtained by concatenating the target strings of the edges of this path.</S>
      </P>
      <P>
        <S ID="S-20487">the optimization is performed over the set of target suffixes (completions) rather than the set of complete target sentences.</S>
        <S ID="S-20488">To solve this maximization problem, an approach similar to that proposed for AT in Section 4.1 has been adopted.</S>
        <S ID="S-20489">First, given the source sentence, a word graph is extracted from the SFST.</S>
        <S ID="S-20490">In this case, the word graph is just (a pruned version of) the Viterbi search trellis obtained when translating the whole source sentence.</S>
        <S ID="S-20491">The main difference between the word graphs generated with ATs and SFSTs is how the nodes and edges are defined in each case.</S>
        <S ID="S-20492">On the one hand, the nodes are defined as partial hypotheses of the search procedure in the AT approach, whereas the nodes in the case of SFSTs can be directly mapped into states in the SFST representing a joint (source word/target string) language model.</S>
        <S ID="S-20493">On the other hand, the scores associated with the edges in the AT approach are computed from a combination of the language and translation models, whereas in the case of SFSTs these scores simply come from the joint language model estimated by the GIATI technique.</S>
        <S ID="S-20494">Once the word graph has been generated, the search for the most probable completion as stated in Equation (6) is carried out in two steps, in a similar way to that explained for the AT approach.</S>
        <S ID="S-20495">In this case, the computation entailed by both the editdistance (prefix error-correcting) and the remaining search is significantly accelerated by visiting the nodes in topological order and by the incorporation of the beam-search technique (<REF ID="R-01" RPTR="1">Amengual and Vidal 1998</REF>).</S>
        <S ID="S-20496">Moreover, the error-correcting algorithm takes advantage of the incremental way in which the user prefix is generated, parsing only the new suffix appended by the user in the last interaction.</S>
        <S ID="S-20497">It may be the case that a user prefix ends in an incomplete word during the interactive translation process.</S>
        <S ID="S-20498">Therefore, it is necessary to start the translation completion with a word whose prefix matches this unfinished word.</S>
        <S ID="S-20499">The proposed algorithm thus searches for such a word.</S>
        <S ID="S-20500">First, it considers the target words of the edges leaving the nodes returned by the error-correcting algorithm.</S>
        <S ID="S-20501">If this initial search fails, then a matching word is looked up in the word-graph vocabulary.</S>
        <S ID="S-20502">Finally, as a last resort, the whole transducer vocabulary is taken into consideration to find a matching word; otherwise this incomplete word is treated as an entire word.</S>
        <S ID="S-20503">This error-correcting algorithm returns a set of nodes from which the best completion would be selected according to the best backward score.</S>
        <S ID="S-20504">Moreover, n-best completions can also be produced.</S>
        <S ID="S-20505">Among many weighted-graph n-best path algorithms which are available, the recursive enumeration algorithm presented in Jim&#233;nez and 16 Barrachina et al.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5. Experimental Framework</HEADER>
      <P>
        <S ID="S-20506">The models and search procedures introduced in the previous sections were assessed through a series of IPMT experiments with different corpora.</S>
        <S ID="S-20507">These corpora, along with the corresponding pre- and post-processing and assessment procedures, are presented in this section.</S>
      </P>
      <P>
        <S ID="S-20508">Usually, MT models are trained on a pre-processed version of an original corpus.</S>
        <S ID="S-20509">Preprocessing provides a simpler representation of the training corpus which makes token or word forms more homogeneous.</S>
        <S ID="S-20510">In this way automatic training of the MT models is boosted, and the amount of computation decreases.</S>
        <S ID="S-20511">The pre-processing steps are: tokenization, removing unnecessary case information, and tagging some special tokens like numerical sequences, e-mail addresses, and URLs (&#8220;categorization&#8221;).</S>
        <S ID="S-20512">In translation from a source language to a target language, there are some words which are translated identically (because they have the same spelling in both languages).</S>
        <S ID="S-20513">Therefore, we identify them in the corpus and replace them with some generic tags to help the translation system.</S>
        <S ID="S-20514">Post-processing takes place after the translation in order to hide the internal representation of the text from the user.</S>
        <S ID="S-20515">Thus, the user will only work with an output which is very similar to human-generated texts.</S>
        <S ID="S-20516">In detail, the post-processing steps are: detokenization, true-casing, and replacing the tags with their corresponding words.</S>
        <S ID="S-20517">In an IPMT scenario, the pre-/post-processing must run in real-time and should be reversible as much as possible.</S>
        <S ID="S-20518">In each human&#8211;machine interaction, the current prefix has to be pre-processed for the interactive-predictive engine and then the generated completion has to be post-processed for the user.</S>
        <S ID="S-20519">It is crucial that the pre-processing of prefixes is fully compatible with the training corpus.</S>
      </P>
      <P>
        <S ID="S-20520">Six bilingual corpora were used for two different tasks and three different language pairs in the framework of the TT2 project (SchlumbergerSema S.A.</S>
        <S ID="S-20521">et al. 2001).</S>
        <S ID="S-20522">The language pairs involved were English&#8211;Spanish, English&#8211;French, and English&#8211; German (<REF ID="R-26" RPTR="22">Khadivi and Goutte 2003</REF>), and the tasks were Xerox (Xerox printer manuals) and EU (Bulletin of the European Union).</S>
        <S ID="S-20523">The three Xerox corpora were obtained from different user manuals for Xerox printers (SchlumbergerSema S.A.</S>
        <S ID="S-20524">et al. 2001).</S>
        <S ID="S-20525">The main features of these corpora are shown in Table 1.</S>
        <S ID="S-20526">Dividing the corpora into training and test sets was performed by randomly selecting (without replacement) a specified amount of test sentences and leaving the remaining ones for training.</S>
        <S ID="S-20527">It is worth noting that the manuals were not the same in each pair of languages.</S>
        <S ID="S-20528">Even though all training and test sets have similar size, this probably explains why the perplexity varies considerably over the different language pairs.</S>
        <S ID="S-20529">The vocabulary size was computed using the tokenized and true-case corpus.</S>
        <S ID="S-20530">The three bilingual EU corpora were extracted from the Bulletin of the European Union, which exists in all official languages of the European Union (Khadivi and Goutte</S>
      </P>
      <P>
        <S ID="S-20531">Table 1 The Xerox corpora.</S>
        <S ID="S-20532">For all the languages, the training/test full-sentence overlap and the rate of out-of-vocabulary test-set words were less than 10% and 1%, respectively.</S>
        <S ID="S-20533">Trigram models were used to compute the test word perplexity.</S>
        <S ID="S-20534">(K and M denote thousands and millions, respectively.</S>
        <S ID="S-20535">)</S>
      </P>
      <P>
        <S ID="S-20536">2003) and is publicly available on the Internet.</S>
        <S ID="S-20537">The corpora used in the experiments which are described subsequently were again acquired and processed in the framework of the TT2 project.</S>
        <S ID="S-20538">The main features of these corpora are shown in Table 2.</S>
        <S ID="S-20539">The vocabulary size and the training and test set partitions were obtained in a similar way as with the Xerox corpora.</S>
      </P>
      <P>
        <S ID="S-20540">In all the experiments reported in this article, system performance is assessed by comparing test sentence translations produced by the translation systems with the corresponding target language references of the test set.</S>
        <S ID="S-20541">Some of the computed assessment figures measure the quality of the translation engines without any system&#8211;user interactivity: &#65533; Word error rate (WER): The minimum number of substitution, insertion, and deletion operations needed to convert the word strings produced by the translation system into the corresponding single-reference word strings.</S>
        <S ID="S-20542">WER is normalized by the overall number of words in the reference sentences (<REF ID="R-38" RPTR="35">Och and Ney 2003</REF>).</S>
        <S ID="S-20543">Table 2 The EU corpora.</S>
        <S ID="S-20544">For all the languages, the training/test full-sentence overlap and the rate of out-of-vocabulary test-set words were less than 3% and 0.2%, respectively.</S>
        <S ID="S-20545">Trigram models were used to compute the test word perplexity.</S>
        <S ID="S-20546">(K and M denote thousands and millions, respectively.</S>
        <S ID="S-20547">)</S>
      </P>
      <P>
        <S ID="S-20548">n-grams of the hypothesized translation which occur in the reference translations (<REF ID="R-41" RPTR="40">Papineni et al. 2001</REF>).</S>
        <S ID="S-20549">Other assessment figures are aimed at estimating the effort needed by a human translator to produce correct translations using the interactive system.</S>
        <S ID="S-20550">To this end, the target translations which a real user would have in mind are simulated by the given references.</S>
        <S ID="S-20551">The first translation hypothesis for each given source sentence is compared with a single reference translation and the longest common character prefix (LCP) is obtained.</S>
        <S ID="S-20552">The first non-matching character is replaced by the corresponding reference character and then a new system hypothesis is produced.</S>
        <S ID="S-20553">This process is iterated until a full match with the reference is obtained.</S>
        <S ID="S-20554">Each computation of the LCP would correspond to the user looking for the next error and moving the pointer to the corresponding position of the translation hypothesis.</S>
        <S ID="S-20555">Each character replacement, on the other hand, would correspond to a keystroke of the user.</S>
        <S ID="S-20556">If the first non-matching character is the first character of the new system hypothesis in a given iteration, no LCP computation is needed; that is, no pointer movement would be made by the user.</S>
        <S ID="S-20557">Bearing this in mind, we define the following interactive-predictive performance measures:</S>
      </P>
      <P>
        <S ID="S-20558">Keystroke ratio (KSR): Number of keystrokes divided by the total number of reference characters.</S>
        <S ID="S-20559">Mouse-action ratio (MAR): Number of pointer movements plus one more count per sentence (aimed at simulating the user action needed to accept the final translation), divided by the total number of reference characters.</S>
        <S ID="S-20560">Keystroke and mouse-action ratio (KSMR): KSR plus MAR.</S>
        <S ID="S-20561">Note that KSR estimates only the user&#8217;s actions on the keyboard whereas MAR estimates actions for which the user would typically use the mouse.</S>
        <S ID="S-20562">From a user point of view the two types of actions are different and require different types of effort (Macklovitch, Nguyen, and Silva 2005; <REF ID="R-33" RPTR="24">Macklovitch 2006</REF>).</S>
        <S ID="S-20563">In any case, as an approximation, KSMR accounts for both KSR and MAR, assuming that both actions require a similar effort.</S>
        <S ID="S-20564">In the case of SMT systems, it is well known that an automatically computed quality measure like BLEU correlates quite well with human judgment (Callison-Burch, Osborne, and Koehn 2006).</S>
        <S ID="S-20565">In the case of IPMT, we should keep in mind that the main goal of (automatic) assessment is to estimate the effort of the human translator.</S>
        <S ID="S-20566">Moreover, translation quality is not an issue here, because the (simulated) human intervention ensures &#8220;perfect&#8221; translation results.</S>
        <S ID="S-20567">The important question is whether the (estimated) productivity of the human translator can really be increased or not by the IPMT approach.</S>
        <S ID="S-20568">In order to answer this question, the KSR and KSMR measures will be used in the IPMT experiments to be reported in the next section.</S>
        <S ID="S-20569">In order to show the statistical significance of the results, all the assessment figures reported in the next section are accompanied by the corresponding 95% confidence intervals.</S>
        <S ID="S-20570">These intervals have been computed using bootstrap sampling techniques, as proposed by <REF ID="R-05" RPTR="6">Bisani and Ney (2004)</REF>, <REF ID="R-29" RPTR="23">Koehn (2004)</REF>, and Zhang and Vogel (2004).</S>
      </P>
      <P>
        <S ID="S-20571">Two types of results are reported for each corpus and for each translation approach.</S>
        <S ID="S-20572">The first are conventional MT results, obtained as a reference to give an idea of the &#8220;classical&#8221; MT difficulty of the selected tasks.</S>
        <S ID="S-20573">The second aim is to assess the interactive MT (IPMT) approach proposed in this article.</S>
        <S ID="S-20574">The results are presented in different subsections.</S>
        <S ID="S-20575">The first two subsections present the MT and IPMT results for the 1-best translation obtained by the different techniques in the Xerox and EU tasks, respectively.</S>
        <S ID="S-20576">The third subsection presents further IPMT results for the 5-best translations on a single pair of languages.</S>
        <S ID="S-20577">Some of these results may differ from results presented in previous works (<REF ID="R-19" RPTR="20">Cubel et al. 2003</REF>; Och, Zens, and Ney 2003; <REF ID="R-16" RPTR="15">Civera et al. 2004</REF><REF ID="R-17" RPTR="17">Civera et al. 2004</REF>a; <REF ID="R-18" RPTR="18">Cubel et al. 2004</REF>; <REF ID="R-02" RPTR="2">Bender et al. 2005</REF>).</S>
        <S ID="S-20578">The differences are due to variations in the pre-/post-processing procedures and/or recent improvements of the search techniques used by the different systems.</S>
      </P>
      <P>
        <S ID="S-20579">In this section, the translation results obtained using ATs, PBMs, and SFSTs for all six language pairs of the Xerox corpus are reported.</S>
        <S ID="S-20580">Word-based trigram and class-based five-gram target-language models were used for the AT models (the parameters of the log-linear model are tuned so as to minimize WER on a development corpus); wordbased trigram target-language models were used for PBMs and trigrams were used to infer GIATI SFSTs.</S>
        <S ID="S-20581">Off-line MT Results.</S>
        <S ID="S-20582">MT results with ATs, PBMs, and SFSTs are presented in Figure 3.</S>
        <S ID="S-20583">Results obtained using the PBMs are slightly but consistently better that those achieved using the other models.</S>
        <S ID="S-20584">In general, the different techniques perform similarly for the various translation directions.</S>
        <S ID="S-20585">However, the English&#8211;Spanish language pair is the one for which the best translations can be produced.</S>
        <S ID="S-20586">IPMT Results.</S>
        <S ID="S-20587">Performance has been measured in terms of KSRs and MARs (KSR and MAR are represented as the lower and upper portions of each bar, respectively, and KSMR is the whole bar length).</S>
        <S ID="S-20588">The results are shown in Figure 4.</S>
      </P>
      <P>
        <S ID="S-20589">IPMT results for the Xerox corpus.</S>
        <S ID="S-20590">In each bar, KSR is represented by the lower portion, MAR by the upper portion, and KSMR is the whole bar.</S>
        <S ID="S-20591">Segments above the bars show the 95% confidence intervals.</S>
        <S ID="S-20592">En = English; Sp = Spanish; Fr = French; Ge = German.</S>
        <S ID="S-20593">According to these results, a human translator assisted by an AT-based or a SFSTbased interactive system would only need an effort equivalent to typing about 20% of the characters in order to produce the correct translations for the Spanish to English task; or even less than 20% if a PBM-based system is used.</S>
        <S ID="S-20594">For the Xerox task, off-line MT performance and IPMT results show similar tendencies.</S>
        <S ID="S-20595">The PBMs show better performance for both the off-line MT and for the IPMT assessment figures.</S>
        <S ID="S-20596">The AT and SFST models perform more or less equivalently.</S>
        <S ID="S-20597">In both scenarios, the best results were achieved for the Spanish&#8211;English language pair followed by French&#8211;English and German&#8211;English.</S>
        <S ID="S-20598">The computing times needed by all the systems involved in these experiments were well within the range of the on-line operational requirements.</S>
        <S ID="S-20599">The average initial time for each source test sentence was very low (less than 50 msec) for PBMs and SFSTs and adequate for ATs (772 msec).</S>
        <S ID="S-20600">In the case of ATs and SFSTs, this included the time required for the generation of the initial word-graph of each sentence.</S>
        <S ID="S-20601">Moreover, the most critical times incurred in the successive IPMT iterations were very low in all the cases: 18 msec for ATs, 99 msec for PBMs, and 9 msec for SFSTs.</S>
        <S ID="S-20602">Note, however, that these average times are not exactly comparable because of the differences in the computer hardware used by each system (2 Ghz AMD, 1.5 Ghz Pentium, and 2.4 Ghz Pentium for ATs, PBMs, and SFSTs, respectively).</S>
      </P>
      <P>
        <S ID="S-20603">The translation results using the AT, PBM, and SFST approaches for all six language pairs of the EU corpus are reported in this section.</S>
        <S ID="S-20604">As for the Xerox corpora, in the AT experiments, word-based trigram and class-based five-gram target-language models were used; in the PBM experiments, word-based trigram and class-based five-gram target-language models were also used and five-grams were used to infer GIATI SFSTs.</S>
        <S ID="S-20605">Off-line MT Results.</S>
        <S ID="S-20606">Figure 5 presents the results obtained using ATs, PBMs, and SFSTs.</S>
        <S ID="S-20607">Generally speaking, the results are comparable to those obtained on the Xerox corpus with the exception of the English&#8211;Spanish language pair, which were better.</S>
        <S ID="S-20608">With these corpora, the best results were obtained with the ATs and PBMs for all the pairs and the best translation direction was French-to-English with all the models used.</S>
      </P>
      <P>
        <S ID="S-20609">Figure 5 Off-line MT results (BLEU and WER) for the EU corpus.</S>
        <S ID="S-20610">Segments above the bars show the 95% confidence intervals.</S>
        <S ID="S-20611">En = English; Sp = Spanish; Fr = French; Ge = German.</S>
        <S ID="S-20612">IPMT Results.</S>
        <S ID="S-20613">Figure 6 shows the performance of the AT, PBM, and SFST systems in terms of KSRs and MARs in a similar way as for the Xerox corpora.</S>
        <S ID="S-20614">As in the MT experiments, the results are comparable to those obtained on the Xerox corpus, with the exception of the English&#8211;Spanish pair.</S>
        <S ID="S-20615">Similarly, as in MT, the best results were obtained for the French-to-English translation direction.</S>
        <S ID="S-20616">Although EU is a more open-domain task, the results demonstrate again the potential benefit of computer-assisted translation systems.</S>
        <S ID="S-20617">Using PBMs, a human translator would only need an effort equivalent to typing about 20% of the characters in order to produce the correct translations for French-to-English translation direction, whereas for ATs and SFSTs the effort would be about 30%.</S>
        <S ID="S-20618">For the other language pairs, the efforts would be about 20&#8211;30% and 35% of the characters for PBMs and ATs/SFSTs, respectively.</S>
        <S ID="S-20619">The systemwise correlation between MT and IPMT results on this corpus is not as clear as in the Xerox case.</S>
        <S ID="S-20620">One possible cause is the much larger size of the EU corpus compared to the Xerox corpus.</S>
        <S ID="S-20621">In order to run the EU experiments within reasonable time limits, all the systems have required the use of beam search and/or other Figure 6 IPMT results for the EU corpus.</S>
        <S ID="S-20622">In each bar, KSR is represented by the lower portion, MAR by the upper portion and KSMR is the whole bar.</S>
        <S ID="S-20623">Segments above the bars show the 95% confidence intervals.</S>
        <S ID="S-20624">En = English; Sp = Spanish; Fr = French; Ge = German.</S>
      </P>
      <P>
        <S ID="S-20625">suboptimal pruning techniques, although this was largely unnecessary for the Xerox corpus.</S>
        <S ID="S-20626">Clearly, the pruning effects are different in the off-line (MT) and the on-line (IPMT) search processes and the differences may lead to wide performance variations for the AT, PBM, and SFST approaches.</S>
        <S ID="S-20627">Nevertheless, as can be seen in <REF ID="R-02" RPTR="3">Bender et al. (2005)</REF>, the degradation in system performance due to pruning is generally not too substantial and sufficiently accurate real-time interactive operation could also be achieved in the EU task with the three systems tested.</S>
      </P>
      <P>
        <S ID="S-20628">Further experiments were carried out to study the usefulness of n-best hypotheses in the interactive framework.</S>
        <S ID="S-20629">In this scenario, the user can choose one out of n proposed translation suffixes and then proceed as in the usual IPMT paradigm.</S>
        <S ID="S-20630">As with the previous experiments, the automated evaluation is based on a selected target sentence that best matches a prefix of the reference translation in each IPMT iteration (therefore KSR is minimized).</S>
        <S ID="S-20631">Here, only IPMT results for the English-to-Spanish translation direction are reported for both Xerox and EU tasks, using a list of the five best translations.</S>
        <S ID="S-20632">These results are shown in Tables 3 and 4.</S>
        <S ID="S-20633">In all the cases there is a clear and significant accuracy improvement when moving from single-best to 5-best translations.</S>
        <S ID="S-20634">This gain in translation quality diminishes in a log-wise fashion as we increase the number of best translations.</S>
        <S ID="S-20635">From a practical point of view, the improvements provided by using n-best completions would come at the cost of the user having to ponder which of these completions is more suitable.</S>
        <S ID="S-20636">In a real operational environment, this additional user effort may or may not outweigh the</S>
      </P>
      <P>
        <S ID="S-20637">IPMT results reported in the previous section provide reasonable estimations of potential savings of human translator effort, assuming that the goal is to obtain high quality translations.</S>
        <S ID="S-20638">In real work, however, several practical issues not discussed in this article may significantly affect the actual system usability and overall user productivity.</S>
        <S ID="S-20639">One of the most obvious issues is that a carefully designed graphical user interface (GUI) is needed to let the users actually be in command of the translation process, so that they really feel the system is assisting them rather than the other way around.</S>
        <S ID="S-20640">In addition, an adequate GUI has to provide adequate means for the users to easily and intuitively change at will IPMT engine parameters that may have an impact on their way of working with the system.</S>
        <S ID="S-20641">To name just a few: The maximum length of system hypotheses, the value of n for n-best suggestions, or the &#8220;interaction step granularity&#8221;; that is, whether the system should react at each user keystroke, or at the end of each complete typed word, or after a sufficiently long typing pause, and so on.</S>
        <S ID="S-20642">Clearly, all these important issues are beyond the scope of the present article.</S>
        <S ID="S-20643">But we can comment that, in the TT2 project, complete prototypes of some of the systems presented in this article, including the necessary GUI, were actually implemented and thoroughly evaluated by professional human translators in their working environment (Macklovitch, Nguyen, and Silva 2005; <REF ID="R-33" RPTR="25">Macklovitch 2006</REF>).</S>
        <S ID="S-20644">The results of these field tests showed that the actual productivity depended not only on the individual translators, but also on the given test texts.</S>
        <S ID="S-20645">In cases where these texts were quite unrelated to the training data, the system did not significantly help the human translators to increase their productivity.</S>
        <S ID="S-20646">However, when the test texts were reasonably well related to the training data, high productivity gains were registered&#8212; close to what could be expected according to the KSR/MAR empirical results.</S>
        <S ID="S-20647">8.</S>
        <S ID="S-20648">Concluding Remarks The IPMT paradigm proposed in this article allows for a close collaboration between a human translator and a machine translation system.</S>
        <S ID="S-20649">This paradigm entails an iterative process where, in each iteration, a data-driven machine translation engine suggests a completion for the current prefix of a target sentence which a human translator can accept, modify, or ignore.</S>
        <S ID="S-20650">This idea was originally proposed in the TransType project (Langlais, Foster, and Lapalme 2000), where a simple engine was used which only supported single-token suggestions.</S>
        <S ID="S-20651">Furthering these ideas, in the TransType2 project (SchlumbergerSema S.A.</S>
        <S ID="S-20652">et al. 2001), state-of-the-art statistical machine translation systems have been developed and integrated in the IPMT framework.</S>
        <S ID="S-20653">In a laboratory environment, results on two different tasks suggest that the proposed techniques can reduce the typing effort needed to produce a high-quality translation of a given source text by as much as 80% with respect to the effort needed to simply type the whole translation.</S>
        <S ID="S-20654">In real conditions, a high productivity gain was achieved in many cases.</S>
        <S ID="S-20655">We have studied here IPMT from the point of view of a standalone CAT tool.</S>
        <S ID="S-20656">Nevertheless, IPMT can of course be easily and conveniently combined with other popular translator workbench tools.</S>
        <S ID="S-20657">More specifically, IPMT lends itself particularly 24 Barrachina et al.</S>
      </P>
      <P>
        <S ID="S-20658">well to addressing the typical lack of generalization capabilities of translation memories.</S>
        <S ID="S-20659">When used as a CAT tool, translation memories allow the human translator to keep producing increasingly long segments of correct target text.</S>
        <S ID="S-20660">Clearly, these segments can be used by an IPMT engine to suggest to the translator possible translations for source text segments that are not found in the translation memories as exact matches.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS>
    <P>
      <S ID="S-20661">This work has been partially supported by the ST Programme of European Union under grant IST-2001-32091, by the Spanish project TIC&#8211;2003-08681-C02-02, and the Spanish research programme Consolider Ingenio-2010 CSD2007-00018.</S>
      <S ID="S-20662">The authors wish to thank the anonymous reviewers for their criticisms and suggestions.</S>
    </P>
  </ACKNOWLEDGMENTS>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>J C Amengual</RAUTHOR>
      <REFTITLE>The EuTrans-I speech translation system.</REFTITLE>
      <DATE>2000</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>J C Amengual</RAUTHOR>
      <REFTITLE>Efficient error-correcting Viterbi parsing.</REFTITLE>
      <DATE>1998</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>O Bender</RAUTHOR>
      <REFTITLE>Comparison of generation strategies for interactive machine translation.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>A L Berger</RAUTHOR>
      <REFTITLE>Language translation apparatus and method of using context-based translation models. United States Patent No.</REFTITLE>
      <DATE>1996</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>J Berstel</RAUTHOR>
      <REFTITLE>Transductions and Context-Free Languages.B.G.Teubner,Stuttgart.</REFTITLE>
      <DATE>1979</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>M Bisani</RAUTHOR>
      <REFTITLE>Bootstrap estimates for confidence intervals in ASR performance evaluation.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>L Bowker</RAUTHOR>
      <REFTITLE>Computer-Aided Translation Technology: A Practical Introduction, chapter 5: Translation-memory systems. Didactics of Translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>P F Brown</RAUTHOR>
      <REFTITLE>A statistical approach to machine translation.</REFTITLE>
      <DATE>1990</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>P F Brown</RAUTHOR>
      <REFTITLE>The mathematics of statistical machine translation: Parameter estimation.</REFTITLE>
      <DATE>1993</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>C Callison-Burch</RAUTHOR>
      <REFTITLE>Re-evaluating the role of BLEU in machine translation research.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>F Casacuberta</RAUTHOR>
      <REFTITLE>Some approaches to statistical and finite-state speech-to-speech translation. Computer Speech and Language,</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>F Casacuberta</RAUTHOR>
      <REFTITLE>Machine translation with inferred stochastic finite-state transducers.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>F Casacuberta</RAUTHOR>
      <REFTITLE>Learning finite-state models for machine translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>F Casacuberta</RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="14">
      <RAUTHOR></RAUTHOR>
      <REFTITLE>Inference of finite-state transducers from regular languages. Pattern Recognition,</REFTITLE>
      <DATE></DATE>
    </REFERENCE>
    <REFERENCE ID="15">
      <RAUTHOR>F Casacuberta</RAUTHOR>
      <REFTITLE>Pattern recognition approaches for speech-to-speech translation. Cybernetic and Systems:</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="16">
      <RAUTHOR>J Civera</RAUTHOR>
      <REFTITLE>From machine translation to computer assisted translation using finite-state models.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="17">
      <RAUTHOR>J Civera</RAUTHOR>
      <REFTITLE>A syntactic pattern recognition approach to computer assisted translation.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="18">
      <RAUTHOR>E Cubel</RAUTHOR>
      <REFTITLE>Finite-state models for computer assisted translation.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="19">
      <RAUTHOR>E Cubel</RAUTHOR>
      <REFTITLE>Adapting finite-state translation to the TransType2 project.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="20">
      <RAUTHOR>G Foster</RAUTHOR>
      <REFTITLE>Text Prediction for Translators.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="21">
      <RAUTHOR>G Foster</RAUTHOR>
      <REFTITLE>Target-text mediated interactive machine translation.</REFTITLE>
      <DATE>1997</DATE>
    </REFERENCE>
    <REFERENCE ID="22">
      <RAUTHOR>P Isabelle</RAUTHOR>
      <REFTITLE>Special issue on new tools for human translators.</REFTITLE>
      <DATE>1997</DATE>
    </REFERENCE>
    <REFERENCE ID="23">
      <RAUTHOR>F Jelinek</RAUTHOR>
      <REFTITLE>Statistical Methods for Speech Recognition.</REFTITLE>
      <DATE>1998</DATE>
    </REFERENCE>
    <REFERENCE ID="24">
      <RAUTHOR>V M Jim&#233;nez</RAUTHOR>
      <REFTITLE>Computing the k shortest paths: a new algorithm and an experimental comparison.</REFTITLE>
      <DATE>1999</DATE>
    </REFERENCE>
    <REFERENCE ID="25">
      <RAUTHOR>M Kay</RAUTHOR>
      <REFTITLE>The proper place of men and machines in language translation. Machine Translation, 12:3&#8211;23. [This article first appeared as a Xerox PARC Working Paper in</REFTITLE>
      <DATE>1997</DATE>
    </REFERENCE>
    <REFERENCE ID="26">
      <RAUTHOR>S Khadivi</RAUTHOR>
      <REFTITLE>Tools for corpus alignment and evaluation of the alignments (deliverable d4.9).</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="27">
      <RAUTHOR>S Khadivi</RAUTHOR>
      <REFTITLE>Integration of speech to computer-assisted translation using finite-state automata.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="28">
      <RAUTHOR>S Khadivi</RAUTHOR>
      <REFTITLE>Automatic text dictation in computer-assisted translation.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="29">
      <RAUTHOR>P Koehn</RAUTHOR>
      <REFTITLE>Statistical significance tests for machine translation evaluation.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="30">
      <RAUTHOR>P Koehn</RAUTHOR>
      <REFTITLE>Statistical phrase-based translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="31">
      <RAUTHOR>P Langlais</RAUTHOR>
      <REFTITLE>TransType: a computer-aided translation typing system.</REFTITLE>
      <DATE>2000</DATE>
    </REFERENCE>
    <REFERENCE ID="32">
      <RAUTHOR>P Langlais</RAUTHOR>
      <REFTITLE>Transtype: Development-evaluation cycles to boost translator&#8217;s productivity.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="33">
      <RAUTHOR>E Macklovitch</RAUTHOR>
      <REFTITLE>TransType2: The last word.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="34">
      <RAUTHOR>E Macklovitch</RAUTHOR>
      <REFTITLE>User evaluation report.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="35">
      <RAUTHOR>D Marcu</RAUTHOR>
      <REFTITLE>A phrase-based, joint probability model for statistical machine translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="36">
      <RAUTHOR>H Ney</RAUTHOR>
      <REFTITLE>Algorithms for statistical translation of spoken language.</REFTITLE>
      <DATE>2000</DATE>
    </REFERENCE>
    <REFERENCE ID="37">
      <RAUTHOR>F J Och</RAUTHOR>
      <REFTITLE>An efficient method for determining bilingual word classes.</REFTITLE>
      <DATE>1999</DATE>
    </REFERENCE>
    <REFERENCE ID="38">
      <RAUTHOR>F J Och</RAUTHOR>
      <REFTITLE>A systematic comparison of various statistical alignment models.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="39">
      <RAUTHOR>F J Och</RAUTHOR>
      <REFTITLE>The alignment template approach to statistical machine translation.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="40">
      <RAUTHOR>F J Och</RAUTHOR>
      <REFTITLE>Efficient search for interactive statistical machine translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="41">
      <RAUTHOR>K Papineni</RAUTHOR>
      <REFTITLE>BLEU: a method for automatic evaluation of machine translation.</REFTITLE>
      <DATE>2001</DATE>
    </REFERENCE>
    <REFERENCE ID="42">
      <RAUTHOR>D Pic&#243;</RAUTHOR>
      <REFTITLE>Some statistical-estimation methods for stochastic finite-state transducers.</REFTITLE>
      <DATE>2001</DATE>
    </REFERENCE>
    <REFERENCE ID="43">
      <RAUTHOR>W H Press</RAUTHOR>
      <REFTITLE>Numerical Recipes in C++: The Art of Scientific Computing.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="44">
      <RAUTHOR>S A SchlumbergerSema</RAUTHOR>
      <REFTITLE>Intituto Tecnol&#243;gico de Inform&#225;tica, Rheinisch Westf&#228;lische Technische Hochschule Aachen Lehrstul f&#252;r Informatik VI, Recherche Appliqu&#233;e en</REFTITLE>
      <DATE>2001</DATE>
    </REFERENCE>
    <REFERENCE ID="45">
      <RAUTHOR>Zhaoxiong</RAUTHOR>
      <REFTITLE>Interactive approach in machine translation systems.</REFTITLE>
      <DATE>1997</DATE>
    </REFERENCE>
    <REFERENCE ID="46">
      <RAUTHOR>J Slocum</RAUTHOR>
      <REFTITLE>A survey of machine translation: Its history, current status and future prospects.</REFTITLE>
      <DATE>1985</DATE>
    </REFERENCE>
    <REFERENCE ID="47">
      <RAUTHOR>H Somers</RAUTHOR>
      <REFTITLE>Computers and Translation: a Translator&#8217;s Guide, chapter 3: Translation memory systems. John Benjamins,</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="48">
      <RAUTHOR>J Tom&#225;s</RAUTHOR>
      <REFTITLE>Monotone statistical translation using word groups.</REFTITLE>
      <DATE>2001</DATE>
    </REFERENCE>
    <REFERENCE ID="49">
      <RAUTHOR>J Tom&#225;s</RAUTHOR>
      <REFTITLE>Combining phrase-based and template-based alignment models in statistical translation. In Pattern Recognition and Image Analysis,</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="50">
      <RAUTHOR>J Tom&#225;s</RAUTHOR>
      <REFTITLE>Statistical machine translation decoding using target word reordering.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="51">
      <RAUTHOR>J Tom&#225;s</RAUTHOR>
      <REFTITLE>Statistical phrase-based models for interactive computer-assisted translation.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="52">
      <RAUTHOR>J Tom&#225;s</RAUTHOR>
      <REFTITLE>A pattern recognition approach to machine translation: Monotone and non-monotone phrase-based statistical models.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="53">
      <RAUTHOR>M Tomita</RAUTHOR>
      <REFTITLE>Feasibility study of personal/interactive machine translation systems.</REFTITLE>
      <DATE>1985</DATE>
    </REFERENCE>
    <REFERENCE ID="54">
      <RAUTHOR>N Ueffing</RAUTHOR>
      <REFTITLE>Generation of word graphs in statistical machine translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="55">
      <RAUTHOR>E Vidal</RAUTHOR>
      <REFTITLE>Finite-state speech-to-speech translation.</REFTITLE>
      <DATE>1997</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
