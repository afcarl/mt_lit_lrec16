<document>
  <filename>C10-1135</filename>
  <authors/>
  <title>Joint Tokenization and Translation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>As tokenization is usually ambiguous for many natural languages such as Chinese and Korean, tokenization errors might potentially introduce translation mistakes for translation systems that rely on 1-best tokenizations. While using lattices to offer more alternatives to translation systems have elegantly alleviated this problem, we take a further step to tokenize and translate jointly. Taking a sequence of atomic units that can be combined to form words in different ways as input, our joint decoder produces a tokenization on the source side and a translation on the target side simultaneously. By integrating tokenization and translation features in a discriminative framework, our joint decoder outperforms the baseline translation systems using 1-best tokenizations and lattices significantly on both Chinese- English and Korean-Chinese tasks. Interestingly, as a tokenizer, our joint decoder achieves significant improvements over monolingual Chinese tokenizers.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>As tokenization is usually ambiguous for many natural languages such as Chinese and Korean, tokenization errors might potentially introduce translation mistakes for translation systems that rely on 1-best tokenizations.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>While using lattices to offer more alternatives to translation systems have elegantly alleviated this problem, we take a further step to tokenize and translate jointly.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Taking a sequence of atomic units that can be combined to form words in different ways as input, our joint decoder produces a tokenization on the source side and a translation on the target side simultaneously.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>By integrating tokenization and translation features in a discriminative framework, our joint decoder outperforms the baseline translation systems using 1-best tokenizations and lattices significantly on both Chinese- English and Korean-Chinese tasks.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Interestingly, as a tokenizer, our joint decoder achieves significant improvements over monolingual Chinese tokenizers.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Tokenization plays an important role in statistical machine translation (SMT) because tokenizing a source-language sentence is always the first step in SMT systems. Based on the type of input, Mi and Huang (2008) distinguish between two categories of SMT systems : string-based systems (Koehn et al., 2003; Chiang, 2007; Galley et al.,
source
target
source
target
string
string
tokenization tokenize translate translation
(a)
tokenize+translate
(b)
tokenization
translation
2006; Shen et al., 2008) that take a string as input and tree-based systems (Liu et al., 2006; Mi et al., 2008) that take a tree as input. Note that a treebased system still needs to first tokenize the input sentence and then obtain a parse tree or forest of the sentence. As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps.
As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially introduce translation mistakes. As some languages such as Chinese have no spaces in their writing systems, how to segment sentences into appropriate words has a direct impact on translation performance (Xu et al., 2005; Chang et al., 2008; Zhang et al., 2008). In addition, although agglutinative languages such as Korean incorporate spaces between &#8220;words&#8221;, which consist of multiple morphemes, the granularity is too coarse and makes the training data
considerably sparse. Studies reveal that segmenting &#8220;words&#8221; into morphemes effectively improves translating morphologically rich languages (Oflazer, 2008). More importantly, a tokenization close to a gold standard does not necessarily leads to better translation quality (Chang et al., 2008; Zhang et al., 2008). Therefore, it is necessary to offer more tokenizations to SMT systems to alleviate the tokenization error propagation problem. Recently, many researchers have shown that replacing 1-best tokenizations with lattices improves translation performance significantly (Xu et al., 2005; Dyer et al., 2008; Dyer, 2009).
We take a next step towards the direction of offering more tokenizations to SMT systems by proposing joint tokenization and translation. As shown in Figure 1(b), our approach tokenizes and translates jointly to find a tokenization and a translation for a source-language string simultaneously. We integrate translation and tokenization models into a discriminative framework (Och and Ney, 2002), within which tokenization and translation models interact with each other. Experiments show that joint tokenization and translation outperforms its separate counterparts (1- best tokenizations and lattices) significantly on the NIST 2004 and 2005 Chinese-English test sets. Our joint decoder also reports positive results on Korean-Chinese translation. As a tokenizer, our joint decoder achieves significantly better tokenization accuracy than three monolingual Chinese tokenizers.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Tokenization plays an important role in statistical machine translation (SMT) because tokenizing a source-language sentence is always the first step in SMT systems.</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Based on the type of input, Mi and Huang (2008) distinguish between two categories of SMT systems : string-based systems (Koehn et al., 2003; Chiang, 2007; Galley et al.,</text>
              <doc_id>6</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>source</text>
              <doc_id>7</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>target</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>source</text>
              <doc_id>9</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>target</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>string</text>
              <doc_id>11</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>string</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tokenization tokenize translate translation</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(a)</text>
              <doc_id>14</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tokenize+translate</text>
              <doc_id>15</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(b)</text>
              <doc_id>16</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tokenization</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>translation</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2006; Shen et al., 2008) that take a string as input and tree-based systems (Liu et al., 2006; Mi et al., 2008) that take a tree as input.</text>
              <doc_id>19</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Note that a treebased system still needs to first tokenize the input sentence and then obtain a parse tree or forest of the sentence.</text>
              <doc_id>20</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>As shown in Figure 1(a), we refer to this pipeline as separate tokenization and translation because they are divided into single steps.</text>
              <doc_id>21</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>As tokenization for many languages is usually ambiguous, SMT systems that separate tokenization and translation suffer from a major drawback: tokenization errors potentially introduce translation mistakes.</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As some languages such as Chinese have no spaces in their writing systems, how to segment sentences into appropriate words has a direct impact on translation performance (Xu et al., 2005; Chang et al., 2008; Zhang et al., 2008).</text>
              <doc_id>23</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In addition, although agglutinative languages such as Korean incorporate spaces between &#8220;words&#8221;, which consist of multiple morphemes, the granularity is too coarse and makes the training data</text>
              <doc_id>24</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>considerably sparse.</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Studies reveal that segmenting &#8220;words&#8221; into morphemes effectively improves translating morphologically rich languages (Oflazer, 2008).</text>
              <doc_id>26</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>More importantly, a tokenization close to a gold standard does not necessarily leads to better translation quality (Chang et al., 2008; Zhang et al., 2008).</text>
              <doc_id>27</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, it is necessary to offer more tokenizations to SMT systems to alleviate the tokenization error propagation problem.</text>
              <doc_id>28</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Recently, many researchers have shown that replacing 1-best tokenizations with lattices improves translation performance significantly (Xu et al., 2005; Dyer et al., 2008; Dyer, 2009).</text>
              <doc_id>29</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We take a next step towards the direction of offering more tokenizations to SMT systems by proposing joint tokenization and translation.</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As shown in Figure 1(b), our approach tokenizes and translates jointly to find a tokenization and a translation for a source-language string simultaneously.</text>
              <doc_id>31</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We integrate translation and tokenization models into a discriminative framework (Och and Ney, 2002), within which tokenization and translation models interact with each other.</text>
              <doc_id>32</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Experiments show that joint tokenization and translation outperforms its separate counterparts (1- best tokenizations and lattices) significantly on the NIST 2004 and 2005 Chinese-English test sets.</text>
              <doc_id>33</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Our joint decoder also reports positive results on Korean-Chinese translation.</text>
              <doc_id>34</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>As a tokenizer, our joint decoder achieves significantly better tokenization accuracy than three monolingual Chinese tokenizers.</text>
              <doc_id>35</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Separate Tokenization and Translation</title>
        <text>Tokenization is to split a string of characters into meaningful elements, which are often referred to as words. Typically, machine translation separates tokenization from decoding as a preprocessing step. An input string is first preprocessed by a tokenizer, and then is translated based on the tokenized result. Take the SCFG-based model (Chiang, 2007) as an example. Given the character sequence of Figure 2(a), a tokenizer first splits it into the word sequence as shown in Figure 2(b), then the decoder translates the word sequence using the rules in Table 1. This approach makes the translation process simple and efficient. However, it may not be
&#65533; &#65533; &#65533; &#65533; &#65533; &#65533; &#65533;
0 1 2 3 4 5 6 7
optimal for machine translation. Firstly, optimal granularity is unclear for machine translation. We might face severe data sparseness problem by using large granularity, while losing much useful information with small one. Consider the example in Figure 2. It is reasonable to split duo fen into two words as duo and fen, since they have oneto-one alignments to the target side. Nevertheless, while you and wang also have one-to-one alignments, it is risky to segment them into two words. Because the decoder is prone to translate wang as a verb look without the context you. Secondly, there may be tokenization errors. In Figure2(c), tao fei ke is recognized as a Chinese person name with the second name tao and the first name fei-ke, but the whole string tao fei ke should be a name of the Indonesian badminton player.
Therefore, it is necessary to offer more tokenizations to SMT systems to alleviate the tokenization error propagation problem. Recently, many researchers have shown that replacing 1- best tokenizations with lattices improves translation performance significantly. In this approach, a lattice compactly encodes many tokenizations and is fixed before decoding.
1 2
0 1 2 3 4 5 6 7</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Tokenization is to split a string of characters into meaningful elements, which are often referred to as words.</text>
              <doc_id>36</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Typically, machine translation separates tokenization from decoding as a preprocessing step.</text>
              <doc_id>37</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>An input string is first preprocessed by a tokenizer, and then is translated based on the tokenized result.</text>
              <doc_id>38</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Take the SCFG-based model (Chiang, 2007) as an example.</text>
              <doc_id>39</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Given the character sequence of Figure 2(a), a tokenizer first splits it into the word sequence as shown in Figure 2(b), then the decoder translates the word sequence using the rules in Table 1.</text>
              <doc_id>40</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This approach makes the translation process simple and efficient.</text>
              <doc_id>41</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>However, it may not be</text>
              <doc_id>42</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#65533; &#65533; &#65533; &#65533; &#65533; &#65533; &#65533;</text>
              <doc_id>43</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0 1 2 3 4 5 6 7</text>
              <doc_id>44</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>optimal for machine translation.</text>
              <doc_id>45</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Firstly, optimal granularity is unclear for machine translation.</text>
              <doc_id>46</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We might face severe data sparseness problem by using large granularity, while losing much useful information with small one.</text>
              <doc_id>47</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Consider the example in Figure 2.</text>
              <doc_id>48</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>It is reasonable to split duo fen into two words as duo and fen, since they have oneto-one alignments to the target side.</text>
              <doc_id>49</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Nevertheless, while you and wang also have one-to-one alignments, it is risky to segment them into two words.</text>
              <doc_id>50</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Because the decoder is prone to translate wang as a verb look without the context you.</text>
              <doc_id>51</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Secondly, there may be tokenization errors.</text>
              <doc_id>52</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>In Figure2(c), tao fei ke is recognized as a Chinese person name with the second name tao and the first name fei-ke, but the whole string tao fei ke should be a name of the Indonesian badminton player.</text>
              <doc_id>53</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Therefore, it is necessary to offer more tokenizations to SMT systems to alleviate the tokenization error propagation problem.</text>
              <doc_id>54</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Recently, many researchers have shown that replacing 1- best tokenizations with lattices improves translation performance significantly.</text>
              <doc_id>55</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In this approach, a lattice compactly encodes many tokenizations and is fixed before decoding.</text>
              <doc_id>56</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 2</text>
              <doc_id>57</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0 1 2 3 4 5 6 7</text>
              <doc_id>58</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Joint Tokenization and Translation</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>59</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Model</title>
            <text>We take a next step towards the direction of offering more tokenizations to SMT systems by proposing joint tokenization and translation. As shown in Figure 1(b), the decoder takes an untokenized string as input, and then tokenizes the source side string while building the corresponding translation of the target side. Since the traditional rules like those in Table 1 natively include tokenization information, we can directly apply them for simultaneous construction of tokenization and translation by the source side and target side of rules respectively. In Figure 3, our joint model takes the character sequence in Figure 2(a) as input, and synchronously conducts both translation and tokenization using the rules in Table 1.
As our model conducts tokenization during decoding, we can integrate tokenization models as features together with translation features under the discriminative framework. We expect tokenization and translation could collaborate with each other. Tokenization offers translation with good tokenized results, while translation helps tokenization to eliminate ambiguity. Formally, the probability of a derivation D is represented as
P (D) &#8733; &#8719; i
&#966; i (D) &#955; i (1)
where &#966; i are features defined on derivations including translation and tokenization, and &#955; i are feature weights. We totally use 16 features:
&#8226; 8 traditional translation features (Chiang, 2007): 4 rule scores (direct and reverse translation scores; direct and reverse lexical translation scores); language model of the target side; 3 penalties for word count, extracted rule and glue rule.
&#8226; 8 tokenization features: maximum entropy model, language model and word count of the source side (Section 3.2). To handle the Out Of Vocabulary (OOV) problem (Section 3.3), we also introduce 5 OOV features: OOV character count and 4 OOV discount features.
Since our model is still a string-based model, the CKY algorithm and cube pruning are still applicable for our model to find the derivation with max score.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We take a next step towards the direction of offering more tokenizations to SMT systems by proposing joint tokenization and translation.</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As shown in Figure 1(b), the decoder takes an untokenized string as input, and then tokenizes the source side string while building the corresponding translation of the target side.</text>
                  <doc_id>61</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Since the traditional rules like those in Table 1 natively include tokenization information, we can directly apply them for simultaneous construction of tokenization and translation by the source side and target side of rules respectively.</text>
                  <doc_id>62</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In Figure 3, our joint model takes the character sequence in Figure 2(a) as input, and synchronously conducts both translation and tokenization using the rules in Table 1.</text>
                  <doc_id>63</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As our model conducts tokenization during decoding, we can integrate tokenization models as features together with translation features under the discriminative framework.</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We expect tokenization and translation could collaborate with each other.</text>
                  <doc_id>65</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Tokenization offers translation with good tokenized results, while translation helps tokenization to eliminate ambiguity.</text>
                  <doc_id>66</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Formally, the probability of a derivation D is represented as</text>
                  <doc_id>67</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (D) &#8733; &#8719; i</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#966; i (D) &#955; i (1)</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where &#966; i are features defined on derivations including translation and tokenization, and &#955; i are feature weights.</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We totally use 16 features:</text>
                  <doc_id>71</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; 8 traditional translation features (Chiang, 2007): 4 rule scores (direct and reverse translation scores; direct and reverse lexical translation scores); language model of the target side; 3 penalties for word count, extracted rule and glue rule.</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; 8 tokenization features: maximum entropy model, language model and word count of the source side (Section 3.2).</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To handle the Out Of Vocabulary (OOV) problem (Section 3.3), we also introduce 5 OOV features: OOV character count and 4 OOV discount features.</text>
                  <doc_id>74</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Since our model is still a string-based model, the CKY algorithm and cube pruning are still applicable for our model to find the derivation with max score.</text>
                  <doc_id>75</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Adding Tokenization Features</title>
            <text>Maximum Entropy model (ME). We first introduce ME model feature for tokenization by casting it as a labeling problem (Xue and Shen, 2003; Ng and Low, 2004). We label a character with the following 4 types:
&#8226; b: the begin of a word
&#8226; m: the middle of a word
&#8226; e: the end of a word
&#8226; s: a single-character word
Taking the tokenization you-wang of the string you wang for example, we first create a label sequence b e for the tokenization you-wang and then calculate the probability of tokenization by
P (you-wang | you wang)
= P (b e | you wang)
= P (b | you, you wang)
&#215; P (e | wang, you wang)
Given a tokenization w1 L with L words for a character sequence c n 1 , we firstly create labels ln 1 for every characters and then calculate the probability by
n
P (w1 L |cn 1 ) = P (ln 1 |cn 1 ) = &#8719; P (l i |c i , c n 1 ) (2)
i=1
Under the ME framework, the probability of assigning the character c with the label l is represented as:
P (l|c, c n 1 ) = exp[&#8721; i &#955; ih i (l, c, c n 1 )] &#8721;l &#8242; exp[&#8721; i &#955; ih i (l &#8242; , c, c n 1 )] (3)
where h i is feature function, &#955; i is the feature weight of h i . We use the feature templates the same as Jiang et al., (2008) to extract features for ME model. Since we directly construct tokenization when decoding, it is straight to calculate the ME model score of a tokenization according to formula (2) and (3).
Language Model (LM). We also use the n- gram language model to calculate the probability of a tokenization w L 1 :
P (w L 1 ) = L &#8719;
i=1
P (w i |w i&#8722;1 i&#8722;n+1 ) (4)
For instance, we compute the probability of the tokenization shown in Figure 2(b) under a 3-gram model by
P (tao-fei-ke)
&#215;P (you-wang | tao-fei-ke)
&#215;P (duo | tao-fei-ke, you-wang)
&#215;P (fen | you-wang, duo)
Word Count (WC). This feature counts the number of words in a tokenization. Language model is prone to assign higher probabilities to short sentences in a biased way. This feature can compensate this bias by encouraging long sentences. Furthermore, using this feature, we can optimize the granularity of tokenization for translation. If larger granularity is preferable for translation, then we can use this feature to punish the tokenization containing more words.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Maximum Entropy model (ME).</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We first introduce ME model feature for tokenization by casting it as a labeling problem (Xue and Shen, 2003; Ng and Low, 2004).</text>
                  <doc_id>77</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We label a character with the following 4 types:</text>
                  <doc_id>78</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; b: the begin of a word</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; m: the middle of a word</text>
                  <doc_id>80</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; e: the end of a word</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; s: a single-character word</text>
                  <doc_id>82</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Taking the tokenization you-wang of the string you wang for example, we first create a label sequence b e for the tokenization you-wang and then calculate the probability of tokenization by</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (you-wang | you wang)</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>= P (b e | you wang)</text>
                  <doc_id>85</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>= P (b | you, you wang)</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#215; P (e | wang, you wang)</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Given a tokenization w1 L with L words for a character sequence c n 1 , we firstly create labels ln 1 for every characters and then calculate the probability by</text>
                  <doc_id>88</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (w1 L |cn 1 ) = P (ln 1 |cn 1 ) = &#8719; P (l i |c i , c n 1 ) (2)</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Under the ME framework, the probability of assigning the character c with the label l is represented as:</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (l|c, c n 1 ) = exp[&#8721; i &#955; ih i (l, c, c n 1 )] &#8721;l &#8242; exp[&#8721; i &#955; ih i (l &#8242; , c, c n 1 )] (3)</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where h i is feature function, &#955; i is the feature weight of h i .</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We use the feature templates the same as Jiang et al., (2008) to extract features for ME model.</text>
                  <doc_id>95</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Since we directly construct tokenization when decoding, it is straight to calculate the ME model score of a tokenization according to formula (2) and (3).</text>
                  <doc_id>96</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Language Model (LM).</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We also use the n- gram language model to calculate the probability of a tokenization w L 1 :</text>
                  <doc_id>98</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (w L 1 ) = L &#8719;</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (w i |w i&#8722;1 i&#8722;n+1 ) (4)</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For instance, we compute the probability of the tokenization shown in Figure 2(b) under a 3-gram model by</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (tao-fei-ke)</text>
                  <doc_id>103</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#215;P (you-wang | tao-fei-ke)</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#215;P (duo | tao-fei-ke, you-wang)</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#215;P (fen | you-wang, duo)</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Word Count (WC).</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This feature counts the number of words in a tokenization.</text>
                  <doc_id>108</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Language model is prone to assign higher probabilities to short sentences in a biased way.</text>
                  <doc_id>109</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This feature can compensate this bias by encouraging long sentences.</text>
                  <doc_id>110</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Furthermore, using this feature, we can optimize the granularity of tokenization for translation.</text>
                  <doc_id>111</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>If larger granularity is preferable for translation, then we can use this feature to punish the tokenization containing more words.</text>
                  <doc_id>112</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Considering All Tokenizations</title>
            <text>Obviously, we can construct the potential tokenizations and translations by only using the extracted rules, in line with traditional translation decoding. However, it may limits the potential tokenization space. Consider a string you wang. If you-wang is not reachable by the extracted rules, the tokenization you-wang will never be considered under this way. However, the decoder may still create a derivation by splitting the string as small as possible with tokenization you wang and translating you with a and wang with look, which may hurt the translation performance. This case happens frequently for named entity especially. Overall, it is necessary to assure that the decoder can derive all potential tokenizations (Section 4.1.3).
To assure that, when a span is not tokenized into a single word by the extracted rules, we will add an operation, which is considering the entire span as an OOV. That is, we tokenize the entire span into a single word with a translation that is the copy of source side. We can define the set of all potential tokenizations &#964;(c n 1 ) for the character sequence c n 1 in a recursive way by
n&#8722;1
&#964;(c n 1 ) = &#8899; {&#964;(c i 1 ) &#8855; {w(c n i+1 )}} (5)
i
here w(c n i+1 ) means a word contains characters c n i+1 and &#8855; means the times of two sets. According to this recursive definition, it is easy to prove that all tokenizations is reachable by using the glue rule (S &#8658; SX, SX) and the added operation. Here, glue rule is used to concatenate the translation and tokenization of the two variables S and X, which acts the role of the operator &#8855; in equation (5). Consequently, this introduces a large number of OOVs. In order to control the generation of OOVs, we introduce the following OOV features: OOV Character Count (OCC). This feature counts the number of characters covered by OOV. We can control the number of OOV characters by this feature. It counts 3 when tao-fei-ke is an OOV, since tao-fei-ke has 3 characters.
OOV Discount (OD). The chances to be OOVs vary for words with different counts of characters. We can directly attack this problem by adding features OD i that reward or punish OOV words which contains with i characters, or OD i,j for OOVs contains with i to j characters. 4 OD features are used in this paper: 1, 2, 3 and 4+. For example, OD 3 counts 1 when the word tao-fei-ke is an OOV.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Obviously, we can construct the potential tokenizations and translations by only using the extracted rules, in line with traditional translation decoding.</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, it may limits the potential tokenization space.</text>
                  <doc_id>114</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Consider a string you wang.</text>
                  <doc_id>115</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>If you-wang is not reachable by the extracted rules, the tokenization you-wang will never be considered under this way.</text>
                  <doc_id>116</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>However, the decoder may still create a derivation by splitting the string as small as possible with tokenization you wang and translating you with a and wang with look, which may hurt the translation performance.</text>
                  <doc_id>117</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This case happens frequently for named entity especially.</text>
                  <doc_id>118</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Overall, it is necessary to assure that the decoder can derive all potential tokenizations (Section 4.1.3).</text>
                  <doc_id>119</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To assure that, when a span is not tokenized into a single word by the extracted rules, we will add an operation, which is considering the entire span as an OOV.</text>
                  <doc_id>120</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>That is, we tokenize the entire span into a single word with a translation that is the copy of source side.</text>
                  <doc_id>121</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We can define the set of all potential tokenizations &#964;(c n 1 ) for the character sequence c n 1 in a recursive way by</text>
                  <doc_id>122</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n&#8722;1</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#964;(c n 1 ) = &#8899; {&#964;(c i 1 ) &#8855; {w(c n i+1 )}} (5)</text>
                  <doc_id>124</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>here w(c n i+1 ) means a word contains characters c n i+1 and &#8855; means the times of two sets.</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>According to this recursive definition, it is easy to prove that all tokenizations is reachable by using the glue rule (S &#8658; SX, SX) and the added operation.</text>
                  <doc_id>127</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Here, glue rule is used to concatenate the translation and tokenization of the two variables S and X, which acts the role of the operator &#8855; in equation (5).</text>
                  <doc_id>128</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Consequently, this introduces a large number of OOVs.</text>
                  <doc_id>129</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In order to control the generation of OOVs, we introduce the following OOV features: OOV Character Count (OCC).</text>
                  <doc_id>130</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This feature counts the number of characters covered by OOV.</text>
                  <doc_id>131</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>We can control the number of OOV characters by this feature.</text>
                  <doc_id>132</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>It counts 3 when tao-fei-ke is an OOV, since tao-fei-ke has 3 characters.</text>
                  <doc_id>133</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>OOV Discount (OD).</text>
                  <doc_id>134</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The chances to be OOVs vary for words with different counts of characters.</text>
                  <doc_id>135</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We can directly attack this problem by adding features OD i that reward or punish OOV words which contains with i characters, or OD i,j for OOVs contains with i to j characters.</text>
                  <doc_id>136</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>4 OD features are used in this paper: 1, 2, 3 and 4+.</text>
                  <doc_id>137</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For example, OD 3 counts 1 when the word tao-fei-ke is an OOV.</text>
                  <doc_id>138</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Experiments</title>
        <text>In this section, we try to answer the following questions:
1. Does the joint method outperform conventional methods that separate tokenization from decoding. (Section 4.1)
2. How about the tokenization performance of the joint decoder? (Section 4.2)</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this section, we try to answer the following questions:</text>
              <doc_id>139</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1.</text>
              <doc_id>140</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Does the joint method outperform conventional methods that separate tokenization from decoding.</text>
              <doc_id>141</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>(Section 4.1)</text>
              <doc_id>142</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.</text>
              <doc_id>143</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>How about the tokenization performance of the joint decoder?</text>
              <doc_id>144</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>(Section 4.2)</text>
              <doc_id>145</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Translation Evaluation</title>
            <text>We use the SCFG model (Chiang, 2007) for our experiments. We firstly work on the Chinese- English translation task. The bilingual training data contains 1.5M sentence pairs coming from LDC data. 1 The monolingual data for training English language model includes Xinhua portion of the GIGAWORD corpus, which contains 238M English words. We use the NIST evaluation sets of 2002 (MT02) as our development data set, and sets of 2004(MT04) and 2005(MT05) as test sets. We use the corpus derived from the People&#8217;s Daily (Renmin Ribao) in Feb. to Jun. 1998 containing 6M words for training LM and ME tokenization models.
Translation Part. We used GIZA++ (Och and Ney, 2003) to perform word alignment in both directions, and grow-diag-final-and (Koehn et al., 2003) to generate symmetric word alignment. We extracted the SCFG rules as describing in Chiang (2007). The language model were trained by the
SRILM toolkit (Stolcke, 2002). 2 Case insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance.
Tokenization Part. We used the toolkit implemented by Zhang (2004) to train the ME model. Three Chinese word segmenters were used for comparing: ICTCLAS (ICT) developed by institute of Computing Technology Chinese Academy of Sciences (Zhang et al., 2003); SF developed at Stanford University (Huihsin et al., 2005) and ME which exploits the ME model described in section (3.2).
4.1.1 Joint Vs. Separate
We compared our joint tokenization and translation with the conventional separate methods. The input of separate tokenization and translation can either be a single segmentation or a lattice. The lattice combines the 1-best segmentations of segmenters. Same as Dyer et al., (2008), we also extracted rules from a combined bilingual corpus which contains three copies from different segmenters. We refer to this version of rules as All.
Table 2 shows the result. 3 Using all rule table, our joint method significantly outperforms the best single system SF by +1.96 and +1.66 points on MT04 and MT05 respectively, and also outperforms the lattice-based system by +1.46 and +0.93 points. However, the 8 tokenization features have small impact on the lattice system, probably because the tokenization space limited
2 The calculation of LM probabilities for OOVs is done
by the SRILM without special treatment by ourself. 3 The weights are retrained for different test conditions, so
do the experiments in other sections.
ME LM WC OCC OD MT05 &#215; &#215; &#215; &#215; &#215; 24.97 &#8730; &#8730; &#215; &#215; &#215; &#215; 25.30
&#215; &#8730; &#215; &#215; &#215; 24.70
&#215; &#215; &#8730; &#215; &#215; 24.84
&#215; &#215; &#215; &#8730; &#215; 25.51
&#215; &#215; &#215; &#215; 25.34 &#8730; &#8730;
&#8730; &#215; &#8730; &#8730; &#8730; &#215; &#8730; &#215; 25.74
26.37
by lattice has been created from good tokenization. Not surprisingly, our decoding method is about 2.6 times slower than lattice method with tokenization features, since the joint decoder takes character sequences as input, which is about 1.7 times longer than the corresponding word sequences tokenized by segmenters. (Section 4.1.4). The number of extracted rules with different segment methods are quite close, while the All version contains about 45% more rules than the single systems. With the same rule table, our joint method improves the performance over separate method up to +3.03 and +3.26 points (ME). Interestingly, comparing with the separate method, the tokenization of training data has smaller effect on joint method. The BLEU scores of MT04 and MT05 fluctuate about 0.5 and 0.7 points when applying the joint method, while the difference of separate method is up to 2 and 3 points respectively. It shows that the joint method is more robust to segmentation performance.
4.1.2 Effect of Tokenization Model
We also investigated the effect of tokenization features on translation. In order to reduce the time for tuning weights and decoding, we extracted rules from the FBIS part of the bilingual corpus, and trained a 4-gram English language model on the English side of FBIS.
Table 3 shows the result. Only using the 8 translation features, our system achieves a BLEU score of 24.97. By activating all tokenization features, the joint decoder obtains an absolute improvement by 1.4 BLEU points. When only adding one single tokenization feature, the LM and WC fail to show improvement, which may result from their bias to short or long tokenizations. How- ever, these two features have complementary advantages and collaborate well when using them together (line 8). The OCC and OD features also contribute improvements which reflects the fact that handling the generation of OOV is important for the joint model.
4.1.3 Considering All Tokenizations?
In order to explain the necessary of considering all potential tokenizations, we compare the performances of whether to tokenize a span as a single word or not as illustrated in section 3.3. When only tokenizing by the extracted rules, we obtain 34.37 BLEU on MT05, which is about 0.5 points lower than considering all tokenizations shown in Table 2. This indicates that spuriously limitation of the tokenization space may degenerate translation performance.
4.1.4 Results Analysis
To better understand why the joint method can improve the translation quality, this section shows some details of the results on the MT05 data set.
Table 4 shows the granularity and OOV word counts of different configurations. The lattice method reduces the OOV words quite a lot which is 23% and 70% comparing with ICT and ME. In contrast, the joint method gain an absolute improvement even thought the OOV count do not decrease. It seems the lattice method prefers to translate more characters (since smaller granularity and less OOVs), while our method is inclined to maintain integrity of words (since larger granularity and more OOVs). This also explains the difficulty of deciding optimal tokenization for translation before decoding. There are some named entities or idioms that
are split into smaller granularity by the segmenters. For example:&#8220;&#65533;&#252;&#8221; which is an English name &#8220;Stone&#8221; or &#8220;&#65533;-&#65533;-&#217;&#8221; which means &#8220;teenage&#8221;. Although the separate method is possible to translate them using smaller granularity, the translation results are in fact wrong. In contrast, the joint method tokenizes them as entire OOV words, however, it may result a better translation for the whole sentence.
We also count the overlap of the segments used by the Joint All system towards the single segmentation systems. The tokenization result of Joint All contains 29, 644 words, and shares 28, 159 , 27, 772 and 27, 407 words with ICT , SF and ME respectively. And 46 unique words appear only in the joint method, where most of them are named entity.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We use the SCFG model (Chiang, 2007) for our experiments.</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We firstly work on the Chinese- English translation task.</text>
                  <doc_id>147</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The bilingual training data contains 1.5M sentence pairs coming from LDC data.</text>
                  <doc_id>148</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>1 The monolingual data for training English language model includes Xinhua portion of the GIGAWORD corpus, which contains 238M English words.</text>
                  <doc_id>149</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We use the NIST evaluation sets of 2002 (MT02) as our development data set, and sets of 2004(MT04) and 2005(MT05) as test sets.</text>
                  <doc_id>150</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We use the corpus derived from the People&#8217;s Daily (Renmin Ribao) in Feb.</text>
                  <doc_id>151</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>to Jun.</text>
                  <doc_id>152</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>1998 containing 6M words for training LM and ME tokenization models.</text>
                  <doc_id>153</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Translation Part.</text>
                  <doc_id>154</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We used GIZA++ (Och and Ney, 2003) to perform word alignment in both directions, and grow-diag-final-and (Koehn et al., 2003) to generate symmetric word alignment.</text>
                  <doc_id>155</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We extracted the SCFG rules as describing in Chiang (2007).</text>
                  <doc_id>156</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The language model were trained by the</text>
                  <doc_id>157</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>SRILM toolkit (Stolcke, 2002).</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>2 Case insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance.</text>
                  <doc_id>159</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Tokenization Part.</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We used the toolkit implemented by Zhang (2004) to train the ME model.</text>
                  <doc_id>161</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Three Chinese word segmenters were used for comparing: ICTCLAS (ICT) developed by institute of Computing Technology Chinese Academy of Sciences (Zhang et al., 2003); SF developed at Stanford University (Huihsin et al., 2005) and ME which exploits the ME model described in section (3.2).</text>
                  <doc_id>162</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.1.1 Joint Vs.</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Separate</text>
                  <doc_id>164</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We compared our joint tokenization and translation with the conventional separate methods.</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The input of separate tokenization and translation can either be a single segmentation or a lattice.</text>
                  <doc_id>166</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The lattice combines the 1-best segmentations of segmenters.</text>
                  <doc_id>167</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Same as Dyer et al., (2008), we also extracted rules from a combined bilingual corpus which contains three copies from different segmenters.</text>
                  <doc_id>168</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We refer to this version of rules as All.</text>
                  <doc_id>169</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 2 shows the result.</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>3 Using all rule table, our joint method significantly outperforms the best single system SF by +1.96 and +1.66 points on MT04 and MT05 respectively, and also outperforms the lattice-based system by +1.46 and +0.93 points.</text>
                  <doc_id>171</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, the 8 tokenization features have small impact on the lattice system, probably because the tokenization space limited</text>
                  <doc_id>172</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 The calculation of LM probabilities for OOVs is done</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>by the SRILM without special treatment by ourself.</text>
                  <doc_id>174</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>3 The weights are retrained for different test conditions, so</text>
                  <doc_id>175</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>do the experiments in other sections.</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>ME LM WC OCC OD MT05 &#215; &#215; &#215; &#215; &#215; 24.97 &#8730; &#8730; &#215; &#215; &#215; &#215; 25.30</text>
                  <doc_id>177</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#215; &#8730; &#215; &#215; &#215; 24.70</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#215; &#215; &#8730; &#215; &#215; 24.84</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#215; &#215; &#215; &#8730; &#215; 25.51</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#215; &#215; &#215; &#215; 25.34 &#8730; &#8730;</text>
                  <doc_id>181</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8730; &#215; &#8730; &#8730; &#8730; &#215; &#8730; &#215; 25.74</text>
                  <doc_id>182</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>26.37</text>
                  <doc_id>183</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>by lattice has been created from good tokenization.</text>
                  <doc_id>184</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Not surprisingly, our decoding method is about 2.6 times slower than lattice method with tokenization features, since the joint decoder takes character sequences as input, which is about 1.7 times longer than the corresponding word sequences tokenized by segmenters.</text>
                  <doc_id>185</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>(Section 4.1.4).</text>
                  <doc_id>186</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The number of extracted rules with different segment methods are quite close, while the All version contains about 45% more rules than the single systems.</text>
                  <doc_id>187</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>With the same rule table, our joint method improves the performance over separate method up to +3.03 and +3.26 points (ME).</text>
                  <doc_id>188</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Interestingly, comparing with the separate method, the tokenization of training data has smaller effect on joint method.</text>
                  <doc_id>189</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The BLEU scores of MT04 and MT05 fluctuate about 0.5 and 0.7 points when applying the joint method, while the difference of separate method is up to 2 and 3 points respectively.</text>
                  <doc_id>190</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>It shows that the joint method is more robust to segmentation performance.</text>
                  <doc_id>191</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.1.2 Effect of Tokenization Model</text>
                  <doc_id>192</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We also investigated the effect of tokenization features on translation.</text>
                  <doc_id>193</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In order to reduce the time for tuning weights and decoding, we extracted rules from the FBIS part of the bilingual corpus, and trained a 4-gram English language model on the English side of FBIS.</text>
                  <doc_id>194</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 3 shows the result.</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Only using the 8 translation features, our system achieves a BLEU score of 24.97.</text>
                  <doc_id>196</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>By activating all tokenization features, the joint decoder obtains an absolute improvement by 1.4 BLEU points.</text>
                  <doc_id>197</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>When only adding one single tokenization feature, the LM and WC fail to show improvement, which may result from their bias to short or long tokenizations.</text>
                  <doc_id>198</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>How- ever, these two features have complementary advantages and collaborate well when using them together (line 8).</text>
                  <doc_id>199</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The OCC and OD features also contribute improvements which reflects the fact that handling the generation of OOV is important for the joint model.</text>
                  <doc_id>200</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.1.3 Considering All Tokenizations?</text>
                  <doc_id>201</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In order to explain the necessary of considering all potential tokenizations, we compare the performances of whether to tokenize a span as a single word or not as illustrated in section 3.3.</text>
                  <doc_id>202</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>When only tokenizing by the extracted rules, we obtain 34.37 BLEU on MT05, which is about 0.5 points lower than considering all tokenizations shown in Table 2.</text>
                  <doc_id>203</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This indicates that spuriously limitation of the tokenization space may degenerate translation performance.</text>
                  <doc_id>204</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.1.4 Results Analysis</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To better understand why the joint method can improve the translation quality, this section shows some details of the results on the MT05 data set.</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 4 shows the granularity and OOV word counts of different configurations.</text>
                  <doc_id>207</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The lattice method reduces the OOV words quite a lot which is 23% and 70% comparing with ICT and ME.</text>
                  <doc_id>208</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast, the joint method gain an absolute improvement even thought the OOV count do not decrease.</text>
                  <doc_id>209</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>It seems the lattice method prefers to translate more characters (since smaller granularity and less OOVs), while our method is inclined to maintain integrity of words (since larger granularity and more OOVs).</text>
                  <doc_id>210</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This also explains the difficulty of deciding optimal tokenization for translation before decoding.</text>
                  <doc_id>211</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>There are some named entities or idioms that</text>
                  <doc_id>212</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>are split into smaller granularity by the segmenters.</text>
                  <doc_id>213</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For example:&#8220;&#65533;&#252;&#8221; which is an English name &#8220;Stone&#8221; or &#8220;&#65533;-&#65533;-&#217;&#8221; which means &#8220;teenage&#8221;.</text>
                  <doc_id>214</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Although the separate method is possible to translate them using smaller granularity, the translation results are in fact wrong.</text>
                  <doc_id>215</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast, the joint method tokenizes them as entire OOV words, however, it may result a better translation for the whole sentence.</text>
                  <doc_id>216</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We also count the overlap of the segments used by the Joint All system towards the single segmentation systems.</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The tokenization result of Joint All contains 29, 644 words, and shares 28, 159 , 27, 772 and 27, 407 words with ICT , SF and ME respectively.</text>
                  <doc_id>218</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>And 46 unique words appear only in the joint method, where most of them are named entity.</text>
                  <doc_id>219</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Chinese Word Segmentation Evaluation</title>
            <text>We also test the tokenization performance of our model on Chinese word segmentation task. We randomly selected 3k sentences from the corpus of People&#8217;s Daily in Jan. 1998. 1k sentences were used for tuning weights, while the other 2k sentences were for testing. We use MERT (Och, 2003) to tune the weights by minimizing the error measured by F 1 score.
As shown in Table 5, with all features activated, our joint decoder achieves an F 1 score of 97.70 which reduces the tokenization error comparing with the best single segmenter ICT by 8.7%. Similar to the translation performance evaluation, our joint decoder outperforms the best segmenter with any version of rule tables.
4.2.1 Effect of Target Side Information
We compared the effect of the 4 Rule Scores (RS), target side Language Model (LM) on tokenization. Table 6 shows the effect on Chinese word segmentation. When only use tokenization features, our joint decoder achieves an F 1 score of 97.37. Only integrating language model or rule scores, the joint decoder achieves an absolute improvement of 0.3 point in F 1 score, which reduces the error rate by 11.4%. However, when combining them together, the F 1 score deduces slightly, which may result from the weight tuning. Using all feature, the performance comes to 97.70. Overall, our experiment shows that the target side information can improve the source side tokenization under a supervised way, and outperform stateof-the-art systems.
4.2.2 Best Tokenization = Best Translation?
Previous works (Zhang et al., 2008; Chang et al., 2008) have shown that preprocessing the input string for decoder by better segmenters do not always improve the translation quality, we reverify this by testing whether the joint decoder produces good tokenization and good translation at the same time. To answer the question, we used the feature weights optimized by maximizing BLEU for tokenization and used the weights optimized by maximizing F 1 for translation. We test BLEU on MT05 and F 1 score on the test data used in segmentation evaluation experiments. By tuning weights regarding to BLEU (the configuration for Joint All in table 2), our decoder achieves a BLEU score of 34.88 and an F 1 score of 92.49. Similarly, maximizing F 1 (the configuration for the last line in table 6) leads to a much lower BLEU of 27.43, although the F 1 is up to 97.70. This suggests that better tokenization may not always lead to better translations and vice versa
Rule #Rule Method Test Time Morph 46M 21.61 4.12 Separate Refined 55M 21.21 4.63 All 74M Joint 21.93* 5.10
even by the joint decoding. This also indicates the hard of artificially defining the best tokenization for translation.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We also test the tokenization performance of our model on Chinese word segmentation task.</text>
                  <doc_id>220</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We randomly selected 3k sentences from the corpus of People&#8217;s Daily in Jan.</text>
                  <doc_id>221</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>1998.</text>
                  <doc_id>222</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>1k sentences were used for tuning weights, while the other 2k sentences were for testing.</text>
                  <doc_id>223</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We use MERT (Och, 2003) to tune the weights by minimizing the error measured by F 1 score.</text>
                  <doc_id>224</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As shown in Table 5, with all features activated, our joint decoder achieves an F 1 score of 97.70 which reduces the tokenization error comparing with the best single segmenter ICT by 8.7%.</text>
                  <doc_id>225</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Similar to the translation performance evaluation, our joint decoder outperforms the best segmenter with any version of rule tables.</text>
                  <doc_id>226</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.2.1 Effect of Target Side Information</text>
                  <doc_id>227</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We compared the effect of the 4 Rule Scores (RS), target side Language Model (LM) on tokenization.</text>
                  <doc_id>228</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Table 6 shows the effect on Chinese word segmentation.</text>
                  <doc_id>229</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>When only use tokenization features, our joint decoder achieves an F 1 score of 97.37.</text>
                  <doc_id>230</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Only integrating language model or rule scores, the joint decoder achieves an absolute improvement of 0.3 point in F 1 score, which reduces the error rate by 11.4%.</text>
                  <doc_id>231</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>However, when combining them together, the F 1 score deduces slightly, which may result from the weight tuning.</text>
                  <doc_id>232</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Using all feature, the performance comes to 97.70.</text>
                  <doc_id>233</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Overall, our experiment shows that the target side information can improve the source side tokenization under a supervised way, and outperform stateof-the-art systems.</text>
                  <doc_id>234</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.2.2 Best Tokenization = Best Translation?</text>
                  <doc_id>235</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Previous works (Zhang et al., 2008; Chang et al., 2008) have shown that preprocessing the input string for decoder by better segmenters do not always improve the translation quality, we reverify this by testing whether the joint decoder produces good tokenization and good translation at the same time.</text>
                  <doc_id>236</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To answer the question, we used the feature weights optimized by maximizing BLEU for tokenization and used the weights optimized by maximizing F 1 for translation.</text>
                  <doc_id>237</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We test BLEU on MT05 and F 1 score on the test data used in segmentation evaluation experiments.</text>
                  <doc_id>238</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>By tuning weights regarding to BLEU (the configuration for Joint All in table 2), our decoder achieves a BLEU score of 34.88 and an F 1 score of 92.49.</text>
                  <doc_id>239</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Similarly, maximizing F 1 (the configuration for the last line in table 6) leads to a much lower BLEU of 27.43, although the F 1 is up to 97.70.</text>
                  <doc_id>240</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This suggests that better tokenization may not always lead to better translations and vice versa</text>
                  <doc_id>241</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Rule #Rule Method Test Time Morph 46M 21.61 4.12 Separate Refined 55M 21.21 4.63 All 74M Joint 21.93* 5.10</text>
                  <doc_id>242</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>even by the joint decoding.</text>
                  <doc_id>243</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This also indicates the hard of artificially defining the best tokenization for translation.</text>
                  <doc_id>244</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>4.3 Korean-Chinese Translation</title>
            <text>We also test our model on a quite different task: Korean-Chinese. Korean is an agglutinative language, which comes from different language family comparing with Chinese.
We used a newswire corpus containing 256k sentence pairs as training data. The development and test data set contain 1K sentence each with one single reference. We used the target side of training set for language model training. The Korean part of these data were tokenized into morpheme sequence as atomic unit for our experiments.
We compared three methods. First is directly use morpheme sequence (Morph). The second one is refined data (Refined), where we use selective morphological segmentation (Oflazer, 2008) for combining morpheme together on the training data. Since the selective method needs alignment information which is unavailable in the decoding, the test data is still of morpheme sequence. These two methods still used traditional decoding method. The third one extracting rules from combined (All) data of methods 1 and 2, and using joint decoder to exploit the different granularity of rules.
Table 7 shows the result. Since there is no gold standard data for tokenization, we do not use ME and LM tokenization features here. However, our joint method can still significantly (p &lt; 0.05) improve the performance by about +0.3 points. This also reflects the importance of optimizing granularity for morphological complex languages.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We also test our model on a quite different task: Korean-Chinese.</text>
                  <doc_id>245</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Korean is an agglutinative language, which comes from different language family comparing with Chinese.</text>
                  <doc_id>246</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We used a newswire corpus containing 256k sentence pairs as training data.</text>
                  <doc_id>247</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The development and test data set contain 1K sentence each with one single reference.</text>
                  <doc_id>248</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We used the target side of training set for language model training.</text>
                  <doc_id>249</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The Korean part of these data were tokenized into morpheme sequence as atomic unit for our experiments.</text>
                  <doc_id>250</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We compared three methods.</text>
                  <doc_id>251</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>First is directly use morpheme sequence (Morph).</text>
                  <doc_id>252</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The second one is refined data (Refined), where we use selective morphological segmentation (Oflazer, 2008) for combining morpheme together on the training data.</text>
                  <doc_id>253</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Since the selective method needs alignment information which is unavailable in the decoding, the test data is still of morpheme sequence.</text>
                  <doc_id>254</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>These two methods still used traditional decoding method.</text>
                  <doc_id>255</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The third one extracting rules from combined (All) data of methods 1 and 2, and using joint decoder to exploit the different granularity of rules.</text>
                  <doc_id>256</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 7 shows the result.</text>
                  <doc_id>257</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Since there is no gold standard data for tokenization, we do not use ME and LM tokenization features here.</text>
                  <doc_id>258</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, our joint method can still significantly (p &lt; 0.05) improve the performance by about +0.3 points.</text>
                  <doc_id>259</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This also reflects the importance of optimizing granularity for morphological complex languages.</text>
                  <doc_id>260</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Related Work</title>
        <text>Methods have been proposed to optimize tokenization for word alignment. For example, word alignment can be simplified by packing (Ma et al., 2007) several consecutive words together. Word alignment and tokenization can also be optimized by maximizing the likelihood of bilingual corpus (Chung and Gildea, 2009; Xu et al., 2008). In fact, these work are orthogonal to our joint method, since they focus on training step while we are concerned of decoding. We believe we can further the performance by combining these two kinds of work.
Our work also has connections to multilingual tokenization (Snyder and Barzilay, 2008). While they have verified that tokenization can be improved by multilingual learning, our work shows that we can also improve tokenization by collaborating with translation task in a supervised way.
More recently, Liu and Liu (2010) also shows the effect of joint method. They integrate parsing and translation into a single step and improve the performance of translation significantly.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Methods have been proposed to optimize tokenization for word alignment.</text>
              <doc_id>261</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For example, word alignment can be simplified by packing (Ma et al., 2007) several consecutive words together.</text>
              <doc_id>262</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Word alignment and tokenization can also be optimized by maximizing the likelihood of bilingual corpus (Chung and Gildea, 2009; Xu et al., 2008).</text>
              <doc_id>263</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In fact, these work are orthogonal to our joint method, since they focus on training step while we are concerned of decoding.</text>
              <doc_id>264</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We believe we can further the performance by combining these two kinds of work.</text>
              <doc_id>265</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Our work also has connections to multilingual tokenization (Snyder and Barzilay, 2008).</text>
              <doc_id>266</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>While they have verified that tokenization can be improved by multilingual learning, our work shows that we can also improve tokenization by collaborating with translation task in a supervised way.</text>
              <doc_id>267</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>More recently, Liu and Liu (2010) also shows the effect of joint method.</text>
              <doc_id>268</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>They integrate parsing and translation into a single step and improve the performance of translation significantly.</text>
              <doc_id>269</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>6 Conclusion</title>
        <text>We have presented a novel method for joint tokenization and translation which directly combines the tokenization model into the decoding phase. Allowing tokenization and translation to collaborate with each other, tokenization can be optimized for translation, while translation also makes contribution to tokenization performance under a supervised way. We believe that our approach can be applied to other string-based model such as phrase-based model (Koehn et al., 2003), stringto-tree model (Galley et al., 2006) and string-todependency model (Shen et al., 2008).
The authors were supported by SK Telecom C&amp;I Business, and National Natural Science Foundation of China, Contracts 60736014 and 60903138. We thank the anonymous reviewers for their insightful comments. We are also grateful to Wenbin Jiang, Zhiyang Wang and Zongcheng Ji for their helpful feedback.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have presented a novel method for joint tokenization and translation which directly combines the tokenization model into the decoding phase.</text>
              <doc_id>270</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Allowing tokenization and translation to collaborate with each other, tokenization can be optimized for translation, while translation also makes contribution to tokenization performance under a supervised way.</text>
              <doc_id>271</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We believe that our approach can be applied to other string-based model such as phrase-based model (Koehn et al., 2003), stringto-tree model (Galley et al., 2006) and string-todependency model (Shen et al., 2008).</text>
              <doc_id>272</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The authors were supported by SK Telecom C&amp;I Business, and National Natural Science Foundation of China, Contracts 60736014 and 60903138.</text>
              <doc_id>273</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We thank the anonymous reviewers for their insightful comments.</text>
              <doc_id>274</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We are also grateful to Wenbin Jiang, Zhiyang Wang and Zongcheng Ji for their helpful feedback.</text>
              <doc_id>275</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: An SCFG derivation given the tokenization of Figure 2(b).</caption>
        <reference_text>In PAGE 3: ...1 2 3 4 5 6 7 1 2 3 Figure 3: A derivation of the joint model for the tokenization in Figure 2(b) and the translation in Figure 2 by using the rules in  Table1 . trianglesolid means tokenization while squaresolid represents translation....  In PAGE 3: ... As shown in Figure 1(b), the decoder takes an un- tokenized string as input, and then tokenizes the source side string while building the correspond- ing translation of the target side. Since the tradi- tional rules like those in  Table1  natively include tokenization information, we can directly apply them for simultaneous construction of tokeniza- tion and translation by the source side and target side of rules respectively. In Figure 3, our joint model takes the character sequence in Figure 2(a) as input, and synchronously conducts both trans- lation and tokenization using the rules in Table 1....</reference_text>
        <page_num>2</page_num>
        <head>
          <rows>
            <row>
              <cell>clarity.</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>r1</cell>
              <cell>tao-fei-ke ?Taufik</cell>
            </row>
            <row>
              <cell>r2</cell>
              <cell>duo fen ? gain a point</cell>
            </row>
            <row>
              <cell>r3</cell>
              <cell>x1 you-wang x2 ? x1 will have the chance to x2</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Comparison of Separate and Joint methods in terms of BLEU and speed (second per sentence). Columns Train and Test represents the tokenization methods for training and testing respectively. Column TFs stands for whether the 8 tokenization features is used ( &#8730; ) or not (&#215;). ICT, SF and ME are segmenter names for preprocessing. All means combined corpus processed by the three segmenters. Lattice represent the system implemented as Dyer et al., (2008). ** means significantly (Koehn, 2004) better than Lattice (p &lt; 0.01).</caption>
        <reference_text></reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>Method</cell>
              <cell>Train</cell>
              <cell>#Rule</cell>
              <cell>Test</cell>
              <cell>TFs</cell>
              <cell>MT04</cell>
              <cell>MT05</cell>
              <cell>Speed</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell></cell>
              <cell>ICT</cell>
              <cell>151M</cell>
              <cell>ICT</cell>
              <cell>&#215;</cell>
              <cell>34.82</cell>
              <cell>33.06</cell>
              <cell>2.48</cell>
            </row>
            <row>
              <cell></cell>
              <cell>SF</cell>
              <cell>148M</cell>
              <cell>SF</cell>
              <cell>&#215;</cell>
              <cell>35.29</cell>
              <cell>33.22</cell>
              <cell>2.55</cell>
            </row>
            <row>
              <cell>Separate</cell>
              <cell>ME</cell>
              <cell>141M</cell>
              <cell>ME</cell>
              <cell>&#215;</cell>
              <cell>33.71</cell>
              <cell>30.91</cell>
              <cell>2.34</cell>
            </row>
            <row>
              <cell>All</cell>
              <cell>219M</cell>
              <cell>Lattice</cell>
              <cell>&#8730;
&#215; 35.79 33.95 3.83
35.85 33.76 6.79</cell>
            </row>
            <row>
              <cell>ICT 151M
36.92 34.69 17.66
SF 148M &#8730; 37.02 34.56 17.37 Joint
Character
ME 141M 36.78 34.17 17.23</cell>
            </row>
            <row>
              <cell>All</cell>
              <cell>219M</cell>
              <cell>37.25**</cell>
              <cell>34.88**</cell>
              <cell>17.52</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Effect of tokenization features on Chinese-English translation task.  ?  denotes using a tokenization feature while  ?  denotes that it is inactive.</caption>
        <reference_text>In PAGE 6: ... In order to reduce the time for tuning weights and decoding, we extracted rules from the FBIS part of the bilingual corpus, and trained a 4-gram English language model on the English side of FBIS.  Table3  shows the result. Only using the 8 trans- lation features, our system achieves a BLEU score of 24....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>ME</cell>
              <cell>LM</cell>
              <cell>WC</cell>
              <cell>OCC</cell>
              <cell>OD</cell>
              <cell>MT05</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>? ?</cell>
              <cell>?</cell>
              <cell>?</cell>
              <cell>?</cell>
              <cell>?</cell>
              <cell>24.97</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>? ?</cell>
              <cell>?</cell>
              <cell>?</cell>
              <cell>?</cell>
              <cell>25.30</cell>
            </row>
            <row>
              <cell>?</cell>
              <cell>None</cell>
              <cell>? ?</cell>
              <cell>?</cell>
              <cell>?</cell>
              <cell>24.70</cell>
            </row>
            <row>
              <cell>?</cell>
              <cell>?</cell>
              <cell>None</cell>
              <cell>? ?</cell>
              <cell>?</cell>
              <cell>24.84</cell>
            </row>
            <row>
              <cell>?</cell>
              <cell>?</cell>
              <cell>?</cell>
              <cell>None</cell>
              <cell>? ?</cell>
              <cell>25.51</cell>
            </row>
            <row>
              <cell>?</cell>
              <cell>? ?</cell>
              <cell>? ?</cell>
              <cell>?</cell>
              <cell>None</cell>
              <cell>25.34</cell>
            </row>
            <row>
              <cell>? ?</cell>
              <cell>?</cell>
              <cell>?</cell>
              <cell>? ?</cell>
              <cell>? ?</cell>
              <cell>25.74</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>26.37</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: Granularity (Grau, counts of character per word) and counts of OOV words of different methods on MT05. The subscript of joint means the type of rule table.</caption>
        <reference_text>In PAGE 6: ....1.4 Results Analysis To better understand why the joint method can improve the translation quality, this section shows some details of the results on the MT05 data set.  Table4  shows the granularity and OOV word counts of different configurations. The lattice method reduces the OOV words quite a lot which is 23% and 70% comparing with ICT and ME....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Method</cell>
              <cell>BLEU</cell>
              <cell>#Word</cell>
              <cell>Grau</cell>
              <cell>#OOV</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>ICT</cell>
              <cell>33.06</cell>
              <cell>30,602</cell>
              <cell>1.65</cell>
              <cell>644</cell>
            </row>
            <row>
              <cell>SF</cell>
              <cell>33.22</cell>
              <cell>30,119</cell>
              <cell>1.68</cell>
              <cell>882</cell>
            </row>
            <row>
              <cell>ME</cell>
              <cell>30.91</cell>
              <cell>29,717</cell>
              <cell>1.70</cell>
              <cell>1,614</cell>
            </row>
            <row>
              <cell>Lattice</cell>
              <cell>33.95</cell>
              <cell>30,315</cell>
              <cell>1.66</cell>
              <cell>494</cell>
            </row>
            <row>
              <cell>JointICT#@#@Joint ICT</cell>
              <cell>34.69</cell>
              <cell>29,723</cell>
              <cell>1.70</cell>
              <cell>996</cell>
            </row>
            <row>
              <cell>JointSF#@#@Joint SF</cell>
              <cell>34.56</cell>
              <cell>29,839</cell>
              <cell>1.69</cell>
              <cell>972</cell>
            </row>
            <row>
              <cell>JointME#@#@Joint ME</cell>
              <cell>34.17</cell>
              <cell>29,771</cell>
              <cell>1.70</cell>
              <cell>1,062</cell>
            </row>
            <row>
              <cell>JointAll#@#@Joint All</cell>
              <cell>34.88</cell>
              <cell>29,644</cell>
              <cell>1.70</cell>
              <cell>883</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TableSeer</source>
        <caption>Table 5: Comparison of segmentation performance in terms of F 1 score and speed (second per sentence). Type column means the segmenter for monolingual method, while represents the rule tables used by joint method.</caption>
        <reference_text>In PAGE 7: ... We use MERT (Och, 2003) to tune the weights by minimizing the error measured by F1 score. As shown in  Table5 , with all features activated, our joint decoder achieves an F1 score of 97.70 which reduces the tokenization error comparing with the best single segmenter ICT by 8....</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>Method</cell>
              <cell>Type</cell>
              <cell>F1</cell>
              <cell>Time</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>ICT</cell>
              <cell>97.47</cell>
              <cell>0.010</cell>
            </row>
            <row>
              <cell>Monolingual</cell>
              <cell>SF</cell>
              <cell>97.48</cell>
              <cell>0.007</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>ME</cell>
              <cell>95.53</cell>
              <cell>0.008</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>ICT</cell>
              <cell>97.68</cell>
              <cell>9.382</cell>
            </row>
            <row>
              <cell>Joint</cell>
              <cell>SF</cell>
              <cell>97.68</cell>
              <cell>10.454</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>ME</cell>
              <cell>97.60</cell>
              <cell>10.451</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>All</cell>
              <cell>97.70</cell>
              <cell>9.248</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>6</id>
        <source>TET</source>
        <caption>Table 6: Effect of the target side information on Chinese word segmentation. TFs stands for the 8 tokenization features. All represents all the 16 features.</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Feature</cell>
              <cell>F 1</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>TFs</cell>
              <cell>97.37</cell>
            </row>
            <row>
              <cell>TFs + RS</cell>
              <cell>97.65</cell>
            </row>
            <row>
              <cell>TFs + LM</cell>
              <cell>97.67</cell>
            </row>
            <row>
              <cell>TFs + RS + LM</cell>
              <cell>97.62</cell>
            </row>
            <row>
              <cell>All</cell>
              <cell>97.70</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Pi-Chuan Chang</author>
          <author>Michel Galley</author>
          <author>Christopher D Manning</author>
        </authors>
        <title>Optimizing Chinese word segmentation for machine translation performance.</title>
        <publication>In the Third Workshop on SMT.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>Hierarchical phrase-based translation.</title>
        <publication>None</publication>
        <pages>228</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Tagyoung Chung</author>
          <author>Daniel Gildea</author>
        </authors>
        <title>Unsupervised tokenization for machine translation.</title>
        <publication>In Proc.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>EMNLP</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Christopher Dyer</author>
          <author>Smaranda Muresan</author>
          <author>Philip Resnik</author>
        </authors>
        <title>Generalizing word lattice translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>5</id>
        <authors/>
        <title>None</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Chris Dyer</author>
        </authors>
        <title>Using a maximum entropy model to build segmentation lattices for mt.</title>
        <publication>In Proc.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>NAACL</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Michel Galley</author>
          <author>Jonathan Graehl</author>
          <author>Kevin Knight</author>
          <author>Daniel Marcu</author>
          <author>Steve DeNeefe</author>
          <author>Wei Wang</author>
          <author>Ignacio Thayer</author>
        </authors>
        <title>Scalable inference and training of context-rich syntactic translation models.</title>
        <publication>In Proc.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>ACL</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Tseng Huihsin</author>
          <author>Pichuan Chang</author>
          <author>Galen Andrew</author>
          <author>Daniel Jurafsky</author>
          <author>Christopher Manning</author>
        </authors>
        <title>A conditional random field word segmenter.</title>
        <publication>In Fourth SIGHAN Workshop.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Wenbin Jiang</author>
          <author>Liang Huang</author>
          <author>Qun Liu</author>
          <author>Yajuan L&#252;</author>
        </authors>
        <title>A cascaded linear model for joint chinese word segmentation and part-of-speech tagging.</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz Josef Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical phrase-based translation.</title>
        <publication>In Proc.</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>HLT-NAACL</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Philipp Koehn</author>
        </authors>
        <title>Statistical significance tests for machine translation evaluation.</title>
        <publication>In Proc. EMNLP</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Yang Liu</author>
          <author>Qun Liu</author>
        </authors>
        <title>Joint parsing and translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>16</id>
        <authors/>
        <title>None</title>
        <publication>In Proc. Coling</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Yang Liu</author>
          <author>Qun Liu</author>
          <author>Shouxun Lin</author>
        </authors>
        <title>Treeto-string alignment template for statistical machine translation.</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Yanjun Ma</author>
          <author>Nicolas Stroppa</author>
          <author>Andy Way</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>19</id>
        <authors/>
        <title>Bootstrapping word alignment via word packing.</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Haitao Mi</author>
          <author>Liang Huang</author>
          <author>Qun Liu</author>
        </authors>
        <title>Forestbased translation.</title>
        <publication>In Proc. of ACL</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Hwee Tou Ng</author>
          <author>Jin Kiat Low</author>
        </authors>
        <title>Chinese partof-speech tagging: One-at-a-time or all-at-once? word-based or character-based?</title>
        <publication>In Proc. EMNLP</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Franz J Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Discriminative training and maximum entropy models for statistical machine translation.</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A systematic comparison of various statistical alignment models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Kemal Oflazer</author>
        </authors>
        <title>Statistical machine translation into a morphologically complex language.</title>
        <publication>In Proc.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>CICL</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>27</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>Wei-Jing Zhu</author>
        </authors>
        <title>Bleu: a method for automatic evaluation of machine translation.</title>
        <publication>In Proc.</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>28</id>
        <authors>
          <author>ACL</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>29</id>
        <authors>
          <author>Libin Shen</author>
          <author>Xu Jinxi</author>
          <author>Weischedel Ralph</author>
        </authors>
        <title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>30</id>
        <authors>
          <author>Benjamin Snyder</author>
          <author>Regina Barzilay</author>
        </authors>
        <title>Unsupervised multilingual learning for morphological segmentation.</title>
        <publication>In Proc. ACL</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>31</id>
        <authors>
          <author>Andreas Stolcke</author>
        </authors>
        <title>Srilm &#8211; an extensible language modeling toolkit.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>32</id>
        <authors>
          <author>Jia Xu</author>
          <author>Evgeny Matusov</author>
          <author>Richard Zens</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Integrated chinese word segmentation in statistical machine translation.</title>
        <publication>In Proc. IWSLT2005.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>33</id>
        <authors>
          <author>Jia Xu</author>
          <author>Jianfeng Gao</author>
          <author>Kristina Toutanova</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Bayesian semi-supervised chinese word segmentation for statistical machine translation.</title>
        <publication>In Proc. Coling</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>34</id>
        <authors>
          <author>Nianwen Xue</author>
          <author>Libin Shen</author>
        </authors>
        <title>Chinese word segmentation as LMR tagging.</title>
        <publication>In SIGHAN Workshop.</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>35</id>
        <authors>
          <author>Hua-Ping Zhang</author>
          <author>Hong-Kui Yu</author>
          <author>De-Yi Xiong</author>
          <author>Qun Liu</author>
        </authors>
        <title>Hhmm-based chinese lexical analyzer ictclas.</title>
        <publication>In the Second SIGHAN Workshop.</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>36</id>
        <authors>
          <author>Ruiqiang Zhang</author>
          <author>Keiji Yasuda</author>
          <author>Eiichiro Sumita</author>
        </authors>
        <title>Improved statistical machine translation by multiple Chinese word segmentation.</title>
        <publication>In the Third Workshop on SMT.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Chang et al., 2008</string>
        <sentence_id>1156</sentence_id>
        <char_offset>188</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>0</reference_id>
        <string>Chang et al., 2008</string>
        <sentence_id>1160</sentence_id>
        <char_offset>116</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>0</reference_id>
        <string>Chang et al., 2008</string>
        <sentence_id>1362</sentence_id>
        <char_offset>36</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>1</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>1139</sentence_id>
        <char_offset>142</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>1</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>1172</sentence_id>
        <char_offset>27</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>1</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>1204</sentence_id>
        <char_offset>38</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>1</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>1272</sentence_id>
        <char_offset>23</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>2</reference_id>
        <string>Chung and Gildea, 2009</string>
        <sentence_id>1396</sentence_id>
        <char_offset>104</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>4</reference_id>
        <string>Dyer et al., (2008)</string>
        <sentence_id>1294</sentence_id>
        <char_offset>8</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>4</reference_id>
        <string>Dyer et al., 2008</string>
        <sentence_id>1162</sentence_id>
        <char_offset>153</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>5</reference_id>
        <string>(2008)</string>
        <sentence_id>1139</sentence_id>
        <char_offset>41</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>5</reference_id>
        <string>(2008)</string>
        <sentence_id>1227</sentence_id>
        <char_offset>55</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>5</reference_id>
        <string>(2008)</string>
        <sentence_id>1294</sentence_id>
        <char_offset>21</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>6</reference_id>
        <string>Dyer, 2009</string>
        <sentence_id>1162</sentence_id>
        <char_offset>172</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>10</reference_id>
        <string>Huihsin et al., 2005</string>
        <sentence_id>1288</sentence_id>
        <char_offset>203</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>11</reference_id>
        <string>Jiang et al., (2008)</string>
        <sentence_id>1227</sentence_id>
        <char_offset>41</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>12</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>1139</sentence_id>
        <char_offset>122</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>12</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>1281</sentence_id>
        <char_offset>106</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>12</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>1405</sentence_id>
        <char_offset>100</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>15</reference_id>
        <string>Liu and Liu (2010)</string>
        <sentence_id>1401</sentence_id>
        <char_offset>15</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>16</reference_id>
        <string>(2010)</string>
        <sentence_id>1401</sentence_id>
        <char_offset>27</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>17</reference_id>
        <string>Liu et al., 2006</string>
        <sentence_id>1152</sentence_id>
        <char_offset>77</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>18</reference_id>
        <string>Ma et al., 2007</string>
        <sentence_id>1395</sentence_id>
        <char_offset>58</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>19</reference_id>
        <string>(2007)</string>
        <sentence_id>1282</sentence_id>
        <char_offset>52</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>20</reference_id>
        <string>Mi et al., 2008</string>
        <sentence_id>1152</sentence_id>
        <char_offset>95</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>21</reference_id>
        <string>Ng and Low, 2004</string>
        <sentence_id>1209</sentence_id>
        <char_offset>110</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>22</reference_id>
        <string>Och and Ney, 2002</string>
        <sentence_id>1165</sentence_id>
        <char_offset>82</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>23</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>1281</sentence_id>
        <char_offset>16</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>24</reference_id>
        <string>Och, 2003</string>
        <sentence_id>1350</sentence_id>
        <char_offset>13</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>25</reference_id>
        <string>Oflazer, 2008</string>
        <sentence_id>1159</sentence_id>
        <char_offset>119</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>25</reference_id>
        <string>Oflazer, 2008</string>
        <sentence_id>1379</sentence_id>
        <char_offset>93</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>27</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>1285</sentence_id>
        <char_offset>30</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>29</reference_id>
        <string>Shen et al., 2008</string>
        <sentence_id>1152</sentence_id>
        <char_offset>6</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>29</reference_id>
        <string>Shen et al., 2008</string>
        <sentence_id>1405</sentence_id>
        <char_offset>194</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>30</reference_id>
        <string>Snyder and Barzilay, 2008</string>
        <sentence_id>1399</sentence_id>
        <char_offset>60</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>31</reference_id>
        <string>Stolcke, 2002</string>
        <sentence_id>1284</sentence_id>
        <char_offset>15</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>32</reference_id>
        <string>Xu et al., 2005</string>
        <sentence_id>1156</sentence_id>
        <char_offset>171</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>32</reference_id>
        <string>Xu et al., 2005</string>
        <sentence_id>1162</sentence_id>
        <char_offset>136</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>33</reference_id>
        <string>Xu et al., 2008</string>
        <sentence_id>1396</sentence_id>
        <char_offset>128</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>34</reference_id>
        <string>Xue and Shen, 2003</string>
        <sentence_id>1209</sentence_id>
        <char_offset>90</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>35</reference_id>
        <string>Zhang et al., 2003</string>
        <sentence_id>1288</sentence_id>
        <char_offset>145</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>36</reference_id>
        <string>Zhang et al., 2008</string>
        <sentence_id>1156</sentence_id>
        <char_offset>208</char_offset>
      </citation>
      <citation>
        <id>42</id>
        <reference_id>36</reference_id>
        <string>Zhang et al., 2008</string>
        <sentence_id>1160</sentence_id>
        <char_offset>136</char_offset>
      </citation>
      <citation>
        <id>43</id>
        <reference_id>36</reference_id>
        <string>Zhang et al., 2008</string>
        <sentence_id>1362</sentence_id>
        <char_offset>16</char_offset>
      </citation>
    </citations>
  </content>
</document>
