<document>
  <filename>P14-1072</filename>
  <authors>
    <author>Hui Zhang</author>
  </authors>
  <title>Kneser-Ney Smoothing on Expected Counts</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>Widely used in speech and language processing, Kneser-Ney (KN) smoothing has consistently been shown to be one of the best-performing smoothing methods. However, KN smoothing assumes integer counts, limiting its potential uses&#8212;for example, inside Expectation-Maximization. In this paper, we propose a generalization of KN smoothing that operates on fractional counts, or, more precisely, on distributions over counts. We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts, and apply it to two tasks where KN smoothing was not applicable before: one in language model adaptation, and the other in word alignment. In both cases, our method improves performance significantly.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Widely used in speech and language processing, Kneser-Ney (KN) smoothing has consistently been shown to be one of the best-performing smoothing methods.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, KN smoothing assumes integer counts, limiting its potential uses&#8212;for example, inside Expectation-Maximization.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, we propose a generalization of KN smoothing that operates on fractional counts, or, more precisely, on distributions over counts.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts, and apply it to two tasks where KN smoothing was not applicable before: one in language model adaptation, and the other in word alignment.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In both cases, our method improves performance significantly.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>In speech and language processing, smoothing is essential to reduce overfitting, and Kneser-Ney (KN) smoothing (Kneser and Ney, 1995; Chen and Goodman, 1999) has consistently proven to be among the best-performing and most widely used methods. However, KN smoothing assumes integer counts, whereas in many NLP tasks, training instances appear with possibly fractional weights. Such cases have been noted for language modeling (Goodman, 2001; Goodman, 2004), domain adaptation (Tam and Schultz, 2008), grapheme-tophoneme conversion (Bisani and Ney, 2008), and phrase-based translation (Andr&#233;s-Ferrer, 2010; Wuebker et al., 2012).
For example, in Expectation-Maximization (Dempster et al., 1977), the Expectation (E) step computes the posterior distribution over possible completions of the data, and the Maximization (M) step reestimates the model parameters as if that distribution had actually been observed. In most cases, the M step is identical to estimating the model from complete data, except that counts of observations from the E step are fractional. It is common to apply add-one smoothing to the M step, but we cannot apply KN smoothing. Another example is instance weighting. If we assign a weight to each training instance to indicate how important it is (say, its relevance to a particular domain), and the counts are not integral, then we again cannot train the model using KN smoothing.
In this paper, we propose a generalization of KN smoothing (called expected KN smoothing) that operates on fractional counts, or, more precisely, on distributions over counts. We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts. We demonstrate how to apply expected KN to two tasks where KN smoothing was not applicable before. One is language model domain adaptation, and the other is word alignment using the IBM models (Brown et al., 1993). In both tasks, expected KN smoothing improves performance significantly.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In speech and language processing, smoothing is essential to reduce overfitting, and Kneser-Ney (KN) smoothing (Kneser and Ney, 1995; Chen and Goodman, 1999) has consistently proven to be among the best-performing and most widely used methods.</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, KN smoothing assumes integer counts, whereas in many NLP tasks, training instances appear with possibly fractional weights.</text>
              <doc_id>6</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Such cases have been noted for language modeling (Goodman, 2001; Goodman, 2004), domain adaptation (Tam and Schultz, 2008), grapheme-tophoneme conversion (Bisani and Ney, 2008), and phrase-based translation (Andr&#233;s-Ferrer, 2010; Wuebker et al., 2012).</text>
              <doc_id>7</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For example, in Expectation-Maximization (Dempster et al., 1977), the Expectation (E) step computes the posterior distribution over possible completions of the data, and the Maximization (M) step reestimates the model parameters as if that distribution had actually been observed.</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In most cases, the M step is identical to estimating the model from complete data, except that counts of observations from the E step are fractional.</text>
              <doc_id>9</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>It is common to apply add-one smoothing to the M step, but we cannot apply KN smoothing.</text>
              <doc_id>10</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Another example is instance weighting.</text>
              <doc_id>11</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>If we assign a weight to each training instance to indicate how important it is (say, its relevance to a particular domain), and the counts are not integral, then we again cannot train the model using KN smoothing.</text>
              <doc_id>12</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this paper, we propose a generalization of KN smoothing (called expected KN smoothing) that operates on fractional counts, or, more precisely, on distributions over counts.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts.</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We demonstrate how to apply expected KN to two tasks where KN smoothing was not applicable before.</text>
              <doc_id>15</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>One is language model domain adaptation, and the other is word alignment using the IBM models (Brown et al., 1993).</text>
              <doc_id>16</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In both tasks, expected KN smoothing improves performance significantly.</text>
              <doc_id>17</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Smoothing on integral counts</title>
        <text>Before presenting our method, we review KN smoothing on integer counts as applied to language models, although, as we will demonstrate in Section 7, KN smoothing is applicable to other tasks as well.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Before presenting our method, we review KN smoothing on integer counts as applied to language models, although, as we will demonstrate in Section 7, KN smoothing is applicable to other tasks as well.</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Maximum likelihood estimation</title>
            <text>Let uw stand for an n-gram, where u stands for the (n &#8722; 1) context words and w, the predicted word. Let c(uw) be the number of occurrences of uw. We use a bullet (&#8226;) to indicate summation over words, that is, c(u&#8226;) = &#8721; w c(uw). Under maximum-likelihood estimation (MLE), we max-
imize
&#8721; L = c(uw) log p(w | u),
uw
obtaining the solution</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Let uw stand for an n-gram, where u stands for the (n &#8722; 1) context words and w, the predicted word.</text>
                  <doc_id>19</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Let c(uw) be the number of occurrences of uw.</text>
                  <doc_id>20</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We use a bullet (&#8226;) to indicate summation over words, that is, c(u&#8226;) = &#8721; w c(uw).</text>
                  <doc_id>21</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Under maximum-likelihood estimation (MLE), we max-</text>
                  <doc_id>22</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>imize</text>
                  <doc_id>23</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; L = c(uw) log p(w | u),</text>
                  <doc_id>24</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>uw</text>
                  <doc_id>25</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>obtaining the solution</text>
                  <doc_id>26</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Absolute discounting</title>
            <text>p mle (w | u) = c(uw) c(u&#8226;) . (1)
Absolute discounting (Ney et al., 1994) &#8211; on which KN smoothing is based &#8211; tries to generalize better to unseen data by subtracting a discount from each seen n-gram&#8217;s count and distributing the subtracted discounts to unseen n-grams. For now, we assume that the discount is a constant D, so that the smoothed counts are &#9127; &#9130;&#9128; c(uw) &#8722; D if c(uw) &gt; 0
&#732;c(uw) = &#9130;&#9129; n 1+ (u&#8226;)Dq u (w) otherwise
where n 1+ (u&#8226;) = |{w | c(uw) &gt; 0}| is the number of word types observed after context u, and q u (w) specifies how to distribute the subtracted discounts among unseen n-gram types. Maximizing the likelihood of the smoothed counts &#732;c, we get
&#9127;
&#9130;&#9128; p(w | u) =
&#9130;&#9129;
c(uw) &#8722; D c(u&#8226;)
n 1+ (u&#8226;)Dq u (w) c(u&#8226;)
if c(uw) &gt; 0
otherwise.
(2)
How to choose D and q u (w) are described in the next two sections.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>p mle (w | u) = c(uw) c(u&#8226;) .</text>
                  <doc_id>27</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(1)</text>
                  <doc_id>28</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Absolute discounting (Ney et al., 1994) &#8211; on which KN smoothing is based &#8211; tries to generalize better to unseen data by subtracting a discount from each seen n-gram&#8217;s count and distributing the subtracted discounts to unseen n-grams.</text>
                  <doc_id>29</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For now, we assume that the discount is a constant D, so that the smoothed counts are &#9127; &#9130;&#9128; c(uw) &#8722; D if c(uw) &gt; 0</text>
                  <doc_id>30</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#732;c(uw) = &#9130;&#9129; n 1+ (u&#8226;)Dq u (w) otherwise</text>
                  <doc_id>31</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where n 1+ (u&#8226;) = |{w | c(uw) &gt; 0}| is the number of word types observed after context u, and q u (w) specifies how to distribute the subtracted discounts among unseen n-gram types.</text>
                  <doc_id>32</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Maximizing the likelihood of the smoothed counts &#732;c, we get</text>
                  <doc_id>33</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9127;</text>
                  <doc_id>34</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9130;&#9128; p(w | u) =</text>
                  <doc_id>35</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9130;&#9129;</text>
                  <doc_id>36</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c(uw) &#8722; D c(u&#8226;)</text>
                  <doc_id>37</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n 1+ (u&#8226;)Dq u (w) c(u&#8226;)</text>
                  <doc_id>38</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>if c(uw) &gt; 0</text>
                  <doc_id>39</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>otherwise.</text>
                  <doc_id>40</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(2)</text>
                  <doc_id>41</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>How to choose D and q u (w) are described in the next two sections.</text>
                  <doc_id>42</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>2.3 Estimating D by leaving-one-out</title>
            <text>The discount D can be chosen by various means; in absolute discounting, it is chosen by the method of leaving one out. Given N training instances, we form the probability of each instance under the MLE using the other (N &#8722; 1) instances as training data; then we maximize the log-likelihood of all those instances. The probability of an n-gram token uw using the other tokens as training data is
&#9127;
&#9130;&#9128; p loo (w | u) =
&#9130;&#9129;
c(uw) &#8722; 1 &#8722; D c(u&#8226;) &#8722; 1
(n 1+ (u&#8226;) &#8722; 1)Dq u (w) c(u&#8226;) &#8722; 1
c(uw) &gt; 1
c(uw) = 1.
We want to find the D that maximizes the
leaving-one-out log-likelihood &#8721;
L loo = c(uw) log p loo (w | u)
uw
&#8721; = c(uw) log c(uw) &#8722; 1 &#8722; D c(u&#8226;) &#8722; 1
uw|c(uw)&gt;1
&#8721; + log (n 1+(u&#8226;) &#8722; 1)Dq u (w) c(u&#8226;) &#8722; 1
uw|c(uw)=1
&#8721; = rn r log(r &#8722; 1 &#8722; D) + n 1 log D + C, (3)
r&gt;1
where n r = |{uw | c(uw) = r}| is the number of n- gram types appearing r times, and C is a constant not depending on D. Setting the partial derivative with respect to D to zero, we have &#8721; = &#8722; &#8706;L loo &#8706;D
r&gt;1
rn r r &#8722; 1 &#8722; D + n 1 D
n 1
&#8721;
D = rn r r &#8722; 1 &#8722; D &#8805; 2n 2 1 &#8722; D .
r&gt;1
Solving for D, we have
D &#8804;
n 1 n 1 + 2n 2 . (4)
Theoretically, we can use iterative methods to optimize D. But in practice, setting D to this upper bound is effective and simple (Ney et al., 1994; Chen and Goodman, 1999).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The discount D can be chosen by various means; in absolute discounting, it is chosen by the method of leaving one out.</text>
                  <doc_id>43</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Given N training instances, we form the probability of each instance under the MLE using the other (N &#8722; 1) instances as training data; then we maximize the log-likelihood of all those instances.</text>
                  <doc_id>44</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The probability of an n-gram token uw using the other tokens as training data is</text>
                  <doc_id>45</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9127;</text>
                  <doc_id>46</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9130;&#9128; p loo (w | u) =</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9130;&#9129;</text>
                  <doc_id>48</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c(uw) &#8722; 1 &#8722; D c(u&#8226;) &#8722; 1</text>
                  <doc_id>49</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(n 1+ (u&#8226;) &#8722; 1)Dq u (w) c(u&#8226;) &#8722; 1</text>
                  <doc_id>50</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c(uw) &gt; 1</text>
                  <doc_id>51</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c(uw) = 1.</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We want to find the D that maximizes the</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>leaving-one-out log-likelihood &#8721;</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L loo = c(uw) log p loo (w | u)</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>uw</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; = c(uw) log c(uw) &#8722; 1 &#8722; D c(u&#8226;) &#8722; 1</text>
                  <doc_id>57</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>uw|c(uw)&gt;1</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; + log (n 1+(u&#8226;) &#8722; 1)Dq u (w) c(u&#8226;) &#8722; 1</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>uw|c(uw)=1</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; = rn r log(r &#8722; 1 &#8722; D) + n 1 log D + C, (3)</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r&gt;1</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where n r = |{uw | c(uw) = r}| is the number of n- gram types appearing r times, and C is a constant not depending on D.</text>
                  <doc_id>63</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Setting the partial derivative with respect to D to zero, we have &#8721; = &#8722; &#8706;L loo &#8706;D</text>
                  <doc_id>64</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r&gt;1</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>rn r r &#8722; 1 &#8722; D + n 1 D</text>
                  <doc_id>66</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n 1</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>D = rn r r &#8722; 1 &#8722; D &#8805; 2n 2 1 &#8722; D .</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r&gt;1</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Solving for D, we have</text>
                  <doc_id>71</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>D &#8804;</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n 1 n 1 + 2n 2 .</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(4)</text>
                  <doc_id>74</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Theoretically, we can use iterative methods to optimize D.</text>
                  <doc_id>75</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>But in practice, setting D to this upper bound is effective and simple (Ney et al., 1994; Chen and Goodman, 1999).</text>
                  <doc_id>76</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>2.4 Estimating the lower-order distribution</title>
            <text>Finally, q u (w) is defined to be proportional to an (n &#8722; 1)-gram model p &#8242; (w | u &#8242; ), where u &#8242; is the (n &#8722; 2)-gram suffix of u. That is,
q u (w) = &#947;(u)p &#8242; (w | u &#8242; ),
where &#947;(u) is an auxiliary function chosen to make the distribution p(w | u) in (2) sum to one.
Absolute discounting chooses p &#8242; (w | u &#8242; ) to be the maximum-likelihood unigram distribution; under KN smoothing (Kneser and Ney, 1995), it is chosen to make p in (2) satisfy the following constraint for all (n &#8722; 1)-grams u &#8242; w: &#8721; p mle (u &#8242; w) = p(w | vu &#8242; )p mle (vu &#8242; ). (5)
v
Substituting in the definition of p mle from (1) and p from (2) and canceling terms, we get &#8721; c(u &#8242; w) = (c(vu &#8242; w) &#8722; D)
v|c(vu &#8242; w)&gt;0
+
&#8721;
v|c(vu &#8242; w)=0
n 1+ (vu &#8242; &#8226;)D&#947;(vu &#8242; )p &#8242; (w | u &#8242; ).
Solving for p &#8242; (w | u &#8242; ), we have &#8721;
p &#8242; (w | u &#8242; v|c(vu ) = &#8242; w)&gt;0 1
&#8721;v|c(vu &#8242; w)=0 n 1+ (vu &#8242; &#8226;)&#947;(vu &#8242; ) .
Kneser and Ney assume the denominator is constant in w and renormalize to get an approximation
where
p &#8242; (w | u &#8242; ) &#8776; n 1+(&#8226;u &#8242; w) n 1+ (&#8226;u &#8242; &#8226;) , (6)
n 1+ (&#8226;u &#8242; w) = |{v | c(vu &#8242; w) &gt; 0}| &#8721; n 1+ (&#8226;u &#8242; &#8226;) = n 1+ (&#8226;u &#8242; w).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Finally, q u (w) is defined to be proportional to an (n &#8722; 1)-gram model p &#8242; (w | u &#8242; ), where u &#8242; is the (n &#8722; 2)-gram suffix of u.</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>That is,</text>
                  <doc_id>78</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>q u (w) = &#947;(u)p &#8242; (w | u &#8242; ),</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where &#947;(u) is an auxiliary function chosen to make the distribution p(w | u) in (2) sum to one.</text>
                  <doc_id>80</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Absolute discounting chooses p &#8242; (w | u &#8242; ) to be the maximum-likelihood unigram distribution; under KN smoothing (Kneser and Ney, 1995), it is chosen to make p in (2) satisfy the following constraint for all (n &#8722; 1)-grams u &#8242; w: &#8721; p mle (u &#8242; w) = p(w | vu &#8242; )p mle (vu &#8242; ).</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(5)</text>
                  <doc_id>82</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>v</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Substituting in the definition of p mle from (1) and p from (2) and canceling terms, we get &#8721; c(u &#8242; w) = (c(vu &#8242; w) &#8722; D)</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>v|c(vu &#8242; w)&gt;0</text>
                  <doc_id>85</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>+</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>v|c(vu &#8242; w)=0</text>
                  <doc_id>88</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n 1+ (vu &#8242; &#8226;)D&#947;(vu &#8242; )p &#8242; (w | u &#8242; ).</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Solving for p &#8242; (w | u &#8242; ), we have &#8721;</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p &#8242; (w | u &#8242; v|c(vu ) = &#8242; w)&gt;0 1</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;v|c(vu &#8242; w)=0 n 1+ (vu &#8242; &#8226;)&#947;(vu &#8242; ) .</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Kneser and Ney assume the denominator is constant in w and renormalize to get an approximation</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p &#8242; (w | u &#8242; ) &#8776; n 1+(&#8226;u &#8242; w) n 1+ (&#8226;u &#8242; &#8226;) , (6)</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n 1+ (&#8226;u &#8242; w) = |{v | c(vu &#8242; w) &gt; 0}| &#8721; n 1+ (&#8226;u &#8242; &#8226;) = n 1+ (&#8226;u &#8242; w).</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Count distributions</title>
        <text>w
The computation of D and p &#8242; above made use of n r and n r+ , which presupposes integer counts. But in many applications, the counts are not integral, but fractional. How do we apply KN smoothing in such cases? In this section, we introduce count distributions as a way of circumventing this problem.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>w</text>
              <doc_id>97</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The computation of D and p &#8242; above made use of n r and n r+ , which presupposes integer counts.</text>
              <doc_id>98</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>But in many applications, the counts are not integral, but fractional.</text>
              <doc_id>99</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>How do we apply KN smoothing in such cases?</text>
              <doc_id>100</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In this section, we introduce count distributions as a way of circumventing this problem.</text>
              <doc_id>101</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Definition</title>
            <text>In the E step of EM, we compute a probability distribution (according to the current model) over all possible completions of the observed data, and the expected counts of all types, which may be fractional. However, note that in each completion of the data, the counts are integral. Although it does not make sense to compute n r or n r+ on fractional counts, it does make sense to compute them on possible completions.
In other situations where fractional counts arise, we can still think of the counts as expectations under some distribution over possible &#8220;realizations&#8221; of the data. For example, if we assign a weight between zero and one to every instance in a corpus, we can interpret each instance&#8217;s weight as the probability of that instance occurring or not, yielding a distribution over possible subsets of the data.
Let X be a random variable ranging over possible realizations of the data, and let c X (uw) be the count of uw in realization X. The expectation E[c X (uw)] is the familiar fractional expected count of uw, but we can also compute the probabilities p(c X (uw) = r) for any r. From now on, for brevity, we drop the subscript X and understand c(uw) to be a random variable depending on X. The n r (u&#8226;) and n r+ (u&#8226;) and related quantities also become random variables depending on X.
For example, suppose that our data consists of the following bigrams, with their weights:</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In the E step of EM, we compute a probability distribution (according to the current model) over all possible completions of the observed data, and the expected counts of all types, which may be fractional.</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, note that in each completion of the data, the counts are integral.</text>
                  <doc_id>103</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Although it does not make sense to compute n r or n r+ on fractional counts, it does make sense to compute them on possible completions.</text>
                  <doc_id>104</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In other situations where fractional counts arise, we can still think of the counts as expectations under some distribution over possible &#8220;realizations&#8221; of the data.</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For example, if we assign a weight between zero and one to every instance in a corpus, we can interpret each instance&#8217;s weight as the probability of that instance occurring or not, yielding a distribution over possible subsets of the data.</text>
                  <doc_id>106</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Let X be a random variable ranging over possible realizations of the data, and let c X (uw) be the count of uw in realization X.</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The expectation E[c X (uw)] is the familiar fractional expected count of uw, but we can also compute the probabilities p(c X (uw) = r) for any r.</text>
                  <doc_id>108</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>From now on, for brevity, we drop the subscript X and understand c(uw) to be a random variable depending on X.</text>
                  <doc_id>109</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The n r (u&#8226;) and n r+ (u&#8226;) and related quantities also become random variables depending on X.</text>
                  <doc_id>110</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For example, suppose that our data consists of the following bigrams, with their weights:</text>
                  <doc_id>111</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Efficient computation</title>
            <text>How to compute these probabilities and expectations depends in general on the structure of the model. If we assume that all occurrences of uw are independent (although in fact they are not always), the computation is very easy. If there are k occurrences of uw, each occurring with probability p i , the count c(uw) is distributed according to the Poisson-binomial distribution (Hong, 2013). The expected count E[c(uw)] is just &#8721; i p i , and the distribution of c(uw) can be computed as follows:
p(c(uw) = r) = s(k, r)
where s(k, r) is defined by the recurrence &#9127; s(k &#8722; 1, r)(1 &#8722; p k ) &#9130;&#9128; + s(k &#8722; 1, r &#8722; 1)p k if 0 &#8804; r &#8804; k
s(k, r) = 1 if k = r = 0 &#9130;&#9129; 0 otherwise.
We can also compute { &#8721;
p(c(uw) &#8805; r) = max s(m, r), 1 &#8722;
r &#8242; &lt;r
} s(m, r &#8242; ) ,
the floor operation being needed to protect against rounding errors, and we can compute &#8721; E[n r (u&#8226;)] = p(c(uw) = r)
w
&#8721; E[n r+ (u&#8226;)] = p(c(uw) &#8805; r).
w
Since, as we shall see, we only need to compute these quantities up to a small value of r (2 or 4), this takes time linear in k.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>How to compute these probabilities and expectations depends in general on the structure of the model.</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If we assume that all occurrences of uw are independent (although in fact they are not always), the computation is very easy.</text>
                  <doc_id>113</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>If there are k occurrences of uw, each occurring with probability p i , the count c(uw) is distributed according to the Poisson-binomial distribution (Hong, 2013).</text>
                  <doc_id>114</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The expected count E[c(uw)] is just &#8721; i p i , and the distribution of c(uw) can be computed as follows:</text>
                  <doc_id>115</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(c(uw) = r) = s(k, r)</text>
                  <doc_id>116</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where s(k, r) is defined by the recurrence &#9127; s(k &#8722; 1, r)(1 &#8722; p k ) &#9130;&#9128; + s(k &#8722; 1, r &#8722; 1)p k if 0 &#8804; r &#8804; k</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>s(k, r) = 1 if k = r = 0 &#9130;&#9129; 0 otherwise.</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We can also compute { &#8721;</text>
                  <doc_id>119</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(c(uw) &#8805; r) = max s(m, r), 1 &#8722;</text>
                  <doc_id>120</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r &#8242; &lt;r</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>} s(m, r &#8242; ) ,</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the floor operation being needed to protect against rounding errors, and we can compute &#8721; E[n r (u&#8226;)] = p(c(uw) = r)</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w</text>
                  <doc_id>124</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; E[n r+ (u&#8226;)] = p(c(uw) &#8805; r).</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Since, as we shall see, we only need to compute these quantities up to a small value of r (2 or 4), this takes time linear in k.</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Smoothing on count distributions</title>
        <text>We are now ready to describe how to apply KN smoothing to count distributions. Below, we recapitulate the derivation of KN smoothing presented in Section 2, using the expected log-likelihood in place of the log-likelihood and applying KN smoothing to each possible realization of the data.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We are now ready to describe how to apply KN smoothing to count distributions.</text>
              <doc_id>128</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Below, we recapitulate the derivation of KN smoothing presented in Section 2, using the expected log-likelihood in place of the log-likelihood and applying KN smoothing to each possible realization of the data.</text>
              <doc_id>129</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Maximum likelihood estimation</title>
            <text>The MLE objective function is the expected loglikelihood, &#9121; &#9124; &#8721; E[L] = E &#9122;&#9123; c(uw) log p(w | u) &#9125;&#9126;
uw
&#8721; = E[c(uw)] log p(w | u)
uw
whose maximum is
p mle (w | u) = E[c(uw)] E[c(u&#8226;)] . (7)</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The MLE objective function is the expected loglikelihood, &#9121; &#9124; &#8721; E[L] = E &#9122;&#9123; c(uw) log p(w | u) &#9125;&#9126;</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>uw</text>
                  <doc_id>131</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; = E[c(uw)] log p(w | u)</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>uw</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>whose maximum is</text>
                  <doc_id>134</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p mle (w | u) = E[c(uw)] E[c(u&#8226;)] .</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(7)</text>
                  <doc_id>136</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Absolute discounting</title>
            <text>If we apply absolute discounting to every realization of the data, the expected smoothed counts are &#8721; E[&#732;c(uw)] = p(c(uw) = r)(r &#8722; D)
r&gt;0
+ p(c(uw) = 0)E[n 1+ (u&#8226;)]Dq u (w)
= E[c(uw)] &#8722; p(c(uw) &gt; 0)D
+ p(c(uw) = 0)E[n 1+ (u&#8226;)]Dq u (w) (8)
where, to be precise, the expectation E[n 1+ (u&#8226;)] should be conditioned on c(uw) = 0; in practice, it seems safe to ignore this. The MLE is then
p(w | u) = E[&#732;c(uw)] E[&#732;c(u&#8226;)] . (9)</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>If we apply absolute discounting to every realization of the data, the expected smoothed counts are &#8721; E[&#732;c(uw)] = p(c(uw) = r)(r &#8722; D)</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r&gt;0</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>+ p(c(uw) = 0)E[n 1+ (u&#8226;)]Dq u (w)</text>
                  <doc_id>139</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>= E[c(uw)] &#8722; p(c(uw) &gt; 0)D</text>
                  <doc_id>140</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>+ p(c(uw) = 0)E[n 1+ (u&#8226;)]Dq u (w) (8)</text>
                  <doc_id>141</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where, to be precise, the expectation E[n 1+ (u&#8226;)] should be conditioned on c(uw) = 0; in practice, it seems safe to ignore this.</text>
                  <doc_id>142</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The MLE is then</text>
                  <doc_id>143</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(w | u) = E[&#732;c(uw)] E[&#732;c(u&#8226;)] .</text>
                  <doc_id>144</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(9)</text>
                  <doc_id>145</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>4.3 Estimating D by leaving-one-out</title>
            <text>It would not be clear how to perform leavingone-out estimation on fractional counts, but here we have a distribution over realizations of the data, each with integral counts, and we can perform leaving-one-out estimation on each of these. In other words, our goal is to find the D that maximizes the expected leaving-one-out loglikelihood, which is just the expected value of (3):
E[L loo ] = E [ &#8721; n 1 log D + rn r log(r &#8722; 1 &#8722; D) + C ]
r&gt;1
= E[n 1 ] log D
&#8721; + rE[n r ] log(r &#8722; 1 &#8722; D) + C,
r&gt;1
where C is a constant not depending on D. We have made the assumption that the n r are independent.
By exactly the same reasoning as before, we obtain an upper bound for D:
D &#8804;
E[n 1 ] E[n 1 ] + 2E[n 2 ] . (10)
In our example above, D =
1.52 1.52+2&#183;0.24 = 0.76.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>It would not be clear how to perform leavingone-out estimation on fractional counts, but here we have a distribution over realizations of the data, each with integral counts, and we can perform leaving-one-out estimation on each of these.</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In other words, our goal is to find the D that maximizes the expected leaving-one-out loglikelihood, which is just the expected value of (3):</text>
                  <doc_id>147</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>E[L loo ] = E [ &#8721; n 1 log D + rn r log(r &#8722; 1 &#8722; D) + C ]</text>
                  <doc_id>148</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r&gt;1</text>
                  <doc_id>149</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>= E[n 1 ] log D</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; + rE[n r ] log(r &#8722; 1 &#8722; D) + C,</text>
                  <doc_id>151</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r&gt;1</text>
                  <doc_id>152</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where C is a constant not depending on D.</text>
                  <doc_id>153</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We have made the assumption that the n r are independent.</text>
                  <doc_id>154</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>By exactly the same reasoning as before, we obtain an upper bound for D:</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>D &#8804;</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>E[n 1 ] E[n 1 ] + 2E[n 2 ] .</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(10)</text>
                  <doc_id>158</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In our example above, D =</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1.52 1.52+2&#183;0.24 = 0.76.</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>4.4 Estimating the lower-order distribution</title>
            <text>We again require p &#8242; to satisfy the marginal constraint (5). Substituting in (7) and solving for p &#8242; as in Section 2.4, we obtain the solution
p &#8242; (w | u &#8242; ) = E[n 1+(&#8226;u &#8242; w)] E[n 1+ (&#8226;u &#8242; &#8226;)] . (11)
For the example above, the estimates for the unigram model p &#8242; (w) are</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We again require p &#8242; to satisfy the marginal constraint (5).</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Substituting in (7) and solving for p &#8242; as in Section 2.4, we obtain the solution</text>
                  <doc_id>162</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p &#8242; (w | u &#8242; ) = E[n 1+(&#8226;u &#8242; w)] E[n 1+ (&#8226;u &#8242; &#8226;)] .</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(11)</text>
                  <doc_id>164</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For the example above, the estimates for the unigram model p &#8242; (w) are</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>4</index>
            <title>4.5 Extensions</title>
            <text>p &#8242; (cat) = 0.86 0.86+0.9 &#8776; 0.489
p &#8242; (dog) = 0.9 0.86+0.9 &#8776; 0.511.
Chen and Goodman (1999) introduce three extensions to Kneser-Ney smoothing which are now standard. For our experiments, we used all three, for both integral counts and count distributions.
4.5.1 Interpolation In interpolated KN smoothing, the subtracted discounts are redistributed not only among unseen events but also seen events. That is,
&#732;c(uw) = max{0, c(uw) &#8722; D} + n 1+ (u&#8226;)Dp &#8242; (w | u &#8242; ).
In this case, &#947;(u) is always equal to one, so that q u (w) = p &#8242; (w | u &#8242; ). (Also note that (6) becomes an exact solution to the marginal constraint.) Theoretically, this requires us to derive a new estimate for D. However, as this is not trivial, nearly all implementations simply use the original estimate (4).
On count distributions, the smoothed counts become
E[&#732;c(uw)] = E[c(uw)] &#8722; p(c(uw) &gt; 0)D
+ E[n 1+ (u&#8226;)]Dp &#8242; (w | u &#8242; ). (12)
In our example, the smoothed counts are:
which give the smoothed probability estimates: p(cat | fat) =
0.766 0.766+0.334 = 0.696
p(dog | fat) =
0.334 0.766+0.334 = 0.304
p(dog | big) =
0.334 0.334+0.556 = 0.371
p(cat | big) =
0.556 0.334+0.556 = 0.629.
4.5.2 Modified discounts Modified KN smoothing uses a different discount D r for each count r &lt; 3, and a discount D 3+ for counts r &#8805; 3. On count distributions, a similar argument to the above leads to the estimates:
D 1 &#8804; 1 &#8722; 2Y E[n 2] E[n 1 ]
D 2 &#8804; 2 &#8722; 3Y E[n 3] E[n 2 ]
D 3+ &#8776; 3 &#8722; 4Y E[n 4] E[n 3 ]
Y =
E[n 1 ] E[n 1 ] + 2E[n 2 ] . (13)
One side-effect of this change is that (6) is no longer the correct solution to the marginal constraint (Teh, 2006; Sundermeyer et al., 2011). Although this problem can be fixed, standard implementations simply use (6).
4.5.3 Recursive smoothing In the original KN method, the lower-order model p &#8242; was estimated using (6); recursive KN smoothing applies KN smoothing to p &#8242; . To do this, we need to reconstruct counts whose MLE is (6). On integral counts, this is simple: we generate, for each n-gram type vu &#8242; w, an (n&#8722;1)-gram token u &#8242; w, for a total of n 1+ (&#8226;u &#8242; w) tokens. We then apply KN smoothing to these counts.
Analogously, on count distributions, for each n- gram type vu &#8242; w, we generate an (n &#8722; 1)-gram token u &#8242; w with probability p(c(vu &#8242; w) &gt; 0). Since &#8721; E[c(u &#8242; w)] = p(c(vu &#8242; w) &gt; 0) = E[n 1+ (&#8226;u &#8242; w)],
v
this has (11) as its MLE and therefore satisfies the marginal constraint. We then apply expected KN smoothing to these count distributions.
For the example above, the count distributions used for the unigram distribution would be:</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>p &#8242; (cat) = 0.86 0.86+0.9 &#8776; 0.489</text>
                  <doc_id>166</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p &#8242; (dog) = 0.9 0.86+0.9 &#8776; 0.511.</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Chen and Goodman (1999) introduce three extensions to Kneser-Ney smoothing which are now standard.</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For our experiments, we used all three, for both integral counts and count distributions.</text>
                  <doc_id>169</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.5.1 Interpolation In interpolated KN smoothing, the subtracted discounts are redistributed not only among unseen events but also seen events.</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>That is,</text>
                  <doc_id>171</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#732;c(uw) = max{0, c(uw) &#8722; D} + n 1+ (u&#8226;)Dp &#8242; (w | u &#8242; ).</text>
                  <doc_id>172</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In this case, &#947;(u) is always equal to one, so that q u (w) = p &#8242; (w | u &#8242; ).</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(Also note that (6) becomes an exact solution to the marginal constraint.</text>
                  <doc_id>174</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>) Theoretically, this requires us to derive a new estimate for D.</text>
                  <doc_id>175</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>However, as this is not trivial, nearly all implementations simply use the original estimate (4).</text>
                  <doc_id>176</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>On count distributions, the smoothed counts become</text>
                  <doc_id>177</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>E[&#732;c(uw)] = E[c(uw)] &#8722; p(c(uw) &gt; 0)D</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>+ E[n 1+ (u&#8226;)]Dp &#8242; (w | u &#8242; ).</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(12)</text>
                  <doc_id>180</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In our example, the smoothed counts are:</text>
                  <doc_id>181</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>which give the smoothed probability estimates: p(cat | fat) =</text>
                  <doc_id>182</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.766 0.766+0.334 = 0.696</text>
                  <doc_id>183</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(dog | fat) =</text>
                  <doc_id>184</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.334 0.766+0.334 = 0.304</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(dog | big) =</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.334 0.334+0.556 = 0.371</text>
                  <doc_id>187</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(cat | big) =</text>
                  <doc_id>188</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.556 0.334+0.556 = 0.629.</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.5.2 Modified discounts Modified KN smoothing uses a different discount D r for each count r &lt; 3, and a discount D 3+ for counts r &#8805; 3.</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>On count distributions, a similar argument to the above leads to the estimates:</text>
                  <doc_id>191</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>D 1 &#8804; 1 &#8722; 2Y E[n 2] E[n 1 ]</text>
                  <doc_id>192</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>D 2 &#8804; 2 &#8722; 3Y E[n 3] E[n 2 ]</text>
                  <doc_id>193</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>D 3+ &#8776; 3 &#8722; 4Y E[n 4] E[n 3 ]</text>
                  <doc_id>194</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Y =</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>E[n 1 ] E[n 1 ] + 2E[n 2 ] .</text>
                  <doc_id>196</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(13)</text>
                  <doc_id>197</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>One side-effect of this change is that (6) is no longer the correct solution to the marginal constraint (Teh, 2006; Sundermeyer et al., 2011).</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Although this problem can be fixed, standard implementations simply use (6).</text>
                  <doc_id>199</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.5.3 Recursive smoothing In the original KN method, the lower-order model p &#8242; was estimated using (6); recursive KN smoothing applies KN smoothing to p &#8242; .</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To do this, we need to reconstruct counts whose MLE is (6).</text>
                  <doc_id>201</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>On integral counts, this is simple: we generate, for each n-gram type vu &#8242; w, an (n&#8722;1)-gram token u &#8242; w, for a total of n 1+ (&#8226;u &#8242; w) tokens.</text>
                  <doc_id>202</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We then apply KN smoothing to these counts.</text>
                  <doc_id>203</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Analogously, on count distributions, for each n- gram type vu &#8242; w, we generate an (n &#8722; 1)-gram token u &#8242; w with probability p(c(vu &#8242; w) &gt; 0).</text>
                  <doc_id>204</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Since &#8721; E[c(u &#8242; w)] = p(c(vu &#8242; w) &gt; 0) = E[n 1+ (&#8226;u &#8242; w)],</text>
                  <doc_id>205</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>v</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>this has (11) as its MLE and therefore satisfies the marginal constraint.</text>
                  <doc_id>207</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We then apply expected KN smoothing to these count distributions.</text>
                  <doc_id>208</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For the example above, the count distributions used for the unigram distribution would be:</text>
                  <doc_id>209</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>5</index>
            <title>4.6 Summary</title>
            <text>The computational complexity of expected KN is almost identical to KN on integral counts. The main addition is computing and storing the count distributions. Using the dynamic program in Section 3.2, computing the distributions for each r is linear in the number of n-gram types, and we only need to compute the distributions up to r = 2 (or r = 4 for modified KN), and store them for r = 0 (or up to r = 2 for modified KN).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The computational complexity of expected KN is almost identical to KN on integral counts.</text>
                  <doc_id>210</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The main addition is computing and storing the count distributions.</text>
                  <doc_id>211</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Using the dynamic program in Section 3.2, computing the distributions for each r is linear in the number of n-gram types, and we only need to compute the distributions up to r = 2 (or r = 4 for modified KN), and store them for r = 0 (or up to r = 2 for modified KN).</text>
                  <doc_id>212</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Related Work</title>
        <text>Witten-Bell (WB) smoothing is somewhat easier than KN to adapt to fractional counts. The SRI- LM toolkit (Stolcke, 2002) implements a method which we call fractional WB:
p(w | u) = &#955;(u)p mle (w | u) + (1 &#8722; &#955;(u))p &#8242; (w | u &#8242; ) E[c(u)]
&#955;(u) = E[c(u)] + n 1+ (u&#8226;) ,
where n 1+ (u&#8226;) is the number of word types observed after context u, computed by ignoring all weights. This method, although simple, inconsistently uses weights for counting tokens but not types. Moreover, as we will see below, it does not perform as well as expected KN.
The only previous adaptation of KN smoothing to fractional counts that we are aware of is that of Tam and Schultz (2008) and Bisani and Ney (2008), called fractional KN. This method subtracts D directly from the fractional counts, zeroing out counts that are smaller than D. The discount D must be set by minimizing an error metric on held-out data using a line search (Tam, p. c.) or Powell&#8217;s method (Bisani and Ney, 2008), requiring repeated estimation and evaluation of the language model. By contrast, we choose D by leaving-oneout. Like KN on integral counts, our method has a closed-form approximation and requires neither held-out data nor trial and error.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Witten-Bell (WB) smoothing is somewhat easier than KN to adapt to fractional counts.</text>
              <doc_id>213</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The SRI- LM toolkit (Stolcke, 2002) implements a method which we call fractional WB:</text>
              <doc_id>214</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>p(w | u) = &#955;(u)p mle (w | u) + (1 &#8722; &#955;(u))p &#8242; (w | u &#8242; ) E[c(u)]</text>
              <doc_id>215</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#955;(u) = E[c(u)] + n 1+ (u&#8226;) ,</text>
              <doc_id>216</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where n 1+ (u&#8226;) is the number of word types observed after context u, computed by ignoring all weights.</text>
              <doc_id>217</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This method, although simple, inconsistently uses weights for counting tokens but not types.</text>
              <doc_id>218</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, as we will see below, it does not perform as well as expected KN.</text>
              <doc_id>219</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The only previous adaptation of KN smoothing to fractional counts that we are aware of is that of Tam and Schultz (2008) and Bisani and Ney (2008), called fractional KN.</text>
              <doc_id>220</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This method subtracts D directly from the fractional counts, zeroing out counts that are smaller than D.</text>
              <doc_id>221</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The discount D must be set by minimizing an error metric on held-out data using a line search (Tam, p.</text>
              <doc_id>222</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>c.</text>
              <doc_id>223</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>) or Powell&#8217;s method (Bisani and Ney, 2008), requiring repeated estimation and evaluation of the language model.</text>
              <doc_id>224</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>By contrast, we choose D by leaving-oneout.</text>
              <doc_id>225</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Like KN on integral counts, our method has a closed-form approximation and requires neither held-out data nor trial and error.</text>
              <doc_id>226</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>6 Language model adaptation</title>
        <text>N-gram language models are widely used in applications like machine translation and speech recognition to select fluent output sentences. Although they can easily be trained on large amounts of data, in order to perform well, they should be trained on data containing the right kind of language. For example, if we want to model spoken language, then we should train on spoken language data. If we train on newswire, then a spoken sentence might be regarded as ill-formed, because the distribution of sentences in these two domains are very different. In practice, we often have limited-size training data from a specific domain, and large amounts of data consisting of language from a variety of domains (we call this general-domain data). How can we utilize the large general-domain dataset to help us train a model on a specific domain?
Many methods (Lin et al., 1997; Gao et al., 2002; Klakow, 2000; Moore and Lewis, 2010; Axelrod et al., 2011) rank sentences in the generaldomain data according to their similarity to the in-domain data and select only those with score higher than some threshold. Such methods are effective and widely used. However, sometimes it is hard to say whether a sentence is totally in-domain or out-of-domain; for example, quoted speech in a news report might be partly in-domain if the domain of interest is broadcast conversation. Here, we propose to assign each sentence a probability to indicate how likely it is to belong to the domain of interest, and train a language model using expected KN smoothing. We show that this approach yields models with much better perplexity than the original sentence-selection approach.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>N-gram language models are widely used in applications like machine translation and speech recognition to select fluent output sentences.</text>
              <doc_id>227</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Although they can easily be trained on large amounts of data, in order to perform well, they should be trained on data containing the right kind of language.</text>
              <doc_id>228</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For example, if we want to model spoken language, then we should train on spoken language data.</text>
              <doc_id>229</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>If we train on newswire, then a spoken sentence might be regarded as ill-formed, because the distribution of sentences in these two domains are very different.</text>
              <doc_id>230</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In practice, we often have limited-size training data from a specific domain, and large amounts of data consisting of language from a variety of domains (we call this general-domain data).</text>
              <doc_id>231</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>How can we utilize the large general-domain dataset to help us train a model on a specific domain?</text>
              <doc_id>232</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Many methods (Lin et al., 1997; Gao et al., 2002; Klakow, 2000; Moore and Lewis, 2010; Axelrod et al., 2011) rank sentences in the generaldomain data according to their similarity to the in-domain data and select only those with score higher than some threshold.</text>
              <doc_id>233</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Such methods are effective and widely used.</text>
              <doc_id>234</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, sometimes it is hard to say whether a sentence is totally in-domain or out-of-domain; for example, quoted speech in a news report might be partly in-domain if the domain of interest is broadcast conversation.</text>
              <doc_id>235</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Here, we propose to assign each sentence a probability to indicate how likely it is to belong to the domain of interest, and train a language model using expected KN smoothing.</text>
              <doc_id>236</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We show that this approach yields models with much better perplexity than the original sentence-selection approach.</text>
              <doc_id>237</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>6.1 Method</title>
            <text>One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010). They first train two language models, p in on a set of in-domain data, and p out on a set of generaldomain data. Then each sentence w is assigned a score
H(w) = log(p in(w)) &#8722; log(p out (w)) . |w| They set a threshold on the score to select a subset.
We adapt this approach as follows. After selection, for each sentence in the subset, we use a sigmoid function to map the scores into probabilities:
p(w is in-domain) = 1 1 + exp(&#8722;H(w)) .
perplexity 260
160 .
140 . 0
. . fractional KN . . fractional WB . . integral KN . . expected KN
0.2 0.4 0.6 0.8
sentences selected (&#215;10 7 )
1.2 1.4
Then we use the weighted subset to train a language model with expected KN smoothing.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010).</text>
                  <doc_id>238</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>They first train two language models, p in on a set of in-domain data, and p out on a set of generaldomain data.</text>
                  <doc_id>239</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Then each sentence w is assigned a score</text>
                  <doc_id>240</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>H(w) = log(p in(w)) &#8722; log(p out (w)) .</text>
                  <doc_id>241</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>|w| They set a threshold on the score to select a subset.</text>
                  <doc_id>242</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We adapt this approach as follows.</text>
                  <doc_id>243</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>After selection, for each sentence in the subset, we use a sigmoid function to map the scores into probabilities:</text>
                  <doc_id>244</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(w is in-domain) = 1 1 + exp(&#8722;H(w)) .</text>
                  <doc_id>245</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>perplexity 260</text>
                  <doc_id>246</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>160 .</text>
                  <doc_id>247</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>140 .</text>
                  <doc_id>248</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>0</text>
                  <doc_id>249</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>.</text>
                  <doc_id>250</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>251</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>fractional KN .</text>
                  <doc_id>252</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>253</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>fractional WB .</text>
                  <doc_id>254</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>255</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>integral KN .</text>
                  <doc_id>256</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>257</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>expected KN</text>
                  <doc_id>258</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.2 0.4 0.6 0.8</text>
                  <doc_id>259</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>sentences selected (&#215;10 7 )</text>
                  <doc_id>260</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1.2 1.4</text>
                  <doc_id>261</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Then we use the weighted subset to train a language model with expected KN smoothing.</text>
                  <doc_id>262</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>6.2 Experiments</title>
            <text>Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data. They use the in-domain training data to select a subset of the generaldomain data, build a language model on the selected subset, and evaluate its perplexity on the indomain test data. Here, we follow this experimental framework and compare Moore and Lewis&#8217;s unweighted method to our weighted method. For our experiments, we used all the English data allowed for the BOLT Phase 1 Chinese- English evaluation. We took 60k sentences (1.7M words) of web forum data as in-domain data, further subdividing it into 54k sentences (1.5M words) for training, 3k sentences (100k words) for testing, and 3k sentences (100k words) for future use. The remaining 12.7M sentences (268M words) we treated as general-domain data.
We trained trigram language models and compared expected KN smoothing against integral KN smoothing, fractional WB smoothing, and fractional KN smoothing, measuring perplexity across various subset sizes (Figure 1). For fractional KN, for each subset size, we optimized D to mini-
mize perplexity on the test set to give it the greatest possible advantage; nevertheless, it is clearly the worst performer. Expected KN consistently gives the best perplexity, and, at the optimal subset size, obtains better perplexity (148) than the other methods (156 for integral KN, 162 for fractional WB and 197 for fractional KN). Finally, we note that integral KN is very sensitive to the subset size, whereas expected KN and the other methods are more robust.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data.</text>
                  <doc_id>263</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>They use the in-domain training data to select a subset of the generaldomain data, build a language model on the selected subset, and evaluate its perplexity on the indomain test data.</text>
                  <doc_id>264</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Here, we follow this experimental framework and compare Moore and Lewis&#8217;s unweighted method to our weighted method.</text>
                  <doc_id>265</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For our experiments, we used all the English data allowed for the BOLT Phase 1 Chinese- English evaluation.</text>
                  <doc_id>266</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We took 60k sentences (1.7M words) of web forum data as in-domain data, further subdividing it into 54k sentences (1.5M words) for training, 3k sentences (100k words) for testing, and 3k sentences (100k words) for future use.</text>
                  <doc_id>267</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The remaining 12.7M sentences (268M words) we treated as general-domain data.</text>
                  <doc_id>268</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We trained trigram language models and compared expected KN smoothing against integral KN smoothing, fractional WB smoothing, and fractional KN smoothing, measuring perplexity across various subset sizes (Figure 1).</text>
                  <doc_id>269</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For fractional KN, for each subset size, we optimized D to mini-</text>
                  <doc_id>270</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>mize perplexity on the test set to give it the greatest possible advantage; nevertheless, it is clearly the worst performer.</text>
                  <doc_id>271</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Expected KN consistently gives the best perplexity, and, at the optimal subset size, obtains better perplexity (148) than the other methods (156 for integral KN, 162 for fractional WB and 197 for fractional KN).</text>
                  <doc_id>272</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, we note that integral KN is very sensitive to the subset size, whereas expected KN and the other methods are more robust.</text>
                  <doc_id>273</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>7</index>
        <title>7 Word Alignment</title>
        <text>In this section, we show how to apply expected KN to the IBM word alignment models (Brown et al., 1993). This illustrates both how to use expected KN inside EM and how to use it beyond language modeling. Of course, expected KN can be applied to other instances of EM besides word alignment.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this section, we show how to apply expected KN to the IBM word alignment models (Brown et al., 1993).</text>
              <doc_id>274</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This illustrates both how to use expected KN inside EM and how to use it beyond language modeling.</text>
              <doc_id>275</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Of course, expected KN can be applied to other instances of EM besides word alignment.</text>
              <doc_id>276</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>7.1 Problem</title>
            <text>Given a French sentence f = f 1 f 2 &#183; &#183; &#183; f m and its English translation e = e 1 e 2 &#183; &#183; &#183; e n , an alignment a is a sequence a 1 , a 2 , . . . , a m , where a i is the index of the English word which generates the French word f i , or NULL. As is common, we assume that each French word can only be generated from one English word or from NULL (Brown et al., 1993; Och and Ney, 2003; Vogel et al., 1996).
The IBM models and related models define probability distributions p(a, f | e, &#952;), which model how likely a French sentence f is to be generated from an English sentence e with word alignment a. Different models parameterize this probability distribution in different ways. For example, Model 1 only models the lexical translation probabilities:
p(a, f | e, &#952;) &#8733; m&#8719;
p( f j | e a j ).
j=1
Models 2&#8211;5 and the HMM model introduce additional components to model word order and fertility. All, however, have the lexical translation model p( f j | e i ) in common. It also contains most of the model&#8217;s parameters and is where overfitting occurs most. Thus, here we only apply KN smoothing to the lexical translation probabilities, leaving the other model components for future work.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Given a French sentence f = f 1 f 2 &#183; &#183; &#183; f m and its English translation e = e 1 e 2 &#183; &#183; &#183; e n , an alignment a is a sequence a 1 , a 2 , .</text>
                  <doc_id>277</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>278</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>279</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>, a m , where a i is the index of the English word which generates the French word f i , or NULL.</text>
                  <doc_id>280</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>As is common, we assume that each French word can only be generated from one English word or from NULL (Brown et al., 1993; Och and Ney, 2003; Vogel et al., 1996).</text>
                  <doc_id>281</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The IBM models and related models define probability distributions p(a, f | e, &#952;), which model how likely a French sentence f is to be generated from an English sentence e with word alignment a.</text>
                  <doc_id>282</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Different models parameterize this probability distribution in different ways.</text>
                  <doc_id>283</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For example, Model 1 only models the lexical translation probabilities:</text>
                  <doc_id>284</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(a, f | e, &#952;) &#8733; m&#8719;</text>
                  <doc_id>285</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p( f j | e a j ).</text>
                  <doc_id>286</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=1</text>
                  <doc_id>287</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Models 2&#8211;5 and the HMM model introduce additional components to model word order and fertility.</text>
                  <doc_id>288</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>All, however, have the lexical translation model p( f j | e i ) in common.</text>
                  <doc_id>289</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It also contains most of the model&#8217;s parameters and is where overfitting occurs most.</text>
                  <doc_id>290</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Thus, here we only apply KN smoothing to the lexical translation probabilities, leaving the other model components for future work.</text>
                  <doc_id>291</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>7.2 Method</title>
            <text>The f and e are observed, while a is a latent variable. Normally, in the E step, we collect expected counts E[c(e, f )] for each e and f . Then, in the M step, we find the parameter values that maximize their likelihood. However, MLE is prone to overfitting, one symptom of which is the &#8220;garbage collection&#8221; phenomenon where a rare English word is wrongly aligned to many French words.
To reduce overfitting, we use expected KN smoothing during the M step. That is, during the E step, we calculate the distribution of c(e, f ) for each e and f , and during the M step, we train a language model on bigrams e f using expected KN smoothing (that is, with u = e and w = f ). This gives a smoothed probability estimate for p( f | e).
One question that arises is: what distribution to use as the lower-order distribution p &#8242; ? Following common practice in language modeling, we use the unigram distribution p( f ) as the lower-order distribution. We could also use the uniform distribution over word types, or a distribution that assigns zero probability to all known word types. (The latter case is equivalent to a backoff language model, where, since all bigrams are known, the lower-order model is never used.) Below, we compare the performance of all three choices.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The f and e are observed, while a is a latent variable.</text>
                  <doc_id>292</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Normally, in the E step, we collect expected counts E[c(e, f )] for each e and f .</text>
                  <doc_id>293</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Then, in the M step, we find the parameter values that maximize their likelihood.</text>
                  <doc_id>294</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>However, MLE is prone to overfitting, one symptom of which is the &#8220;garbage collection&#8221; phenomenon where a rare English word is wrongly aligned to many French words.</text>
                  <doc_id>295</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To reduce overfitting, we use expected KN smoothing during the M step.</text>
                  <doc_id>296</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>That is, during the E step, we calculate the distribution of c(e, f ) for each e and f , and during the M step, we train a language model on bigrams e f using expected KN smoothing (that is, with u = e and w = f ).</text>
                  <doc_id>297</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This gives a smoothed probability estimate for p( f | e).</text>
                  <doc_id>298</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>One question that arises is: what distribution to use as the lower-order distribution p &#8242; ?</text>
                  <doc_id>299</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Following common practice in language modeling, we use the unigram distribution p( f ) as the lower-order distribution.</text>
                  <doc_id>300</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We could also use the uniform distribution over word types, or a distribution that assigns zero probability to all known word types.</text>
                  <doc_id>301</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>(The latter case is equivalent to a backoff language model, where, since all bigrams are known, the lower-order model is never used.</text>
                  <doc_id>302</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>) Below, we compare the performance of all three choices.</text>
                  <doc_id>303</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>7.3 Alignment experiments</title>
            <text>We modified GIZA++ (Och and Ney, 2003) to perform expected KN smoothing as described above. Smoothing is enabled or disabled with a command-line switch, making direct comparisons simple. Our implementation is publicly available as open-source software. 1 We carried out experiments on two language pairs: Arabic to English and Czech to English. For Arabic-English, we used 5.4+4.3 million words of parallel text from the NIST 2009 constrained task, 2 and 346 word-aligned sentence pairs (LDC2006E86) for evaluation. For Czech- English, we used all 2.0+2.2 million words of training data from the WMT 2009 shared task, and 515 word-aligned sentence pairs (Bojar and Prokopov&#225;, 2006) for evaluation.
For all methods, we used five iterations of IBM Models 1, 2, and HMM, followed by three iterations of IBM Models 3 and 4. We applied expected KN smoothing to all iterations of all models. We aligned in both the foreign-to-English
1 https://github.com/hznlp/giza-kn 2 All data was used except for: United Nations proceedings (LDC2004E13), ISI Automatically Extracted Parallel Text (LDC2007E08), and Ummah newswire text (LDC2004T18).
and English-to-foreign directions and then used the grow-diag-final method to symmetrize them (Koehn et al., 2003), and evaluated the alignments using F-measure against gold word alignments.
As shown in Table 1, for KN smoothing, interpolation with the unigram distribution performs the best, while for WB smoothing, interestingly, interpolation with the uniform distribution performs the best. The difference can be explained by the way the two smoothing methods estimate p &#8242; . Consider again a training example with a word e that occurs nowhere else in the training data. In WB smoothing, p &#8242; ( f ) is the empirical unigram distribution. If f contains a word that is much more frequent than the correct translation of e, then smoothing may actually encourage the model to wrongly align e with the frequent word. This is much less of a problem in KN smoothing, where p &#8242; is estimated from bigram types rather than bigram tokens. We also compared with variational Bayes (Riley and Gildea, 2012) and fractional KN. Overall, expected KN performs the best. Variational Bayes is not consistent across different language pairs. While fractional KN does beat the baseline for both language pairs, the value of D, which we optimized D to maximize F1, is not consistent across language pairs: as shown in Figure 2, on Arabic- English, a smaller D is better, while for Czech- English, a larger D is better. By contrast, expected KN uses a closed-form expression for D that outperforms the best performance of fractional KN.
Table 2 shows that, if we apply expected KN smoothing to only selected stages of training, adding smoothing always brings an improvement,
alignment F1 72
64 .
0
. . Cze-Eng . . Ara-Eng
0.2 0.4
D
0.6 0.8
with the best setting being to smooth all stages. This shows that expected KN smoothing is consistently effective. It is also interesting to note that smoothing is less helpful for the fertility-based Models 3 and 4. Whether this is because modeling fertility makes them less susceptible to &#8220;garbage collection,&#8221; or the way they approximate the E step makes them less amenable to smoothing, or another reason, would require further investigation.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We modified GIZA++ (Och and Ney, 2003) to perform expected KN smoothing as described above.</text>
                  <doc_id>304</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Smoothing is enabled or disabled with a command-line switch, making direct comparisons simple.</text>
                  <doc_id>305</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Our implementation is publicly available as open-source software.</text>
                  <doc_id>306</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>1 We carried out experiments on two language pairs: Arabic to English and Czech to English.</text>
                  <doc_id>307</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For Arabic-English, we used 5.4+4.3 million words of parallel text from the NIST 2009 constrained task, 2 and 346 word-aligned sentence pairs (LDC2006E86) for evaluation.</text>
                  <doc_id>308</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For Czech- English, we used all 2.0+2.2 million words of training data from the WMT 2009 shared task, and 515 word-aligned sentence pairs (Bojar and Prokopov&#225;, 2006) for evaluation.</text>
                  <doc_id>309</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For all methods, we used five iterations of IBM Models 1, 2, and HMM, followed by three iterations of IBM Models 3 and 4.</text>
                  <doc_id>310</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We applied expected KN smoothing to all iterations of all models.</text>
                  <doc_id>311</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We aligned in both the foreign-to-English</text>
                  <doc_id>312</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 https://github.com/hznlp/giza-kn 2 All data was used except for: United Nations proceedings (LDC2004E13), ISI Automatically Extracted Parallel Text (LDC2007E08), and Ummah newswire text (LDC2004T18).</text>
                  <doc_id>313</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and English-to-foreign directions and then used the grow-diag-final method to symmetrize them (Koehn et al., 2003), and evaluated the alignments using F-measure against gold word alignments.</text>
                  <doc_id>314</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As shown in Table 1, for KN smoothing, interpolation with the unigram distribution performs the best, while for WB smoothing, interestingly, interpolation with the uniform distribution performs the best.</text>
                  <doc_id>315</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The difference can be explained by the way the two smoothing methods estimate p &#8242; .</text>
                  <doc_id>316</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Consider again a training example with a word e that occurs nowhere else in the training data.</text>
                  <doc_id>317</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In WB smoothing, p &#8242; ( f ) is the empirical unigram distribution.</text>
                  <doc_id>318</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>If f contains a word that is much more frequent than the correct translation of e, then smoothing may actually encourage the model to wrongly align e with the frequent word.</text>
                  <doc_id>319</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This is much less of a problem in KN smoothing, where p &#8242; is estimated from bigram types rather than bigram tokens.</text>
                  <doc_id>320</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>We also compared with variational Bayes (Riley and Gildea, 2012) and fractional KN.</text>
                  <doc_id>321</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Overall, expected KN performs the best.</text>
                  <doc_id>322</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>Variational Bayes is not consistent across different language pairs.</text>
                  <doc_id>323</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>While fractional KN does beat the baseline for both language pairs, the value of D, which we optimized D to maximize F1, is not consistent across language pairs: as shown in Figure 2, on Arabic- English, a smaller D is better, while for Czech- English, a larger D is better.</text>
                  <doc_id>324</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>By contrast, expected KN uses a closed-form expression for D that outperforms the best performance of fractional KN.</text>
                  <doc_id>325</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 2 shows that, if we apply expected KN smoothing to only selected stages of training, adding smoothing always brings an improvement,</text>
                  <doc_id>326</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>alignment F1 72</text>
                  <doc_id>327</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>64 .</text>
                  <doc_id>328</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0</text>
                  <doc_id>329</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>.</text>
                  <doc_id>330</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>331</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Cze-Eng .</text>
                  <doc_id>332</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>333</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Ara-Eng</text>
                  <doc_id>334</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.2 0.4</text>
                  <doc_id>335</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>D</text>
                  <doc_id>336</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.6 0.8</text>
                  <doc_id>337</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>with the best setting being to smooth all stages.</text>
                  <doc_id>338</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This shows that expected KN smoothing is consistently effective.</text>
                  <doc_id>339</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It is also interesting to note that smoothing is less helpful for the fertility-based Models 3 and 4.</text>
                  <doc_id>340</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Whether this is because modeling fertility makes them less susceptible to &#8220;garbage collection,&#8221; or the way they approximate the E step makes them less amenable to smoothing, or another reason, would require further investigation.</text>
                  <doc_id>341</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>7.4 Translation experiments</title>
            <text>Finally, we ran MT experiments to see whether the improved alignments also lead to improved translations. We used the same training data as before. For the Arabic-English tasks, we used the NIST 2008 test set as development data and the NIST 2009 test set as test data; for the Czech-English tasks, we used the WMT 2008 test set as development data and the WMT 2009 test set as test data.
We used the Moses toolkit (Koehn et al., 2007) to build MT systems using various alignments (for expected KN, we used the one interpolated with the unigram distribution, and for fractional WB, we used the one interpolated with the uniform distribution). We used a trigram language model trained on Gigaword (AFP, AP Worldstream, CNA, and Xinhua portions), and minimum error-rate training (Och, 2003) to tune the feature weights.
Table 1 shows that, although the relationship between alignment F1 and Bleu is not very consistent, expected KN smoothing achieves the best Bleu among all these methods and is significantly better than the baseline (p &lt; 0.01).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Finally, we ran MT experiments to see whether the improved alignments also lead to improved translations.</text>
                  <doc_id>342</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We used the same training data as before.</text>
                  <doc_id>343</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For the Arabic-English tasks, we used the NIST 2008 test set as development data and the NIST 2009 test set as test data; for the Czech-English tasks, we used the WMT 2008 test set as development data and the WMT 2009 test set as test data.</text>
                  <doc_id>344</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We used the Moses toolkit (Koehn et al., 2007) to build MT systems using various alignments (for expected KN, we used the one interpolated with the unigram distribution, and for fractional WB, we used the one interpolated with the uniform distribution).</text>
                  <doc_id>345</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We used a trigram language model trained on Gigaword (AFP, AP Worldstream, CNA, and Xinhua portions), and minimum error-rate training (Och, 2003) to tune the feature weights.</text>
                  <doc_id>346</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 1 shows that, although the relationship between alignment F1 and Bleu is not very consistent, expected KN smoothing achieves the best Bleu among all these methods and is significantly better than the baseline (p &lt; 0.01).</text>
                  <doc_id>347</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>8</index>
        <title>8 Conclusion</title>
        <text>For a long time, and as noted by many authors, the usage of KN smoothing has been limited by its restriction to integer counts. In this paper, we addressed this issue by treating fractional counts as distributions over integer counts and generalizing KN smoothing to operate on these distributions. This generalization makes KN smoothing, widely considered to be the best-performing smoothing method, applicable to many new areas. We have demonstrated the effectiveness of our method in two such areas and showed significant improvements in both.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>For a long time, and as noted by many authors, the usage of KN smoothing has been limited by its restriction to integer counts.</text>
              <doc_id>348</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, we addressed this issue by treating fractional counts as distributions over integer counts and generalizing KN smoothing to operate on these distributions.</text>
              <doc_id>349</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This generalization makes KN smoothing, widely considered to be the best-performing smoothing method, applicable to many new areas.</text>
              <doc_id>350</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We have demonstrated the effectiveness of our method in two such areas and showed significant improvements in both.</text>
              <doc_id>351</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>9</index>
        <title>Acknowledgements</title>
        <text>We thank Qing Dou, Ashish Vaswani, Wilson Yik- Cheung Tam, and the anonymous reviewers for their input to this work. This research was supported in part by DOI IBC grant D12AP00225.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We thank Qing Dou, Ashish Vaswani, Wilson Yik- Cheung Tam, and the anonymous reviewers for their input to this work.</text>
              <doc_id>352</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This research was supported in part by DOI IBC grant D12AP00225.</text>
              <doc_id>353</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Expected KN (interpolating with the unigram distribution) consistently outperforms all other methods. For variational Bayes, we followed Riley and Gildea (2012) in setting &#945; to zero (so that the choice of p &#8242; is irrelevant). For fractional KN, we chose D to maximize F1 (see Figure 2).</caption>
        <reference_text>In PAGE 8: ..., 2003), and evaluated the alignments using F-measure against gold word alignments. As shown in  Table1 , for KN smoothing, in- terpolation with the unigram distribution performs the best, while for WB smoothing, interestingly, interpolation with the uniform distribution per- forms the best. The di?erence can be explained by the way the two smoothing methods estimate p?....  In PAGE 9: ... We used a trigram language model trained on Gigaword (AFP, AP World- stream, CNA, and Xinhua portions), and minimum error-rate training (Och, 2003) to tune the feature weights.  Table1  shows that, although the relationship between alignment F1 and Bleu is not very con- sistent, expected KN smoothing achieves the best Bleu among all these methods and is significantly better than the baseline (p  lt; 0.01)....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>Alignment F1</cell>
              <cell>Bleu</cell>
              <cell>Alignment F1</cell>
              <cell>Alignment F1</cell>
              <cell>Bleu</cell>
              <cell>Bleu</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Smoothing</cell>
              <cell>p?#@#@p &#8242;</cell>
              <cell>Ara-Eng</cell>
              <cell>Cze-Eng</cell>
              <cell>Ara-Eng</cell>
              <cell>Cze-Eng</cell>
            </row>
            <row>
              <cell>none (baseline)</cell>
              <cell>?#@#@&#8211;</cell>
              <cell>66.5</cell>
              <cell>67.2</cell>
              <cell>37.0</cell>
              <cell>16.6</cell>
            </row>
            <row>
              <cell>variational Bayes</cell>
              <cell>uniform</cell>
              <cell>65.7</cell>
              <cell>65.5</cell>
              <cell>36.5</cell>
              <cell>16.6</cell>
            </row>
            <row>
              <cell></cell>
              <cell>unigram</cell>
              <cell>60.1</cell>
              <cell>63.7</cell>
              <cell>?#@#@&#8211;</cell>
              <cell>?#@#@&#8211;</cell>
            </row>
            <row>
              <cell>fractional WB</cell>
              <cell>uniform</cell>
              <cell>60.8</cell>
              <cell>66.5</cell>
              <cell>37.8</cell>
              <cell>16.9</cell>
            </row>
            <row>
              <cell></cell>
              <cell>zero</cell>
              <cell>60.8</cell>
              <cell>65.2</cell>
              <cell>?#@#@&#8211;</cell>
              <cell>?#@#@&#8211;</cell>
            </row>
            <row>
              <cell>fractional KN</cell>
              <cell>unigram</cell>
              <cell>67.7</cell>
              <cell>70.2</cell>
              <cell>37.2</cell>
              <cell>16.5</cell>
            </row>
            <row>
              <cell></cell>
              <cell>unigram</cell>
              <cell>69.7</cell>
              <cell>71.9</cell>
              <cell>38.2</cell>
              <cell>17.0</cell>
            </row>
            <row>
              <cell>expected KN</cell>
              <cell>uniform</cell>
              <cell>69.4</cell>
              <cell>71.3</cell>
              <cell>?#@#@&#8211;</cell>
              <cell>?#@#@&#8211;</cell>
            </row>
            <row>
              <cell></cell>
              <cell>zero</cell>
              <cell>69.2</cell>
              <cell>71.9</cell>
              <cell>?#@#@&#8211;</cell>
              <cell>?#@#@&#8211;</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Smoothing more stages of training makes alignment accuracy go up. For each row, we smoothed all iterations of the models indicated. Key: H = HMM model; &#8226; = smoothing enabled; &#9702; = smoothing disabled.</caption>
        <reference_text>In PAGE 8: ... By contrast, expected KN uses a closed-form expression for D that out- performs the best performance of fractional KN.  Table2  shows that, if we apply expected KN smoothing to only selected stages of training, adding smoothing always brings an improvement, . 0 0....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>Smoothed models</cell>
              <cell>Smoothed models#@#@Alignment F1</cell>
              <cell>Smoothed models</cell>
              <cell>Smoothed models</cell>
              <cell>Smoothed models</cell>
              <cell>Alignment F1</cell>
              <cell>Alignment F1</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>1</cell>
              <cell>2</cell>
              <cell>H</cell>
              <cell>3</cell>
              <cell>4</cell>
              <cell>Ara-Eng</cell>
              <cell>Cze-Eng</cell>
            </row>
            <row>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>66.5</cell>
              <cell>67.2</cell>
            </row>
            <row>
              <cell>?#@#@&#8226;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>67.3</cell>
              <cell>67.9</cell>
            </row>
            <row>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#8226;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>68.0</cell>
              <cell>68.7</cell>
            </row>
            <row>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#8226;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>68.6</cell>
              <cell>70.0</cell>
            </row>
            <row>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#8226;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>66.9</cell>
              <cell>68.4</cell>
            </row>
            <row>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#9702;</cell>
              <cell>?#@#@&#8226;</cell>
              <cell>67.0</cell>
              <cell>68.6</cell>
            </row>
            <row>
              <cell>?#@#@&#8226;</cell>
              <cell>?#@#@&#8226;</cell>
              <cell>?#@#@&#8226;</cell>
              <cell>?#@#@&#8226;</cell>
              <cell>?#@#@&#8226;</cell>
              <cell>69.7</cell>
              <cell>71.9</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Jes&#250;s Andr&#233;s-Ferrer</author>
        </authors>
        <title>Statistical approaches for natural language modelling and monotone statistical machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Amittai Axelrod</author>
          <author>Xiaodong He</author>
          <author>Jianfeng Gao</author>
        </authors>
        <title>Domain adaptation via pseudo in-domain data selection.</title>
        <publication>In Proc. EMNLP,</publication>
        <pages>355--362</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Maximilian Bisani</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Jointsequence models for grapheme-to-phoneme conversion.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Ondr&#283;j Bojar</author>
          <author>Magdalena Prokopov&#225;</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>F Brown</author>
          <author>Stephen A Della Pietra</author>
          <author>J Vincent</author>
        </authors>
        <title>Czech-English word alignment. In</title>
        <publication>Proc. LREC,</publication>
        <pages>1236--1239</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Della Pietra</author>
          <author>Robert L Mercer</author>
        </authors>
        <title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1993</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Stanley F Chen</author>
          <author>Joshua Goodman</author>
        </authors>
        <title>An empirical study of smoothing techniques for language modeling. Computer Speech and Language,</title>
        <publication>None</publication>
        <pages>13--359</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>A P Dempster</author>
          <author>N M Laird</author>
          <author>D B Rubin</author>
        </authors>
        <title>Maximum likelihood from incomplete data via the EM algorithm.</title>
        <publication>None</publication>
        <pages>39--1</pages>
        <date>1977</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Jianfeng Gao</author>
          <author>Joshua Goodman</author>
          <author>Mingjing Li</author>
          <author>KaiFu Lee</author>
        </authors>
        <title>Toward a unified approach to statistical language modeling for Chinese.</title>
        <publication>None</publication>
        <pages>1--3</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Joshua T Goodman</author>
        </authors>
        <title>A bit of progress in language modeling: Extended version.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Joshua Goodman</author>
        </authors>
        <title>Exponential priors for maximum entropy models.</title>
        <publication>In Proc. HLT-NAACL,</publication>
        <pages>305--312</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Yili Hong</author>
        </authors>
        <title>On computing the distribution function for the Poisson binomial distribution.</title>
        <publication>Computational Statistics and Data Analysis,</publication>
        <pages>59--41</pages>
        <date>2013</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Dietrich Klakow</author>
        </authors>
        <title>Selecting articles from the language model training corpus.</title>
        <publication>In Proc. ICASSP,</publication>
        <pages>1695--1698</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Kneser</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Improved backing-off for M-gram language modeling.</title>
        <publication>In Proc. ICASSP</publication>
        <pages>181--184</pages>
        <date>1995</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz Josef Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical phrase-based translation.</title>
        <publication>In Proc. HLT-NAACL,</publication>
        <pages>127--133</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Joern Wuebker</author>
          <author>Mei-Yuh Hwang</author>
          <author>Chris Quirk</author>
        </authors>
        <title>Leave-one-out phrase model training for large-scale deployment.</title>
        <publication>In Proc. WMT,</publication>
        <pages>460--467</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Hieu Hoang</author>
          <author>Alexandra Birch</author>
          <author>Chris Callison-Burch</author>
          <author>Marcello Federico</author>
          <author>Nicola Bertoldi</author>
          <author>Brooke Cowan</author>
          <author>Wade Shen</author>
        </authors>
        <title>Moses: Open source toolkit for statistical machine translation.</title>
        <publication>In Proc. ACL, Companion Volume,</publication>
        <pages>177--180</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Sung-Chien Lin</author>
          <author>Chi-Lung Tsai</author>
          <author>Lee-Feng Chien</author>
          <author>KehJiann Chen</author>
          <author>Lin-Shan Lee</author>
        </authors>
        <title>Chinese language model adaptation based on document classification and multiple domain-specific language models.</title>
        <publication>In Proc. Eurospeech,</publication>
        <pages>1463--1466</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Robert Moore</author>
          <author>William Lewis</author>
        </authors>
        <title>Intelligent selection of language model training data. In</title>
        <publication>Proc. ACL,</publication>
        <pages>220--224</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Hermann Ney</author>
          <author>Ute Essen</author>
          <author>Reinhard Kneser</author>
        </authors>
        <title>On structuring probabilistic dependencies in stochastic language modelling.</title>
        <publication>None</publication>
        <pages>1</pages>
        <date>1994</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A systematic comparison of various statistical alignment models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>In Proc. ACL,</publication>
        <pages>160--167</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Darcey Riley</author>
          <author>Daniel Gildea</author>
        </authors>
        <title>Improving the IBM alignment models using variational Bayes.</title>
        <publication>In Proc. ACL (Volume 2: Short Papers),</publication>
        <pages>306--310</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Andreas Stolcke</author>
        </authors>
        <title>SRILM &#8211; an extensible language modeling toolkit.</title>
        <publication>In Proc. International Conference on Spoken Language Processing,</publication>
        <pages>901--904</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>Martin Sundermeyer</author>
          <author>Ralf Schl&#252;ter</author>
          <author>Hermann Ney</author>
        </authors>
        <title>On the estimation of discount parameters for language model smoothing. In</title>
        <publication>Proc. Interspeech,</publication>
        <pages>1433--1436</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Yik-Cheung Tam</author>
          <author>Tanja Schultz</author>
        </authors>
        <title>Correlated bigram LSA for unsupervised language model adaptation.</title>
        <publication>In Proc. NIPS,</publication>
        <pages>1633--1640</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>Yee Whye Teh</author>
        </authors>
        <title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
        <publication>In Proc. COLING-ACL,</publication>
        <pages>985--992</pages>
        <date>2006</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Andr&#233;s-Ferrer, 2010</string>
        <sentence_id>39036</sentence_id>
        <char_offset>208</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Axelrod et al., 2011</string>
        <sentence_id>39298</sentence_id>
        <char_offset>87</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Bisani and Ney, 2008</string>
        <sentence_id>39036</sentence_id>
        <char_offset>155</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>2</reference_id>
        <string>Bisani and Ney, 2008</string>
        <sentence_id>39253</sentence_id>
        <char_offset>22</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>2</reference_id>
        <string>Bisani and Ney (2008)</string>
        <sentence_id>39249</sentence_id>
        <char_offset>125</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>3</reference_id>
        <string>Bojar and Prokopov&#225;, 2006</string>
        <sentence_id>39335</sentence_id>
        <char_offset>139</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>6</reference_id>
        <string>Chen and Goodman, 1999</string>
        <sentence_id>39034</sentence_id>
        <char_offset>134</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>6</reference_id>
        <string>Chen and Goodman, 1999</string>
        <sentence_id>39104</sentence_id>
        <char_offset>90</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>6</reference_id>
        <string>Chen and Goodman (1999)</string>
        <sentence_id>39195</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>7</reference_id>
        <string>Dempster et al., 1977</string>
        <sentence_id>39037</sentence_id>
        <char_offset>42</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>8</reference_id>
        <string>Gao et al., 2002</string>
        <sentence_id>39298</sentence_id>
        <char_offset>32</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>9</reference_id>
        <string>Goodman, 2001</string>
        <sentence_id>39036</sentence_id>
        <char_offset>50</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>10</reference_id>
        <string>Goodman, 2004</string>
        <sentence_id>39036</sentence_id>
        <char_offset>65</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>11</reference_id>
        <string>Hong, 2013</string>
        <sentence_id>39138</sentence_id>
        <char_offset>151</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>12</reference_id>
        <string>Klakow, 2000</string>
        <sentence_id>39298</sentence_id>
        <char_offset>50</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>13</reference_id>
        <string>Kneser and Ney, 1995</string>
        <sentence_id>39034</sentence_id>
        <char_offset>112</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>13</reference_id>
        <string>Kneser and Ney, 1995</string>
        <sentence_id>39109</sentence_id>
        <char_offset>115</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>14</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>39340</sentence_id>
        <char_offset>95</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>15</reference_id>
        <string>Wuebker et al., 2012</string>
        <sentence_id>39036</sentence_id>
        <char_offset>229</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>16</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>39371</sentence_id>
        <char_offset>27</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>17</reference_id>
        <string>Lin et al., 1997</string>
        <sentence_id>39298</sentence_id>
        <char_offset>14</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>18</reference_id>
        <string>Moore and Lewis (2010)</string>
        <sentence_id>39256</sentence_id>
        <char_offset>69</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>18</reference_id>
        <string>Moore and Lewis (2010)</string>
        <sentence_id>39281</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>18</reference_id>
        <string>Moore and Lewis, 2010</string>
        <sentence_id>39298</sentence_id>
        <char_offset>64</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>19</reference_id>
        <string>Ney et al., 1994</string>
        <sentence_id>39057</sentence_id>
        <char_offset>22</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>19</reference_id>
        <string>Ney et al., 1994</string>
        <sentence_id>39104</sentence_id>
        <char_offset>72</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>20</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>39307</sentence_id>
        <char_offset>124</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>20</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>39330</sentence_id>
        <char_offset>20</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>21</reference_id>
        <string>Och, 2003</string>
        <sentence_id>39372</sentence_id>
        <char_offset>135</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>23</reference_id>
        <string>Stolcke, 2002</string>
        <sentence_id>39243</sentence_id>
        <char_offset>21</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>24</reference_id>
        <string>Sundermeyer et al., 2011</string>
        <sentence_id>39225</sentence_id>
        <char_offset>116</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>25</reference_id>
        <string>Tam and Schultz (2008)</string>
        <sentence_id>39249</sentence_id>
        <char_offset>98</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>25</reference_id>
        <string>Tam and Schultz, 2008</string>
        <sentence_id>39036</sentence_id>
        <char_offset>100</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>26</reference_id>
        <string>Teh, 2006</string>
        <sentence_id>39225</sentence_id>
        <char_offset>105</char_offset>
      </citation>
    </citations>
  </content>
</document>
