<PAPER>
  <FILENO/>
  <TITLE>A Semantic Evaluation of Machine Translation Lexical Choice</TITLE>
  <AUTHORS/>
  <ABSTRACT>
    <A-S ID="S-52259">While automatic metrics of translation quality are invaluable for machine translation research, deeper understanding of translation errors require more focused evaluations designed to target specific aspects of translation quality.</A-S>
    <A-S ID="S-52260">We show that Word Sense Disambiguation (WSD) can be used to evaluate the quality of machine translation lexical choice, by applying a standard phrase-based SMT system on the SemEval2010 Cross-Lingual WSD task.</A-S>
    <A-S ID="S-52261">This case study reveals that the SMT system does not perform as well as a WSD system trained on the exact same parallel data, and that local context models based on source phrases and target n-grams are much weaker representations of context than the simple templates used by the WSD system.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-52262">Much research has focused on automatically evaluating the quality of Machine Translation (MT) by comparing automatic translations to human translations on samples of a few thousand sentences.</S>
        <S ID="S-52263">Many metrics (<REF ID="R-43" RPTR="53">Papineni et al., 2002</REF>; <REF ID="R-02" RPTR="1">Banerjee and Lavie, 2005</REF>; <REF ID="R-22" RPTR="29">Gim&#233;nez and M&#225;rquez, 2007</REF>; <REF ID="R-33" RPTR="44">Lo and Wu, 2011</REF>, for instance) have been proposed to estimate the adequacy and fluency of machine translation and evaluated based on their correlatation with human judgements of translation quality (<REF ID="R-04" RPTR="3">Callison-Burch et al., 2010</REF>).</S>
        <S ID="S-52264">While these metrics have proven invaluable in driving progress in MT research, finergrained evaluations of translation quality are necessary to provide a more focused analysis of translation errors.</S>
        <S ID="S-52265">When developing complex MT systems, comparing BLEU or TER scores is not sufficient to understand what improved or what went wrong.</S>
        <S ID="S-52266">Error analysis can of course be done manually (Vilar et al., 2006), but it is often too slow and expensive to be performed as often as needed during system development.</S>
        <S ID="S-52267">Several metrics have been recently proposed to evaluate specific aspects of translation quality such as word order (<REF ID="R-03" RPTR="2">Birch et al., 2010</REF>; <REF ID="R-10" RPTR="16">Chen et al., 2012</REF>).</S>
        <S ID="S-52268">While word order is indirectly taken into account by BLEU, TER or METEOR scores, dedicated metrics provide a direct evaluation that lets us understand whether a given system&#8217;s reordering performance improved during system development.</S>
        <S ID="S-52269">Word order metrics provide a complementary tool for targeting evaluation and analysis to a specific aspect of machine translation quality.</S>
      </P>
      <P>
        <S ID="S-52270">There has not been as much work on evaluating the lexical choice performance of MT: does a MT system preserve the meaning of words in translation?</S>
        <S ID="S-52271">This is of course measured indirectly by commonly used global metrics, but a more focused evaluation can help us gain a better understanding of the behavior of MT systems.</S>
      </P>
      <P>
        <S ID="S-52272">In this paper, we show that MT lexical choice can be framed and evaluated as a standard Word Sense Disambiguation (WSD) task.</S>
        <S ID="S-52273">We leverage existing WSD shared tasks in order to evaluate whether word meaning is preserved in translation.</S>
        <S ID="S-52274">Let us emphasize that, just like reordering metrics, our WSD evaluation is meant to complement global metrics of translation quality.</S>
        <S ID="S-52275">In previous work, intrinsic evaluations of lexical choice have been performed using either semi-automatically constructed data sets</S>
      </P>
      <P>
        <S ID="S-52276">based on MT reference translations (Gim&#233;nez and M&#224;rquez, 2008; <REF ID="R-06" RPTR="10">Carpuat and Wu, 2008</REF>), or manually constructed word sense disambiguation test beds that do not exactly match MT lexical choice (<REF ID="R-05" RPTR="5">Carpuat and Wu, 2005</REF>).</S>
        <S ID="S-52277">We will show how existing Cross-Lingual Word Sense Disambiguation tasks (<REF ID="R-29" RPTR="36">Lefever and Hoste, 2010</REF>; <REF ID="R-30" RPTR="42">Lefever and Hoste, 2013</REF>) can be directly seen as machine translation lexical choice (Section 2): their sense inventory is based on translations in a second language rather than arbitrary sense representations used in other WSD tasks (<REF ID="R-05" RPTR="6">Carpuat and Wu, 2005</REF>); unlike in MT evaluation settings, human annotators can more easily provide a complete representation of all correct meanings of a word.</S>
        <S ID="S-52278">Second, we show how using this task for evaluating the lexical choice performance of several phrase-based SMT systems (PB- SMT) gives some insights into their strengths and weaknesses (Section 5).</S>
      </P>
      <P>
        <S ID="S-52279">2 Selecting a Word Sense Disambiguation Task to Evaluate MT Lexical Choice</S>
      </P>
      <P>
        <S ID="S-52280">Word Sense Disambiguation consists in determining the correct sense of a word in context.</S>
        <S ID="S-52281">This challenging problem has been studied from a rich variety of persectives in Natural Language Processing (see <REF ID="R-00" RPTR="0">Agirre and Edmonds (2006)</REF> for an overview.</S>
        <S ID="S-52282">) The Senseval and SemEval series of evaluations (<REF ID="R-17" RPTR="24">Edmonds and Cotton, 2001</REF>; Mihalcea and Edmonds, 2004; Agirre et al., 2007) have driven the standardization of methodology for evaluating WSD systems.</S>
        <S ID="S-52283">Many shared tasks were organized over the years, providing evaluation settings that vary along several dimensions, including:</S>
      </P>
      <P>
        <S ID="S-52284">&#8226; target vocabulary: in all word tasks, systems are expected to tag all content words in running text (<REF ID="R-42" RPTR="52">Palmer et al., 2001</REF>), while in lexical sample tasks, the evaluation considers a smaller predefined set of target words (<REF ID="R-36" RPTR="46">Mihalcea et al., 2004</REF>; <REF ID="R-29" RPTR="37">Lefever and Hoste, 2010</REF>).</S>
      </P>
      <P>
        <S ID="S-52285">&#8226; language: English is by far the most studied language, but the disambiguation of words in other languages such as Chinese (<REF ID="R-23" RPTR="30">Jin et al., 2007</REF>) has been considered.</S>
      </P>
      <P>
        <S ID="S-52286">&#8226; sense inventory: many tasks use WordNet senses (Fellbaum, 1998), but other sense representations have been used, including alternate semantic databases such as HowNet (<REF ID="R-15" RPTR="21">Dong, 1998</REF>), or lexicalizations in one or more languages (<REF ID="R-13" RPTR="19">Chklovski et al., 2004</REF>).</S>
      </P>
      <P>
        <S ID="S-52287">The Cross-Lingual Word Sense Disambiguation (CLWSD) task introduced at a recent edition of SemEval (<REF ID="R-29" RPTR="38">Lefever and Hoste, 2010</REF>) is an English lexical sample task that uses translations in other European languages as a sense inventory.</S>
        <S ID="S-52288">As a result, it is particularly well suited to evaluating machine translation lexical choice.</S>
      </P>
      <P>
        <S ID="S-52289">2.1 Translations as Word Sense Representations</S>
      </P>
      <P>
        <S ID="S-52290">The CLWSD task is essentially the same task as MT lexical choice: given English target words in context, systems are asked to predict translations in other European languages.</S>
        <S ID="S-52291">The gold standard consists of translations proposed by several bilingual humans, as can be seen in Table 1.</S>
        <S ID="S-52292">MT system predictions can be compared to human annotations directly, without introducing additional sources of ambiguity and mismatches due to representation differences.</S>
        <S ID="S-52293">This contrasts with our previous work on evaluating MT on a WSD task (<REF ID="R-05" RPTR="7">Carpuat and Wu, 2005</REF>), which used text annotated with abstract sense categories from the HowNet knowledge base (<REF ID="R-15" RPTR="22">Dong, 1998</REF>).</S>
        <S ID="S-52294">In HowNet, each word is defined using a concept, constructed as a combination of basic units of meaning, called sememes.</S>
        <S ID="S-52295">Words that share the same concept can be viewed as synonyms.</S>
        <S ID="S-52296">Evaluating MT using a gold standard of HowNet categories requires to map translations from the MT output to the HowNet representation.</S>
        <S ID="S-52297">Some categories are annotated with English translations, but additional effort is required in order to cover all translation candidates produced by the MT system.</S>
      </P>
      <P>
        <S ID="S-52298">2.2 Controlled Learning Conditions</S>
      </P>
      <P>
        <S ID="S-52299">Another advantage of the CLWSD task is that it provides controlled learning conditions (even though it is an unsupervised task with no annotated training data.</S>
        <S ID="S-52300">) The gold labels for CLWSD are learned from parallel corpora.</S>
        <S ID="S-52301">As a result MT lexical choice models can be estimated on the exact same data.</S>
        <S ID="S-52302">Translations for English words in the lexical sample are extracted from a semi-automatic word alignment of</S>
      </P>
      <P>
        <S ID="S-52303">sentences from the Europarl parallel corpus (<REF ID="R-28" RPTR="35">Koehn, 2005</REF>).</S>
        <S ID="S-52304">These translations are then manually clustered into senses.</S>
        <S ID="S-52305">When constructing the gold annotation, human annotators are given occurrences of target words in context.</S>
        <S ID="S-52306">For each occurrence, they select a sense cluster and provide all translations from this cluster that are correct in this specific context.</S>
        <S ID="S-52307">Since three annotators contribute, each test occurrence is therefore tagged with a set of translations in another language, along with a frequency which represents the number of annotators who selected it.</S>
        <S ID="S-52308">A more detailed description of the annotation process can be found in (<REF ID="R-29" RPTR="39">Lefever and Hoste, 2010</REF>).</S>
        <S ID="S-52309">Again, this contrasts with our previous work on evaluating MT on a HowNet-based Chinese WSD task, where Chinese sentences were manually annotated with HowNet senses which were completely unrelated to the parallel corpus used for training the SMT system.</S>
        <S ID="S-52310">Using CLWSD as an evaluation of MT lexical choice solves this issue and provides controlled learning conditions.</S>
      </P>
      <P>
        <S ID="S-52311">2.3 CLWSD evaluates the semantic adequacy of MT lexical choice</S>
      </P>
      <P>
        <S ID="S-52312">A key challenge in MT evaluation lies in deciding whether the meaning of the translation is correct when it does not exactly match the reference translation.</S>
        <S ID="S-52313">METEOR uses WordNet synonyms and learned paraphrases tables (<REF ID="R-14" RPTR="20">Denkowski and Lavie, 2010</REF>).</S>
        <S ID="S-52314">MEANT uses vector-space based lexical similarity scores (<REF ID="R-34" RPTR="45">Lo et al., 2012</REF>).</S>
        <S ID="S-52315">While these methods lead to higher correlations with human judgements on average, they are not ideal for a fine-grained evaluation of lexical choice: similarity scores are defined independently of context and might give credit to incorrect translations (<REF ID="R-07" RPTR="12">Carpuat et al., 2012</REF>).</S>
        <S ID="S-52316">In contrast, CLWSD solves this difficult problem by providing all correct translation candidates in context according to several human annotators.</S>
        <S ID="S-52317">These multiple translations provide a more complete representation of the correct meaning of each occurrence of a word in context.</S>
        <S ID="S-52318">The CLWSD annotation procedure is designed to easily let human annotators provide many correct translation alternatives for a word.</S>
        <S ID="S-52319">Producing many correct annotations for a complete sentence is a much more expensive undertaking: crowdsourcing can help alleviate the cost of obtaining a small number of reference translation (Zbib et al., 2012), but acquiring a complete representation of all possible translations of a source sentence is a much more complex task (<REF ID="R-16" RPTR="23">Dreyer and Marcu, 2012</REF>).</S>
        <S ID="S-52320">Machine translation evaluations typically use between one and four reference translations, which provide a very incomplete representation of the correct semantics of the input sentence in the output language.</S>
        <S ID="S-52321">CLWSD provides a more complete representation through the multiple gold translations available.</S>
      </P>
      <P>
        <S ID="S-52322">2.4 Limitations</S>
      </P>
      <P>
        <S ID="S-52323">The main drawback of using CLWSD to evaluate lexical choice is that CLWSD is a lexical sample task, which only evaluates disambiguation of 20 English nouns.</S>
        <S ID="S-52324">This arbitrary sample of words does not let us target words or phrases that might be specifically interesting for MT.</S>
      </P>
      <P>
        <S ID="S-52325">In addition, the data available through the shared task does not let us evaluate complete translations of the CLWSD test sentences, since full references translations are not available.</S>
        <S ID="S-52326">Instead of using</S>
      </P>
      <P>
        <S ID="S-52327">a WSD dataset for MT purposes, we could take the converse approach andautomatically construct a WSD test set based on MT evaluation corpora (<REF ID="R-45" RPTR="55">Vickrey et al., 2005</REF>; Gim&#233;nez and M&#224;rquez, 2008; <REF ID="R-06" RPTR="11">Carpuat and Wu, 2008</REF>; <REF ID="R-07" RPTR="13">Carpuat et al., 2012</REF>).</S>
        <S ID="S-52328">However, this approach suffers from noisy automatic alignments between source and reference, as well as from a limited representation of the correct meaning of words in context due to the limited number of reference translations.</S>
        <S ID="S-52329">Other SemEval tasks such as the Cross-Lingual Lexical Substitution Task (<REF ID="R-37" RPTR="47">Mihalcea et al., 2010</REF>) would also provide an appropriate test bed.</S>
        <S ID="S-52330">We focused on the CLWSD task, since it uses senses drawn from the Europarl parallel corpus, and therefore offers more constrained settings for comparison between systems.</S>
        <S ID="S-52331">The lexical substitution task targets verbs and adjectives in addition to nouns, and would therefore be an interesting test case to consider in future work.</S>
      </P>
      <P>
        <S ID="S-52332">2.5 Official and MT-centric Evaluation Metrics</S>
      </P>
      <P>
        <S ID="S-52333">In order to make comparison with other systems possible, we follow the standard evaluation framework defined for the task and score the output of all our systems using four different metrics, computed using the scoring tool made available by the organizers.</S>
        <S ID="S-52334">The difference between system predictions and gold standard annotations are quantified using precision and recall scores 1 , defined as follows.</S>
        <S ID="S-52335">Given a set T of test items and a set H of annotators, H i is the set of translation proposed by all annotators h for instance i &#8712; T .</S>
        <S ID="S-52336">Each translation type res in H i has an associated frequency freq res , which represents the number of human annotators which selected res as one of their top 3 translations.</S>
        <S ID="S-52337">Given a set of system answers A of items i &#8712; T such that the system provides at least one answer, a i : i &#8712; A is is the set of answers from the system for instance i.</S>
        <S ID="S-52338">For each i, the scorer computes the intersection of the system answers a i and the gold standard H i .</S>
      </P>
      <P>
        <S ID="S-52339">Systems propose as many answers as deemed nec-</S>
      </P>
      <P>
        <S ID="S-52340">1 In this paper, we focus on evaluating translation systems</S>
      </P>
      <P>
        <S ID="S-52341">whose task is to produce a single complete translation for a given sentence.</S>
        <S ID="S-52342">As a result, we only focus on the 1-best MT output and do not report the relaxed out-of-five evaluation setting also considered in the official SemEval task.</S>
      </P>
      <P>
        <S ID="S-52343">essary, but the scores are divided by the number of guesses in order not to favor systems that output many answers per instance.</S>
      </P>
      <P>
        <S ID="S-52344">&#8721;</S>
      </P>
      <P>
        <S ID="S-52345">res&#8712;a i</S>
      </P>
      <P>
        <S ID="S-52346">freq res |a i ||H i |</S>
      </P>
      <P>
        <S ID="S-52347">Precision = 1 &#8721;</S>
      </P>
      <P>
        <S ID="S-52348">|A| a i :i&#8712;A &#8721;</S>
      </P>
      <P>
        <S ID="S-52349">Recall = 1 &#8721;</S>
      </P>
      <P>
        <S ID="S-52350">res&#8712;a freq res i</S>
      </P>
      <P>
        <S ID="S-52351">|T | a i :i&#8712;T |a i ||H i |</S>
      </P>
      <P>
        <S ID="S-52352">We also report Mode Precision and Mode Recall scores: instead of comparing system answers to the full set of gold standard translations H i for an instance i &#8712; T , the Mode Precision and Recall scores only use a single gold translation, which is the translation chosen most frequently by the human annotators.</S>
        <S ID="S-52353">In addition, we compute the 1-gram precision component of the BLEU score (<REF ID="R-43" RPTR="54">Papineni et al., 2002</REF>), denoted as BLEU1 in the result tables 2 .</S>
        <S ID="S-52354">In contrast with the official CLWSD evaluation scores described above, BLEU1 gives equal weight to all translation candidates, which can be seen as multiple references.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 PBSMT system</HEADER>
      <P>
        <S ID="S-52380">We use a typical phrase-based SMT system trained for English-to-Spanish translation.</S>
        <S ID="S-52381">Its application to the CLWSD task affects the selection of training data and its preprocessing, but the SMT model design and learning strategies are exactly the same as for conventional translation tasks.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>3.1 Model</HEADER>
        <P>
          <S ID="S-52355">We use the NRC&#8217;s PORTAGE phrase-based SMT system, which implements a standard phrasal beamsearch decoder with cube pruning.</S>
          <S ID="S-52356">Translation hypotheses are scored according to the following features:</S>
        </P>
        <P>
          <S ID="S-52357">&#8226; 4 phrase-table scores: phrasal translation probabilites with Kneser-Ney smoothing and Zens- Ney lexical smoothing in both translation directions (<REF ID="R-09" RPTR="15">Chen et al., 2011</REF>)</S>
        </P>
        <P>
          <S ID="S-52358">&#8226; 6 hierarchical lexicalized reordering scores, which represent the orientation of the current phrase with respect to the previous block that could have been translated as a single phrase (<REF ID="R-21" RPTR="27">Galley and Manning, 2008</REF>)</S>
        </P>
        <P>
          <S ID="S-52359">2 even though it does not include the length penalty used in</S>
        </P>
        <P>
          <S ID="S-52360">the BLEU score.</S>
        </P>
        <P>
          <S ID="S-52361">&#8226; a word penalty, which scores the length of the output sentence</S>
        </P>
        <P>
          <S ID="S-52362">&#8226; a word-displacement distortion penalty</S>
        </P>
        <P>
          <S ID="S-52363">&#8226; a Kneser-Ney smoothed 5-gram Spanish language model</S>
        </P>
        <P>
          <S ID="S-52364">Weights for these features are learned using a batch version of the MIRA algorithm(<REF ID="R-11" RPTR="17">Cherry and Foster, 2012</REF>).</S>
          <S ID="S-52365">Phrase pairs are extracted from IBM4 alignments obtained with GIZA++(<REF ID="R-40" RPTR="50">Och and Ney, 2003</REF>).</S>
          <S ID="S-52366">We learn phrase translation candidates for phrases of length 1 to 7.</S>
          <S ID="S-52367">Converting the PBSMT output for CLWSD requires a final straightforward mapping step.</S>
          <S ID="S-52368">We use the phrasal alignment between SMT input and output to isolate the translation candidates for the CLWSD target word.</S>
          <S ID="S-52369">When it maps to a multiword phrase in the target language, we use the word within the phrase that has the highest translation IBM1 translation probability given the CLWSD target word of interest.</S>
          <S ID="S-52370">Note that there is no need to perform any manual mapping between SMT output and sense inventories as in (<REF ID="R-05" RPTR="8">Carpuat and Wu, 2005</REF>).</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.2 Data</HEADER>
        <P>
          <S ID="S-52371">The core training corpus is the exact same set of sentences from Europarl that were used to learn the sense inventory, in order to ensure that PBSMT knows the same translations as the human annotators who built the gold standard.</S>
          <S ID="S-52372">There are about 900k sentence pairs, since only 1-to1 alignments that exist in all the languages considered in CLWSD were used (<REF ID="R-29" RPTR="40">Lefever and Hoste, 2010</REF>).</S>
        </P>
        <P>
          <S ID="S-52373">We exploit additional corpora from the WMT2012 translation task, using the full Europarl corpus to train language models, and for one experiment the news-commentary parallel corpus (see Section 9.</S>
          <S ID="S-52374">)</S>
        </P>
        <P>
          <S ID="S-52375">These parallel corpora are used to learn the translation, reordering and language models.</S>
          <S ID="S-52376">The loglinear feature weights are learned on a development set of 3000 sentences sampled from the WMT2012 development test sets.</S>
          <S ID="S-52377">They are selected based on their distance to the CLWSD trial and test sentences (<REF ID="R-38" RPTR="48">Moore and Lewis, 2010</REF>).</S>
        </P>
        <P>
          <S ID="S-52378">We tokenize and lemmatize all English and Spanish text using the FreeLing tools (<REF ID="R-41" RPTR="51">Padr&#243; and Stanilovsky, 2012</REF>).</S>
          <S ID="S-52379">We use lemma representations to perform translation, since the CLWSD targets and translations are lemmatized.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 WSD system</HEADER>
      <P>
        <S ID="S-52398"></S>
      </P>
      <DIV DEPTH="1">
        <HEADER>4.1 Model</HEADER>
        <P>
          <S ID="S-52382">We also train a dedicated WSD system for this task in order to perform a controlled comparison with the SMT system.</S>
          <S ID="S-52383">Many WSD systems have been evaluated on the SemEval test bed used here, however, they differ in terms of resources used, training data and preprocessing pipelines.</S>
          <S ID="S-52384">In order to control for these parameters, we build a WSD system trained on the exact same training corpus, preprocessing and word alignment as the SMT system described above.</S>
        </P>
        <P>
          <S ID="S-52385">We cast WSD as a generic ranking problem with linear models.</S>
          <S ID="S-52386">Given a word in context x, translation candidates t are ranked according to the following model: f(x, t) = &#8721; i &#955; i&#966; i (x, t), where &#966; i (x, t) represent binary features that fire when specific clues are observed in a context x.</S>
        </P>
        <P>
          <S ID="S-52387">Context clues are based on standard feature templates in many supervised WSD approaches (<REF ID="R-19" RPTR="25">Florian et al., 2002</REF>; van Gompel, 2010; Lefever et al., 2011):</S>
        </P>
        <P>
          <S ID="S-52388">&#8226; words in a window of 2 words around the disambiguation target.</S>
        </P>
        <P>
          <S ID="S-52389">&#8226; part-of-speech tags in a window of 2 words around the disambiguation target</S>
        </P>
        <P>
          <S ID="S-52390">&#8226; bag-of-word context: all nouns, verbs and adjectives in the context x</S>
        </P>
        <P>
          <S ID="S-52391">At training time, each example (x, t) is assigned a cost based on the translation observed in parallel corpora: f(x, t) = 0 if t = t aligned , f(x, t) = 1 otherwise .</S>
          <S ID="S-52392">Feature weights &#955; i can be learned in many ways.</S>
          <S ID="S-52393">We optimize logistic loss using stochastic gradient descent 3 .</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>4.2 Data</HEADER>
        <P>
          <S ID="S-52394">The training instances for the supervised WSD system are built automatically by (1) extracting all occurrences of English target words in context, and (2) annotating them with their aligned Spanish lemma.</S>
        </P>
        <P>
          <S ID="S-52395">3 we use the optimizer from http://hunch.net/&#732; vw v7.1.2</S>
        </P>
        <P>
          <S ID="S-52396">We obtain a total of 33139 training instances for all targets (an average of 1656 per target, with a minimum of 30 and a maximum of 5414).</S>
          <S ID="S-52397">Note that this process does not require any manual annotation.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 WSD systems can outperform PBSMT</HEADER>
      <P>
        <S ID="S-52399">Table 2 summarizes the main results.</S>
        <S ID="S-52400">PBSMT outperforms the most frequent sense baseline by a wide margin, and interestingly also yields better results than many of the dedicated WSD systems that participated in the SemEval task.</S>
        <S ID="S-52401">However, PBSMT performance does not match that of the most frequent sense oracle (which uses sense frequencies observed in the test set rather than training set).</S>
        <S ID="S-52402">The WSD system trained on the same word-aligned parallel corpus as the PBSMT system achieves the best performance.</S>
        <S ID="S-52403">It also obtains better results than all but the top system in the official results (<REF ID="R-29" RPTR="41">Lefever and Hoste, 2010</REF>).</S>
      </P>
      <P>
        <S ID="S-52404">The results in Table 2 are quite different from those reported by <REF ID="R-05" RPTR="4">Carpuat and Wu (2005)</REF> on a Chinese WSD task.</S>
        <S ID="S-52405">The Chinese-English PBSMT system performed much worse than any of the dedicated WSD systems on that task.</S>
        <S ID="S-52406">While our WSD system outperforms PBSMT on the CLWSD task too, the difference is not as large, and the PBSMT system is competitive when compared to the full set of systems that were evaluated on this task.</S>
        <S ID="S-52407">This confirms that the CLWSD task represents a more fair benchmark for comparing PBSMT with WSD systems.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>6 Impact of PBSMT Context Models</HEADER>
      <P>
        <S ID="S-52408">What is the impact of PBSMT context models on lexical choice accuracy?</S>
        <S ID="S-52409">Table 3 provides an overview of experiments where we vary the context size available to the PBSMT system.</S>
        <S ID="S-52410">The main PB-</S>
      </P>
      <P>
        <S ID="S-52411">SMT system in the top row uses the default settings presented in Section 3.</S>
      </P>
      <P>
        <S ID="S-52412">In the first set of experiments, we evaluate the impact of the source side context on CLWSD performance.</S>
        <S ID="S-52413">Phrasal translations represent the core of PBSMT systems: they capture collocational context in the source language, and they are therefore are less ambiguous than single words (<REF ID="R-25" RPTR="32">Koehn and Knight, 2003</REF>; <REF ID="R-26" RPTR="33">Koehn et al., 2003</REF>).</S>
        <S ID="S-52414">The default PBSMT learns translations for sources phrases of length ranging from 1 to 7 words.</S>
        <S ID="S-52415">Limiting the PBSMT system to translate shorter phrases (Rows l = 1 and l = 3 in Table 3) surprisingly improves CLWSD performance, even though it degrades BLEU score on WMT test sets.</S>
        <S ID="S-52416">The source context captured by longer phrases therefore does not provide the right disambiguating information in this context.</S>
      </P>
      <P>
        <S ID="S-52417">In the second set of experiments, we evaluate the impact of the context size in the target language, by varying the size of the n-gram language model used.</S>
        <S ID="S-52418">The default PBSMT system used a 5-gram language model.</S>
        <S ID="S-52419">Reducing the n-gram order to 3, 2, 1 and increasing it to 7 both degrade performance.</S>
        <S ID="S-52420">Shorter n-grams do not provide enough disambiguating context, while longer n-grams are more sparse and perhaps do not generalize well outside of the training corpus.</S>
      </P>
      <P>
        <S ID="S-52421">Finally, we report a last experiment which uses a bilingual language model to enrich the context representation in PBSMT (<REF ID="R-39" RPTR="49">Niehues et al., 2011</REF>).</S>
        <S ID="S-52422">This language model is estimated on word pairs formed</S>
      </P>
      <P>
        <S ID="S-52423">by target words augmented with their aligned source words.</S>
        <S ID="S-52424">We use a 4-gram model, trained using Good- Turing discounting.</S>
        <S ID="S-52425">This only results in small improvements (&lt; 0.1) over the standard PBSMT system, and remains far below the performance of the dedicated WSD system.</S>
      </P>
      <P>
        <S ID="S-52426">These results show that source phrases are weak representations of context for the purpose of lexical choice.</S>
        <S ID="S-52427">Target n&#8722;gram context is more useful than source phrasal context, which can surprisingly harm lexical choice accuracy.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>7 Impact of PBSMT Reordering Models</HEADER>
      <P>
        <S ID="S-52428">While the phrase-table is the core of PBSMT system, the reordering model used in our system is heavily lexicalized.</S>
        <S ID="S-52429">In this section, we evaluate its impact on CLWSD performance.</S>
        <S ID="S-52430">The standard PB- SMT system uses a hierarchical lexicalized reordering model (<REF ID="R-21" RPTR="28">Galley and Manning, 2008</REF>) in addition to the distance-based distortion limit.</S>
        <S ID="S-52431">Unlike lexicalized reordering(<REF ID="R-27" RPTR="34">Koehn et al., 2007</REF>), which models the orientation of a phrase with respect to the previous phrase, hierarchical reordering models define the orientation of a phrase with respect to the previous block that could have been translated as a single phrase.</S>
        <S ID="S-52432">In Table 4, we show that lexicalized reordering model benefit CLWSD performance, and that the hierarchical model performs slightly better than the non-hierarchical overall.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>8 Impact of phrase translation selection</HEADER>
      <P>
        <S ID="S-52433">In this section, we consider the impact of various methods for selecting phrase translations on the lexical choice performance of PBSMT.</S>
        <S ID="S-52434">First, we investigate the impact of limiting the number of translation candidates considered for</S>
      </P>
      <P>
        <S ID="S-52435">each source phrase in the phrase-table.</S>
        <S ID="S-52436">The main PBSMT system uses t = 50 translation candidates per source phrase.</S>
        <S ID="S-52437">Limiting that number to 20 and increasing it to 100 both have a very small impact on CLWSD.</S>
        <S ID="S-52438">Second, we prune the phrase-table using a statistical significance test to measure (<REF ID="R-24" RPTR="31">Johnson et al., 2007</REF>).</S>
        <S ID="S-52439">This pruning strategy aims to drastically decrease the size of the phrase-table without degrading translation performance by removing noisy phrase pairs.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>9 Impact of training corpus</HEADER>
      <P>
        <S ID="S-52440">Since increasing the amount of training data is a reliable way to improve translation performance, we evaluate the impact of training the PBSMT system on more than the Europarl data used for controlled comparison with WSD.</S>
        <S ID="S-52441">We increase the parallel training corpus with the WMT-12 News Commentary parallel data 4 .</S>
        <S ID="S-52442">This yields an additional training set of roughly160k sentence pairs.</S>
        <S ID="S-52443">We build linear mixture models to combine translation, reordering and language models learned on Europarl and News Commentary corpora (<REF ID="R-20" RPTR="26">Foster and Kuhn, 2007</REF>).</S>
        <S ID="S-52444">As can be seen in Table 6, this approach improves all CLWSD scores except for 1-gram precision.</S>
        <S ID="S-52445">The decrease in 1-gram precision indicates that the addition of the news corpus introduces new translation candidates that differ from those used in the gold inventory.</S>
        <S ID="S-52446">Interestingly, the additional data is not sufficient to match the performance of the WSD system learned on Europarl only (see Table 2).</S>
        <S ID="S-52447">While additional data should be used when available, richer context features are valuable to make the most of existing data.</S>
      </P>
      <P>
        <S ID="S-52448">4 http://www.statmt.org/wmt12/translation-task.html</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>10 Conclusion</HEADER>
      <P>
        <S ID="S-52449">We use a SemEval Cross-Lingual WSD task to evaluate the lexical choice performance of a typical phrase-based SMT system.</S>
        <S ID="S-52450">Unlike conventional WSD task that rely on abstract sense inventories rather than translations, cross-lingual WSD provides a fair setting for comparing SMT with dedicated WSD systems.</S>
        <S ID="S-52451">Unlike conventional evaluations of machine translation quality, the cross-lingual WSD task lets us isolate a specific aspect of translation quality and show how it is affected by different components of the phrase-based SMT system.</S>
        <S ID="S-52452">Unlike in previous evaluations on conventional WSD tasks (<REF ID="R-05" RPTR="9">Carpuat and Wu, 2005</REF>), phrase-based SMT performance is on par with many dedicated WSD systems.</S>
        <S ID="S-52453">However, the phrase-based SMT system does not perform as well as a WSD system trained on the exact same parallel data.</S>
        <S ID="S-52454">Analysis shows that while many SMT components can potentially have an impact on SMT lexical choice, CLWSD accuracy is most affected by the length of source phrases and order of target n-gram language models.</S>
        <S ID="S-52455">Using shorter source phrases actually improves lexical choice accuracy.</S>
        <S ID="S-52456">The official results for the CLWSD task at SemEval 2013 evaluation provide further insights (<REF ID="R-30" RPTR="43">Lefever and Hoste, 2013</REF>): our PBSMT system can achieve top precision as measured using the top prediction as in this paper, but does not perform as well as other submitted systems when taking into account the top 5 predictions (<REF ID="R-08" RPTR="14">Carpuat, 2013</REF>).</S>
        <S ID="S-52457">This suggests that local context models based on source phrases and target n-grams are much weaker representations of context than the simple templates used by WSD systems, and that even strong PBSMT systems can benefit from context models developed for WSD.</S>
        <S ID="S-52458">New learning algorithms (Chiang et al., 2009; <REF ID="R-11" RPTR="18">Cherry and Foster, 2012</REF>, for instance) finally make it possible for PBSMT to reliably learn from many more features than the typical system used here.</S>
        <S ID="S-52459">Evaluations such as the CLWSD task will provide useful tools for analyzing the impact of these features on lexical choice and inform feature design in increasingly large and complex systems.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS/>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>E Agirre</RAUTHOR>
      <REFTITLE>Word Sense Disambiguation: Algorithms and Applications. Text, Speech, and Language Technology Series.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR></RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>Satanjeev Banerjee</RAUTHOR>
      <REFTITLE>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgement.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>Alexandra Birch</RAUTHOR>
      <REFTITLE>Metrics for MT evaluation: Evaluating reordering.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>Chris Callison-Burch</RAUTHOR>
      <REFTITLE>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>Marine Carpuat</RAUTHOR>
      <REFTITLE>Evaluating the Word Sense Disambiguation Performance of Statistical Machine Translation.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>Marine Carpuat</RAUTHOR>
      <REFTITLE>Evaluation of Context-Dependent Phrasal Translation Lexicons for Statistical Machine Translation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>Marine Carpuat</RAUTHOR>
      <REFTITLE>Domain adaptation in machine translation: Final report.</REFTITLE>
      <DATE>2012</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>Marine Carpuat</RAUTHOR>
      <REFTITLE>Nrc: A machine translation approach to cross-lingual word sense disambiguation (SemEval-2013 Task 10).</REFTITLE>
      <DATE>2013</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>Boxing Chen</RAUTHOR>
      <REFTITLE>Unpacking and transforming feature functions: New ways to smooth phrase tables.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>Boxing Chen</RAUTHOR>
      <REFTITLE>Port: a precision-order-recall mt evaluation metric for tuning.</REFTITLE>
      <DATE>2012</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>Colin Cherry</RAUTHOR>
      <REFTITLE>Batch tuning strategies for statistical machine translation.</REFTITLE>
      <DATE>2012</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>David Chiang</RAUTHOR>
      <REFTITLE>11,001 new features for statistical machine translation.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>Timothy Chklovski</RAUTHOR>
      <REFTITLE>The Senseval-3 Multilingual English-Hindi lexical sample task.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="14">
      <RAUTHOR>Michael Denkowski</RAUTHOR>
      <REFTITLE>METEORNEXT and the METEOR paraphrase tables: improved evaluation support for five target languages.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="15">
      <RAUTHOR>Zhendong Dong</RAUTHOR>
      <REFTITLE>Knowledge description: what, how and who?</REFTITLE>
      <DATE>1998</DATE>
    </REFERENCE>
    <REFERENCE ID="16">
      <RAUTHOR>Markus Dreyer</RAUTHOR>
      <REFTITLE>Hyter: Meaning-equivalent semantics for translation evaluation.</REFTITLE>
      <DATE>2012</DATE>
    </REFERENCE>
    <REFERENCE ID="17">
      <RAUTHOR>Philip Edmonds</RAUTHOR>
      <REFTITLE>Senseval-2: Overview.</REFTITLE>
      <DATE>2001</DATE>
    </REFERENCE>
    <REFERENCE ID="18">
      <RAUTHOR></RAUTHOR>
      <REFTITLE>WordNet: An Electronic Lexical Database.</REFTITLE>
      <DATE>1998</DATE>
    </REFERENCE>
    <REFERENCE ID="19">
      <RAUTHOR>Radu Florian</RAUTHOR>
      <REFTITLE>Combining classifiers for word sense disambiguation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="20">
      <RAUTHOR>George Foster</RAUTHOR>
      <REFTITLE>Mixture-model adaptation for SMT.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="21">
      <RAUTHOR>Michel Galley</RAUTHOR>
      <REFTITLE>A simple and effective hierarchical phrase reordering model.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="22">
      <RAUTHOR>Jes&#250;s Gim&#233;nez</RAUTHOR>
      <REFTITLE>Linguistic Features for Automatic Evaluation of Heterogenous MT Systems.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="23">
      <RAUTHOR>Peng Jin</RAUTHOR>
      <REFTITLE>Semeval2007 task 05: Multilingual chinese-english lexical sample.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="24">
      <RAUTHOR>Howard Johnson</RAUTHOR>
      <REFTITLE>Improving Translation Quality by Discarding Most of the Phrasetable.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="25">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Feature-Rich Statistical Translation of Noun Phrases.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="26">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Statistical Phrase-based Translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="27">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Chris Dyer, Ondrej Bojar,</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="28">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Europarl: A parallel corpus for statistical machine translation.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="29">
      <RAUTHOR>Lefever</RAUTHOR>
      <REFTITLE>Semeval-2010 task 3: Cross-lingual word sense disambiguation.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="30">
      <RAUTHOR>Els Lefever</RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2013</DATE>
    </REFERENCE>
    <REFERENCE ID="31">
      <RAUTHOR></RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE></DATE>
    </REFERENCE>
    <REFERENCE ID="32">
      <RAUTHOR>Els Lefever</RAUTHOR>
      <REFTITLE>Parasense or how to use parallel corpora for word sense disambiguation.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="33">
      <RAUTHOR>Chi-kiu Lo</RAUTHOR>
      <REFTITLE>Meant: an inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility via semantic frames.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="34">
      <RAUTHOR>Chi-kiu Lo</RAUTHOR>
      <REFTITLE>Fully automatic semantic MT evaluation.</REFTITLE>
      <DATE>2012</DATE>
    </REFERENCE>
    <REFERENCE ID="35">
      <RAUTHOR></RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="36">
      <RAUTHOR>Rada Mihalcea</RAUTHOR>
      <REFTITLE>The Senseval-3 English lexical sample task.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="37">
      <RAUTHOR>Rada Mihalcea</RAUTHOR>
      <REFTITLE>SemEval-2010 Task 2: Cross-Lingual Lexical Substitution.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="38">
      <RAUTHOR>Robert C Moore</RAUTHOR>
      <REFTITLE>Intelligent selection of language model training data.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="39">
      <RAUTHOR>Jan Niehues</RAUTHOR>
      <REFTITLE>Wider context by using bilingual language models in machine translation.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="40">
      <RAUTHOR>Franz Josef Och</RAUTHOR>
      <REFTITLE>A Systematic Comparison of Various Statistical Alignment Models.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="41">
      <RAUTHOR>Llu&#237;s Padr&#243;</RAUTHOR>
      <REFTITLE>FreeLing 3.0: Towards wider multilinguality.</REFTITLE>
      <DATE>2012</DATE>
    </REFERENCE>
    <REFERENCE ID="42">
      <RAUTHOR>Martha Palmer</RAUTHOR>
      <REFTITLE>English tasks: All-words and verb lexical sample.</REFTITLE>
      <DATE>2001</DATE>
    </REFERENCE>
    <REFERENCE ID="43">
      <RAUTHOR>Kishore Papineni</RAUTHOR>
      <REFTITLE>BLEU: a method for automatic evaluation of machine translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="44">
      <RAUTHOR>Maarten van Gompel</RAUTHOR>
      <REFTITLE>Uvt-wsd1: A cross-lingual word sense disambiguation system.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="45">
      <RAUTHOR>David Vickrey</RAUTHOR>
      <REFTITLE>Word-Sense Disambiguation for Machine Translation.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="46">
      <RAUTHOR>David Vilar</RAUTHOR>
      <REFTITLE>Error Analysis of Machine Translation Output.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
