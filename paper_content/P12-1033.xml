<document>
  <filename>P12-1033</filename>
  <authors/>
  <title>Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l 0 -norm</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>Two decades after their invention, the IBM word-based translation models, widely available in the GIZA++ toolkit, remain the dominant approach to word alignment and an integral part of many statistical translation systems. Although many models have surpassed them in accuracy, none have supplanted them in practice. In this paper, we propose a simple extension to the IBM models: an l 0 prior to encourage sparsity in the word-to-word translation model. We explain how to implement this extension efficiently for large-scale data (also released as a modification to GIZA++) and demonstrate, in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 Bleu).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Two decades after their invention, the IBM word-based translation models, widely available in the GIZA++ toolkit, remain the dominant approach to word alignment and an integral part of many statistical translation systems.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Although many models have surpassed them in accuracy, none have supplanted them in practice.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, we propose a simple extension to the IBM models: an l 0 prior to encourage sparsity in the word-to-word translation model.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We explain how to implement this extension efficiently for large-scale data (also released as a modification to GIZA++) and demonstrate, in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 Bleu).</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Automatic word alignment is a vital component of nearly all current statistical translation pipelines. Although state-of-the-art translation models use rules that operate on units bigger than words (like phrases or tree fragments), they nearly always use word alignments to drive extraction of those translation rules. The dominant approach to word alignment has been the IBM models (Brown et al., 1993) together with the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems.
In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Gra&#231;a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has.
In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporating an l 0 prior, inspired by the principle of minimum description length (Barron et al., 1998), to encourage sparsity in the word-to-word translation model (Section 2.2). This extension follows our previous work on unsupervised part-ofspeech tagging (Vaswani et al., 2010), but enables it to scale to the large datasets typical in word alignment, using an efficient training method based on projected gradient descent (Section 2.3). Experiments on Czech-, Arabic-, Chinese- and Urdu- English translation (Section 3) demonstrate consistent significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 Bleu). Our implementation has been released as a simple modification to the GIZA++ toolkit that can be used as a drop-in replacement for GIZA++ in any existing MT pipeline.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Automatic word alignment is a vital component of nearly all current statistical translation pipelines.</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Although state-of-the-art translation models use rules that operate on units bigger than words (like phrases or tree fragments), they nearly always use word alignments to drive extraction of those translation rules.</text>
              <doc_id>5</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The dominant approach to word alignment has been the IBM models (Brown et al., 1993) together with the HMM model (Vogel et al., 1996).</text>
              <doc_id>6</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>These models are unsupervised, making them applicable to any language pair for which parallel text is available.</text>
              <doc_id>7</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004).</text>
              <doc_id>8</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>These properties make them the default choice for most statistical MT systems.</text>
              <doc_id>9</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice.</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010).</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Although manually-aligned data is very valuable, it is only available for a small number of language pairs.</text>
              <doc_id>12</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Other models are unsupervised like the IBM models (Liang et al., 2006; Gra&#231;a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has.</text>
              <doc_id>13</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality.</text>
              <doc_id>14</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It extends the IBM/HMM models by incorporating an l 0 prior, inspired by the principle of minimum description length (Barron et al., 1998), to encourage sparsity in the word-to-word translation model (Section 2.2).</text>
              <doc_id>15</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This extension follows our previous work on unsupervised part-ofspeech tagging (Vaswani et al., 2010), but enables it to scale to the large datasets typical in word alignment, using an efficient training method based on projected gradient descent (Section 2.3).</text>
              <doc_id>16</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Experiments on Czech-, Arabic-, Chinese- and Urdu- English translation (Section 3) demonstrate consistent significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 Bleu).</text>
              <doc_id>17</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Our implementation has been released as a simple modification to the GIZA++ toolkit that can be used as a drop-in replacement for GIZA++ in any existing MT pipeline.</text>
              <doc_id>18</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Method</title>
        <text>We start with a brief review of the IBM and HMM word alignment models, then describe how to extend them with a smoothed l 0 prior and how to efficiently train them.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We start with a brief review of the IBM and HMM word alignment models, then describe how to extend them with a smoothed l 0 prior and how to efficiently train them.</text>
              <doc_id>19</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 IBM Models and HMM</title>
            <text>Given a French string f = f 1 &#183; &#183; &#183; f j &#183; &#183; &#183; f m and an English string e = e 1 &#183; &#183; &#183; e i &#183; &#183; &#183; e l , these models describe the process by which the French string is generated by the English string via the alignment a = a 1 , . . . , a j , . . . , a m . Each a j is a hidden variables, indicating which English word e a j the French word f j is aligned to.
In IBM Model 1&#8211;2 and the HMM model, the joint probability of the French sentence and alignment given the English sentence is
P(f, a | e) = m&#8719;
d(a j | a j&#8722;1 , j)t( f j | e a j ). (1)
j=1
The parameters of these models are the distortion probabilities d(a j | a j&#8722;1 , j) and the translation probabilities t( f j | e a j ). The three models differ in their estimation of d, but the differences do not concern us here. All three models, as well as IBM Models 3&#8211;5, share the same t. For further details of these models, the reader is referred to the original papers describing them (Brown et al., 1993; Vogel et al., 1996).
Let &#952; stand for all the parameters of the model. The standard training procedure is to find the parameter values that maximize the likelihood, or, equivalently, minimize the negative log-likelihood of the observed data:
This is done using the Expectation-Maximization (EM) algorithm (Dempster et al., 1977).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Given a French string f = f 1 &#183; &#183; &#183; f j &#183; &#183; &#183; f m and an English string e = e 1 &#183; &#183; &#183; e i &#183; &#183; &#183; e l , these models describe the process by which the French string is generated by the English string via the alignment a = a 1 , .</text>
                  <doc_id>20</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>21</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>22</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>, a j , .</text>
                  <doc_id>23</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>24</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>25</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>, a m .</text>
                  <doc_id>26</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Each a j is a hidden variables, indicating which English word e a j the French word f j is aligned to.</text>
                  <doc_id>27</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In IBM Model 1&#8211;2 and the HMM model, the joint probability of the French sentence and alignment given the English sentence is</text>
                  <doc_id>28</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P(f, a | e) = m&#8719;</text>
                  <doc_id>29</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>d(a j | a j&#8722;1 , j)t( f j | e a j ).</text>
                  <doc_id>30</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(1)</text>
                  <doc_id>31</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=1</text>
                  <doc_id>32</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The parameters of these models are the distortion probabilities d(a j | a j&#8722;1 , j) and the translation probabilities t( f j | e a j ).</text>
                  <doc_id>33</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The three models differ in their estimation of d, but the differences do not concern us here.</text>
                  <doc_id>34</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>All three models, as well as IBM Models 3&#8211;5, share the same t.</text>
                  <doc_id>35</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For further details of these models, the reader is referred to the original papers describing them (Brown et al., 1993; Vogel et al., 1996).</text>
                  <doc_id>36</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Let &#952; stand for all the parameters of the model.</text>
                  <doc_id>37</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The standard training procedure is to find the parameter values that maximize the likelihood, or, equivalently, minimize the negative log-likelihood of the observed data:</text>
                  <doc_id>38</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>This is done using the Expectation-Maximization (EM) algorithm (Dempster et al., 1977).</text>
                  <doc_id>39</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 MAP-EM with the l 0 -norm</title>
            <text>Maximum likelihood training is prone to overfitting, especially in models with many parameters. In word alignment, one well-known manifestation of overfitting is that rare words can act as &#8220;garbage collectors&#8221;
a
(Moore, 2004), aligning to many unrelated words. This hurts alignment precision and rule-extraction recall. Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Gra&#231;a et al., 2010).
We have previously proposed another simple remedy to overfitting in the context of unsupervised part-of-speech tagging (Vaswani et al., 2010), which is to minimize the size of the model using a smoothed l 0 prior. Applying this prior to an HMM improves tagging accuracy for both Italian and English.
Here, our goal is to apply a similar prior in a word-alignment model to the word-to-word translation probabilities t( f | e). We leave the distortion models alone, since they are not very large, and there is not much reason to believe that we can profit from compacting them.
With the addition of the l 0 prior, the MAP (maximum a posteriori) objective function is
where
and
( ) &#710;&#952; = arg min &#8722; log P(f | e, &#952;)P(&#952;)
&#952;
P(&#952;) &#8733; exp ( &#8722;&#945;&#8214;&#952;&#8214; &#946; 0
&#8214;&#952;&#8214; &#946; 0 = &#8721;
e, f
)
( 1 &#8722; exp ) &#8722;t( f | e)
&#946;
(4)
(5)
(6)
is a smoothed approximation of the l 0 -norm. The hyperparameter &#946; controls the tightness of the approximation, as illustrated in Figure 1. Substituting back into (4) and dropping constant terms, we get the following optimization problem: minimize
&#8721; &#8722; log P(f | e, &#952;) &#8722; &#945; exp
e, f
&#8722;t( f | e) &#946; (7)
f
We can carry out the optimization in (7) with the MAP-EM algorithm (Bishop, 2006). EM and MAP- EM share the same E-step; the difference lies in the
0.8
0.6
0.4
0.2
0
0 0.2 0.4 0.6 0.8 1
(10); we seek to minimize this function. As in previous work (Vaswani et al., 2010), we optimize each set of parameters {t(&#183; | e)} separately for each English word type e. The inputs to the PGD are the expected counts E[C(e, f )] and the current word-toword conditional probabilities &#952;. We run PGD for K iterations, producing a sequence of intermediate parameter vectors &#952; 1 , . . . , &#952; k , . . . , &#952; K . Each iteration has two steps, a projection step and a line search.
Projection step In this step, we compute:
&#952; k = [ &#952; k &#8722; s&#8711;F(&#952; k ) ] &#8710; (11)
M-step. For vanilla EM, the M-step is: &#9115; &#9118;
&#8721; &#710;&#952; = arg min &#9116;&#9117; &#8722; E[C(e, f )] log t( f | e) &#9119;&#9120;
&#952; e, f
(9)
again subject to the constraints (8). The count C(e, f ) is the number of times that f occurs aligned to e. For MAP-EM, it is:
&#710;&#952; = arg min
&#952;
( &#8721; &#8722; E[C(e, f )] log t( f | e) &#8722;
e, f
&#8721; &#945; exp
e, f
) (10) &#8722;t( f | e)
&#946;
This optimization problem is non-convex, and we do not know of a closed-form solution. Previously (Vaswani et al., 2010), we used ALGENCAN, a nonlinear optimization toolkit, but this solution does not scale well to the number of parameters involved in word alignment models. Instead, we use a simpler and more scalable method which we describe in the next section.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Maximum likelihood training is prone to overfitting, especially in models with many parameters.</text>
                  <doc_id>40</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In word alignment, one well-known manifestation of overfitting is that rare words can act as &#8220;garbage collectors&#8221;</text>
                  <doc_id>41</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>a</text>
                  <doc_id>42</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(Moore, 2004), aligning to many unrelated words.</text>
                  <doc_id>43</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This hurts alignment precision and rule-extraction recall.</text>
                  <doc_id>44</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Gra&#231;a et al., 2010).</text>
                  <doc_id>45</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We have previously proposed another simple remedy to overfitting in the context of unsupervised part-of-speech tagging (Vaswani et al., 2010), which is to minimize the size of the model using a smoothed l 0 prior.</text>
                  <doc_id>46</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Applying this prior to an HMM improves tagging accuracy for both Italian and English.</text>
                  <doc_id>47</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Here, our goal is to apply a similar prior in a word-alignment model to the word-to-word translation probabilities t( f | e).</text>
                  <doc_id>48</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We leave the distortion models alone, since they are not very large, and there is not much reason to believe that we can profit from compacting them.</text>
                  <doc_id>49</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>With the addition of the l 0 prior, the MAP (maximum a posteriori) objective function is</text>
                  <doc_id>50</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where</text>
                  <doc_id>51</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>( ) &#710;&#952; = arg min &#8722; log P(f | e, &#952;)P(&#952;)</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#952;</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P(&#952;) &#8733; exp ( &#8722;&#945;&#8214;&#952;&#8214; &#946; 0</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8214;&#952;&#8214; &#946; 0 = &#8721;</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e, f</text>
                  <doc_id>57</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>)</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>( 1 &#8722; exp ) &#8722;t( f | e)</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#946;</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(4)</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(5)</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(6)</text>
                  <doc_id>63</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>is a smoothed approximation of the l 0 -norm.</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The hyperparameter &#946; controls the tightness of the approximation, as illustrated in Figure 1.</text>
                  <doc_id>65</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Substituting back into (4) and dropping constant terms, we get the following optimization problem: minimize</text>
                  <doc_id>66</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; &#8722; log P(f | e, &#952;) &#8722; &#945; exp</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e, f</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8722;t( f | e) &#946; (7)</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We can carry out the optimization in (7) with the MAP-EM algorithm (Bishop, 2006).</text>
                  <doc_id>71</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>EM and MAP- EM share the same E-step; the difference lies in the</text>
                  <doc_id>72</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.8</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.6</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.4</text>
                  <doc_id>75</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.2</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0 0.2 0.4 0.6 0.8 1</text>
                  <doc_id>78</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(10); we seek to minimize this function.</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As in previous work (Vaswani et al., 2010), we optimize each set of parameters {t(&#183; | e)} separately for each English word type e. The inputs to the PGD are the expected counts E[C(e, f )] and the current word-toword conditional probabilities &#952;.</text>
                  <doc_id>80</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We run PGD for K iterations, producing a sequence of intermediate parameter vectors &#952; 1 , .</text>
                  <doc_id>81</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>82</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>83</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>, &#952; k , .</text>
                  <doc_id>84</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>85</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>86</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>, &#952; K .</text>
                  <doc_id>87</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>Each iteration has two steps, a projection step and a line search.</text>
                  <doc_id>88</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Projection step In this step, we compute:</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#952; k = [ &#952; k &#8722; s&#8711;F(&#952; k ) ] &#8710; (11)</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>M-step.</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For vanilla EM, the M-step is: &#9115; &#9118;</text>
                  <doc_id>92</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; &#710;&#952; = arg min &#9116;&#9117; &#8722; E[C(e, f )] log t( f | e) &#9119;&#9120;</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#952; e, f</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(9)</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>again subject to the constraints (8).</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The count C(e, f ) is the number of times that f occurs aligned to e. For MAP-EM, it is:</text>
                  <doc_id>97</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#710;&#952; = arg min</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#952;</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>( &#8721; &#8722; E[C(e, f )] log t( f | e) &#8722;</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e, f</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; &#945; exp</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e, f</text>
                  <doc_id>103</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>) (10) &#8722;t( f | e)</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#946;</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>This optimization problem is non-convex, and we do not know of a closed-form solution.</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Previously (Vaswani et al., 2010), we used ALGENCAN, a nonlinear optimization toolkit, but this solution does not scale well to the number of parameters involved in word alignment models.</text>
                  <doc_id>107</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Instead, we use a simpler and more scalable method which we describe in the next section.</text>
                  <doc_id>108</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>2.3 Projected gradient descent</title>
            <text>Following Schoenemann (2011b), we use projected gradient descent (PGD) to solve the M-step (but with the l 0 -norm instead of the l 1 -norm). Gradient projection methods are attractive solutions to constrained optimization problems, particularly when the constraints on the parameters are simple (Bertsekas, 1999). Let F(&#952;) be the objective function in This moves &#952; in the direction of steepest descent (&#8711;F) with step size s, and then the function [&#183;] &#8710; projects the resulting point onto the simplex; that is, it finds the nearest point that satisfies the constraints (8).
The gradient &#8711;F(&#952; k ) is
&#8706;F E[C( f, e)] = &#8722; + &#945; &#8722;t( f | e) exp &#8706;t( f | e) t( f | e) &#946; &#946; (12)
In contrast to Schoenemann (2011b), we use an O(n log n) algorithm for the projection step due to Duchi et. al. (2008), shown in Pseudocode 1.
Pseudocode 1 Project input vector u &#8712; R n onto the probability simplex.
v = u sorted in non-increasing order &#961; = 0 for i = 1 to n do (&#8721;
if v i &#8722; 1 ir=1
i
v r &#8722; 1 ) &gt; 0 then
&#961; = i
end if end for (&#8721; &#951; = 1 &#961;
&#961; r=1 v r &#8722; 1 )
w r = max{v r &#8722; &#951;, 0} for 1 &#8804; r &#8804; n return w
Line search Next, we move to a point between &#952; k and &#952; k that satisfies the Armijo condition,
F(&#952; k + &#948; m ) &#8804; F(&#952; k ) + &#963; ( &#8711;F(&#952; k ) &#183; &#948; m )
(13)
where &#948; m = &#947; m (&#952; k &#8722; &#952; k ) and &#963; and &#947; are both constants in (0, 1). We try values m = 1, 2, . . . until the Armijo condition (13) is satisfied or the limit m = 20
Pseudocode 2 Find a point between &#952; k and &#952; k that satisfies the Armijo condition.
F min = F(&#952; k ) &#952; min = &#952; k for m = 1 to( 20 do) &#948; m = &#947; m &#952; k &#8722; &#952; k
if F(&#952; k + &#948; m ) &lt; F min then F min = F(&#952; k + &#948; m ) &#952; min = &#952; k + &#948; m
end if if F(&#952; k + &#948; m ) &#8804; F(&#952; k ) + &#963; ( &#8711;F(&#952; k ) &#183; &#948; m )
then
break
end if end for &#952; k+1 = &#952; min return &#952; k+1
is reached. (Note that we don&#8217;t allow m = 0 because this can cause &#952; k + &#948; m to land on the boundary of the probability simplex, where the objective function is undefined.) Then we set &#952; k+1 to the point in {&#952; k } &#8746; {&#952; k + &#948; m | 1 &#8804; m &#8804; 20} that minimizes F. The line search algorithm is summarized in Pseudocode 2.
In our implementation, we set &#947; = 0.5 and &#963; = 0.5. We keep s fixed for all PGD iterations; we experimented with s &#8712; {0.1, 0.5} and did not observe significant changes in F-score. We run the projection step and line search alternately for at most K iterations, terminating early if there is no change in &#952; k from one iteration to the next. We set K = 35 for the large Arabic-English experiment; for all other conditions, we set K = 50. These choices were made to balance efficiency and accuracy. We found that values of K between 30 and 75 were generally reasonable.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Following Schoenemann (2011b), we use projected gradient descent (PGD) to solve the M-step (but with the l 0 -norm instead of the l 1 -norm).</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Gradient projection methods are attractive solutions to constrained optimization problems, particularly when the constraints on the parameters are simple (Bertsekas, 1999).</text>
                  <doc_id>110</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Let F(&#952;) be the objective function in This moves &#952; in the direction of steepest descent (&#8711;F) with step size s, and then the function [&#183;] &#8710; projects the resulting point onto the simplex; that is, it finds the nearest point that satisfies the constraints (8).</text>
                  <doc_id>111</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The gradient &#8711;F(&#952; k ) is</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8706;F E[C( f, e)] = &#8722; + &#945; &#8722;t( f | e) exp &#8706;t( f | e) t( f | e) &#946; &#946; (12)</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In contrast to Schoenemann (2011b), we use an O(n log n) algorithm for the projection step due to Duchi et.</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>al. (2008), shown in Pseudocode 1.</text>
                  <doc_id>115</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Pseudocode 1 Project input vector u &#8712; R n onto the probability simplex.</text>
                  <doc_id>116</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>v = u sorted in non-increasing order &#961; = 0 for i = 1 to n do (&#8721;</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>if v i &#8722; 1 ir=1</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i</text>
                  <doc_id>119</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>v r &#8722; 1 ) &gt; 0 then</text>
                  <doc_id>120</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#961; = i</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>end if end for (&#8721; &#951; = 1 &#961;</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#961; r=1 v r &#8722; 1 )</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w r = max{v r &#8722; &#951;, 0} for 1 &#8804; r &#8804; n return w</text>
                  <doc_id>124</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Line search Next, we move to a point between &#952; k and &#952; k that satisfies the Armijo condition,</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>F(&#952; k + &#948; m ) &#8804; F(&#952; k ) + &#963; ( &#8711;F(&#952; k ) &#183; &#948; m )</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(13)</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where &#948; m = &#947; m (&#952; k &#8722; &#952; k ) and &#963; and &#947; are both constants in (0, 1).</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We try values m = 1, 2, .</text>
                  <doc_id>129</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>130</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>131</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>until the Armijo condition (13) is satisfied or the limit m = 20</text>
                  <doc_id>132</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Pseudocode 2 Find a point between &#952; k and &#952; k that satisfies the Armijo condition.</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>F min = F(&#952; k ) &#952; min = &#952; k for m = 1 to( 20 do) &#948; m = &#947; m &#952; k &#8722; &#952; k</text>
                  <doc_id>134</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>if F(&#952; k + &#948; m ) &lt; F min then F min = F(&#952; k + &#948; m ) &#952; min = &#952; k + &#948; m</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>end if if F(&#952; k + &#948; m ) &#8804; F(&#952; k ) + &#963; ( &#8711;F(&#952; k ) &#183; &#948; m )</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>then</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>break</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>end if end for &#952; k+1 = &#952; min return &#952; k+1</text>
                  <doc_id>139</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>is reached.</text>
                  <doc_id>140</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(Note that we don&#8217;t allow m = 0 because this can cause &#952; k + &#948; m to land on the boundary of the probability simplex, where the objective function is undefined.</text>
                  <doc_id>141</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>) Then we set &#952; k+1 to the point in {&#952; k } &#8746; {&#952; k + &#948; m | 1 &#8804; m &#8804; 20} that minimizes F. The line search algorithm is summarized in Pseudocode 2.</text>
                  <doc_id>142</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In our implementation, we set &#947; = 0.5 and &#963; = 0.5.</text>
                  <doc_id>143</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We keep s fixed for all PGD iterations; we experimented with s &#8712; {0.1, 0.5} and did not observe significant changes in F-score.</text>
                  <doc_id>144</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We run the projection step and line search alternately for at most K iterations, terminating early if there is no change in &#952; k from one iteration to the next.</text>
                  <doc_id>145</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We set K = 35 for the large Arabic-English experiment; for all other conditions, we set K = 50.</text>
                  <doc_id>146</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>These choices were made to balance efficiency and accuracy.</text>
                  <doc_id>147</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We found that values of K between 30 and 75 were generally reasonable.</text>
                  <doc_id>148</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Experiments</title>
        <text>To demonstrate the effect of the l 0 -norm on the IBM models, we performed experiments on four translation tasks: Arabic-English, Chinese-English, and Urdu-English from the NIST Open MT Evaluation, and the Czech-English translation from the Workshop on Machine Translation (WMT) shared task. We measured the accuracy of word alignments generated by GIZA++ with and without the l 0 -norm, and also translation accuracy of systems trained using the word alignments. Across all tests, we found strong improvements from adding the l 0 -norm.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>To demonstrate the effect of the l 0 -norm on the IBM models, we performed experiments on four translation tasks: Arabic-English, Chinese-English, and Urdu-English from the NIST Open MT Evaluation, and the Czech-English translation from the Workshop on Machine Translation (WMT) shared task.</text>
              <doc_id>149</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We measured the accuracy of word alignments generated by GIZA++ with and without the l 0 -norm, and also translation accuracy of systems trained using the word alignments.</text>
              <doc_id>150</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Across all tests, we found strong improvements from adding the l 0 -norm.</text>
              <doc_id>151</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Training</title>
            <text>We have implemented our algorithm as an opensource extension to GIZA++. 1 Usage of the extension is identical to standard GIZA++, except that the user can switch the l 0 prior on or off, and adjust the hyperparameters &#945; and &#946;.
For vanilla EM, we ran five iterations of Model 1, five iterations of HMM, and ten iterations of Model 4. For our approach, we first ran one iteration of Model 1, followed by four iterations of Model 1 with smoothed l 0 , followed by five iterations of HMM with smoothed l 0 . Finally, we ran ten iterations of Model 4. 2
We used the following parallel data:
&#8226; Chinese-English: selected data from the constrained task of the NIST 2009 Open MT Evaluation. 3
&#8226; Arabic-English: all available data for the constrained track of NIST 2009, excluding United Nations proceedings (LDC2004E13), ISI Automatically Extracted Parallel Text (LDC2007E08), and Ummah newswire text (LDC2004T18), for a total of 5.4+4.3 million words. We also experimented on a larger Arabic-English parallel text of 44+37 million words from the DARPA GALE program.
&#8226; Urdu-English: all available data for the constrained track of NIST 2009.
1 The code can be downloaded from the first author&#8217;s website
at http://www.isi.edu/&#732;avaswani/giza-pp-l0.html. 2 GIZA++ allows changing some heuristic parameters for
efficient training. Currently, we set two of these to zero: mincountincrease and probcutoff. In the default setting, both are set to 10 &#8722;7 . We set probcutoff to 0 because we would like the optimization to learn the parameter values. For a fair comparison, we applied the same setting to our vanilla EM training as well. To test, we ran GIZA++ with the default setting on the smaller of our two Arabic-English datasets with the same number of iterations and found no change in F-score. 3 LDC catalog numbers LDC2003E07, LDC2003E14,
LDC2005E83, LDC2005T06, LDC2006E24, LDC2006E34, LDC2006E85, LDC2006E86, LDC2006E92, and LDC2006E93.
president of the u
foreign affairs u u
u institute shuqin liu was also
u u u at the meeting .
present
over 4000
u
u u u
from guests
home and abroad attended the opening ceremony .
it &#8217;s extremely troublesome to get
u via land .
there
r&#250;gu&#466;
u y&#224;o u u l&#249;l&#249; zhu&#462;n
u u q&#249; dehu&#224; ne
u , u h&#283;n u h&#283;n u h&#283;n u h&#283;n
u m&#225;fan
de
u , (c)
u
this was after u care of , four taken
u
blockhouses were blown up . zh&#232;ge ch&#249;l&#464; w&#225;n y&#464;h&#242;u ne
u , h&#225;i
u u
(d)
u zh&#224; u le
s&#236;ge di&#257;ob&#462;o
u .
&#8226; Czech-English: A corpus of 4 million words of Czech-English data from the News Commentary corpus. 4
We set the hyperparameters &#945; and &#946; by tuning on gold-standard word alignments (to maximize F1) when possible. For Arabic-English and Chinese- English, we used 346 and 184 hand-aligned sentences from LDC2006E86 and LDC2006E93. Similarly, for Czech-English, 515 hand-aligned sentences were available (Bojar and Prokopov&#225;, 2006). But for Urdu-English, since we did not have any gold alignments, we used &#945; = 10 and &#946; = 0.05. We did not choose a large &#945;, as the dataset was small, and we chose a conservative value for &#946;. We ran word alignment in both directions and symmetrized using grow-diag-final (Koehn et al., 2003). For models with the smoothed l 0 prior, we tuned &#945; and &#946; separately in each direction.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We have implemented our algorithm as an opensource extension to GIZA++.</text>
                  <doc_id>152</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>1 Usage of the extension is identical to standard GIZA++, except that the user can switch the l 0 prior on or off, and adjust the hyperparameters &#945; and &#946;.</text>
                  <doc_id>153</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For vanilla EM, we ran five iterations of Model 1, five iterations of HMM, and ten iterations of Model 4.</text>
                  <doc_id>154</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For our approach, we first ran one iteration of Model 1, followed by four iterations of Model 1 with smoothed l 0 , followed by five iterations of HMM with smoothed l 0 .</text>
                  <doc_id>155</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, we ran ten iterations of Model 4.</text>
                  <doc_id>156</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>2</text>
                  <doc_id>157</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We used the following parallel data:</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Chinese-English: selected data from the constrained task of the NIST 2009 Open MT Evaluation.</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>3</text>
                  <doc_id>160</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Arabic-English: all available data for the constrained track of NIST 2009, excluding United Nations proceedings (LDC2004E13), ISI Automatically Extracted Parallel Text (LDC2007E08), and Ummah newswire text (LDC2004T18), for a total of 5.4+4.3 million words.</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We also experimented on a larger Arabic-English parallel text of 44+37 million words from the DARPA GALE program.</text>
                  <doc_id>162</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Urdu-English: all available data for the constrained track of NIST 2009.</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 The code can be downloaded from the first author&#8217;s website</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>at http://www.isi.edu/&#732;avaswani/giza-pp-l0.html.</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>2 GIZA++ allows changing some heuristic parameters for</text>
                  <doc_id>166</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>efficient training.</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Currently, we set two of these to zero: mincountincrease and probcutoff.</text>
                  <doc_id>168</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In the default setting, both are set to 10 &#8722;7 .</text>
                  <doc_id>169</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We set probcutoff to 0 because we would like the optimization to learn the parameter values.</text>
                  <doc_id>170</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For a fair comparison, we applied the same setting to our vanilla EM training as well.</text>
                  <doc_id>171</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>To test, we ran GIZA++ with the default setting on the smaller of our two Arabic-English datasets with the same number of iterations and found no change in F-score.</text>
                  <doc_id>172</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>3 LDC catalog numbers LDC2003E07, LDC2003E14,</text>
                  <doc_id>173</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>LDC2005E83, LDC2005T06, LDC2006E24, LDC2006E34, LDC2006E85, LDC2006E86, LDC2006E92, and LDC2006E93.</text>
                  <doc_id>174</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>president of the u</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>foreign affairs u u</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u institute shuqin liu was also</text>
                  <doc_id>177</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u u u at the meeting .</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>present</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>over 4000</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u</text>
                  <doc_id>181</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u u u</text>
                  <doc_id>182</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>from guests</text>
                  <doc_id>183</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>home and abroad attended the opening ceremony .</text>
                  <doc_id>184</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>it &#8217;s extremely troublesome to get</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u via land .</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>there</text>
                  <doc_id>187</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r&#250;gu&#466;</text>
                  <doc_id>188</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u y&#224;o u u l&#249;l&#249; zhu&#462;n</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u u q&#249; dehu&#224; ne</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u , u h&#283;n u h&#283;n u h&#283;n u h&#283;n</text>
                  <doc_id>191</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u m&#225;fan</text>
                  <doc_id>192</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>de</text>
                  <doc_id>193</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u , (c)</text>
                  <doc_id>194</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>this was after u care of , four taken</text>
                  <doc_id>196</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>blockhouses were blown up .</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>zh&#232;ge ch&#249;l&#464; w&#225;n y&#464;h&#242;u ne</text>
                  <doc_id>199</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u , h&#225;i</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u u</text>
                  <doc_id>201</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(d)</text>
                  <doc_id>202</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u zh&#224; u le</text>
                  <doc_id>203</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>s&#236;ge di&#257;ob&#462;o</text>
                  <doc_id>204</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u .</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Czech-English: A corpus of 4 million words of Czech-English data from the News Commentary corpus.</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>4</text>
                  <doc_id>207</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We set the hyperparameters &#945; and &#946; by tuning on gold-standard word alignments (to maximize F1) when possible.</text>
                  <doc_id>208</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For Arabic-English and Chinese- English, we used 346 and 184 hand-aligned sentences from LDC2006E86 and LDC2006E93.</text>
                  <doc_id>209</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Similarly, for Czech-English, 515 hand-aligned sentences were available (Bojar and Prokopov&#225;, 2006).</text>
                  <doc_id>210</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>But for Urdu-English, since we did not have any gold alignments, we used &#945; = 10 and &#946; = 0.05.</text>
                  <doc_id>211</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We did not choose a large &#945;, as the dataset was small, and we chose a conservative value for &#946;.</text>
                  <doc_id>212</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We ran word alignment in both directions and symmetrized using grow-diag-final (Koehn et al., 2003).</text>
                  <doc_id>213</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>For models with the smoothed l 0 prior, we tuned &#945; and &#946; separately in each direction.</text>
                  <doc_id>214</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Alignment</title>
            <text>First, we evaluated alignment accuracy directly by comparing against gold-standard word alignments.
4 This data is available at http://statmt.org/wmt10.
The results are shown in the alignment F1 column of Table 1. We used balanced F-measure rather than alignment error rate as our metric (Fraser and Marcu, 2007).
Following Dyer et al. (2011), we also measured the average fertility, &#732;&#966; sing. , of once-seen source words in the symmetrized alignments. Our alignments show smaller fertility for once-seen words, suggesting that they suffer from &#8220;garbage collection&#8221; effects less than the baseline alignments do.
The fact that we had to use hand-aligned data to tune the hyperparameters &#945; and &#946; means that our method is no longer completely unsupervised. However, our observation is that alignment accuracy is actually fairly robust to the choice of these hyperparameters, as shown in Table 2. As we will see below, we still obtained strong improvements in translation quality when hand-aligned data was unavailable. We also tried generating 50 word classes using the tool provided in GIZA++. We found that adding word classes improved alignment quality a little, but more so for the baseline system (see Table 3). We used the alignments generated by training with word classes for our translation experiments.
&#946;
&#8211;
0.5
0.1
0.05
0.01
0.005
0.001
Figure 2 shows four examples of Chinese- English alignment, comparing the baseline with our smoothed-l 0 method. In all four cases, the baseline produces incorrect extra alignments that prevent good translation rules from being extracted while the smoothed-l 0 results are correct. In particular, the baseline system demonstrates typical &#8220;garbage collection&#8221; behavior (Moore, 2004) in all four examples.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>First, we evaluated alignment accuracy directly by comparing against gold-standard word alignments.</text>
                  <doc_id>215</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4 This data is available at http://statmt.org/wmt10.</text>
                  <doc_id>216</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The results are shown in the alignment F1 column of Table 1.</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We used balanced F-measure rather than alignment error rate as our metric (Fraser and Marcu, 2007).</text>
                  <doc_id>218</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Following Dyer et al. (2011), we also measured the average fertility, &#732;&#966; sing.</text>
                  <doc_id>219</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>, of once-seen source words in the symmetrized alignments.</text>
                  <doc_id>220</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Our alignments show smaller fertility for once-seen words, suggesting that they suffer from &#8220;garbage collection&#8221; effects less than the baseline alignments do.</text>
                  <doc_id>221</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The fact that we had to use hand-aligned data to tune the hyperparameters &#945; and &#946; means that our method is no longer completely unsupervised.</text>
                  <doc_id>222</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, our observation is that alignment accuracy is actually fairly robust to the choice of these hyperparameters, as shown in Table 2.</text>
                  <doc_id>223</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As we will see below, we still obtained strong improvements in translation quality when hand-aligned data was unavailable.</text>
                  <doc_id>224</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We also tried generating 50 word classes using the tool provided in GIZA++.</text>
                  <doc_id>225</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We found that adding word classes improved alignment quality a little, but more so for the baseline system (see Table 3).</text>
                  <doc_id>226</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We used the alignments generated by training with word classes for our translation experiments.</text>
                  <doc_id>227</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#946;</text>
                  <doc_id>228</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8211;</text>
                  <doc_id>229</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.5</text>
                  <doc_id>230</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.1</text>
                  <doc_id>231</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.05</text>
                  <doc_id>232</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.01</text>
                  <doc_id>233</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.005</text>
                  <doc_id>234</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.001</text>
                  <doc_id>235</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 2 shows four examples of Chinese- English alignment, comparing the baseline with our smoothed-l 0 method.</text>
                  <doc_id>236</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In all four cases, the baseline produces incorrect extra alignments that prevent good translation rules from being extracted while the smoothed-l 0 results are correct.</text>
                  <doc_id>237</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In particular, the baseline system demonstrates typical &#8220;garbage collection&#8221; behavior (Moore, 2004) in all four examples.</text>
                  <doc_id>238</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Translation</title>
            <text>We then tested the effect of word alignments on translation quality using the hierarchical phrasebased translation system Hiero (Chiang, 2007). We used a fairly standard set of features: seven inherited from Pharaoh (Koehn et al., 2003), a secsetting
ond language model, and penalties for the glue rule, identity rules, unknown-word rules, and two kinds of number/name rules. The feature weights were discriminatively trained using MIRA (Chiang et al., 2008). We used two 5-gram language models, one on the combined English sides of the NIST 2009 Arabic-English and Chinese-English constrained tracks (385M words), and another on 2 billion words of English. For each language pair, we extracted grammar rules from the same data that were used for word alignment. The development data that were used for discriminative training were: for Chinese-English and Arabic-English, data from the NIST 2004 and NIST 2006 test sets, plus newsgroup data from the
GALE program (LDC2006E92); for Urdu-English, half of the NIST 2008 test set; for Czech-English, a training set of 2051 sentences provided by the WMT10 translation workshop.
The results are shown in the Bleu column of Table 1. We used case-insensitive IBM Bleu (closest reference length) as our metric. Significance testing was carried out using bootstrap resampling with 1000 samples (Koehn, 2004; Zhang et al., 2004).
All of the tests showed significant improvements (p &lt; 0.01), ranging from +0.4 Bleu to +1.4 Bleu. For Urdu, even though we didn&#8217;t have manual alignments to tune hyperparameters, we got significant gains over a good baseline. This is promising for languages that do not have any manually aligned data.
Ideally, one would want to tune &#945; and &#946; to maximize Bleu. However, this is prohibitively expensive, especially if we must tune them separately in each alignment direction before symmetrization. We ran some contrastive experiments to investigate the impact of hyperparameter tuning on translation quality. For the smaller Arabic-English corpus, we symmetrized all combinations of the two top-scoring alignments (according to F1) in each direction, yielding four sets of alignments. Table 4 shows Bleu scores for translation models learned from these alignments. Unfortunately, we find that optimizing F1 is not optimal for Bleu&#8212;using the second-best alignments yields a further improvement of 0.5 Bleu on the NIST 2009 data, which is statistically significant (p &lt; 0.05).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We then tested the effect of word alignments on translation quality using the hierarchical phrasebased translation system Hiero (Chiang, 2007).</text>
                  <doc_id>239</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We used a fairly standard set of features: seven inherited from Pharaoh (Koehn et al., 2003), a secsetting</text>
                  <doc_id>240</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>ond language model, and penalties for the glue rule, identity rules, unknown-word rules, and two kinds of number/name rules.</text>
                  <doc_id>241</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The feature weights were discriminatively trained using MIRA (Chiang et al., 2008).</text>
                  <doc_id>242</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We used two 5-gram language models, one on the combined English sides of the NIST 2009 Arabic-English and Chinese-English constrained tracks (385M words), and another on 2 billion words of English.</text>
                  <doc_id>243</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For each language pair, we extracted grammar rules from the same data that were used for word alignment.</text>
                  <doc_id>244</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The development data that were used for discriminative training were: for Chinese-English and Arabic-English, data from the NIST 2004 and NIST 2006 test sets, plus newsgroup data from the</text>
                  <doc_id>245</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>GALE program (LDC2006E92); for Urdu-English, half of the NIST 2008 test set; for Czech-English, a training set of 2051 sentences provided by the WMT10 translation workshop.</text>
                  <doc_id>246</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The results are shown in the Bleu column of Table 1.</text>
                  <doc_id>247</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We used case-insensitive IBM Bleu (closest reference length) as our metric.</text>
                  <doc_id>248</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Significance testing was carried out using bootstrap resampling with 1000 samples (Koehn, 2004; Zhang et al., 2004).</text>
                  <doc_id>249</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>All of the tests showed significant improvements (p &lt; 0.01), ranging from +0.4 Bleu to +1.4 Bleu.</text>
                  <doc_id>250</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For Urdu, even though we didn&#8217;t have manual alignments to tune hyperparameters, we got significant gains over a good baseline.</text>
                  <doc_id>251</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This is promising for languages that do not have any manually aligned data.</text>
                  <doc_id>252</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Ideally, one would want to tune &#945; and &#946; to maximize Bleu.</text>
                  <doc_id>253</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, this is prohibitively expensive, especially if we must tune them separately in each alignment direction before symmetrization.</text>
                  <doc_id>254</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We ran some contrastive experiments to investigate the impact of hyperparameter tuning on translation quality.</text>
                  <doc_id>255</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For the smaller Arabic-English corpus, we symmetrized all combinations of the two top-scoring alignments (according to F1) in each direction, yielding four sets of alignments.</text>
                  <doc_id>256</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Table 4 shows Bleu scores for translation models learned from these alignments.</text>
                  <doc_id>257</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Unfortunately, we find that optimizing F1 is not optimal for Bleu&#8212;using the second-best alignments yields a further improvement of 0.5 Bleu on the NIST 2009 data, which is statistically significant (p &lt; 0.05).</text>
                  <doc_id>258</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Related Work</title>
        <text>Schoenemann (2011a), taking inspiration from Bodrumlu et al. (2009), uses integer linear programming to optimize IBM Model 1&#8211;2 and the HMM with the l 0 -norm. This method, however, does not outperform GIZA++. In later work, Schoenemann (2011b) used projected gradient descent for the l 1 - norm. Here, we have adopted his use of projected gradient descent, but using a smoothed l 0 -norm.
Liang et al. (2006) show how to train IBM models in both directions simultaneously by adding a term to the log-likelihood that measures the agreement between the two directions. Gra&#231;a et al. (2010) explore modifications to the HMM model that encourage bijectivity and symmetry. The modifications take the form of constraints on the posterior distribution over alignments that is computed during the E-step. Mermer and Sara&#231;lar (2011) explore a Bayesian version of IBM Model 1, applying sparse Dirichlet priors to t. However, because this method requires the use of Monte Carlo methods, it is not clear how well it can scale to larger datasets.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Schoenemann (2011a), taking inspiration from Bodrumlu et al. (2009), uses integer linear programming to optimize IBM Model 1&#8211;2 and the HMM with the l 0 -norm.</text>
              <doc_id>259</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This method, however, does not outperform GIZA++.</text>
              <doc_id>260</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In later work, Schoenemann (2011b) used projected gradient descent for the l 1 - norm.</text>
              <doc_id>261</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Here, we have adopted his use of projected gradient descent, but using a smoothed l 0 -norm.</text>
              <doc_id>262</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Liang et al. (2006) show how to train IBM models in both directions simultaneously by adding a term to the log-likelihood that measures the agreement between the two directions.</text>
              <doc_id>263</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Gra&#231;a et al. (2010) explore modifications to the HMM model that encourage bijectivity and symmetry.</text>
              <doc_id>264</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The modifications take the form of constraints on the posterior distribution over alignments that is computed during the E-step.</text>
              <doc_id>265</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Mermer and Sara&#231;lar (2011) explore a Bayesian version of IBM Model 1, applying sparse Dirichlet priors to t.</text>
              <doc_id>266</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>However, because this method requires the use of Monte Carlo methods, it is not clear how well it can scale to larger datasets.</text>
              <doc_id>267</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>5 Conclusion</title>
        <text>We have extended the IBM models and HMM model by the addition of an l 0 prior to the word-to-word translation model, which compacts the word-toword translation table, reducing overfitting, and, in particular, the &#8220;garbage collection&#8221; effect. We have shown how to perform MAP-EM with this prior efficiently, even for large datasets. The method is implemented as a modification to the open-source toolkit GIZA++, and we have shown that it significantly improves translation quality across four different language pairs. Even though we have used a small set of gold-standard alignments to tune our hyperparameters, we found that performance was fairly robust to variation in the hyperparameters, and translation performance was good even when goldstandard alignments were unavailable. We hope that our method, due to its simplicity, generality, and effectiveness, will find wide application for training better statistical translation systems.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have extended the IBM models and HMM model by the addition of an l 0 prior to the word-to-word translation model, which compacts the word-toword translation table, reducing overfitting, and, in particular, the &#8220;garbage collection&#8221; effect.</text>
              <doc_id>268</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We have shown how to perform MAP-EM with this prior efficiently, even for large datasets.</text>
              <doc_id>269</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The method is implemented as a modification to the open-source toolkit GIZA++, and we have shown that it significantly improves translation quality across four different language pairs.</text>
              <doc_id>270</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Even though we have used a small set of gold-standard alignments to tune our hyperparameters, we found that performance was fairly robust to variation in the hyperparameters, and translation performance was good even when goldstandard alignments were unavailable.</text>
              <doc_id>271</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We hope that our method, due to its simplicity, generality, and effectiveness, will find wide application for training better statistical translation systems.</text>
              <doc_id>272</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>Acknowledgments</title>
        <text>We are indebted to Thomas Schoenemann for initial discussions and pilot experiments that led to this work, and to the anonymous reviewers for their valuable comments. We thank Jason Riesa for providing the Arabic-English and Chinese-English hand-aligned data and the alignment visualization tool, and Chris Dyer for the Czech-English handaligned data. This research was supported in part by DARPA under contract DOI-NBC D11AP00244 and a Google Faculty Research Award to L. H.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We are indebted to Thomas Schoenemann for initial discussions and pilot experiments that led to this work, and to the anonymous reviewers for their valuable comments.</text>
              <doc_id>273</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We thank Jason Riesa for providing the Arabic-English and Chinese-English hand-aligned data and the alignment visualization tool, and Chris Dyer for the Czech-English handaligned data.</text>
              <doc_id>274</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This research was supported in part by DARPA under contract DOI-NBC D11AP00244 and a Google Faculty Research Award to L.</text>
              <doc_id>275</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>H.</text>
              <doc_id>276</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Adding the l 0 -norm to the IBM models improves both alignment and translation accuracy across four different language pairs. The word trans column also shows that the number of distinct word translations (i.e., the size of the lexical weighting table) is reduced. The &#732;&#966; sing. column shows the average fertility of once-seen source words. For Czech-English, the year refers to the WMT shared task; for all other language pairs, the year refers to the NIST Open MT Evaluation. &#8727; Half of this test set was also used for tuning feature weights.</caption>
        <reference_text>In PAGE 6: ...org/wmt10. The results are shown in the alignment F1 col- umn of  Table1 . We used balanced F-measure rather than alignment error rate as our metric (Fraser and Marcu, 2007)....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>task</cell>
              <cell>data (M)</cell>
              <cell>system</cell>
              <cell>align F1 (%)</cell>
              <cell>word trans (M)</cell>
              <cell>? ?sing.</cell>
              <cell>None</cell>
              <cell>Bleu (%)</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>2008</cell>
              <cell>2009</cell>
              <cell>2010</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>baseline</cell>
              <cell>73.2</cell>
              <cell>3.5</cell>
              <cell>6.2</cell>
              <cell>28.7</cell>
            </row>
            <row>
              <cell>Chi-Eng</cell>
              <cell>9.6+12</cell>
              <cell>lscript0-norm</cell>
              <cell>76.5</cell>
              <cell>2.0</cell>
              <cell>3.3</cell>
              <cell>29.5</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>difference</cell>
              <cell>+3.3</cell>
              <cell>?43%</cell>
              <cell>?47%</cell>
              <cell>+0.8</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>baseline</cell>
              <cell>65.0</cell>
              <cell>3.1</cell>
              <cell>4.5</cell>
              <cell>39.8</cell>
              <cell>42.5</cell>
            </row>
            <row>
              <cell>Ara-Eng</cell>
              <cell>5.4+4.3</cell>
              <cell>lscript0-norm</cell>
              <cell>70.8</cell>
              <cell>1.8</cell>
              <cell>1.8</cell>
              <cell>41.1</cell>
              <cell>43.7</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>difference</cell>
              <cell>+5.9</cell>
              <cell>?39%</cell>
              <cell>?60%</cell>
              <cell>+1.3</cell>
              <cell>+1.2</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>baseline</cell>
              <cell>66.2</cell>
              <cell>15</cell>
              <cell>5.0</cell>
              <cell>41.6</cell>
              <cell>44.9</cell>
            </row>
            <row>
              <cell>Ara-Eng</cell>
              <cell>44+37</cell>
              <cell>lscript0-norm</cell>
              <cell>71.8</cell>
              <cell>7.9</cell>
              <cell>1.8</cell>
              <cell>42.5</cell>
              <cell>45.3</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>difference</cell>
              <cell>+5.6</cell>
              <cell>?47%</cell>
              <cell>?64%</cell>
              <cell>+0.9</cell>
              <cell>+0.4</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>baseline</cell>
              <cell>None</cell>
              <cell>1.7</cell>
              <cell>4.5</cell>
              <cell>None</cell>
              <cell>29.8</cell>
            </row>
            <row>
              <cell>Urd-Eng</cell>
              <cell>1.7+1.5</cell>
              <cell>lscript0-norm</cell>
              <cell>None</cell>
              <cell>1.2</cell>
              <cell>2.2</cell>
              <cell>None</cell>
              <cell>31.2</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>difference</cell>
              <cell>None</cell>
              <cell>?29%</cell>
              <cell>?51%</cell>
              <cell>None</cell>
              <cell>+1.4</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>baseline</cell>
              <cell>65.6</cell>
              <cell>1.5</cell>
              <cell>3.0</cell>
              <cell>None</cell>
              <cell>17.3</cell>
              <cell>18.0</cell>
            </row>
            <row>
              <cell>Cze-Eng</cell>
              <cell>2.1+2.3</cell>
              <cell>lscript0-norm</cell>
              <cell>72.3</cell>
              <cell>1.0</cell>
              <cell>1.4</cell>
              <cell>None</cell>
              <cell>17.9</cell>
              <cell>18.4</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>difference</cell>
              <cell>+6.7</cell>
              <cell>?33%</cell>
              <cell>?53%</cell>
              <cell>None</cell>
              <cell>+0.6</cell>
              <cell>+0.4</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Almost all hyperparameter settings achieve higher F-scores than the baseline IBM Model 4 and HMM model for Arabic-English alignment (&#945; = 0).</caption>
        <reference_text>In PAGE 6: ... The fact that we had to use hand-aligned data to tune the hyperparameters ? and ? means that our method is no longer completely unsupervised. How- ever, our observation is that alignment accuracy is actually fairly robust to the choice of these hyperpa- rameters, as shown in  Table2 . As we will see below, we still obtained strong improvements in translation quality when hand-aligned data was unavailable....</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>?   ?</cell>
              <cell>model     HMM   M4</cell>
              <cell>10</cell>
              <cell>25</cell>
              <cell>50</cell>
              <cell>75</cell>
              <cell>100</cell>
              <cell>250</cell>
              <cell>500</cell>
              <cell>750</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>0.5</cell>
              <cell>HMM</cell>
              <cell>46.3</cell>
              <cell>48.4</cell>
              <cell>52.8</cell>
              <cell>55.7</cell>
              <cell>57.5</cell>
              <cell>61.5</cell>
              <cell>62.6</cell>
              <cell>62.7</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>M4</cell>
              <cell>51.7</cell>
              <cell>53.7</cell>
              <cell>56.4</cell>
              <cell>58.6</cell>
              <cell>59.8</cell>
              <cell>63.3</cell>
              <cell>64.4</cell>
              <cell>64.8</cell>
            </row>
            <row>
              <cell>0.1</cell>
              <cell>HMM</cell>
              <cell>55.6</cell>
              <cell>60.4</cell>
              <cell>61.6</cell>
              <cell>62.1</cell>
              <cell>61.9</cell>
              <cell>61.8</cell>
              <cell>60.2</cell>
              <cell>60.1</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>M4</cell>
              <cell>58.2</cell>
              <cell>62.4</cell>
              <cell>64.0</cell>
              <cell>64.4</cell>
              <cell>64.8</cell>
              <cell>65.5</cell>
              <cell>65.6</cell>
              <cell>65.9</cell>
            </row>
            <row>
              <cell>0.05</cell>
              <cell>HMM</cell>
              <cell>59.1</cell>
              <cell>61.4</cell>
              <cell>62.4</cell>
              <cell>62.5</cell>
              <cell>62.3</cell>
              <cell>60.8</cell>
              <cell>58.7</cell>
              <cell>57.7</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>M4</cell>
              <cell>61.0</cell>
              <cell>63.5</cell>
              <cell>64.6</cell>
              <cell>65.3</cell>
              <cell>65.3</cell>
              <cell>65.4</cell>
              <cell>65.7</cell>
              <cell>65.7</cell>
            </row>
            <row>
              <cell>0.01</cell>
              <cell>HMM</cell>
              <cell>59.7</cell>
              <cell>61.6</cell>
              <cell>60.0</cell>
              <cell>59.5</cell>
              <cell>58.7</cell>
              <cell>56.9</cell>
              <cell>55.7</cell>
              <cell>54.7</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>M4</cell>
              <cell>62.9</cell>
              <cell>65.0</cell>
              <cell>65.1</cell>
              <cell>65.2</cell>
              <cell>65.1</cell>
              <cell>65.4</cell>
              <cell>65.3</cell>
              <cell>65.4</cell>
            </row>
            <row>
              <cell>0.005</cell>
              <cell>HMM</cell>
              <cell>58.1</cell>
              <cell>59.0</cell>
              <cell>58.3</cell>
              <cell>57.6</cell>
              <cell>57.0</cell>
              <cell>55.9</cell>
              <cell>53.9</cell>
              <cell>51.7</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>M4</cell>
              <cell>62.0</cell>
              <cell>64.1</cell>
              <cell>64.5</cell>
              <cell>64.5</cell>
              <cell>64.5</cell>
              <cell>65.0</cell>
              <cell>64.8</cell>
              <cell>64.6</cell>
            </row>
            <row>
              <cell>0.001</cell>
              <cell>HMM</cell>
              <cell>51.7</cell>
              <cell>52.1</cell>
              <cell>51.4</cell>
              <cell>49.3</cell>
              <cell>50.4</cell>
              <cell>46.8</cell>
              <cell>45.4</cell>
              <cell>44.0</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>M4</cell>
              <cell>59.8</cell>
              <cell>61.3</cell>
              <cell>61.5</cell>
              <cell>61.0</cell>
              <cell>61.8</cell>
              <cell>61.2</cell>
              <cell>61.0</cell>
              <cell>61.2</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Adding word classes improves the F-score in both directions for Arabic-English alignment by a little, for the baseline system more so than ours.</caption>
        <reference_text>In PAGE 6: ... We also tried generating 50 word classes using the tool provided in GIZA++. We found that adding word classes improved alignment quality a little, but more so for the baseline system (see  Table3 ). We used the alignments generated by training with word classes for our translation experiments....</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>word classes?</cell>
              <cell>None</cell>
              <cell>word classes?</cell>
              <cell>word classes?</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>direction</cell>
              <cell>system</cell>
              <cell>no</cell>
              <cell>yes</cell>
            </row>
            <row>
              <cell></cell>
              <cell>baseline</cell>
              <cell>49.0</cell>
              <cell>52.1</cell>
            </row>
            <row>
              <cell>P(f | e)#@#@P( f | e)</cell>
              <cell>lscript0-norm#@#@l 0 -norm</cell>
              <cell>63.9</cell>
              <cell>65.9</cell>
            </row>
            <row>
              <cell></cell>
              <cell>difference</cell>
              <cell>+14.9</cell>
              <cell>+13.8</cell>
            </row>
            <row>
              <cell></cell>
              <cell>baseline</cell>
              <cell>64.3</cell>
              <cell>65.2</cell>
            </row>
            <row>
              <cell>P(e | f)#@#@P(e | f )</cell>
              <cell>lscript0-norm#@#@l 0 -norm</cell>
              <cell>69.2</cell>
              <cell>70.3</cell>
            </row>
            <row>
              <cell></cell>
              <cell>difference</cell>
              <cell>+4.9</cell>
              <cell>+5.1</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: Optimizing hyperparameters on alignment F1 score does not necessarily lead to optimal Bleu. The first two columns indicate whether we used the first- or second-best alignments in each direction (according to F1); the third column shows the F1 of the symmetrized alignments, whose corresponding Bleu scores are shown in the last two columns.</caption>
        <reference_text>None</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>setting#@#@</cell>
              <cell>setting#@#@align F1 (%)</cell>
              <cell>align F1 (%)#@#@Bleu (%)</cell>
              <cell>Bleu (%)</cell>
              <cell>Bleu (%)</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>t(f | e)#@#@t( f | e)</cell>
              <cell>t(e | f)#@#@t(e | f )</cell>
              <cell></cell>
              <cell>2008</cell>
              <cell>2009</cell>
            </row>
            <row>
              <cell>1st</cell>
              <cell>1st</cell>
              <cell>70.8</cell>
              <cell>41.1</cell>
              <cell>43.7</cell>
            </row>
            <row>
              <cell>1st</cell>
              <cell>2nd</cell>
              <cell>70.7</cell>
              <cell>41.1</cell>
              <cell>43.8</cell>
            </row>
            <row>
              <cell>2nd</cell>
              <cell>1st</cell>
              <cell>70.7</cell>
              <cell>40.7</cell>
              <cell>44.1</cell>
            </row>
            <row>
              <cell>2nd</cell>
              <cell>2nd</cell>
              <cell>70.9</cell>
              <cell>41.1</cell>
              <cell>44.2</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Andrew Barron</author>
          <author>Jorma Rissanen</author>
          <author>Bin Yu</author>
        </authors>
        <title>The minimum description length principle in coding and modeling.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Dimitri P Bertsekas</author>
        </authors>
        <title>Nonlinear Programming. Athena Scientific.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Christopher M Bishop</author>
        </authors>
        <title>None</title>
        <publication>Pattern Recognition and Machine Learning.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Tugba Bodrumlu</author>
          <author>Kevin Knight</author>
          <author>Sujith Ravi</author>
        </authors>
        <title>A new objective function for word alignment.</title>
        <publication>In Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Ond&#345;ej Bojar</author>
          <author>Magdalena Prokopov&#225;</author>
        </authors>
        <title>CzechEnglish word alignment.</title>
        <publication>In Proceedings of LREC.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Peter F Brown</author>
          <author>Stephen A Della Pietra</author>
          <author>Vincent J Della Pietra</author>
          <author>Robert L Mercer</author>
        </authors>
        <title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1993</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>David Chiang</author>
          <author>Yuval Marton</author>
          <author>Philip Resnik</author>
        </authors>
        <title>Online large-margin training of syntactic and structural translation features.</title>
        <publication>In Proceedings of EMNLP.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>Hierarchical phrase-based translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>A P Dempster</author>
          <author>N M Laird</author>
          <author>D B Rubin</author>
        </authors>
        <title>Maximum likelihood from incomplete data via the EM algorithm.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1977</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>John Duchi</author>
          <author>Shai Shalev-Shwartz</author>
          <author>Yoram Singer</author>
          <author>Tushar Chandra</author>
        </authors>
        <title>Efficient projections onto the l 1 -ball for learning in high dimensions.</title>
        <publication>In Proceedings of ICML.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Chris Dyer</author>
          <author>Jonathan H Clark</author>
          <author>Alon Lavie</author>
          <author>Noah A Smith</author>
        </authors>
        <title>Unsupervised word alignment with arbitrary features.</title>
        <publication>In Proceedings of ACL.</publication>
        <pages>None</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Alexander Fraser</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Measuring word alignment quality for statistical machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Jo&#227;o V Gra&#231;a</author>
          <author>Kuzman Ganchev</author>
          <author>Ben Taskar</author>
        </authors>
        <title>Learning tractable word alignment models with complex constraints.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz Joseph Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical phrase-based translation.</title>
        <publication>In Proceedings of NAACL.</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Philipp Koehn</author>
        </authors>
        <title>Statistical significance tests for machine translation evaluation.</title>
        <publication>In Proceedings of EMNLP.</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Percy Liang</author>
          <author>Ben Taskar</author>
          <author>Dan Klein</author>
        </authors>
        <title>Alignment by agreement.</title>
        <publication>In Proceedings of HLT-NAACL.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Co&#351;kun Mermer</author>
          <author>Murat Sara&#231;lar</author>
        </authors>
        <title>Bayesian word alignment for statistical machine translation.</title>
        <publication>In Proceedings of ACL HLT.</publication>
        <pages>None</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Robert C Moore</author>
        </authors>
        <title>Improving IBM wordalignment Model 1.</title>
        <publication>In Proceedings of ACL.</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Robert Moore</author>
        </authors>
        <title>A discriminative framework for bilingual word alignment.</title>
        <publication>In Proceedings of HLTEMNLP.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Franz Joseph Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>The alignment template approach to statistical machine translation.</title>
        <publication>None</publication>
        <pages>30--417</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Jason Riesa</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Hierarchical search for word alignment.</title>
        <publication>In Proceedings of ACL.</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Thomas Schoenemann</author>
        </authors>
        <title>Probabilistic word alignment under the L 0 -norm.</title>
        <publication>In Proceedings of CoNLL.</publication>
        <pages>None</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Thomas Schoenemann</author>
        </authors>
        <title>Regularizing mono- and bi-word models for word alignment.</title>
        <publication>In Proceedings of IJCNLP.</publication>
        <pages>None</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Ben Taskar</author>
          <author>Lacoste-Julien Simon</author>
          <author>Klein Dan</author>
        </authors>
        <title>A discriminative matching approach to word alignment.</title>
        <publication>In Proceedings of HLT-EMNLP.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>Ashish Vaswani</author>
          <author>Adam Pauls</author>
          <author>David Chiang</author>
        </authors>
        <title>Efficient optimization of an MDL-inspired objective function for unsupervised part-of-speech tagging.</title>
        <publication>In Proceedings of ACL.</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Stephan Vogel</author>
          <author>Hermann Ney</author>
          <author>Christoph Tillmann</author>
        </authors>
        <title>HMM-based word alignment in statistical translation.</title>
        <publication>In Proceedings of COLING.</publication>
        <pages>None</pages>
        <date>1996</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Barron et al., 1998</string>
        <sentence_id>34641</sentence_id>
        <char_offset>118</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Bertsekas, 1999</string>
        <sentence_id>34735</sentence_id>
        <char_offset>155</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Bishop, 2006</string>
        <sentence_id>34696</sentence_id>
        <char_offset>68</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>3</reference_id>
        <string>Bodrumlu et al. (2009)</string>
        <sentence_id>34885</sentence_id>
        <char_offset>45</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>4</reference_id>
        <string>Bojar and Prokopov&#225;, 2006</string>
        <sentence_id>34833</sentence_id>
        <char_offset>73</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>5</reference_id>
        <string>Brown et al., 1993</string>
        <sentence_id>34632</sentence_id>
        <char_offset>65</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>5</reference_id>
        <string>Brown et al., 1993</string>
        <sentence_id>34661</sentence_id>
        <char_offset>100</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>6</reference_id>
        <string>Chiang et al., 2008</string>
        <sentence_id>34865</sentence_id>
        <char_offset>62</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>7</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>34862</sentence_id>
        <char_offset>129</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>8</reference_id>
        <string>Dempster et al., 1977</string>
        <sentence_id>34664</sentence_id>
        <char_offset>64</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>10</reference_id>
        <string>Dyer et al., 2011</string>
        <sentence_id>34639</sentence_id>
        <char_offset>91</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>10</reference_id>
        <string>Dyer et al. (2011)</string>
        <sentence_id>34842</sentence_id>
        <char_offset>10</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>11</reference_id>
        <string>Fraser and Marcu, 2007</string>
        <sentence_id>34841</sentence_id>
        <char_offset>75</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>12</reference_id>
        <string>Gra&#231;a et al. (2010)</string>
        <sentence_id>34890</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>12</reference_id>
        <string>Gra&#231;a et al., 2010</string>
        <sentence_id>34639</sentence_id>
        <char_offset>71</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>12</reference_id>
        <string>Gra&#231;a et al., 2010</string>
        <sentence_id>34670</sentence_id>
        <char_offset>107</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>13</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>34836</sentence_id>
        <char_offset>80</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>13</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>34863</sentence_id>
        <char_offset>73</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>14</reference_id>
        <string>Koehn, 2004</string>
        <sentence_id>34872</sentence_id>
        <char_offset>83</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>15</reference_id>
        <string>Liang et al. (2006)</string>
        <sentence_id>34889</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>15</reference_id>
        <string>Liang et al., 2006</string>
        <sentence_id>34639</sentence_id>
        <char_offset>51</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>16</reference_id>
        <string>Mermer and Sara&#231;lar (2011)</string>
        <sentence_id>34892</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>17</reference_id>
        <string>Moore, 2004</string>
        <sentence_id>34668</sentence_id>
        <char_offset>1</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>17</reference_id>
        <string>Moore, 2004</string>
        <sentence_id>34670</sentence_id>
        <char_offset>63</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>17</reference_id>
        <string>Moore, 2004</string>
        <sentence_id>34861</sentence_id>
        <char_offset>87</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>18</reference_id>
        <string>Moore, 2005</string>
        <sentence_id>34637</sentence_id>
        <char_offset>118</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>19</reference_id>
        <string>Och and Ney, 2004</string>
        <sentence_id>34634</sentence_id>
        <char_offset>74</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>20</reference_id>
        <string>Riesa and Marcu, 2010</string>
        <sentence_id>34637</sentence_id>
        <char_offset>152</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>21</reference_id>
        <string>Schoenemann (2011</string>
        <sentence_id>34734</sentence_id>
        <char_offset>10</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>21</reference_id>
        <string>Schoenemann (2011</string>
        <sentence_id>34739</sentence_id>
        <char_offset>15</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>21</reference_id>
        <string>Schoenemann (2011</string>
        <sentence_id>34885</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>21</reference_id>
        <string>Schoenemann (2011</string>
        <sentence_id>34887</sentence_id>
        <char_offset>15</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>22</reference_id>
        <string>Schoenemann (2011</string>
        <sentence_id>34734</sentence_id>
        <char_offset>10</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>22</reference_id>
        <string>Schoenemann (2011</string>
        <sentence_id>34739</sentence_id>
        <char_offset>15</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>22</reference_id>
        <string>Schoenemann (2011</string>
        <sentence_id>34885</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>22</reference_id>
        <string>Schoenemann (2011</string>
        <sentence_id>34887</sentence_id>
        <char_offset>15</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>23</reference_id>
        <string>Taskar et al., 2005</string>
        <sentence_id>34637</sentence_id>
        <char_offset>131</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>24</reference_id>
        <string>Vaswani et al., 2010</string>
        <sentence_id>34642</sentence_id>
        <char_offset>80</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>24</reference_id>
        <string>Vaswani et al., 2010</string>
        <sentence_id>34671</sentence_id>
        <char_offset>120</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>24</reference_id>
        <string>Vaswani et al., 2010</string>
        <sentence_id>34705</sentence_id>
        <char_offset>21</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>24</reference_id>
        <string>Vaswani et al., 2010</string>
        <sentence_id>34732</sentence_id>
        <char_offset>12</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>25</reference_id>
        <string>Vogel et al., 1996</string>
        <sentence_id>34632</sentence_id>
        <char_offset>114</char_offset>
      </citation>
      <citation>
        <id>42</id>
        <reference_id>25</reference_id>
        <string>Vogel et al., 1996</string>
        <sentence_id>34661</sentence_id>
        <char_offset>120</char_offset>
      </citation>
    </citations>
  </content>
</document>
