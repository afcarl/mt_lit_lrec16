<document>
  <filename>P11-2075</filename>
  <authors>
    <author>Kevin Duh</author>
    <author>Akinori Fujino</author>
  </authors>
  <title>Is Machine Translation Ripe for Cross-lingual Sentiment Classification?</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>Recent advances in Machine Translation (MT) have brought forth a new paradigm for building NLP applications in low-resource scenarios. To build a sentiment classifier for a language with no labeled resources, one can translate labeled data from another language, then train a classifier on the translated text. This can be viewed as a domain adaptation problem, where labeled translations and test data have some mismatch. Various prior work have achieved positive results using this approach.
In this opinion piece, we take a step back and make some general statements about crosslingual adaptation problems. First, we claim that domain mismatch is not caused by MT errors, and accuracy degradation will occur even in the case of perfect MT. Second, we argue that the cross-lingual adaptation problem is qualitatively different from other (monolingual) adaptation problems in NLP; thus new adaptation algorithms ought to be considered. This paper will describe a series of carefullydesigned experiments that led us to these conclusions.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Recent advances in Machine Translation (MT) have brought forth a new paradigm for building NLP applications in low-resource scenarios.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To build a sentiment classifier for a language with no labeled resources, one can translate labeled data from another language, then train a classifier on the translated text.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This can be viewed as a domain adaptation problem, where labeled translations and test data have some mismatch.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Various prior work have achieved positive results using this approach.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this opinion piece, we take a step back and make some general statements about crosslingual adaptation problems.</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>First, we claim that domain mismatch is not caused by MT errors, and accuracy degradation will occur even in the case of perfect MT.</text>
              <doc_id>5</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Second, we argue that the cross-lingual adaptation problem is qualitatively different from other (monolingual) adaptation problems in NLP; thus new adaptation algorithms ought to be considered.</text>
              <doc_id>6</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This paper will describe a series of carefullydesigned experiments that led us to these conclusions.</text>
              <doc_id>7</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Summary</title>
        <text>Question 1: If MT gave perfect translations (semantically), do we still have a domain adaptation challenge in cross-lingual sentiment classification?
Answer: Yes. The reason is that while many translations of a word may be valid, the MT system might have a systematic bias. For example, the word &#8220;awesome&#8221; might be prevalent in English reviews, but in 429 translated reviews, the word &#8220;excellent&#8221; is generated instead. From the perspective of MT, this translation is correct and preserves sentiment polarity. But from the perspective of a classifier, there is a domain mismatch due to differences in word distributions.
Question 2: Can we apply standard adaptation algorithms developed for other (monolingual) adaptation problems to cross-lingual adaptation?
Answer: No. It appears that the interaction between target unlabeled data and source data can be rather unexpected in the case of cross-lingual adaptation. We do not know the reason, but our experiments show that the accuracy of adaptation algorithms in cross-lingual scenarios have much higher variance than monolingual scenarios.
The goal of this opinion piece is to argue the need to better understand the characteristics of domain adaptation in cross-lingual problems. We invite the reader to disagree with our conclusion (that the true barrier to good performance is not insufficient MT quality, but inappropriate domain adaptation methods). Here we present a series of experiments that led us to this conclusion. First we describe the experiment design (&#167;2) and baselines (&#167;3), before answering Question 1 (&#167;4) and Question 2 (&#167;5).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Question 1: If MT gave perfect translations (semantically), do we still have a domain adaptation challenge in cross-lingual sentiment classification?</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Answer: Yes.</text>
              <doc_id>9</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The reason is that while many translations of a word may be valid, the MT system might have a systematic bias.</text>
              <doc_id>10</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For example, the word &#8220;awesome&#8221; might be prevalent in English reviews, but in 429 translated reviews, the word &#8220;excellent&#8221; is generated instead.</text>
              <doc_id>11</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>From the perspective of MT, this translation is correct and preserves sentiment polarity.</text>
              <doc_id>12</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>But from the perspective of a classifier, there is a domain mismatch due to differences in word distributions.</text>
              <doc_id>13</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Question 2: Can we apply standard adaptation algorithms developed for other (monolingual) adaptation problems to cross-lingual adaptation?</text>
              <doc_id>14</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Answer: No.</text>
              <doc_id>15</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It appears that the interaction between target unlabeled data and source data can be rather unexpected in the case of cross-lingual adaptation.</text>
              <doc_id>16</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We do not know the reason, but our experiments show that the accuracy of adaptation algorithms in cross-lingual scenarios have much higher variance than monolingual scenarios.</text>
              <doc_id>17</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The goal of this opinion piece is to argue the need to better understand the characteristics of domain adaptation in cross-lingual problems.</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We invite the reader to disagree with our conclusion (that the true barrier to good performance is not insufficient MT quality, but inappropriate domain adaptation methods).</text>
              <doc_id>19</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Here we present a series of experiments that led us to this conclusion.</text>
              <doc_id>20</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>First we describe the experiment design (&#167;2) and baselines (&#167;3), before answering Question 1 (&#167;4) and Question 2 (&#167;5).</text>
              <doc_id>21</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Experiment Design</title>
        <text>The cross-lingual setup is this: we have labeled data from source domain S and wish to build a sentiment classifier for target domain T . Domain mismatch can arise from language differences (e.g. English vs. translated text) or market differences (e.g. DVD vs. Book reviews). Our experiments will involve fixing
T to a common testset and varying S. This allows us to experiment with different settings for adaptation. We use the Amazon review dataset of Prettenhofer (2010) 1 , due to its wide range of languages (English [EN], Japanese [JP], French [FR], German [DE]) and markets (music, DVD, books). Unlike Prettenhofer (2010), we reverse the direction of cross-lingual adaptation and consider English as target. English is not a low-resource language, but this setting allows for more comparisons. Each source dataset has 2000 reviews, equally balanced between positive and negative. The target has 2000 test samples, large unlabeled data (25k, 30k, 50k samples respectively for Music, DVD, and Books), and an additional 2000 labeled data reserved for oracle experiments. Texts in JP, FR, and DE are translated word-by-word into English with Google Translate. 2 We perform three sets of experiments, shown in Table 1. Table 2 lists all the results; we will interpret them in the following sections.
Target (T ) Source (S) 1 Music-EN Music-JP, Music-FR, Music-DE, DVD-EN, Book-EN 2 DVD-EN DVD-JP, DVD-FR, DVD-DE, Music-EN, Book-EN 3 Book-EN Book-JP, Book-FR, Book-DE, Music-EN, DVD-EN
3 How much performance degradation occurs in cross-lingual adaptation?
First, we need to quantify the accuracy degradation under different source data, without consideration of domain adaptation methods. So we train a SVM classifier on labeled source data 3 , and directly apply it on test data. The oracle setting, which has no domain-mismatch (e.g. train on Music-EN, test on Music-EN), achieves an average test accuracy of (81.6 + 80.9 + 80.0)/3 = 80.8% 4 . Aver-
1 http://www.webis.de/research/corpora/webis-cls-10 2 This is done by querying foreign words to build a bilingual
dictionary. The words are converted to tfidf unigram features. 3 For all methods we try here, 5% of the 2000 labeled source
samples are held-out for parameter tuning. 4 See column EN of Table 2, Supervised SVM results.
age cross-lingual accuracies are: 69.4% (JP), 75.6% (FR), 77.0% (DE), so degradations compared to oracle are: -11% (JP), -5% (FR), -4% (DE). 5 Crossmarket degradations are around -6% 6 . Observation 1: Degradations due to market and language mismatch are comparable in several cases (e.g. MUSIC-DE and DVD-EN perform similarly for target MUSIC-EN). Observation 2: The ranking of source language by decreasing accuracy is DE &gt; FR &gt; JP. Does this mean JP-EN is a more difficult language pair for MT? The next section will show that this is not necessarily the case. Certainly, the domain mismatch for JP is larger than DE, but this could be due to phenomenon other than MT errors.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The cross-lingual setup is this: we have labeled data from source domain S and wish to build a sentiment classifier for target domain T .</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Domain mismatch can arise from language differences (e.g. English vs. translated text) or market differences (e.g. DVD vs.</text>
              <doc_id>23</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Book reviews).</text>
              <doc_id>24</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Our experiments will involve fixing</text>
              <doc_id>25</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>T to a common testset and varying S.</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This allows us to experiment with different settings for adaptation.</text>
              <doc_id>27</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We use the Amazon review dataset of Prettenhofer (2010) 1 , due to its wide range of languages (English [EN], Japanese [JP], French [FR], German [DE]) and markets (music, DVD, books).</text>
              <doc_id>28</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Unlike Prettenhofer (2010), we reverse the direction of cross-lingual adaptation and consider English as target.</text>
              <doc_id>29</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>English is not a low-resource language, but this setting allows for more comparisons.</text>
              <doc_id>30</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Each source dataset has 2000 reviews, equally balanced between positive and negative.</text>
              <doc_id>31</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The target has 2000 test samples, large unlabeled data (25k, 30k, 50k samples respectively for Music, DVD, and Books), and an additional 2000 labeled data reserved for oracle experiments.</text>
              <doc_id>32</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Texts in JP, FR, and DE are translated word-by-word into English with Google Translate.</text>
              <doc_id>33</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>2 We perform three sets of experiments, shown in Table 1.</text>
              <doc_id>34</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Table 2 lists all the results; we will interpret them in the following sections.</text>
              <doc_id>35</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Target (T ) Source (S) 1 Music-EN Music-JP, Music-FR, Music-DE, DVD-EN, Book-EN 2 DVD-EN DVD-JP, DVD-FR, DVD-DE, Music-EN, Book-EN 3 Book-EN Book-JP, Book-FR, Book-DE, Music-EN, DVD-EN</text>
              <doc_id>36</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3 How much performance degradation occurs in cross-lingual adaptation?</text>
              <doc_id>37</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>First, we need to quantify the accuracy degradation under different source data, without consideration of domain adaptation methods.</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>So we train a SVM classifier on labeled source data 3 , and directly apply it on test data.</text>
              <doc_id>39</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The oracle setting, which has no domain-mismatch (e.g. train on Music-EN, test on Music-EN), achieves an average test accuracy of (81.6 + 80.9 + 80.0)/3 = 80.8% 4 .</text>
              <doc_id>40</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Aver-</text>
              <doc_id>41</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 http://www.webis.de/research/corpora/webis-cls-10 2 This is done by querying foreign words to build a bilingual</text>
              <doc_id>42</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>dictionary.</text>
              <doc_id>43</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The words are converted to tfidf unigram features.</text>
              <doc_id>44</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>3 For all methods we try here, 5% of the 2000 labeled source</text>
              <doc_id>45</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>samples are held-out for parameter tuning.</text>
              <doc_id>46</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>4 See column EN of Table 2, Supervised SVM results.</text>
              <doc_id>47</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>age cross-lingual accuracies are: 69.4% (JP), 75.6% (FR), 77.0% (DE), so degradations compared to oracle are: -11% (JP), -5% (FR), -4% (DE).</text>
              <doc_id>48</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>5 Crossmarket degradations are around -6% 6 .</text>
              <doc_id>49</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Observation 1: Degradations due to market and language mismatch are comparable in several cases (e.g. MUSIC-DE and DVD-EN perform similarly for target MUSIC-EN).</text>
              <doc_id>50</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Observation 2: The ranking of source language by decreasing accuracy is DE &gt; FR &gt; JP.</text>
              <doc_id>51</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Does this mean JP-EN is a more difficult language pair for MT?</text>
              <doc_id>52</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The next section will show that this is not necessarily the case.</text>
              <doc_id>53</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Certainly, the domain mismatch for JP is larger than DE, but this could be due to phenomenon other than MT errors.</text>
              <doc_id>54</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>4 Where exactly is the domain mismatch?</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>55</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Theory of Domain Adaptation</title>
            <text>We analyze domain adaptation by the concepts of labeling and instance mismatch (Jiang and Zhai, 2007). Let p t (x,y) = p t (y|x)p t (x) be the target distribution of samples x (e.g. unigram feature vector) and labels y (positive / negative). Let p s (x,y) = p s (y|x)p s (x) be the corresponding source distribution. We assume that one (or both) of the following distributions differ between source and target:
&#8226; Instance mismatch: p s (x) &#8800; p t (x).
&#8226; Labeling mismatch: p s (y|x) &#8800; p t (y|x).
Instance mismatch implies that the input feature vectors have different distribution (e.g. one dataset uses the word &#8220;excellent&#8221; often, while the other uses the word &#8220;awesome&#8221;). This degrades performance because classifiers trained on &#8220;excellent&#8221; might not know how to classify texts with the word &#8220;awesome.&#8221; The solution is to tie together these features (Blitzer et al., 2006) or re-weight the input distribution (Sugiyama et al., 2008). Under some assumptions (i.e. covariate shift), oracle accuracy can be achieved theoretically (Shimodaira, 2000). Labeling mismatch implies the same input has different labels in different domains. For example, the JP word meaning &#8220;excellent&#8221; may be mistranslated as &#8220;bad&#8221; in English. Then, positive JP
5 See &#8220;Adapt by Language&#8221; columns of Table 2. Note
JP+FR+DE condition has 6000 labeled samples, so is not directly comparable to other adaptation scenarios (2000 samples). Nevertheless, mixing languages seem to give good results. 6 See &#8220;Adapt by Market&#8221; columns of Table 2.
Target Classifier Oracle Adapt by Language Adapt by Market EN JP FR DE JP+FR+DE MUSIC DVD BOOK
reviews will be associated with the word &#8220;bad&#8221;: p s (y = +1|x = bad) will be high, whereas the true conditional distribution should have high p t (y = &#8722;1|x = bad) instead. There are several cases for labeling mismatch, depending on how the polarity changes (Table 3). The solution is to filter out these noisy samples (Jiang and Zhai, 2007) or optimize loosely-linked objectives through shared parameters or Bayesian priors (Finkel and Manning, 2009).
Which mismatch is responsible for accuracy degradations in cross-lingual adaptation?
&#8226; Instance mismatch: Systematic MT bias generates word distributions different from naturallyoccurring English. (Translation may be valid.)
&#8226; Label mismatch: MT error mis-translates a word into something with different polarity.
Conclusion from &#167;4.2 and &#167;4.3: Instance mismatch occurs often; MT error appears minimal.
Mis-translated polarity Effect</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We analyze domain adaptation by the concepts of labeling and instance mismatch (Jiang and Zhai, 2007).</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Let p t (x,y) = p t (y|x)p t (x) be the target distribution of samples x (e.g. unigram feature vector) and labels y (positive / negative).</text>
                  <doc_id>57</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Let p s (x,y) = p s (y|x)p s (x) be the corresponding source distribution.</text>
                  <doc_id>58</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We assume that one (or both) of the following distributions differ between source and target:</text>
                  <doc_id>59</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Instance mismatch: p s (x) &#8800; p t (x).</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Labeling mismatch: p s (y|x) &#8800; p t (y|x).</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Instance mismatch implies that the input feature vectors have different distribution (e.g. one dataset uses the word &#8220;excellent&#8221; often, while the other uses the word &#8220;awesome&#8221;).</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This degrades performance because classifiers trained on &#8220;excellent&#8221; might not know how to classify texts with the word &#8220;awesome.&#8221; The solution is to tie together these features (Blitzer et al., 2006) or re-weight the input distribution (Sugiyama et al., 2008).</text>
                  <doc_id>63</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Under some assumptions (i.e. covariate shift), oracle accuracy can be achieved theoretically (Shimodaira, 2000).</text>
                  <doc_id>64</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Labeling mismatch implies the same input has different labels in different domains.</text>
                  <doc_id>65</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For example, the JP word meaning &#8220;excellent&#8221; may be mistranslated as &#8220;bad&#8221; in English.</text>
                  <doc_id>66</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Then, positive JP</text>
                  <doc_id>67</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5 See &#8220;Adapt by Language&#8221; columns of Table 2.</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Note</text>
                  <doc_id>69</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>JP+FR+DE condition has 6000 labeled samples, so is not directly comparable to other adaptation scenarios (2000 samples).</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Nevertheless, mixing languages seem to give good results.</text>
                  <doc_id>71</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>6 See &#8220;Adapt by Market&#8221; columns of Table 2.</text>
                  <doc_id>72</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Target Classifier Oracle Adapt by Language Adapt by Market EN JP FR DE JP+FR+DE MUSIC DVD BOOK</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>reviews will be associated with the word &#8220;bad&#8221;: p s (y = +1|x = bad) will be high, whereas the true conditional distribution should have high p t (y = &#8722;1|x = bad) instead.</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>There are several cases for labeling mismatch, depending on how the polarity changes (Table 3).</text>
                  <doc_id>75</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The solution is to filter out these noisy samples (Jiang and Zhai, 2007) or optimize loosely-linked objectives through shared parameters or Bayesian priors (Finkel and Manning, 2009).</text>
                  <doc_id>76</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Which mismatch is responsible for accuracy degradations in cross-lingual adaptation?</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Instance mismatch: Systematic MT bias generates word distributions different from naturallyoccurring English.</text>
                  <doc_id>78</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(Translation may be valid.</text>
                  <doc_id>79</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>)</text>
                  <doc_id>80</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Label mismatch: MT error mis-translates a word into something with different polarity.</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Conclusion from &#167;4.2 and &#167;4.3: Instance mismatch occurs often; MT error appears minimal.</text>
                  <doc_id>82</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Mis-translated polarity Effect</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Analysis of Instance Mismatch</title>
            <text>To measure instance mismatch, we compute statistics between p s (x) and p t (x), or approximations thereof: First, we calculate a (normalized) average feature from all samples of source S, which represents the unigram distribution of MT output. Similarly, the average feature vector for target T approximates the unigram distribution of English reviews p t (x). Then we measure:
&#8226; KL Divergence between Avg(S) and Avg(T ), where Avg() is the average vector.
&#8226; Set Coverage of Avg(T ) on Avg(S): how many word (type) in T appears at least once in S.
Both measures correlate strongly with final accuracy, as seen in Figure 1. The correlation coefficients are r = &#8722;0.78 for KL Divergence and r = 0.71 for Coverage, both statistically significant (p &lt; 0.05). This implies that instance mismatch is an important reason for the degradations seen in Section 3. 7</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>To measure instance mismatch, we compute statistics between p s (x) and p t (x), or approximations thereof: First, we calculate a (normalized) average feature from all samples of source S, which represents the unigram distribution of MT output.</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Similarly, the average feature vector for target T approximates the unigram distribution of English reviews p t (x).</text>
                  <doc_id>85</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Then we measure:</text>
                  <doc_id>86</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; KL Divergence between Avg(S) and Avg(T ), where Avg() is the average vector.</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Set Coverage of Avg(T ) on Avg(S): how many word (type) in T appears at least once in S.</text>
                  <doc_id>88</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Both measures correlate strongly with final accuracy, as seen in Figure 1.</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The correlation coefficients are r = &#8722;0.78 for KL Divergence and r = 0.71 for Coverage, both statistically significant (p &lt; 0.05).</text>
                  <doc_id>90</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This implies that instance mismatch is an important reason for the degradations seen in Section 3.</text>
                  <doc_id>91</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>7</text>
                  <doc_id>92</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>4.3 Analysis of Labeling Mismatch</title>
            <text>We measure labeling mismatch by looking at differences in the weight vectors of oracle SVM and adapted SVM. Intuitively, if a feature has positive weight in the oracle SVM, but negative weight in the adapted SVM, then it is likely a MT mis-translation
7 The observant reader may notice that cross-market points
exhibit higher coverage but equal accuracy (74-78%) to some cross-lingual points. This suggests that MT output may be more constrained in vocabulary than naturally-occurring English.
KL Divergence Test Coverage 0.35
0.3
0.25
0.2
0.15
0.1
0.05
0.9
0.8
0.7
0.6
Algorithm 1 Measuring labeling mismatch Input: Weight vectors for source w s and target w t Input: Target data average sample vector avg(T ) Output: Polarity flip rate f 1: Normalize: w s = avg(T ) * w s ; w t = avg(T ) * w t 2: Set S + = { K most positive features in w s } 3: Set S &#8722; = { K most negative features in w s } 4: Set T + = { K most positive features in w t } 5: Set T &#8722; = { K most negative features in w t } 6: for each feature i &#8712; T + do 7: if i &#8712; S &#8722; then f = f + 1 8: end for 9: for each feature j &#8712; T &#8722; do 10: if j &#8712; S + then f = f + 1 11: end for 12: f = f 2K
0.5
is causing the polarity flip. Algorithm 1 (with K=2000) shows how we compute polarity flip rate. 8
We found that the polarity flip rate does not correlate well with accuracy at all (r = 0.04). Conclusion: Labeling mismatch is not a factor in performance degradation. Nevertheless, we note there is a surprising large number of flips (24% on average). A manual check of the flipped words in BOOK-JP revealed few MT mistakes. Only 3.7% of 450 random EN-JP word pairs checked can be judged as blatantly incorrect (without sentence context). The majority of flipped words do not have a clear sentiment orientation (e.g. &#8220;amazon&#8221;, &#8220;human&#8221;, &#8220;moreover&#8221;).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We measure labeling mismatch by looking at differences in the weight vectors of oracle SVM and adapted SVM.</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Intuitively, if a feature has positive weight in the oracle SVM, but negative weight in the adapted SVM, then it is likely a MT mis-translation</text>
                  <doc_id>94</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>7 The observant reader may notice that cross-market points</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>exhibit higher coverage but equal accuracy (74-78%) to some cross-lingual points.</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This suggests that MT output may be more constrained in vocabulary than naturally-occurring English.</text>
                  <doc_id>97</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>KL Divergence Test Coverage 0.35</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.3</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.25</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.2</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.15</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.1</text>
                  <doc_id>103</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.05</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.9</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.8</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.7</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.6</text>
                  <doc_id>108</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Algorithm 1 Measuring labeling mismatch Input: Weight vectors for source w s and target w t Input: Target data average sample vector avg(T ) Output: Polarity flip rate f 1: Normalize: w s = avg(T ) * w s ; w t = avg(T ) * w t 2: Set S + = { K most positive features in w s } 3: Set S &#8722; = { K most negative features in w s } 4: Set T + = { K most positive features in w t } 5: Set T &#8722; = { K most negative features in w t } 6: for each feature i &#8712; T + do 7: if i &#8712; S &#8722; then f = f + 1 8: end for 9: for each feature j &#8712; T &#8722; do 10: if j &#8712; S + then f = f + 1 11: end for 12: f = f 2K</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.5</text>
                  <doc_id>110</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>is causing the polarity flip.</text>
                  <doc_id>111</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Algorithm 1 (with K=2000) shows how we compute polarity flip rate.</text>
                  <doc_id>112</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>8</text>
                  <doc_id>113</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We found that the polarity flip rate does not correlate well with accuracy at all (r = 0.04).</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Conclusion: Labeling mismatch is not a factor in performance degradation.</text>
                  <doc_id>115</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Nevertheless, we note there is a surprising large number of flips (24% on average).</text>
                  <doc_id>116</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>A manual check of the flipped words in BOOK-JP revealed few MT mistakes.</text>
                  <doc_id>117</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Only 3.7% of 450 random EN-JP word pairs checked can be judged as blatantly incorrect (without sentence context).</text>
                  <doc_id>118</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The majority of flipped words do not have a clear sentiment orientation (e.g. &#8220;amazon&#8221;, &#8220;human&#8221;, &#8220;moreover&#8221;).</text>
                  <doc_id>119</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>5 Are standard adaptation algorithms applicable to cross-lingual problems?</title>
        <text>One of the breakthroughs in cross-lingual text classification is the realization that it can be cast as domain adaptation. This makes available a host of preexisting adaptation algorithms for improving over supervised results. However, we argue that it may be
8 The feature normalization in Step 1 is important to ensure
that the weight magnitudes are comparable.
better to &#8220;adapt&#8221; the standard adaptation algorithm to the cross-lingual setting. We arrived at this conclusion by trying the adapted counterpart of SVMs off-the-shelf. Recently, (Bergamo and Torresani, 2010) showed that Transductive SVMs (TSVM), originally developed for semi-supervised learning, are also strong adaptation methods. The idea is to train on source data like a SVM, but encourage the classification boundary to divide through low density regions in the unlabeled target data.
Table 2 shows that TSVM outperforms SVM in all but one case for cross-market adaptation, but gives mixed results for cross-lingual adaptation. This is a puzzling result considering that both use the same unlabeled data. Why does TSVM exhibit such a large variance on cross-lingual problems, but not on cross-market problems? Is unlabeled target data interacting with source data in some unexpected way?
Certainly there are several successful studies (Wan, 2009; Wei and Pal, 2010; Banea et al., 2008), but we think it is important to consider the possibility that cross-lingual adaptation has some fundamental differences. We conjecture that adapting from artificially-generated text (e.g. MT output) is a different story than adapting from naturallyoccurring text (e.g. cross-market). In short, MT is ripe for cross-lingual adaptation; what is not ripe is probably our understanding of the special characteristics of the adaptation problem.
References
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer Hassan. 2008. Multilingual subjectivity analysis using machine translation. In Proc. of Conference on Empirical Methods in Natural Language Processing (EMNLP). Alessandro Bergamo and Lorenzo Torresani. 2010. Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach. In Advances in Neural Information Processing Systems (NIPS). John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspondence learning. In Proc. of Conference on Empirical Methods in Natural Language Processing (EMNLP). Jenny Rose Finkel and Chris Manning. 2009. Hierarchical Bayesian domain adaptation. In Proc. of NAACL Human Language Technologies (HLT).
Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In Proc. of the Association for Computational Linguistics (ACL). Peter Prettenhofer and Benno Stein. 2010. Crosslanguage text classification using structural correspondence learning. In Proc. of the Association for Computational Linguistics (ACL). Hidetoshi Shimodaira. 2000. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of Statistical Planning and Inferenc, 90. Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima,
Hisashi Kashima, Paul von B&#252;nau, and Motoaki Kawanabe. 2008. Direct importance estimation for covariate shift adaptation. Annals of the Institute of Statistical Mathematics, 60(4). Xiaojun Wan. 2009. Co-training for cross-lingual sentiment classification. In Proc. of the Association for Computational Linguistics (ACL). Bin Wei and Chris Pal. 2010. Cross lingual adaptation:
an experiment on sentiment classification. In Proceedings of the ACL 2010 Conference Short Papers.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>One of the breakthroughs in cross-lingual text classification is the realization that it can be cast as domain adaptation.</text>
              <doc_id>120</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This makes available a host of preexisting adaptation algorithms for improving over supervised results.</text>
              <doc_id>121</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, we argue that it may be</text>
              <doc_id>122</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>8 The feature normalization in Step 1 is important to ensure</text>
              <doc_id>123</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>that the weight magnitudes are comparable.</text>
              <doc_id>124</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>better to &#8220;adapt&#8221; the standard adaptation algorithm to the cross-lingual setting.</text>
              <doc_id>125</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We arrived at this conclusion by trying the adapted counterpart of SVMs off-the-shelf.</text>
              <doc_id>126</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Recently, (Bergamo and Torresani, 2010) showed that Transductive SVMs (TSVM), originally developed for semi-supervised learning, are also strong adaptation methods.</text>
              <doc_id>127</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The idea is to train on source data like a SVM, but encourage the classification boundary to divide through low density regions in the unlabeled target data.</text>
              <doc_id>128</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Table 2 shows that TSVM outperforms SVM in all but one case for cross-market adaptation, but gives mixed results for cross-lingual adaptation.</text>
              <doc_id>129</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This is a puzzling result considering that both use the same unlabeled data.</text>
              <doc_id>130</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Why does TSVM exhibit such a large variance on cross-lingual problems, but not on cross-market problems?</text>
              <doc_id>131</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Is unlabeled target data interacting with source data in some unexpected way?</text>
              <doc_id>132</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Certainly there are several successful studies (Wan, 2009; Wei and Pal, 2010; Banea et al., 2008), but we think it is important to consider the possibility that cross-lingual adaptation has some fundamental differences.</text>
              <doc_id>133</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We conjecture that adapting from artificially-generated text (e.g. MT output) is a different story than adapting from naturallyoccurring text (e.g. cross-market).</text>
              <doc_id>134</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In short, MT is ripe for cross-lingual adaptation; what is not ripe is probably our understanding of the special characteristics of the adaptation problem.</text>
              <doc_id>135</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>References</text>
              <doc_id>136</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer Hassan.</text>
              <doc_id>137</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>138</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Multilingual subjectivity analysis using machine translation.</text>
              <doc_id>139</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of Conference on Empirical Methods in Natural Language Processing (EMNLP).</text>
              <doc_id>140</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Alessandro Bergamo and Lorenzo Torresani.</text>
              <doc_id>141</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>142</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach.</text>
              <doc_id>143</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Advances in Neural Information Processing Systems (NIPS).</text>
              <doc_id>144</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>John Blitzer, Ryan McDonald, and Fernando Pereira.</text>
              <doc_id>145</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2006.</text>
              <doc_id>146</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Domain adaptation with structural correspondence learning.</text>
              <doc_id>147</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of Conference on Empirical Methods in Natural Language Processing (EMNLP).</text>
              <doc_id>148</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Jenny Rose Finkel and Chris Manning.</text>
              <doc_id>149</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>150</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Hierarchical Bayesian domain adaptation.</text>
              <doc_id>151</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of NAACL Human Language Technologies (HLT).</text>
              <doc_id>152</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Jing Jiang and ChengXiang Zhai.</text>
              <doc_id>153</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>154</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Instance weighting for domain adaptation in NLP.</text>
              <doc_id>155</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the Association for Computational Linguistics (ACL).</text>
              <doc_id>156</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Peter Prettenhofer and Benno Stein.</text>
              <doc_id>157</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>158</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Crosslanguage text classification using structural correspondence learning.</text>
              <doc_id>159</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the Association for Computational Linguistics (ACL).</text>
              <doc_id>160</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Hidetoshi Shimodaira.</text>
              <doc_id>161</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>2000.</text>
              <doc_id>162</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Improving predictive inference under covariate shift by weighting the loglikelihood function.</text>
              <doc_id>163</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>Journal of Statistical Planning and Inferenc, 90.</text>
              <doc_id>164</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima,</text>
              <doc_id>165</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Hisashi Kashima, Paul von B&#252;nau, and Motoaki Kawanabe.</text>
              <doc_id>166</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>167</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Direct importance estimation for covariate shift adaptation.</text>
              <doc_id>168</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Annals of the Institute of Statistical Mathematics, 60(4).</text>
              <doc_id>169</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Xiaojun Wan.</text>
              <doc_id>170</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>171</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Co-training for cross-lingual sentiment classification.</text>
              <doc_id>172</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the Association for Computational Linguistics (ACL).</text>
              <doc_id>173</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Bin Wei and Chris Pal.</text>
              <doc_id>174</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>175</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Cross lingual adaptation:</text>
              <doc_id>176</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>an experiment on sentiment classification.</text>
              <doc_id>177</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the ACL 2010 Conference Short Papers.</text>
              <doc_id>178</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Test accuracies (%) for English Music/DVD/Book reviews. Each column is an adaptation scenario using different source data. The source data may vary by language or by market. For example, the first row shows that for the target of Music-EN, the accuracy of a SVM trained on translated JP reviews (in the same market) is 68.5, while the accuracy of a SVM trained on DVD reviews (in the same language) is 76.8. &#8220;Oracle&#8221; indicates training on the same market and same language domain as the target. &#8220;JP+FR+DE&#8221; indicates the concatenation of JP, FR, DE as source data. Boldface shows the winner of Supervised vs. Adapted.</caption>
        <reference_text>In PAGE 2: ...2 We perform three sets of experiments, shown in Table 1.  Table2  lists all the results; we will interpret them in the following sections. Target (T ) Source (S) 1 Music-EN Music-JP, Music-FR, Music-DE, DVD-EN, Book-EN 2 DVD-EN DVD-JP, DVD-FR, DVD-DE, Music-EN, Book-EN 3 Book-EN Book-JP, Book-FR, Book-DE, Music-EN, DVD-EN Table 1: Experiment setups: Fix T , vary S....  In PAGE 2: ...ictionary. The words are converted to tfidf unigram features. 3For all methods we try here, 5% of the 2000 labeled source samples are held-out for parameter tuning. 4See column EN of  Table2 , Supervised SVM results. age cross-lingual accuracies are: 69....  In PAGE 4: ... The idea is to train on source data like a SVM, but encourage the classification boundary to divide through low den- sity regions in the unlabeled target data.  Table2  shows that TSVM outperforms SVM in all but one case for cross-market adaptation, but gives mixed results for cross-lingual adaptation. This is a puzzling result considering that both use the same unlabeled data....</reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>Target</cell>
              <cell>Classifier</cell>
              <cell>Oracle   EN</cell>
              <cell>JP</cell>
              <cell>Adapt by Language   FR</cell>
              <cell>Adapt by Language   DE</cell>
              <cell>Adapt by Language   JP+FR+DE</cell>
              <cell>Adapt by Market  MUSIC</cell>
              <cell>Adapt by Market   DVD</cell>
              <cell>Adapt by Market    BOOK</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>MUSIC-EN</cell>
              <cell>Supervised SVM</cell>
              <cell>81.6</cell>
              <cell>68.5</cell>
              <cell>75.2</cell>
              <cell>76.3</cell>
              <cell>80.3</cell>
              <cell>-</cell>
              <cell>76.8</cell>
              <cell>74.1</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Adapted TSVM</cell>
              <cell>79.6</cell>
              <cell>73.0</cell>
              <cell>74.6</cell>
              <cell>77.9</cell>
              <cell>78.6</cell>
              <cell>-</cell>
              <cell>78.4</cell>
              <cell>75.6</cell>
            </row>
            <row>
              <cell>DVD-EN</cell>
              <cell>Supervised SVM</cell>
              <cell>80.9</cell>
              <cell>70.1</cell>
              <cell>76.4</cell>
              <cell>77.4</cell>
              <cell>79.7</cell>
              <cell>75.2</cell>
              <cell>-</cell>
              <cell>74.5</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Adapted TSVM</cell>
              <cell>81.0</cell>
              <cell>71.4</cell>
              <cell>75.5</cell>
              <cell>76.3</cell>
              <cell>78.4</cell>
              <cell>74.8</cell>
              <cell>-</cell>
              <cell>76.7</cell>
            </row>
            <row>
              <cell>BOOK-EN</cell>
              <cell>Supervised SVM</cell>
              <cell>80.0</cell>
              <cell>69.6</cell>
              <cell>75.4</cell>
              <cell>77.4</cell>
              <cell>79.9</cell>
              <cell>73.4</cell>
              <cell>76.2</cell>
              <cell>-</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Adapted TSVM</cell>
              <cell>81.2</cell>
              <cell>73.8</cell>
              <cell>77.6</cell>
              <cell>76.7</cell>
              <cell>79.5</cell>
              <cell>75.1</cell>
              <cell>77.4</cell>
              <cell>-</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Label mismatch: mis-translating positive (+), negative (&#8722;), or neutral (0) words have different effects. We think the first two cases have graceful degradation, but the third case may be catastrophic.</caption>
        <reference_text>In PAGE 3: ...ata. Boldface shows the winner of Supervised vs. Adapted. reviews will be associated with the word  bad : ps(y = +1|x = bad) will be high, whereas the true conditional distribution should have high pt(y = ?1|x = bad) instead. There are several cases for labeling mismatch, depending on how the polarity changes ( Table3 ). The solution is to filter out these noisy samples (Jiang and Zhai, 2007) or optimize loosely-linked objectives through shared parameters or Bayesian priors (Finkel and Manning, 2009)....</reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>Mis-translated polarity</cell>
              <cell>Effect</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>? ? 0</cell>
              <cell>Loose a discriminative</cell>
            </row>
            <row>
              <cell>e.g. ( good  ?  the )</cell>
              <cell>feature</cell>
            </row>
            <row>
              <cell>0 ? ?</cell>
              <cell>Increased overlap in</cell>
            </row>
            <row>
              <cell>e.g. ( the  ?  good )</cell>
              <cell>positive/negative data</cell>
            </row>
            <row>
              <cell>+ ? ? and ? ? +</cell>
              <cell>Association with</cell>
            </row>
            <row>
              <cell>e.g. ( good  ?  bad )</cell>
              <cell>opposite label</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Carmen Banea</author>
          <author>Rada Mihalcea</author>
          <author>Janyce Wiebe</author>
          <author>Samer Hassan</author>
        </authors>
        <title>Multilingual subjectivity analysis using machine translation.</title>
        <publication>In Proc. of Conference on Empirical Methods in Natural Language Processing (EMNLP).</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Alessandro Bergamo</author>
          <author>Lorenzo Torresani</author>
        </authors>
        <title>Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach.</title>
        <publication>In Advances in Neural Information Processing Systems (NIPS).</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>John Blitzer</author>
          <author>Ryan McDonald</author>
          <author>Fernando Pereira</author>
        </authors>
        <title>Domain adaptation with structural correspondence learning.</title>
        <publication>In Proc. of Conference on Empirical Methods in Natural Language Processing (EMNLP).</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Jenny Rose Finkel</author>
          <author>Chris Manning</author>
        </authors>
        <title>Hierarchical Bayesian domain adaptation.</title>
        <publication>In Proc. of NAACL Human Language Technologies (HLT).</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Jing Jiang</author>
          <author>ChengXiang Zhai</author>
        </authors>
        <title>Instance weighting for domain adaptation in NLP.</title>
        <publication>In Proc. of the Association for Computational Linguistics (ACL).</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Peter Prettenhofer</author>
          <author>Benno Stein</author>
        </authors>
        <title>Crosslanguage text classification using structural correspondence learning.</title>
        <publication>In Proc. of the Association for Computational Linguistics (ACL).</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Hidetoshi Shimodaira</author>
        </authors>
        <title>Improving predictive inference under covariate shift by weighting the loglikelihood function.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Masashi Sugiyama</author>
          <author>Taiji Suzuki</author>
          <author>Shinichi Nakajima</author>
          <author>Hisashi Kashima</author>
          <author>Paul von B&#252;nau</author>
          <author>Motoaki Kawanabe</author>
        </authors>
        <title>Direct importance estimation for covariate shift adaptation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Xiaojun Wan</author>
        </authors>
        <title>Co-training for cross-lingual sentiment classification.</title>
        <publication>In Proc. of the Association for Computational Linguistics (ACL).</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>1</reference_id>
        <string>Bergamo and Torresani, 2010</string>
        <sentence_id>33890</sentence_id>
        <char_offset>11</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>2</reference_id>
        <string>Blitzer et al., 2006</string>
        <sentence_id>33825</sentence_id>
        <char_offset>179</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>3</reference_id>
        <string>Finkel and Manning, 2009</string>
        <sentence_id>33838</sentence_id>
        <char_offset>157</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>4</reference_id>
        <string>Jiang and Zhai, 2007</string>
        <sentence_id>33818</sentence_id>
        <char_offset>80</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>4</reference_id>
        <string>Jiang and Zhai, 2007</string>
        <sentence_id>33838</sentence_id>
        <char_offset>51</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>6</reference_id>
        <string>Shimodaira, 2000</string>
        <sentence_id>33826</sentence_id>
        <char_offset>94</char_offset>
      </citation>
    </citations>
  </content>
</document>
