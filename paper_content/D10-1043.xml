<document>
  <filename>D10-1043</filename>
  <authors/>
  <title>None</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>1 Introduction</title>
        <text>Recently syntax-based methods have achieved very promising results and attracted increasing interests in statistical machine translation (SMT) research community due to their ability to provide informative context structure information and convenience in carrying out word transformation and sub-span reordering. Fundamentally, syntax-based SMT views translation as a structural transformation process. Generally speaking, from modeling viewpoint, a syntax-based model tries to convert the source structures into target structures iteratively and recursively while from decoding viewpoint a syntax-based system segments an input tree/forest into many sub-fragments, translates each of them separately, combines the translated sub-fragments and then finds out the best combinations. Therefore, from bilingual viewpoint, we face two fundamental problems: the mapping between bilingual structures and the way of carrying out the target structures combination.
For the first issue, a number of models have been proposed to model the structure mapping between tree and string (Galley et al., 2004; Liu et al., 2006; Yamada and Knight, 2001; DeNeefe and Knight, 2009) and between tree and tree (Eisner, 2003; Zhang et al., 2007 &amp; 2008; Liu et al., 2009). However, one of the major challenges is that all the current models only allow one-to-one mapping from one source frontier non-terminal node (Galley et al., 2004) to one target frontier non-terminal node in a bilingual translation rule. Therefore, all those translation equivalents with one-to-many frontier non-terminal node mapping cannot be covered by the current state-of-the-art models. This may largely compromise the modeling ability of translation rules. For the second problem, currently, the combination is driven by only the source side (both tree-to-string model and tree-to-tree model only check the source span compatibility when combining different target structures in decoding) or only the
target side (string to tree model). There is no well study in considering both the source side information and the compatibility between different target syntactic structures during combination. In addition, it is well known that the traditional tree-to-tree models suffer heavily from the data sparseness issue in training and the spurious-ambiguity translation path issue (the same translation with different syntactic structures) in decoding.
In addition, because of the performance limitation of automatic syntactic parser, researchers propose using packed forest (Tomita, 1987; Klein and Manning, 2001; Huang, 2008) 1 instead of 1-best parse tree to carry out training (Mi and Huang, 2008) and decoding (Mi et al., 2008) in order to reduce the side effect caused by parsing errors of the one-best tree. However, when we apply the tree-to-tree model to the bilingual forest structures, both training and decoding become very complicated.
In this paper, to address the first issue, we propose a framework to model the non-isomorphic translation process from source tree fragment to target tree sequence, allowing any one source frontier non-terminal node to be translated into any number of target frontier non-terminal nodes. For the second issue, we propose a technology to model the combination task by considering both sides&#8217; syntactic structure information. We evaluate and integrate the two technologies into forest-based tree to tree sequence translation. Experimental results on the NIST-2003 and NIST-2005 Chinese-English translation tasks show that our methods significantly outperform the forest-based tree to string and previous tree to tree models as well as the phrase-based model.
The remaining of the paper is organized as following. Section 2 reviews the related work. In section 3 and section 4, we discuss the proposed forest-based rule extraction (non-isomorphic mapping) and decoding algorithms (target syntax information usage). Finally we report the experimental results in section 5 and conclude the paper in section 6.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Recently syntax-based methods have achieved very promising results and attracted increasing interests in statistical machine translation (SMT) research community due to their ability to provide informative context structure information and convenience in carrying out word transformation and sub-span reordering.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Fundamentally, syntax-based SMT views translation as a structural transformation process.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Generally speaking, from modeling viewpoint, a syntax-based model tries to convert the source structures into target structures iteratively and recursively while from decoding viewpoint a syntax-based system segments an input tree/forest into many sub-fragments, translates each of them separately, combines the translated sub-fragments and then finds out the best combinations.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, from bilingual viewpoint, we face two fundamental problems: the mapping between bilingual structures and the way of carrying out the target structures combination.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For the first issue, a number of models have been proposed to model the structure mapping between tree and string (Galley et al., 2004; Liu et al., 2006; Yamada and Knight, 2001; DeNeefe and Knight, 2009) and between tree and tree (Eisner, 2003; Zhang et al., 2007 &amp; 2008; Liu et al., 2009).</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, one of the major challenges is that all the current models only allow one-to-one mapping from one source frontier non-terminal node (Galley et al., 2004) to one target frontier non-terminal node in a bilingual translation rule.</text>
              <doc_id>5</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, all those translation equivalents with one-to-many frontier non-terminal node mapping cannot be covered by the current state-of-the-art models.</text>
              <doc_id>6</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This may largely compromise the modeling ability of translation rules.</text>
              <doc_id>7</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>For the second problem, currently, the combination is driven by only the source side (both tree-to-string model and tree-to-tree model only check the source span compatibility when combining different target structures in decoding) or only the</text>
              <doc_id>8</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>target side (string to tree model).</text>
              <doc_id>9</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>There is no well study in considering both the source side information and the compatibility between different target syntactic structures during combination.</text>
              <doc_id>10</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In addition, it is well known that the traditional tree-to-tree models suffer heavily from the data sparseness issue in training and the spurious-ambiguity translation path issue (the same translation with different syntactic structures) in decoding.</text>
              <doc_id>11</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In addition, because of the performance limitation of automatic syntactic parser, researchers propose using packed forest (Tomita, 1987; Klein and Manning, 2001; Huang, 2008) 1 instead of 1-best parse tree to carry out training (Mi and Huang, 2008) and decoding (Mi et al., 2008) in order to reduce the side effect caused by parsing errors of the one-best tree.</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, when we apply the tree-to-tree model to the bilingual forest structures, both training and decoding become very complicated.</text>
              <doc_id>13</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this paper, to address the first issue, we propose a framework to model the non-isomorphic translation process from source tree fragment to target tree sequence, allowing any one source frontier non-terminal node to be translated into any number of target frontier non-terminal nodes.</text>
              <doc_id>14</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For the second issue, we propose a technology to model the combination task by considering both sides&#8217; syntactic structure information.</text>
              <doc_id>15</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We evaluate and integrate the two technologies into forest-based tree to tree sequence translation.</text>
              <doc_id>16</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Experimental results on the NIST-2003 and NIST-2005 Chinese-English translation tasks show that our methods significantly outperform the forest-based tree to string and previous tree to tree models as well as the phrase-based model.</text>
              <doc_id>17</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The remaining of the paper is organized as following.</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Section 2 reviews the related work.</text>
              <doc_id>19</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In section 3 and section 4, we discuss the proposed forest-based rule extraction (non-isomorphic mapping) and decoding algorithms (target syntax information usage).</text>
              <doc_id>20</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Finally we report the experimental results in section 5 and conclude the paper in section 6.</text>
              <doc_id>21</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>2 Related Work</title>
        <text>Much effort has been done in the syntax-based translation modeling. Yamada and Knight (2001) propose
1 A packed forest is a compact representation of a set of trees
with sharing substructures; formally, it is defined as a triple a triple&#65533; &#65533;,&#65533;,&#65533; &#65533;, where &#65533; is non-terminal node set, &#65533; is hyper-edge set and &#65533; is leaf node set (i.e. all sentence words). Every node in &#65533; covers a consecutive sequence of leaf, every hyper-edge in &#65533; connect the father node to its children nodes as in a tree. Figure 8 is a packed forest contains two trees.
a string to tree model. Galley et al. (2004) propose the GHKM scheme to model the string-to-tree mapping. Liu et al. (2006) propose a tree-to-string translation model. Liu et al. (2007) propose the tree sequence to string model to capture rules covered by continuous sequence of trees. Shieber (2007), De- Neefe and Knight (2009) and Carreras and Collins (2009) propose synchronous tree adjoin grammar to capture more tree-string mapping beyond the GHKM scheme. Zhang et al. (2009a) propose the concept of virtual node to reform a tree sequence as a tree, and design efficient algorithms for tree sequence model in forest context. All these works only consider either the source side or the target side syntax information.
To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. Zhang et al. (2008) propose tree sequence-based tree-to-tree modeling. Liu et al. (2009) propose efficient algorithms for tree-to-tree model in the forest-based training and decoding scheme. One common limitation of the above works is they only allow the one-to-one mapping between each non-terminal frontier node, and thus they suffer from the issue of rule coverage. On the other hand, due to the data sparseness issue and model coverage issue, previous tree-to-tree (Zhang et al., 2008; Liu et al., 2009) decoder has to rely solely on the span information or source side information to combine the target syntactic structures, without checking the compatibility of the merging nodes, in order not to fail many translation paths. Thus, this solution fails to effectively utilize the target structure information.
To address this issue, tree sequence (Liu et al., 2007; Zhang et al., 2008) and virtual node (Zhang et al., 2009a) are two concepts with promising results reported. In this paper, with the help of these two concepts, we propose a novel framework to solve the one-to-many non-isomorphic mapping issue. In addition, our proposed solution of using target syntax information enables our forest-based tree-to-tree sequence translation decoding algorithm to not only capture bilingual forest information but also have almost the same complexity as forest-based tree-to-string translation. This reduces the time/space complexity exponentially.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Much effort has been done in the syntax-based translation modeling.</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Yamada and Knight (2001) propose</text>
              <doc_id>23</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 A packed forest is a compact representation of a set of trees</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>with sharing substructures; formally, it is defined as a triple a triple&#65533; &#65533;,&#65533;,&#65533; &#65533;, where &#65533; is non-terminal node set, &#65533; is hyper-edge set and &#65533; is leaf node set (i.e. all sentence words).</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Every node in &#65533; covers a consecutive sequence of leaf, every hyper-edge in &#65533; connect the father node to its children nodes as in a tree.</text>
              <doc_id>26</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Figure 8 is a packed forest contains two trees.</text>
              <doc_id>27</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>a string to tree model.</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Galley et al. (2004) propose the GHKM scheme to model the string-to-tree mapping.</text>
              <doc_id>29</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Liu et al. (2006) propose a tree-to-string translation model.</text>
              <doc_id>30</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Liu et al. (2007) propose the tree sequence to string model to capture rules covered by continuous sequence of trees.</text>
              <doc_id>31</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Shieber (2007), De- Neefe and Knight (2009) and Carreras and Collins (2009) propose synchronous tree adjoin grammar to capture more tree-string mapping beyond the GHKM scheme.</text>
              <doc_id>32</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Zhang et al. (2009a) propose the concept of virtual node to reform a tree sequence as a tree, and design efficient algorithms for tree sequence model in forest context.</text>
              <doc_id>33</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>All these works only consider either the source side or the target side syntax information.</text>
              <doc_id>34</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level.</text>
              <doc_id>35</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Zhang et al. (2008) propose tree sequence-based tree-to-tree modeling.</text>
              <doc_id>36</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Liu et al. (2009) propose efficient algorithms for tree-to-tree model in the forest-based training and decoding scheme.</text>
              <doc_id>37</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>One common limitation of the above works is they only allow the one-to-one mapping between each non-terminal frontier node, and thus they suffer from the issue of rule coverage.</text>
              <doc_id>38</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>On the other hand, due to the data sparseness issue and model coverage issue, previous tree-to-tree (Zhang et al., 2008; Liu et al., 2009) decoder has to rely solely on the span information or source side information to combine the target syntactic structures, without checking the compatibility of the merging nodes, in order not to fail many translation paths.</text>
              <doc_id>39</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Thus, this solution fails to effectively utilize the target structure information.</text>
              <doc_id>40</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To address this issue, tree sequence (Liu et al., 2007; Zhang et al., 2008) and virtual node (Zhang et al., 2009a) are two concepts with promising results reported.</text>
              <doc_id>41</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, with the help of these two concepts, we propose a novel framework to solve the one-to-many non-isomorphic mapping issue.</text>
              <doc_id>42</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In addition, our proposed solution of using target syntax information enables our forest-based tree-to-tree sequence translation decoding algorithm to not only capture bilingual forest information but also have almost the same complexity as forest-based tree-to-string translation.</text>
              <doc_id>43</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This reduces the time/space complexity exponentially.</text>
              <doc_id>44</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>3 Tree to Tree Sequence Rules</title>
        <text>The motivation of introducing tree to tree sequence rules is to add target syntax information to tree-to-string rules. Following, we first briefly review the definition of tree-to-string rules, and then describe the tree-to-tree sequence rules.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The motivation of introducing tree to tree sequence rules is to add target syntax information to tree-to-string rules.</text>
              <doc_id>45</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Following, we first briefly review the definition of tree-to-string rules, and then describe the tree-to-tree sequence rules.</text>
              <doc_id>46</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Tree to String Rules</title>
            <text>ADVP
AD
VP
&#21162; &#21147; (try hard to) VP
VV
&#23398; &#20064; (study)
try hard to study
is no corresponding sub-tree covering and only covering it in the target side.
Given the example rules in Fig. 2, what are their corresponding rules with target syntax information? The answer is that the previous tree or tree sequence-based models fail to model the Rule 1 and Rule 2 at Fig. 2, since at frontier node level they only allow one-to-one node mapping but the solution is one-to-many non-terminal frontier node mapping. The concept of &#8220;virtual node&#8221; (Zhang et al. 2009a) is a solution to this issue. To facilitate discussion, we first introduce three concepts.
Fig. 1. A word-aligned sentence pair with source tree
Fig. 3. A word-aligned bi-parsed tree
Fig. 2 Examples of tree to string rules
Fig. 2 illustrates the examples of tree to string rules extracted from Fig. 1. The tree-to-string rule is very simple. Its source side is a sub-tree of source parse tree and its target side is a string with only one variable/non-terminal X. The source side and the target side is translation of each other with the constraint of word alignments. Please note that there is no any target syntactic or linguistic information used in the tree-to-string model.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>ADVP</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>AD</text>
                  <doc_id>48</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VP</text>
                  <doc_id>49</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#21162; &#21147; (try hard to) VP</text>
                  <doc_id>50</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VV</text>
                  <doc_id>51</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#23398; &#20064; (study)</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>try hard to study</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>is no corresponding sub-tree covering and only covering it in the target side.</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Given the example rules in Fig.</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>2, what are their corresponding rules with target syntax information?</text>
                  <doc_id>56</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The answer is that the previous tree or tree sequence-based models fail to model the Rule 1 and Rule 2 at Fig.</text>
                  <doc_id>57</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>2, since at frontier node level they only allow one-to-one node mapping but the solution is one-to-many non-terminal frontier node mapping.</text>
                  <doc_id>58</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The concept of &#8220;virtual node&#8221; (Zhang et al. 2009a) is a solution to this issue.</text>
                  <doc_id>59</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>To facilitate discussion, we first introduce three concepts.</text>
                  <doc_id>60</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Fig.</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>1.</text>
                  <doc_id>62</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>A word-aligned sentence pair with source tree</text>
                  <doc_id>63</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Fig.</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>3.</text>
                  <doc_id>65</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>A word-aligned bi-parsed tree</text>
                  <doc_id>66</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Fig.</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>2 Examples of tree to string rules</text>
                  <doc_id>68</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Fig.</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>2 illustrates the examples of tree to string rules extracted from Fig.</text>
                  <doc_id>70</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>1.</text>
                  <doc_id>71</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The tree-to-string rule is very simple.</text>
                  <doc_id>72</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Its source side is a sub-tree of source parse tree and its target side is a string with only one variable/non-terminal X.</text>
                  <doc_id>73</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The source side and the target side is translation of each other with the constraint of word alignments.</text>
                  <doc_id>74</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Please note that there is no any target syntactic or linguistic information used in the tree-to-string model.</text>
                  <doc_id>75</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Tree to Tree Sequence Rules</title>
            <text>It is more challenging when extracting rules with target tree structure as constraint. Fig. 3 extends Fig. 1 with target tree structure. The problem is that, given a source tree node, we are able to find its target string translation, but these target string may not form a linguistic sub-tree. For example, in Fig. 3, the source tree node &#8220;ADVP&#8221; in solid eclipse is translated to &#8220;try hard to&#8221; in the target sentence, but there Fig. 4. A restructured tree with a virtual span root
&#8226; Def. 1. The &#8220;node sequence&#8221; is a sequence of nodes (either leaf or internal nodes) covering a consecutive span. For example, in Fig 3, &#8220;VBP RB TO&#8221; and &#8220;VBP ADVP TO&#8221; are two &#8220;node sequence&#8221; covering the same span &#8220;try hard to&#8221;.
&#8226; Def. 2. The &#8220;root node sequence&#8221; of a span is such a node sequence that any node in this sequence could not be a child of a node in other node sequence of the span. Intuitively, the &#8220;root node sequence&#8221; of a span is the node sequence with the highest topology level. For example, &#8220;VBP ADVP TO&#8221; is the &#8220;root node sequence&#8221; of the span of &#8220;try hard to&#8221;. It is easy to prove that given any span, there exist one and only one &#8220;root node sequence&#8221;.
&#8226; Def. 3. The &#8220;span root&#8221; of a span is such a node that if the &#8220;root node sequence&#8221; contains only one tree node, then the &#8220;span root&#8221; is this tree node; otherwise, the &#8220;span root&#8221; is the virtual father node (Zhang et al., 2009a) of the &#8220;root node sequence&#8221;. Fig. 4 illustrates the reformed Fig. 3 by introducing the virtual node &#8220;VBP+ADVP+TO&#8221; as the &#8220;span root&#8221; of the span of &#8220;try hard to&#8221;.
The &#8220;span root&#8221; facilitates us to extract rules with target side structure information. Given a sub-tree of the source tree, we have a set of non-terminal frontier nodes. For each such frontier node, we can find its corresponding target &#8220;span root&#8221;. If the &#8220;span root&#8221; is a virtual node, then we add it into the target tree as a virtual segmentation joint point. After adding the &#8220;span root&#8221; as joint point, we are able to ensure that each frontier source node has only one corresponding target node, then we can use any traditional rule extraction algorithm to extract rules, including those rules with one-to-many non-terminal frontier mappings.
Fig. 5 lists the corresponding rules with target structure information of the tree-to-string rules in Fig 2. All the three rules cannot be extracted by previous tree-to-tree mapping methods (Liu et al., 2009). The previous tree-sequence-based methods (Zhang et al., 2008; Zhang et al., 2009a) can extracted rule 3 since they allow one-to-many mapping in root node level. But they cannot extract rule 1 and rule 2. Therefore, for any tree-to-string rule, our method can always find the corresponding tree-to-tree sequence rule. As a result, our rule coverage is the same as tree-to-string framework while our rules contain more informative target syntax information. Later we will show that using our decoding algorithm the tree-to-tree sequence search space is exponentially reduced to the same as tree-to-string search space. That is to say, we do not need to worry about the exponential search space issue of tree-to-tree sequence model existing in previous work.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>It is more challenging when extracting rules with target tree structure as constraint.</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Fig.</text>
                  <doc_id>77</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>3 extends Fig.</text>
                  <doc_id>78</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>1 with target tree structure.</text>
                  <doc_id>79</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The problem is that, given a source tree node, we are able to find its target string translation, but these target string may not form a linguistic sub-tree.</text>
                  <doc_id>80</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For example, in Fig.</text>
                  <doc_id>81</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>3, the source tree node &#8220;ADVP&#8221; in solid eclipse is translated to &#8220;try hard to&#8221; in the target sentence, but there Fig.</text>
                  <doc_id>82</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>4.</text>
                  <doc_id>83</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>A restructured tree with a virtual span root</text>
                  <doc_id>84</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Def.</text>
                  <doc_id>85</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>1.</text>
                  <doc_id>86</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The &#8220;node sequence&#8221; is a sequence of nodes (either leaf or internal nodes) covering a consecutive span.</text>
                  <doc_id>87</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For example, in Fig 3, &#8220;VBP RB TO&#8221; and &#8220;VBP ADVP TO&#8221; are two &#8220;node sequence&#8221; covering the same span &#8220;try hard to&#8221;.</text>
                  <doc_id>88</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Def.</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>2.</text>
                  <doc_id>90</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The &#8220;root node sequence&#8221; of a span is such a node sequence that any node in this sequence could not be a child of a node in other node sequence of the span.</text>
                  <doc_id>91</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Intuitively, the &#8220;root node sequence&#8221; of a span is the node sequence with the highest topology level.</text>
                  <doc_id>92</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For example, &#8220;VBP ADVP TO&#8221; is the &#8220;root node sequence&#8221; of the span of &#8220;try hard to&#8221;.</text>
                  <doc_id>93</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>It is easy to prove that given any span, there exist one and only one &#8220;root node sequence&#8221;.</text>
                  <doc_id>94</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Def.</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>3.</text>
                  <doc_id>96</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The &#8220;span root&#8221; of a span is such a node that if the &#8220;root node sequence&#8221; contains only one tree node, then the &#8220;span root&#8221; is this tree node; otherwise, the &#8220;span root&#8221; is the virtual father node (Zhang et al., 2009a) of the &#8220;root node sequence&#8221;.</text>
                  <doc_id>97</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Fig.</text>
                  <doc_id>98</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>4 illustrates the reformed Fig.</text>
                  <doc_id>99</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>3 by introducing the virtual node &#8220;VBP+ADVP+TO&#8221; as the &#8220;span root&#8221; of the span of &#8220;try hard to&#8221;.</text>
                  <doc_id>100</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The &#8220;span root&#8221; facilitates us to extract rules with target side structure information.</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Given a sub-tree of the source tree, we have a set of non-terminal frontier nodes.</text>
                  <doc_id>102</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For each such frontier node, we can find its corresponding target &#8220;span root&#8221;.</text>
                  <doc_id>103</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>If the &#8220;span root&#8221; is a virtual node, then we add it into the target tree as a virtual segmentation joint point.</text>
                  <doc_id>104</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>After adding the &#8220;span root&#8221; as joint point, we are able to ensure that each frontier source node has only one corresponding target node, then we can use any traditional rule extraction algorithm to extract rules, including those rules with one-to-many non-terminal frontier mappings.</text>
                  <doc_id>105</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Fig.</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>5 lists the corresponding rules with target structure information of the tree-to-string rules in Fig 2.</text>
                  <doc_id>107</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>All the three rules cannot be extracted by previous tree-to-tree mapping methods (Liu et al., 2009).</text>
                  <doc_id>108</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The previous tree-sequence-based methods (Zhang et al., 2008; Zhang et al., 2009a) can extracted rule 3 since they allow one-to-many mapping in root node level.</text>
                  <doc_id>109</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>But they cannot extract rule 1 and rule 2.</text>
                  <doc_id>110</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, for any tree-to-string rule, our method can always find the corresponding tree-to-tree sequence rule.</text>
                  <doc_id>111</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>As a result, our rule coverage is the same as tree-to-string framework while our rules contain more informative target syntax information.</text>
                  <doc_id>112</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Later we will show that using our decoding algorithm the tree-to-tree sequence search space is exponentially reduced to the same as tree-to-string search space.</text>
                  <doc_id>113</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>That is to say, we do not need to worry about the exponential search space issue of tree-to-tree sequence model existing in previous work.</text>
                  <doc_id>114</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Rule Extraction in Tree Context</title>
            <text>Given a word aligned tree pair, we first extract the set of minimum tree to string rules (Galley et al. 2004), then for each tree-to-string rule, we can easily extract its corresponding tree-to-tree sequence rule by introducing the virtual span root node. After that, we generate the composite rules by iteratively combining small rules.
Fig. 5. Tree-to-tree sequence rules Fig. 6. Rule combination and virtual node removing
Please note that in generating composite rules, if the joint node is a virtual node, we have to recover the original link and remove this virtual node to avoid unnecessary ambiguity. Fig. 6 illustrates the combination process of rule 2 and rule 3 in Fig. 5. As a result, all of our extract rules do not contain any internal virtual nodes.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Given a word aligned tree pair, we first extract the set of minimum tree to string rules (Galley et al. 2004), then for each tree-to-string rule, we can easily extract its corresponding tree-to-tree sequence rule by introducing the virtual span root node.</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>After that, we generate the composite rules by iteratively combining small rules.</text>
                  <doc_id>116</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Fig.</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>5.</text>
                  <doc_id>118</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Tree-to-tree sequence rules Fig.</text>
                  <doc_id>119</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>6.</text>
                  <doc_id>120</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Rule combination and virtual node removing</text>
                  <doc_id>121</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Please note that in generating composite rules, if the joint node is a virtual node, we have to recover the original link and remove this virtual node to avoid unnecessary ambiguity.</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Fig.</text>
                  <doc_id>123</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>6 illustrates the combination process of rule 2 and rule 3 in Fig.</text>
                  <doc_id>124</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>5.</text>
                  <doc_id>125</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>As a result, all of our extract rules do not contain any internal virtual nodes.</text>
                  <doc_id>126</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>3.4 Rule Extraction in Forest Context</title>
            <text>In forest pair context, we also first generate the minimum tree-to-string rule set as Mi et al. (2008), and for each tree-to-string rule, we find its corresponding tree-to-tree sequence rules, and then do rule composition.
In tree pair context, given a tree-to-string rule, there is one and only one corresponding tree-to-tree sequence rule. But in forest pair context, given one such tree-to-string rule, there are many corresponding tree-to-tree sequence rules. All these sub-trees form one or more sub-forests 2 of the entire big target forest. If we can identify the sub-forests, i.e., all of the hyper-edges of the sub-forests, we can retrieve all the sub-trees from the sub-forests as the target sides of the corresponding tree-to-tree sequence rules.
Given a source sub-tree, we can obtain the target root span where the target sub-forests start and the frontier spans where the target sub-forests stop. To indentify all the hyper-edges in the sub-forests, we start from every node covering the root span, traverse from top to down, mark all the hyper-edges visited and stop at the node if its span is a sub-span of one of the forest frontier spans or if it is a word node. The reason we stop at the node once it fell into a frontier span (i.e. the span of the node is a sub-span of the frontier span) is to guarantee that given any frontier span, we could stop at the &#8220;root node sequence&#8221; of this span by Def. 2.
For example, Fig. 7 is a source sub-tree of rule 2 in Fig. 5 and the circled part in Fig. 8 is one of its corresponding target sub-forests. Its corresponding target root span is [1,4] (corresponding to source root &#8220;VP&#8221; ) and its corresponding target frontier span is {[1,3], study[4,4]}. Now given the target forest, we start from node VP[1,4] and traverse from top to down, finally stop at following nodes: VBP[1,1], ADVP[2,2], TO[3,3], study .
2 All the sub-forests cover the same span. But their roots have
different grammar tags as the roots&#8217; names. The root may be a virtual span root node in the case of the one-to-many frontier non-terminal node mappings.
Please note that the starting root node must be a single node, being either a normal forest node or a virtual &#8220;span root&#8221; node. The virtual &#8220;span root&#8221; node serves as the frontier node of upper rules and root node of the currently being extracted rules. Because we extract rules in a top-to-down manner, the necessary virtual &#8220;span root&#8221; node for current sub-forest has already been added into the global forest when extracting upper level rules.
Figure 7. A source sub-tree in rule 2
Fig. 8. The corresponding target sub-forest for the tree of Figure 7.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In forest pair context, we also first generate the minimum tree-to-string rule set as Mi et al. (2008), and for each tree-to-string rule, we find its corresponding tree-to-tree sequence rules, and then do rule composition.</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In tree pair context, given a tree-to-string rule, there is one and only one corresponding tree-to-tree sequence rule.</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>But in forest pair context, given one such tree-to-string rule, there are many corresponding tree-to-tree sequence rules.</text>
                  <doc_id>129</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>All these sub-trees form one or more sub-forests 2 of the entire big target forest.</text>
                  <doc_id>130</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>If we can identify the sub-forests, i.e., all of the hyper-edges of the sub-forests, we can retrieve all the sub-trees from the sub-forests as the target sides of the corresponding tree-to-tree sequence rules.</text>
                  <doc_id>131</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Given a source sub-tree, we can obtain the target root span where the target sub-forests start and the frontier spans where the target sub-forests stop.</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To indentify all the hyper-edges in the sub-forests, we start from every node covering the root span, traverse from top to down, mark all the hyper-edges visited and stop at the node if its span is a sub-span of one of the forest frontier spans or if it is a word node.</text>
                  <doc_id>133</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The reason we stop at the node once it fell into a frontier span (i.e. the span of the node is a sub-span of the frontier span) is to guarantee that given any frontier span, we could stop at the &#8220;root node sequence&#8221; of this span by Def.</text>
                  <doc_id>134</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>2.</text>
                  <doc_id>135</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For example, Fig.</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>7 is a source sub-tree of rule 2 in Fig.</text>
                  <doc_id>137</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>5 and the circled part in Fig.</text>
                  <doc_id>138</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>8 is one of its corresponding target sub-forests.</text>
                  <doc_id>139</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Its corresponding target root span is [1,4] (corresponding to source root &#8220;VP&#8221; ) and its corresponding target frontier span is {[1,3], study[4,4]}.</text>
                  <doc_id>140</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Now given the target forest, we start from node VP[1,4] and traverse from top to down, finally stop at following nodes: VBP[1,1], ADVP[2,2], TO[3,3], study .</text>
                  <doc_id>141</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 All the sub-forests cover the same span.</text>
                  <doc_id>142</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>But their roots have</text>
                  <doc_id>143</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>different grammar tags as the roots&#8217; names.</text>
                  <doc_id>144</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The root may be a virtual span root node in the case of the one-to-many frontier non-terminal node mappings.</text>
                  <doc_id>145</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Please note that the starting root node must be a single node, being either a normal forest node or a virtual &#8220;span root&#8221; node.</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The virtual &#8220;span root&#8221; node serves as the frontier node of upper rules and root node of the currently being extracted rules.</text>
                  <doc_id>147</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Because we extract rules in a top-to-down manner, the necessary virtual &#8220;span root&#8221; node for current sub-forest has already been added into the global forest when extracting upper level rules.</text>
                  <doc_id>148</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 7.</text>
                  <doc_id>149</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A source sub-tree in rule 2</text>
                  <doc_id>150</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Fig.</text>
                  <doc_id>151</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>8.</text>
                  <doc_id>152</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The corresponding target sub-forest for the tree of Figure 7.</text>
                  <doc_id>153</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>4</index>
            <title>3.5 Fractional Count of Rule</title>
            <text>Following Mi and Huang (2008) and Liu et al. (2009), we assign a fractional count to a rule to measure how likely it appears given the context of the forest pair. In following equation, &#8220;S&#8221; means source sub-tree, &#8220;T&#8221; means target sub-tree, &#8220;SF&#8221; is source forest and &#8220;TF&#8221; is the target forest.
&#65533;&#65533;&#65533;, &#65533; |&#65533;&#65533;, &#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;|&#65533;&#65533;, &#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;|&#65533;&#65533;, &#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;|&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;|&#65533;&#65533;&#65533;
The above equation means the fractional count of a source-target tree pair is just the product of each of their fractional count in corresponding forest context in following equation.
&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; |&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;
&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#8719;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533; &#65533; &#8719;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;
where &#65533; and &#65533; are the outside and inside probabilities. In addition, if a sub-tree root is a virtual node (formed by a root node sequence), then we use following equation to approximate the outside probability of the virtual node.
&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533; &#65533;</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Following Mi and Huang (2008) and Liu et al. (2009), we assign a fractional count to a rule to measure how likely it appears given the context of the forest pair.</text>
                  <doc_id>154</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In following equation, &#8220;S&#8221; means source sub-tree, &#8220;T&#8221; means target sub-tree, &#8220;SF&#8221; is source forest and &#8220;TF&#8221; is the target forest.</text>
                  <doc_id>155</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533;&#65533;&#65533;, &#65533; |&#65533;&#65533;, &#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;|&#65533;&#65533;, &#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;|&#65533;&#65533;, &#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;|&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;|&#65533;&#65533;&#65533;</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The above equation means the fractional count of a source-target tree pair is just the product of each of their fractional count in corresponding forest context in following equation.</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; |&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#8719;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533; &#65533; &#8719;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where &#65533; and &#65533; are the outside and inside probabilities.</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, if a sub-tree root is a virtual node (formed by a root node sequence), then we use following equation to approximate the outside probability of the virtual node.</text>
                  <doc_id>161</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533; &#65533;</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>4 Decoding</title>
        <text># &#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533; &#65533;&#65533;
&#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533; &#65533;&#65533;</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text># &#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533; &#65533;&#65533;</text>
              <doc_id>163</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533; &#65533;&#65533;</text>
              <doc_id>164</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Traditional Forest-based Decoding</title>
            <text>A typical translation process of a forest-based system is to first convert the source packed forest into a target translation forest, and then apply search algorithm to find the best translation result from this target translation forest (Mi et al., 2008).
For the tree-to-string model, the forest conversion process is as following: given an input packed forest, we do pattern matching (Zhang et al., 2009b) with the source side structures in the rule set. For each matched rule, we establish its target side as a hyper-edge in the target forest.
Fig. 9. A forest conversion step in a tree to string model
Fig. 9 exemplifies a conversion step in the tree to string model. A sub-tree structure with two hyper-edge &#8220;VP[2,4] =&gt; ADVP[2,2] VP[3,4]&#8221; and &#8220;VP[3,4] =&gt; ADVP[3,4] VP[4,4]&#8221; is converted into a target hyper-edge &#8220;X-VP[2,4] =&gt; X-ADVP[3,3] X-ADVP[2,2] X-VP[4,4] &#8221;. The node &#8220;X-VP[4,4]&#8221; in the target forest means that its syntactic label in target forest is &#8220;X&#8221; and it is translated from the source node &#8220;VP[4,4]&#8221; in the source forest. In this target hyper-edge, &#8220;X-ADVP[3,3] X-ADVP[2,2]&#8221; means the translation from source node &#8220;ADVP[3,3]&#8221; is put before the translation from &#8220;ADVP[2,2]&#8221;, representing a structure reordering.
4.2 Toward Bilingual Syntax-aware Translation Generation
As we could see in section 4.1, there is only one kind of non-terminal symbol &#8220;X&#8221; in the target side. It is a big challenge to rely on such a coarse label to generate a translation with fine syntactic quality. For example, a source node may be translated into a &#8220;NP&#8221; (noun phrase) in target side. However, in this rule set with the only symbol &#8220;X&#8221;, it may be merged with upper structure as a &#8220;VP&#8221; (verb phrase) instead, because there is no way to favor one over another. In this case, the target tree does not well model the translation syntactically. In addition, all of the internal structure information in the target side is ignored by the tree-to-string rules.
One natural solution to the above issue is to use the tree to tree/tree sequence model, which have richer target syntax structures for more discriminative probability and finer labels to guide the combination process. However, the tree to tree/tree sequence model may face very severe computational problem and so-called &#8220;spurious ambiguities&#8221; issue. Theoretically, if in the tree-to-tree sequence model-based decoding, we just give a penalty to the incompatible-node combinations instead of pruning out the translation paths, then the set of sentences generated by the tree-to-tree sequence model is identical to that of the tree-to-string model since every tree-to-tree sequence rule can be projected into a tree-to-string rule. Motivated by this, we propose a solution call parallel hypothesis spaces searching to solve the computational and &#8220;spurious ambiguities&#8221; issues mentioned above. In the meanwhile, we can fully utilize the target structure information to guide translation.
We restructure the tree-to-tree sequence rule set by grouping all the rules according to their corresponding tree-to-string rules. This behaves like a &#8220;tree-to-forest&#8221; rule. The &#8220;forest&#8221; encodes all the tree sequences with same corresponding string. With the re-constructed rule set, during decoding, we generate two target translation hypothesis spaces (in the form of packed forests) synchronously by the tree-to-string
rules and tree-to-tree sequence rules, and maintain the projection between them. In other words, we generate hypothesis (searching) from the tree-to-string forest and calculate the probability (evaluating syntax goodness) for each hypothesis by the hyper-edges in the tree-to-tree sequence forest.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>A typical translation process of a forest-based system is to first convert the source packed forest into a target translation forest, and then apply search algorithm to find the best translation result from this target translation forest (Mi et al., 2008).</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For the tree-to-string model, the forest conversion process is as following: given an input packed forest, we do pattern matching (Zhang et al., 2009b) with the source side structures in the rule set.</text>
                  <doc_id>166</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For each matched rule, we establish its target side as a hyper-edge in the target forest.</text>
                  <doc_id>167</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Fig.</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>9.</text>
                  <doc_id>169</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>A forest conversion step in a tree to string model</text>
                  <doc_id>170</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Fig.</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>9 exemplifies a conversion step in the tree to string model.</text>
                  <doc_id>172</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>A sub-tree structure with two hyper-edge &#8220;VP[2,4] =&gt; ADVP[2,2] VP[3,4]&#8221; and &#8220;VP[3,4] =&gt; ADVP[3,4] VP[4,4]&#8221; is converted into a target hyper-edge &#8220;X-VP[2,4] =&gt; X-ADVP[3,3] X-ADVP[2,2] X-VP[4,4] &#8221;.</text>
                  <doc_id>173</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The node &#8220;X-VP[4,4]&#8221; in the target forest means that its syntactic label in target forest is &#8220;X&#8221; and it is translated from the source node &#8220;VP[4,4]&#8221; in the source forest.</text>
                  <doc_id>174</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In this target hyper-edge, &#8220;X-ADVP[3,3] X-ADVP[2,2]&#8221; means the translation from source node &#8220;ADVP[3,3]&#8221; is put before the translation from &#8220;ADVP[2,2]&#8221;, representing a structure reordering.</text>
                  <doc_id>175</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.2 Toward Bilingual Syntax-aware Translation Generation</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As we could see in section 4.1, there is only one kind of non-terminal symbol &#8220;X&#8221; in the target side.</text>
                  <doc_id>177</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It is a big challenge to rely on such a coarse label to generate a translation with fine syntactic quality.</text>
                  <doc_id>178</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For example, a source node may be translated into a &#8220;NP&#8221; (noun phrase) in target side.</text>
                  <doc_id>179</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>However, in this rule set with the only symbol &#8220;X&#8221;, it may be merged with upper structure as a &#8220;VP&#8221; (verb phrase) instead, because there is no way to favor one over another.</text>
                  <doc_id>180</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In this case, the target tree does not well model the translation syntactically.</text>
                  <doc_id>181</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, all of the internal structure information in the target side is ignored by the tree-to-string rules.</text>
                  <doc_id>182</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>One natural solution to the above issue is to use the tree to tree/tree sequence model, which have richer target syntax structures for more discriminative probability and finer labels to guide the combination process.</text>
                  <doc_id>183</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, the tree to tree/tree sequence model may face very severe computational problem and so-called &#8220;spurious ambiguities&#8221; issue.</text>
                  <doc_id>184</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Theoretically, if in the tree-to-tree sequence model-based decoding, we just give a penalty to the incompatible-node combinations instead of pruning out the translation paths, then the set of sentences generated by the tree-to-tree sequence model is identical to that of the tree-to-string model since every tree-to-tree sequence rule can be projected into a tree-to-string rule.</text>
                  <doc_id>185</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Motivated by this, we propose a solution call parallel hypothesis spaces searching to solve the computational and &#8220;spurious ambiguities&#8221; issues mentioned above.</text>
                  <doc_id>186</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In the meanwhile, we can fully utilize the target structure information to guide translation.</text>
                  <doc_id>187</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We restructure the tree-to-tree sequence rule set by grouping all the rules according to their corresponding tree-to-string rules.</text>
                  <doc_id>188</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This behaves like a &#8220;tree-to-forest&#8221; rule.</text>
                  <doc_id>189</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The &#8220;forest&#8221; encodes all the tree sequences with same corresponding string.</text>
                  <doc_id>190</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>With the re-constructed rule set, during decoding, we generate two target translation hypothesis spaces (in the form of packed forests) synchronously by the tree-to-string</text>
                  <doc_id>191</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>rules and tree-to-tree sequence rules, and maintain the projection between them.</text>
                  <doc_id>192</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In other words, we generate hypothesis (searching) from the tree-to-string forest and calculate the probability (evaluating syntax goodness) for each hypothesis by the hyper-edges in the tree-to-tree sequence forest.</text>
                  <doc_id>193</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.3 Parallel Hypothesis Spaces</title>
            <text>the group of its corresponding latent hyper-edges is the small wires inside it.
We rely on the explicit hyper-edges to enumerate possible hypothesis while using the latent hyper-edges to measure its translation probability and syntax goodness. Thus, the complexity of the search space is reduced into the tree-to-string model level, while keeping the target language generation syntactic aware. More importantly, we thoroughly avoid those spurious ambiguities introduced by the tree-to-tree sequence rules.
Fig. 10. Mapping from tree-to-tree sequence into tree-to-string rule
In this subsection, we describe what the parallel search spaces are and how to construct them. As shown at Fig. 10, given a tree-to-tree sequence rule, it is easy to find its corresponding tree-to-string rule by simply ignoring the target inside structure and renaming the root and leaves non-terminal labels into &#8220;X&#8221;. We iterate through the tree-to-tree sequence rule set, find its corresponding tree-to-string rule and then group those rules with the same tree-to-string projection. After that, the original tree-to-tree sequence rule set becomes a set of smaller rule sets. Each of them is indexed by a unique tree-to-string rule.
We apply the tree-to-string rules to generate an explicit target translation forest to represent the target sentences space. At the same time, whenever a tree-to-string rule is applied, we also retrieve its corresponding tree-to-tree sequence rule set and generate a set of latent hyper-edges with fine-grained syntax information. In this case, we have two parallel forests, one with coarse explicit hyper-edges and the other fine and latent. Given a hyper-edge (or a node) in the coarse forest, there are a group of corresponding latent hyper-edges (or nodes) with finer syntax labels in the fine forest. Accordingly, given a tree in the coarse forest, there is a corresponding sub-forest in the latent fine forest. We can view the latent fine forest as imbedded inside the explicit coarse forest. If an explicit hyper-edge is viewed as a big cable, then
Fig. 11. Derivation path and derivation forest
In this subsection, we show exactly how our decoder finds the best result from the parallel spaces. We generate hypothesis by traversing the coarse forest in the parallel spaces with cube-pruning (Huang and Chiang, 2007). Given a newly generated hypothesis, it is affiliated with a derivation path (tree) in the coarse forest and a group of derivation paths (sub-forest) in the finer forest. As shown in Fig. 11, the left part is the derivation path formed by a coarse hyper-edge, consisting the newly-generated sub-tree &#8220;X =&gt; X X X&#8221; connecting with three previously-generated sub paths while the right part is the derivation forest formed by newly-generated finer hyper-edges rooted at &#8220;VP&#8221; and &#8220;S&#8221;, and previously-generated sub-forests.
In this paper, we use the sum of probabilities of all the derivation paths in the finer forest to measure the quality of the candidate translation suggested by the hypothesis. From Fig. 11, we can see there may be more than one corresponding finer forests, it is easy to understand that the sum of all the trees&#8217; probabilities in these finer forests is equal to the sum of the inside probability of all these root nodes of these finer forests. We adopt the dynamic programming to compute the probability of the finer forest: whenever we generate a new hypothesis by concatenating a
coarse hyper-edge and its sub-path, we find its corresponding finer hyper-edges and sub-forests, do the combination and accumulate probabilities from bottom to up. For the coarse hyper-edge, because there is only one label &#8220;X&#8221;, any sub-path could be easily concatenated with upper structure covering the same sub-span without the need of checking label compatibility. While for the finer hyper-edges, we only link the root nodes of sub-forests to upper hyper-edges with the same linking node label. This is to guarantee syntactic goodness. In case there are some leaf nodes of the upper hyper-edges fail to find corresponding sub-forest roots with the same label (e.g. the &#8220;NP&#8221; in red color in the rightmost of Fig 11), we simply link it into the nodes with the least inside probability (among these sub-forests), and at the same time give a penalty score to this combination. If some root nodes of some sub-forest still cannot find upper leaf nodes to concatenate (e.g. the &#8220;CP&#8221; in red color in Fig. 11), we simply ignore them. After the combination process, it is straightforward to accumulate the inside probability dynamically from bottom up.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>the group of its corresponding latent hyper-edges is the small wires inside it.</text>
                  <doc_id>194</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We rely on the explicit hyper-edges to enumerate possible hypothesis while using the latent hyper-edges to measure its translation probability and syntax goodness.</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Thus, the complexity of the search space is reduced into the tree-to-string model level, while keeping the target language generation syntactic aware.</text>
                  <doc_id>196</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>More importantly, we thoroughly avoid those spurious ambiguities introduced by the tree-to-tree sequence rules.</text>
                  <doc_id>197</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Fig.</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>10.</text>
                  <doc_id>199</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Mapping from tree-to-tree sequence into tree-to-string rule</text>
                  <doc_id>200</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In this subsection, we describe what the parallel search spaces are and how to construct them.</text>
                  <doc_id>201</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As shown at Fig.</text>
                  <doc_id>202</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>10, given a tree-to-tree sequence rule, it is easy to find its corresponding tree-to-string rule by simply ignoring the target inside structure and renaming the root and leaves non-terminal labels into &#8220;X&#8221;.</text>
                  <doc_id>203</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We iterate through the tree-to-tree sequence rule set, find its corresponding tree-to-string rule and then group those rules with the same tree-to-string projection.</text>
                  <doc_id>204</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>After that, the original tree-to-tree sequence rule set becomes a set of smaller rule sets.</text>
                  <doc_id>205</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Each of them is indexed by a unique tree-to-string rule.</text>
                  <doc_id>206</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We apply the tree-to-string rules to generate an explicit target translation forest to represent the target sentences space.</text>
                  <doc_id>207</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>At the same time, whenever a tree-to-string rule is applied, we also retrieve its corresponding tree-to-tree sequence rule set and generate a set of latent hyper-edges with fine-grained syntax information.</text>
                  <doc_id>208</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In this case, we have two parallel forests, one with coarse explicit hyper-edges and the other fine and latent.</text>
                  <doc_id>209</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Given a hyper-edge (or a node) in the coarse forest, there are a group of corresponding latent hyper-edges (or nodes) with finer syntax labels in the fine forest.</text>
                  <doc_id>210</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Accordingly, given a tree in the coarse forest, there is a corresponding sub-forest in the latent fine forest.</text>
                  <doc_id>211</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We can view the latent fine forest as imbedded inside the explicit coarse forest.</text>
                  <doc_id>212</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>If an explicit hyper-edge is viewed as a big cable, then</text>
                  <doc_id>213</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Fig.</text>
                  <doc_id>214</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>11.</text>
                  <doc_id>215</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Derivation path and derivation forest</text>
                  <doc_id>216</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In this subsection, we show exactly how our decoder finds the best result from the parallel spaces.</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We generate hypothesis by traversing the coarse forest in the parallel spaces with cube-pruning (Huang and Chiang, 2007).</text>
                  <doc_id>218</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Given a newly generated hypothesis, it is affiliated with a derivation path (tree) in the coarse forest and a group of derivation paths (sub-forest) in the finer forest.</text>
                  <doc_id>219</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>As shown in Fig.</text>
                  <doc_id>220</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>11, the left part is the derivation path formed by a coarse hyper-edge, consisting the newly-generated sub-tree &#8220;X =&gt; X X X&#8221; connecting with three previously-generated sub paths while the right part is the derivation forest formed by newly-generated finer hyper-edges rooted at &#8220;VP&#8221; and &#8220;S&#8221;, and previously-generated sub-forests.</text>
                  <doc_id>221</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In this paper, we use the sum of probabilities of all the derivation paths in the finer forest to measure the quality of the candidate translation suggested by the hypothesis.</text>
                  <doc_id>222</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>From Fig.</text>
                  <doc_id>223</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>11, we can see there may be more than one corresponding finer forests, it is easy to understand that the sum of all the trees&#8217; probabilities in these finer forests is equal to the sum of the inside probability of all these root nodes of these finer forests.</text>
                  <doc_id>224</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We adopt the dynamic programming to compute the probability of the finer forest: whenever we generate a new hypothesis by concatenating a</text>
                  <doc_id>225</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>coarse hyper-edge and its sub-path, we find its corresponding finer hyper-edges and sub-forests, do the combination and accumulate probabilities from bottom to up.</text>
                  <doc_id>226</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For the coarse hyper-edge, because there is only one label &#8220;X&#8221;, any sub-path could be easily concatenated with upper structure covering the same sub-span without the need of checking label compatibility.</text>
                  <doc_id>227</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>While for the finer hyper-edges, we only link the root nodes of sub-forests to upper hyper-edges with the same linking node label.</text>
                  <doc_id>228</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This is to guarantee syntactic goodness.</text>
                  <doc_id>229</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In case there are some leaf nodes of the upper hyper-edges fail to find corresponding sub-forest roots with the same label (e.g. the &#8220;NP&#8221; in red color in the rightmost of Fig 11), we simply link it into the nodes with the least inside probability (among these sub-forests), and at the same time give a penalty score to this combination.</text>
                  <doc_id>230</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>If some root nodes of some sub-forest still cannot find upper leaf nodes to concatenate (e.g. the &#8220;CP&#8221; in red color in Fig.</text>
                  <doc_id>231</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>11), we simply ignore them.</text>
                  <doc_id>232</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>After the combination process, it is straightforward to accumulate the inside probability dynamically from bottom up.</text>
                  <doc_id>233</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>5 Experiment</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>234</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Experimental Settings</title>
            <text>We evaluate our method on the Chinese-English translation task. We first carry out a series empirical study on a set of parallel data with 30K sentence pairs, and then do experiment on a larger data set to ensure that the effectiveness of our method is consistent across data set of different size. We use the NIST 2002 test set as our dev set, and NIST 2003 and NIST 2005 test sets as our test set. A 3-gram language model is trained on the target side of the training data by the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We train Charniak&#8217;s parser (Charniak, 2000) on CTB5.0 for Chinese and ETB3.0 for English and modify it to output packed forest. GIZA++ (Och and Ney, 2003) and the heuristics &#8220;grow-diag-final-and&#8221; are used to generate m-to-n word alignments. For the MER training (Och, 2003), Koehn&#8217;s MER trainer (Koehn, 2007) is modified for our system. For significance test, we use Zhang et al.&#8217;s implementation (Zhang et al, 2004). Our evaluation metrics is case-sensitive closest BLEU-4 (Papineni et al., 2002). We use following features in our systems: 1) bidirectional tree-to-tree sequence probability, 2) bidirectional tree-to-string probability, 3) bidirectional lexical translation probability, 4) target language model, 5) source tree probability 6) the average number of unmatched nodes in the target forest. 7) the length of the target translation, 8) the number of glue rules used.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We evaluate our method on the Chinese-English translation task.</text>
                  <doc_id>235</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We first carry out a series empirical study on a set of parallel data with 30K sentence pairs, and then do experiment on a larger data set to ensure that the effectiveness of our method is consistent across data set of different size.</text>
                  <doc_id>236</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We use the NIST 2002 test set as our dev set, and NIST 2003 and NIST 2005 test sets as our test set.</text>
                  <doc_id>237</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>A 3-gram language model is trained on the target side of the training data by the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995).</text>
                  <doc_id>238</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We train Charniak&#8217;s parser (Charniak, 2000) on CTB5.0 for Chinese and ETB3.0 for English and modify it to output packed forest.</text>
                  <doc_id>239</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>GIZA++ (Och and Ney, 2003) and the heuristics &#8220;grow-diag-final-and&#8221; are used to generate m-to-n word alignments.</text>
                  <doc_id>240</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>For the MER training (Och, 2003), Koehn&#8217;s MER trainer (Koehn, 2007) is modified for our system.</text>
                  <doc_id>241</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>For significance test, we use Zhang et al.&#8217;s implementation (Zhang et al, 2004).</text>
                  <doc_id>242</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>Our evaluation metrics is case-sensitive closest BLEU-4 (Papineni et al., 2002).</text>
                  <doc_id>243</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>We use following features in our systems: 1) bidirectional tree-to-tree sequence probability, 2) bidirectional tree-to-string probability, 3) bidirectional lexical translation probability, 4) target language model, 5) source tree probability 6) the average number of unmatched nodes in the target forest.</text>
                  <doc_id>244</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>7) the length of the target translation, 8) the number of glue rules used.</text>
                  <doc_id>245</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Empirical Study on Small Data</title>
            <text>We set forest pruning threshold (Mi et al., 2008) to 8 on both source and target forests for rule extraction. For each source sub-tree, we set its height up to 3, width up to 7 and extract up to 10-best target structures. In decoding, we set the pruning threshold to 10 for the input source forest. Table 1 compares the performance in NIST 2003 data set of our method and several state-of-the-art systems as our baseline.
1) MOSES: phrase-based system (Koehn et al.,
2007) 2) FT2S: forest-based tree-to-string system (Mi
and Huang, 2008; Mi et al., 2008) 3) FT2T: forest-based tree-to-tree system (Liu et
al., 2009). 4) FT2TS (1to1): our forest-based tree-to-tree
sequence system, where 1to1 means only one-to-one frontier non-terminal node mapping is allowed, thus the system does not follow our non-isomorphic mapping framework. 5) FT2TS (1toN): our forest-based tree-to-tree
sequence system that allows one-to-many frontier non-terminal node mapping by following our non-isomorphic mapping framework
In addition, our proposed parallel searching space (PSS) technology can be applied to both tree to tree and tree to sequence systems. Thus in table 1, for the tree-to-tree/tree sequence systems, we report two BLEU scores, one uses this technology (withPSS) and one does not (noPSS).
Table 1. Performance comparison of different methods
From Table 1, we can see that:
1) All the syntax-based systems (except FT2T (noPSS) (23.40)) consistently outperform the phrase-based system MOSES significantly ( &#65533; &#65533; 0.01), indicating that syntactic knowledge is very useful to SMT. 2) The PSS technology shows significant performance improvement &#65533;&#65533; &#65533; 0.01&#65533; in all models, which clearly shows effectiveness of the PSS technology in utilizing target structures for target language generation. 3) FT2TS (1toN) significantly outperforms
(&#65533; &#65533; 0.01) FT2TS (1to1) in both cases (noPSS and withPSS). This convincingly shows the effectiveness of our non-isomorphic mapping framework in capturing the non-isomorphic structure translation equivalences. 4) Both FT2TS systems significantly outperform
FT2T( &#65533; &#65533; 0.01). This verifies the effectiveness of tree sequence rules. 5) FT2TS shows different level of performance
improvements over FT2S with the best case having 1.6 (27.70-26.10) BLEU score improvement over FT2S. This suggests that the target structure information is very useful, but we need to find a correct way to effectively utilize it.
Table 2. Statistics on node mapping in forest, where &#8220;1to1&#8221; means the number of nodes in source forest that can be translated into one node in target forest and &#8220;1toN&#8221; means the number of nodes in source forest that have to be translated into more than one node in target forest, where the node refers to non-terminal nodes only
Model # of rules T2S covered
FT2T 295732 26.8%
FT2TS(1to1) 631487 57.1%
FT2TS (1toN) 1945168 100%
Table 3. Statistics of rule coverage, where &#8220;T2S covered&#8221; means the percentage of tree-to-string rules that can be covered by the model
Table 2 studies the node isomorphism between bilingual forest pair. We can see that the non-isomorphic node translation mapping (1toN) accounts for 57.6% (=1.36/(1+1.36)) of all the forest non-terminal nodes with target translation. This means that the one-to-many node mapping is a major issue in structure transformation. It also empirically justifies the importance of our non-isomorphic mapping framework.
Table 3 shows the rule coverage of different bilingual structure mapping model. FT2T only covers 26.8% tree-to-string rules, so it performs worse than FT2S as shown in Table 1. FT2TS (1to1) does not allow one-to-many frontier node mapping, so it could only recover the non-isomorphic node mapping in the root level, while FT2TS (1toN) could make it at both root and leaf levels. Therefore, it is not surprising that in Table 3, FT2TS (1toN) cover many more rules than FT2TS (1to1) because given a source tree, there are many leaves, if any one of them is non-isomorphic, then it could not be covered by the FT2TS (1to1).
Decoding Method
Traditional: FT2TS (1toN) (noPPS) Ours: FT2TS (1toN) (withPPS)
BLEU-4 Speed (sec/sent)
26.30 152.6
27.70 5.22
Table 4. Performance and speed comparison
Table 4 clearly shows the advantage of our decoder over the traditional one. Ours could not only generate better translation result, but also be 152.6/5.22&gt;30 times faster. This mainly attributes to two reasons: 1) one-to-many frontier node mapping equipments the model with more ability to capture more non-isomorphic structure mappings than traditional models, and 2) &#8220;parallel search space&#8221; enables the decoder to fully utilize target syntactic information, but keeping the size of search space the same as that a &#8220;tree to string&#8221; model explores.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We set forest pruning threshold (Mi et al., 2008) to 8 on both source and target forests for rule extraction.</text>
                  <doc_id>246</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For each source sub-tree, we set its height up to 3, width up to 7 and extract up to 10-best target structures.</text>
                  <doc_id>247</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In decoding, we set the pruning threshold to 10 for the input source forest.</text>
                  <doc_id>248</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Table 1 compares the performance in NIST 2003 data set of our method and several state-of-the-art systems as our baseline.</text>
                  <doc_id>249</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1) MOSES: phrase-based system (Koehn et al.,</text>
                  <doc_id>250</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2007) 2) FT2S: forest-based tree-to-string system (Mi</text>
                  <doc_id>251</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and Huang, 2008; Mi et al., 2008) 3) FT2T: forest-based tree-to-tree system (Liu et</text>
                  <doc_id>252</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>al., 2009).</text>
                  <doc_id>253</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>4) FT2TS (1to1): our forest-based tree-to-tree</text>
                  <doc_id>254</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>sequence system, where 1to1 means only one-to-one frontier non-terminal node mapping is allowed, thus the system does not follow our non-isomorphic mapping framework.</text>
                  <doc_id>255</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>5) FT2TS (1toN): our forest-based tree-to-tree</text>
                  <doc_id>256</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>sequence system that allows one-to-many frontier non-terminal node mapping by following our non-isomorphic mapping framework</text>
                  <doc_id>257</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In addition, our proposed parallel searching space (PSS) technology can be applied to both tree to tree and tree to sequence systems.</text>
                  <doc_id>258</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Thus in table 1, for the tree-to-tree/tree sequence systems, we report two BLEU scores, one uses this technology (withPSS) and one does not (noPSS).</text>
                  <doc_id>259</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 1.</text>
                  <doc_id>260</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Performance comparison of different methods</text>
                  <doc_id>261</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>From Table 1, we can see that:</text>
                  <doc_id>262</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1) All the syntax-based systems (except FT2T (noPSS) (23.40)) consistently outperform the phrase-based system MOSES significantly ( &#65533; &#65533; 0.01), indicating that syntactic knowledge is very useful to SMT.</text>
                  <doc_id>263</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>2) The PSS technology shows significant performance improvement &#65533;&#65533; &#65533; 0.01&#65533; in all models, which clearly shows effectiveness of the PSS technology in utilizing target structures for target language generation.</text>
                  <doc_id>264</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>3) FT2TS (1toN) significantly outperforms</text>
                  <doc_id>265</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(&#65533; &#65533; 0.01) FT2TS (1to1) in both cases (noPSS and withPSS).</text>
                  <doc_id>266</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This convincingly shows the effectiveness of our non-isomorphic mapping framework in capturing the non-isomorphic structure translation equivalences.</text>
                  <doc_id>267</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>4) Both FT2TS systems significantly outperform</text>
                  <doc_id>268</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>FT2T( &#65533; &#65533; 0.01).</text>
                  <doc_id>269</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This verifies the effectiveness of tree sequence rules.</text>
                  <doc_id>270</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>5) FT2TS shows different level of performance</text>
                  <doc_id>271</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>improvements over FT2S with the best case having 1.6 (27.70-26.10) BLEU score improvement over FT2S.</text>
                  <doc_id>272</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This suggests that the target structure information is very useful, but we need to find a correct way to effectively utilize it.</text>
                  <doc_id>273</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 2.</text>
                  <doc_id>274</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Statistics on node mapping in forest, where &#8220;1to1&#8221; means the number of nodes in source forest that can be translated into one node in target forest and &#8220;1toN&#8221; means the number of nodes in source forest that have to be translated into more than one node in target forest, where the node refers to non-terminal nodes only</text>
                  <doc_id>275</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Model # of rules T2S covered</text>
                  <doc_id>276</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>FT2T 295732 26.8%</text>
                  <doc_id>277</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>FT2TS(1to1) 631487 57.1%</text>
                  <doc_id>278</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>FT2TS (1toN) 1945168 100%</text>
                  <doc_id>279</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 3.</text>
                  <doc_id>280</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Statistics of rule coverage, where &#8220;T2S covered&#8221; means the percentage of tree-to-string rules that can be covered by the model</text>
                  <doc_id>281</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 2 studies the node isomorphism between bilingual forest pair.</text>
                  <doc_id>282</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We can see that the non-isomorphic node translation mapping (1toN) accounts for 57.6% (=1.36/(1+1.36)) of all the forest non-terminal nodes with target translation.</text>
                  <doc_id>283</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This means that the one-to-many node mapping is a major issue in structure transformation.</text>
                  <doc_id>284</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>It also empirically justifies the importance of our non-isomorphic mapping framework.</text>
                  <doc_id>285</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 3 shows the rule coverage of different bilingual structure mapping model.</text>
                  <doc_id>286</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>FT2T only covers 26.8% tree-to-string rules, so it performs worse than FT2S as shown in Table 1.</text>
                  <doc_id>287</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>FT2TS (1to1) does not allow one-to-many frontier node mapping, so it could only recover the non-isomorphic node mapping in the root level, while FT2TS (1toN) could make it at both root and leaf levels.</text>
                  <doc_id>288</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, it is not surprising that in Table 3, FT2TS (1toN) cover many more rules than FT2TS (1to1) because given a source tree, there are many leaves, if any one of them is non-isomorphic, then it could not be covered by the FT2TS (1to1).</text>
                  <doc_id>289</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Decoding Method</text>
                  <doc_id>290</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Traditional: FT2TS (1toN) (noPPS) Ours: FT2TS (1toN) (withPPS)</text>
                  <doc_id>291</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BLEU-4 Speed (sec/sent)</text>
                  <doc_id>292</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>26.30 152.6</text>
                  <doc_id>293</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>27.70 5.22</text>
                  <doc_id>294</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 4.</text>
                  <doc_id>295</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Performance and speed comparison</text>
                  <doc_id>296</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 4 clearly shows the advantage of our decoder over the traditional one.</text>
                  <doc_id>297</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Ours could not only generate better translation result, but also be 152.6/5.22&gt;30 times faster.</text>
                  <doc_id>298</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This mainly attributes to two reasons: 1) one-to-many frontier node mapping equipments the model with more ability to capture more non-isomorphic structure mappings than traditional models, and 2) &#8220;parallel search space&#8221; enables the decoder to fully utilize target syntactic information, but keeping the size of search space the same as that a &#8220;tree to string&#8221; model explores.</text>
                  <doc_id>299</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>5.3 Results on Larger Data Set</title>
            <text>We also carry out experiment on a larger dataset consisting of the small dataset used in last section and the FBIS corpus. In total, there are 280K parallel sentence pairs with 9.3M Chinese words and 11.8M English words. A 3-gram language model is trained on the target side of the parallel corpus and the GI- GA3 Xinhua portion. We compare our system (FT2TS with 1toN and withPPS) with two state-of-the-art baselines: the phrase-based system MOSES and the forest-based tree-to-string system
implemented by us. Table 5 clearly shows the effectiveness of our method is consistent across small and larger corpora, outperforming FT2S by 1.6-1.8 BLEU and the MOSES by 3.3-4.0 BLEU statistically significantly (p&lt;0.01).
Table 5. Performance on larger data set</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We also carry out experiment on a larger dataset consisting of the small dataset used in last section and the FBIS corpus.</text>
                  <doc_id>300</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In total, there are 280K parallel sentence pairs with 9.3M Chinese words and 11.8M English words.</text>
                  <doc_id>301</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>A 3-gram language model is trained on the target side of the parallel corpus and the GI- GA3 Xinhua portion.</text>
                  <doc_id>302</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We compare our system (FT2TS with 1toN and withPPS) with two state-of-the-art baselines: the phrase-based system MOSES and the forest-based tree-to-string system</text>
                  <doc_id>303</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>implemented by us.</text>
                  <doc_id>304</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Table 5 clearly shows the effectiveness of our method is consistent across small and larger corpora, outperforming FT2S by 1.6-1.8 BLEU and the MOSES by 3.3-4.0 BLEU statistically significantly (p&lt;0.01).</text>
                  <doc_id>305</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 5.</text>
                  <doc_id>306</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Performance on larger data set</text>
                  <doc_id>307</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>6 Conclusions</title>
        <text>In this paper, we propose a framework to address the issue of bilingual non-isomorphic structure mapping and a novel parallel searching space scheme to effectively utilize target syntactic structure information in the context of forest-based tree to tree sequence machine translation. Based on this framework, we design an efficient algorithm to extract tree-to-tree sequence translation rules from word aligned bilingual forest pairs. We also elaborate the parallel searching space-based decoding algorithm and the node label checking scheme, which leads to very efficient decoding speed as fast as the forest-based tree-to-string model does, at the same time is able to utilize informative target structure knowledge. We evaluate our methods on both small and large training data sets and two NIST test sets. Experimental results show our methods statistically significantly outperform the state-of-the-art models across different size of corpora and different test sets. In the future, we are interested in testing our algorithm at forest-based tree sequence to tree sequence translation.
References
Eugene Charniak. 2000. A maximum-entropy inspired
parser. NAACL-00.
Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. Syntax-based language models for statistical machine translation. MT Summit IX. 40&#8211;46.
David Chiang. 2007. Hierarchical phrase-based translation.Computational Linguistics, 33(2).
Steve DeNeefe, Kevin Knight. 2009. Synchronous Tree Adjoining Machine Translation. EMNLP-2009. 727-736.
Jason Eisner. 2003. Learning non-isomorphic tree mappings for MT. ACL-03 (companion volume).
Michel Galley, Mark Hopkins, Kevin Knight and Daniel Marcu. 2004. What&#8217;s in a translation rule? HLT-NAACL-04. 273-280.
Liang Huang. 2008. Forest Reranking: Discriminative Parsing with Non-Local Features. ACL-HLT-08. 586-594 Liang Huang and David Chiang. 2005. Better k-best Parsing. IWPT-05. 53-64
Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. ACL-07. 144&#8211;151
Dan Klein and Christopher D. Manning. 2001. Parsing
and Hypergraphs. IWPT-2001.
Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for M-gram language modeling. ICASSP-95, 181-184
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. ACL-07. 177-180. (poster)
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-String Alignment Template for Statistical Machine Translation. COLING-ACL-06. 609-616.
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 2007. Forest-to-String Statistical Translation Rules. ACL-07. 704-711.
Yang Liu, Yajuan L&#252;, Qun Liu. 2009. Improving Tree-to-Tree Translation with Packed Forests. ACL-09. 558-566
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-based translation. ACL-HLT-08. 192-199.
Haitao Mi and Liang Huang. 2008. Forest-based Translation Rule Extraction. EMNLP-08. 206-214.
Franz J. Och. 2003. Minimum error rate training in statistical machine translation. ACL-03. 160-167.
Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics. 29(1) 19-51.
Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. ACL-02. 311-318.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. ICSLP-02. 901-904.
Masaru Tomita. 1987. An Efficient Augmented-Context-Free Parsing Algorithm. Computational Linguistics 13(1-2): 31-46
Xavier Carreras and Michael Collins. 2009. Non-projective Parsing for Statistical Machine Translation. EMNLP-2009. 200-209.
K. Yamada and K. Knight. 2001. A Syntax-Based Statistical Translation Model. ACL-01. 523-530.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and Chew Lim Tan. 2009a. Forest-based Tree Sequence to String Translation Model. ACL-IJCNLP-09. 172-180.
Hui Zhang, Min Zhang, Haizhou Li, and Chew Lim Tan. 2009b. Fast Translation Rule Matching for Syntax-based Statistical Machine Translation. EMNLP-09. 1037-1045.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Chew Lim Tan and Sheng Li. 2007. A Tree-to-Tree Alignment-based model for statistical Machine translation. MT-Summit-07. 535-542
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, Sheng Li. 2008. A Tree Sequence Alignment-based Tree-to-Tree Translation Model. ACL-HLT-08. 559-567.
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? LREC-04. 2051-2054.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper, we propose a framework to address the issue of bilingual non-isomorphic structure mapping and a novel parallel searching space scheme to effectively utilize target syntactic structure information in the context of forest-based tree to tree sequence machine translation.</text>
              <doc_id>308</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Based on this framework, we design an efficient algorithm to extract tree-to-tree sequence translation rules from word aligned bilingual forest pairs.</text>
              <doc_id>309</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We also elaborate the parallel searching space-based decoding algorithm and the node label checking scheme, which leads to very efficient decoding speed as fast as the forest-based tree-to-string model does, at the same time is able to utilize informative target structure knowledge.</text>
              <doc_id>310</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We evaluate our methods on both small and large training data sets and two NIST test sets.</text>
              <doc_id>311</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Experimental results show our methods statistically significantly outperform the state-of-the-art models across different size of corpora and different test sets.</text>
              <doc_id>312</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In the future, we are interested in testing our algorithm at forest-based tree sequence to tree sequence translation.</text>
              <doc_id>313</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>References</text>
              <doc_id>314</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Eugene Charniak.</text>
              <doc_id>315</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2000.</text>
              <doc_id>316</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A maximum-entropy inspired</text>
              <doc_id>317</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>parser.</text>
              <doc_id>318</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>NAACL-00.</text>
              <doc_id>319</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Eugene Charniak, Kevin Knight, and Kenji Yamada.</text>
              <doc_id>320</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>321</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Syntax-based language models for statistical machine translation.</text>
              <doc_id>322</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>MT Summit IX.</text>
              <doc_id>323</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>40&#8211;46.</text>
              <doc_id>324</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>David Chiang.</text>
              <doc_id>325</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>326</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Hierarchical phrase-based translation.Computational Linguistics, 33(2).</text>
              <doc_id>327</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Steve DeNeefe, Kevin Knight.</text>
              <doc_id>328</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>329</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Synchronous Tree Adjoining Machine Translation.</text>
              <doc_id>330</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>EMNLP-2009.</text>
              <doc_id>331</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>727-736.</text>
              <doc_id>332</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Jason Eisner.</text>
              <doc_id>333</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>334</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Learning non-isomorphic tree mappings for MT.</text>
              <doc_id>335</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>ACL-03 (companion volume).</text>
              <doc_id>336</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Michel Galley, Mark Hopkins, Kevin Knight and Daniel Marcu.</text>
              <doc_id>337</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2004.</text>
              <doc_id>338</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>What&#8217;s in a translation rule?</text>
              <doc_id>339</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>HLT-NAACL-04.</text>
              <doc_id>340</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>273-280.</text>
              <doc_id>341</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Liang Huang.</text>
              <doc_id>342</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>343</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Forest Reranking: Discriminative Parsing with Non-Local Features.</text>
              <doc_id>344</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>ACL-HLT-08.</text>
              <doc_id>345</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>586-594 Liang Huang and David Chiang.</text>
              <doc_id>346</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>347</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Better k-best Parsing.</text>
              <doc_id>348</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>IWPT-05.</text>
              <doc_id>349</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>53-64</text>
              <doc_id>350</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Liang Huang and David Chiang.</text>
              <doc_id>351</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>352</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Forest rescoring: Faster decoding with integrated language models.</text>
              <doc_id>353</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>ACL-07.</text>
              <doc_id>354</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>144&#8211;151</text>
              <doc_id>355</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Dan Klein and Christopher D. Manning.</text>
              <doc_id>356</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2001.</text>
              <doc_id>357</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Parsing</text>
              <doc_id>358</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and Hypergraphs.</text>
              <doc_id>359</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>IWPT-2001.</text>
              <doc_id>360</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Reinhard Kneser and Hermann Ney.</text>
              <doc_id>361</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1995.</text>
              <doc_id>362</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Improved backing-off for M-gram language modeling.</text>
              <doc_id>363</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>ICASSP-95, 181-184</text>
              <doc_id>364</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin and Evan Herbst.</text>
              <doc_id>365</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>366</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Moses: Open Source Toolkit for Statistical Machine Translation.</text>
              <doc_id>367</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>ACL-07.</text>
              <doc_id>368</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>177-180.</text>
              <doc_id>369</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>(poster)</text>
              <doc_id>370</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Yang Liu, Qun Liu and Shouxun Lin.</text>
              <doc_id>371</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>372</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Tree-to-String Alignment Template for Statistical Machine Translation.</text>
              <doc_id>373</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>COLING-ACL-06.</text>
              <doc_id>374</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>609-616.</text>
              <doc_id>375</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Yang Liu, Yun Huang, Qun Liu and Shouxun Lin.</text>
              <doc_id>376</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>377</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Forest-to-String Statistical Translation Rules.</text>
              <doc_id>378</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>ACL-07.</text>
              <doc_id>379</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>704-711.</text>
              <doc_id>380</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Yang Liu, Yajuan L&#252;, Qun Liu.</text>
              <doc_id>381</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>382</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Improving Tree-to-Tree Translation with Packed Forests.</text>
              <doc_id>383</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>ACL-09.</text>
              <doc_id>384</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>558-566</text>
              <doc_id>385</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Haitao Mi, Liang Huang, and Qun Liu.</text>
              <doc_id>386</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>387</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Forest-based translation.</text>
              <doc_id>388</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>ACL-HLT-08.</text>
              <doc_id>389</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>192-199.</text>
              <doc_id>390</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Haitao Mi and Liang Huang.</text>
              <doc_id>391</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>392</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Forest-based Translation Rule Extraction.</text>
              <doc_id>393</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>EMNLP-08.</text>
              <doc_id>394</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>206-214.</text>
              <doc_id>395</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Franz J. Och.</text>
              <doc_id>396</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>397</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Minimum error rate training in statistical machine translation.</text>
              <doc_id>398</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>ACL-03.</text>
              <doc_id>399</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>160-167.</text>
              <doc_id>400</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Franz Josef Och and Hermann Ney.</text>
              <doc_id>401</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>402</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A Systematic Comparison of Various Statistical Alignment Models.</text>
              <doc_id>403</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics.</text>
              <doc_id>404</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>29(1) 19-51.</text>
              <doc_id>405</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu.</text>
              <doc_id>406</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>407</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>BLEU: a method for automatic evaluation of machine translation.</text>
              <doc_id>408</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>ACL-02.</text>
              <doc_id>409</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>311-318.</text>
              <doc_id>410</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Andreas Stolcke.</text>
              <doc_id>411</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>412</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>SRILM - an extensible language</text>
              <doc_id>413</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>modeling toolkit.</text>
              <doc_id>414</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>ICSLP-02.</text>
              <doc_id>415</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>901-904.</text>
              <doc_id>416</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Masaru Tomita.</text>
              <doc_id>417</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1987.</text>
              <doc_id>418</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>An Efficient Augmented-Context-Free Parsing Algorithm.</text>
              <doc_id>419</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics 13(1-2): 31-46</text>
              <doc_id>420</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Xavier Carreras and Michael Collins.</text>
              <doc_id>421</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>422</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Non-projective Parsing for Statistical Machine Translation.</text>
              <doc_id>423</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>EMNLP-2009.</text>
              <doc_id>424</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>200-209.</text>
              <doc_id>425</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>K. Yamada and K. Knight.</text>
              <doc_id>426</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2001.</text>
              <doc_id>427</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A Syntax-Based Statistical Translation Model.</text>
              <doc_id>428</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>ACL-01.</text>
              <doc_id>429</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>523-530.</text>
              <doc_id>430</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and Chew Lim Tan.</text>
              <doc_id>431</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009a.</text>
              <doc_id>432</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Forest-based Tree Sequence to String Translation Model.</text>
              <doc_id>433</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>ACL-IJCNLP-09.</text>
              <doc_id>434</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>172-180.</text>
              <doc_id>435</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Hui Zhang, Min Zhang, Haizhou Li, and Chew Lim Tan.</text>
              <doc_id>436</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009b.</text>
              <doc_id>437</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Fast Translation Rule Matching for Syntax-based Statistical Machine Translation.</text>
              <doc_id>438</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>EMNLP-09.</text>
              <doc_id>439</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>1037-1045.</text>
              <doc_id>440</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Chew Lim Tan and Sheng Li.</text>
              <doc_id>441</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>442</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A Tree-to-Tree Alignment-based model for statistical Machine translation.</text>
              <doc_id>443</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>MT-Summit-07.</text>
              <doc_id>444</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>535-542</text>
              <doc_id>445</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, Sheng Li.</text>
              <doc_id>446</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>447</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A Tree Sequence Alignment-based Tree-to-Tree Translation Model.</text>
              <doc_id>448</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>ACL-HLT-08.</text>
              <doc_id>449</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>559-567.</text>
              <doc_id>450</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Ying Zhang, Stephan Vogel, Alex Waibel.</text>
              <doc_id>451</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2004.</text>
              <doc_id>452</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?</text>
              <doc_id>453</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>LREC-04.</text>
              <doc_id>454</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>2051-2054.</text>
              <doc_id>455</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1. Performance comparison of different methods</caption>
        <reference_text>In PAGE 8: ... In decoding, we set the pruning threshold to 10  for the input source forest.  Table1  compares the  performance in NIST 2003 data set of our method  and several state-of-the-art systems as our baseline.  1)  MOSES: phrase-based system (Koehn et al....  In PAGE 8: ...Table 1. Performance comparison of different methods    From  Table1 , we can see that:   ...  In PAGE 9: ... FT2T only covers  26.8% tree-to-string rules, so it performs worse than  FT2S as shown in  Table1 . FT2TS (1to1) does not  allow one-to-many frontier node mapping, so it could  only recover the non-isomorphic node mapping in  the root level, while FT2TS (1toN) could make it at  both root and leaf levels....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>FT2T#@#@Model</cell>
              <cell>noPSS 23.40#@#@BLEU-4</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>MOSES</cell>
              <cell>withPSS 24.46#@#@23.39</cell>
            </row>
            <row>
              <cell>FT2TS (1to1)#@#@FT2S</cell>
              <cell>noPSS 25.39#@#@26.10</cell>
            </row>
            <row>
              <cell>FT2T</cell>
              <cell>withPSS 26.58#@#@noPSS 23.40
withPSS 24.46</cell>
            </row>
            <row>
              <cell>FT2TS (1toN)#@#@FT2TS (1to1)</cell>
              <cell>noPSS 26.30#@#@noPSS 25.39
withPSS 26.58</cell>
            </row>
            <row>
              <cell>FT2TS (1toN)</cell>
              <cell>withPSS  27.70#@#@noPSS 26.30
withPSS 27.70</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2. Statistics on node mapping in forest, where &#8220;1to1&#8221; means the number of nodes in source forest that can be translated into one node in target forest and &#8220;1toN&#8221; means the number of nodes in source forest that have to be translated into more than one node in target forest, where the node refers to non-terminal nodes only</caption>
        <reference_text>In PAGE 9: ...1%  FT2TS (1toN)  1945168  100%  Table 3. Statistics of rule coverage, where  T2S  covered  means the percentage of tree-to-string  rules that can be covered by the model   Table2  studies the node isomorphism between bi- lingual forest pair. We can see that the  non-isomorphic node translation mapping (1toN)  accounts for 57....</reference_text>
        <page_num>9</page_num>
        <head>
          <rows>
            <row>
              <cell>improvements over FT2S with the best case</cell>
              <cell>improvements over FT2S with the best case</cell>
              <cell>improvements over FT2S with the best case</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>having 1.6 (27.70-26.10) BLEU score im-</cell>
              <cell>having 1.6 (27.70-26.10) BLEU score im-</cell>
              <cell>having 1.6 (27.70-26.10) BLEU score im-</cell>
            </row>
            <row>
              <cell>provement over FT2S. This suggests that the</cell>
              <cell>provement over FT2S. This suggests that the</cell>
              <cell>provement over FT2S. This suggests that the</cell>
            </row>
            <row>
              <cell>target structure information is very useful, but</cell>
              <cell>target structure information is very useful, but</cell>
              <cell>target structure information is very useful, but</cell>
            </row>
            <row>
              <cell>we need to find a correct way to effectively</cell>
              <cell>we need to find a correct way to effectively</cell>
              <cell>we need to find a correct way to effectively</cell>
            </row>
            <row>
              <cell>u</cell>
              <cell>t</cell>
              <cell>i</cell>
              <cell>l</cell>
              <cell>i</cell>
              <cell>z</cell>
              <cell>e</cell>
              <cell></cell>
              <cell>i</cell>
              <cell>t</cell>
              <cell>.</cell>
            </row>
            <row>
              <cell>1to1</cell>
              <cell>1toN</cell>
              <cell>ratio</cell>
            </row>
            <row>
              <cell>1735871 2363771 1:1.36</cell>
              <cell>1735871 2363771 1:1.36</cell>
              <cell>1735871 2363771 1:1.36</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TableSeer</source>
        <caption>Table 5. Performance on larger data set</caption>
        <reference_text>In PAGE 10: ...Table5  clearly shows the effec- tiveness of our method is consistent across small and  larger corpora, outperforming FT2S by 1.6-1....</reference_text>
        <page_num>10</page_num>
        <head>
          <rows>
            <row>
              <cell>Model</cell>
              <cell>BLEU</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell></cell>
              <cell>NIST2003</cell>
              <cell>NIST2005</cell>
            </row>
            <row>
              <cell>MOSES</cell>
              <cell>29.51</cell>
              <cell>27.53</cell>
            </row>
            <row>
              <cell>FT2S</cell>
              <cell>31.21</cell>
              <cell>29.72</cell>
            </row>
            <row>
              <cell>FT2TS</cell>
              <cell>32.88</cell>
              <cell>31.50</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references/>
    <citations/>
  </content>
</document>
