<PAPER>
  <FILENO/>
  <TITLE>A Systematic Analysis of Translation Model Search Spaces</TITLE>
  <AUTHORS>
    <AUTHOR>Michael Auli</AUTHOR>
    <AUTHOR>Adam Lopez</AUTHOR>
    <AUTHOR>Hieu Hoang</AUTHOR>
  </AUTHORS>
  <ABSTRACT>
    <A-S ID="S-41109">Translation systems are complex, and most metrics do little to pinpoint causes of error or isolate system differences.</A-S>
    <A-S ID="S-41110">We use a simple technique to discover induction errors, which occur when good translations are absent from model search spaces.</A-S>
    <A-S ID="S-41111">Our results show that a common pruning heuristic drastically increases induction error, and also strongly suggest that the search spaces of phrase-based and hierarchical phrase-based models are highly overlapping despite the well known structural differences.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-41112">Most empirical work in translation analyzes models and algorithms using BLEU (<REF ID="R-14" RPTR="19">Papineni et al., 2002</REF>) and related metrics.</S>
        <S ID="S-41113">Though such metrics are useful as sanity checks in iterative system development, they are less useful as analytical tools.</S>
        <S ID="S-41114">The performance of a translation system depends on the complex interaction of several different components.</S>
        <S ID="S-41115">Since metrics assess only output, they fail to inform us about the consequences of these interactions, and thus provide no insight into the errors made by a system, or into the design tradeoffs of competing systems.</S>
      </P>
      <P>
        <S ID="S-41116">In this work, we show that it is possible to obtain such insights by analyzing translation system components in isolation.</S>
        <S ID="S-41117">We focus on model search spaces (&#167;2), posing a very simple question: Given a model and a sentence pair, does the search space contain the sentence pair?</S>
        <S ID="S-41118">Applying this method to the analysis and comparison of French- English translation using both phrase-based and hierarchical phrase-based systems yields surprising results, which we analyze quantitatively and qualitatively.</S>
      </P>
      <P>
        <S ID="S-41119">&#8226; First, we analyze the induction error of a</S>
      </P>
      <P>
        <S ID="S-41120">model, a measure on the completeness of the search space.</S>
        <S ID="S-41121">We find that low weight phrase translations typically discarded by heuristic pruning nearly triples the number of reference sentences that can be exactly reconstructed by either model (&#167;3).</S>
      </P>
      <P>
        <S ID="S-41122">&#8226; Second, we find that the high-probability regions in the search spaces of phrase-based and hierarchical systems are nearly identical (&#167;4).</S>
        <S ID="S-41123">This means that reported differences between the models are due to their rankings of competing hypotheses, rather than structural differences of the derivations they produce.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Models, Search Spaces, and Errors</HEADER>
      <P>
        <S ID="S-41124">A translation model consists of two distinct elements: an unweighted ruleset, and a parameterization (<REF ID="R-09" RPTR="11">Lopez, 2008</REF><REF ID="R-10" RPTR="14">Lopez, 2008</REF>a; 2009).</S>
        <S ID="S-41125">A ruleset licenses the steps by which a source string f 1 ...f I may be rewritten as a target string e 1 ...e J .</S>
        <S ID="S-41126">A parameterization defines a weight function over every sequence of rule applications.</S>
        <S ID="S-41127">In a phrase-based model, the ruleset is simply the unweighted phrase table, where each phrase pair f i ...f i &#8242;/e j ...e j &#8242; states that phrase f i ...f i &#8242; in the source can be rewritten as e j ...e j &#8242; in the target.</S>
        <S ID="S-41128">The model operates by iteratively applying rewrites to the source sentence until each source word has been consumed by exactly one rule.</S>
        <S ID="S-41129">There are two additional heuristic rules: The distortion limit dl constrains distances over which phrases can be reordered, and the translation option limit tol constrains the number of target phrases that may be considered for any given source phrase.</S>
        <S ID="S-41130">Together, these rules completely determine the finite set of all possible target sentences for a given source sentence.</S>
        <S ID="S-41131">We call this set of target sentences the model search space.</S>
      </P>
      <P>
        <S ID="S-41132">The parameterization of the model includes all information needed to score any particular se-</S>
      </P>
      <P>
        <S ID="S-41133">quence of rule applications.</S>
        <S ID="S-41134">In our phrase-based model, it typically includes phrase translation probabilities, lexical translation probabilities, language model probabilities, word counts, and coefficients on the linear combination of these.</S>
        <S ID="S-41135">The combination of large rulesets and complex parameterizations typically makes search intractable, requiring the use of approximate search.</S>
        <S ID="S-41136">It is important to note that, regardless of the parameterization or search used, the set of all possible output sentences is still a function of only the ruleset.</S>
        <S ID="S-41137"><REF ID="R-04" RPTR="5">Germann et al. (2004)</REF> identify two types of translation system error: model error and search error.</S>
        <S ID="S-41138">1 Model error occurs when the optimal path through the search space leads to an incorrect translation.</S>
        <S ID="S-41139">Search error occurs when the approximate search technique causes the decoder to select a translation other than the optimum.</S>
      </P>
      <P>
        <S ID="S-41140">Given the decomposition outlined above, it seems clear that model error depends on parameterization, while search error depends on approximate search.</S>
        <S ID="S-41141">However, there is no error type that clearly depends on the ruleset (Table 1).</S>
        <S ID="S-41142">We therefore identify a new type of error on the ruleset: induction error.</S>
        <S ID="S-41143">Induction error occurs when the search space does not contain the correct target sentence at all, and is thus a more fundamental defect than model error.</S>
        <S ID="S-41144">This is difficult to measure, since there could be many correct translations and there is no way to see whether they are all absent from the search space.</S>
        <S ID="S-41145">2 However, if we assume that a given reference sentence is ground truth, then as a proxy we can simply ask whether or not the model search space contains the reference.</S>
        <S ID="S-41146">This assumption is of course too strong, but over a sufficiently large test set, it should correlate with metrics which depend on the reference, since under most metrics, exactly reproducing the reference results in a perfect score.</S>
        <S ID="S-41147">More loosely, it should correlate with translation accuracy&#8212;even if there are many good translations, a model which is systematically unable to produce any reference sentences from a sufficiently large test sample is almost certainly deficient in some way.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Does Ruleset Pruning Matter?</HEADER>
      <P>
        <S ID="S-41148">The heuristic translation option limit tol controls the number of translation rules considered per</S>
      </P>
      <P>
        <S ID="S-41149">1 They also identify variants within these types.</S>
        <S ID="S-41150">2 It can also be gamed by using a model that can generate</S>
      </P>
      <P>
        <S ID="S-41151">any English word from any French word.</S>
        <S ID="S-41152">However, this is not a problem for the real models we investigate here.</S>
      </P>
      <P>
        <S ID="S-41153">ruleset parameterization search</S>
      </P>
      <P>
        <S ID="S-41154">induction error model error search error</S>
      </P>
      <P>
        <S ID="S-41155">Phrase Probability p(e|f) 0.8</S>
      </P>
      <P>
        <S ID="S-41156">0.6</S>
      </P>
      <P>
        <S ID="S-41157">0.4</S>
      </P>
      <P>
        <S ID="S-41158">0.2</S>
      </P>
      <P>
        <S ID="S-41159">Translation Options</S>
      </P>
      <P>
        <S ID="S-41160">source span.</S>
        <S ID="S-41161">It plays a major role in keeping the search space manageable.</S>
        <S ID="S-41162">Ignoring reordering, the complexity of the search in a phrase-based model is O(n tol ), where n is the number of French spans.</S>
        <S ID="S-41163">Therefore tol has a major effect on efficiency.</S>
        <S ID="S-41164">Tight pruning with tol is often assumed without question to be a worthwhile tradeoff.</S>
        <S ID="S-41165">However, we wish to examine this assumption more closely.</S>
      </P>
      <P>
        <S ID="S-41166">Consider the French word probl&#232;me.</S>
        <S ID="S-41167">It has 288 different translation options in the phrase table of our French-English phrase-based system.</S>
        <S ID="S-41168">The phrase translation probability p(e|f) over these options is a familiar Zipf distribution (Figure 1).</S>
        <S ID="S-41169">The most likely candidate translation for the word is problem with a probability of 0.71, followed by issue with a much smaller probability of 0.12.</S>
        <S ID="S-41170">Further down, we find challenge at rank 25, obstacle at 44 and dilemma at rank 105.</S>
        <S ID="S-41171">Depending on the context, these might be perfectly good translations.</S>
        <S ID="S-41172">However, with a typical tol of 20, most of these options are not considered during decoding.</S>
      </P>
      <P>
        <S ID="S-41173">Table 2 shows that 93.8% of rules are available during decoding with the standard tol setting and only about 0.1% of French spans of the entire ruleset have more than 20 translation options.</S>
        <S ID="S-41174">It seems as if already most of the information is available when using the default limit.</S>
        <S ID="S-41175">However, a tol of 20 can clearly exclude good translations as illustrated by our example.</S>
        <S ID="S-41176">Therefore we hypothesize the following: Increasing the translation option limit gives the decoder a larger vocabulary which in turn will decrease the induction error.</S>
        <S ID="S-41177">We sup-</S>
      </P>
      <P>
        <S ID="S-41178">port this hypothesis experimentally in &#167;5.4.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 How Similar are Model Search Spaces?</HEADER>
      <P>
        <S ID="S-41179">Most work on hierarchical phrase-based translation focuses quite intently on its structural differences from phrase-based translation.</S>
      </P>
      <P>
        <S ID="S-41180">&#8226; A hierarchical model can translate discontiguous groups of words as a unit.</S>
        <S ID="S-41181">A phrasebased model cannot.</S>
        <S ID="S-41182"><REF ID="R-09" RPTR="12">Lopez (2008</REF><REF ID="R-10" RPTR="15">Lopez (2008</REF>b) gives indirect experimental evidence that this difference affects performance.</S>
      </P>
      <P>
        <S ID="S-41183">&#8226; A standard phrase-based model can reorder phrases arbitrarily within the distortion limit, while the hierarchical model requires some lexical evidence for movement, resorting to monotone translation otherwise.</S>
      </P>
      <P>
        <S ID="S-41184">&#8226; While both models can indirectly model word deletion in the context of phrases, the hierarchical model can delete words using non-local context due to its use of discontiguous phrases.</S>
      </P>
      <P>
        <S ID="S-41185">The underlying assumption in most discussions of these models is that these differences in their generative stories are responsible for differences in performance.</S>
        <S ID="S-41186">We believe that this assumption should be investigated empirically.</S>
        <S ID="S-41187">In an interesting analysis of phrase-based and hierarchical translation, Zollmann et al. (2008) forced a phrase-based system to produce the translations generated by a hierarchical system.</S>
        <S ID="S-41188">Unfortunately, their analysis is incomplete; they do not perform the analysis in both directions.</S>
        <S ID="S-41189">In &#167;5.5 we extend their work by requiring each system to generate the 1-best output of the other.</S>
        <S ID="S-41190">This allows us to see how their search spaces differ.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Experiments</HEADER>
      <P>
        <S ID="S-41324">We analyse rulesets in isolation, removing the influence of the parametrization and heuristics as much as possible for each system as follows: First, we disabled beam search to avoid pruning based on parametrization weights.</S>
        <S ID="S-41325">Second, we require our decoders to generate the reference via disallowing reference-incompatible hypothesis or chart entries.</S>
        <S ID="S-41326">This leaves only some search restrictions such as the distortion limit for the phrase-based system for which we controlled, or the maximum number of source words involved in a rule application for the hierarchical system.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>5.1 Experimental Systems</HEADER>
        <P>
          <S ID="S-41191">Our phrase-based system is Moses (<REF ID="R-07" RPTR="8">Koehn et al., 2007</REF>).</S>
          <S ID="S-41192">We set its stack size to 10 5 , disabled the beam threshold, and varied the translation option limit tol.</S>
          <S ID="S-41193">Forced translation was implemented by <REF ID="R-15" RPTR="20">Schwartz (2008)</REF> who ensures that hypothesis are a prefix of the reference to be generated.</S>
        </P>
        <P>
          <S ID="S-41194">Our hierarchical system is Hiero (<REF ID="R-02" RPTR="3">Chiang, 2007</REF>), modified to construct rules from a small sample of occurrences of each source phrase in training as described by <REF ID="R-09" RPTR="13">Lopez (2008</REF><REF ID="R-10" RPTR="16">Lopez (2008</REF>b).</S>
          <S ID="S-41195">The search parameters restricting the number of rules or chart entries as well as the minimum threshold were set to very high values (10 50 ) to prevent pruning.</S>
          <S ID="S-41196">Forced translation was implemented by discarding rules and chart entries which do not match the reference.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>5.2 Experimental Data</HEADER>
        <P>
          <S ID="S-41197">We conducted experiments in French-English translation, attempting to make the experimental conditions for both systems as equal as possible.</S>
          <S ID="S-41198">Each system was trained on French-English Europarl (<REF ID="R-08" RPTR="10">Koehn, 2005</REF>), version 3 (40M words).</S>
          <S ID="S-41199">The corpus was aligned with GIZA++ (<REF ID="R-12" RPTR="17">Och and Ney, 2003</REF>) and symmetrized with the grow-diag-finaland heuristic (<REF ID="R-06" RPTR="7">Koehn et al., 2003</REF>).</S>
          <S ID="S-41200">A trigram language model with modified Kneser-Ney discounting and interpolation was used as produced by the SRILM toolkit (<REF ID="R-16" RPTR="21">Stolcke, 2002</REF>).</S>
          <S ID="S-41201">Systems were optimized on the WMT08 French-English development data (2000 sentences) using minimum error rate training (<REF ID="R-13" RPTR="18">Och, 2003</REF>) and tested on the WMT08 test data (2000 sentences).</S>
          <S ID="S-41202">Rules based on unaligned words at the edges of foreign and source spans were not allowed unless otherwise stated, this is denoted as the tightness con-</S>
        </P>
        <P>
          <S ID="S-41203">Reachability (%) 35</S>
        </P>
        <P>
          <S ID="S-41204">dl=6</S>
        </P>
        <P>
          <S ID="S-41205">dl=7</S>
        </P>
        <P>
          <S ID="S-41206">dl=8</S>
        </P>
        <P>
          <S ID="S-41207">dl=9</S>
        </P>
        <P>
          <S ID="S-41208">dl=10</S>
        </P>
        <P>
          <S ID="S-41209">dl=11</S>
        </P>
        <P>
          <S ID="S-41210">dl=12</S>
        </P>
        <P>
          <S ID="S-41211">dl=13</S>
        </P>
        <P>
          <S ID="S-41212">dl=14</S>
        </P>
        <P>
          <S ID="S-41213">dl=15</S>
        </P>
        <P>
          <S ID="S-41214">dl=16</S>
        </P>
        <P>
          <S ID="S-41215">10 20 50 100 200 400 800 All</S>
        </P>
        <P>
          <S ID="S-41216">Translation Option Limit</S>
        </P>
        <P>
          <S ID="S-41217">straint.</S>
          <S ID="S-41218"><REF ID="R-00" RPTR="0">Ayan and Dorr (2006)</REF> showed that under certain conditions, this constraint could have significant impact on system performance.</S>
          <S ID="S-41219">The maximum phrase lengths for both the hierarchical and phrase-based system were set to 7.</S>
          <S ID="S-41220">The distortion limit (dl) for the phrase-based system was set to 6 unless otherwise mentioned.</S>
          <S ID="S-41221">All other settings were left at their default values as described by <REF ID="R-02" RPTR="2">Chiang (2007)</REF> and <REF ID="R-07" RPTR="9">Koehn et al. (2007)</REF>.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>5.3 Metric: Reference Reachability</HEADER>
        <P>
          <S ID="S-41222">We measure system performance in terms of reference reachability, which is the inverse of induction error: A system is required to be able to exactly reproduce the reference, otherwise we regard the result as an error.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>5.4 Analysis of Ruleset Pruning</HEADER>
        <P>
          <S ID="S-41223">In &#167;3 we outlined the hypothesis that increasing the number of English translation options per French span can increase performance.</S>
          <S ID="S-41224">Here we present results for both phrase-based and hierarchical systems to support this claim.</S>
        </P>
        <P>
          <S ID="S-41225">5.4.1 Quantitative Results</S>
        </P>
        <P>
          <S ID="S-41226">Figure 2 shows the experimental results when forcing our phrase-based system to generate unseen test data.</S>
          <S ID="S-41227">We observe more than 30% increase in reachability from tol = 20 to tol = 50 for all dl &#8805; 6 which supports our hypothesis that increasing tol by a small multiple can have a significant impact on performance.</S>
          <S ID="S-41228">With no limit on tol, reachability nearly triples.</S>
        </P>
        <P>
          <S ID="S-41229">Notably, the increase stems from the small fraction of French spans (0.1%) which have more than 20 translation options (Table 2).</S>
          <S ID="S-41230">There are only 16 French spans (Table 3) which have more than 1000 translation options, however, utilising these can still achieve an increase in reachability of up to 5%.</S>
          <S ID="S-41231">The list shown in Table 3 includes common articles, interpuncutation, conjunctions, prepositions but also verbs which have unreliable alignment points and therefore a very long tail of low probability translation options.</S>
          <S ID="S-41232">Yet, the largest increase does not stem from using such unreliable translation options, but rather when increasing tol by a relatively small amount.</S>
          <S ID="S-41233">The increases we see in reachability are proportional to the size of the ruleset: The highest increases in ruleset size can be seen between tol = 20 and tol = 200 (Table 2), similarly, reachability performance has then the largest increase.</S>
          <S ID="S-41234">For higher tol settings both the increases of ruleset size and reachability are smaller.</S>
        </P>
        <P>
          <S ID="S-41235">Figure 3 plots the average number of words per sentence for the reachable sentences.</S>
          <S ID="S-41236">The average sentence length increases by up to six words when using all translation options.</S>
          <S ID="S-41237">The black line represents the average number of words per sentence of the reference set.</S>
          <S ID="S-41238">This shows that longer and more complex sentences can be generated when using more translation options.</S>
        </P>
        <P>
          <S ID="S-41239">Similarly, for our hierarchical system (see Fig-</S>
        </P>
        <P>
          <S ID="S-41240">Average Number of Words per Sentence 32</S>
        </P>
        <P>
          <S ID="S-41241">dl=6</S>
        </P>
        <P>
          <S ID="S-41242">dl=7</S>
        </P>
        <P>
          <S ID="S-41243">dl=8</S>
        </P>
        <P>
          <S ID="S-41244">dl=9</S>
        </P>
        <P>
          <S ID="S-41245">dl=10</S>
        </P>
        <P>
          <S ID="S-41246">dl=11</S>
        </P>
        <P>
          <S ID="S-41247">dl=12</S>
        </P>
        <P>
          <S ID="S-41248">dl=13</S>
        </P>
        <P>
          <S ID="S-41249">dl=14</S>
        </P>
        <P>
          <S ID="S-41250">dl=15</S>
        </P>
        <P>
          <S ID="S-41251">dl=16</S>
        </P>
        <P>
          <S ID="S-41252">Reference</S>
        </P>
        <P>
          <S ID="S-41253">14 20 50 100 200 400 800 All</S>
        </P>
        <P>
          <S ID="S-41254">Translation Option Limit</S>
        </P>
        <P>
          <S ID="S-41255">Reachability (%) 40</S>
        </P>
        <P>
          <S ID="S-41256">Sample Limit (SL)</S>
        </P>
        <P>
          <S ID="S-41257">ure 4) we find that reachability can be more than doubled when drawing a richer ruleset sample than in the baseline setting.</S>
          <S ID="S-41258">Those results are not directly comparable to the phrase-based system due to the slightly different nature of the parameters which were varied: In the phrase-based case we have tol different English spans per French span.</S>
          <S ID="S-41259">In the hierarchical system it is very likely to have duplicate French spans in the sample drawn from training data.</S>
          <S ID="S-41260">Yet, the trend is the same and thus supports our claim.</S>
        </P>
        <P>
          <S ID="S-41261">5.4.2 Qualitative Results</S>
        </P>
        <P>
          <S ID="S-41262">We were interested how the performance increase could be achieved and therefore looked into which kind of translation options were involved when a translation was generable with a higher tol setting.</S>
          <S ID="S-41263">One possibility is that the long tail of translation options includes all kinds of English spans that match some part of the reference but are simply an artifact of unreliable alignment points.</S>
          <S ID="S-41264">We looked at the first twenty translations produced by our phrase-based system under dl = 10 which could not be generated with tol = 20 but with tol = 50.</S>
          <S ID="S-41265">The aim was to find out which translation options made it possible to reach the reference under tol = 50.</S>
        </P>
        <P>
          <S ID="S-41266">We found that nearly half (9) involved translation options which used a common or less common translation of the foreign span.</S>
          <S ID="S-41267">The first four translations in Table 4 are examples for that.</S>
          <S ID="S-41268">When allowing unaligned words at the rule edges it turns out that even 13 out of 20 translations are based on sound translation options.</S>
          <S ID="S-41269">The remaining sentences involved translation options which were an artifact of unreliable alignment points.</S>
          <S ID="S-41270">An example rule is la / their, which erroneously translates a common determiner into an equally common adjective.</S>
          <S ID="S-41271">The last translation in Figure 4 involves such a translation option.</S>
        </P>
        <P>
          <S ID="S-41272">This analysis demonstrates that the performance increase between tol = 20 to tol = 50 is to a considerable extent based on translation options which are meaningful.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>5.5 Analysis of Mutual Reachability</HEADER>
        <P>
          <S ID="S-41273">The aim of this analysis was to find out by how much the high-probability search spaces of the phrase-based and hierarchical models differ.</S>
          <S ID="S-41274">The necessary data was obtained via forcing each system to produce the 1-best translation of the other system denoted as the unconstrained translation.</S>
          <S ID="S-41275">This unconstrained translation used the standard setting for the number of translation options.</S>
        </P>
        <P>
          <S ID="S-41276">We controlled for the way unaligned words were handled during rule extraction: The phrasebased system allowed unaligned words at the edges of phrases while the hierarchical system did not.</S>
          <S ID="S-41277">We varied this condition for the phrase-based system.</S>
          <S ID="S-41278">The distortion limit of the phrase-based system was set to 10.</S>
          <S ID="S-41279">This is equal to the maximum span a rule can be applied within the hierarchical system.</S>
        </P>
        <P>
          <S ID="S-41280">We carried out the same experiment for German-English and English-German translation which serve as examples for translating into a mor-</S>
        </P>
        <P>
          <S ID="S-41281">phologically simpler and more complex language respectively.</S>
          <S ID="S-41282">The test and training sets for these languages are similarly sized and are from the WMT08 shared task.</S>
        </P>
        <P>
          <S ID="S-41283">5.5.1 Quantitative Results</S>
        </P>
        <P>
          <S ID="S-41284">Table 5 shows the mutual reachability performance for our phrase-based and hierarchical system.</S>
          <S ID="S-41285">The hierarchical system can generate almost all of the 1-best phrase-based translations, particularly when unaligned words at rule edges are disallowed which is the most equal condition we experimented with.</S>
          <S ID="S-41286">The phrase-based reachability for English-German using tight rulesets is remarkably low.</S>
          <S ID="S-41287">We found that this is because the hierarchical model allows unaligned words around gaps under the tight constraint.</S>
          <S ID="S-41288">This makes it very hard for the phrase-based system to reach the hierarchical translation.</S>
          <S ID="S-41289">However, the phrase-based system can overcome this problem when the tightness constraint is loosened (last row in Table 5).</S>
        </P>
        <P>
          <S ID="S-41290">Table 6 shows the translation performance measured in BLEU for both systems for normal unconstrained translation.</S>
          <S ID="S-41291">It can be seen that the difference is rather marginal which is in line with our reachability results.</S>
        </P>
        <P>
          <S ID="S-41292">We were interested why certain translations of one system were not reachable by the other system.</S>
          <S ID="S-41293">The following two subsections describe our analysis of these translations for the French- English language pair.</S>
        </P>
        <P>
          <S ID="S-41294">5.5.2 Qualitative Analysis of Unreachable Hierarchical Translations</S>
        </P>
        <P>
          <S ID="S-41295">We analysed the first twenty translations within the set of unreachable hierarchical translations when disallowing unaligned words at rule edges to find out why the phrase-based system fails to reach them.</S>
          <S ID="S-41296">Two aspects were considered in this analysis: First, the successful hierarchical derivation and second, the relevant part of the phrase-based ruleset which was involved in the failed forced translation i.e. how much of the input and the reference could be covered by the raw phrase-pairs available to the phrase-based system.</S>
          <S ID="S-41297">Within the examined subset, the majority of sentences (14) involved hierarchical rules which could not be replicated by the phrase-based sys-</S>
        </P>
        <P>
          <S ID="S-41298">tem.</S>
          <S ID="S-41299">We described this as the first structural difference in &#167;4.</S>
          <S ID="S-41300">Almost all of these translations (12 out of 14) could not be generated because of the third structural difference which involved rule that omits the translation of a word within the French span.</S>
          <S ID="S-41301">An example is the rule X &#8594; estX 1 ordinaireX 2 /isX 1 X 2 which omits a translation for the French word ordinaire in the English span.</S>
          <S ID="S-41302">For this particular subset the capability of the hierarchical system to capture long-distance reorderings did not make the difference, but rather the ability to drop words within a translation rule.</S>
        </P>
        <P>
          <S ID="S-41303">The phrase-based system cannot learn many rules which omit the translation of words because we disallowed unaligned words at phrase edges.</S>
          <S ID="S-41304">The hierarchical system has the same restriction, but the constraint does not prohibit rules which have unaligned words within the rule.</S>
          <S ID="S-41305">This allows the hierarchical system to learn rules such as the one presented above.</S>
          <S ID="S-41306">The phrase-based system can learn similar knowledge, although less general, if it is allowed to have unaligned words at the phrase edges.</S>
          <S ID="S-41307">In fact, without this constraint 13 out of the 20 analysed rules can be generated by the phrase-based system.</S>
        </P>
        <P>
          <S ID="S-41308">Figure 5 shows a seemingly simple hierarchical translation which fails to be constructed by the phrase-based system: The second rule application involves both the reordering of the translation of postaux and the omittance of a translation for concurrence.</S>
          <S ID="S-41309">This translation could be easily captured by a phrase-pair, however, it requires that the training data contains exactly such an example which was not the case.</S>
          <S ID="S-41310">The closest rule the phrase-based rulestore contains is des services postaux / postal services which fails since it does not cover all of the input.</S>
          <S ID="S-41311">This is an example for when the generalisation of the hierarchical model is superior to the phrase-based approach.</S>
        </P>
        <P>
          <S ID="S-41312">5.5.3 Qualitative Analysis of Unreachable Phrase-based Translations</S>
        </P>
        <P>
          <S ID="S-41313">The size of the set of unreachable phrase-based translations is only 0.6% or 12 sentences.</S>
          <S ID="S-41314">This means that almost all of the 1-best outputs of the phrase-based translations can be reached by the hierarchical system.</S>
          <S ID="S-41315">Similarly to above, we analysed which words of the input as well as which words of the phrase-based translation can be covered by the available hierarchical translation rules.</S>
        </P>
        <P>
          <S ID="S-41316">We found that all of the translations were not generable because of the second structural difference we identified in &#167;4.</S>
          <S ID="S-41317">The hierarchical ruleset did not contain a rule with the necessary lexical evidence to perform the same reordering as the phrase-based model.</S>
          <S ID="S-41318">Figure 6 shows a phrasebased translation which could not be reached by the hierarchical system because a rule of the form X &#8594; &#233;lectoralesX 1 /X 1 electoral would be required to move the translation of &#233;lectorales (electoral) just before the translation of r&#233;unions (meetings).</S>
          <S ID="S-41319">Inspection of the hierarchical ruleset reveals that such a rule is not available and so the translation cannot be generated.</S>
        </P>
        <P>
          <S ID="S-41320">The small size of the set of unreachable phrasebased translations shows that the lexically informed reordering mechanism of the hierarchical model is not a large obstacle in generating most of the phrase-based outputs.</S>
        </P>
        <P>
          <S ID="S-41321">In summary, each system can reproduce nearly all of the highest-scoring outputs of the other system.</S>
          <S ID="S-41322">This shows that the 1-best regions of both systems are nearly identical despite the differences discussed in &#167;4.</S>
          <S ID="S-41323">This means that differences in observed system performance are probably attributable to the degree of model error and search error in each system.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>6 Related Work and Open Questions</HEADER>
      <P>
        <S ID="S-41327"><REF ID="R-18" RPTR="23">Zhang et al. (2008)</REF> and <REF ID="R-17" RPTR="22">Wellington et al. (2006)</REF> answer the question: what is the minimal grammar that can be induced to completely describe a training set?</S>
        <S ID="S-41328">We look at the related question of what a heuristically induced ruleset can translate in an unseen test set, considering both phrase- and grammar-based models.</S>
        <S ID="S-41329">We also extend the work of Zollmann et al. (2008) on Chinese-English, performing the analysis in both directions and providing a detailed qualitative explanation.</S>
      </P>
      <P>
        <S ID="S-41330">Our focus has been on the induction error of models, a previously unstudied cause of transla-</S>
      </P>
      <P>
        <S ID="S-41331">) )</S>
      </P>
      <P>
        <S ID="S-41332">tion errors.</S>
        <S ID="S-41333">Although the results described here are striking, our exact match criterion for reachability is surely too strict&#8212;for example, we report an error if even a single comma is missing.</S>
        <S ID="S-41334">One solution is to use a more tolerant criterion such as WER and measure the amount of deviation from the reference.</S>
        <S ID="S-41335">We could also maximize BLEU with respect to the reference as in <REF ID="R-03" RPTR="4">Dreyer et al. (2007)</REF>, but it is less interpretable.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>7 Conclusion and Future Work</HEADER>
      <P>
        <S ID="S-41336">Sparse distributions are common in natural language processing, and machine translation is no exception.</S>
        <S ID="S-41337">We showed that utilizing more of the entire distribution can dramatically improve the coverage of translation models, and possibly their accuracy.</S>
        <S ID="S-41338">Accounting for sparsity explicitly has achieved significant improvements in other areas such as in part of speech tagging (<REF ID="R-05" RPTR="6">Goldwater and Griffiths, 2007</REF>).</S>
        <S ID="S-41339">Considering the entire tail is challenging, since the search space grows exponentially with the number of translation options.</S>
        <S ID="S-41340">A first step might be to use features that facilitate more variety in the top 20 translation options.</S>
        <S ID="S-41341">A more elaborate aim is to look into alternatives to maximum likelihood hood estimation such as in <REF ID="R-01" RPTR="1">Blunsom and Osborne (2008)</REF>.</S>
        <S ID="S-41342">Additionally, our expressiveness analysis shows clearly that the 1-best region of hierarchical and phrase-based models is nearly identical.</S>
        <S ID="S-41343">Discounting cases in which systems handle unaligned words differently, we observe an overlap of between 96% and 99% across three language pairs.</S>
        <S ID="S-41344">This implies that the main difference between the models is in their parameterization, rather than in the structural differences in the types of translations they can produce.</S>
        <S ID="S-41345">Our results also suggest that the search spaces of both models are highly overlapping: The results for the 1-best region allow the conjecture that also other parts of the search space are behaving similarly since it appears rather unlikely that spaces are nearly disjoint with only the 1-best region being nearly identical.</S>
        <S ID="S-41346">In future work we aim to use n-best lists or lattices to more precisely measure search space overlap.</S>
        <S ID="S-41347">We also aim to analyse the effects of the model and search errors for these systems.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS>
    <P>
      <S ID="S-41348">This research was supported by the Euromatrix Project funded by the European Commission (6th Framework Programme).</S>
      <S ID="S-41349">The experiments were conducted using the resources provided by the Edinburgh Compute and Data Facility (ECDF).</S>
      <S ID="S-41350">Many thanks to the three anonymous reviewers for very helpful comments on earlier drafts.</S>
    </P>
  </ACKNOWLEDGMENTS>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>N F Ayan</RAUTHOR>
      <REFTITLE>Going beyond AER: An extensive analysis of word alignments and their impact on MT.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>P Blunsom</RAUTHOR>
      <REFTITLE>Probabilistic inference for machine translation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>D Chiang</RAUTHOR>
      <REFTITLE>Hierarchical phrase-based translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>M Dreyer</RAUTHOR>
      <REFTITLE>Comparing reordering constraints for SMT using efficient BLEU oracle computation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>U Germann</RAUTHOR>
      <REFTITLE>Fast and optimal decoding for machine translation.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>S Goldwater</RAUTHOR>
      <REFTITLE>A fully Bayesian approach to unsupervised part-of-speech tagging.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>P Koehn</RAUTHOR>
      <REFTITLE>Statistical phrase-based translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>P Koehn</RAUTHOR>
      <REFTITLE>Moses: Open source toolkit for statistical machine translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>P Koehn</RAUTHOR>
      <REFTITLE>Europarl: A parallel corpus for statistical machine translation.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>A Lopez</RAUTHOR>
      <REFTITLE>Statistical machine translation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>A Lopez</RAUTHOR>
      <REFTITLE>Tera-scale translation models via pattern matching.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>A Lopez</RAUTHOR>
      <REFTITLE>Translation as weighted deduction.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>F J Och</RAUTHOR>
      <REFTITLE>A systematic comparison of various statistical alignment models.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>F J Och</RAUTHOR>
      <REFTITLE>Minimum error rate training in statistical machine translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="14">
      <RAUTHOR>K Papineni</RAUTHOR>
      <REFTITLE>BLEU: A method for automatic evaluation of machine translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="15">
      <RAUTHOR>L Schwartz</RAUTHOR>
      <REFTITLE>Multi-source translation methods.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="16">
      <RAUTHOR>A Stolcke</RAUTHOR>
      <REFTITLE>SRILM &#8211; an extensible language modeling toolkit.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="17">
      <RAUTHOR>B Wellington</RAUTHOR>
      <REFTITLE>Empirical lower bounds on the complexity of translational equivalence.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="18">
      <RAUTHOR>H Zhang</RAUTHOR>
      <REFTITLE>Extracting synchronous grammar rules from word-level alignments in linear time.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
