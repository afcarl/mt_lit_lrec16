<PAPER>
  <FILENO/>
  <TITLE>Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</TITLE>
  <AUTHORS>
    <AUTHOR>George Foster</AUTHOR>
    <AUTHOR>Cyril Goutte</AUTHOR>
  </AUTHORS>
  <ABSTRACT>
    <A-S ID="S-10234">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</A-S>
    <A-S ID="S-10235">This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure.</A-S>
    <A-S ID="S-10236">We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-10237">Domain adaptation is a common concern when optimizing empirical NLP applications.</S>
        <S ID="S-10238">Even when there is training data available in the domain of interest, there is often additional data from other domains that could in principle be used to improve performance.</S>
        <S ID="S-10239">Realizing gains in practice can be challenging, however, particularly when the target domain is distant from the background data.</S>
        <S ID="S-10240">For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.), which precludes a single universal approach to adaptation.</S>
      </P>
      <P>
        <S ID="S-10241">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
        <S ID="S-10242">This is a standard adaptation problem for SMT.</S>
        <S ID="S-10243">It is difficult when IN and OUT are dissimilar, as they are in the cases we study.</S>
        <S ID="S-10244">For simplicity, we assume that OUT is homogeneous.</S>
        <S ID="S-10245">The techniques we develop can be extended in a relatively straightforward manner to the more general case when OUT consists of multiple sub-domains.</S>
      </P>
      <P>
        <S ID="S-10246">There is a fairly large body of work on SMT adaptation.</S>
        <S ID="S-10247">We introduce several new ideas.</S>
        <S ID="S-10248">First, we aim to explicitly characterize examples from OUT as belonging to general language or not.</S>
        <S ID="S-10249">Previous approaches have tried to find examples that are similar to the target domain.</S>
        <S ID="S-10250">This is less effective in our setting, where IN and OUT are disparate.</S>
        <S ID="S-10251">The idea of distinguishing between general and domain-specific examples is due to <REF ID="R-05" RPTR="8">Daum&#233; and Marcu (2006)</REF>, who used a maximum-entropy model with latent variables to capture the degree of specificity.</S>
        <S ID="S-10252"><REF ID="R-06" RPTR="9">Daum&#233; (2007)</REF><REF ID="R-04" RPTR="3">(2007)</REF><REF ID="R-07" RPTR="10">(2007)</REF><REF ID="R-24" RPTR="46">(2007)</REF> applies a related idea in a simpler way, by splitting features into general and domain-specific versions.</S>
        <S ID="S-10253">This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (<REF ID="R-13" RPTR="28">Jiang and Zhai, 2007</REF>) to downweight domain-specific examples in OUT.</S>
        <S ID="S-10254">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
      </P>
      <P>
        <S ID="S-10255">Our second contribution is to apply instance</S>
      </P>
      <P>
        <S ID="S-10256">weighting at the level of phrase pairs.</S>
        <S ID="S-10257">Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.</S>
        <S ID="S-10258">For instance, the sentence Similar improvements in haemoglobin levels were reported in the scientific literature for other epoetins would likely be considered domain-specific despite the presence of general phrases like were reported in.</S>
        <S ID="S-10259">Phrase-level granularity distinguishes our work from previous work by <REF ID="R-19" RPTR="38">Matsoukas et al (2009)</REF>, who weight sentences according to sub-corpus and genre membership.</S>
      </P>
      <P>
        <S ID="S-10260">Finally, we make some improvements to baseline approaches.</S>
        <S ID="S-10261">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
        <S ID="S-10262">This is a simple and effective alternative to setting weights discriminatively to maximize a metric such as BLEU.</S>
        <S ID="S-10263">A similar maximumlikelihood approach was used by <REF ID="R-10" RPTR="22">Foster and Kuhn (2007)</REF><REF ID="R-04" RPTR="4">(2007)</REF><REF ID="R-07" RPTR="11">(2007)</REF><REF ID="R-24" RPTR="47">(2007)</REF>, but for language models only.</S>
        <S ID="S-10264">For comparison to information-retrieval inspired baselines, eg (<REF ID="R-18" RPTR="33">L&#252; et al., 2007</REF>), we select sentences from OUT using language model perplexities from IN.</S>
        <S ID="S-10265">This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries, then pooling the match results.</S>
      </P>
      <P>
        <S ID="S-10266">The paper is structured as follows.</S>
        <S ID="S-10267">Section 2 describes our baseline techniques for SMT adaptation, and section 3 describes the instance-weighting approach.</S>
        <S ID="S-10268">Experiments are presented in section 4.</S>
        <S ID="S-10269">Section 5 covers relevant previous work on SMT adaptation, and section 6 concludes.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Baseline SMT Adaptation Techniques</HEADER>
      <P>
        <S ID="S-10315">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>
        <S ID="S-10316">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S>
        <S ID="S-10317">Thus, provided at least this amount of IN data is available&#8212;as it is in our setting&#8212;adapting these weights is straightforward.</S>
        <S ID="S-10318">We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(w|h) of a target word w following an ngram h; and the translation models (TM) p(s|t) and p(t|s), which give the probability of source phrase s translating to target phrase t, and vice versa.</S>
        <S ID="S-10319">We do not adapt the alignment procedure for generating the phrase table from which the TM distributions are derived.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>2.1 Simple Baselines</HEADER>
        <P>
          <S ID="S-10270">The natural baseline approach is to concatenate data from IN and OUT.</S>
          <S ID="S-10271">Its success depends on the two domains being relatively close, and on the OUT corpus not being so large as to overwhelm the contribution of IN.</S>
        </P>
        <P>
          <S ID="S-10272">When OUT is large and distinct, its contribution can be controlled by training separate IN and OUT models, and weighting their combination.</S>
          <S ID="S-10273">An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (<REF ID="R-21" RPTR="41">Och, 2003</REF>).</S>
          <S ID="S-10274">This has the potential drawback of increasing the number of features, which can make MERT less stable (<REF ID="R-11" RPTR="23">Foster and Kuhn, 2009</REF>).</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.2 Linear Combinations</HEADER>
        <P>
          <S ID="S-10275">Apart from MERT difficulties, a conceptual problem with log-linear combination is that it multiplies feature probabilities, essentially forcing different features to agree on high-scoring candidates.</S>
          <S ID="S-10276">This is appropriate in cases where it is sanctioned by Bayes&#8217; law, such as multiplying LM and TM probabilities, but for adaptation a more suitable framework is often a mixture model in which each event may be generated from some domain.</S>
          <S ID="S-10277">This leads to a linear combination of domain-specific probabilities, with weights in [0, 1], normalized to sum to 1.</S>
        </P>
        <P>
          <S ID="S-10278">Linear weights are difficult to incorporate into the standard MERT procedure because they are &#8220;hidden&#8221; within a top-level probability that represents the linear combination.</S>
          <S ID="S-10279">1 Following previous work (<REF ID="R-10" RPTR="19">Foster and Kuhn, 2007</REF>), we circumvent this problem by choosing weights to optimize corpus loglikelihood, which is roughly speaking the training criterion used by the LM and TM themselves.</S>
        </P>
        <P>
          <S ID="S-10280">1 This precludes the use of exact line-maximization within</S>
        </P>
        <P>
          <S ID="S-10281">Powell&#8217;s algorithm (<REF ID="R-21" RPTR="42">Och, 2003</REF>), for instance.</S>
        </P>
        <P>
          <S ID="S-10282">For the LM, adaptive weights are set as follows: &#8721; &#710;&#945; = argmax &#732;p(w, h) log &#8721; &#945; i p i (w|h), (1)</S>
        </P>
        <P>
          <S ID="S-10283">&#945; i w,h</S>
        </P>
        <P>
          <S ID="S-10284">where &#945; is a weight vector containing an element &#945; i for each domain (just IN and OUT in our case), p i are the corresponding domain-specific models, and &#732;p(w, h) is an empirical distribution from a targetlanguage training corpus&#8212;we used the IN dev set for this.</S>
        </P>
        <P>
          <S ID="S-10285">It is not immediately obvious how to formulate an equivalent to equation (1) for an adapted TM, because there is no well-defined objective for learning TMs from parallel corpora.</S>
          <S ID="S-10286">This has led previous workers to adopt ad hoc linear weighting schemes (<REF ID="R-08" RPTR="15">Finch and Sumita, 2008</REF>; <REF ID="R-10" RPTR="20">Foster and Kuhn, 2007</REF>; <REF ID="R-18" RPTR="34">L&#252; et al., 2007</REF>).</S>
          <S ID="S-10287">However, we note that the final conditional estimates p(s|t) from a given phrase table maximize the likelihood of joint empirical phrase pair counts over a word-aligned corpus.</S>
          <S ID="S-10288">This suggests a direct parallel to (1):</S>
        </P>
        <P>
          <S ID="S-10289">&#710;&#945; = argmax</S>
        </P>
        <P>
          <S ID="S-10290">&#945;</S>
        </P>
        <P>
          <S ID="S-10291">&#8721; &#732;p(s, t) log &#8721;</S>
        </P>
        <P>
          <S ID="S-10292">s,t i</S>
        </P>
        <P>
          <S ID="S-10293">&#945; i p i (s|t), (2)</S>
        </P>
        <P>
          <S ID="S-10294">where &#732;p(s, t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.</S>
          <S ID="S-10295">2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (<REF ID="R-01" RPTR="1">Bacchiani et al., 2004</REF>).</S>
          <S ID="S-10296">For the TM, this is:</S>
        </P>
        <P>
          <S ID="S-10297">p(s|t) = c I(s, t)+&#946;p o (s|t) , (3) c I (t)+&#946;</S>
        </P>
        <P>
          <S ID="S-10298">where c I (s, t) is the count in the IN phrase table of pair (s, t), p o (s|t) is its probability under the OUT TM, and c I (t) = &#8721; s &#8242; c I(s &#8242; ,t).</S>
          <S ID="S-10299">This is motivated by taking &#946;p o (s|t) to be the parameters of a Dirichlet prior on phrase probabilities, then maximizing posterior estimates p(s|t) given the IN corpus.</S>
          <S ID="S-10300">Intuitively, it places more weight on OUT when less evidence from IN is available.</S>
          <S ID="S-10301">To set &#946;, we used the same criterion as for &#945;, over a dev corpus:</S>
        </P>
        <P>
          <S ID="S-10302">&#8721; &#710;&#946; = argmax</S>
        </P>
        <P>
          <S ID="S-10303">&#946; s,t</S>
        </P>
        <P>
          <S ID="S-10304">&#732;p(s, t) log c I(s, t)+&#946;p o (s|t) .</S>
          <S ID="S-10305">c I (t)+&#946;</S>
        </P>
        <P>
          <S ID="S-10306">2 Using non-adapted IBM models trained on all available IN</S>
        </P>
        <P>
          <S ID="S-10307">and OUT data.</S>
        </P>
        <P>
          <S ID="S-10308">The MAP combination was used for TM probabilities only, in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (<REF ID="R-14" RPTR="29">Kneser and Ney, 1995</REF>).</S>
          <S ID="S-10309">3</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.3 Sentence Selection</HEADER>
        <P>
          <S ID="S-10310">Motivated by information retrieval, a number of approaches choose &#8220;relevant&#8221; sentence pairs from OUT by matching individual source sentences from IN (<REF ID="R-12" RPTR="24">Hildebrand et al., 2005</REF>; <REF ID="R-18" RPTR="35">L&#252; et al., 2007</REF>), or individual target hypotheses (<REF ID="R-31" RPTR="55">Zhao et al., 2004</REF>).</S>
          <S ID="S-10311">The matching sentence pairs are then added to the IN corpus, and the system is re-trained.</S>
          <S ID="S-10312">Although matching is done at the sentence level, this information is subsequently discarded when all matches are pooled.</S>
        </P>
        <P>
          <S ID="S-10313">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
          <S ID="S-10314">The number of top-ranked pairs to retain is chosen to optimize dev-set BLEU score.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Instance Weighting</HEADER>
      <P>
        <S ID="S-10430">The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.</S>
        <S ID="S-10431"><REF ID="R-19" RPTR="39">Matsoukas et al (2009)</REF> generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S>
        <S ID="S-10432">The weight on each sentence is a value in [0, 1] computed by a perceptron with Boolean features that indicate collection and genre membership.</S>
      </P>
      <P>
        <S ID="S-10433">We extend the Matsoukas et al approach in several ways.</S>
        <S ID="S-10434">First, we learn weights on individual phrase pairs rather than sentences.</S>
        <S ID="S-10435">Intuitively, as suggested by the example in the introduction, this is the right granularity to capture domain effects.</S>
        <S ID="S-10436">Second, rather than relying on a division of the corpus into manually-assigned portions, we use features intended to capture the usefulness of each phrase pair.</S>
        <S ID="S-10437">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously.</S>
      </P>
      <P>
        <S ID="S-10438">3 <REF ID="R-01" RPTR="0">Bacchiani et al (2004)</REF> solve this problem by reconstituting joint counts from smoothed conditional estimates and unsmoothed marginals, but this seems somewhat unsatisfactory.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>3.1 Model</HEADER>
        <P>
          <S ID="S-10320">The overall adapted TM is a combination of the form:</S>
        </P>
        <P>
          <S ID="S-10321">p(s|t) =&#945; t p I (s|t) + (1 &#8722; &#945; t ) p o (s|t), (4)</S>
        </P>
        <P>
          <S ID="S-10322">where p I (s|t) is derived from the IN corpus using relative-frequency estimates, and p o (s|t) is an instance-weighted model derived from the OUT corpus.</S>
          <S ID="S-10323">This combination generalizes (2) and (3): we use either &#945; t = &#945; to obtain a fixed-weight linear combination, or &#945; t = c I (t)/(c I (t)+&#946;) to obtain a MAP combination.</S>
        </P>
        <P>
          <S ID="S-10324">We model p o (s|t) using a MAP criterion over weighted phrase-pair counts:</S>
        </P>
        <P>
          <S ID="S-10325">p o (s|t) = c &#955;(s, t)+&#947;u(s|t) &#8721;</S>
        </P>
        <P>
          <S ID="S-10326">s &#8242; c &#955;(s &#8242; ,t)+&#947;</S>
        </P>
        <P>
          <S ID="S-10327">(5)</S>
        </P>
        <P>
          <S ID="S-10328">where c &#955; (s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and &#947; is a prior weight.</S>
          <S ID="S-10329">The original OUT counts c o (s, t) are weighted by a logistic function w &#955; (s, t):</S>
        </P>
        <P>
          <S ID="S-10330">c &#955; (s, t) = c o (s, t) w &#955; (s, t) (6)</S>
        </P>
        <P>
          <S ID="S-10331">= c o (s, t) [1 + exp(&#8722; &#8721; i &#955; i f i (s, t))] &#8722;1 ,</S>
        </P>
        <P>
          <S ID="S-10332">where each f i (s, t) is a feature intended to characterize the usefulness of (s, t), weighted by &#955; i .</S>
        </P>
        <P>
          <S ID="S-10333">The mixing parameters and feature weights (collectively &#966;) are optimized simultaneously using devset maximum likelihood as before:</S>
        </P>
        <P>
          <S ID="S-10334">&#710;&#966; = argmax</S>
        </P>
        <P>
          <S ID="S-10335">&#966;</S>
        </P>
        <P>
          <S ID="S-10336">&#8721; &#732;p(s, t) log p(s|t; &#966;).</S>
          <S ID="S-10337">(7)</S>
        </P>
        <P>
          <S ID="S-10338">s,t</S>
        </P>
        <P>
          <S ID="S-10339">This is a somewhat less direct objective than used by Matsoukas et al, who make an iterative approximation to expected TER.</S>
          <S ID="S-10340">However, it is robust, efficient, and easy to implement.</S>
          <S ID="S-10341">4</S>
        </P>
        <P>
          <S ID="S-10342">To perform the maximization in (7), we used the popular L-BFGS algorithm (<REF ID="R-17" RPTR="32">Liu and Nocedal, 1989</REF>), which requires gradient information.</S>
          <S ID="S-10343">Dropping the conditioning on &#966; for brevity, and letting &#175;c &#955; (s, t) = c &#955; (s, t) +&#947;u(s|t), and &#175;c &#955; (t) =</S>
        </P>
        <P>
          <S ID="S-10344">4 Note that the probabilities in (7) need only be evaluated</S>
        </P>
        <P>
          <S ID="S-10345">over the support of &#732;p(s, t), which is quite small when this distribution is derived from a dev set.</S>
          <S ID="S-10346">Maximizing (7) is thus much faster than a typical MERT run.</S>
        </P>
        <P>
          <S ID="S-10347">&#8721;</S>
        </P>
        <P>
          <S ID="S-10348">s &#8242; &#175;c &#955;(s &#8242; ,t):</S>
        </P>
        <P>
          <S ID="S-10349">&#8706; log p(s|t) &#8706;&#945; t = k t</S>
        </P>
        <P>
          <S ID="S-10350">&#8706; log p(s|t) &#8706;&#947; = 1 &#8722; &#945; t</S>
        </P>
        <P>
          <S ID="S-10351">p(s|t)</S>
        </P>
        <P>
          <S ID="S-10352">&#8706; log p(s|t) = 1 &#8722; &#945; t</S>
        </P>
        <P>
          <S ID="S-10353">&#8706;&#955; i p(s|t) [</S>
        </P>
        <P>
          <S ID="S-10354">pI (s|t)</S>
        </P>
        <P>
          <S ID="S-10355">p(s|t) &#8722; p ] o(s|t) p(s|t)</S>
        </P>
        <P>
          <S ID="S-10356">[ u(s|t)</S>
        </P>
        <P>
          <S ID="S-10357">&#175;c &#955; (t) &#8722; &#175;c ] &#955;(s, t) &#175;c &#955; (t) 2 [</S>
        </P>
        <P>
          <S ID="S-10358">c&#955; &#8242; (s, t)</S>
        </P>
        <P>
          <S ID="S-10359">i</S>
        </P>
        <P>
          <S ID="S-10360">&#8722; &#175;c &#955;(s, t)c &#955; &#8242;</S>
        </P>
        <P>
          <S ID="S-10361">i</S>
        </P>
        <P>
          <S ID="S-10362">(t)</S>
        </P>
        <P>
          <S ID="S-10363">&#175;c &#955; (t) &#175;c &#955; (t) 2</S>
        </P>
        <P>
          <S ID="S-10364">where:</S>
        </P>
        <P>
          <S ID="S-10365">{ 1 fixed weight k t = &#8722;c I (t)/(c I (t)+&#946;) 2 MAP</S>
        </P>
        <P>
          <S ID="S-10366">and: c &#955; &#8242;</S>
        </P>
        <P>
          <S ID="S-10367">i</S>
        </P>
        <P>
          <S ID="S-10368">(s, t) =f i (s, t)(1 &#8722; w &#955; (s, t))c &#955; (s, t) c &#955; &#8242;</S>
        </P>
        <P>
          <S ID="S-10369">i</S>
        </P>
        <P>
          <S ID="S-10370">(t) = &#8721; s &#8242; c &#955; &#8242;</S>
        </P>
        <P>
          <S ID="S-10371">i</S>
        </P>
        <P>
          <S ID="S-10372">(s &#8242; ,t).</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.2 Interpretation and Variants</HEADER>
        <P>
          <S ID="S-10373">To motivate weighting joint OUT counts as in (6), we begin with the &#8220;ideal&#8221; objective for setting multinomial phrase probabilities &#952; = {p(s|t), &#8704;st}, which is the likelihood with respect to the true IN distribution p&#206;(s, t).</S>
          <S ID="S-10374"><REF ID="R-13" RPTR="26">Jiang and Zhai (2007)</REF><REF ID="R-04" RPTR="5">(2007)</REF><REF ID="R-07" RPTR="12">(2007)</REF><REF ID="R-24" RPTR="48">(2007)</REF> suggest the following derivation, making use of the true OUT distribution p&#244;(s, t):</S>
        </P>
        <P>
          <S ID="S-10375">= argmax</S>
        </P>
        <P>
          <S ID="S-10376">&#952;</S>
        </P>
        <P>
          <S ID="S-10377">&#8776; argmax</S>
        </P>
        <P>
          <S ID="S-10378">&#952;</S>
        </P>
        <P>
          <S ID="S-10379">&#8721; p&#206;(s, t) log p &#952; (s|t) (8)</S>
        </P>
        <P>
          <S ID="S-10380">s,t</S>
        </P>
        <P>
          <S ID="S-10381">&#8721;</S>
        </P>
        <P>
          <S ID="S-10382">s,t</S>
        </P>
        <P>
          <S ID="S-10383">&#8721;</S>
        </P>
        <P>
          <S ID="S-10384">s,t</S>
        </P>
        <P>
          <S ID="S-10385">p&#206;(s, t) p&#244;(s, t) p &#244;(s, t) log p &#952; (s|t)</S>
        </P>
        <P>
          <S ID="S-10386">p&#206;(s, t) p&#244;(s, t) c o(s, t) log p &#952; (s|t),</S>
        </P>
        <P>
          <S ID="S-10387">where c o (s, t) are the counts from OUT, as in (6).</S>
          <S ID="S-10388">This has solutions:</S>
        </P>
        <P>
          <S ID="S-10389">p&#710;&#952;(s|t) = p &#206;</S>
        </P>
        <P>
          <S ID="S-10390">(s, t) p&#244;(s, t) c o(s, t)/ &#8721; s &#8242; p&#206;(s &#8242; ,t) p&#244;(s &#8242; ,t) c o(s &#8242; ,t),</S>
        </P>
        <P>
          <S ID="S-10391">and from the similarity to (5), assuming &#947; =0, we see that w &#955; (s, t) can be interpreted as approximating p&#206;(s, t)/p&#244;(s, t).</S>
          <S ID="S-10392">The logistic function, whose outputs are in [0, 1], forces p&#206;(s, t) &#8804; p&#244;(s, t).</S>
          <S ID="S-10393">This is not unreasonable given the application to phrase pairs from OUT, but it suggests that an interesting alternative might be to use a plain log-linear weighting ]</S>
        </P>
        <P>
          <S ID="S-10394">function exp( &#8721; i &#955; if i (s, t)), with outputs in [0, &#8734;].</S>
          <S ID="S-10395">We have not yet tried this.</S>
        </P>
        <P>
          <S ID="S-10396">An alternate approximation to (8) would be to let w &#955; (s, t) directly approximate p&#206;(s, t).</S>
          <S ID="S-10397">With the additional assumption that (s, t) can be restricted to the support of c o (s, t), this is equivalent to a &#8220;flat&#8221; alternative to (6) in which each non-zero c o (s, t) is set to one.</S>
          <S ID="S-10398">This variant is tested in the experiments below.</S>
          <S ID="S-10399">A final alternate approach would be to combine weighted joint frequencies rather than conditional estimates, ie: c I (s, t) +w &#955; (s, t)c o (, s, t), suitably normalized.</S>
          <S ID="S-10400">5 Such an approach could be simulated by a MAP-style combination in which separate &#946;(t) values were maintained for each t.</S>
          <S ID="S-10401">This would make the model more powerful, but at the cost of having to learn to downweight OUT separately for each t, which we suspect would require more training data for reliable performance.</S>
          <S ID="S-10402">We have not explored this strategy.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.3 Simple Features</HEADER>
        <P>
          <S ID="S-10403">We used 22 features for the logistic weighting model, divided into two groups: one intended to reflect the degree to which a phrase pair belongs to general language, and one intended to capture similarity to the IN domain.</S>
        </P>
        <P>
          <S ID="S-10404">The 14 general-language features embody straightforward cues: frequency, &#8220;centrality&#8221; as reflected in model scores, and lack of burstiness.</S>
          <S ID="S-10405">They are:</S>
        </P>
        <P>
          <S ID="S-10406">&#8226; total number of tokens in the phrase pair (1);</S>
        </P>
        <P>
          <S ID="S-10407">&#8226; OUT corpus frequency (1);</S>
        </P>
        <P>
          <S ID="S-10408">&#8226; OUT-corpus frequencies of rarest source and target words (2);</S>
        </P>
        <P>
          <S ID="S-10409">&#8226; perplexities for OUT IBM1 models, in both directions (2);</S>
        </P>
        <P>
          <S ID="S-10410">&#8226; average and minimum source and target word &#8220;document frequencies&#8221; in the OUT corpus, using successive 100-line pseudo-documents 6 (4); and</S>
        </P>
        <P>
          <S ID="S-10411">5 We are grateful to an anonymous reviewer for pointing this</S>
        </P>
        <P>
          <S ID="S-10412">out.</S>
          <S ID="S-10413">6 One of our experimental settings lacks document boundaries, and we used this approximation in both settings for consistency.</S>
        </P>
        <P>
          <S ID="S-10414">&#8226; average and minimum source and target word values from the OUT corpus of the following statistic, intended to reflect degree of burstiness (higher values indicate less bursty behaviour): g/(L &#8722; L/(l + 1) + &#603;), where g is the sum over all sentences containing the word of the distance (number of sentences) to the nearest sentence that also contains the word, L is the total number of sentences, l is the number of sentences that contain the word, and &#603; is a small constant (4).</S>
        </P>
        <P>
          <S ID="S-10415">The 8 similarity-to-IN features are based on word frequencies and scores from various models trained on the IN corpus:</S>
        </P>
        <P>
          <S ID="S-10416">&#8226; 1gram and 2gram source and target perplexities according to the IN LM (4); 7</S>
        </P>
        <P>
          <S ID="S-10417">&#8226; source and target OOV counts with respect to IN (2); and</S>
        </P>
        <P>
          <S ID="S-10418">&#8226; perplexities for IN IBM1 models, in both directions (2).</S>
        </P>
        <P>
          <S ID="S-10419">To avoid numerical problems, each feature was normalized by subtracting its mean and dividing by its standard deviation.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.4 SVM Feature</HEADER>
        <P>
          <S ID="S-10420">In addition to using the simple features directly, we also trained an SVM classifier with these features to distinguish between IN and OUT phrase pairs.</S>
          <S ID="S-10421">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples:</S>
        </P>
        <P>
          <S ID="S-10422">1.</S>
          <S ID="S-10423">Pairs from OUT that are not in IN, but whose source phrase is.</S>
        </P>
        <P>
          <S ID="S-10424">2.</S>
          <S ID="S-10425">Pairs from OUT that are not in IN, but whose source phrase is, and where the intersection of IN and OUT translations for that source phrase is empty.</S>
        </P>
        <P>
          <S ID="S-10426">7 In the case of the Chinese experiments below, source LMs</S>
        </P>
        <P>
          <S ID="S-10427">were trained using text segmented with the LDC segmenter, as were the other Chinese models in our system.</S>
        </P>
        <P>
          <S ID="S-10428">The classifier trained using the 2nd definition had higher accuracy on a development set.</S>
          <S ID="S-10429">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 Experiments</HEADER>
      <P>
        <S ID="S-10476"></S>
      </P>
      <DIV DEPTH="1">
        <HEADER>4.1 Corpora and System</HEADER>
        <P>
          <S ID="S-10439">We carried out translation experiments in two different settings.</S>
          <S ID="S-10440">The first setting uses the European Medicines Agency (EMEA) corpus (<REF ID="R-25" RPTR="51">Tiedemann, 2009</REF>) as IN, and the Europarl (EP) corpus (www.statmt.org/europarl) as OUT, for English/French translation in both directions.</S>
          <S ID="S-10441">The dev and test sets were randomly chosen from the EMEA corpus.</S>
          <S ID="S-10442">Figure 1 shows sample sentences from these domains, which are widely divergent.</S>
        </P>
        <P>
          <S ID="S-10443">The second setting uses the news-related subcorpora for the NIST09 MT Chinese to English evaluation 8 as IN, and the remaining NIST parallel Chinese/English corpora (UN, Hong Kong Laws, and Hong Kong Hansard) as OUT.</S>
          <S ID="S-10444">The dev corpus was taken from the NIST05 evaluation set, augmented with some randomly-selected material reserved from the training set.</S>
          <S ID="S-10445">The NIST06 and NIST08 evaluation sets were used for testing.</S>
          <S ID="S-10446">(Thus the domain of the dev and test corpora matches IN.</S>
          <S ID="S-10447">) Compared to the EMEA/EP setting, the two domains in the NIST setting are less homogeneous and more similar to each other; there is also considerably more IN text available.</S>
        </P>
        <P>
          <S ID="S-10448">The corpora for both settings are summarized in table 1.</S>
        </P>
        <P>
          <S ID="S-10449">8 www.itl.nist.gov/iad/mig//tests/mt/2009</S>
        </P>
        <P>
          <S ID="S-10450">The reference medicine for Silapo is EPREX/ERYPO, which contains epoetin alfa.</S>
          <S ID="S-10451">Le m&#233;dicament de r&#233;f&#233;rence de Silapo est EPREX/ERYPO, qui contient de l&#8217;&#233;po&#233;tine alfa.</S>
          <S ID="S-10452">&#8212; I would also like to point out to commissioner Liikanen that it is not easy to take a matter to a national court.</S>
          <S ID="S-10453">Je voudrais pr&#233;ciser, &#224; l&#8217;adresse du commissaire Liikanen, qu&#8217;il n&#8217;est pas ais&#233; de recourir aux tribunaux nationaux.</S>
        </P>
        <P>
          <S ID="S-10454">We used a standard one-pass phrase-based system (<REF ID="R-16" RPTR="31">Koehn et al., 2003</REF>), with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S>
          <S ID="S-10455">Feature weights were set using Och&#8217;s MERT algorithm (<REF ID="R-21" RPTR="43">Och, 2003</REF>).</S>
          <S ID="S-10456">The corpus was wordaligned using both HMM and IBM2 models, and the phrase table was the union of phrases extracted from these separate alignments, with a length limit of 7.</S>
          <S ID="S-10457">It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>4.2 Results</HEADER>
        <P>
          <S ID="S-10458">Table 2 shows results for both settings and all methods described in sections 2 and 3.</S>
          <S ID="S-10459">The 1st block contains the simple baselines from section 2.1.</S>
          <S ID="S-10460">The natural baseline (baseline) outperforms the pure IN system only for EMEA/EP fren.</S>
          <S ID="S-10461">Log-linear combination (loglin) improves on this in all cases, and also beats the pure IN system.</S>
        </P>
        <P>
          <S ID="S-10462">The 2nd block contains the IR system, which was tuned by selecting text in multiples of the size of the EMEA training corpus, according to dev set performance.</S>
          <S ID="S-10463">This significantly underperforms log-linear combination.</S>
        </P>
        <P>
          <S ID="S-10464">The 3rd block contains the mixture baselines.</S>
          <S ID="S-10465">The linear LM (lin lm), TM (lin tm) and MAP TM (map tm) used with non-adapted counterparts perform in all cases slightly worse than the log-linear combination, which adapts both LM and TM components.</S>
          <S ID="S-10466">However, when the linear LM is combined with a</S>
        </P>
        <P>
          <S ID="S-10467">linear TM (lm+lin tm) or MAP TM (lm+map TM), the results are much better than a log-linear combination for the EMEA setting, and on a par for NIST.</S>
          <S ID="S-10468">This is consistent with the nature of these two settings: log-linear combination, which effectively takes the intersection of IN and OUT, does relatively better on NIST, where the domains are broader and closer together.</S>
          <S ID="S-10469">Somewhat surprisingly, there do not appear to be large systematic differences between linear and MAP combinations.</S>
          <S ID="S-10470">The 4th block contains instance-weighting models trained on all features, used within a MAP TM combination, and with a linear LM mixture.</S>
          <S ID="S-10471">The iw all map variant uses a non-0 &#947; weight on a uniform prior in p o (s|t), and outperforms a version with &#947; =0(iw all) and the &#8220;flattened&#8221; variant described in section 3.2.</S>
          <S ID="S-10472">Clearly, retaining the original frequencies is important for good performance, and globally smoothing the final weighted frequencies is crucial.</S>
          <S ID="S-10473">This best instance-weighting model beats the equivalant model without instance weights by between 0.6 BLEU and 1.8 BLEU, and beats the log-linear baseline by a large margin.</S>
        </P>
        <P>
          <S ID="S-10474">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S>
          <S ID="S-10475">The general-language features have a slight advantage over the similarity features, and both are better than the SVM feature.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Related Work</HEADER>
      <P>
        <S ID="S-10477">We have already mentioned the closely related work by <REF ID="R-19" RPTR="40">Matsoukas et al (2009)</REF> on discriminative corpus weighting, and <REF ID="R-13" RPTR="27">Jiang and Zhai (2007)</REF><REF ID="R-04" RPTR="6">(2007)</REF><REF ID="R-07" RPTR="13">(2007)</REF><REF ID="R-24" RPTR="49">(2007)</REF> on (nondiscriminative) instance weighting.</S>
        <S ID="S-10478">It is difficult to directly compare the Matsoukas et al results with ours, since our out-of-domain corpus is homogeneous; given heterogeneous training data, however, it would be trivial to include Matsoukas-style identity features in our instance-weighting model.</S>
        <S ID="S-10479">Although these authors report better gains than ours, they are with respect to a non-adapted baseline.</S>
        <S ID="S-10480">Finally, we note that Jiang&#8217;s instance-weighting framework is broader than we have presented above, encompassing among other possibilities the use of unlabelled IN data, which is applicable to SMT settings where source-only IN corpora are available.</S>
      </P>
      <P>
        <S ID="S-10481">It is also worth pointing out a connection with Daum&#233;&#8217;s <REF ID="R-04" RPTR="7">(2007)</REF><REF ID="R-07" RPTR="14">(2007)</REF><REF ID="R-24" RPTR="50">(2007)</REF> work that splits each feature into domain-specific and general copies.</S>
        <S ID="S-10482">At first glance, this seems only peripherally related to our work, since the specific/general distinction is made for features rather than instances.</S>
        <S ID="S-10483">However, for multinomial models like our LMs and TMs, there is a one to one correspondence between instances and features, eg the correspondence between a phrase pair (s, t) and its conditional multinomial probability p(s|t).</S>
        <S ID="S-10484">As mentioned above, it is not obvious how to apply Daum&#233;&#8217;s approach to multinomials, which do not have a mechanism for combining split features.</S>
        <S ID="S-10485">Recent work by <REF ID="R-09" RPTR="18">Finkel and Manning (2009)</REF> which re-casts Daum&#233;&#8217;s approach in a hierarchical MAP framework may be applicable to this problem.</S>
      </P>
      <P>
        <S ID="S-10486">Moving beyond directly related work, major themes in SMT adaptation include the IR (<REF ID="R-12" RPTR="25">Hildebrand et al., 2005</REF>; <REF ID="R-18" RPTR="36">L&#252; et al., 2007</REF>; <REF ID="R-31" RPTR="56">Zhao et al., 2004</REF>) and mixture (<REF ID="R-08" RPTR="16">Finch and Sumita, 2008</REF>; <REF ID="R-10" RPTR="21">Foster and Kuhn, 2007</REF>; <REF ID="R-15" RPTR="30">Koehn and Schroeder, 2007</REF>; <REF ID="R-18" RPTR="37">L&#252; et al., 2007</REF>) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi</S>
      </P>
      <P>
        <S ID="S-10487">and Federico, 2009; <REF ID="R-26" RPTR="52">Ueffing et al., 2007</REF>; <REF ID="R-22" RPTR="44">Schwenk and Senellart, 2009</REF>).</S>
        <S ID="S-10488">There has also been some work on adapting the word alignment model prior to phrase extraction (<REF ID="R-03" RPTR="2">Civera and Juan, 2007</REF>; <REF ID="R-29" RPTR="53">Wu et al., 2005</REF>), and on dynamically choosing a dev set (<REF ID="R-30" RPTR="54">Xu et al., 2007</REF>).</S>
        <S ID="S-10489">Other work includes transferring latent topic distributions from source to target language for LM adaptation, (<REF ID="R-23" RPTR="45">Tam et al., 2007</REF>) and adapting features at the sentence level to different categories of sentence (<REF ID="R-08" RPTR="17">Finch and Sumita, 2008</REF>).</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>6 Conclusion</HEADER>
      <P>
        <S ID="S-10490">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
        <S ID="S-10491">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S>
        <S ID="S-10492">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair&#8217;s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
        <S ID="S-10493">These estimates are in turn combined linearly with relative-frequency estimates from an in-domain phrase table.</S>
        <S ID="S-10494">Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.</S>
        <S ID="S-10495">We obtained positive results using a very simple phrase-based system in two different adaptation settings: using English/French Europarl to improve a performance on a small, specialized medical domain; and using non-news portions of the NIST09 training material to improve performance on the news-related corpora.</S>
        <S ID="S-10496">In both cases, the instanceweighting approach improved over a wide range of baselines, giving gains of over 2 BLEU points over the best non-adapted baseline, and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).</S>
        <S ID="S-10497">In future work we plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion.</S>
        <S ID="S-10498">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S>
        <S ID="S-10499">Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS/>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>ACL</RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>Michel Bacchiani</RAUTHOR>
      <REFTITLE>Language model adaptation with MAP estimation and the perceptron algorithm.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>Nicola Bertoldi</RAUTHOR>
      <REFTITLE>Domain adaptation for statistical machine translation with monolingual resources.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>Jorge Civera</RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR></RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>Hal Daum&#233;</RAUTHOR>
      <REFTITLE>Domain Adaptation for Statistical Classifiers.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>Hal Daum&#233;</RAUTHOR>
      <REFTITLE>Frustratingly Easy Domain Adaptation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR></RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>Andrew Finch</RAUTHOR>
      <REFTITLE>Dynamic model interpolation for statistical machine translation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>Jenny Rose Finkel</RAUTHOR>
      <REFTITLE>Hierarchical Bayesian domain adaptation.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>George Foster</RAUTHOR>
      <REFTITLE>Mixture-model adaptation for SMT.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>George Foster</RAUTHOR>
      <REFTITLE>Stabilizing minimum error rate training.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>Almut Silja Hildebrand</RAUTHOR>
      <REFTITLE>Adaptation of the translation model for statistical machine translation based on information retrieval.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>Jing Jiang</RAUTHOR>
      <REFTITLE>Instance Weighting for Domain Adaptation in NLP.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="14">
      <RAUTHOR>Reinhard Kneser</RAUTHOR>
      <REFTITLE>Improved backing-off for m-gram language modeling.</REFTITLE>
      <DATE>1995</DATE>
    </REFERENCE>
    <REFERENCE ID="15">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Experiments in domain adaptation for statistical machine translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="16">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Statistical phrase-based translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="17">
      <RAUTHOR>D C Liu</RAUTHOR>
      <REFTITLE>On the limited memory method for large scale optimization.</REFTITLE>
      <DATE>1989</DATE>
    </REFERENCE>
    <REFERENCE ID="18">
      <RAUTHOR>Yajuan L&#252;</RAUTHOR>
      <REFTITLE>Improving Statistical Machine Translation Performance by Training Data Selection and Optimization.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="19">
      <RAUTHOR>Spyros Matsoukas</RAUTHOR>
      <REFTITLE>Discriminative corpus weight estimation for machine translation.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="20">
      <RAUTHOR>NAACL</RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="21">
      <RAUTHOR>Franz Josef Och</RAUTHOR>
      <REFTITLE>Minimum error rate training for statistical machine translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="22">
      <RAUTHOR>Holger Schwenk</RAUTHOR>
      <REFTITLE>Translation model adaptation for an arabic/french news translation system by lightly-supervised training.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="23">
      <RAUTHOR>Yik-Cheung Tam</RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="24">
      <RAUTHOR></RAUTHOR>
      <REFTITLE>Bilingual-LSA Based LM Adaptation for Spoken Language Translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="25">
      <RAUTHOR>Jorg Tiedemann</RAUTHOR>
      <REFTITLE>News from opus - a collection of multilingual parallel corpora with tools and interfaces.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="26">
      <RAUTHOR>Nicola Ueffing</RAUTHOR>
      <REFTITLE>Transductive learning for statistical machine translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="27">
      <RAUTHOR>WMT</RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="28">
      <RAUTHOR>WMT</RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="29">
      <RAUTHOR>Hua Wu</RAUTHOR>
      <REFTITLE>Alignment model adaptation for domain-specific word alignment.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="30">
      <RAUTHOR>Jia Xu</RAUTHOR>
      <REFTITLE>Domain dependent statistical machine translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="31">
      <RAUTHOR>Bing Zhao</RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
