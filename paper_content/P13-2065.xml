<document>
  <filename>P13-2065</filename>
  <authors/>
  <title>Stem Translation with Affix-Based Rule Selection for Agglutinative Languages</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>Current translation models are mainly designed for languages with limited morphology, which are not readily applicable to agglutinative languages as the difference in the way lexical forms are generated. In this paper, we propose a novel approach for translating agglutinative languages by treating stems and affixes differently. We employ stem as the atomic translation unit to alleviate data spareness. In addition, we associate each stemgranularity translation rule with a distribution of related affixes, and select desirable rules according to the similarity of their affix distributions with given spans to be translated. Experimental results show that our approach significantly improves the translation performance on tasks of translating from three Turkic languages to Chinese.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Current translation models are mainly designed for languages with limited morphology, which are not readily applicable to agglutinative languages as the difference in the way lexical forms are generated.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, we propose a novel approach for translating agglutinative languages by treating stems and affixes differently.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We employ stem as the atomic translation unit to alleviate data spareness.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In addition, we associate each stemgranularity translation rule with a distribution of related affixes, and select desirable rules according to the similarity of their affix distributions with given spans to be translated.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Experimental results show that our approach significantly improves the translation performance on tasks of translating from three Turkic languages to Chinese.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Currently, most methods on statistical machine translation (SMT) are developed for translation of languages with limited morphology (e.g., English, Chinese). They assumed that word was the atomic translation unit (ATU), always ignoring the internal morphological structure of word. This assumption can be traced back to the original IBM word-based models (Brown et al., 1993) and several significantly improved models, including phrase-based (Och and Ney, 2004; Koehn et al., 2003), hierarchical (Chiang, 2005) and syntactic (Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006) models. These improved models worked well for translating languages like English with large scale parallel corpora available.
Different from languages with limited morphology, words of agglutinative languages are formed mainly by concatenation of stems and affixes. Generally, a stem can attach with several affixes, thus leading to tens of hundreds of possible inflected variants of lexicons for a single stem. Modeling each lexical form as a separate word will generate high out-of-vocabulary rate for SMT. Theoretically, ways like morphological analysis and increasing bilingual corpora could alleviate the problem of data sparsity, but most agglutinative languages are less-studied and suffer from the problem of resource-scarceness. Therefore, previous research mainly focused on the different inflected variants of the same stem and made various transformation of input by morphological analysis, such as (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006; Habash and Sadat, 2006; Bisazza and Federico, 2009; Wang et al., 2011). These work still assume that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes.
In agglutinative languages, stem is the base part of word not including inflectional affixes. Affix, especially inflectional affix, indicates different grammatical categories such as tense, person, number and case, etc., which is useful for translation rule disambiguation. Therefore, we employ stem as the atomic translation unit and use affix information to guide translation rule selection. Stem-granularity translation rules have much larger coverage and can lower the OOV rate. Affix based rule selection takes advantage of auxiliary syntactic roles of affixes to make a better rule selection. In this way, we can achieve a balance between rule coverage and matching accuracy, and ultimately improve the translation performance.
zunyi yighin ||| &#65533;&#65533; &#65533;&#65533; &#65533; ||| i gha
(A) Instances of translation rule
zunyi yighin ||| &#65533; &#65533;&#65533; &#65533;&#65533; &#65533; ||| i da zunyi yighin ||| &#65533;&#65533; &#65533;&#65533; &#65533; ||| i gha
i /SUF
Original:zunyi yighin+i+da Meaning:on zunyi conference
da /SUF (3)
i /SUF
gha /SUF Original:zunyi yighin+i+gha Meaning:of zunyi conference
(B)Translation rules with affix distribution
zunyi yighin ||| &#65533;&#65533; &#65533;&#65533; &#65533; ||| i:0 gha:0.09 zunyi yighin ||| &#65533; &#65533;&#65533; &#65533;&#65533; &#65533; ||| i:0 da:0.24</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Currently, most methods on statistical machine translation (SMT) are developed for translation of languages with limited morphology (e.g., English, Chinese).</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>They assumed that word was the atomic translation unit (ATU), always ignoring the internal morphological structure of word.</text>
              <doc_id>6</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This assumption can be traced back to the original IBM word-based models (Brown et al., 1993) and several significantly improved models, including phrase-based (Och and Ney, 2004; Koehn et al., 2003), hierarchical (Chiang, 2005) and syntactic (Quirk et al., 2005; Galley et al., 2006; Liu et al., 2006) models.</text>
              <doc_id>7</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>These improved models worked well for translating languages like English with large scale parallel corpora available.</text>
              <doc_id>8</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Different from languages with limited morphology, words of agglutinative languages are formed mainly by concatenation of stems and affixes.</text>
              <doc_id>9</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Generally, a stem can attach with several affixes, thus leading to tens of hundreds of possible inflected variants of lexicons for a single stem.</text>
              <doc_id>10</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Modeling each lexical form as a separate word will generate high out-of-vocabulary rate for SMT.</text>
              <doc_id>11</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Theoretically, ways like morphological analysis and increasing bilingual corpora could alleviate the problem of data sparsity, but most agglutinative languages are less-studied and suffer from the problem of resource-scarceness.</text>
              <doc_id>12</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, previous research mainly focused on the different inflected variants of the same stem and made various transformation of input by morphological analysis, such as (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006; Habash and Sadat, 2006; Bisazza and Federico, 2009; Wang et al., 2011).</text>
              <doc_id>13</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>These work still assume that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes.</text>
              <doc_id>14</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In agglutinative languages, stem is the base part of word not including inflectional affixes.</text>
              <doc_id>15</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Affix, especially inflectional affix, indicates different grammatical categories such as tense, person, number and case, etc., which is useful for translation rule disambiguation.</text>
              <doc_id>16</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, we employ stem as the atomic translation unit and use affix information to guide translation rule selection.</text>
              <doc_id>17</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Stem-granularity translation rules have much larger coverage and can lower the OOV rate.</text>
              <doc_id>18</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Affix based rule selection takes advantage of auxiliary syntactic roles of affixes to make a better rule selection.</text>
              <doc_id>19</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In this way, we can achieve a balance between rule coverage and matching accuracy, and ultimately improve the translation performance.</text>
              <doc_id>20</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>zunyi yighin ||| &#65533;&#65533; &#65533;&#65533; &#65533; ||| i gha</text>
              <doc_id>21</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(A) Instances of translation rule</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>zunyi yighin ||| &#65533; &#65533;&#65533; &#65533;&#65533; &#65533; ||| i da zunyi yighin ||| &#65533;&#65533; &#65533;&#65533; &#65533; ||| i gha</text>
              <doc_id>23</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i /SUF</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Original:zunyi yighin+i+da Meaning:on zunyi conference</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>da /SUF (3)</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i /SUF</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>gha /SUF Original:zunyi yighin+i+gha Meaning:of zunyi conference</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(B)Translation rules with affix distribution</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>zunyi yighin ||| &#65533;&#65533; &#65533;&#65533; &#65533; ||| i:0 gha:0.09 zunyi yighin ||| &#65533; &#65533;&#65533; &#65533;&#65533; &#65533; ||| i:0 da:0.24</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Affix Based Rule Selection Model</title>
        <text>Figure 1 (B) shows two translation rules along with affix distributions. Here a translation rule contains three parts: the source part (on stem level), the target part, and the related affix distribution (represented as a vector). We can see that, although the source part of the two translation rules are identical, their affix distributions are quite different. Affix &#8220;gha&#8221; in the first rule indicates that something is affiliated to a subject, similar to &#8220;of&#8221; in English. And &#8220;da&#8221; in second rule implies location information. Therefore, given a span &#8220;zunyi/STM yighin/STM+i/SUF+da/SUF+...&#8221; to be translated, we hope to encourage our model to select the second translation rule. We can achieve this by calculating similarity between the affix distributions of the translation rule and the span.
The affix distribution can be obtained by keeping the related affixes for each rule instance during translation rule extraction ((A) in Figure 1). After extracting and scoring stem-granularity rules in a traditional way, we extract stem-granularity rules again by keeping affix information and compute the affix distribution with tf-idf (Salton and Buckley, 1987). Finally, the affix distribution will be added to the previous stem-granularity rules.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Figure 1 (B) shows two translation rules along with affix distributions.</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Here a translation rule contains three parts: the source part (on stem level), the target part, and the related affix distribution (represented as a vector).</text>
              <doc_id>32</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We can see that, although the source part of the two translation rules are identical, their affix distributions are quite different.</text>
              <doc_id>33</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Affix &#8220;gha&#8221; in the first rule indicates that something is affiliated to a subject, similar to &#8220;of&#8221; in English.</text>
              <doc_id>34</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>And &#8220;da&#8221; in second rule implies location information.</text>
              <doc_id>35</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, given a span &#8220;zunyi/STM yighin/STM+i/SUF+da/SUF+...&#8221; to be translated, we hope to encourage our model to select the second translation rule.</text>
              <doc_id>36</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We can achieve this by calculating similarity between the affix distributions of the translation rule and the span.</text>
              <doc_id>37</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The affix distribution can be obtained by keeping the related affixes for each rule instance during translation rule extraction ((A) in Figure 1).</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>After extracting and scoring stem-granularity rules in a traditional way, we extract stem-granularity rules again by keeping affix information and compute the affix distribution with tf-idf (Salton and Buckley, 1987).</text>
              <doc_id>39</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Finally, the affix distribution will be added to the previous stem-granularity rules.</text>
              <doc_id>40</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Affix Distribution Estimation</title>
            <text>Formally, translation rule instances with the same source part can be treated as a document collection 1 , so each rule instance in the collection is
1 We employ concepts from text classification to illustrate
how to estimate affix distribution.
some kind of document. Our goal is to classify the source parts into the target parts on the document collection level with the help of affix distribution. Accordingly, we employ vector space model (VSM) to represent affix distribution of each rule instance. In this model, the feature weights are represented by the classic tf-idf (Salton and Buckley, 1987):
tf i,j =
n i,j &#8721;
k n k,j
|D| idf i,j = log |j : a i &#8712; r j | (1)
tfidf i,j = tf i,j &#215; idf i,j
where tfidf i,j is the weight of affix a i in translation rule instance r j . n i,j indicates the number of occurrence of affix a i in r j . |D| is the number of rule instance with the same source part, and |j : a i &#8712; r j | is the number of rule instance which contains affix a i within |D|.
Let&#8217;s take the suffix &#8220;gha&#8221; from (A 1 ) in Figure 1 as an example. We assume that there are only three instances of translation rules extracted from parallel corpus ((A) in Figure 1). We can see that &#8220;gha&#8221; only appear once in (A 1 ) and also appear once in whole instances. Therefore, tf gha,(A1 ) is 0.5 and idf gha,(A1 ) is log(3/2). tfidf gha,(A1 ) is the product of tf gha,(A1 ) and idf gha,(A1 ) which is 0.09.
Given a set of N translation rule instances with the same source and target part, we define the centroid vector d r according to the centroid-based classification algorithm (Han and Karypis, 2000),
d r = 1 &#8721; d i (2) N
i&#8712;N
Data set #Sent.
d r is the final affix distribution. By comparing the similarity of affix distributions, we are able to decide whether a translation rule is suitable for a span to be translated. In this work, similarity is measured using the cosine distance similarity metric, given by
sim(d 1 , d 2 ) =
d 1 &#183; d 2 &#8741;d 1 &#8741; &#215; &#8741;d 2 &#8741; (3)
where d i corresponds to a vector indicating affix distribution, and &#8220;&#183;&#8221; denotes the inner product of the two vectors. Therefore, for a specific span to be translated, we first analyze it to get the corresponding stem sequence and related affix distribution represented as a vector. Then the stem sequence is used to search the translation rule table. If the source part is matched, the similarity will be calculated for each candidate translation rule by cosine similarity (as in equation 3). Therefore, in addition to the traditional translation features on stem level, our model also adds the affix similarity score as a dynamic feature into the log-linear model (Och and Ney, 2002).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Formally, translation rule instances with the same source part can be treated as a document collection 1 , so each rule instance in the collection is</text>
                  <doc_id>41</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 We employ concepts from text classification to illustrate</text>
                  <doc_id>42</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>how to estimate affix distribution.</text>
                  <doc_id>43</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>some kind of document.</text>
                  <doc_id>44</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Our goal is to classify the source parts into the target parts on the document collection level with the help of affix distribution.</text>
                  <doc_id>45</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Accordingly, we employ vector space model (VSM) to represent affix distribution of each rule instance.</text>
                  <doc_id>46</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In this model, the feature weights are represented by the classic tf-idf (Salton and Buckley, 1987):</text>
                  <doc_id>47</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>tf i,j =</text>
                  <doc_id>48</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n i,j &#8721;</text>
                  <doc_id>49</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k n k,j</text>
                  <doc_id>50</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>|D| idf i,j = log |j : a i &#8712; r j | (1)</text>
                  <doc_id>51</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>tfidf i,j = tf i,j &#215; idf i,j</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where tfidf i,j is the weight of affix a i in translation rule instance r j .</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>n i,j indicates the number of occurrence of affix a i in r j .</text>
                  <doc_id>54</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>|D| is the number of rule instance with the same source part, and |j : a i &#8712; r j | is the number of rule instance which contains affix a i within |D|.</text>
                  <doc_id>55</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Let&#8217;s take the suffix &#8220;gha&#8221; from (A 1 ) in Figure 1 as an example.</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We assume that there are only three instances of translation rules extracted from parallel corpus ((A) in Figure 1).</text>
                  <doc_id>57</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We can see that &#8220;gha&#8221; only appear once in (A 1 ) and also appear once in whole instances.</text>
                  <doc_id>58</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, tf gha,(A1 ) is 0.5 and idf gha,(A1 ) is log(3/2).</text>
                  <doc_id>59</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>tfidf gha,(A1 ) is the product of tf gha,(A1 ) and idf gha,(A1 ) which is 0.09.</text>
                  <doc_id>60</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Given a set of N translation rule instances with the same source and target part, we define the centroid vector d r according to the centroid-based classification algorithm (Han and Karypis, 2000),</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>d r = 1 &#8721; d i (2) N</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i&#8712;N</text>
                  <doc_id>63</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Data set #Sent.</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>d r is the final affix distribution.</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>By comparing the similarity of affix distributions, we are able to decide whether a translation rule is suitable for a span to be translated.</text>
                  <doc_id>66</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In this work, similarity is measured using the cosine distance similarity metric, given by</text>
                  <doc_id>67</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>sim(d 1 , d 2 ) =</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>d 1 &#183; d 2 &#8741;d 1 &#8741; &#215; &#8741;d 2 &#8741; (3)</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where d i corresponds to a vector indicating affix distribution, and &#8220;&#183;&#8221; denotes the inner product of the two vectors.</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, for a specific span to be translated, we first analyze it to get the corresponding stem sequence and related affix distribution represented as a vector.</text>
                  <doc_id>71</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Then the stem sequence is used to search the translation rule table.</text>
                  <doc_id>72</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>If the source part is matched, the similarity will be calculated for each candidate translation rule by cosine similarity (as in equation 3).</text>
                  <doc_id>73</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, in addition to the traditional translation features on stem level, our model also adds the affix similarity score as a dynamic feature into the log-linear model (Och and Ney, 2002).</text>
                  <doc_id>74</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Related Work</title>
        <text>Most previous work on agglutinative language translation mainly focus on Turkish and Finnish. Bisazza and Federico (2009) and Mermer and Saraclar (2011) optimized morphological analysis as a pre-processing step to improve the translation between Turkish and English. Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). Yang and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish. Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table. These works still assumed that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes.
There are also some work that employed the context information to make a better choice of translation rules (Carpuat and Wu, 2007; Chan et al., 2007; He et al., 2008; Cui et al., 2010). all the work employed rich context information, such as POS, syntactic, etc., and experiments were mostly done on less inflectional languages (i.e. Chinese, English) and resourceful languages (i.e. Arabic).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Most previous work on agglutinative language translation mainly focus on Turkish and Finnish.</text>
              <doc_id>75</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Bisazza and Federico (2009) and Mermer and Saraclar (2011) optimized morphological analysis as a pre-processing step to improve the translation between Turkish and English.</text>
              <doc_id>76</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007).</text>
              <doc_id>77</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Yang and Kirchhoff (2006) backed off surface form to stem when translating OOV words of Finnish.</text>
              <doc_id>78</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Luong and Kan (2010) and Luong et al. (2010) focused on Finnish-English translation through improving word alignment and enhancing phrase table.</text>
              <doc_id>79</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>These works still assumed that the atomic translation unit is word, stem or morpheme, without considering the difference between stems and affixes.</text>
              <doc_id>80</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>There are also some work that employed the context information to make a better choice of translation rules (Carpuat and Wu, 2007; Chan et al., 2007; He et al., 2008; Cui et al., 2010).</text>
              <doc_id>81</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>all the work employed rich context information, such as POS, syntactic, etc., and experiments were mostly done on less inflectional languages (i.e. Chinese, English) and resourceful languages (i.e. Arabic).</text>
              <doc_id>82</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Experiments</title>
        <text>In this work, we conduct our experiments on three different agglutinative languages, including Uyghur, Kazakh and Kirghiz. All of them are derived from Altaic language family, belonging to Turkic languages, and mostly spoken by people in Central Asia. There are about 24 million people take these languages as mother tongue. All of the tasks are derived from the evaluation of China Workshop of Machine Translation (CWMT) 2 . Table 1 shows the statistics of data sets.
For the language model, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 5-gram model with the target side of training corpus. And phrase-based Moses 3 is used as our
2 http://mt.xmu.edu.cn/cwmt2011/en/index.html. 3 http://www.statmt.org/moses/
baseline SMT system. The decoding weights are optimized with MERT (Och, 2003) to maximum word-level BLEU scores (Papineni et al., 2002).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this work, we conduct our experiments on three different agglutinative languages, including Uyghur, Kazakh and Kirghiz.</text>
              <doc_id>83</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>All of them are derived from Altaic language family, belonging to Turkic languages, and mostly spoken by people in Central Asia.</text>
              <doc_id>84</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>There are about 24 million people take these languages as mother tongue.</text>
              <doc_id>85</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>All of the tasks are derived from the evaluation of China Workshop of Machine Translation (CWMT) 2 .</text>
              <doc_id>86</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Table 1 shows the statistics of data sets.</text>
              <doc_id>87</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For the language model, we use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 5-gram model with the target side of training corpus.</text>
              <doc_id>88</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>And phrase-based Moses 3 is used as our</text>
              <doc_id>89</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2 http://mt.xmu.edu.cn/cwmt2011/en/index.html.</text>
              <doc_id>90</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>3 http://www.statmt.org/moses/</text>
              <doc_id>91</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>baseline SMT system.</text>
              <doc_id>92</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The decoding weights are optimized with MERT (Och, 2003) to maximum word-level BLEU scores (Papineni et al., 2002).</text>
              <doc_id>93</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Using Unsupervised Morphological Analyzer</title>
            <text>As most agglutinative languages are resourcepoor, we employ unsupervised learning method to obtain the morphological structure. Following the approach in (Virpioja et al., 2007), we employ the Morfessor 4 Categories-MAP algorithm (Creutz and Lagus, 2005). It applies a hierarchical model with three categories (prefix, stem, and suffix) in an unsupervised way. From Table 1 we can see that vocabulary sizes of the three languages are reduced obviously after unsupervised morphological analysis. Table 2 shows the translation results. All the three translation tasks achieve obvious improvements with the proposed model, which always performs better than only employ word, stem and morph. For the Uyghur to Chinese translation (UY-CH) task in Table 2, performances after unsupervised morphological analysis are always better than the baseline. And we gain up to +2.6 BLEU points improvements with affix compared to the baseline. For the Kazakh to Chinese translation (KA-CH) task, the improvements are also significant. We achieve +2.27 and +0.77 improvements compared to the baseline and stem, respectively. As for the Kirghiz to Chinese translation (KI-CH) task, improvements seem relative small compared to the other two language pairs. However, it also gains +0.91 BLEU points over the baseline.
4 http://www.cis.hut.fi/projects/morpho/
BLEU score(%)
36.5
36 35.5
35 34.5
34 33.5
33 32.5
32 31.5 word
morph
Unsupervised Supervised
stem</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>As most agglutinative languages are resourcepoor, we employ unsupervised learning method to obtain the morphological structure.</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Following the approach in (Virpioja et al., 2007), we employ the Morfessor 4 Categories-MAP algorithm (Creutz and Lagus, 2005).</text>
                  <doc_id>95</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It applies a hierarchical model with three categories (prefix, stem, and suffix) in an unsupervised way.</text>
                  <doc_id>96</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>From Table 1 we can see that vocabulary sizes of the three languages are reduced obviously after unsupervised morphological analysis.</text>
                  <doc_id>97</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Table 2 shows the translation results.</text>
                  <doc_id>98</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>All the three translation tasks achieve obvious improvements with the proposed model, which always performs better than only employ word, stem and morph.</text>
                  <doc_id>99</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>For the Uyghur to Chinese translation (UY-CH) task in Table 2, performances after unsupervised morphological analysis are always better than the baseline.</text>
                  <doc_id>100</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>And we gain up to +2.6 BLEU points improvements with affix compared to the baseline.</text>
                  <doc_id>101</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>For the Kazakh to Chinese translation (KA-CH) task, the improvements are also significant.</text>
                  <doc_id>102</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>We achieve +2.27 and +0.77 improvements compared to the baseline and stem, respectively.</text>
                  <doc_id>103</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>As for the Kirghiz to Chinese translation (KI-CH) task, improvements seem relative small compared to the other two language pairs.</text>
                  <doc_id>104</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>However, it also gains +0.91 BLEU points over the baseline.</text>
                  <doc_id>105</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4 http://www.cis.hut.fi/projects/morpho/</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BLEU score(%)</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>36.5</text>
                  <doc_id>108</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>36 35.5</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>35 34.5</text>
                  <doc_id>110</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>34 33.5</text>
                  <doc_id>111</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>33 32.5</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>32 31.5 word</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>morph</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Unsupervised Supervised</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>stem</text>
                  <doc_id>116</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Using Supervised Morphological Analyzer</title>
            <text>Taking it further, we also want to see the effect of supervised analysis on our model. A generative statistical model of morphological analysis for Uyghur was developed according to (Mairehaba et al., 2012). Table 3 shows the difference of statistics of training corpus after supervised and unsupervised analysis. Supervised method generates fewer type of stems and affixes than the unsupervised approach. As we can see from Figure 2, except for the morph method, stem and affix based approaches perform better after supervised analysis. The results show that our approach can obtain even better translation performance if better morphological analyzers are available. Supervised morphological analysis generates more meaningful morphemes, which lead to better disambiguation of translation rules.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Taking it further, we also want to see the effect of supervised analysis on our model.</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A generative statistical model of morphological analysis for Uyghur was developed according to (Mairehaba et al., 2012).</text>
                  <doc_id>118</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Table 3 shows the difference of statistics of training corpus after supervised and unsupervised analysis.</text>
                  <doc_id>119</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Supervised method generates fewer type of stems and affixes than the unsupervised approach.</text>
                  <doc_id>120</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>As we can see from Figure 2, except for the morph method, stem and affix based approaches perform better after supervised analysis.</text>
                  <doc_id>121</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The results show that our approach can obtain even better translation performance if better morphological analyzers are available.</text>
                  <doc_id>122</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Supervised morphological analysis generates more meaningful morphemes, which lead to better disambiguation of translation rules.</text>
                  <doc_id>123</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Conclusions and Future Work</title>
        <text>In this paper we propose a novel framework for agglutinative language translation by treating stem and affix differently. We employ the stem sequence as the main part for training and decoding. Besides, we associate each stem-granularity translation rule with an affix distribution, which could be used to make better translation decisions by calculating the affix distribution similarity be- affix 367 tween the rule and the instance to be translated. We conduct our model on three different language pairs, all of which substantially improved the translation performance. The procedure is totally language-independent, and we expect that other language pairs could benefit from our approach.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper we propose a novel framework for agglutinative language translation by treating stem and affix differently.</text>
              <doc_id>124</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We employ the stem sequence as the main part for training and decoding.</text>
              <doc_id>125</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Besides, we associate each stem-granularity translation rule with an affix distribution, which could be used to make better translation decisions by calculating the affix distribution similarity be- affix 367 tween the rule and the instance to be translated.</text>
              <doc_id>126</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We conduct our model on three different language pairs, all of which substantially improved the translation performance.</text>
              <doc_id>127</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The procedure is totally language-independent, and we expect that other language pairs could benefit from our approach.</text>
              <doc_id>128</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>Acknowledgments</title>
        <text>like to thank the anonymous reviewers for their insightful comments and those who helped to modify the paper.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>like to thank the anonymous reviewers for their insightful comments and those who helped to modify the paper.</text>
              <doc_id>129</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Statistics of data sets. &#8727;N means the number of reference, morph is short to morpheme. UY, KA, KI, CH represent Uyghur, Kazakh, Kirghiz and Chinese respectively.</caption>
        <reference_text>In PAGE 3: ... All of the tasks are derived from the evaluation of Chi- na Workshop of Machine Translation (CWMT)2.  Table1  shows the statistics of data sets. For the language model, we use the SRI Lan- guage Modeling Toolkit (Stolcke, 2002) to train a 5-gram model with the target side of training corpus....</reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>Data set</cell>
              <cell>#Sent.</cell>
              <cell>word</cell>
              <cell>#Type   stem</cell>
              <cell>morph</cell>
              <cell>word</cell>
              <cell>#Token   stem</cell>
              <cell>morph</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>UY-CH-Train.</cell>
              <cell>50K</cell>
              <cell>69K</cell>
              <cell>39K</cell>
              <cell>42K</cell>
              <cell>1.2M</cell>
              <cell>1.2M</cell>
              <cell>1.6M</cell>
            </row>
            <row>
              <cell>UY-CH-Dev.</cell>
              <cell>0.7K*4</cell>
              <cell>5.9K</cell>
              <cell>4.1K</cell>
              <cell>4.6K</cell>
              <cell>18K</cell>
              <cell>18K</cell>
              <cell>23.5K</cell>
            </row>
            <row>
              <cell>UY-CH-Test.</cell>
              <cell>0.7K*1</cell>
              <cell>4.7K</cell>
              <cell>3.3K</cell>
              <cell>3.8K</cell>
              <cell>14K</cell>
              <cell>14K</cell>
              <cell>17.8K</cell>
            </row>
            <row>
              <cell>KA-CH-Train.</cell>
              <cell>50K</cell>
              <cell>62K</cell>
              <cell>40K</cell>
              <cell>42K</cell>
              <cell>1.1M</cell>
              <cell>1.1M</cell>
              <cell>1.3M</cell>
            </row>
            <row>
              <cell>KA-CH-Dev.</cell>
              <cell>0.7K*4</cell>
              <cell>5.3K</cell>
              <cell>4.2K</cell>
              <cell>4.5K</cell>
              <cell>15K</cell>
              <cell>15K</cell>
              <cell>18K</cell>
            </row>
            <row>
              <cell>KA-CH-Test.</cell>
              <cell>0.2K*1</cell>
              <cell>2.6K</cell>
              <cell>2.0K</cell>
              <cell>2.3K</cell>
              <cell>8.6K</cell>
              <cell>8.6K</cell>
              <cell>10.8K</cell>
            </row>
            <row>
              <cell>KI-CH-Train.</cell>
              <cell>50K</cell>
              <cell>53K</cell>
              <cell>27K</cell>
              <cell>31K</cell>
              <cell>1.2M</cell>
              <cell>1.2M</cell>
              <cell>1.5M</cell>
            </row>
            <row>
              <cell>KI-CH-Dev.</cell>
              <cell>0.5K*4</cell>
              <cell>4.1K</cell>
              <cell>3.1K</cell>
              <cell>3.5K</cell>
              <cell>12K</cell>
              <cell>12K</cell>
              <cell>15K</cell>
            </row>
            <row>
              <cell>KI-CH-Test.</cell>
              <cell>0.2K*4</cell>
              <cell>2.2K</cell>
              <cell>1.8K</cell>
              <cell>2.1K</cell>
              <cell>4.7K</cell>
              <cell>4.7K</cell>
              <cell>5.8K</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Translation results from Turkic languages to Chinese. word: ATU is surface form, stem: ATU is represented stem, morph: ATU denotes morpheme, affix: stem translation with affix distribution similarity. BLEU scores in bold means significantly better than the baseline according to (Koehn, 2004) for p-value less than 0.01.</caption>
        <reference_text></reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell>UY-CH</cell>
              <cell>KA-CH</cell>
              <cell>KI-CH</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>word</cell>
              <cell>31.74 +0.0</cell>
              <cell>28.64 +0.0</cell>
              <cell>35.05 +0.0</cell>
            </row>
            <row>
              <cell>stem</cell>
              <cell>33.74 +2.0</cell>
              <cell>30.14 +1.5</cell>
              <cell>35.52 +0.47</cell>
            </row>
            <row>
              <cell>morph</cell>
              <cell>32.69 +0.95</cell>
              <cell>29.21 +0.57</cell>
              <cell>34.97 &#8722;0.08</cell>
            </row>
            <row>
              <cell>affix</cell>
              <cell>34.34 +2.6</cell>
              <cell>30.19 +2.27</cell>
              <cell>35.96 +0.91</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Statistics of training corpus after unsupervised(Unsup) and supervised(Sup) morphological analysis.</caption>
        <reference_text>In PAGE 4: ..., 2012).  Table3  shows the difference of statistics of training corpus after supervised and unsupervised analysis. Supervised method gen- erates fewer type of stems and affixes than the unsupervised approach....</reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>36.5</cell>
              <cell>Unsupervised</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>35.5  36</cell>
              <cell>Supervised</cell>
            </row>
            <row>
              <cell>3</cell>
              <cell>4</cell>
              <cell>.</cell>
              <cell>5</cell>
              <cell></cell>
              <cell></cell>
              <cell>3</cell>
              <cell>5</cell>
            </row>
            <row>
              <cell>3</cell>
              <cell>3</cell>
              <cell>.</cell>
              <cell>5</cell>
              <cell></cell>
              <cell></cell>
              <cell>3</cell>
              <cell>4</cell>
            </row>
            <row>
              <cell>32.5  33 BLEU score(%)</cell>
              <cell>BLEU score(%)</cell>
            </row>
            <row>
              <cell>3</cell>
              <cell>1</cell>
              <cell>.</cell>
              <cell>5</cell>
              <cell></cell>
              <cell></cell>
              <cell>3</cell>
              <cell>2</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>wordmorphstemaffix</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Arianna Bisazza</author>
          <author>Marcello Federico</author>
        </authors>
        <title>Morphological pre-processing for Turkish to English statistical machine translation.</title>
        <publication>In Proceedings of IWSLT,</publication>
        <pages>129--135</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Peter F Brown</author>
          <author>Vincent J Della Pietra</author>
          <author>Stephen A Della Pietra</author>
          <author>Robert L Mercer</author>
        </authors>
        <title>The mathematics of statistical machine translation: parameter estimation.</title>
        <publication>None</publication>
        <pages>311</pages>
        <date>1993</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Marine Carpuat</author>
          <author>Dekai Wu</author>
        </authors>
        <title>Improving statistical machine translation using word sense disambiguation.</title>
        <publication>In Proceedings of EMNLP-CoNLL,</publication>
        <pages>61--72</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Yee Seng Chan</author>
          <author>Hwee Tou Ng</author>
          <author>David Chiang</author>
        </authors>
        <title>Word sense disambiguation improves statistical machine translation.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>33--40</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>A hierarchical phrasebased model for statistical machine translation.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>263--270</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Mathias Creutz</author>
          <author>Krista Lagus</author>
        </authors>
        <title>Inducing the morphological lexicon of a natural language from unannotated text.</title>
        <publication>In Proceedings of AKRR,</publication>
        <pages>106--113</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Lei Cui</author>
          <author>Dongdong Zhang</author>
          <author>Mu Li</author>
          <author>Ming Zhou</author>
          <author>Tiejun Zhao</author>
        </authors>
        <title>A joint rule selection model for hierarchical phrase-based translation.</title>
        <publication>In Proceedings of ACL, Short Papers,</publication>
        <pages>6--11</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Michel Galley</author>
          <author>Jonathan Graehl</author>
          <author>Kevin Knight</author>
          <author>Daniel Marcu</author>
          <author>Steve DeNeefe</author>
          <author>Wei Wang</author>
          <author>Ignacio Thayer</author>
        </authors>
        <title>Scalable inference and training of context-rich syntactic translation models.</title>
        <publication>In Proceedings of COLING/ACL,</publication>
        <pages>961--968</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Sharon Goldwater</author>
          <author>David McClosky</author>
        </authors>
        <title>Improving statistical MT through morphological analysis.</title>
        <publication>In Proceedings of HLT-EMNLP,</publication>
        <pages>676--683</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Nizar Habash</author>
          <author>Fatiha Sadat</author>
        </authors>
        <title>Arabic preprocessing schemes for statistical machine translation.</title>
        <publication>In Proceedings of NAACL, Short Papers,</publication>
        <pages>49--52</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Eui-Hong Sam Han</author>
          <author>George Karypis</author>
        </authors>
        <title>Centroid-based document classification: analysis experimental results.</title>
        <publication>In Proceedings of PKDD,</publication>
        <pages>424--431</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Zhongjun He</author>
          <author>Qun Liu</author>
          <author>Shouxun Lin</author>
        </authors>
        <title>Improving statistical machine translation using lexicalized rule selection.</title>
        <publication>In Proceedings of COLING,</publication>
        <pages>321--328</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Hieu Hoang</author>
        </authors>
        <title>Factored translation models.</title>
        <publication>In Proceedings of EMNLPCoNLL,</publication>
        <pages>868--876</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz Josef Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical phrase-based translation.</title>
        <publication>In Proceedings of NAACL,</publication>
        <pages>48--54</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Philipp Koehn</author>
        </authors>
        <title>Statistical significance tests for machine translation evaluation.</title>
        <publication>In Proceedings of EMNLP,</publication>
        <pages>388--395</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Young-Suk Lee</author>
        </authors>
        <title>Morphological analysis for statistical machine translation.</title>
        <publication>In Proceedings of HLT-NAACL, Short Papers,</publication>
        <pages>57--60</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Yang Liu</author>
          <author>Qun Liu</author>
          <author>Shouxun Lin</author>
        </authors>
        <title>Treeto-string alignment template for statistical machine translation.</title>
        <publication>In Proceedings of COLING-ACL,</publication>
        <pages>609--616</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Minh-Thang Luong</author>
          <author>Min-Yen Kan</author>
        </authors>
        <title>Enhancing morphological alignment for translating highly inflected languages.</title>
        <publication>In Proceedings of COLING,</publication>
        <pages>743--751</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Minh-Thang Luong</author>
          <author>Preslav Nakov</author>
          <author>Min-Yen Kan</author>
        </authors>
        <title>A hybrid morpheme-word representation for machine translation of morphologically rich languages.</title>
        <publication>In Proceedings of EMNLP,</publication>
        <pages>148--157</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Aili Mairehaba</author>
          <author>Wenbin Jiang</author>
          <author>Zhiyang Wang</author>
          <author>Yibulayin Tuergen</author>
          <author>Qun Liu</author>
        </authors>
        <title>Directed graph model of Uyghur morphological analysis.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Coskun Mermer</author>
          <author>Murat Saraclar</author>
        </authors>
        <title>Unsupervised Turkish morphological segmentation for statistical machine translation.</title>
        <publication>In Workshop of MT and Morphologically-rich Languages.</publication>
        <pages>None</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Discriminative training and maximum entropy models for statistical machine translation.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>295--302</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>The alignment template approach to statistical machine translation.</title>
        <publication>None</publication>
        <pages>417--449</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>160--167</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>BLEU: a method for automatic evaluation of machine translation.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>311--318</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Chris Quirk</author>
          <author>Arul Menezes</author>
          <author>Colin Cherry</author>
        </authors>
        <title>Dependency treelet translation: syntactically informed phrasal SMT.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>271--279</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>Gerard Salton</author>
          <author>Chris Buckley</author>
        </authors>
        <title>Term weighting approaches in automatic text retrieval.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1987</date>
      </reference>
      <reference>
        <id>27</id>
        <authors>
          <author>Andreas Stolcke</author>
        </authors>
        <title>SRILM - an extensible language modeling toolkit.</title>
        <publication>In Proceedings of ICSLP,</publication>
        <pages>311--318</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>28</id>
        <authors>
          <author>Sami Virpioja</author>
          <author>Jaakko J V&#228;yrynen</author>
          <author>Mathias Creutz</author>
          <author>Markus Sadeniemi</author>
        </authors>
        <title>Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner.</title>
        <publication>In Proceedings of MT SUMMIT,</publication>
        <pages>491--498</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>29</id>
        <authors>
          <author>Zhiyang Wang</author>
          <author>Yajuan L&#252;</author>
          <author>Qun Liu</author>
        </authors>
        <title>Multi-granularity word alignment and decoding for agglutinative language translation.</title>
        <publication>In Proceedings of MT SUMMIT,</publication>
        <pages>360--367</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>30</id>
        <authors>
          <author>Mei Yang</author>
          <author>Katrin Kirchhoff</author>
        </authors>
        <title>Phrase-based backoff models for machine translation of highly inflected languages.</title>
        <publication>In Proceedings of EACL,</publication>
        <pages>1017--1020</pages>
        <date>2006</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Bisazza and Federico (2009)</string>
        <sentence_id>38420</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>0</reference_id>
        <string>Bisazza and Federico, 2009</string>
        <sentence_id>38357</sentence_id>
        <char_offset>265</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>Brown et al., 1993</string>
        <sentence_id>38351</sentence_id>
        <char_offset>74</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>2</reference_id>
        <string>Carpuat and Wu, 2007</string>
        <sentence_id>38425</sentence_id>
        <char_offset>109</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>3</reference_id>
        <string>Chan et al., 2007</string>
        <sentence_id>38425</sentence_id>
        <char_offset>131</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>4</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>38351</sentence_id>
        <char_offset>215</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>5</reference_id>
        <string>Creutz and Lagus, 2005</string>
        <sentence_id>38428</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>6</reference_id>
        <string>Cui et al., 2010</string>
        <sentence_id>38425</sentence_id>
        <char_offset>167</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>7</reference_id>
        <string>Galley et al., 2006</string>
        <sentence_id>38351</sentence_id>
        <char_offset>264</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>8</reference_id>
        <string>Goldwater and McClosky, 2005</string>
        <sentence_id>38357</sentence_id>
        <char_offset>185</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>9</reference_id>
        <string>Habash and Sadat, 2006</string>
        <sentence_id>38357</sentence_id>
        <char_offset>241</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>10</reference_id>
        <string>Han and Karypis, 2000</string>
        <sentence_id>38395</sentence_id>
        <char_offset>174</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>11</reference_id>
        <string>He et al., 2008</string>
        <sentence_id>38425</sentence_id>
        <char_offset>150</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>12</reference_id>
        <string>Koehn and Hoang, 2007</string>
        <sentence_id>38421</sentence_id>
        <char_offset>130</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>13</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>38351</sentence_id>
        <char_offset>180</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>15</reference_id>
        <string>Lee, 2004</string>
        <sentence_id>38357</sentence_id>
        <char_offset>174</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>16</reference_id>
        <string>Liu et al., 2006</string>
        <sentence_id>38351</sentence_id>
        <char_offset>285</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>17</reference_id>
        <string>Luong and Kan (2010)</string>
        <sentence_id>38423</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>18</reference_id>
        <string>Luong et al. (2010)</string>
        <sentence_id>38423</sentence_id>
        <char_offset>25</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>19</reference_id>
        <string>Mairehaba et al., 2012</string>
        <sentence_id>38451</sentence_id>
        <char_offset>96</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>20</reference_id>
        <string>Mermer and Saraclar (2011)</string>
        <sentence_id>38420</sentence_id>
        <char_offset>32</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>21</reference_id>
        <string>Och and Ney, 2002</string>
        <sentence_id>38408</sentence_id>
        <char_offset>173</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>22</reference_id>
        <string>Och and Ney, 2004</string>
        <sentence_id>38351</sentence_id>
        <char_offset>161</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>23</reference_id>
        <string>Och, 2003</string>
        <sentence_id>38467</sentence_id>
        <char_offset>46</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>24</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>38467</sentence_id>
        <char_offset>92</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>25</reference_id>
        <string>Quirk et al., 2005</string>
        <sentence_id>38351</sentence_id>
        <char_offset>244</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>26</reference_id>
        <string>Salton and Buckley, 1987</string>
        <sentence_id>38417</sentence_id>
        <char_offset>191</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>26</reference_id>
        <string>Salton and Buckley, 1987</string>
        <sentence_id>38381</sentence_id>
        <char_offset>74</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>27</reference_id>
        <string>Stolcke, 2002</string>
        <sentence_id>38462</sentence_id>
        <char_offset>66</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>28</reference_id>
        <string>Virpioja et al., 2007</string>
        <sentence_id>38428</sentence_id>
        <char_offset>27</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>29</reference_id>
        <string>Wang et al., 2011</string>
        <sentence_id>38357</sentence_id>
        <char_offset>293</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>30</reference_id>
        <string>Yang and Kirchhoff, 2006</string>
        <sentence_id>38357</sentence_id>
        <char_offset>215</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>30</reference_id>
        <string>Yang and Kirchhoff (2006)</string>
        <sentence_id>38422</sentence_id>
        <char_offset>0</char_offset>
      </citation>
    </citations>
  </content>
</document>
