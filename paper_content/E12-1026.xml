<document>
  <filename>E12-1026</filename>
  <authors>
    <author>Gennadi Lembersky</author>
    <author>Noam Ordan</author>
  </authors>
  <title>Adapting Translation Models to Translationese Improves SMT</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>Translation models used for statistical machine translation are compiled from parallel corpora; such corpora are manually translated, but the direction of translation is usually unknown, and is consequently ignored. However, much research in Translation Studies indicates that the direction of translation matters, as translated language (translationese) has many unique properties. Specifically, phrase tables constructed from parallel corpora translated in the same direction as the translation task perform better than ones constructed from corpora translated in the opposite direction.
We reconfirm that this is indeed the case, but emphasize the importance of using also texts translated in the &#8216;wrong&#8217; direction. We take advantage of information pertaining to the direction of translation in constructing phrase tables, by adapting the translation model to the special properties of translationese. We define entropybased measures that estimate the correspondence of target-language phrases to translationese, thereby eliminating the need to annotate the parallel corpus with information pertaining to the direction of translation. We show that incorporating these measures as features in the phrase tables of statistical machine translation systems results in consistent, statistically significant improvement in the quality of the translation.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Translation models used for statistical machine translation are compiled from parallel corpora; such corpora are manually translated, but the direction of translation is usually unknown, and is consequently ignored.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, much research in Translation Studies indicates that the direction of translation matters, as translated language (translationese) has many unique properties.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Specifically, phrase tables constructed from parallel corpora translated in the same direction as the translation task perform better than ones constructed from corpora translated in the opposite direction.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We reconfirm that this is indeed the case, but emphasize the importance of using also texts translated in the &#8216;wrong&#8217; direction.</text>
              <doc_id>3</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We take advantage of information pertaining to the direction of translation in constructing phrase tables, by adapting the translation model to the special properties of translationese.</text>
              <doc_id>4</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We define entropybased measures that estimate the correspondence of target-language phrases to translationese, thereby eliminating the need to annotate the parallel corpus with information pertaining to the direction of translation.</text>
              <doc_id>5</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We show that incorporating these measures as features in the phrase tables of statistical machine translation systems results in consistent, statistically significant improvement in the quality of the translation.</text>
              <doc_id>6</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Much research in Translation Studies indicates that translated texts have unique characteristics that set them apart from original texts (Toury, 1980; Gellerstam, 1986; Toury, 1995). Known as translationese, translated texts (in any language) constitute a genre, or a dialect, of the target language, which reflects both artifacts of the translation process and traces of the original language from which the texts were translated. Among the better-known properties of translationese are simplification and explicitation (Baker, 1993, 1995, 1996): translated texts tend to be shorter, to have lower type/token ratio, and to use certain discourse markers more frequently than original texts. Incidentally, translated texts are so markedly different from original ones that automatic classification can identify them with very high accuracy (van Halteren, 2008; Baroni and Bernardini, 2006; Ilisei et al., 2010; Koppel and Ordan, 2011). Contemporary Statistical Machine Translation
(SMT) systems use parallel corpora to train translation models that reflect source- and targetlanguage phrase correspondences. Typically, SMT systems ignore the direction of translation used to produce those corpora. Given the unique properties of translationese, however, it is reasonable to assume that this direction may affect the quality of the translation. Recently, Kurokawa et al. (2009) showed that this is indeed the case. They train a system to translate between French and English (and vice versa) using a Frenchtranslated-to-English parallel corpus, and then an English-translated-to-French one. They find that in translating into French the latter parallel corpus yields better results, whereas for translating into English it is better to use the former.
Usually, of course, the translation direction of a parallel corpus is unknown. Therefore, Kurokawa et al. (2009) train an SVM-based classifier to predict which side of a bi-text is the origin and which one is the translation, and only use the subset of the corpus that corresponds to the translation direction of the task in training their translation model.
We use these results as our departure point, but improve them in two major ways. First, we demonstrate that the other subset of the corpus, reflecting translation in the &#8216;wrong&#8217; direction, is also important for the translation task, and must not be ignored; second, we show that explicit information on the direction of translation of the parallel corpus, whether manually-annotated or machine-learned, is not mandatory. This is achieved by casting the problem in the framework of domain adaptation: we use domain-adaptation techniques to direct the SMT system toward producing output that better reflects the properties of translationese. We show that SMT systems adapted to translationese produce better translations than vanilla systems trained on exactly the same resources. We confirm these findings using an automatic evaluation metric, BLEU (Papineni et al., 2002), as well as through a qualitative analysis of the results.
Our departure point is the results of Kurokawa et al. (2009), which we successfully replicate in Section 3. First (Section 4), we explain why translation quality improves when the parallel corpus is translated in the &#8216;right&#8217; direction. We do so by showing that the subset of the corpus that was translated in the direction of the translation task (the &#8216;right&#8217; direction, henceforth source-to-target, or S &#8594; T ) yields phrase tables that are better suited for translation of the original language than the subset translated in the reverse direction (the &#8216;wrong&#8217; direction, henceforth target-to-source, or T &#8594; S). We use several statistical measures that indicate the better quality of the phrase tables in the former case.
Then (Section 5), we explore ways to build a translation model that is adapted to the unique properties of translationese. We first show that using the entire parallel corpus, including texts that are translated both in the &#8216;right&#8217; and in the &#8216;wrong&#8217; direction, improves the quality of the results. Furthermore, we show that the direction of translation used for producing the parallel corpus can be approximated by defining several entropybased measures that correlate well with translationese, and, consequently, with the quality of the translation.
Specifically, we use the entire corpus, create a single, unified phrase table and then use the statistical measures mentioned above, and in particular cross-entropy, as a clue for selecting phrase pairs from this table. The benefit of this method is that not only does it yield the best results, but it also eliminates the need to directly predict the direction of translation of the parallel corpus. The main contribution of this work, therefore, is a methodology that improves the quality of SMT by building translation models that are adapted to the nature of translationese.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Much research in Translation Studies indicates that translated texts have unique characteristics that set them apart from original texts (Toury, 1980; Gellerstam, 1986; Toury, 1995).</text>
              <doc_id>7</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Known as translationese, translated texts (in any language) constitute a genre, or a dialect, of the target language, which reflects both artifacts of the translation process and traces of the original language from which the texts were translated.</text>
              <doc_id>8</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Among the better-known properties of translationese are simplification and explicitation (Baker, 1993, 1995, 1996): translated texts tend to be shorter, to have lower type/token ratio, and to use certain discourse markers more frequently than original texts.</text>
              <doc_id>9</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Incidentally, translated texts are so markedly different from original ones that automatic classification can identify them with very high accuracy (van Halteren, 2008; Baroni and Bernardini, 2006; Ilisei et al., 2010; Koppel and Ordan, 2011).</text>
              <doc_id>10</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Contemporary Statistical Machine Translation</text>
              <doc_id>11</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(SMT) systems use parallel corpora to train translation models that reflect source- and targetlanguage phrase correspondences.</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Typically, SMT systems ignore the direction of translation used to produce those corpora.</text>
              <doc_id>13</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Given the unique properties of translationese, however, it is reasonable to assume that this direction may affect the quality of the translation.</text>
              <doc_id>14</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Recently, Kurokawa et al. (2009) showed that this is indeed the case.</text>
              <doc_id>15</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>They train a system to translate between French and English (and vice versa) using a Frenchtranslated-to-English parallel corpus, and then an English-translated-to-French one.</text>
              <doc_id>16</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>They find that in translating into French the latter parallel corpus yields better results, whereas for translating into English it is better to use the former.</text>
              <doc_id>17</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Usually, of course, the translation direction of a parallel corpus is unknown.</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, Kurokawa et al. (2009) train an SVM-based classifier to predict which side of a bi-text is the origin and which one is the translation, and only use the subset of the corpus that corresponds to the translation direction of the task in training their translation model.</text>
              <doc_id>19</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We use these results as our departure point, but improve them in two major ways.</text>
              <doc_id>20</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>First, we demonstrate that the other subset of the corpus, reflecting translation in the &#8216;wrong&#8217; direction, is also important for the translation task, and must not be ignored; second, we show that explicit information on the direction of translation of the parallel corpus, whether manually-annotated or machine-learned, is not mandatory.</text>
              <doc_id>21</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This is achieved by casting the problem in the framework of domain adaptation: we use domain-adaptation techniques to direct the SMT system toward producing output that better reflects the properties of translationese.</text>
              <doc_id>22</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We show that SMT systems adapted to translationese produce better translations than vanilla systems trained on exactly the same resources.</text>
              <doc_id>23</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We confirm these findings using an automatic evaluation metric, BLEU (Papineni et al., 2002), as well as through a qualitative analysis of the results.</text>
              <doc_id>24</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Our departure point is the results of Kurokawa et al. (2009), which we successfully replicate in Section 3.</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>First (Section 4), we explain why translation quality improves when the parallel corpus is translated in the &#8216;right&#8217; direction.</text>
              <doc_id>26</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We do so by showing that the subset of the corpus that was translated in the direction of the translation task (the &#8216;right&#8217; direction, henceforth source-to-target, or S &#8594; T ) yields phrase tables that are better suited for translation of the original language than the subset translated in the reverse direction (the &#8216;wrong&#8217; direction, henceforth target-to-source, or T &#8594; S).</text>
              <doc_id>27</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We use several statistical measures that indicate the better quality of the phrase tables in the former case.</text>
              <doc_id>28</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Then (Section 5), we explore ways to build a translation model that is adapted to the unique properties of translationese.</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We first show that using the entire parallel corpus, including texts that are translated both in the &#8216;right&#8217; and in the &#8216;wrong&#8217; direction, improves the quality of the results.</text>
              <doc_id>30</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Furthermore, we show that the direction of translation used for producing the parallel corpus can be approximated by defining several entropybased measures that correlate well with translationese, and, consequently, with the quality of the translation.</text>
              <doc_id>31</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Specifically, we use the entire corpus, create a single, unified phrase table and then use the statistical measures mentioned above, and in particular cross-entropy, as a clue for selecting phrase pairs from this table.</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The benefit of this method is that not only does it yield the best results, but it also eliminates the need to directly predict the direction of translation of the parallel corpus.</text>
              <doc_id>33</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The main contribution of this work, therefore, is a methodology that improves the quality of SMT by building translation models that are adapted to the nature of translationese.</text>
              <doc_id>34</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Related Work</title>
        <text>Kurokawa et al. (2009) are the first to address the direction of translation in the context of SMT. Their main finding is that using the S &#8594; T portion of the parallel corpus results in mucqqh better translation quality than when the T &#8594; S portion is used for training the translation model. We indeed replicate these results here (Section 3), and view them as a baseline. Additionally, we show that the T &#8594; S portion is also important for machine translation and thus should not be discarded. Using information-theory measures, and in particular cross-entropy, we gain statistically significant improvements in translation quality beyond the results of Kurokawa et al. (2009). Furthermore, we eliminate the need to (manually or automatically) detect the direction of translation of the parallel corpus.
Lembersky et al. (2011) also investigate the relations between translationese and machine translation. Focusing on the language model (LM), they show that LMs trained on translated texts yield better translation quality than LMs compiled from original texts. They also show that perplexity is a good discriminator between original and translated texts.
Our current work is closely related to research in domain-adaptation. In a typical domain adaptation scenario, a system is trained on a large corpus of &#8220;general&#8221; (out-of-domain) training material, with a small portion of in-domain training texts. In our case, the translation model is trained on a large parallel corpus, of which some (generally unknown) subset is &#8220;in-domain&#8221; (S &#8594; T ), and some other subset is &#8220;out-of-domain&#8221; (T &#8594; S). Most existing adaptation methods focus on selecting in-domain data from a general domain corpus. In particular, perplexity is used to score the sentences in the general-domain corpus according to an in-domain language model. Gao et al. (2002) and Moore and Lewis (2010) apply this method to language modeling, while Foster
et al. (2010) and Axelrod et al. (2011) use it on the translation model. Moore and Lewis (2010) suggest a slightly different approach, using crossentropy difference as a ranking function.
Domain adaptation methods are usually applied at the corpus level, while we focus on an adaptation of the phrase table used for SMT. In this sense, our work follows Foster et al. (2010), who weigh out-of-domain phrase pairs according to their relevance to the target domain. They use multiple features that help distinguish between phrase pairs in the general domain and those in the specific domain. We rely on features that are motivated by the findings of Translation Studies, having established their relevance through a comparative analysis of the phrase tables. In particular, we use measures such as translation model entropy, inspired by Koehn et al. (2009). Additionally, we apply the method suggested by Moore and Lewis (2010) using perplexity ratio instead of cross-entropy difference.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Kurokawa et al. (2009) are the first to address the direction of translation in the context of SMT.</text>
              <doc_id>35</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Their main finding is that using the S &#8594; T portion of the parallel corpus results in mucqqh better translation quality than when the T &#8594; S portion is used for training the translation model.</text>
              <doc_id>36</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We indeed replicate these results here (Section 3), and view them as a baseline.</text>
              <doc_id>37</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Additionally, we show that the T &#8594; S portion is also important for machine translation and thus should not be discarded.</text>
              <doc_id>38</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Using information-theory measures, and in particular cross-entropy, we gain statistically significant improvements in translation quality beyond the results of Kurokawa et al. (2009).</text>
              <doc_id>39</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Furthermore, we eliminate the need to (manually or automatically) detect the direction of translation of the parallel corpus.</text>
              <doc_id>40</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Lembersky et al. (2011) also investigate the relations between translationese and machine translation.</text>
              <doc_id>41</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Focusing on the language model (LM), they show that LMs trained on translated texts yield better translation quality than LMs compiled from original texts.</text>
              <doc_id>42</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>They also show that perplexity is a good discriminator between original and translated texts.</text>
              <doc_id>43</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Our current work is closely related to research in domain-adaptation.</text>
              <doc_id>44</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In a typical domain adaptation scenario, a system is trained on a large corpus of &#8220;general&#8221; (out-of-domain) training material, with a small portion of in-domain training texts.</text>
              <doc_id>45</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In our case, the translation model is trained on a large parallel corpus, of which some (generally unknown) subset is &#8220;in-domain&#8221; (S &#8594; T ), and some other subset is &#8220;out-of-domain&#8221; (T &#8594; S).</text>
              <doc_id>46</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Most existing adaptation methods focus on selecting in-domain data from a general domain corpus.</text>
              <doc_id>47</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In particular, perplexity is used to score the sentences in the general-domain corpus according to an in-domain language model.</text>
              <doc_id>48</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Gao et al. (2002) and Moore and Lewis (2010) apply this method to language modeling, while Foster</text>
              <doc_id>49</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>et al. (2010) and Axelrod et al. (2011) use it on the translation model.</text>
              <doc_id>50</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Moore and Lewis (2010) suggest a slightly different approach, using crossentropy difference as a ranking function.</text>
              <doc_id>51</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Domain adaptation methods are usually applied at the corpus level, while we focus on an adaptation of the phrase table used for SMT.</text>
              <doc_id>52</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this sense, our work follows Foster et al. (2010), who weigh out-of-domain phrase pairs according to their relevance to the target domain.</text>
              <doc_id>53</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>They use multiple features that help distinguish between phrase pairs in the general domain and those in the specific domain.</text>
              <doc_id>54</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We rely on features that are motivated by the findings of Translation Studies, having established their relevance through a comparative analysis of the phrase tables.</text>
              <doc_id>55</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In particular, we use measures such as translation model entropy, inspired by Koehn et al. (2009).</text>
              <doc_id>56</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Additionally, we apply the method suggested by Moore and Lewis (2010) using perplexity ratio instead of cross-entropy difference.</text>
              <doc_id>57</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Experimental Setup</title>
        <text>The tasks we focus on are translation between French and English, in both directions. We use the Hansard corpus, containing transcripts of the Canadian parliament from 1996&#8211;2007, as the source of all parallel data. The Hansard is a bilingual French&#8211;English corpus comprising approximately 80% English-original texts and 20% French-original texts. Crucially, each sentence pair in the corpus is annotated with the direction of translation. Both English and French are lowercased and tokenized using MOSES (Koehn et al., 2007). Sentences longer than 80 words are discarded.
To address the effect of the corpus size, we compile six subsets of different sizes (250K, 500K, 750K, 1M, 1.25M and 1.5M parallel sentences) from each portion (English-original and French-original) of the corpus. Additionally, we use the devtest section of the Hansard corpus to randomly select French-original and English-original sentences that are used for tuning (1,000 sentences each) and evaluation (5,000 sentences each). French-to-English MT systems are tuned and tested on French-original sentences and English-to-French systems on Englishoriginal ones.
To replicate the results of Kurokawa et al. (2009) and set up a baseline, we train twelve French-to-English and twelve English-to-French phrase-based (PB-) SMT systems using the MOSES toolkit (Koehn et al., 2007), each trained on a different subset of the corpus. We use GIZA++ (Och and Ney, 2000) with grow-diagfinal alignment, and extract phrases of length up to 10 words. We prune the resulting phrase tables as in Johnson et al. (2007), using at most 30 translations per source phrase and discarding singleton phrase pairs. We construct English and French 5-gram language models from the English and French subsections of the Europarl-V6 corpus (Koehn, 2005), using interpolated modified Kneser-Ney discounting (Chen, 1998) and no cut-off on all n-grams. Europarl consists of a large number of subsets translated from various languages, and is therefore unlikely to be biased towards a specific source language. The reordering model used in all MT systems is trained on the union of the 1.5M French-original and the 1.5M Englishoriginal subsets, using msd-bidirectional-fe reordering. We use the MERT algorithm (Och, 2003) for tuning and BLEU (Papineni et al., 2002) as our evaluation metric. We test the statistical significance of the differences between the results using the bootstrap resampling method (Koehn, 2004).
A word on notation: We use &#8216;English-original&#8217; (EO) and &#8216;French-original&#8217; (FO) to refer to the subsets of the corpus that are translated from English to French and from French to English, respectively. The translation tasks are English-to- French (E2F) and French-to-English (F2E). We thus use &#8216;S &#8594; T &#8217; when the FO corpus is used for the F2E task or when the EO corpus is used for the E2F task; and &#8216;T &#8594; S&#8217; when the FO corpus is used for the E2F task or when the EO corpus is used for the F2E task.
Table 1 depicts the BLEU scores of the baseline systems. The data are consistent with the findings of Kurokawa et al. (2009): systems trained on S &#8594; T parallel texts outperform systems trained on T &#8594; S texts, even when the latter are much larger. The difference in BLEU score can be as high as 3 points.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The tasks we focus on are translation between French and English, in both directions.</text>
              <doc_id>58</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We use the Hansard corpus, containing transcripts of the Canadian parliament from 1996&#8211;2007, as the source of all parallel data.</text>
              <doc_id>59</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The Hansard is a bilingual French&#8211;English corpus comprising approximately 80% English-original texts and 20% French-original texts.</text>
              <doc_id>60</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Crucially, each sentence pair in the corpus is annotated with the direction of translation.</text>
              <doc_id>61</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Both English and French are lowercased and tokenized using MOSES (Koehn et al., 2007).</text>
              <doc_id>62</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Sentences longer than 80 words are discarded.</text>
              <doc_id>63</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To address the effect of the corpus size, we compile six subsets of different sizes (250K, 500K, 750K, 1M, 1.25M and 1.5M parallel sentences) from each portion (English-original and French-original) of the corpus.</text>
              <doc_id>64</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Additionally, we use the devtest section of the Hansard corpus to randomly select French-original and English-original sentences that are used for tuning (1,000 sentences each) and evaluation (5,000 sentences each).</text>
              <doc_id>65</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>French-to-English MT systems are tuned and tested on French-original sentences and English-to-French systems on Englishoriginal ones.</text>
              <doc_id>66</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To replicate the results of Kurokawa et al. (2009) and set up a baseline, we train twelve French-to-English and twelve English-to-French phrase-based (PB-) SMT systems using the MOSES toolkit (Koehn et al., 2007), each trained on a different subset of the corpus.</text>
              <doc_id>67</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We use GIZA++ (Och and Ney, 2000) with grow-diagfinal alignment, and extract phrases of length up to 10 words.</text>
              <doc_id>68</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We prune the resulting phrase tables as in Johnson et al. (2007), using at most 30 translations per source phrase and discarding singleton phrase pairs.</text>
              <doc_id>69</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We construct English and French 5-gram language models from the English and French subsections of the Europarl-V6 corpus (Koehn, 2005), using interpolated modified Kneser-Ney discounting (Chen, 1998) and no cut-off on all n-grams.</text>
              <doc_id>70</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Europarl consists of a large number of subsets translated from various languages, and is therefore unlikely to be biased towards a specific source language.</text>
              <doc_id>71</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The reordering model used in all MT systems is trained on the union of the 1.5M French-original and the 1.5M Englishoriginal subsets, using msd-bidirectional-fe reordering.</text>
              <doc_id>72</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We use the MERT algorithm (Och, 2003) for tuning and BLEU (Papineni et al., 2002) as our evaluation metric.</text>
              <doc_id>73</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>We test the statistical significance of the differences between the results using the bootstrap resampling method (Koehn, 2004).</text>
              <doc_id>74</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A word on notation: We use &#8216;English-original&#8217; (EO) and &#8216;French-original&#8217; (FO) to refer to the subsets of the corpus that are translated from English to French and from French to English, respectively.</text>
              <doc_id>75</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The translation tasks are English-to- French (E2F) and French-to-English (F2E).</text>
              <doc_id>76</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We thus use &#8216;S &#8594; T &#8217; when the FO corpus is used for the F2E task or when the EO corpus is used for the E2F task; and &#8216;T &#8594; S&#8217; when the FO corpus is used for the E2F task or when the EO corpus is used for the F2E task.</text>
              <doc_id>77</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Table 1 depicts the BLEU scores of the baseline systems.</text>
              <doc_id>78</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The data are consistent with the findings of Kurokawa et al. (2009): systems trained on S &#8594; T parallel texts outperform systems trained on T &#8594; S texts, even when the latter are much larger.</text>
              <doc_id>79</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The difference in BLEU score can be as high as 3 points.</text>
              <doc_id>80</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Analysis of the Phrase Tables</title>
        <text>The baseline results suggest that S &#8594; T and T &#8594; S phrase tables differ substantially, presumably due to the different characteristics of original
Task: French-to-English Corpus subset S &#8594; T T &#8594; S
Task: English-to-French Corpus subset S &#8594; T T &#8594; S
and translated texts. In this section we explain the better translation quality in terms of the better quality of the respective phrase tables, as defined by a number of statistical measures. We first relate these measures to the unique properties of translationese.
Translated texts tend to be simpler than original ones along a number of criteria. Generally, translated texts are not as rich and variable as original ones, and in particular, their type/token ratio is lower. Consequently, we expect S &#8594; T phrase tables (which are based on a parallel corpus whose source is original texts, and whose target is translationese) to have more unique source phrases and a lower number of translations per source phrase. A large number of unique source phrases suggests better coverage of the source text, while a small number of translations per source phrase means a lower phrase table entropy. Entropy-based measures are well-established tools to assess the quality of a phrase table. Phrase table entropy captures the amount of uncertainty involved in choosing candidate translation phrases (Koehn et al., 2009).
Given a source phrase s and a phrase table T with translations t of s whose probabilities are p(t|s), the entropy H of s is:
H(s) = &#8722; &#8721; p(t|s) &#215; log 2 p(t|s) (1)
t&#8712;T
There are two major flavors of the phrase table entropy metric: Lambert et al. (2011) calculate the average entropy over all translation options for each source phrase (henceforth, phrase table entropy or PtEnt), whereas Koehn et al. (2009) search through all possible segmentations of the source sentence to find the optimal covering set of test sentences that minimizes the average entropy of the source phrases in the covering set (henceforth, covering set entropy or CovEnt).
We also propose a metric that assesses the quality of the source side of a phrase table. The metric finds the minimal covering set of a given text in the source language using source phrases from a particular phrase table, and outputs the average length of a phrase in the covering set (henceforth, covering set average length or CovLen). Lembersky et al. (2011) show that perplexity distinguishes well between translated and original texts. Moreover, perplexity reflects the degree of &#8216;relatedness&#8217; of a given phrase to original language or to translationese. Motivated by this observation, we design two cross-entropy-based measures to assess how well each phrase table fits the genre of translationese. Since MT systems are evaluated against human translations, we believe that this factor may have a significant impact on translation performance. The cross-entropy of a text T = w 1 , w 2 , &#183; &#183; &#183; w N according to a language model L is:
H(T, L) = &#8722; 1 N N&#8721;
log 2 L(w i ) (2)
i=1
We build language models of translated texts as follows. For English translationese, we extract 170,000 French-original sentences from the English portion of Europarl, and 3,000 English-translated-from-French sentences from the Hansard corpus (disjoint from the training, development and test sets, of course). We use each corpus to train a trigram language model with interpolated modified Kneser-Ney discounting and no cut-off. All out-of-vocabulary words are mapped to a special token, &#12296;unk&#12297;. Then, we interpolate the Hansard and Europarl language models to minimize the perplexity of the target side of the development set (&#955; = 0.58). For French translationese, we use 270,000 sentences from Europarl and 3,000 sentences from Hansard, &#955; = 0.81. Finally, we compute the cross-entropy of each target phrase in the phrase tables according to these language models.
As with the entropy-based measures, we define two cross-entropy metrics: phrase table crossentropy or PtCrEnt calculates the average crossentropy over weighted cross-entropies of all translation options for each source phrase, and covering set cross-entropy or CovCrEnt finds the optimal covering set of test sentences that minimizes the weighted cross-entropy of the source phrase in the covering set. Given a phrase table T and a language model L, the weighted cross-entropy W for a source phrase s is:
W (s, L) = &#8722; &#8721; t&#8712;T H(t, L) &#215; p(t|s) (3)
where H(t, L) is the cross-entropy of t according to a language model L.
Table 2 depicts various statistical measures computed on the phrase tables corresponding to our 24 SMT systems. 1 The data meet our preliminary expectations: S &#8594; T phrase tables have more unique source phrases, but fewer translation options per source phrase. They have lower entropy and cross-entropy, but higher covering set length. In order to asses the correspondence of each measure to translation quality, we compute the correlation of BLEU scores from Table 1 with each of the measures specified in Table 2; we compute the correlation coefficient R 2 (the square of Pearson&#8217;s product-moment correlation coefficient) by fitting a simple linear regression model. Table 3 lists the results. Only the covering set cross-entropy measure shows stability over the French-to-English and English-to-French translation tasks, with R 2 equals to 0.56 and 0.54, respectively. Other measures are sensitive to the translation task: covering set entropy has the highest correlation with BLEU (R 2 = 0.94) when translating French-to-English, but it drops to 0.46 for the reverse task. The covering set average length measure shows similar behavior: R 2 drops from 0.75 in French-to-English to 0.56 in Englishto-French. Still, the correlation of these measures with BLEU is high.
Consequently, we use the three best measures, namely covering set entropy, cross-entropy and average length, as indicators of better translations, more similar to translationese. Crucially,
1 The phrase tables were pruned, retaining only phrases
that are included in the evaluation set.
these measures are computed directly on the phrase table, and do not require reference translations or meta-information pertaining to the direction of translation of the parallel phrase.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The baseline results suggest that S &#8594; T and T &#8594; S phrase tables differ substantially, presumably due to the different characteristics of original</text>
              <doc_id>81</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Task: French-to-English Corpus subset S &#8594; T T &#8594; S</text>
              <doc_id>82</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Task: English-to-French Corpus subset S &#8594; T T &#8594; S</text>
              <doc_id>83</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and translated texts.</text>
              <doc_id>84</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this section we explain the better translation quality in terms of the better quality of the respective phrase tables, as defined by a number of statistical measures.</text>
              <doc_id>85</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We first relate these measures to the unique properties of translationese.</text>
              <doc_id>86</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Translated texts tend to be simpler than original ones along a number of criteria.</text>
              <doc_id>87</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Generally, translated texts are not as rich and variable as original ones, and in particular, their type/token ratio is lower.</text>
              <doc_id>88</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Consequently, we expect S &#8594; T phrase tables (which are based on a parallel corpus whose source is original texts, and whose target is translationese) to have more unique source phrases and a lower number of translations per source phrase.</text>
              <doc_id>89</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>A large number of unique source phrases suggests better coverage of the source text, while a small number of translations per source phrase means a lower phrase table entropy.</text>
              <doc_id>90</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Entropy-based measures are well-established tools to assess the quality of a phrase table.</text>
              <doc_id>91</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Phrase table entropy captures the amount of uncertainty involved in choosing candidate translation phrases (Koehn et al., 2009).</text>
              <doc_id>92</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Given a source phrase s and a phrase table T with translations t of s whose probabilities are p(t|s), the entropy H of s is:</text>
              <doc_id>93</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>H(s) = &#8722; &#8721; p(t|s) &#215; log 2 p(t|s) (1)</text>
              <doc_id>94</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>t&#8712;T</text>
              <doc_id>95</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>There are two major flavors of the phrase table entropy metric: Lambert et al. (2011) calculate the average entropy over all translation options for each source phrase (henceforth, phrase table entropy or PtEnt), whereas Koehn et al. (2009) search through all possible segmentations of the source sentence to find the optimal covering set of test sentences that minimizes the average entropy of the source phrases in the covering set (henceforth, covering set entropy or CovEnt).</text>
              <doc_id>96</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We also propose a metric that assesses the quality of the source side of a phrase table.</text>
              <doc_id>97</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The metric finds the minimal covering set of a given text in the source language using source phrases from a particular phrase table, and outputs the average length of a phrase in the covering set (henceforth, covering set average length or CovLen).</text>
              <doc_id>98</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Lembersky et al. (2011) show that perplexity distinguishes well between translated and original texts.</text>
              <doc_id>99</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, perplexity reflects the degree of &#8216;relatedness&#8217; of a given phrase to original language or to translationese.</text>
              <doc_id>100</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Motivated by this observation, we design two cross-entropy-based measures to assess how well each phrase table fits the genre of translationese.</text>
              <doc_id>101</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Since MT systems are evaluated against human translations, we believe that this factor may have a significant impact on translation performance.</text>
              <doc_id>102</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The cross-entropy of a text T = w 1 , w 2 , &#183; &#183; &#183; w N according to a language model L is:</text>
              <doc_id>103</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>H(T, L) = &#8722; 1 N N&#8721;</text>
              <doc_id>104</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>log 2 L(w i ) (2)</text>
              <doc_id>105</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1</text>
              <doc_id>106</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We build language models of translated texts as follows.</text>
              <doc_id>107</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For English translationese, we extract 170,000 French-original sentences from the English portion of Europarl, and 3,000 English-translated-from-French sentences from the Hansard corpus (disjoint from the training, development and test sets, of course).</text>
              <doc_id>108</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We use each corpus to train a trigram language model with interpolated modified Kneser-Ney discounting and no cut-off.</text>
              <doc_id>109</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>All out-of-vocabulary words are mapped to a special token, &#12296;unk&#12297;.</text>
              <doc_id>110</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Then, we interpolate the Hansard and Europarl language models to minimize the perplexity of the target side of the development set (&#955; = 0.58).</text>
              <doc_id>111</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>For French translationese, we use 270,000 sentences from Europarl and 3,000 sentences from Hansard, &#955; = 0.81.</text>
              <doc_id>112</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we compute the cross-entropy of each target phrase in the phrase tables according to these language models.</text>
              <doc_id>113</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>As with the entropy-based measures, we define two cross-entropy metrics: phrase table crossentropy or PtCrEnt calculates the average crossentropy over weighted cross-entropies of all translation options for each source phrase, and covering set cross-entropy or CovCrEnt finds the optimal covering set of test sentences that minimizes the weighted cross-entropy of the source phrase in the covering set.</text>
              <doc_id>114</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Given a phrase table T and a language model L, the weighted cross-entropy W for a source phrase s is:</text>
              <doc_id>115</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>W (s, L) = &#8722; &#8721; t&#8712;T H(t, L) &#215; p(t|s) (3)</text>
              <doc_id>116</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where H(t, L) is the cross-entropy of t according to a language model L.</text>
              <doc_id>117</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Table 2 depicts various statistical measures computed on the phrase tables corresponding to our 24 SMT systems.</text>
              <doc_id>118</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1 The data meet our preliminary expectations: S &#8594; T phrase tables have more unique source phrases, but fewer translation options per source phrase.</text>
              <doc_id>119</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>They have lower entropy and cross-entropy, but higher covering set length.</text>
              <doc_id>120</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In order to asses the correspondence of each measure to translation quality, we compute the correlation of BLEU scores from Table 1 with each of the measures specified in Table 2; we compute the correlation coefficient R 2 (the square of Pearson&#8217;s product-moment correlation coefficient) by fitting a simple linear regression model.</text>
              <doc_id>121</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Table 3 lists the results.</text>
              <doc_id>122</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Only the covering set cross-entropy measure shows stability over the French-to-English and English-to-French translation tasks, with R 2 equals to 0.56 and 0.54, respectively.</text>
              <doc_id>123</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Other measures are sensitive to the translation task: covering set entropy has the highest correlation with BLEU (R 2 = 0.94) when translating French-to-English, but it drops to 0.46 for the reverse task.</text>
              <doc_id>124</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The covering set average length measure shows similar behavior: R 2 drops from 0.75 in French-to-English to 0.56 in Englishto-French.</text>
              <doc_id>125</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Still, the correlation of these measures with BLEU is high.</text>
              <doc_id>126</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Consequently, we use the three best measures, namely covering set entropy, cross-entropy and average length, as indicators of better translations, more similar to translationese.</text>
              <doc_id>127</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Crucially,</text>
              <doc_id>128</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 The phrase tables were pruned, retaining only phrases</text>
              <doc_id>129</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>that are included in the evaluation set.</text>
              <doc_id>130</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>these measures are computed directly on the phrase table, and do not require reference translations or meta-information pertaining to the direction of translation of the parallel phrase.</text>
              <doc_id>131</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>5 Translation Model Adaptation</title>
        <text>We have thus established the fact that S &#8594; T phrase tables have an advantage over T &#8594; S ones that stems directly from the different characteristics of original and translated texts. We have also identified three statistical measures that explain most of the variability in translation quality. We now explore ways for taking advantage of the entire parallel corpus, including translations in both directions, in light of the above findings. Our goal is to establish the best method to address the issue of different translation direction components in the parallel corpus.
First, we simply take the union of the two subsets of the parallel corpus. We create three different mixtures of FO and EO: 500K sentences each of FO and EO (&#8216;MIX1&#8217;), 500K sentences of FO and 1M sentences of EO (&#8216;MIX2&#8217;), and 1M sentences of FO and 500K sentences of EO (&#8216;MIX3&#8217;). We use these corpora to train Frenchto-English and English-to-French MT systems, evaluating their quality on the evaluation sets described in Section 3. We use the same Moses configuration as well as the same language and reordering models as in Section 3. Table 4 reports the results, comparing them to the results obtained for the baseline MT systems trained on individual French-original and English-original bi-texts (see Section 3). 2 Note that the mixed corpus includes many more sentences than each of the baseline models; this is a
2 Recall that when translating from French to English,
S &#8594; T means that the bi-text is French-original; when translating from English to French, S &#8594; T means it is Englishoriginal.
Task: French-to-English Set Total Source AvgTran PtEnt CovEnt PtCrEnt CovCrEnt CovLen
realistic scenario, in which one can opt either to use the entire parallel corpus, or only its S &#8594; T subset. Even with a corpus several times as large, however, the &#8216;mixed&#8217; MT systems perform only slightly better than the S &#8594; T ones. On one hand, this means that one can train MT systems on S &#8594; T data only, at the expense of only a minor loss in quality. On the other hand, it is obvious that the T &#8594; S component also contributes to translation quality. We now look at ways to better utilize this portion.
We compute the measures established in the
previous section on phrase tables trained on the MIX corpora, and compare them with the same measures computed for phrase tables trained on the relevant S &#8594; T corpus for both translation tasks. Table 5 displays the figures for the MIX1 corpus: Phrase tables trained on mixed corpora have higher covering set average length, similar covering set entropy, but significantly worse covering set cross-entropy. Consequently, improving covering set cross-entropy has the greatest potential for improving translation quality. We therefore use this feature to &#8216;encourage&#8217; the decoder to
select translation options that are more related to the genre of translated texts.
We do so by adding to each phrase pair in the phrase tables an additional factor, as a measure of its fitness to the genre of translationese. We experiment with two such factors. First, we use the language models described in Section 4 to compute the cross-entropy of each translation option according to this model. We add cross-entropy as an additional score of a translation pair that can be tuned by MERT (we refer to this system as CrEnt). Since cross-entropy is &#8216;the lower the better&#8217; metric, we adjust the range of values used by MERT for this score to be negative. Second, following Moore and Lewis (2010), we define an adapting feature that not only measures how close phrases are to translated language, but also how far they are from original language, and use it as a factor in a phrase table (this system is referred to as PplRatio). We build two additional language models of original texts as follows. For original English, we extract 135,000 English-original sentences from the English portion of Europarl, and 2,700 English-original sentences from the Hansard corpus. We train a trigram language model with interpolated modified Kneser-Ney discounting on each corpus and we interpolate both models to minimize the perplexity of the source side of the development set for the English-to-French translation task (&#955; = 0.49). For original French, we use 110,000 sentences from Europarl and 2,900 sentences from Hansard, &#955; = 0.61. Finally, for each target phrase t in the phrase table we compute the ratio of the perplexity of t according to the original language model L o and the perplexity of t with respect to the translated model L t (see Section 4). In other words, the factor F is computed as follows:
F (t) = H(t, L o) H(t, L t ) (4)
We apply these techniques to the French-to- English and English-to-French phrase tables built from the mixed corpora and use each phrase table to train an SMT system. Table 6 summarizes the performance of these systems. All systems outperform the corresponding Union systems. &#8216;CrEnt&#8217; systems show significant improvements (p &lt; 0.05) on balanced scenarios (&#8216;MIX1&#8217;) and on scenarios biased towards the S &#8594; T component (&#8216;MIX2&#8217; in the French-to-English task, &#8216;MIX3&#8217; in English-to-French). &#8216;PplRatio&#8217; systems exhibit more consistent behavior, showing small, but statistically significant improvement (p &lt; 0.05) in all scenarios.
Note again that all systems in the same column are trained on exactly the same corpus and have exactly the same phrase tables. The only difference is an additional factor in the phrase table that &#8220;encourages&#8221; the decoder to select translation op-
tions that are closer to translated texts than to original ones.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have thus established the fact that S &#8594; T phrase tables have an advantage over T &#8594; S ones that stems directly from the different characteristics of original and translated texts.</text>
              <doc_id>132</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We have also identified three statistical measures that explain most of the variability in translation quality.</text>
              <doc_id>133</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We now explore ways for taking advantage of the entire parallel corpus, including translations in both directions, in light of the above findings.</text>
              <doc_id>134</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Our goal is to establish the best method to address the issue of different translation direction components in the parallel corpus.</text>
              <doc_id>135</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>First, we simply take the union of the two subsets of the parallel corpus.</text>
              <doc_id>136</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We create three different mixtures of FO and EO: 500K sentences each of FO and EO (&#8216;MIX1&#8217;), 500K sentences of FO and 1M sentences of EO (&#8216;MIX2&#8217;), and 1M sentences of FO and 500K sentences of EO (&#8216;MIX3&#8217;).</text>
              <doc_id>137</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We use these corpora to train Frenchto-English and English-to-French MT systems, evaluating their quality on the evaluation sets described in Section 3.</text>
              <doc_id>138</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We use the same Moses configuration as well as the same language and reordering models as in Section 3.</text>
              <doc_id>139</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Table 4 reports the results, comparing them to the results obtained for the baseline MT systems trained on individual French-original and English-original bi-texts (see Section 3).</text>
              <doc_id>140</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2 Note that the mixed corpus includes many more sentences than each of the baseline models; this is a</text>
              <doc_id>141</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2 Recall that when translating from French to English,</text>
              <doc_id>142</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S &#8594; T means that the bi-text is French-original; when translating from English to French, S &#8594; T means it is Englishoriginal.</text>
              <doc_id>143</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Task: French-to-English Set Total Source AvgTran PtEnt CovEnt PtCrEnt CovCrEnt CovLen</text>
              <doc_id>144</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>realistic scenario, in which one can opt either to use the entire parallel corpus, or only its S &#8594; T subset.</text>
              <doc_id>145</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Even with a corpus several times as large, however, the &#8216;mixed&#8217; MT systems perform only slightly better than the S &#8594; T ones.</text>
              <doc_id>146</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>On one hand, this means that one can train MT systems on S &#8594; T data only, at the expense of only a minor loss in quality.</text>
              <doc_id>147</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>On the other hand, it is obvious that the T &#8594; S component also contributes to translation quality.</text>
              <doc_id>148</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We now look at ways to better utilize this portion.</text>
              <doc_id>149</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We compute the measures established in the</text>
              <doc_id>150</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>previous section on phrase tables trained on the MIX corpora, and compare them with the same measures computed for phrase tables trained on the relevant S &#8594; T corpus for both translation tasks.</text>
              <doc_id>151</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Table 5 displays the figures for the MIX1 corpus: Phrase tables trained on mixed corpora have higher covering set average length, similar covering set entropy, but significantly worse covering set cross-entropy.</text>
              <doc_id>152</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Consequently, improving covering set cross-entropy has the greatest potential for improving translation quality.</text>
              <doc_id>153</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We therefore use this feature to &#8216;encourage&#8217; the decoder to</text>
              <doc_id>154</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>select translation options that are more related to the genre of translated texts.</text>
              <doc_id>155</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We do so by adding to each phrase pair in the phrase tables an additional factor, as a measure of its fitness to the genre of translationese.</text>
              <doc_id>156</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We experiment with two such factors.</text>
              <doc_id>157</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>First, we use the language models described in Section 4 to compute the cross-entropy of each translation option according to this model.</text>
              <doc_id>158</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We add cross-entropy as an additional score of a translation pair that can be tuned by MERT (we refer to this system as CrEnt).</text>
              <doc_id>159</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Since cross-entropy is &#8216;the lower the better&#8217; metric, we adjust the range of values used by MERT for this score to be negative.</text>
              <doc_id>160</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Second, following Moore and Lewis (2010), we define an adapting feature that not only measures how close phrases are to translated language, but also how far they are from original language, and use it as a factor in a phrase table (this system is referred to as PplRatio).</text>
              <doc_id>161</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We build two additional language models of original texts as follows.</text>
              <doc_id>162</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>For original English, we extract 135,000 English-original sentences from the English portion of Europarl, and 2,700 English-original sentences from the Hansard corpus.</text>
              <doc_id>163</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>We train a trigram language model with interpolated modified Kneser-Ney discounting on each corpus and we interpolate both models to minimize the perplexity of the source side of the development set for the English-to-French translation task (&#955; = 0.49).</text>
              <doc_id>164</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>For original French, we use 110,000 sentences from Europarl and 2,900 sentences from Hansard, &#955; = 0.61.</text>
              <doc_id>165</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Finally, for each target phrase t in the phrase table we compute the ratio of the perplexity of t according to the original language model L o and the perplexity of t with respect to the translated model L t (see Section 4).</text>
              <doc_id>166</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>In other words, the factor F is computed as follows:</text>
              <doc_id>167</doc_id>
              <sec_id>11</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>F (t) = H(t, L o) H(t, L t ) (4)</text>
              <doc_id>168</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We apply these techniques to the French-to- English and English-to-French phrase tables built from the mixed corpora and use each phrase table to train an SMT system.</text>
              <doc_id>169</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Table 6 summarizes the performance of these systems.</text>
              <doc_id>170</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>All systems outperform the corresponding Union systems.</text>
              <doc_id>171</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>&#8216;CrEnt&#8217; systems show significant improvements (p &lt; 0.05) on balanced scenarios (&#8216;MIX1&#8217;) and on scenarios biased towards the S &#8594; T component (&#8216;MIX2&#8217; in the French-to-English task, &#8216;MIX3&#8217; in English-to-French).</text>
              <doc_id>172</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>&#8216;PplRatio&#8217; systems exhibit more consistent behavior, showing small, but statistically significant improvement (p &lt; 0.05) in all scenarios.</text>
              <doc_id>173</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Note again that all systems in the same column are trained on exactly the same corpus and have exactly the same phrase tables.</text>
              <doc_id>174</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The only difference is an additional factor in the phrase table that &#8220;encourages&#8221; the decoder to select translation op-</text>
              <doc_id>175</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tions that are closer to translated texts than to original ones.</text>
              <doc_id>176</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>6 Analysis</title>
        <text>In order to study the effect of the adaptation qualitatively, rather than quantitatively, we focus on several concrete examples. We compare translations produced by the &#8216;Union&#8217; (henceforth baseline) and by the &#8216;PplRatio&#8217; (henceforth adapted) French-English SMT systems. We manually inspect 200 sentences of length between 15 and 25 from the French-English evaluation set. In many cases, the adapted system produces more fluent and accurate translations. In the following examples, the baseline system generates common translations of French words that are adequate for a wider context, whereas the adapted system chooses less common, but more suitable translations: Source J&#8217;ai eu cette perception et j&#8217;&#233;tais assez
certain que &#231;a allait se faire. Baseline I had that perception and I was enough
certain it was going do. Adapted I had that perception and I was quite
certain it was going do. Source J&#8217;attends donc que vous en demandiez la
permission, monsieur le Pr&#233;sident. Baseline I look so that you seek permission, mr.
chairman. Adapted I await, then, that you seek permission,
mr. chairman.
In quite a few cases, the baseline system leaves out important words from the source sentence, producing ungrammatical, even illegible translations, whereas the adapted system generates good translations. Careful traceback reveals that the baseline system &#8216;splits&#8217; the source sentence into phrases differently (and less optimally) than the adapted system. Apparently, when the decoder is coerced to select translation options that are more adapted to translationese, it tends to select source phrases that are more related to original texts, resulting in more successful coverage of the source sentence: Source Pourtant, lorsqu&#8217; on les avait pr&#233;sent&#233;s,
c&#8217;&#233;tait pour corriger les probl&#232;mes li&#233;s au PCSRA. Baseline Yet when they had presented, it was to
correct the problems the CAIS program. Adapted Yet when they had presented, it was to
correct the problems associated with CAIS. Source Cependant, je pense qu&#8217;il est pr&#233;matur&#233;
de le faire actuellement, &#233;tant donn&#233; que le ministre a lanc&#233; cette tourn&#233;e. Baseline However, I think it is premature to the
right now, since the minister launched this tour. Adapted However, I think it is premature to do
so now, given that the minister has launched this tour.
Finally, there are often cultural differences between languages, specifically the use of a 24-hour clock (common in French) vs. a 12-hour clock (common in English). The adapted system is more consistent in translating the former to the latter:
Source On avait d&#233;cid&#233; de poursuivre la s&#233;ance jusqu&#8217; &#224; 18 heures, mais on n&#8217;aura pas le temps de faire un autre tour de table. Baseline We had decided to continue the meeting
until 18 hours, but we will not have the time to do another round. Adapted We had decided to continue the meeting
until 6 p.m., but we won&#8217;t have the time to do another round. Source Vu qu&#8217;il est 17h 20, je suis d&#8217;accord
pour qu&#8217;on ne discute pas de ma motion imm&#233;diatement. Baseline Seen that it is 17h 20, I agree that we are
not talking about my motion immediately. Adapted Given that it is 5:20, I agree that we are
not talking about my motion immediately.
In (human) translation circles, translating out of one&#8217;s mother tongue is considered unprofessional, even unethical (Beeby, 2009). Many professional associations in Europe urge translators to work exclusively into their mother tongue (Pavlovi&#263;, 2007). The two kinds of automatic systems built in this paper reflect only partly the human situation, but they do so in a crucial way. The S &#8594; T systems learn examples from many human translators who follow the decree according to which translation should be made into one&#8217;s native tongue. The T &#8594; S systems are flipped directions of humans&#8217; input and output. The S &#8594; T direction proved to be more fluent, accurate and even more culturally sensitive. This has to do with fact that the translators &#8216;cover&#8217; the source texts more fully, having a better &#8216;translation model&#8217;.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In order to study the effect of the adaptation qualitatively, rather than quantitatively, we focus on several concrete examples.</text>
              <doc_id>177</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We compare translations produced by the &#8216;Union&#8217; (henceforth baseline) and by the &#8216;PplRatio&#8217; (henceforth adapted) French-English SMT systems.</text>
              <doc_id>178</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We manually inspect 200 sentences of length between 15 and 25 from the French-English evaluation set.</text>
              <doc_id>179</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In many cases, the adapted system produces more fluent and accurate translations.</text>
              <doc_id>180</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In the following examples, the baseline system generates common translations of French words that are adequate for a wider context, whereas the adapted system chooses less common, but more suitable translations: Source J&#8217;ai eu cette perception et j&#8217;&#233;tais assez</text>
              <doc_id>181</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>certain que &#231;a allait se faire.</text>
              <doc_id>182</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Baseline I had that perception and I was enough</text>
              <doc_id>183</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>certain it was going do.</text>
              <doc_id>184</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Adapted I had that perception and I was quite</text>
              <doc_id>185</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>certain it was going do.</text>
              <doc_id>186</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Source J&#8217;attends donc que vous en demandiez la</text>
              <doc_id>187</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>permission, monsieur le Pr&#233;sident.</text>
              <doc_id>188</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Baseline I look so that you seek permission, mr.</text>
              <doc_id>189</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>chairman.</text>
              <doc_id>190</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Adapted I await, then, that you seek permission,</text>
              <doc_id>191</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>mr.</text>
              <doc_id>192</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>chairman.</text>
              <doc_id>193</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In quite a few cases, the baseline system leaves out important words from the source sentence, producing ungrammatical, even illegible translations, whereas the adapted system generates good translations.</text>
              <doc_id>194</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Careful traceback reveals that the baseline system &#8216;splits&#8217; the source sentence into phrases differently (and less optimally) than the adapted system.</text>
              <doc_id>195</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Apparently, when the decoder is coerced to select translation options that are more adapted to translationese, it tends to select source phrases that are more related to original texts, resulting in more successful coverage of the source sentence: Source Pourtant, lorsqu&#8217; on les avait pr&#233;sent&#233;s,</text>
              <doc_id>196</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>c&#8217;&#233;tait pour corriger les probl&#232;mes li&#233;s au PCSRA.</text>
              <doc_id>197</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Baseline Yet when they had presented, it was to</text>
              <doc_id>198</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>correct the problems the CAIS program.</text>
              <doc_id>199</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Adapted Yet when they had presented, it was to</text>
              <doc_id>200</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>correct the problems associated with CAIS.</text>
              <doc_id>201</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Source Cependant, je pense qu&#8217;il est pr&#233;matur&#233;</text>
              <doc_id>202</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>de le faire actuellement, &#233;tant donn&#233; que le ministre a lanc&#233; cette tourn&#233;e.</text>
              <doc_id>203</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Baseline However, I think it is premature to the</text>
              <doc_id>204</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>right now, since the minister launched this tour.</text>
              <doc_id>205</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Adapted However, I think it is premature to do</text>
              <doc_id>206</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>so now, given that the minister has launched this tour.</text>
              <doc_id>207</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Finally, there are often cultural differences between languages, specifically the use of a 24-hour clock (common in French) vs. a 12-hour clock (common in English).</text>
              <doc_id>208</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The adapted system is more consistent in translating the former to the latter:</text>
              <doc_id>209</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Source On avait d&#233;cid&#233; de poursuivre la s&#233;ance jusqu&#8217; &#224; 18 heures, mais on n&#8217;aura pas le temps de faire un autre tour de table.</text>
              <doc_id>210</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Baseline We had decided to continue the meeting</text>
              <doc_id>211</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>until 18 hours, but we will not have the time to do another round.</text>
              <doc_id>212</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Adapted We had decided to continue the meeting</text>
              <doc_id>213</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>until 6 p.m., but we won&#8217;t have the time to do another round.</text>
              <doc_id>214</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Source Vu qu&#8217;il est 17h 20, je suis d&#8217;accord</text>
              <doc_id>215</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>pour qu&#8217;on ne discute pas de ma motion imm&#233;diatement.</text>
              <doc_id>216</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Baseline Seen that it is 17h 20, I agree that we are</text>
              <doc_id>217</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>not talking about my motion immediately.</text>
              <doc_id>218</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Adapted Given that it is 5:20, I agree that we are</text>
              <doc_id>219</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>not talking about my motion immediately.</text>
              <doc_id>220</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In (human) translation circles, translating out of one&#8217;s mother tongue is considered unprofessional, even unethical (Beeby, 2009).</text>
              <doc_id>221</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Many professional associations in Europe urge translators to work exclusively into their mother tongue (Pavlovi&#263;, 2007).</text>
              <doc_id>222</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The two kinds of automatic systems built in this paper reflect only partly the human situation, but they do so in a crucial way.</text>
              <doc_id>223</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The S &#8594; T systems learn examples from many human translators who follow the decree according to which translation should be made into one&#8217;s native tongue.</text>
              <doc_id>224</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The T &#8594; S systems are flipped directions of humans&#8217; input and output.</text>
              <doc_id>225</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The S &#8594; T direction proved to be more fluent, accurate and even more culturally sensitive.</text>
              <doc_id>226</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>This has to do with fact that the translators &#8216;cover&#8217; the source texts more fully, having a better &#8216;translation model&#8217;.</text>
              <doc_id>227</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>7</index>
        <title>7 Conclusion</title>
        <text>Phrase tables trained on parallel corpora that were translated in the same direction as the translation task perform better than ones trained on corpora translated in the opposite direction. Nonetheless, even &#8216;wrong&#8217; phrase tables contribute to the translation quality. We analyze both &#8216;correct&#8217; and &#8216;wrong&#8217; phrase tables, uncovering a great deal of difference between them. We use insights from Translation Studies to explain these differences; we then adapt the translation model to the nature of translationese. We incorporate information-theoretic measures that correlate well with translationese into phrase tables as an additional score that can be tuned by MERT, and show a statistically significant improvement in the translation quality over all baseline systems. We also analyze the results qualitatively, showing that SMT systems adapted to translationese tend to produce more coherent and fluent outputs than the baseline systems. An additional advantage of our approach is that it does not require an annotation of the translation direction of the parallel corpus. It is completely generic and can be applied to any language pair, domain or corpus. This work can be extended in various directions. We plan to further explore the use of two phrase tables, one for each direction-determined subset of the parallel corpus. Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a maximum a posteriori combination (Bacchiani et al., 2006). We also plan to upweight the S &#8594; T subset of the parallel corpus and train a single phrase table on the concatenated corpus. Finally, we intend to extend this work by combining the translation-model adaptation we present here with the language-model adaptation suggested by Lembersky et al. (2011) in a unified system that is more tuned to generating translationese.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Phrase tables trained on parallel corpora that were translated in the same direction as the translation task perform better than ones trained on corpora translated in the opposite direction.</text>
              <doc_id>228</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Nonetheless, even &#8216;wrong&#8217; phrase tables contribute to the translation quality.</text>
              <doc_id>229</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We analyze both &#8216;correct&#8217; and &#8216;wrong&#8217; phrase tables, uncovering a great deal of difference between them.</text>
              <doc_id>230</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We use insights from Translation Studies to explain these differences; we then adapt the translation model to the nature of translationese.</text>
              <doc_id>231</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We incorporate information-theoretic measures that correlate well with translationese into phrase tables as an additional score that can be tuned by MERT, and show a statistically significant improvement in the translation quality over all baseline systems.</text>
              <doc_id>232</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We also analyze the results qualitatively, showing that SMT systems adapted to translationese tend to produce more coherent and fluent outputs than the baseline systems.</text>
              <doc_id>233</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>An additional advantage of our approach is that it does not require an annotation of the translation direction of the parallel corpus.</text>
              <doc_id>234</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>It is completely generic and can be applied to any language pair, domain or corpus.</text>
              <doc_id>235</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>This work can be extended in various directions.</text>
              <doc_id>236</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>We plan to further explore the use of two phrase tables, one for each direction-determined subset of the parallel corpus.</text>
              <doc_id>237</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a maximum a posteriori combination (Bacchiani et al., 2006).</text>
              <doc_id>238</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>We also plan to upweight the S &#8594; T subset of the parallel corpus and train a single phrase table on the concatenated corpus.</text>
              <doc_id>239</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we intend to extend this work by combining the translation-model adaptation we present here with the language-model adaptation suggested by Lembersky et al. (2011) in a unified system that is more tuned to generating translationese.</text>
              <doc_id>240</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>8</index>
        <title>Acknowledgments</title>
        <text>We are grateful to Cyril Goutte, George Foster and Pierre Isabelle for providing us with an annotated version of the Hansard corpus. This research was supported by the Israel Science Foundation (grant No. 137/06) and by a grant from the Israeli Ministry of Science and Technology.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We are grateful to Cyril Goutte, George Foster and Pierre Isabelle for providing us with an annotated version of the Hansard corpus.</text>
              <doc_id>241</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This research was supported by the Israel Science Foundation (grant No.</text>
              <doc_id>242</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>137/06) and by a grant from the Israeli Ministry of Science and Technology.</text>
              <doc_id>243</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: BLEU scores of baseline systems</caption>
        <reference_text>In PAGE 3: ... We thus use ?S ? T ? when the FO corpus is used for the F2E task or when the EO corpus is used for the E2F task; and ?T ? S? when the FO corpus is used for the E2F task or when the EO corpus is used for the F2E task.  Table1  depicts the BLEU scores of the baseline systems. The data are consistent with the findings of Kurokawa et al....  In PAGE 5: ... They have lower en- tropy and cross-entropy, but higher covering set length. In order to asses the correspondence of each measure to translation quality, we compute the correlation of BLEU scores from  Table1  with each of the measures specified in Table 2; we compute the correlation coefficient R2 (the square of Pearson?s product-moment correlation coeffi- cient) by fitting a simple linear regression model. Table 3 lists the results....</reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>Task: French-to-English</cell>
              <cell>Task: French-to-English</cell>
              <cell>Task: French-to-English</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Corpus subset</cell>
              <cell>S ? T</cell>
              <cell>T ? S</cell>
            </row>
            <row>
              <cell>250K</cell>
              <cell>34.35</cell>
              <cell>31.33</cell>
            </row>
            <row>
              <cell>500K</cell>
              <cell>35.21</cell>
              <cell>32.38</cell>
            </row>
            <row>
              <cell>750K</cell>
              <cell>36.12</cell>
              <cell>32.90</cell>
            </row>
            <row>
              <cell>1M</cell>
              <cell>35.73</cell>
              <cell>33.07</cell>
            </row>
            <row>
              <cell>1.25M</cell>
              <cell>36.24</cell>
              <cell>33.23</cell>
            </row>
            <row>
              <cell>1.5M</cell>
              <cell>36.43</cell>
              <cell>33.73</cell>
            </row>
            <row>
              <cell>Task: English-to-French</cell>
              <cell>Task: English-to-French</cell>
              <cell>Task: English-to-French</cell>
            </row>
            <row>
              <cell>Corpus subset</cell>
              <cell>S ? T</cell>
              <cell>T ? S</cell>
            </row>
            <row>
              <cell>250K</cell>
              <cell>27.74</cell>
              <cell>26.58</cell>
            </row>
            <row>
              <cell>500K</cell>
              <cell>29.15</cell>
              <cell>27.19</cell>
            </row>
            <row>
              <cell>750K</cell>
              <cell>29.43</cell>
              <cell>27.63</cell>
            </row>
            <row>
              <cell>1M</cell>
              <cell>29.94</cell>
              <cell>27.88</cell>
            </row>
            <row>
              <cell>1.25M</cell>
              <cell>30.63</cell>
              <cell>27.84</cell>
            </row>
            <row>
              <cell>1.5M</cell>
              <cell>29.89</cell>
              <cell>27.83</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Statistic measures computed on the phrase tables: total size, in tokens (&#8216;Total&#8217;); the number of unique source phrases (&#8216;Source&#8217;); the average number of translations per source phrase (&#8216;AvgTran&#8217;); phrase table entropy (&#8216;PtEnt&#8217;) and covering set entropy (&#8216;CovEnt&#8217;); phrase table cross-entropy (&#8216;PtCrEnt&#8217;) and covering set crossentropy (&#8216;CovCrEnt&#8217;); and the covering set average length (&#8216;CovLen&#8217;)</caption>
        <reference_text>In PAGE 5: ... Given a phrase table T and a language model L, the weighted cross-entropy W for a source phrase s is: W (s, L) = ? summationdisplay t?T H(t, L) ? p(t|s) (3) where H(t, L) is the cross-entropy of t according to a language model L.  Table2  depicts various statistical measures computed on the phrase tables corresponding to our 24 SMT systems.1 The data meet our pre- liminary expectations: S ? T phrase tables have more unique source phrases, but fewer translation options per source phrase....  In PAGE 5: ... They have lower en- tropy and cross-entropy, but higher covering set length. In order to asses the correspondence of each measure to translation quality, we compute the correlation of BLEU scores from Table 1 with each of the measures specified in  Table2 ; we compute the correlation coefficient R2 (the square of Pearson?s product-moment correlation coeffi- cient) by fitting a simple linear regression model. Table 3 lists the results....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>Task: French-to-English</cell>
              <cell>Task: French-to-English</cell>
              <cell>Task: French-to-English</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Set</cell>
              <cell>Total</cell>
              <cell>Source</cell>
              <cell>AvgTran</cell>
              <cell>PtEnt</cell>
              <cell>CovEnt</cell>
              <cell>PtCrEnt</cell>
              <cell>CovCrEnt</cell>
              <cell>CovLen</cell>
            </row>
            <row>
              <cell>250K</cell>
              <cell>231K</cell>
              <cell>69K</cell>
              <cell>3.35</cell>
              <cell>0.86</cell>
              <cell>0.36</cell>
              <cell>3.94</cell>
              <cell>1.64</cell>
              <cell>2.44</cell>
            </row>
            <row>
              <cell>500K</cell>
              <cell>360K</cell>
              <cell>86K</cell>
              <cell>4.21</cell>
              <cell>0.98</cell>
              <cell>0.35</cell>
              <cell>3.52</cell>
              <cell>1.30</cell>
              <cell>2.64</cell>
            </row>
            <row>
              <cell>750K</cell>
              <cell>461K</cell>
              <cell>96K</cell>
              <cell>4.81</cell>
              <cell>1.05</cell>
              <cell>0.35</cell>
              <cell>3.24</cell>
              <cell>1.10</cell>
              <cell>2.77</cell>
            </row>
            <row>
              <cell>1M</cell>
              <cell>544K</cell>
              <cell>103K</cell>
              <cell>5.27</cell>
              <cell>1.10</cell>
              <cell>0.34</cell>
              <cell>3.09</cell>
              <cell>0.99</cell>
              <cell>2.85</cell>
            </row>
            <row>
              <cell>1.25M</cell>
              <cell>619K</cell>
              <cell>109K</cell>
              <cell>5.66</cell>
              <cell>1.14</cell>
              <cell>0.34</cell>
              <cell>2.98</cell>
              <cell>0.91</cell>
              <cell>2.92</cell>
            </row>
            <row>
              <cell>1.5M</cell>
              <cell>684K</cell>
              <cell>114K</cell>
              <cell>6.01</cell>
              <cell>1.18</cell>
              <cell>0.33</cell>
              <cell>2.90</cell>
              <cell>0.85</cell>
              <cell>2.97</cell>
            </row>
            <row>
              <cell>250K</cell>
              <cell>199K</cell>
              <cell>55K</cell>
              <cell>3.65</cell>
              <cell>0.92</cell>
              <cell>0.45</cell>
              <cell>4.00</cell>
              <cell>1.87</cell>
              <cell>2.25</cell>
            </row>
            <row>
              <cell>500K</cell>
              <cell>317K</cell>
              <cell>69K</cell>
              <cell>4.56</cell>
              <cell>1.05</cell>
              <cell>0.43</cell>
              <cell>3.57</cell>
              <cell>1.52</cell>
              <cell>2.42</cell>
            </row>
            <row>
              <cell>750K</cell>
              <cell>405K</cell>
              <cell>78K</cell>
              <cell>5.19</cell>
              <cell>1.12</cell>
              <cell>0.43</cell>
              <cell>3.39</cell>
              <cell>1.35</cell>
              <cell>2.53</cell>
            </row>
            <row>
              <cell>1M</cell>
              <cell>479K</cell>
              <cell>85K</cell>
              <cell>5.66</cell>
              <cell>1.16</cell>
              <cell>0.42</cell>
              <cell>3.21</cell>
              <cell>1.21</cell>
              <cell>2.61</cell>
            </row>
            <row>
              <cell>1.25M</cell>
              <cell>545K</cell>
              <cell>90K</cell>
              <cell>6.07</cell>
              <cell>1.20</cell>
              <cell>0.41</cell>
              <cell>3.11</cell>
              <cell>1.12</cell>
              <cell>2.67</cell>
            </row>
            <row>
              <cell>1.5M</cell>
              <cell>602K</cell>
              <cell>94K</cell>
              <cell>6.43</cell>
              <cell>1.24</cell>
              <cell>0.41</cell>
              <cell>3.04</cell>
              <cell>1.07</cell>
              <cell>2.71</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>Task: English-to-French</cell>
              <cell>Task: English-to-French</cell>
              <cell>Task: English-to-French</cell>
            </row>
            <row>
              <cell>Set</cell>
              <cell>Total</cell>
              <cell>Source</cell>
              <cell>AvgTran</cell>
              <cell>PtEnt</cell>
              <cell>CovEnt</cell>
              <cell>PtCrEnt</cell>
              <cell>CovCrEnt</cell>
              <cell>CovLen</cell>
            </row>
            <row>
              <cell>250K</cell>
              <cell>224K</cell>
              <cell>49K</cell>
              <cell>4.52</cell>
              <cell>1.07</cell>
              <cell>0.63</cell>
              <cell>3.48</cell>
              <cell>1.88</cell>
              <cell>2.08</cell>
            </row>
            <row>
              <cell>500K</cell>
              <cell>346K</cell>
              <cell>61K</cell>
              <cell>5.64</cell>
              <cell>1.21</cell>
              <cell>0.59</cell>
              <cell>3.08</cell>
              <cell>1.49</cell>
              <cell>2.25</cell>
            </row>
            <row>
              <cell>750K</cell>
              <cell>437K</cell>
              <cell>68K</cell>
              <cell>6.39</cell>
              <cell>1.29</cell>
              <cell>0.57</cell>
              <cell>2.91</cell>
              <cell>1.33</cell>
              <cell>2.33</cell>
            </row>
            <row>
              <cell>1M</cell>
              <cell>513K</cell>
              <cell>74K</cell>
              <cell>6.95</cell>
              <cell>1.34</cell>
              <cell>0.55</cell>
              <cell>2.75</cell>
              <cell>1.18</cell>
              <cell>2.41</cell>
            </row>
            <row>
              <cell>1.25M</cell>
              <cell>579K</cell>
              <cell>78K</cell>
              <cell>7.42</cell>
              <cell>1.38</cell>
              <cell>0.54</cell>
              <cell>2.63</cell>
              <cell>1.09</cell>
              <cell>2.46</cell>
            </row>
            <row>
              <cell>1.5M</cell>
              <cell>635K</cell>
              <cell>81K</cell>
              <cell>7.83</cell>
              <cell>1.41</cell>
              <cell>0.53</cell>
              <cell>2.58</cell>
              <cell>1.03</cell>
              <cell>2.50</cell>
            </row>
            <row>
              <cell>250K</cell>
              <cell>220K</cell>
              <cell>46K</cell>
              <cell>4.75</cell>
              <cell>1.12</cell>
              <cell>0.63</cell>
              <cell>3.62</cell>
              <cell>2.09</cell>
              <cell>2.02</cell>
            </row>
            <row>
              <cell>500K</cell>
              <cell>334K</cell>
              <cell>57K</cell>
              <cell>5.82</cell>
              <cell>1.24</cell>
              <cell>0.60</cell>
              <cell>3.24</cell>
              <cell>1.70</cell>
              <cell>2.16</cell>
            </row>
            <row>
              <cell>750K</cell>
              <cell>421K</cell>
              <cell>64K</cell>
              <cell>6.54</cell>
              <cell>1.31</cell>
              <cell>0.58</cell>
              <cell>2.97</cell>
              <cell>1.48</cell>
              <cell>2.25</cell>
            </row>
            <row>
              <cell>1M</cell>
              <cell>489K</cell>
              <cell>69K</cell>
              <cell>7.10</cell>
              <cell>1.36</cell>
              <cell>0.57</cell>
              <cell>2.84</cell>
              <cell>1.35</cell>
              <cell>2.32</cell>
            </row>
            <row>
              <cell>1.25M</cell>
              <cell>550K</cell>
              <cell>73K</cell>
              <cell>7.56</cell>
              <cell>1.40</cell>
              <cell>0.55</cell>
              <cell>2.74</cell>
              <cell>1.25</cell>
              <cell>2.37</cell>
            </row>
            <row>
              <cell>1.5M</cell>
              <cell>603K</cell>
              <cell>76K</cell>
              <cell>7.92</cell>
              <cell>1.43</cell>
              <cell>0.55</cell>
              <cell>2.66</cell>
              <cell>1.17</cell>
              <cell>2.41</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Correlation of BLEU scores with phrase table statistical measures</caption>
        <reference_text>In PAGE 5: ... In order to asses the correspondence of each measure to translation quality, we compute the correlation of BLEU scores from Table 1 with each of the measures specified in Table 2; we compute the correlation coefficient R2 (the square of Pearson?s product-moment correlation coeffi- cient) by fitting a simple linear regression model.  Table3  lists the results. Only the covering set cross-entropy measure shows stability over the French-to-English and English-to-French transla- tion tasks, with R2 equals to 0....</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>Measure</cell>
              <cell>R2 (FR?EN)#@#@R 2 (FR&#8211;EN)</cell>
              <cell>R2 (EN-FR)#@#@R 2 (EN-FR)</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>AvgTran</cell>
              <cell>0.06</cell>
              <cell>0.22</cell>
            </row>
            <row>
              <cell>PtEnt</cell>
              <cell>0.03</cell>
              <cell>0.19</cell>
            </row>
            <row>
              <cell>CovEnt</cell>
              <cell>0.94</cell>
              <cell>0.46</cell>
            </row>
            <row>
              <cell>PtCrEnt</cell>
              <cell>0.33</cell>
              <cell>0.44</cell>
            </row>
            <row>
              <cell>CovCrEnt</cell>
              <cell>0.56</cell>
              <cell>0.54</cell>
            </row>
            <row>
              <cell>CovLen</cell>
              <cell>0.75</cell>
              <cell>0.56</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TET</source>
        <caption>Table 4: Evaluation of the MIX systems</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Task: French-to-English</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System</cell>
              <cell>MIX1</cell>
              <cell>MIX2</cell>
              <cell>MIX3</cell>
            </row>
            <row>
              <cell>Union</cell>
              <cell>35.27</cell>
              <cell>35.36</cell>
              <cell>35.94</cell>
            </row>
            <row>
              <cell>S &#8594; T</cell>
              <cell>35.21</cell>
              <cell>35.21</cell>
              <cell>35.73</cell>
            </row>
            <row>
              <cell>T &#8594; S</cell>
              <cell>32.38</cell>
              <cell>33.07</cell>
              <cell>32.38</cell>
            </row>
            <row>
              <cell>Task: English-to-French</cell>
            </row>
            <row>
              <cell>System</cell>
              <cell>MIX1</cell>
              <cell>MIX2</cell>
              <cell>MIX3</cell>
            </row>
            <row>
              <cell>Union</cell>
              <cell>29.27</cell>
              <cell>30.01</cell>
              <cell>29.44</cell>
            </row>
            <row>
              <cell>S &#8594; T</cell>
              <cell>29.15</cell>
              <cell>29.94</cell>
              <cell>29.15</cell>
            </row>
            <row>
              <cell>T &#8594; S</cell>
              <cell>27.19</cell>
              <cell>27.19</cell>
              <cell>27.88</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TableSeer</source>
        <caption>Table 5: Statistical measures computed for mixed vs. source-to-target phrase tables</caption>
        <reference_text>In PAGE 6: ... We compute the measures established in the previous section on phrase tables trained on the MIX corpora, and compare them with the same measures computed for phrase tables trained on the relevant S ? T corpus for both translation tasks.  Table5  displays the figures for the MIX1 corpus: Phrase tables trained on mixed corpora have higher covering set average length, similar covering set entropy, but significantly worse cov- ering set cross-entropy. Consequently, improving covering set cross-entropy has the greatest poten- tial for improving translation quality....</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>French-to-English</cell>
              <cell>French-to-English</cell>
              <cell>French-to-English</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Measure</cell>
              <cell>MIX1</cell>
              <cell>S ? T</cell>
            </row>
            <row>
              <cell>CovLen</cell>
              <cell>2.78</cell>
              <cell>2.64</cell>
            </row>
            <row>
              <cell>CovEnt</cell>
              <cell>0.37</cell>
              <cell>0.35</cell>
            </row>
            <row>
              <cell>CovCrEnt</cell>
              <cell>1.58</cell>
              <cell>1.10</cell>
            </row>
            <row>
              <cell>English-to-French</cell>
              <cell>English-to-French</cell>
              <cell>English-to-French</cell>
            </row>
            <row>
              <cell>Measure</cell>
              <cell>MIX1</cell>
              <cell>S ? T</cell>
            </row>
            <row>
              <cell>CovLen</cell>
              <cell>2.40</cell>
              <cell>2.25</cell>
            </row>
            <row>
              <cell>CovEnt</cell>
              <cell>0.55</cell>
              <cell>0.58</cell>
            </row>
            <row>
              <cell>CovCrEnt</cell>
              <cell>2.09</cell>
              <cell>1.48</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>6</id>
        <source>TET</source>
        <caption>Table 6: Evaluation of MT Systems</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Task: French-to-English</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System</cell>
              <cell>MIX1</cell>
              <cell>MIX2</cell>
              <cell>MIX3</cell>
            </row>
            <row>
              <cell>Union</cell>
              <cell>35.27</cell>
              <cell>35.36</cell>
              <cell>35.94</cell>
            </row>
            <row>
              <cell>CrEnt</cell>
              <cell>35.54</cell>
              <cell>35.45</cell>
              <cell>36.75</cell>
            </row>
            <row>
              <cell>PplRatio</cell>
              <cell>35.59</cell>
              <cell>35.78</cell>
              <cell>36.22</cell>
            </row>
            <row>
              <cell>Task: English-to-French</cell>
            </row>
            <row>
              <cell>System</cell>
              <cell>MIX1</cell>
              <cell>MIX2</cell>
              <cell>MIX3</cell>
            </row>
            <row>
              <cell>Union</cell>
              <cell>29.27</cell>
              <cell>30.01</cell>
              <cell>29.44</cell>
            </row>
            <row>
              <cell>CrEnt</cell>
              <cell>29.47</cell>
              <cell>30.44</cell>
              <cell>29.45</cell>
            </row>
            <row>
              <cell>PplRatio</cell>
              <cell>29.65</cell>
              <cell>30.34</cell>
              <cell>29.62</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors/>
        <title>None</title>
        <publication>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>355--362</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>1</id>
        <authors/>
        <title>None</title>
        <publication>None</publication>
        <pages>11--1033</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Michiel Bacchiani</author>
          <author>Michael Riley</author>
          <author>Brian Roark</author>
          <author>Richard Sproat</author>
        </authors>
        <title>MAP adaptation of stochastic grammars. Computer Speech and Language,</title>
        <publication>None</publication>
        <pages>68</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Mona Baker</author>
        </authors>
        <title>Corpus linguistics and translation studies: Implications and applications.</title>
        <publication>Text and technology: in honour of John Sinclair,</publication>
        <pages>233--252</pages>
        <date>1993</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Target</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1995</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Mona Baker</author>
        </authors>
        <title>Corpus-based translation studies: The challenges that lie ahead.</title>
        <publication>LSP and Translation. Studies in language engineering in honour of</publication>
        <pages>175--186</pages>
        <date>1996</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Marco Baroni</author>
          <author>Silvia Bernardini</author>
        </authors>
        <title>A new approach to the study of Translationese: Machinelearning the difference between original and translated text.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>7</id>
        <authors/>
        <title>None</title>
        <publication>Routledge Encyclopedia of Translation Studies,</publication>
        <pages>84--88</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Stanley F Chen</author>
        </authors>
        <title>An empirical study of smoothing techniques for language modeling.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>George Foster</author>
          <author>Roland Kuhn</author>
        </authors>
        <title>Mixture-model adaptation for SMT.</title>
        <publication>In Proceedings of the Second Workshop on Statistical Machine Translation,</publication>
        <pages>128--135</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>George Foster</author>
          <author>Cyril Goutte</author>
          <author>Roland Kuhn</author>
        </authors>
        <title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
        <publication>In 263 of the 2010 Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>451--459</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Jianfeng Gao</author>
          <author>Joshua Goodman</author>
          <author>Mingjing Li</author>
          <author>KaiFu Lee</author>
        </authors>
        <title>Toward a unified approach to statistical language modeling for Chinese.</title>
        <publication>None</publication>
        <pages>33</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>acm org10 1145595576 595578</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Martin Gellerstam</author>
        </authors>
        <title>Translationese in Swedish novels translated from English.</title>
        <publication>Translation Studies in Scandinavia,</publication>
        <pages>88--95</pages>
        <date>1986</date>
      </reference>
      <reference>
        <id>14</id>
        <authors/>
        <title>None</title>
        <publication>Proceedings of CICLing-2010: 11th International Conference on Computational Linguistics and Intelligent Text Processing,</publication>
        <pages>503--511</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Howard Johnson</author>
          <author>Joel Martin</author>
          <author>George Foster</author>
          <author>Roland Kuhn</author>
        </authors>
        <title>Improving translation quality by discarding most of the phrasetable.</title>
        <publication>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</publication>
        <pages>967--975</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>16</id>
        <authors/>
        <title>None</title>
        <publication>None</publication>
        <pages>07--07</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Philipp Koehn</author>
        </authors>
        <title>Statistical significance tests for machine translation evaluation.</title>
        <publication>In Proceedings of EMNLP 2004,</publication>
        <pages>388--395</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Philipp Koehn</author>
        </authors>
        <title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
        <publication>In Conference Proceedings: the tenth Machine Translation Summit,</publication>
        <pages>79--86</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>URL http mt-archive info MTS-2005-Koehn pdf Philipp Koehn</author>
          <author>Hieu Hoang</author>
          <author>Alexandra Birch</author>
          <author>Chris Callison-Burch</author>
          <author>Marcello Federico</author>
          <author>Nicola Bertoldi</author>
          <author>Brooke Cowan</author>
          <author>Wade Shen</author>
        </authors>
        <title>Moses: Open source toolkit for statistical machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Birch</author>
          <author>Ralf Steinberger</author>
        </authors>
        <title>Association for Computational Linguistics. URL http://www.aclweb.org/ anthology/P07-2045. Philipp Koehn,</title>
        <publication>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</publication>
        <pages>177--180</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>21</id>
        <authors/>
        <title>462 machine translation systems for Europe.</title>
        <publication>In Machine Translation Summit XII,</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Moshe Koppel</author>
          <author>Noam Ordan</author>
        </authors>
        <title>Translationese and its dialects.</title>
        <publication>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</publication>
        <pages>1318--1326</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>URL http www aclweb org anthologyP11-1132 David Kurokawa</author>
          <author>Cyril Goutte</author>
          <author>Pierre Isabelle</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>24</id>
        <authors/>
        <title>Automatic detection of translated text and its impact on machine translation.</title>
        <publication>In Proceedings of MTSummit XII,</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Patrik Lambert</author>
          <author>Holger Schwenk</author>
          <author>Christophe Servan</author>
          <author>Sadaf Abdul-Rauf</author>
        </authors>
        <title>Investigations on translation model adaptation using monolingual data.</title>
        <publication>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</publication>
        <pages>284--293</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>26</id>
        <authors/>
        <title>Language models for machine translation: Original vs. translated texts.</title>
        <publication>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>363--374</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>27</id>
        <authors/>
        <title>None</title>
        <publication>None</publication>
        <pages>11--1034</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>28</id>
        <authors>
          <author>Robert C Moore</author>
          <author>William Lewis</author>
        </authors>
        <title>Intelligent selection of language model training data.</title>
        <publication>In Proceedings of the ACL 2010 Conference, Short Papers,</publication>
        <pages>220--224</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>29</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>In ACL &#8217;03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</publication>
        <pages>160--167</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>30</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Improved statistical alignment models.</title>
        <publication>In ACL &#8217;00: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</publication>
        <pages>440--447</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>31</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>Wei-Jing Zhu</author>
        </authors>
        <title>BLEU: a method for automatic evaluation of machine translation.</title>
        <publication>In ACL &#8217;02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</publication>
        <pages>311--318</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>32</id>
        <authors>
          <author>Nata&#353;a Pavlovi&#263;</author>
        </authors>
        <title>Directionality in translation and interpreting practice. Report on a questionnaire survey in Croatia.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>33</id>
        <authors/>
        <title>Gideon Toury. Descriptive Translation Studies and beyond.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1980</date>
      </reference>
      <reference>
        <id>34</id>
        <authors>
          <author>John Benjamins</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1995</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>(2011)</string>
        <sentence_id>19946</sentence_id>
        <char_offset>17</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>0</reference_id>
        <string>(2011)</string>
        <sentence_id>19955</sentence_id>
        <char_offset>33</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>0</reference_id>
        <string>(2011)</string>
        <sentence_id>20001</sentence_id>
        <char_offset>79</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>0</reference_id>
        <string>(2011)</string>
        <sentence_id>20004</sentence_id>
        <char_offset>17</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>0</reference_id>
        <string>(2011)</string>
        <sentence_id>20145</sentence_id>
        <char_offset>166</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>3</reference_id>
        <string>Baker, 1993</string>
        <sentence_id>19914</sentence_id>
        <char_offset>90</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>6</reference_id>
        <string>Baroni and Bernardini, 2006</string>
        <sentence_id>19915</sentence_id>
        <char_offset>169</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>7</reference_id>
        <string>(2009)</string>
        <sentence_id>19920</sentence_id>
        <char_offset>26</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>7</reference_id>
        <string>(2009)</string>
        <sentence_id>19924</sentence_id>
        <char_offset>27</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>7</reference_id>
        <string>(2009)</string>
        <sentence_id>19930</sentence_id>
        <char_offset>54</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>7</reference_id>
        <string>(2009)</string>
        <sentence_id>19940</sentence_id>
        <char_offset>16</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>7</reference_id>
        <string>(2009)</string>
        <sentence_id>19944</sentence_id>
        <char_offset>176</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>7</reference_id>
        <string>(2009)</string>
        <sentence_id>19961</sentence_id>
        <char_offset>91</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>7</reference_id>
        <string>(2009)</string>
        <sentence_id>19972</sentence_id>
        <char_offset>44</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>7</reference_id>
        <string>(2009)</string>
        <sentence_id>19984</sentence_id>
        <char_offset>61</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>7</reference_id>
        <string>(2009)</string>
        <sentence_id>20001</sentence_id>
        <char_offset>234</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>8</reference_id>
        <string>Chen, 1998</string>
        <sentence_id>19975</sentence_id>
        <char_offset>188</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>9</reference_id>
        <string>Foster and Kuhn (2007)</string>
        <sentence_id>20143</sentence_id>
        <char_offset>63</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>10</reference_id>
        <string>Foster et al. (2010)</string>
        <sentence_id>19958</sentence_id>
        <char_offset>32</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>11</reference_id>
        <string>Gao et al. (2002)</string>
        <sentence_id>19954</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>13</reference_id>
        <string>Gellerstam, 1986</string>
        <sentence_id>19912</sentence_id>
        <char_offset>151</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>14</reference_id>
        <string>(2010)</string>
        <sentence_id>19954</sentence_id>
        <char_offset>38</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>14</reference_id>
        <string>(2010)</string>
        <sentence_id>19955</sentence_id>
        <char_offset>7</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>14</reference_id>
        <string>(2010)</string>
        <sentence_id>19956</sentence_id>
        <char_offset>16</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>14</reference_id>
        <string>(2010)</string>
        <sentence_id>19958</sentence_id>
        <char_offset>46</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>14</reference_id>
        <string>(2010)</string>
        <sentence_id>19962</sentence_id>
        <char_offset>63</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>14</reference_id>
        <string>(2010)</string>
        <sentence_id>20066</sentence_id>
        <char_offset>34</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>15</reference_id>
        <string>Johnson et al. (2007)</string>
        <sentence_id>19974</sentence_id>
        <char_offset>43</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>17</reference_id>
        <string>Koehn, 2004</string>
        <sentence_id>19979</sentence_id>
        <char_offset>115</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>18</reference_id>
        <string>Koehn, 2005</string>
        <sentence_id>19975</sentence_id>
        <char_offset>122</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>21</reference_id>
        <string>(2009)</string>
        <sentence_id>19920</sentence_id>
        <char_offset>26</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>21</reference_id>
        <string>(2009)</string>
        <sentence_id>19924</sentence_id>
        <char_offset>27</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>21</reference_id>
        <string>(2009)</string>
        <sentence_id>19930</sentence_id>
        <char_offset>54</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>21</reference_id>
        <string>(2009)</string>
        <sentence_id>19940</sentence_id>
        <char_offset>16</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>21</reference_id>
        <string>(2009)</string>
        <sentence_id>19944</sentence_id>
        <char_offset>176</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>21</reference_id>
        <string>(2009)</string>
        <sentence_id>19961</sentence_id>
        <char_offset>91</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>21</reference_id>
        <string>(2009)</string>
        <sentence_id>19972</sentence_id>
        <char_offset>44</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>21</reference_id>
        <string>(2009)</string>
        <sentence_id>19984</sentence_id>
        <char_offset>61</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>21</reference_id>
        <string>(2009)</string>
        <sentence_id>20001</sentence_id>
        <char_offset>234</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>22</reference_id>
        <string>Koppel and Ordan, 2011</string>
        <sentence_id>19915</sentence_id>
        <char_offset>219</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>24</reference_id>
        <string>(2009)</string>
        <sentence_id>19920</sentence_id>
        <char_offset>26</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>24</reference_id>
        <string>(2009)</string>
        <sentence_id>19924</sentence_id>
        <char_offset>27</char_offset>
      </citation>
      <citation>
        <id>42</id>
        <reference_id>24</reference_id>
        <string>(2009)</string>
        <sentence_id>19930</sentence_id>
        <char_offset>54</char_offset>
      </citation>
      <citation>
        <id>43</id>
        <reference_id>24</reference_id>
        <string>(2009)</string>
        <sentence_id>19940</sentence_id>
        <char_offset>16</char_offset>
      </citation>
      <citation>
        <id>44</id>
        <reference_id>24</reference_id>
        <string>(2009)</string>
        <sentence_id>19944</sentence_id>
        <char_offset>176</char_offset>
      </citation>
      <citation>
        <id>45</id>
        <reference_id>24</reference_id>
        <string>(2009)</string>
        <sentence_id>19961</sentence_id>
        <char_offset>91</char_offset>
      </citation>
      <citation>
        <id>46</id>
        <reference_id>24</reference_id>
        <string>(2009)</string>
        <sentence_id>19972</sentence_id>
        <char_offset>44</char_offset>
      </citation>
      <citation>
        <id>47</id>
        <reference_id>24</reference_id>
        <string>(2009)</string>
        <sentence_id>19984</sentence_id>
        <char_offset>61</char_offset>
      </citation>
      <citation>
        <id>48</id>
        <reference_id>24</reference_id>
        <string>(2009)</string>
        <sentence_id>20001</sentence_id>
        <char_offset>234</char_offset>
      </citation>
      <citation>
        <id>49</id>
        <reference_id>25</reference_id>
        <string>Lambert et al. (2011)</string>
        <sentence_id>20001</sentence_id>
        <char_offset>64</char_offset>
      </citation>
      <citation>
        <id>50</id>
        <reference_id>26</reference_id>
        <string>(2011)</string>
        <sentence_id>19946</sentence_id>
        <char_offset>17</char_offset>
      </citation>
      <citation>
        <id>51</id>
        <reference_id>26</reference_id>
        <string>(2011)</string>
        <sentence_id>19955</sentence_id>
        <char_offset>33</char_offset>
      </citation>
      <citation>
        <id>52</id>
        <reference_id>26</reference_id>
        <string>(2011)</string>
        <sentence_id>20001</sentence_id>
        <char_offset>79</char_offset>
      </citation>
      <citation>
        <id>53</id>
        <reference_id>26</reference_id>
        <string>(2011)</string>
        <sentence_id>20004</sentence_id>
        <char_offset>17</char_offset>
      </citation>
      <citation>
        <id>54</id>
        <reference_id>26</reference_id>
        <string>(2011)</string>
        <sentence_id>20145</sentence_id>
        <char_offset>166</char_offset>
      </citation>
      <citation>
        <id>55</id>
        <reference_id>28</reference_id>
        <string>Moore and Lewis (2010)</string>
        <sentence_id>19954</sentence_id>
        <char_offset>22</char_offset>
      </citation>
      <citation>
        <id>56</id>
        <reference_id>28</reference_id>
        <string>Moore and Lewis (2010)</string>
        <sentence_id>19956</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>57</id>
        <reference_id>28</reference_id>
        <string>Moore and Lewis (2010)</string>
        <sentence_id>19962</sentence_id>
        <char_offset>47</char_offset>
      </citation>
      <citation>
        <id>58</id>
        <reference_id>28</reference_id>
        <string>Moore and Lewis (2010)</string>
        <sentence_id>20066</sentence_id>
        <char_offset>18</char_offset>
      </citation>
      <citation>
        <id>59</id>
        <reference_id>29</reference_id>
        <string>Och, 2003</string>
        <sentence_id>19978</sentence_id>
        <char_offset>27</char_offset>
      </citation>
      <citation>
        <id>60</id>
        <reference_id>30</reference_id>
        <string>Och and Ney, 2000</string>
        <sentence_id>19973</sentence_id>
        <char_offset>15</char_offset>
      </citation>
      <citation>
        <id>61</id>
        <reference_id>31</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>19929</sentence_id>
        <char_offset>70</char_offset>
      </citation>
      <citation>
        <id>62</id>
        <reference_id>31</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>19978</sentence_id>
        <char_offset>59</char_offset>
      </citation>
      <citation>
        <id>63</id>
        <reference_id>32</reference_id>
        <string>Pavlovi&#263;, 2007</string>
        <sentence_id>20127</sentence_id>
        <char_offset>104</char_offset>
      </citation>
    </citations>
  </content>
</document>
