<document>
  <filename>W13-0801</filename>
  <authors/>
  <title>A Semantic Evaluation of Machine Translation Lexical Choice</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>While automatic metrics of translation quality are invaluable for machine translation research, deeper understanding of translation errors require more focused evaluations designed to target specific aspects of translation quality. We show that Word Sense Disambiguation (WSD) can be used to evaluate the quality of machine translation lexical choice, by applying a standard phrase-based SMT system on the SemEval2010 Cross-Lingual WSD task. This case study reveals that the SMT system does not perform as well as a WSD system trained on the exact same parallel data, and that local context models based on source phrases and target n-grams are much weaker representations of context than the simple templates used by the WSD system.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>While automatic metrics of translation quality are invaluable for machine translation research, deeper understanding of translation errors require more focused evaluations designed to target specific aspects of translation quality.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We show that Word Sense Disambiguation (WSD) can be used to evaluate the quality of machine translation lexical choice, by applying a standard phrase-based SMT system on the SemEval2010 Cross-Lingual WSD task.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This case study reveals that the SMT system does not perform as well as a WSD system trained on the exact same parallel data, and that local context models based on source phrases and target n-grams are much weaker representations of context than the simple templates used by the WSD system.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Much research has focused on automatically evaluating the quality of Machine Translation (MT) by comparing automatic translations to human translations on samples of a few thousand sentences. Many metrics (Papineni et al., 2002; Banerjee and Lavie, 2005; Gim&#233;nez and M&#225;rquez, 2007; Lo and Wu, 2011, for instance) have been proposed to estimate the adequacy and fluency of machine translation and evaluated based on their correlatation with human judgements of translation quality (Callison-Burch et al., 2010). While these metrics have proven invaluable in driving progress in MT research, finergrained evaluations of translation quality are necessary to provide a more focused analysis of translation errors. When developing complex MT systems, comparing BLEU or TER scores is not sufficient to understand what improved or what went wrong. Error analysis can of course be done manually (Vilar et al., 2006), but it is often too slow and expensive to be performed as often as needed during system development. Several metrics have been recently proposed to evaluate specific aspects of translation quality such as word order (Birch et al., 2010; Chen et al., 2012). While word order is indirectly taken into account by BLEU, TER or METEOR scores, dedicated metrics provide a direct evaluation that lets us understand whether a given system&#8217;s reordering performance improved during system development. Word order metrics provide a complementary tool for targeting evaluation and analysis to a specific aspect of machine translation quality.
There has not been as much work on evaluating the lexical choice performance of MT: does a MT system preserve the meaning of words in translation? This is of course measured indirectly by commonly used global metrics, but a more focused evaluation can help us gain a better understanding of the behavior of MT systems.
In this paper, we show that MT lexical choice can be framed and evaluated as a standard Word Sense Disambiguation (WSD) task. We leverage existing WSD shared tasks in order to evaluate whether word meaning is preserved in translation. Let us emphasize that, just like reordering metrics, our WSD evaluation is meant to complement global metrics of translation quality. In previous work, intrinsic evaluations of lexical choice have been performed using either semi-automatically constructed data sets
based on MT reference translations (Gim&#233;nez and M&#224;rquez, 2008; Carpuat and Wu, 2008), or manually constructed word sense disambiguation test beds that do not exactly match MT lexical choice (Carpuat and Wu, 2005). We will show how existing Cross-Lingual Word Sense Disambiguation tasks (Lefever and Hoste, 2010; Lefever and Hoste, 2013) can be directly seen as machine translation lexical choice (Section 2): their sense inventory is based on translations in a second language rather than arbitrary sense representations used in other WSD tasks (Carpuat and Wu, 2005); unlike in MT evaluation settings, human annotators can more easily provide a complete representation of all correct meanings of a word. Second, we show how using this task for evaluating the lexical choice performance of several phrase-based SMT systems (PB- SMT) gives some insights into their strengths and weaknesses (Section 5).
2 Selecting a Word Sense Disambiguation Task to Evaluate MT Lexical Choice
Word Sense Disambiguation consists in determining the correct sense of a word in context. This challenging problem has been studied from a rich variety of persectives in Natural Language Processing (see Agirre and Edmonds (2006) for an overview.) The Senseval and SemEval series of evaluations (Edmonds and Cotton, 2001; Mihalcea and Edmonds, 2004; Agirre et al., 2007) have driven the standardization of methodology for evaluating WSD systems. Many shared tasks were organized over the years, providing evaluation settings that vary along several dimensions, including:
&#8226; target vocabulary: in all word tasks, systems are expected to tag all content words in running text (Palmer et al., 2001), while in lexical sample tasks, the evaluation considers a smaller predefined set of target words (Mihalcea et al., 2004; Lefever and Hoste, 2010).
&#8226; language: English is by far the most studied language, but the disambiguation of words in other languages such as Chinese (Jin et al., 2007) has been considered.
&#8226; sense inventory: many tasks use WordNet senses (Fellbaum, 1998), but other sense representations have been used, including alternate semantic databases such as HowNet (Dong, 1998), or lexicalizations in one or more languages (Chklovski et al., 2004).
The Cross-Lingual Word Sense Disambiguation (CLWSD) task introduced at a recent edition of SemEval (Lefever and Hoste, 2010) is an English lexical sample task that uses translations in other European languages as a sense inventory. As a result, it is particularly well suited to evaluating machine translation lexical choice.
2.1 Translations as Word Sense Representations
The CLWSD task is essentially the same task as MT lexical choice: given English target words in context, systems are asked to predict translations in other European languages. The gold standard consists of translations proposed by several bilingual humans, as can be seen in Table 1. MT system predictions can be compared to human annotations directly, without introducing additional sources of ambiguity and mismatches due to representation differences. This contrasts with our previous work on evaluating MT on a WSD task (Carpuat and Wu, 2005), which used text annotated with abstract sense categories from the HowNet knowledge base (Dong, 1998). In HowNet, each word is defined using a concept, constructed as a combination of basic units of meaning, called sememes. Words that share the same concept can be viewed as synonyms. Evaluating MT using a gold standard of HowNet categories requires to map translations from the MT output to the HowNet representation. Some categories are annotated with English translations, but additional effort is required in order to cover all translation candidates produced by the MT system.
2.2 Controlled Learning Conditions
Another advantage of the CLWSD task is that it provides controlled learning conditions (even though it is an unsupervised task with no annotated training data.) The gold labels for CLWSD are learned from parallel corpora. As a result MT lexical choice models can be estimated on the exact same data. Translations for English words in the lexical sample are extracted from a semi-automatic word alignment of
sentences from the Europarl parallel corpus (Koehn, 2005). These translations are then manually clustered into senses. When constructing the gold annotation, human annotators are given occurrences of target words in context. For each occurrence, they select a sense cluster and provide all translations from this cluster that are correct in this specific context. Since three annotators contribute, each test occurrence is therefore tagged with a set of translations in another language, along with a frequency which represents the number of annotators who selected it. A more detailed description of the annotation process can be found in (Lefever and Hoste, 2010). Again, this contrasts with our previous work on evaluating MT on a HowNet-based Chinese WSD task, where Chinese sentences were manually annotated with HowNet senses which were completely unrelated to the parallel corpus used for training the SMT system. Using CLWSD as an evaluation of MT lexical choice solves this issue and provides controlled learning conditions.
2.3 CLWSD evaluates the semantic adequacy of MT lexical choice
A key challenge in MT evaluation lies in deciding whether the meaning of the translation is correct when it does not exactly match the reference translation. METEOR uses WordNet synonyms and learned paraphrases tables (Denkowski and Lavie, 2010). MEANT uses vector-space based lexical similarity scores (Lo et al., 2012). While these methods lead to higher correlations with human judgements on average, they are not ideal for a fine-grained evaluation of lexical choice: similarity scores are defined independently of context and might give credit to incorrect translations (Carpuat et al., 2012). In contrast, CLWSD solves this difficult problem by providing all correct translation candidates in context according to several human annotators. These multiple translations provide a more complete representation of the correct meaning of each occurrence of a word in context. The CLWSD annotation procedure is designed to easily let human annotators provide many correct translation alternatives for a word. Producing many correct annotations for a complete sentence is a much more expensive undertaking: crowdsourcing can help alleviate the cost of obtaining a small number of reference translation (Zbib et al., 2012), but acquiring a complete representation of all possible translations of a source sentence is a much more complex task (Dreyer and Marcu, 2012). Machine translation evaluations typically use between one and four reference translations, which provide a very incomplete representation of the correct semantics of the input sentence in the output language. CLWSD provides a more complete representation through the multiple gold translations available.
2.4 Limitations
The main drawback of using CLWSD to evaluate lexical choice is that CLWSD is a lexical sample task, which only evaluates disambiguation of 20 English nouns. This arbitrary sample of words does not let us target words or phrases that might be specifically interesting for MT.
In addition, the data available through the shared task does not let us evaluate complete translations of the CLWSD test sentences, since full references translations are not available. Instead of using
a WSD dataset for MT purposes, we could take the converse approach andautomatically construct a WSD test set based on MT evaluation corpora (Vickrey et al., 2005; Gim&#233;nez and M&#224;rquez, 2008; Carpuat and Wu, 2008; Carpuat et al., 2012). However, this approach suffers from noisy automatic alignments between source and reference, as well as from a limited representation of the correct meaning of words in context due to the limited number of reference translations. Other SemEval tasks such as the Cross-Lingual Lexical Substitution Task (Mihalcea et al., 2010) would also provide an appropriate test bed. We focused on the CLWSD task, since it uses senses drawn from the Europarl parallel corpus, and therefore offers more constrained settings for comparison between systems. The lexical substitution task targets verbs and adjectives in addition to nouns, and would therefore be an interesting test case to consider in future work.
2.5 Official and MT-centric Evaluation Metrics
In order to make comparison with other systems possible, we follow the standard evaluation framework defined for the task and score the output of all our systems using four different metrics, computed using the scoring tool made available by the organizers. The difference between system predictions and gold standard annotations are quantified using precision and recall scores 1 , defined as follows. Given a set T of test items and a set H of annotators, H i is the set of translation proposed by all annotators h for instance i &#8712; T . Each translation type res in H i has an associated frequency freq res , which represents the number of human annotators which selected res as one of their top 3 translations. Given a set of system answers A of items i &#8712; T such that the system provides at least one answer, a i : i &#8712; A is is the set of answers from the system for instance i. For each i, the scorer computes the intersection of the system answers a i and the gold standard H i .
Systems propose as many answers as deemed nec-
1 In this paper, we focus on evaluating translation systems
whose task is to produce a single complete translation for a given sentence. As a result, we only focus on the 1-best MT output and do not report the relaxed out-of-five evaluation setting also considered in the official SemEval task.
essary, but the scores are divided by the number of guesses in order not to favor systems that output many answers per instance.
&#8721;
res&#8712;a i
freq res |a i ||H i |
Precision = 1 &#8721;
|A| a i :i&#8712;A &#8721;
Recall = 1 &#8721;
res&#8712;a freq res i
|T | a i :i&#8712;T |a i ||H i |
We also report Mode Precision and Mode Recall scores: instead of comparing system answers to the full set of gold standard translations H i for an instance i &#8712; T , the Mode Precision and Recall scores only use a single gold translation, which is the translation chosen most frequently by the human annotators. In addition, we compute the 1-gram precision component of the BLEU score (Papineni et al., 2002), denoted as BLEU1 in the result tables 2 . In contrast with the official CLWSD evaluation scores described above, BLEU1 gives equal weight to all translation candidates, which can be seen as multiple references.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Much research has focused on automatically evaluating the quality of Machine Translation (MT) by comparing automatic translations to human translations on samples of a few thousand sentences.</text>
              <doc_id>3</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Many metrics (Papineni et al., 2002; Banerjee and Lavie, 2005; Gim&#233;nez and M&#225;rquez, 2007; Lo and Wu, 2011, for instance) have been proposed to estimate the adequacy and fluency of machine translation and evaluated based on their correlatation with human judgements of translation quality (Callison-Burch et al., 2010).</text>
              <doc_id>4</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>While these metrics have proven invaluable in driving progress in MT research, finergrained evaluations of translation quality are necessary to provide a more focused analysis of translation errors.</text>
              <doc_id>5</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>When developing complex MT systems, comparing BLEU or TER scores is not sufficient to understand what improved or what went wrong.</text>
              <doc_id>6</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Error analysis can of course be done manually (Vilar et al., 2006), but it is often too slow and expensive to be performed as often as needed during system development.</text>
              <doc_id>7</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Several metrics have been recently proposed to evaluate specific aspects of translation quality such as word order (Birch et al., 2010; Chen et al., 2012).</text>
              <doc_id>8</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>While word order is indirectly taken into account by BLEU, TER or METEOR scores, dedicated metrics provide a direct evaluation that lets us understand whether a given system&#8217;s reordering performance improved during system development.</text>
              <doc_id>9</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Word order metrics provide a complementary tool for targeting evaluation and analysis to a specific aspect of machine translation quality.</text>
              <doc_id>10</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>There has not been as much work on evaluating the lexical choice performance of MT: does a MT system preserve the meaning of words in translation?</text>
              <doc_id>11</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This is of course measured indirectly by commonly used global metrics, but a more focused evaluation can help us gain a better understanding of the behavior of MT systems.</text>
              <doc_id>12</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this paper, we show that MT lexical choice can be framed and evaluated as a standard Word Sense Disambiguation (WSD) task.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We leverage existing WSD shared tasks in order to evaluate whether word meaning is preserved in translation.</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Let us emphasize that, just like reordering metrics, our WSD evaluation is meant to complement global metrics of translation quality.</text>
              <doc_id>15</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In previous work, intrinsic evaluations of lexical choice have been performed using either semi-automatically constructed data sets</text>
              <doc_id>16</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>based on MT reference translations (Gim&#233;nez and M&#224;rquez, 2008; Carpuat and Wu, 2008), or manually constructed word sense disambiguation test beds that do not exactly match MT lexical choice (Carpuat and Wu, 2005).</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We will show how existing Cross-Lingual Word Sense Disambiguation tasks (Lefever and Hoste, 2010; Lefever and Hoste, 2013) can be directly seen as machine translation lexical choice (Section 2): their sense inventory is based on translations in a second language rather than arbitrary sense representations used in other WSD tasks (Carpuat and Wu, 2005); unlike in MT evaluation settings, human annotators can more easily provide a complete representation of all correct meanings of a word.</text>
              <doc_id>18</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Second, we show how using this task for evaluating the lexical choice performance of several phrase-based SMT systems (PB- SMT) gives some insights into their strengths and weaknesses (Section 5).</text>
              <doc_id>19</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2 Selecting a Word Sense Disambiguation Task to Evaluate MT Lexical Choice</text>
              <doc_id>20</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Word Sense Disambiguation consists in determining the correct sense of a word in context.</text>
              <doc_id>21</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This challenging problem has been studied from a rich variety of persectives in Natural Language Processing (see Agirre and Edmonds (2006) for an overview.</text>
              <doc_id>22</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>) The Senseval and SemEval series of evaluations (Edmonds and Cotton, 2001; Mihalcea and Edmonds, 2004; Agirre et al., 2007) have driven the standardization of methodology for evaluating WSD systems.</text>
              <doc_id>23</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Many shared tasks were organized over the years, providing evaluation settings that vary along several dimensions, including:</text>
              <doc_id>24</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; target vocabulary: in all word tasks, systems are expected to tag all content words in running text (Palmer et al., 2001), while in lexical sample tasks, the evaluation considers a smaller predefined set of target words (Mihalcea et al., 2004; Lefever and Hoste, 2010).</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; language: English is by far the most studied language, but the disambiguation of words in other languages such as Chinese (Jin et al., 2007) has been considered.</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; sense inventory: many tasks use WordNet senses (Fellbaum, 1998), but other sense representations have been used, including alternate semantic databases such as HowNet (Dong, 1998), or lexicalizations in one or more languages (Chklovski et al., 2004).</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The Cross-Lingual Word Sense Disambiguation (CLWSD) task introduced at a recent edition of SemEval (Lefever and Hoste, 2010) is an English lexical sample task that uses translations in other European languages as a sense inventory.</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As a result, it is particularly well suited to evaluating machine translation lexical choice.</text>
              <doc_id>29</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.1 Translations as Word Sense Representations</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The CLWSD task is essentially the same task as MT lexical choice: given English target words in context, systems are asked to predict translations in other European languages.</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The gold standard consists of translations proposed by several bilingual humans, as can be seen in Table 1.</text>
              <doc_id>32</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>MT system predictions can be compared to human annotations directly, without introducing additional sources of ambiguity and mismatches due to representation differences.</text>
              <doc_id>33</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This contrasts with our previous work on evaluating MT on a WSD task (Carpuat and Wu, 2005), which used text annotated with abstract sense categories from the HowNet knowledge base (Dong, 1998).</text>
              <doc_id>34</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In HowNet, each word is defined using a concept, constructed as a combination of basic units of meaning, called sememes.</text>
              <doc_id>35</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Words that share the same concept can be viewed as synonyms.</text>
              <doc_id>36</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Evaluating MT using a gold standard of HowNet categories requires to map translations from the MT output to the HowNet representation.</text>
              <doc_id>37</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Some categories are annotated with English translations, but additional effort is required in order to cover all translation candidates produced by the MT system.</text>
              <doc_id>38</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.2 Controlled Learning Conditions</text>
              <doc_id>39</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Another advantage of the CLWSD task is that it provides controlled learning conditions (even though it is an unsupervised task with no annotated training data.</text>
              <doc_id>40</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>) The gold labels for CLWSD are learned from parallel corpora.</text>
              <doc_id>41</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>As a result MT lexical choice models can be estimated on the exact same data.</text>
              <doc_id>42</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Translations for English words in the lexical sample are extracted from a semi-automatic word alignment of</text>
              <doc_id>43</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>sentences from the Europarl parallel corpus (Koehn, 2005).</text>
              <doc_id>44</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>These translations are then manually clustered into senses.</text>
              <doc_id>45</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>When constructing the gold annotation, human annotators are given occurrences of target words in context.</text>
              <doc_id>46</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>For each occurrence, they select a sense cluster and provide all translations from this cluster that are correct in this specific context.</text>
              <doc_id>47</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Since three annotators contribute, each test occurrence is therefore tagged with a set of translations in another language, along with a frequency which represents the number of annotators who selected it.</text>
              <doc_id>48</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>A more detailed description of the annotation process can be found in (Lefever and Hoste, 2010).</text>
              <doc_id>49</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Again, this contrasts with our previous work on evaluating MT on a HowNet-based Chinese WSD task, where Chinese sentences were manually annotated with HowNet senses which were completely unrelated to the parallel corpus used for training the SMT system.</text>
              <doc_id>50</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Using CLWSD as an evaluation of MT lexical choice solves this issue and provides controlled learning conditions.</text>
              <doc_id>51</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.3 CLWSD evaluates the semantic adequacy of MT lexical choice</text>
              <doc_id>52</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A key challenge in MT evaluation lies in deciding whether the meaning of the translation is correct when it does not exactly match the reference translation.</text>
              <doc_id>53</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>METEOR uses WordNet synonyms and learned paraphrases tables (Denkowski and Lavie, 2010).</text>
              <doc_id>54</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>MEANT uses vector-space based lexical similarity scores (Lo et al., 2012).</text>
              <doc_id>55</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>While these methods lead to higher correlations with human judgements on average, they are not ideal for a fine-grained evaluation of lexical choice: similarity scores are defined independently of context and might give credit to incorrect translations (Carpuat et al., 2012).</text>
              <doc_id>56</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In contrast, CLWSD solves this difficult problem by providing all correct translation candidates in context according to several human annotators.</text>
              <doc_id>57</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>These multiple translations provide a more complete representation of the correct meaning of each occurrence of a word in context.</text>
              <doc_id>58</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The CLWSD annotation procedure is designed to easily let human annotators provide many correct translation alternatives for a word.</text>
              <doc_id>59</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Producing many correct annotations for a complete sentence is a much more expensive undertaking: crowdsourcing can help alleviate the cost of obtaining a small number of reference translation (Zbib et al., 2012), but acquiring a complete representation of all possible translations of a source sentence is a much more complex task (Dreyer and Marcu, 2012).</text>
              <doc_id>60</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Machine translation evaluations typically use between one and four reference translations, which provide a very incomplete representation of the correct semantics of the input sentence in the output language.</text>
              <doc_id>61</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>CLWSD provides a more complete representation through the multiple gold translations available.</text>
              <doc_id>62</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.4 Limitations</text>
              <doc_id>63</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The main drawback of using CLWSD to evaluate lexical choice is that CLWSD is a lexical sample task, which only evaluates disambiguation of 20 English nouns.</text>
              <doc_id>64</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This arbitrary sample of words does not let us target words or phrases that might be specifically interesting for MT.</text>
              <doc_id>65</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In addition, the data available through the shared task does not let us evaluate complete translations of the CLWSD test sentences, since full references translations are not available.</text>
              <doc_id>66</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Instead of using</text>
              <doc_id>67</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>a WSD dataset for MT purposes, we could take the converse approach andautomatically construct a WSD test set based on MT evaluation corpora (Vickrey et al., 2005; Gim&#233;nez and M&#224;rquez, 2008; Carpuat and Wu, 2008; Carpuat et al., 2012).</text>
              <doc_id>68</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, this approach suffers from noisy automatic alignments between source and reference, as well as from a limited representation of the correct meaning of words in context due to the limited number of reference translations.</text>
              <doc_id>69</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Other SemEval tasks such as the Cross-Lingual Lexical Substitution Task (Mihalcea et al., 2010) would also provide an appropriate test bed.</text>
              <doc_id>70</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We focused on the CLWSD task, since it uses senses drawn from the Europarl parallel corpus, and therefore offers more constrained settings for comparison between systems.</text>
              <doc_id>71</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The lexical substitution task targets verbs and adjectives in addition to nouns, and would therefore be an interesting test case to consider in future work.</text>
              <doc_id>72</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.5 Official and MT-centric Evaluation Metrics</text>
              <doc_id>73</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In order to make comparison with other systems possible, we follow the standard evaluation framework defined for the task and score the output of all our systems using four different metrics, computed using the scoring tool made available by the organizers.</text>
              <doc_id>74</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The difference between system predictions and gold standard annotations are quantified using precision and recall scores 1 , defined as follows.</text>
              <doc_id>75</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Given a set T of test items and a set H of annotators, H i is the set of translation proposed by all annotators h for instance i &#8712; T .</text>
              <doc_id>76</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Each translation type res in H i has an associated frequency freq res , which represents the number of human annotators which selected res as one of their top 3 translations.</text>
              <doc_id>77</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Given a set of system answers A of items i &#8712; T such that the system provides at least one answer, a i : i &#8712; A is is the set of answers from the system for instance i.</text>
              <doc_id>78</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>For each i, the scorer computes the intersection of the system answers a i and the gold standard H i .</text>
              <doc_id>79</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Systems propose as many answers as deemed nec-</text>
              <doc_id>80</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 In this paper, we focus on evaluating translation systems</text>
              <doc_id>81</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>whose task is to produce a single complete translation for a given sentence.</text>
              <doc_id>82</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As a result, we only focus on the 1-best MT output and do not report the relaxed out-of-five evaluation setting also considered in the official SemEval task.</text>
              <doc_id>83</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>essary, but the scores are divided by the number of guesses in order not to favor systems that output many answers per instance.</text>
              <doc_id>84</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8721;</text>
              <doc_id>85</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>res&#8712;a i</text>
              <doc_id>86</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>freq res |a i ||H i |</text>
              <doc_id>87</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Precision = 1 &#8721;</text>
              <doc_id>88</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>|A| a i :i&#8712;A &#8721;</text>
              <doc_id>89</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Recall = 1 &#8721;</text>
              <doc_id>90</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>res&#8712;a freq res i</text>
              <doc_id>91</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>|T | a i :i&#8712;T |a i ||H i |</text>
              <doc_id>92</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We also report Mode Precision and Mode Recall scores: instead of comparing system answers to the full set of gold standard translations H i for an instance i &#8712; T , the Mode Precision and Recall scores only use a single gold translation, which is the translation chosen most frequently by the human annotators.</text>
              <doc_id>93</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In addition, we compute the 1-gram precision component of the BLEU score (Papineni et al., 2002), denoted as BLEU1 in the result tables 2 .</text>
              <doc_id>94</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In contrast with the official CLWSD evaluation scores described above, BLEU1 gives equal weight to all translation candidates, which can be seen as multiple references.</text>
              <doc_id>95</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>3 PBSMT system</title>
        <text>We use a typical phrase-based SMT system trained for English-to-Spanish translation. Its application to the CLWSD task affects the selection of training data and its preprocessing, but the SMT model design and learning strategies are exactly the same as for conventional translation tasks.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We use a typical phrase-based SMT system trained for English-to-Spanish translation.</text>
              <doc_id>96</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Its application to the CLWSD task affects the selection of training data and its preprocessing, but the SMT model design and learning strategies are exactly the same as for conventional translation tasks.</text>
              <doc_id>97</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Model</title>
            <text>We use the NRC&#8217;s PORTAGE phrase-based SMT system, which implements a standard phrasal beamsearch decoder with cube pruning. Translation hypotheses are scored according to the following features:
&#8226; 4 phrase-table scores: phrasal translation probabilites with Kneser-Ney smoothing and Zens- Ney lexical smoothing in both translation directions (Chen et al., 2011)
&#8226; 6 hierarchical lexicalized reordering scores, which represent the orientation of the current phrase with respect to the previous block that could have been translated as a single phrase (Galley and Manning, 2008)
2 even though it does not include the length penalty used in
the BLEU score.
&#8226; a word penalty, which scores the length of the output sentence
&#8226; a word-displacement distortion penalty
&#8226; a Kneser-Ney smoothed 5-gram Spanish language model
Weights for these features are learned using a batch version of the MIRA algorithm(Cherry and Foster, 2012). Phrase pairs are extracted from IBM4 alignments obtained with GIZA++(Och and Ney, 2003). We learn phrase translation candidates for phrases of length 1 to 7. Converting the PBSMT output for CLWSD requires a final straightforward mapping step. We use the phrasal alignment between SMT input and output to isolate the translation candidates for the CLWSD target word. When it maps to a multiword phrase in the target language, we use the word within the phrase that has the highest translation IBM1 translation probability given the CLWSD target word of interest. Note that there is no need to perform any manual mapping between SMT output and sense inventories as in (Carpuat and Wu, 2005).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We use the NRC&#8217;s PORTAGE phrase-based SMT system, which implements a standard phrasal beamsearch decoder with cube pruning.</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Translation hypotheses are scored according to the following features:</text>
                  <doc_id>99</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; 4 phrase-table scores: phrasal translation probabilites with Kneser-Ney smoothing and Zens- Ney lexical smoothing in both translation directions (Chen et al., 2011)</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; 6 hierarchical lexicalized reordering scores, which represent the orientation of the current phrase with respect to the previous block that could have been translated as a single phrase (Galley and Manning, 2008)</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 even though it does not include the length penalty used in</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the BLEU score.</text>
                  <doc_id>103</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; a word penalty, which scores the length of the output sentence</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; a word-displacement distortion penalty</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; a Kneser-Ney smoothed 5-gram Spanish language model</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Weights for these features are learned using a batch version of the MIRA algorithm(Cherry and Foster, 2012).</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Phrase pairs are extracted from IBM4 alignments obtained with GIZA++(Och and Ney, 2003).</text>
                  <doc_id>108</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We learn phrase translation candidates for phrases of length 1 to 7.</text>
                  <doc_id>109</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Converting the PBSMT output for CLWSD requires a final straightforward mapping step.</text>
                  <doc_id>110</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We use the phrasal alignment between SMT input and output to isolate the translation candidates for the CLWSD target word.</text>
                  <doc_id>111</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>When it maps to a multiword phrase in the target language, we use the word within the phrase that has the highest translation IBM1 translation probability given the CLWSD target word of interest.</text>
                  <doc_id>112</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Note that there is no need to perform any manual mapping between SMT output and sense inventories as in (Carpuat and Wu, 2005).</text>
                  <doc_id>113</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Data</title>
            <text>The core training corpus is the exact same set of sentences from Europarl that were used to learn the sense inventory, in order to ensure that PBSMT knows the same translations as the human annotators who built the gold standard. There are about 900k sentence pairs, since only 1-to1 alignments that exist in all the languages considered in CLWSD were used (Lefever and Hoste, 2010).
We exploit additional corpora from the WMT2012 translation task, using the full Europarl corpus to train language models, and for one experiment the news-commentary parallel corpus (see Section 9.)
These parallel corpora are used to learn the translation, reordering and language models. The loglinear feature weights are learned on a development set of 3000 sentences sampled from the WMT2012 development test sets. They are selected based on their distance to the CLWSD trial and test sentences (Moore and Lewis, 2010).
We tokenize and lemmatize all English and Spanish text using the FreeLing tools (Padr&#243; and Stanilovsky, 2012). We use lemma representations to perform translation, since the CLWSD targets and translations are lemmatized.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The core training corpus is the exact same set of sentences from Europarl that were used to learn the sense inventory, in order to ensure that PBSMT knows the same translations as the human annotators who built the gold standard.</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>There are about 900k sentence pairs, since only 1-to1 alignments that exist in all the languages considered in CLWSD were used (Lefever and Hoste, 2010).</text>
                  <doc_id>115</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We exploit additional corpora from the WMT2012 translation task, using the full Europarl corpus to train language models, and for one experiment the news-commentary parallel corpus (see Section 9.</text>
                  <doc_id>116</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>)</text>
                  <doc_id>117</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>These parallel corpora are used to learn the translation, reordering and language models.</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The loglinear feature weights are learned on a development set of 3000 sentences sampled from the WMT2012 development test sets.</text>
                  <doc_id>119</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>They are selected based on their distance to the CLWSD trial and test sentences (Moore and Lewis, 2010).</text>
                  <doc_id>120</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We tokenize and lemmatize all English and Spanish text using the FreeLing tools (Padr&#243; and Stanilovsky, 2012).</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We use lemma representations to perform translation, since the CLWSD targets and translations are lemmatized.</text>
                  <doc_id>122</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>4 WSD system</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>123</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Model</title>
            <text>We also train a dedicated WSD system for this task in order to perform a controlled comparison with the SMT system. Many WSD systems have been evaluated on the SemEval test bed used here, however, they differ in terms of resources used, training data and preprocessing pipelines. In order to control for these parameters, we build a WSD system trained on the exact same training corpus, preprocessing and word alignment as the SMT system described above.
We cast WSD as a generic ranking problem with linear models. Given a word in context x, translation candidates t are ranked according to the following model: f(x, t) = &#8721; i &#955; i&#966; i (x, t), where &#966; i (x, t) represent binary features that fire when specific clues are observed in a context x.
Context clues are based on standard feature templates in many supervised WSD approaches (Florian et al., 2002; van Gompel, 2010; Lefever et al., 2011):
&#8226; words in a window of 2 words around the disambiguation target.
&#8226; part-of-speech tags in a window of 2 words around the disambiguation target
&#8226; bag-of-word context: all nouns, verbs and adjectives in the context x
At training time, each example (x, t) is assigned a cost based on the translation observed in parallel corpora: f(x, t) = 0 if t = t aligned , f(x, t) = 1 otherwise . Feature weights &#955; i can be learned in many ways. We optimize logistic loss using stochastic gradient descent 3 .</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We also train a dedicated WSD system for this task in order to perform a controlled comparison with the SMT system.</text>
                  <doc_id>124</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Many WSD systems have been evaluated on the SemEval test bed used here, however, they differ in terms of resources used, training data and preprocessing pipelines.</text>
                  <doc_id>125</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In order to control for these parameters, we build a WSD system trained on the exact same training corpus, preprocessing and word alignment as the SMT system described above.</text>
                  <doc_id>126</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We cast WSD as a generic ranking problem with linear models.</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Given a word in context x, translation candidates t are ranked according to the following model: f(x, t) = &#8721; i &#955; i&#966; i (x, t), where &#966; i (x, t) represent binary features that fire when specific clues are observed in a context x.</text>
                  <doc_id>128</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Context clues are based on standard feature templates in many supervised WSD approaches (Florian et al., 2002; van Gompel, 2010; Lefever et al., 2011):</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; words in a window of 2 words around the disambiguation target.</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; part-of-speech tags in a window of 2 words around the disambiguation target</text>
                  <doc_id>131</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; bag-of-word context: all nouns, verbs and adjectives in the context x</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>At training time, each example (x, t) is assigned a cost based on the translation observed in parallel corpora: f(x, t) = 0 if t = t aligned , f(x, t) = 1 otherwise .</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Feature weights &#955; i can be learned in many ways.</text>
                  <doc_id>134</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We optimize logistic loss using stochastic gradient descent 3 .</text>
                  <doc_id>135</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Data</title>
            <text>The training instances for the supervised WSD system are built automatically by (1) extracting all occurrences of English target words in context, and (2) annotating them with their aligned Spanish lemma.
3 we use the optimizer from http://hunch.net/&#732; vw v7.1.2
We obtain a total of 33139 training instances for all targets (an average of 1656 per target, with a minimum of 30 and a maximum of 5414). Note that this process does not require any manual annotation.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The training instances for the supervised WSD system are built automatically by (1) extracting all occurrences of English target words in context, and (2) annotating them with their aligned Spanish lemma.</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3 we use the optimizer from http://hunch.net/&#732; vw v7.1.2</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We obtain a total of 33139 training instances for all targets (an average of 1656 per target, with a minimum of 30 and a maximum of 5414).</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Note that this process does not require any manual annotation.</text>
                  <doc_id>139</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>5 WSD systems can outperform PBSMT</title>
        <text>Table 2 summarizes the main results. PBSMT outperforms the most frequent sense baseline by a wide margin, and interestingly also yields better results than many of the dedicated WSD systems that participated in the SemEval task. However, PBSMT performance does not match that of the most frequent sense oracle (which uses sense frequencies observed in the test set rather than training set). The WSD system trained on the same word-aligned parallel corpus as the PBSMT system achieves the best performance. It also obtains better results than all but the top system in the official results (Lefever and Hoste, 2010).
The results in Table 2 are quite different from those reported by Carpuat and Wu (2005) on a Chinese WSD task. The Chinese-English PBSMT system performed much worse than any of the dedicated WSD systems on that task. While our WSD system outperforms PBSMT on the CLWSD task too, the difference is not as large, and the PBSMT system is competitive when compared to the full set of systems that were evaluated on this task. This confirms that the CLWSD task represents a more fair benchmark for comparing PBSMT with WSD systems.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Table 2 summarizes the main results.</text>
              <doc_id>140</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>PBSMT outperforms the most frequent sense baseline by a wide margin, and interestingly also yields better results than many of the dedicated WSD systems that participated in the SemEval task.</text>
              <doc_id>141</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, PBSMT performance does not match that of the most frequent sense oracle (which uses sense frequencies observed in the test set rather than training set).</text>
              <doc_id>142</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The WSD system trained on the same word-aligned parallel corpus as the PBSMT system achieves the best performance.</text>
              <doc_id>143</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>It also obtains better results than all but the top system in the official results (Lefever and Hoste, 2010).</text>
              <doc_id>144</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The results in Table 2 are quite different from those reported by Carpuat and Wu (2005) on a Chinese WSD task.</text>
              <doc_id>145</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The Chinese-English PBSMT system performed much worse than any of the dedicated WSD systems on that task.</text>
              <doc_id>146</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>While our WSD system outperforms PBSMT on the CLWSD task too, the difference is not as large, and the PBSMT system is competitive when compared to the full set of systems that were evaluated on this task.</text>
              <doc_id>147</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This confirms that the CLWSD task represents a more fair benchmark for comparing PBSMT with WSD systems.</text>
              <doc_id>148</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>6 Impact of PBSMT Context Models</title>
        <text>What is the impact of PBSMT context models on lexical choice accuracy? Table 3 provides an overview of experiments where we vary the context size available to the PBSMT system. The main PB-
SMT system in the top row uses the default settings presented in Section 3.
In the first set of experiments, we evaluate the impact of the source side context on CLWSD performance. Phrasal translations represent the core of PBSMT systems: they capture collocational context in the source language, and they are therefore are less ambiguous than single words (Koehn and Knight, 2003; Koehn et al., 2003). The default PBSMT learns translations for sources phrases of length ranging from 1 to 7 words. Limiting the PBSMT system to translate shorter phrases (Rows l = 1 and l = 3 in Table 3) surprisingly improves CLWSD performance, even though it degrades BLEU score on WMT test sets. The source context captured by longer phrases therefore does not provide the right disambiguating information in this context.
In the second set of experiments, we evaluate the impact of the context size in the target language, by varying the size of the n-gram language model used. The default PBSMT system used a 5-gram language model. Reducing the n-gram order to 3, 2, 1 and increasing it to 7 both degrade performance. Shorter n-grams do not provide enough disambiguating context, while longer n-grams are more sparse and perhaps do not generalize well outside of the training corpus.
Finally, we report a last experiment which uses a bilingual language model to enrich the context representation in PBSMT (Niehues et al., 2011). This language model is estimated on word pairs formed
by target words augmented with their aligned source words. We use a 4-gram model, trained using Good- Turing discounting. This only results in small improvements (&lt; 0.1) over the standard PBSMT system, and remains far below the performance of the dedicated WSD system.
These results show that source phrases are weak representations of context for the purpose of lexical choice. Target n&#8722;gram context is more useful than source phrasal context, which can surprisingly harm lexical choice accuracy.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>What is the impact of PBSMT context models on lexical choice accuracy?</text>
              <doc_id>149</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Table 3 provides an overview of experiments where we vary the context size available to the PBSMT system.</text>
              <doc_id>150</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The main PB-</text>
              <doc_id>151</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>SMT system in the top row uses the default settings presented in Section 3.</text>
              <doc_id>152</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the first set of experiments, we evaluate the impact of the source side context on CLWSD performance.</text>
              <doc_id>153</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Phrasal translations represent the core of PBSMT systems: they capture collocational context in the source language, and they are therefore are less ambiguous than single words (Koehn and Knight, 2003; Koehn et al., 2003).</text>
              <doc_id>154</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The default PBSMT learns translations for sources phrases of length ranging from 1 to 7 words.</text>
              <doc_id>155</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Limiting the PBSMT system to translate shorter phrases (Rows l = 1 and l = 3 in Table 3) surprisingly improves CLWSD performance, even though it degrades BLEU score on WMT test sets.</text>
              <doc_id>156</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The source context captured by longer phrases therefore does not provide the right disambiguating information in this context.</text>
              <doc_id>157</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the second set of experiments, we evaluate the impact of the context size in the target language, by varying the size of the n-gram language model used.</text>
              <doc_id>158</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The default PBSMT system used a 5-gram language model.</text>
              <doc_id>159</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Reducing the n-gram order to 3, 2, 1 and increasing it to 7 both degrade performance.</text>
              <doc_id>160</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Shorter n-grams do not provide enough disambiguating context, while longer n-grams are more sparse and perhaps do not generalize well outside of the training corpus.</text>
              <doc_id>161</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Finally, we report a last experiment which uses a bilingual language model to enrich the context representation in PBSMT (Niehues et al., 2011).</text>
              <doc_id>162</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This language model is estimated on word pairs formed</text>
              <doc_id>163</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>by target words augmented with their aligned source words.</text>
              <doc_id>164</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We use a 4-gram model, trained using Good- Turing discounting.</text>
              <doc_id>165</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This only results in small improvements (&lt; 0.1) over the standard PBSMT system, and remains far below the performance of the dedicated WSD system.</text>
              <doc_id>166</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>These results show that source phrases are weak representations of context for the purpose of lexical choice.</text>
              <doc_id>167</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Target n&#8722;gram context is more useful than source phrasal context, which can surprisingly harm lexical choice accuracy.</text>
              <doc_id>168</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>7 Impact of PBSMT Reordering Models</title>
        <text>While the phrase-table is the core of PBSMT system, the reordering model used in our system is heavily lexicalized. In this section, we evaluate its impact on CLWSD performance. The standard PB- SMT system uses a hierarchical lexicalized reordering model (Galley and Manning, 2008) in addition to the distance-based distortion limit. Unlike lexicalized reordering(Koehn et al., 2007), which models the orientation of a phrase with respect to the previous phrase, hierarchical reordering models define the orientation of a phrase with respect to the previous block that could have been translated as a single phrase. In Table 4, we show that lexicalized reordering model benefit CLWSD performance, and that the hierarchical model performs slightly better than the non-hierarchical overall.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>While the phrase-table is the core of PBSMT system, the reordering model used in our system is heavily lexicalized.</text>
              <doc_id>169</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this section, we evaluate its impact on CLWSD performance.</text>
              <doc_id>170</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The standard PB- SMT system uses a hierarchical lexicalized reordering model (Galley and Manning, 2008) in addition to the distance-based distortion limit.</text>
              <doc_id>171</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Unlike lexicalized reordering(Koehn et al., 2007), which models the orientation of a phrase with respect to the previous phrase, hierarchical reordering models define the orientation of a phrase with respect to the previous block that could have been translated as a single phrase.</text>
              <doc_id>172</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In Table 4, we show that lexicalized reordering model benefit CLWSD performance, and that the hierarchical model performs slightly better than the non-hierarchical overall.</text>
              <doc_id>173</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>7</index>
        <title>8 Impact of phrase translation selection</title>
        <text>In this section, we consider the impact of various methods for selecting phrase translations on the lexical choice performance of PBSMT. First, we investigate the impact of limiting the number of translation candidates considered for
each source phrase in the phrase-table. The main PBSMT system uses t = 50 translation candidates per source phrase. Limiting that number to 20 and increasing it to 100 both have a very small impact on CLWSD. Second, we prune the phrase-table using a statistical significance test to measure (Johnson et al., 2007). This pruning strategy aims to drastically decrease the size of the phrase-table without degrading translation performance by removing noisy phrase pairs.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this section, we consider the impact of various methods for selecting phrase translations on the lexical choice performance of PBSMT.</text>
              <doc_id>174</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>First, we investigate the impact of limiting the number of translation candidates considered for</text>
              <doc_id>175</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>each source phrase in the phrase-table.</text>
              <doc_id>176</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The main PBSMT system uses t = 50 translation candidates per source phrase.</text>
              <doc_id>177</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Limiting that number to 20 and increasing it to 100 both have a very small impact on CLWSD.</text>
              <doc_id>178</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Second, we prune the phrase-table using a statistical significance test to measure (Johnson et al., 2007).</text>
              <doc_id>179</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This pruning strategy aims to drastically decrease the size of the phrase-table without degrading translation performance by removing noisy phrase pairs.</text>
              <doc_id>180</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>8</index>
        <title>9 Impact of training corpus</title>
        <text>Since increasing the amount of training data is a reliable way to improve translation performance, we evaluate the impact of training the PBSMT system on more than the Europarl data used for controlled comparison with WSD. We increase the parallel training corpus with the WMT-12 News Commentary parallel data 4 . This yields an additional training set of roughly160k sentence pairs. We build linear mixture models to combine translation, reordering and language models learned on Europarl and News Commentary corpora (Foster and Kuhn, 2007). As can be seen in Table 6, this approach improves all CLWSD scores except for 1-gram precision. The decrease in 1-gram precision indicates that the addition of the news corpus introduces new translation candidates that differ from those used in the gold inventory. Interestingly, the additional data is not sufficient to match the performance of the WSD system learned on Europarl only (see Table 2). While additional data should be used when available, richer context features are valuable to make the most of existing data.
4 http://www.statmt.org/wmt12/translation-task.html</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Since increasing the amount of training data is a reliable way to improve translation performance, we evaluate the impact of training the PBSMT system on more than the Europarl data used for controlled comparison with WSD.</text>
              <doc_id>181</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We increase the parallel training corpus with the WMT-12 News Commentary parallel data 4 .</text>
              <doc_id>182</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This yields an additional training set of roughly160k sentence pairs.</text>
              <doc_id>183</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We build linear mixture models to combine translation, reordering and language models learned on Europarl and News Commentary corpora (Foster and Kuhn, 2007).</text>
              <doc_id>184</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>As can be seen in Table 6, this approach improves all CLWSD scores except for 1-gram precision.</text>
              <doc_id>185</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The decrease in 1-gram precision indicates that the addition of the news corpus introduces new translation candidates that differ from those used in the gold inventory.</text>
              <doc_id>186</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Interestingly, the additional data is not sufficient to match the performance of the WSD system learned on Europarl only (see Table 2).</text>
              <doc_id>187</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>While additional data should be used when available, richer context features are valuable to make the most of existing data.</text>
              <doc_id>188</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>4 http://www.statmt.org/wmt12/translation-task.html</text>
              <doc_id>189</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>9</index>
        <title>10 Conclusion</title>
        <text>We use a SemEval Cross-Lingual WSD task to evaluate the lexical choice performance of a typical phrase-based SMT system. Unlike conventional WSD task that rely on abstract sense inventories rather than translations, cross-lingual WSD provides a fair setting for comparing SMT with dedicated WSD systems. Unlike conventional evaluations of machine translation quality, the cross-lingual WSD task lets us isolate a specific aspect of translation quality and show how it is affected by different components of the phrase-based SMT system. Unlike in previous evaluations on conventional WSD tasks (Carpuat and Wu, 2005), phrase-based SMT performance is on par with many dedicated WSD systems. However, the phrase-based SMT system does not perform as well as a WSD system trained on the exact same parallel data. Analysis shows that while many SMT components can potentially have an impact on SMT lexical choice, CLWSD accuracy is most affected by the length of source phrases and order of target n-gram language models. Using shorter source phrases actually improves lexical choice accuracy. The official results for the CLWSD task at SemEval 2013 evaluation provide further insights (Lefever and Hoste, 2013): our PBSMT system can achieve top precision as measured using the top prediction as in this paper, but does not perform as well as other submitted systems when taking into account the top 5 predictions (Carpuat, 2013). This suggests that local context models based on source phrases and target n-grams are much weaker representations of context than the simple templates used by WSD systems, and that even strong PBSMT systems can benefit from context models developed for WSD. New learning algorithms (Chiang et al., 2009; Cherry and Foster, 2012, for instance) finally make it possible for PBSMT to reliably learn from many more features than the typical system used here. Evaluations such as the CLWSD task will provide useful tools for analyzing the impact of these features on lexical choice and inform feature design in increasingly large and complex systems.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We use a SemEval Cross-Lingual WSD task to evaluate the lexical choice performance of a typical phrase-based SMT system.</text>
              <doc_id>190</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Unlike conventional WSD task that rely on abstract sense inventories rather than translations, cross-lingual WSD provides a fair setting for comparing SMT with dedicated WSD systems.</text>
              <doc_id>191</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Unlike conventional evaluations of machine translation quality, the cross-lingual WSD task lets us isolate a specific aspect of translation quality and show how it is affected by different components of the phrase-based SMT system.</text>
              <doc_id>192</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Unlike in previous evaluations on conventional WSD tasks (Carpuat and Wu, 2005), phrase-based SMT performance is on par with many dedicated WSD systems.</text>
              <doc_id>193</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>However, the phrase-based SMT system does not perform as well as a WSD system trained on the exact same parallel data.</text>
              <doc_id>194</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Analysis shows that while many SMT components can potentially have an impact on SMT lexical choice, CLWSD accuracy is most affected by the length of source phrases and order of target n-gram language models.</text>
              <doc_id>195</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Using shorter source phrases actually improves lexical choice accuracy.</text>
              <doc_id>196</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The official results for the CLWSD task at SemEval 2013 evaluation provide further insights (Lefever and Hoste, 2013): our PBSMT system can achieve top precision as measured using the top prediction as in this paper, but does not perform as well as other submitted systems when taking into account the top 5 predictions (Carpuat, 2013).</text>
              <doc_id>197</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>This suggests that local context models based on source phrases and target n-grams are much weaker representations of context than the simple templates used by WSD systems, and that even strong PBSMT systems can benefit from context models developed for WSD.</text>
              <doc_id>198</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>New learning algorithms (Chiang et al., 2009; Cherry and Foster, 2012, for instance) finally make it possible for PBSMT to reliably learn from many more features than the typical system used here.</text>
              <doc_id>199</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Evaluations such as the CLWSD task will provide useful tools for analyzing the impact of these features on lexical choice and inform feature design in increasingly large and complex systems.</text>
              <doc_id>200</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Example of annotated CLWSD instances from the SemEval 2010 test set. For each gold Spanish translation, we are given the number of annotators who proposed it (out of 3 annotators.)</caption>
        <reference_text></reference_text>
        <page_num>2</page_num>
        <head>
          <rows>
            <row>
              <cell>Target word</cell>
              <cell>ring</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>English context</cell>
              <cell>The twelve stars of the European flag are depicted on the outer ring.</cell>
            </row>
            <row>
              <cell>Gold translations</cell>
              <cell>anillo (3);c&#237;rculo (2);corona (2);aro (1);</cell>
            </row>
            <row>
              <cell>English context</cell>
              <cell>The terrors which Mr Cash expresses about our future in the community have a familiar ring</cell>
            </row>
            <row>
              <cell>about them.</cell>
            </row>
            <row>
              <cell>Gold translations</cell>
              <cell>sonar (3);tinte (3);connotaci&#243;n(2);tono (1);</cell>
            </row>
            <row>
              <cell>English context</cell>
              <cell>The American containment ring around the Soviet bloc had been seriously breached only by</cell>
            </row>
            <row>
              <cell>the Soviet acquisition of military facilities in Cuba.</cell>
            </row>
            <row>
              <cell>Gold translations</cell>
              <cell>cerco (2);c&#237;rculo (2);cord&#243;n (2);barrera (1);blindaje (1);limitaci&#243;n (1);</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Main CLWSD results: PBSMT yields competitive results, but WSD outperforms PBSMT</caption>
        <reference_text></reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell>Mode</cell>
              <cell>Mode</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Prec.</cell>
              <cell>Rec.</cell>
              <cell>Prec.</cell>
              <cell>Rec.</cell>
              <cell>BLEU1</cell>
            </row>
            <row>
              <cell>WSD</cell>
              <cell>25.96</cell>
              <cell>25.58</cell>
              <cell>55.02</cell>
              <cell>54.13</cell>
              <cell>76.06</cell>
            </row>
            <row>
              <cell>PBSMT</cell>
              <cell>23.72</cell>
              <cell>23.69</cell>
              <cell>45.49</cell>
              <cell>45.37</cell>
              <cell>62.72</cell>
            </row>
            <row>
              <cell>MFStest</cell>
              <cell>21.35</cell>
              <cell>21.35</cell>
              <cell>44.50</cell>
              <cell>44.50</cell>
              <cell>65.50</cell>
            </row>
            <row>
              <cell>MFStrain</cell>
              <cell>19.14</cell>
              <cell>19.14</cell>
              <cell>42.00</cell>
              <cell>42.00</cell>
              <cell>59.70</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Impact of source and target context models on PBSMT performance</caption>
        <reference_text>In PAGE 6: ... This confirms that the CLWSD task represents a more fair benchmark for comparing PBSMT with WSD sys- tems. 6 Impact of PBSMT Context Models What is the impact of PBSMT context models on lexical choice accuracy?  Table3  provides an overview of experiments where we vary the context size available to the PBSMT system. The main PB-...  In PAGE 6: ...night, 2003; Koehn et al., 2003). The default PBSMT learns translations for sources phrases of length ranging from 1 to 7 words. Limiting the PBSMT system to translate shorter phrases (Rows l = 1 and l = 3 in  Table3 ) surpris- ingly improves CLWSD performance, even though it degrades BLEU score on WMT test sets. The source context captured by longer phrases therefore does not provide the right disambiguating informa- tion in this context....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell>Mode</cell>
              <cell>Mode</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Prec.</cell>
              <cell>Rec.</cell>
              <cell>Prec.</cell>
              <cell>Rec.</cell>
              <cell>BLEU1</cell>
            </row>
            <row>
              <cell>PBSMT</cell>
              <cell>23.72</cell>
              <cell>23.69</cell>
              <cell>45.49</cell>
              <cell>45.37</cell>
              <cell>62.72</cell>
            </row>
            <row>
              <cell>max source phrase length l</cell>
              <cell>max source phrase length l</cell>
              <cell>max source phrase length l</cell>
            </row>
            <row>
              <cell>l = 1</cell>
              <cell>24.44</cell>
              <cell>24.36</cell>
              <cell>44.50</cell>
              <cell>44.38</cell>
              <cell>65.43</cell>
            </row>
            <row>
              <cell>l = 3</cell>
              <cell>24.27</cell>
              <cell>24.22</cell>
              <cell>46.52</cell>
              <cell>46.41</cell>
              <cell>64.33</cell>
            </row>
            <row>
              <cell>n-gram LM order</cell>
              <cell>n-gram LM order</cell>
            </row>
            <row>
              <cell>n = 3</cell>
              <cell>23.60</cell>
              <cell>23.55</cell>
              <cell>44.58</cell>
              <cell>44.47</cell>
              <cell>61.62</cell>
            </row>
            <row>
              <cell>n = 7</cell>
              <cell>23.58</cell>
              <cell>23.53</cell>
              <cell>46.06</cell>
              <cell>45.94</cell>
              <cell>62.22</cell>
            </row>
            <row>
              <cell>n = 2</cell>
              <cell>23.40</cell>
              <cell>23.35</cell>
              <cell>44.75</cell>
              <cell>44.63</cell>
              <cell>63.02</cell>
            </row>
            <row>
              <cell>n = 1</cell>
              <cell>22.92</cell>
              <cell>22.87</cell>
              <cell>43.00</cell>
              <cell>42.89</cell>
              <cell>58.62</cell>
            </row>
            <row>
              <cell>+bilingual LM</cell>
              <cell>+bilingual LM</cell>
            </row>
            <row>
              <cell>4-gram</cell>
              <cell>23.89</cell>
              <cell>23.84</cell>
              <cell>45.49</cell>
              <cell>45.37</cell>
              <cell>62.62</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TET</source>
        <caption>Table 4: Impact of reordering models: lexicalized reodering does not hurt lexical choice only when hierarchical models are used</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell>Mode</cell>
              <cell>Mode</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Prec.</cell>
              <cell>Rec.</cell>
              <cell>Prec.</cell>
              <cell>Rec.</cell>
              <cell>BLEU1</cell>
            </row>
            <row>
              <cell>+ hier</cell>
              <cell>23.72</cell>
              <cell>23.69</cell>
              <cell>45.49</cell>
              <cell>45.37</cell>
              <cell>62.72</cell>
            </row>
            <row>
              <cell>+ lex</cell>
              <cell>23.69</cell>
              <cell>23.64</cell>
              <cell>46.66</cell>
              <cell>46.54</cell>
              <cell>62.22</cell>
            </row>
            <row>
              <cell>dist</cell>
              <cell>23.42</cell>
              <cell>23.37</cell>
              <cell>45.43</cell>
              <cell>45.30</cell>
              <cell>62.22</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TableSeer</source>
        <caption>Table 5: Impact of translation candidate selection on PBSMT performance</caption>
        <reference_text>None</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell>Mode</cell>
              <cell>Mode</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Prec.</cell>
              <cell>Rec.</cell>
              <cell>Prec.</cell>
              <cell>Rec.</cell>
              <cell>BLEU1</cell>
            </row>
            <row>
              <cell>PBSMT</cell>
              <cell>23.72</cell>
              <cell>23.69</cell>
              <cell>45.49</cell>
              <cell>45.37</cell>
              <cell>62.72</cell>
            </row>
            <row>
              <cell>Number t of translations per phrase</cell>
            </row>
            <row>
              <cell>t = 20</cell>
              <cell>23.68</cell>
              <cell>23.63</cell>
              <cell>45.66</cell>
              <cell>45.54</cell>
              <cell>62.32</cell>
            </row>
            <row>
              <cell>t = 100</cell>
              <cell>23.65</cell>
              <cell>23.60</cell>
              <cell>45.65</cell>
              <cell>45.53</cell>
              <cell>62.52</cell>
            </row>
            <row>
              <cell>Other phrase-table pruning methods</cell>
            </row>
            <row>
              <cell>stat sig</cell>
              <cell>23.71</cell>
              <cell>23.66</cell>
              <cell>45.19</cell>
              <cell>45.07</cell>
              <cell>62.62</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>6</id>
        <source>TET</source>
        <caption>Table 6: Impact of training corpus on PBSMT performance: adding news parallel sentences helps Precision and Recall, but does not match WSD on the Europarl only.</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell>Mode</cell>
              <cell>Mode</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Prec.</cell>
              <cell>Rec.</cell>
              <cell>Prec.</cell>
              <cell>Rec.</cell>
              <cell>BLEU1</cell>
            </row>
            <row>
              <cell>Europarl</cell>
              <cell>23.72</cell>
              <cell>23.69</cell>
              <cell>45.49</cell>
              <cell>45.37</cell>
              <cell>62.72</cell>
            </row>
            <row>
              <cell>+ News</cell>
              <cell>24.34</cell>
              <cell>24.28</cell>
              <cell>47.49</cell>
              <cell>47.37</cell>
              <cell>61.22</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>E Agirre</author>
          <author>P G Edmonds</author>
        </authors>
        <title>Word Sense Disambiguation: Algorithms and Applications. Text, Speech, and Language Technology Series.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>1</id>
        <authors/>
        <title>None</title>
        <publication>Proceedings of SemEval-2007: 4th International Workshop on Semantic Evaluation,</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Satanjeev Banerjee</author>
          <author>Alon Lavie</author>
        </authors>
        <title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgement.</title>
        <publication>In Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at the 43th Annual Meeting of the Association of Computational Linguistics (ACL-2005),</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Alexandra Birch</author>
          <author>Mile Osborne</author>
          <author>Phil Blunsom</author>
        </authors>
        <title>Metrics for MT evaluation: Evaluating reordering.</title>
        <publication>Machine Translation,</publication>
        <pages>24--15</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Chris Callison-Burch</author>
          <author>Philipp Koehn</author>
          <author>Christof Monz</author>
          <author>Kay Peterson</author>
          <author>Mark Przybocki</author>
          <author>Omar F Zaidan</author>
        </authors>
        <title>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</title>
        <publication>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT &#8217;10,</publication>
        <pages>17--53</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Marine Carpuat</author>
          <author>Dekai Wu</author>
        </authors>
        <title>Evaluating the Word Sense Disambiguation Performance of Statistical Machine Translation.</title>
        <publication>In Proceedings of the Second International Joint Conference on Natural Language Processing (IJCNLP),</publication>
        <pages>122--127</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Marine Carpuat</author>
          <author>Dekai Wu</author>
        </authors>
        <title>Evaluation of Context-Dependent Phrasal Translation Lexicons for Statistical Machine Translation.</title>
        <publication>In Proceedings of the sixth conference on Language Resouces and Evaluation (LREC</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Marine Carpuat</author>
          <author>Hal Daum&#233; Alexander Fraser</author>
          <author>Chris Quirk</author>
          <author>Fabienne Braune</author>
        </authors>
        <title>Domain adaptation in machine translation: Final report.</title>
        <publication>In 2012 Johns Hopkins Summer Workshop Final Report.</publication>
        <pages>None</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Marine Carpuat</author>
        </authors>
        <title>Nrc: A machine translation approach to cross-lingual word sense disambiguation (SemEval-2013 Task 10).</title>
        <publication>In Proceedings of SemEval.</publication>
        <pages>None</pages>
        <date>2013</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Boxing Chen</author>
          <author>Roland Kuhn</author>
          <author>George Foster</author>
          <author>Howard Johnson</author>
        </authors>
        <title>Unpacking and transforming feature functions: New ways to smooth phrase tables.</title>
        <publication>In Proceedings of Machine Translation Summit.</publication>
        <pages>None</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Boxing Chen</author>
          <author>Roland Kuhn</author>
          <author>Samuel Larkin</author>
        </authors>
        <title>Port: a precision-order-recall mt evaluation metric for tuning.</title>
        <publication>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>930--939</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Colin Cherry</author>
          <author>George Foster</author>
        </authors>
        <title>Batch tuning strategies for statistical machine translation.</title>
        <publication>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</publication>
        <pages>427--436</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>David Chiang</author>
          <author>Kevin Knight</author>
          <author>Wei Wang</author>
        </authors>
        <title>11,001 new features for statistical machine translation.</title>
        <publication>In NAACL-HLT 2009: Proceedings of the 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</publication>
        <pages>218--226</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Timothy Chklovski</author>
          <author>Rada Mihalcea</author>
          <author>Ted Pedersen</author>
          <author>Amruta Purandare</author>
        </authors>
        <title>The Senseval-3 Multilingual English-Hindi lexical sample task.</title>
        <publication>In Proceedings of Senseval-3, Third International Workshop on Evaluating Word Sense Disambiguation Systems,</publication>
        <pages>5--8</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Michael Denkowski</author>
          <author>Alon Lavie</author>
        </authors>
        <title>METEORNEXT and the METEOR paraphrase tables: improved evaluation support for five target languages.</title>
        <publication>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT &#8217;10,</publication>
        <pages>339--342</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Zhendong Dong</author>
        </authors>
        <title>Knowledge description: what, how and who?</title>
        <publication>In Proceedings of International Symposium on Electronic Dictionary,</publication>
        <pages>None</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Markus Dreyer</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Hyter: Meaning-equivalent semantics for translation evaluation.</title>
        <publication>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</publication>
        <pages>162--171</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Philip Edmonds</author>
          <author>Scott Cotton</author>
        </authors>
        <title>Senseval-2: Overview.</title>
        <publication>In Proceedings of Senseval-2: Second International Workshop on Evaluating Word Snese Disambiguation Systems,</publication>
        <pages>1--5</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>18</id>
        <authors/>
        <title>WordNet: An Electronic Lexical Database.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Radu Florian</author>
          <author>Silviu Cucerzan</author>
          <author>Charles Schafer</author>
          <author>David Yarowsky</author>
        </authors>
        <title>Combining classifiers for word sense disambiguation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>George Foster</author>
          <author>Roland Kuhn</author>
        </authors>
        <title>Mixture-model adaptation for SMT.</title>
        <publication>In Proceedings of the Second Workshop on Statistical Machine Translation,</publication>
        <pages>128--135</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Michel Galley</author>
          <author>Christopher D Manning</author>
        </authors>
        <title>A simple and effective hierarchical phrase reordering model.</title>
        <publication>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &#8217;08,</publication>
        <pages>848--856</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Jes&#250;s Gim&#233;nez</author>
          <author>Llu&#237;s M&#225;rquez</author>
        </authors>
        <title>Linguistic Features for Automatic Evaluation of Heterogenous MT Systems.</title>
        <publication>In Proceedings of the Second Workshop on Statistical Machine Translation,</publication>
        <pages>256--264</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Peng Jin</author>
          <author>Yunfang Wu</author>
          <author>Shiwen Yu</author>
        </authors>
        <title>Semeval2007 task 05: Multilingual chinese-english lexical sample.</title>
        <publication>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</publication>
        <pages>pages</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>Howard Johnson</author>
          <author>Joel Martin</author>
          <author>George Foster</author>
          <author>Roland Kuhn</author>
        </authors>
        <title>Improving Translation Quality by Discarding Most of the Phrasetable.</title>
        <publication>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</publication>
        <pages>967--975</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Kevin Knight</author>
        </authors>
        <title>Feature-Rich Statistical Translation of Noun Phrases.</title>
        <publication>In Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical Phrase-based Translation.</title>
        <publication>In Proceedings of HLT/NAACL-2003,</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>27</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Hieu Hoang</author>
          <author>Alexandra Birch</author>
          <author>Chris Callison-Burch</author>
          <author>Marcello Federico</author>
          <author>Nicola Bertoldi</author>
          <author>Brooke Cowan</author>
          <author>Wade Shen</author>
          <author>Christine Moran</author>
          <author>Richard Zens</author>
        </authors>
        <title>Chris Dyer, Ondrej Bojar,</title>
        <publication>In Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session,</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>28</id>
        <authors>
          <author>Philipp Koehn</author>
        </authors>
        <title>Europarl: A parallel corpus for statistical machine translation.</title>
        <publication>In Machine Translation Summit X,</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>29</id>
        <authors>
          <author>Lefever</author>
          <author>V&#233;ronique Hoste</author>
        </authors>
        <title>Semeval-2010 task 3: Cross-lingual word sense disambiguation.</title>
        <publication>In Proceedings of the 5th International Workshop on Semantic Evaluation,</publication>
        <pages>15--20</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>30</id>
        <authors>
          <author>Els Lefever</author>
          <author>V&#233;ronique Hoste</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2013</date>
      </reference>
      <reference>
        <id>31</id>
        <authors/>
        <title>None</title>
        <publication>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013),</publication>
        <pages>None</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>32</id>
        <authors>
          <author>Els Lefever</author>
          <author>V&#233;ronique Hoste</author>
          <author>Martine De Cock</author>
        </authors>
        <title>Parasense or how to use parallel corpora for word sense disambiguation.</title>
        <publication>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</publication>
        <pages>317--322</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>33</id>
        <authors>
          <author>Chi-kiu Lo</author>
          <author>Dekai Wu</author>
        </authors>
        <title>Meant: an inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility via semantic frames.</title>
        <publication>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT &#8217;11,</publication>
        <pages>220--229</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>34</id>
        <authors>
          <author>Chi-kiu Lo</author>
          <author>Anand Karthik Tumuluru</author>
          <author>Dekai Wu</author>
        </authors>
        <title>Fully automatic semantic MT evaluation.</title>
        <publication>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</publication>
        <pages>243--252</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>35</id>
        <authors/>
        <title>None</title>
        <publication>Proceedings of Senseval-3: Third international Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>36</id>
        <authors>
          <author>Rada Mihalcea</author>
          <author>Timothy Chklovski</author>
          <author>Adam Killgariff</author>
        </authors>
        <title>The Senseval-3 English lexical sample task.</title>
        <publication>In Proceedings of Senseval-3, Third International Workshop on Evaluating Word Sense Disambiguation Systems,</publication>
        <pages>25--28</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>37</id>
        <authors>
          <author>Rada Mihalcea</author>
          <author>Ravi Sinha</author>
          <author>Diana McCarthy</author>
        </authors>
        <title>SemEval-2010 Task 2: Cross-Lingual Lexical Substitution.</title>
        <publication>In Proceedings of the 5th International Workshop on Semantic Evaluation,</publication>
        <pages>9--14</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>38</id>
        <authors>
          <author>Robert C Moore</author>
          <author>William Lewis</author>
        </authors>
        <title>Intelligent selection of language model training data.</title>
        <publication>In Proceedings of the ACL 2010 Conference Short Papers, ACLShort &#8217;10,</publication>
        <pages>220--224</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>39</id>
        <authors>
          <author>Jan Niehues</author>
          <author>Teresa Herrmann</author>
          <author>Stephan Vogel</author>
          <author>Alex Waibel</author>
        </authors>
        <title>Wider context by using bilingual language models in machine translation.</title>
        <publication>In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT &#8217;11,</publication>
        <pages>198--206</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>40</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A Systematic Comparison of Various Statistical Alignment Models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>41</id>
        <authors>
          <author>Llu&#237;s Padr&#243;</author>
          <author>Evgeny Stanilovsky</author>
        </authors>
        <title>FreeLing 3.0: Towards wider multilinguality.</title>
        <publication>In Proceedings of the Language Resources and Evaluation Conference (LREC 2012),</publication>
        <pages>None</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>42</id>
        <authors>
          <author>Martha Palmer</author>
          <author>Christiane Fellbaum</author>
          <author>Scott Cotton</author>
          <author>Lauren Delfs</author>
          <author>Hao Trang Dang</author>
        </authors>
        <title>English tasks: All-words and verb lexical sample.</title>
        <publication>In Proceedings of Senseval-2, Second International Workshop on Evaluating Word Sense Disambiguation Systems,</publication>
        <pages>21--24</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>43</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>BLEU: a method for automatic evaluation of machine translation.</title>
        <publication>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>44</id>
        <authors>
          <author>Maarten van Gompel</author>
        </authors>
        <title>Uvt-wsd1: A cross-lingual word sense disambiguation system.</title>
        <publication>In Proceedings of the 5th International Workshop on Semantic Evaluation,</publication>
        <pages>238--241</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>45</id>
        <authors>
          <author>David Vickrey</author>
          <author>Luke Biewald</author>
          <author>Marc Teyssier</author>
          <author>Daphne Koller</author>
        </authors>
        <title>Word-Sense Disambiguation for Machine Translation.</title>
        <publication>In Joint Human Language Technology conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005),</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>46</id>
        <authors>
          <author>David Vilar</author>
          <author>Jia Xu</author>
          <author>Luis Fernando D&#8217;Haro</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Error Analysis of Machine Translation Output.</title>
        <publication>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC&#8217;06),</publication>
        <pages>697--702</pages>
        <date>2006</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Agirre and Edmonds (2006)</string>
        <sentence_id>52281</sentence_id>
        <char_offset>113</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>2</reference_id>
        <string>Banerjee and Lavie, 2005</string>
        <sentence_id>52263</sentence_id>
        <char_offset>37</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>3</reference_id>
        <string>Birch et al., 2010</string>
        <sentence_id>52267</sentence_id>
        <char_offset>116</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>4</reference_id>
        <string>Callison-Burch et al., 2010</string>
        <sentence_id>52263</sentence_id>
        <char_offset>289</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>5</reference_id>
        <string>Carpuat and Wu (2005)</string>
        <sentence_id>52404</sentence_id>
        <char_offset>66</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>5</reference_id>
        <string>Carpuat and Wu, 2005</string>
        <sentence_id>52276</sentence_id>
        <char_offset>191</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>5</reference_id>
        <string>Carpuat and Wu, 2005</string>
        <sentence_id>52277</sentence_id>
        <char_offset>332</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>5</reference_id>
        <string>Carpuat and Wu, 2005</string>
        <sentence_id>52293</sentence_id>
        <char_offset>70</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>5</reference_id>
        <string>Carpuat and Wu, 2005</string>
        <sentence_id>52370</sentence_id>
        <char_offset>105</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>5</reference_id>
        <string>Carpuat and Wu, 2005</string>
        <sentence_id>52452</sentence_id>
        <char_offset>58</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>6</reference_id>
        <string>Carpuat and Wu, 2008</string>
        <sentence_id>52276</sentence_id>
        <char_offset>63</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>6</reference_id>
        <string>Carpuat and Wu, 2008</string>
        <sentence_id>52327</sentence_id>
        <char_offset>190</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>7</reference_id>
        <string>Carpuat et al., 2012</string>
        <sentence_id>52315</sentence_id>
        <char_offset>254</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>7</reference_id>
        <string>Carpuat et al., 2012</string>
        <sentence_id>52327</sentence_id>
        <char_offset>212</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>8</reference_id>
        <string>Carpuat, 2013</string>
        <sentence_id>52456</sentence_id>
        <char_offset>321</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>9</reference_id>
        <string>Chen et al., 2011</string>
        <sentence_id>52357</sentence_id>
        <char_offset>148</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>10</reference_id>
        <string>Chen et al., 2012</string>
        <sentence_id>52267</sentence_id>
        <char_offset>136</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>11</reference_id>
        <string>Cherry and Foster, 2012</string>
        <sentence_id>52364</sentence_id>
        <char_offset>83</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>11</reference_id>
        <string>Cherry and Foster, 2012</string>
        <sentence_id>52458</sentence_id>
        <char_offset>46</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>13</reference_id>
        <string>Chklovski et al., 2004</string>
        <sentence_id>52286</sentence_id>
        <char_offset>228</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>14</reference_id>
        <string>Denkowski and Lavie, 2010</string>
        <sentence_id>52313</sentence_id>
        <char_offset>61</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>15</reference_id>
        <string>Dong, 1998</string>
        <sentence_id>52286</sentence_id>
        <char_offset>170</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>15</reference_id>
        <string>Dong, 1998</string>
        <sentence_id>52293</sentence_id>
        <char_offset>182</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>16</reference_id>
        <string>Dreyer and Marcu, 2012</string>
        <sentence_id>52319</sentence_id>
        <char_offset>332</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>17</reference_id>
        <string>Edmonds and Cotton, 2001</string>
        <sentence_id>52282</sentence_id>
        <char_offset>50</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>19</reference_id>
        <string>Florian et al., 2002</string>
        <sentence_id>52387</sentence_id>
        <char_offset>89</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>20</reference_id>
        <string>Foster and Kuhn, 2007</string>
        <sentence_id>52443</sentence_id>
        <char_offset>135</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>21</reference_id>
        <string>Galley and Manning, 2008</string>
        <sentence_id>52358</sentence_id>
        <char_offset>189</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>21</reference_id>
        <string>Galley and Manning, 2008</string>
        <sentence_id>52430</sentence_id>
        <char_offset>78</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>22</reference_id>
        <string>Gim&#233;nez and M&#225;rquez, 2007</string>
        <sentence_id>52263</sentence_id>
        <char_offset>63</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>23</reference_id>
        <string>Jin et al., 2007</string>
        <sentence_id>52285</sentence_id>
        <char_offset>125</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>24</reference_id>
        <string>Johnson et al., 2007</string>
        <sentence_id>52438</sentence_id>
        <char_offset>84</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>25</reference_id>
        <string>Koehn and Knight, 2003</string>
        <sentence_id>52413</sentence_id>
        <char_offset>178</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>26</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>52413</sentence_id>
        <char_offset>202</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>27</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>52431</sentence_id>
        <char_offset>30</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>28</reference_id>
        <string>Koehn, 2005</string>
        <sentence_id>52303</sentence_id>
        <char_offset>45</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>29</reference_id>
        <string>Lefever and Hoste, 2010</string>
        <sentence_id>52277</sentence_id>
        <char_offset>73</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>29</reference_id>
        <string>Lefever and Hoste, 2010</string>
        <sentence_id>52284</sentence_id>
        <char_offset>246</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>29</reference_id>
        <string>Lefever and Hoste, 2010</string>
        <sentence_id>52287</sentence_id>
        <char_offset>100</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>29</reference_id>
        <string>Lefever and Hoste, 2010</string>
        <sentence_id>52308</sentence_id>
        <char_offset>71</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>29</reference_id>
        <string>Lefever and Hoste, 2010</string>
        <sentence_id>52372</sentence_id>
        <char_offset>128</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>29</reference_id>
        <string>Lefever and Hoste, 2010</string>
        <sentence_id>52403</sentence_id>
        <char_offset>84</char_offset>
      </citation>
      <citation>
        <id>42</id>
        <reference_id>30</reference_id>
        <string>Lefever and Hoste, 2013</string>
        <sentence_id>52277</sentence_id>
        <char_offset>98</char_offset>
      </citation>
      <citation>
        <id>43</id>
        <reference_id>30</reference_id>
        <string>Lefever and Hoste, 2013</string>
        <sentence_id>52456</sentence_id>
        <char_offset>93</char_offset>
      </citation>
      <citation>
        <id>44</id>
        <reference_id>33</reference_id>
        <string>Lo and Wu, 2011</string>
        <sentence_id>52263</sentence_id>
        <char_offset>90</char_offset>
      </citation>
      <citation>
        <id>45</id>
        <reference_id>34</reference_id>
        <string>Lo et al., 2012</string>
        <sentence_id>52314</sentence_id>
        <char_offset>57</char_offset>
      </citation>
      <citation>
        <id>46</id>
        <reference_id>36</reference_id>
        <string>Mihalcea et al., 2004</string>
        <sentence_id>52284</sentence_id>
        <char_offset>223</char_offset>
      </citation>
      <citation>
        <id>47</id>
        <reference_id>37</reference_id>
        <string>Mihalcea et al., 2010</string>
        <sentence_id>52329</sentence_id>
        <char_offset>73</char_offset>
      </citation>
      <citation>
        <id>48</id>
        <reference_id>38</reference_id>
        <string>Moore and Lewis, 2010</string>
        <sentence_id>52377</sentence_id>
        <char_offset>81</char_offset>
      </citation>
      <citation>
        <id>49</id>
        <reference_id>39</reference_id>
        <string>Niehues et al., 2011</string>
        <sentence_id>52421</sentence_id>
        <char_offset>122</char_offset>
      </citation>
      <citation>
        <id>50</id>
        <reference_id>40</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>52365</sentence_id>
        <char_offset>69</char_offset>
      </citation>
      <citation>
        <id>51</id>
        <reference_id>41</reference_id>
        <string>Padr&#243; and Stanilovsky, 2012</string>
        <sentence_id>52378</sentence_id>
        <char_offset>81</char_offset>
      </citation>
      <citation>
        <id>52</id>
        <reference_id>42</reference_id>
        <string>Palmer et al., 2001</string>
        <sentence_id>52284</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>53</id>
        <reference_id>43</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>52263</sentence_id>
        <char_offset>14</char_offset>
      </citation>
      <citation>
        <id>54</id>
        <reference_id>43</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>52353</sentence_id>
        <char_offset>74</char_offset>
      </citation>
      <citation>
        <id>55</id>
        <reference_id>45</reference_id>
        <string>Vickrey et al., 2005</string>
        <sentence_id>52327</sentence_id>
        <char_offset>141</char_offset>
      </citation>
    </citations>
  </content>
</document>
