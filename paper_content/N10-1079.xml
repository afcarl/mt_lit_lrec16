<document>
  <filename>N10-1079</filename>
  <authors>
    <author>Daniel Ortiz-Mart&#237;nez</author>
    <author>Ismael Garc&#237;a-Varea</author>
  </authors>
  <title>Online Learning for Interactive Statistical Machine Translation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>1 Introduction</title>
        <text>Information technology advances have led to the need for more efficient translation methods. Current MT systems are not able to produce ready-to-use texts. Indeed, MT systems usually require human post-editing to achieve high-quality translations.
One way of taking advantage of MT systems is to combine them with the knowledge of a human translator in the IMT paradigm, which is a special type of the computer-assisted translation paradigm (Isabelle and Church, 1997). An important contribution to IMT technology was pioneered by the TransType project (Foster et al., 1997; Langlais et al., 2002) where data driven MT techniques were adapted for their use in an interactive translation environment.
Following the TransType ideas, Barrachina et al. (2009) proposed a new approach to IMT, in which fully-fledged statistical MT (SMT) systems are used to produce full target sentence hypotheses, or portions thereof, which can be partially or completely accepted and amended by a human translator. Each partial, correct text segment is then used by the SMT system as additional information to achieve improved suggestions. Figure 1 illustrates a typical IMT session.
In this paper, we also focus on the IMT framework. Specifically, we present an IMT system that is able to learn from user feedback. For this purpose, we apply the online learning paradigm to the IMT framework. The online learning techniques that we propose here allow the statistical models involved in the translation process to be incrementally updated.
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 546&#8211;554, Los Angeles, California, June 2010. c&#9675;2010 Association for Computational Linguistics
Figure 2 (inspired from (Vidal et al., 2007)) shows a schematic view of these ideas. Here, f is the input sentence and e is the output derived by the IMT system from f. By observing f and e, the user interacts with the IMT system until the desired output &#234; is produced. The input sentence f and its desired translation &#234; can be used to refine the models used by the system. In general, the model is initially obtained through a classical batch training process from a previously given training sequence of pairs (f i ,e i ) from the task being considered. Now, the models can be extended with the use of valuable user feedback.
f
f , e 1 1 f , e 2 2 . . .
f
Batch Learning
e
k
Interactive SMT System
Incremental Models
feedback/interactions
f
Online Learning</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Information technology advances have led to the need for more efficient translation methods.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Current MT systems are not able to produce ready-to-use texts.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Indeed, MT systems usually require human post-editing to achieve high-quality translations.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>One way of taking advantage of MT systems is to combine them with the knowledge of a human translator in the IMT paradigm, which is a special type of the computer-assisted translation paradigm (Isabelle and Church, 1997).</text>
              <doc_id>3</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>An important contribution to IMT technology was pioneered by the TransType project (Foster et al., 1997; Langlais et al., 2002) where data driven MT techniques were adapted for their use in an interactive translation environment.</text>
              <doc_id>4</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Following the TransType ideas, Barrachina et al. (2009) proposed a new approach to IMT, in which fully-fledged statistical MT (SMT) systems are used to produce full target sentence hypotheses, or portions thereof, which can be partially or completely accepted and amended by a human translator.</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each partial, correct text segment is then used by the SMT system as additional information to achieve improved suggestions.</text>
              <doc_id>6</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Figure 1 illustrates a typical IMT session.</text>
              <doc_id>7</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this paper, we also focus on the IMT framework.</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Specifically, we present an IMT system that is able to learn from user feedback.</text>
              <doc_id>9</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For this purpose, we apply the online learning paradigm to the IMT framework.</text>
              <doc_id>10</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The online learning techniques that we propose here allow the statistical models involved in the translation process to be incrementally updated.</text>
              <doc_id>11</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 546&#8211;554, Los Angeles, California, June 2010. c&#9675;2010 Association for Computational Linguistics</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Figure 2 (inspired from (Vidal et al., 2007)) shows a schematic view of these ideas.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Here, f is the input sentence and e is the output derived by the IMT system from f. By observing f and e, the user interacts with the IMT system until the desired output &#234; is produced.</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The input sentence f and its desired translation &#234; can be used to refine the models used by the system.</text>
              <doc_id>15</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In general, the model is initially obtained through a classical batch training process from a previously given training sequence of pairs (f i ,e i ) from the task being considered.</text>
              <doc_id>16</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Now, the models can be extended with the use of valuable user feedback.</text>
              <doc_id>17</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>f</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>f , e 1 1 f , e 2 2 .</text>
              <doc_id>19</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>20</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>21</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>f</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Batch Learning</text>
              <doc_id>23</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>k</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Interactive SMT System</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Incremental Models</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>feedback/interactions</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>f</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Online Learning</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>2 Interactive machine translation</title>
        <text>IMT can be seen as an evolution of the SMT framework. Given a sentence f from a source language F to be translated into a target sentence e of a target language E, the fundamental equation of SMT (Brown et al., 1993) is the following:
&#234; = argmax {Pr(e |f)} (1)
e
= argmax {Pr(f |e)Pr(e)} (2)
e
where Pr(f |e) is approximated by a translation model that represents the correlation between the source and the target sentence and where P r(e) is approximated by a language model representing the well-formedness of the candidate translation e.
State-of-the-art statistical machine translation systems follow a loglinear approach (Och and Ney, e^
e^ 2002), where direct modelling of the posterior probability Pr(e |f) of Equation (1) is used. In this case, the decision rule is given by the expression:
&#234; = argmax
e
{ &#8721; M }
&#955; m h m (e,f)
m=1
(3)
where each h m (e,f) is a feature function representing a statistical model and &#955; m its weight.
Current MT systems are based on the use of phrase-based models (Koehn et al., 2003) as translation models. The basic idea of Phrase-based Translation (PBT) is to segment the source sentence into phrases, then to translate each source phrase into a target phrase, and finally to reorder the translated target phrases in order to compose the target sentence. If we summarize all the decisions made during the phrase-based translation process by means of the hidden variable &#227; K 1 , we obtain the expression:
Pr(f|e) = &#8721; Pr( &#732;f 1 K , &#227; K 1 | &#7869; K 1 ) (4)
K,&#227; K 1
where each &#227; k &#8712; {1 . . .K} denotes the index of the target phrase &#7869; that is aligned with the k-th source phrase &#732;f k , assuming a segmentation of length K.
According to Equation (4), and following a maximum approximation, the problem stated in Equation (2) can be reframed as:
&#234; &#8776; arg max
e,a
{ p(e) &#183; p(f,a|e) } (5)
In the IMT scenario, we have to find an extension e s for a given prefix e p . To do this we reformulate Equation (5) as follows:
&#234; s &#8776; arg max
e s,a
{ p(es |e p ) &#183; p(f,a|e p ,e s ) } (6)
where the term p(e p ) has been dropped since it does depend neither on e s nor on a.
Thus, the search is restricted to those sentences e which contain e p as prefix. It is also worth mentioning that the similarities between Equation (6) and Equation (5) (note that e p e s &#8801; e) allow us to use the same models whenever the search procedures are adequately modified (Barrachina et al., 2009).
Following the loglinear approach stated in Equation (3), Equation (6) can be rewriten as: { M } &#8721;
&#234; s = argmax &#955; m h m (e,a,f) (7)
e s,a m=1
which is the approach that we follow in this work.
A common problem in IMT arises when the user sets a prefix (e p ) which cannot be found in the phrase-based statistical translation model. Different solutions have been proposed to deal with this problem. The use of word translation graphs, as a compact representation of all possible translations of a source sentence, is proposed in (Barrachina et al., 2009). In (Ortiz-Mart&#237;nez et al., 2009), a technique based on the generation of partial phrasebased alignments is described. This last proposal has also been adopted in this work.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>IMT can be seen as an evolution of the SMT framework.</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Given a sentence f from a source language F to be translated into a target sentence e of a target language E, the fundamental equation of SMT (Brown et al., 1993) is the following:</text>
              <doc_id>32</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#234; = argmax {Pr(e |f)} (1)</text>
              <doc_id>33</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e</text>
              <doc_id>34</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>= argmax {Pr(f |e)Pr(e)} (2)</text>
              <doc_id>35</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e</text>
              <doc_id>36</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where Pr(f |e) is approximated by a translation model that represents the correlation between the source and the target sentence and where P r(e) is approximated by a language model representing the well-formedness of the candidate translation e.</text>
              <doc_id>37</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>State-of-the-art statistical machine translation systems follow a loglinear approach (Och and Ney, e^</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e^ 2002), where direct modelling of the posterior probability Pr(e |f) of Equation (1) is used.</text>
              <doc_id>39</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this case, the decision rule is given by the expression:</text>
              <doc_id>40</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#234; = argmax</text>
              <doc_id>41</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e</text>
              <doc_id>42</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{ &#8721; M }</text>
              <doc_id>43</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#955; m h m (e,f)</text>
              <doc_id>44</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>m=1</text>
              <doc_id>45</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(3)</text>
              <doc_id>46</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where each h m (e,f) is a feature function representing a statistical model and &#955; m its weight.</text>
              <doc_id>47</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Current MT systems are based on the use of phrase-based models (Koehn et al., 2003) as translation models.</text>
              <doc_id>48</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The basic idea of Phrase-based Translation (PBT) is to segment the source sentence into phrases, then to translate each source phrase into a target phrase, and finally to reorder the translated target phrases in order to compose the target sentence.</text>
              <doc_id>49</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>If we summarize all the decisions made during the phrase-based translation process by means of the hidden variable &#227; K 1 , we obtain the expression:</text>
              <doc_id>50</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Pr(f|e) = &#8721; Pr( &#732;f 1 K , &#227; K 1 | &#7869; K 1 ) (4)</text>
              <doc_id>51</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>K,&#227; K 1</text>
              <doc_id>52</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where each &#227; k &#8712; {1 .</text>
              <doc_id>53</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>54</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>.K} denotes the index of the target phrase &#7869; that is aligned with the k-th source phrase &#732;f k , assuming a segmentation of length K.</text>
              <doc_id>55</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>According to Equation (4), and following a maximum approximation, the problem stated in Equation (2) can be reframed as:</text>
              <doc_id>56</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#234; &#8776; arg max</text>
              <doc_id>57</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e,a</text>
              <doc_id>58</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{ p(e) &#183; p(f,a|e) } (5)</text>
              <doc_id>59</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the IMT scenario, we have to find an extension e s for a given prefix e p .</text>
              <doc_id>60</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To do this we reformulate Equation (5) as follows:</text>
              <doc_id>61</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#234; s &#8776; arg max</text>
              <doc_id>62</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e s,a</text>
              <doc_id>63</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{ p(es |e p ) &#183; p(f,a|e p ,e s ) } (6)</text>
              <doc_id>64</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where the term p(e p ) has been dropped since it does depend neither on e s nor on a.</text>
              <doc_id>65</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Thus, the search is restricted to those sentences e which contain e p as prefix.</text>
              <doc_id>66</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It is also worth mentioning that the similarities between Equation (6) and Equation (5) (note that e p e s &#8801; e) allow us to use the same models whenever the search procedures are adequately modified (Barrachina et al., 2009).</text>
              <doc_id>67</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Following the loglinear approach stated in Equation (3), Equation (6) can be rewriten as: { M } &#8721;</text>
              <doc_id>68</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#234; s = argmax &#955; m h m (e,a,f) (7)</text>
              <doc_id>69</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e s,a m=1</text>
              <doc_id>70</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>which is the approach that we follow in this work.</text>
              <doc_id>71</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A common problem in IMT arises when the user sets a prefix (e p ) which cannot be found in the phrase-based statistical translation model.</text>
              <doc_id>72</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Different solutions have been proposed to deal with this problem.</text>
              <doc_id>73</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The use of word translation graphs, as a compact representation of all possible translations of a source sentence, is proposed in (Barrachina et al., 2009).</text>
              <doc_id>74</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In (Ortiz-Mart&#237;nez et al., 2009), a technique based on the generation of partial phrasebased alignments is described.</text>
              <doc_id>75</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This last proposal has also been adopted in this work.</text>
              <doc_id>76</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>3 Related work</title>
        <text>In this paper we present an application of the online learning paradigm to the IMT framework. In the online learning setting, models are trained sample by sample. Our work is also related to model adaptation, although model adaptation and online learning are not exactly the same thing.
The online learning paradigm has been previously applied to train discriminative models in SMT (Liang et al., 2006; Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2008). These works differ from the one presented here in that we apply online learning techniques to train generative models instead of discriminative models. In (Nepveu et al., 2004), dynamic adaptation of an IMT system via cache-based model extensions to language and translation models is proposed. The work by Nepveu et al. (2004) constitutes a domain adaptation technique and not an online learning technique, since the proposed cache components require pre-existent models estimated in batch mode. In addition to this, their IMT system does not use state-of-the-art models.
To our knowledge, the only previous work on online learning for IMT is (Cesa-Bianchi et al., 2008), where a very constrained version of online learning is presented. This constrained version of online learning is not able to extend the translation models due to technical problems with the efficiency of the learning process. In this paper, we present a purely statistical IMT system which is able to incrementally update the parameters of all of the different models that are used in the system, including the translation model, breaking with the above mentioned constraints. What is more, our system is able to learn from scratch, that is, without any preexisting model stored in the system. This is demonstrated empirically in section 5.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper we present an application of the online learning paradigm to the IMT framework.</text>
              <doc_id>77</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In the online learning setting, models are trained sample by sample.</text>
              <doc_id>78</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Our work is also related to model adaptation, although model adaptation and online learning are not exactly the same thing.</text>
              <doc_id>79</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The online learning paradigm has been previously applied to train discriminative models in SMT (Liang et al., 2006; Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2008).</text>
              <doc_id>80</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>These works differ from the one presented here in that we apply online learning techniques to train generative models instead of discriminative models.</text>
              <doc_id>81</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In (Nepveu et al., 2004), dynamic adaptation of an IMT system via cache-based model extensions to language and translation models is proposed.</text>
              <doc_id>82</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The work by Nepveu et al. (2004) constitutes a domain adaptation technique and not an online learning technique, since the proposed cache components require pre-existent models estimated in batch mode.</text>
              <doc_id>83</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In addition to this, their IMT system does not use state-of-the-art models.</text>
              <doc_id>84</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To our knowledge, the only previous work on online learning for IMT is (Cesa-Bianchi et al., 2008), where a very constrained version of online learning is presented.</text>
              <doc_id>85</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This constrained version of online learning is not able to extend the translation models due to technical problems with the efficiency of the learning process.</text>
              <doc_id>86</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, we present a purely statistical IMT system which is able to incrementally update the parameters of all of the different models that are used in the system, including the translation model, breaking with the above mentioned constraints.</text>
              <doc_id>87</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>What is more, our system is able to learn from scratch, that is, without any preexisting model stored in the system.</text>
              <doc_id>88</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This is demonstrated empirically in section 5.</text>
              <doc_id>89</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>4 Online IMT</title>
        <text>In this section we propose an online IMT system. First, we describe the basic IMT system involved in the interactive translation process. Then we introduce the required techniques to incrementally update the statistical models used by the system.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this section we propose an online IMT system.</text>
              <doc_id>90</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>First, we describe the basic IMT system involved in the interactive translation process.</text>
              <doc_id>91</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Then we introduce the required techniques to incrementally update the statistical models used by the system.</text>
              <doc_id>92</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Basic IMT system</title>
            <text>The basic IMT system that we propose uses a loglinear model to generate its translations. According to Equation (7), we introduce a set of seven feature functions (from h 1 to h 7 ):
p(e i |e i&#8722;1 i&#8722;n+1 ) = max{c X(e i i&#8722;n+1 ) &#8722; D n, 0} c X (e i&#8722;1 i&#8722;n+1 ) +
D n c X (e i&#8722;1 i&#8722;n+1 )N 1+(e i&#8722;1 i&#8722;n+1 &#8226;) &#183; p(e i|e i&#8722;1 i&#8722;n+2 ) (8)
c
where D n = n,1
c n,1 +2c n,2
is a fixed discount (c n,1 and c n,2 are the number of n-grams with one and two counts respectively), N 1+ (e i&#8722;1 i&#8722;n+1 &#8226;) is the number of unique words that follows the history e i&#8722;1 i&#8722;n+1 and c X(e i i&#8722;n+1 ) is the count of the n-gram e i i&#8722;n+1 , where c X(&#183;) can represent true counts c T (&#183;) or modified counts c M (&#183;). True counts are used for the higher order n-grams and modified counts for the lower order n-grams. Given a certain n-gram, its modified count consists in the number of different words that precede this n- gram in the training corpus.
Equation (8) corresponds to the probability given by an n-gram language model with an interpolated version of the Kneser-Ney smoothing (Chen and Goodman, 1996).
1 |e| is the length of e, e 0 denotes the begin-of-sentence symbol, e |e|+1 denotes the end-of-sentence symbol, e j i &#8801; ei...ej
&#8226; target sentence-length model (h 2 ) h 2 (e,f) = log(p(|f| | |e|)) = log(&#966; |e| (|f|+0.5)&#8722; &#966; |e| (|f| &#8722;0.5)), where &#966; |e| (&#183;) denotes the cumulative distribution function (cdf) for the normal distribution (the cdf is used here to integrate the normal density function over an interval of length 1). We use a specific normal distribution with mean &#181; |e| and standard deviation &#963; |e| for each possible target sentence length |e|.
&#8226; inverse and direct phrase-based models (h 3 , h 4 ) h 3 (e,a,f) = log( &#8719; K
k=1 p( &#732;f k |&#7869;&#227;k )), where
p( &#732;f k |&#7869;&#227;k ) is defined as follows:
p( &#732;f k |&#7869;&#227;k ) = &#946; &#183; p phr ( &#732;f k |&#7869;&#227;k ) +
(1 &#8722; &#946;).p hmm ( &#732;f k |&#7869;&#227;k ) (9)
In Equation (9), p phr ( &#732;f k |&#7869;&#227;k ) denotes the probability given by a statistical phrase-based dictionary used in regular phrase-based models (see (Koehn et al., 2003) for more details). p hmm ( &#732;f k |&#7869;&#227;k ) is the probability given by an HMM-based (intraphrase) alignment model (see (Vogel et al., 1996)):
| &#732;f|
p hmm ( &#732;f|&#7869;) = &#491; &#8721; &#8719; p( &#732;f j |&#7869; aj ) &#183; p(a j |a j&#8722;1 , |&#7869;|)
a | &#732;f| j=1
(10) The HMM-based alignment model probability is used here for smoothing purposes as described in (Ortiz-Mart&#237;nez et al., 2009).
Analogously h 4 is defined as: h 4 (e,a,f) = log( &#8719; K
k=1 p(&#7869; &#227; k
| &#732;f k ))
&#8226; target phrase-length model (h 5 ) h 5 (e,a,f) = log( &#8719; K
k=1 p(|&#7869; k|)), where p(|&#7869; k |) =
&#948;(1 &#8722; &#948;) |&#7869;k| . h 5 implements a target phrase-length model by means of a geometric distribution with probability of success on each trial &#948;. The use of a geometric distribution penalizes the length of target phrases.
&#8226; source phrase-length model (h 6 ) h 6 (e,a,f) = log( &#8719; K
k=1 p(| &#732;f k | | |&#7869;&#227;k |)),
where p(| &#732;f k | | |&#7869;&#227;k |) = &#948;(1 &#8722; &#948;) abs(| &#732;f k |&#8722;|&#7869;&#227;k |) and abs(&#183;) is the absolute value function. A geometric distribution is used to model this feature (it penalizes the difference between the source and target phrase lengths).
&#8226; distortion model (h 7 ) h 7 (a) = log( &#8719; K
k=1 p(&#227; k|&#227; k&#8722;1 )), where
p(&#227; k |&#227; k&#8722;1 ) = &#948;(1 &#8722; &#948;) abs(b &#227; k
&#8722;l&#227;k&#8722;1 ) , b&#227;k
denotes the beginning position of the source phrase covered by &#227; k and l&#227;k&#8722;1 denotes the last position of the source phrase covered by &#227; k&#8722;1 . A geometric distribution is used to model this feature (it penalizes the reorderings).
The log-linear model, which includes the above described feature functions, is used to generate the suffix e s given the user-validated prefix e p . Specifically, the IMT system generates a partial phrasebased alignment between the user prefix e p and a portion of the source sentence f, and returns the suffix e s as the translation of the remaining portion of f (see (Ortiz-Mart&#237;nez et al., 2009)).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The basic IMT system that we propose uses a loglinear model to generate its translations.</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>According to Equation (7), we introduce a set of seven feature functions (from h 1 to h 7 ):</text>
                  <doc_id>94</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(e i |e i&#8722;1 i&#8722;n+1 ) = max{c X(e i i&#8722;n+1 ) &#8722; D n, 0} c X (e i&#8722;1 i&#8722;n+1 ) +</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>D n c X (e i&#8722;1 i&#8722;n+1 )N 1+(e i&#8722;1 i&#8722;n+1 &#8226;) &#183; p(e i|e i&#8722;1 i&#8722;n+2 ) (8)</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where D n = n,1</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c n,1 +2c n,2</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>is a fixed discount (c n,1 and c n,2 are the number of n-grams with one and two counts respectively), N 1+ (e i&#8722;1 i&#8722;n+1 &#8226;) is the number of unique words that follows the history e i&#8722;1 i&#8722;n+1 and c X(e i i&#8722;n+1 ) is the count of the n-gram e i i&#8722;n+1 , where c X(&#183;) can represent true counts c T (&#183;) or modified counts c M (&#183;).</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>True counts are used for the higher order n-grams and modified counts for the lower order n-grams.</text>
                  <doc_id>101</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Given a certain n-gram, its modified count consists in the number of different words that precede this n- gram in the training corpus.</text>
                  <doc_id>102</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Equation (8) corresponds to the probability given by an n-gram language model with an interpolated version of the Kneser-Ney smoothing (Chen and Goodman, 1996).</text>
                  <doc_id>103</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 |e| is the length of e, e 0 denotes the begin-of-sentence symbol, e |e|+1 denotes the end-of-sentence symbol, e j i &#8801; ei...ej</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; target sentence-length model (h 2 ) h 2 (e,f) = log(p(|f| | |e|)) = log(&#966; |e| (|f|+0.5)&#8722; &#966; |e| (|f| &#8722;0.5)), where &#966; |e| (&#183;) denotes the cumulative distribution function (cdf) for the normal distribution (the cdf is used here to integrate the normal density function over an interval of length 1).</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We use a specific normal distribution with mean &#181; |e| and standard deviation &#963; |e| for each possible target sentence length |e|.</text>
                  <doc_id>106</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; inverse and direct phrase-based models (h 3 , h 4 ) h 3 (e,a,f) = log( &#8719; K</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k=1 p( &#732;f k |&#7869;&#227;k )), where</text>
                  <doc_id>108</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p( &#732;f k |&#7869;&#227;k ) is defined as follows:</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p( &#732;f k |&#7869;&#227;k ) = &#946; &#183; p phr ( &#732;f k |&#7869;&#227;k ) +</text>
                  <doc_id>110</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(1 &#8722; &#946;).p hmm ( &#732;f k |&#7869;&#227;k ) (9)</text>
                  <doc_id>111</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In Equation (9), p phr ( &#732;f k |&#7869;&#227;k ) denotes the probability given by a statistical phrase-based dictionary used in regular phrase-based models (see (Koehn et al., 2003) for more details).</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>p hmm ( &#732;f k |&#7869;&#227;k ) is the probability given by an HMM-based (intraphrase) alignment model (see (Vogel et al., 1996)):</text>
                  <doc_id>113</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>| &#732;f|</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p hmm ( &#732;f|&#7869;) = &#491; &#8721; &#8719; p( &#732;f j |&#7869; aj ) &#183; p(a j |a j&#8722;1 , |&#7869;|)</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>a | &#732;f| j=1</text>
                  <doc_id>116</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(10) The HMM-based alignment model probability is used here for smoothing purposes as described in (Ortiz-Mart&#237;nez et al., 2009).</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Analogously h 4 is defined as: h 4 (e,a,f) = log( &#8719; K</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k=1 p(&#7869; &#227; k</text>
                  <doc_id>119</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>| &#732;f k ))</text>
                  <doc_id>120</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; target phrase-length model (h 5 ) h 5 (e,a,f) = log( &#8719; K</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k=1 p(|&#7869; k|)), where p(|&#7869; k |) =</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#948;(1 &#8722; &#948;) |&#7869;k| .</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>h 5 implements a target phrase-length model by means of a geometric distribution with probability of success on each trial &#948;.</text>
                  <doc_id>124</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The use of a geometric distribution penalizes the length of target phrases.</text>
                  <doc_id>125</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; source phrase-length model (h 6 ) h 6 (e,a,f) = log( &#8719; K</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k=1 p(| &#732;f k | | |&#7869;&#227;k |)),</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where p(| &#732;f k | | |&#7869;&#227;k |) = &#948;(1 &#8722; &#948;) abs(| &#732;f k |&#8722;|&#7869;&#227;k |) and abs(&#183;) is the absolute value function.</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A geometric distribution is used to model this feature (it penalizes the difference between the source and target phrase lengths).</text>
                  <doc_id>129</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; distortion model (h 7 ) h 7 (a) = log( &#8719; K</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k=1 p(&#227; k|&#227; k&#8722;1 )), where</text>
                  <doc_id>131</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(&#227; k |&#227; k&#8722;1 ) = &#948;(1 &#8722; &#948;) abs(b &#227; k</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8722;l&#227;k&#8722;1 ) , b&#227;k</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>denotes the beginning position of the source phrase covered by &#227; k and l&#227;k&#8722;1 denotes the last position of the source phrase covered by &#227; k&#8722;1 .</text>
                  <doc_id>134</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A geometric distribution is used to model this feature (it penalizes the reorderings).</text>
                  <doc_id>135</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The log-linear model, which includes the above described feature functions, is used to generate the suffix e s given the user-validated prefix e p .</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Specifically, the IMT system generates a partial phrasebased alignment between the user prefix e p and a portion of the source sentence f, and returns the suffix e s as the translation of the remaining portion of f (see (Ortiz-Mart&#237;nez et al., 2009)).</text>
                  <doc_id>137</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Extending the IMT system from user feedback</title>
            <text>After translating a source sentence f, a new sentence pair (f,e) is available to feed the IMT system (see Figure 1). In this section we describe how the log-linear model described in section 4.1 is updated given the new sentence pair. To do this, a set of sufficient statistics that can be incrementally updated is maintained for each feature function h i (&#183;). A sufficient statistic for a statistical model is a statistic that captures all the information that is relevant to estimate this model.
Regarding feature function h 1 and according to equation (8), we need to maintain the following data: c k,1 and c k,2 given any order k, N 1+ (&#183;), and c X (&#183;) (see section 4.1 for the meaning of each symbol). Given a new sentence e, and for each k-gram e i i&#8722;k+1 of e where 1 &#8804; k &#8804; n and 1 &#8804; i &#8804; |e|+1, we modify the set of sufficient statistics as it is shown in Algorithm 1. The algorithm checks the changes in the counts of the k-grams to update the set of sufficient statistics. Sufficient statistics for D k are updated following the auxiliar procedure shown in Algorithm 2.
Feature function h 2 requires the incremental calculation of the mean &#181; |e| and the standard deviation &#963; |e| of the normal distribution associated to a target sentence length |e|. For this purpose the procedure described in (Knuth, 1981) can be used. In this procedure, two quantities are maintained for each normal distribution: &#181; |e| and S |e| . Given a new sentence
input : n (higher order), e i i&#8722;k+1 (k-gram), S = {&#8704;j(c j,1 ,c j,2 ),N 1+ (&#183;),c X (&#183;)} (current set of sufficient statistics) output : S (updated set of sufficient statistics) begin
if c T (e i i&#8722;k+1 ) = 0 then if k &#8722; 1 &#8805; 1 then updD(S,k-1,c M (e i&#8722;1 i&#8722;k+2 ),c M(e i&#8722;1 if c M (e i&#8722;1 i&#8722;k+2 ) = 0 then
N 1+ (e i&#8722;1 i&#8722;k+2 ) = N 1+(e i&#8722;1
i&#8722;k+2 ) + 1
c M (e i&#8722;1 i&#8722;k+2 ) = c M(e i&#8722;1 i&#8722;k+2 ) + 1 c M (e i i&#8722;k+2 ) = c M(e i i&#8722;k+2 ) + 1
if k = n then N 1+ (e i&#8722;1 i&#8722;k+1 ) = N 1+(e i&#8722;1 i&#8722;k+1 ) + 1
i&#8722;k+2 )+1)
if k = n then updD(S,k,c T (e i i&#8722;k+1 ),c T(e i i&#8722;k+1 ) + 1)
c T (e i&#8722;1 i&#8722;k+1 )=c T(e i&#8722;1 i&#8722;k+1 ) + 1
c T (e i i&#8722;k+1 )=c T(e i i&#8722;k+1 ) + 1 end
Algorithm 1: Pseudocode for updating the sufficient statistics of a given k-gram
input : S (current set of sufficient statistics),k (order), c (current count), c &#8242; (new count) output : (c k,1 ,c k,2 ) (updated sufficient statistics) begin
if c = 0 then if c &#8242; = 1 then c k,1 = c k,1 + 1 if c &#8242; = 2 then c k,2 = c k,2 + 1
if c = 1 then c k,1 = c k,1 &#8722; 1 if c &#8242; = 2 then c k,2 = c k,2 + 1
if c = 2 then c k,2 = c k,2 &#8722; 1 end
Algorithm 2: Pseudocode for the updD procedure
pair (f,e), the two quantities are updated using a recurrence relation:
&#181; |e| = &#181; &#8242; |e| + (|f| &#8722; &#181;&#8242; |e| )/c(|e|) (11)
S |e| = S &#8242; |e| + (|f| &#8722; &#181;&#8242; |e| )(|f| &#8722; &#181; |e|) (12)
where c(|e|) is the count of the number of sentences of length |e| that have been seen so far, and &#181; &#8242; |e| and
S &#8242; |e| are the quantities previously stored (&#181; |e| is initialized to the source sentence length of the first sample and S |e| is initialized to zero). Finally, the standard deviation &#8730; can be obtained from S as follows: &#963; |e| = S |e| /(c(|e|) &#8722; 1).
Feature functions h 3 and h 4 implement inverse and direct smoothed phrase-based models respectively. Since phrase-based models are symmetric models, only an inverse phrase-based model is maintained (direct probabilities can be efficiently obtained using appropriate data structures, see (Ortiz- Mart&#237;nez et al., 2008)). The inverse phrase model probabilities are estimated from the phrase counts:
p( &#732;f|&#7869;) =
c( &#732;f,&#7869;) &#8721;
&#732;f &#8242; c( &#732;f &#8242; , &#7869;)
(13)
According to Equation (13), the set of sufficient statistics to be stored for the inverse phrase model &#8721;
consists of a set of phrase counts (c( &#732;f,&#7869;) and
&#732;f
c( &#732;f &#8242; , &#7869;) must be stored separately). Given a &#8242; new sentence pair (f, e), the standard phrase-based model estimation method uses a word alignment matrix between f and e to extract the set of phrase pairs that are consistent with the word alignment matrix (see (Koehn et al., 2003) for more details). Once the consistent phrase pairs have been extracted, the phrase counts are updated. The word alignment matrices required for the extraction of phrase pairs are generated by means of the HMM-based models used in the feature functions h 3 and h 4 . Inverse and direct HMM-based models are used here for two purposes: to smooth the phrase-based models via linear interpolation and to generate word alignment matrices. The weights of the interpolation can be estimated from a development corpus. Equation (10) shows the expression of the probability given by an inverse HMM-based model. The probability includes lexical probabilities p(f j |e i ) and alignment probabilities p(a j |a j&#8722;1 , l). Since the alignment in the HMM-based model is determined by a hidden variable, the EM algorithm is required to estimate the parameters of the model (see (Och and Ney, 2003)). However, the standard EM algorithm is not appropriate to incrementally extend our HMM-based models because it is designed to work in batch training scenarios. To solve this problem, we apply the incremental view of the EM algorithm described in (Neal and Hinton, 1998). According to (Och and Ney, 2003), the lexical probability for a
pair of words is given by the expression:
p(f|e) = c(f|e) &#8721;
f &#8242; c(f &#8242; |e)
(14)
where c(f|e) is the expected number of times that the word e is aligned to the word f. The alignment probability is defined in a similar way:
p(a j |a j&#8722;1 , l) = c(a j|a j&#8722;1 , l) &#8721;
a &#8242; c(a&#8242; j j |a j&#8722;1, l)
(15)
where c(a j |a j&#8722;1 , l) denotes the expected number of times that the alignment a j has been seen after the previous alignment a j&#8722;1 given a source sentence composed of l words.
Given the equations (14) and (15), the set of sufficient statistics for the inverse HMM-based model consists of a set of expected counts (numerator and denominator values are stored separately). Given a new sentence pair (f,e), we execute a new iteration of the incremental EM algorithm on the new sample and collect the contributions to the expected counts.
The parameters of the direct HMM-based model are estimated analogously to those of the inverse HMM-based model. Once the direct and the inverse HMM-based model parameters have been modified due to the presentation of a new sentence pair to the IMT system, both models are used to obtain word alignments for the new sentence pair. The resulting direct and inverse word alignment matrices are combined by means of the symmetrization alignment operation (Och and Ney, 2003) before extracting the set of consistent phrase pairs.
HMM-based alignment models are used here because, according to (Och and Ney, 2003) and (Toutanova et al., 2002), they outperform IBM 1 to IBM 4 alignment models while still allowing the exact calculation of the likelihood for a given sentence pair. The &#948; parameters of the geometric distributions associated to the feature functions h 5 , h 6 and h 7 are left fixed. Because of this, there are no sufficient statistics to store for these feature functions.
Finally, the weights of the log-linear combination are not modified due to the presentation of a new sentence pair to the system. These weights can be adjusted off-line by means of a development corpus and well-known optimization techniques.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>After translating a source sentence f, a new sentence pair (f,e) is available to feed the IMT system (see Figure 1).</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this section we describe how the log-linear model described in section 4.1 is updated given the new sentence pair.</text>
                  <doc_id>139</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>To do this, a set of sufficient statistics that can be incrementally updated is maintained for each feature function h i (&#183;).</text>
                  <doc_id>140</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>A sufficient statistic for a statistical model is a statistic that captures all the information that is relevant to estimate this model.</text>
                  <doc_id>141</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Regarding feature function h 1 and according to equation (8), we need to maintain the following data: c k,1 and c k,2 given any order k, N 1+ (&#183;), and c X (&#183;) (see section 4.1 for the meaning of each symbol).</text>
                  <doc_id>142</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Given a new sentence e, and for each k-gram e i i&#8722;k+1 of e where 1 &#8804; k &#8804; n and 1 &#8804; i &#8804; |e|+1, we modify the set of sufficient statistics as it is shown in Algorithm 1.</text>
                  <doc_id>143</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The algorithm checks the changes in the counts of the k-grams to update the set of sufficient statistics.</text>
                  <doc_id>144</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Sufficient statistics for D k are updated following the auxiliar procedure shown in Algorithm 2.</text>
                  <doc_id>145</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Feature function h 2 requires the incremental calculation of the mean &#181; |e| and the standard deviation &#963; |e| of the normal distribution associated to a target sentence length |e|.</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For this purpose the procedure described in (Knuth, 1981) can be used.</text>
                  <doc_id>147</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In this procedure, two quantities are maintained for each normal distribution: &#181; |e| and S |e| .</text>
                  <doc_id>148</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Given a new sentence</text>
                  <doc_id>149</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>input : n (higher order), e i i&#8722;k+1 (k-gram), S = {&#8704;j(c j,1 ,c j,2 ),N 1+ (&#183;),c X (&#183;)} (current set of sufficient statistics) output : S (updated set of sufficient statistics) begin</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>if c T (e i i&#8722;k+1 ) = 0 then if k &#8722; 1 &#8805; 1 then updD(S,k-1,c M (e i&#8722;1 i&#8722;k+2 ),c M(e i&#8722;1 if c M (e i&#8722;1 i&#8722;k+2 ) = 0 then</text>
                  <doc_id>151</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>N 1+ (e i&#8722;1 i&#8722;k+2 ) = N 1+(e i&#8722;1</text>
                  <doc_id>152</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i&#8722;k+2 ) + 1</text>
                  <doc_id>153</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c M (e i&#8722;1 i&#8722;k+2 ) = c M(e i&#8722;1 i&#8722;k+2 ) + 1 c M (e i i&#8722;k+2 ) = c M(e i i&#8722;k+2 ) + 1</text>
                  <doc_id>154</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>if k = n then N 1+ (e i&#8722;1 i&#8722;k+1 ) = N 1+(e i&#8722;1 i&#8722;k+1 ) + 1</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i&#8722;k+2 )+1)</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>if k = n then updD(S,k,c T (e i i&#8722;k+1 ),c T(e i i&#8722;k+1 ) + 1)</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c T (e i&#8722;1 i&#8722;k+1 )=c T(e i&#8722;1 i&#8722;k+1 ) + 1</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c T (e i i&#8722;k+1 )=c T(e i i&#8722;k+1 ) + 1 end</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Algorithm 1: Pseudocode for updating the sufficient statistics of a given k-gram</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>input : S (current set of sufficient statistics),k (order), c (current count), c &#8242; (new count) output : (c k,1 ,c k,2 ) (updated sufficient statistics) begin</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>if c = 0 then if c &#8242; = 1 then c k,1 = c k,1 + 1 if c &#8242; = 2 then c k,2 = c k,2 + 1</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>if c = 1 then c k,1 = c k,1 &#8722; 1 if c &#8242; = 2 then c k,2 = c k,2 + 1</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>if c = 2 then c k,2 = c k,2 &#8722; 1 end</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Algorithm 2: Pseudocode for the updD procedure</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>pair (f,e), the two quantities are updated using a recurrence relation:</text>
                  <doc_id>166</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#181; |e| = &#181; &#8242; |e| + (|f| &#8722; &#181;&#8242; |e| )/c(|e|) (11)</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>S |e| = S &#8242; |e| + (|f| &#8722; &#181;&#8242; |e| )(|f| &#8722; &#181; |e|) (12)</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where c(|e|) is the count of the number of sentences of length |e| that have been seen so far, and &#181; &#8242; |e| and</text>
                  <doc_id>169</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>S &#8242; |e| are the quantities previously stored (&#181; |e| is initialized to the source sentence length of the first sample and S |e| is initialized to zero).</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, the standard deviation &#8730; can be obtained from S as follows: &#963; |e| = S |e| /(c(|e|) &#8722; 1).</text>
                  <doc_id>171</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Feature functions h 3 and h 4 implement inverse and direct smoothed phrase-based models respectively.</text>
                  <doc_id>172</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Since phrase-based models are symmetric models, only an inverse phrase-based model is maintained (direct probabilities can be efficiently obtained using appropriate data structures, see (Ortiz- Mart&#237;nez et al., 2008)).</text>
                  <doc_id>173</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The inverse phrase model probabilities are estimated from the phrase counts:</text>
                  <doc_id>174</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p( &#732;f|&#7869;) =</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c( &#732;f,&#7869;) &#8721;</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#732;f &#8242; c( &#732;f &#8242; , &#7869;)</text>
                  <doc_id>177</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(13)</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>According to Equation (13), the set of sufficient statistics to be stored for the inverse phrase model &#8721;</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>consists of a set of phrase counts (c( &#732;f,&#7869;) and</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#732;f</text>
                  <doc_id>181</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c( &#732;f &#8242; , &#7869;) must be stored separately).</text>
                  <doc_id>182</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Given a &#8242; new sentence pair (f, e), the standard phrase-based model estimation method uses a word alignment matrix between f and e to extract the set of phrase pairs that are consistent with the word alignment matrix (see (Koehn et al., 2003) for more details).</text>
                  <doc_id>183</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Once the consistent phrase pairs have been extracted, the phrase counts are updated.</text>
                  <doc_id>184</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The word alignment matrices required for the extraction of phrase pairs are generated by means of the HMM-based models used in the feature functions h 3 and h 4 .</text>
                  <doc_id>185</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Inverse and direct HMM-based models are used here for two purposes: to smooth the phrase-based models via linear interpolation and to generate word alignment matrices.</text>
                  <doc_id>186</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The weights of the interpolation can be estimated from a development corpus.</text>
                  <doc_id>187</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Equation (10) shows the expression of the probability given by an inverse HMM-based model.</text>
                  <doc_id>188</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>The probability includes lexical probabilities p(f j |e i ) and alignment probabilities p(a j |a j&#8722;1 , l).</text>
                  <doc_id>189</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>Since the alignment in the HMM-based model is determined by a hidden variable, the EM algorithm is required to estimate the parameters of the model (see (Och and Ney, 2003)).</text>
                  <doc_id>190</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>However, the standard EM algorithm is not appropriate to incrementally extend our HMM-based models because it is designed to work in batch training scenarios.</text>
                  <doc_id>191</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>To solve this problem, we apply the incremental view of the EM algorithm described in (Neal and Hinton, 1998).</text>
                  <doc_id>192</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>According to (Och and Ney, 2003), the lexical probability for a</text>
                  <doc_id>193</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>pair of words is given by the expression:</text>
                  <doc_id>194</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(f|e) = c(f|e) &#8721;</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f &#8242; c(f &#8242; |e)</text>
                  <doc_id>196</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(14)</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where c(f|e) is the expected number of times that the word e is aligned to the word f. The alignment probability is defined in a similar way:</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(a j |a j&#8722;1 , l) = c(a j|a j&#8722;1 , l) &#8721;</text>
                  <doc_id>199</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>a &#8242; c(a&#8242; j j |a j&#8722;1, l)</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(15)</text>
                  <doc_id>201</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where c(a j |a j&#8722;1 , l) denotes the expected number of times that the alignment a j has been seen after the previous alignment a j&#8722;1 given a source sentence composed of l words.</text>
                  <doc_id>202</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Given the equations (14) and (15), the set of sufficient statistics for the inverse HMM-based model consists of a set of expected counts (numerator and denominator values are stored separately).</text>
                  <doc_id>203</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Given a new sentence pair (f,e), we execute a new iteration of the incremental EM algorithm on the new sample and collect the contributions to the expected counts.</text>
                  <doc_id>204</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The parameters of the direct HMM-based model are estimated analogously to those of the inverse HMM-based model.</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Once the direct and the inverse HMM-based model parameters have been modified due to the presentation of a new sentence pair to the IMT system, both models are used to obtain word alignments for the new sentence pair.</text>
                  <doc_id>206</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The resulting direct and inverse word alignment matrices are combined by means of the symmetrization alignment operation (Och and Ney, 2003) before extracting the set of consistent phrase pairs.</text>
                  <doc_id>207</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>HMM-based alignment models are used here because, according to (Och and Ney, 2003) and (Toutanova et al., 2002), they outperform IBM 1 to IBM 4 alignment models while still allowing the exact calculation of the likelihood for a given sentence pair.</text>
                  <doc_id>208</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The &#948; parameters of the geometric distributions associated to the feature functions h 5 , h 6 and h 7 are left fixed.</text>
                  <doc_id>209</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Because of this, there are no sufficient statistics to store for these feature functions.</text>
                  <doc_id>210</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Finally, the weights of the log-linear combination are not modified due to the presentation of a new sentence pair to the system.</text>
                  <doc_id>211</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These weights can be adjusted off-line by means of a development corpus and well-known optimization techniques.</text>
                  <doc_id>212</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>5 Experiments</title>
        <text>This section describes the experiments that we carried out to test our online IMT system.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This section describes the experiments that we carried out to test our online IMT system.</text>
              <doc_id>213</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Experimental setup The experiments were performed using the XE-</title>
            <text>ROX XRCE corpus (SchlumbergerSema S.A. et
al., 2001), which consists of translations of Xerox printer manuals involving three different pairs of languages: French-English, Spanish-English, and German-English. The main features of these corpora are shown in Table 1. Partitions into training, development and test were performed. This corpus is used here because it has been extensively used in the literature on IMT to report results. IMT experiments were carried out from English to the other three languages.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>ROX XRCE corpus (SchlumbergerSema S.A.</text>
                  <doc_id>214</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>et</text>
                  <doc_id>215</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>al., 2001), which consists of translations of Xerox printer manuals involving three different pairs of languages: French-English, Spanish-English, and German-English.</text>
                  <doc_id>216</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The main features of these corpora are shown in Table 1.</text>
                  <doc_id>217</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Partitions into training, development and test were performed.</text>
                  <doc_id>218</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This corpus is used here because it has been extensively used in the literature on IMT to report results.</text>
                  <doc_id>219</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>IMT experiments were carried out from English to the other three languages.</text>
                  <doc_id>220</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Assessment criteria</title>
            <text>The evaluation of the techniques presented in this paper were carried out using the Key-stroke and mouse-action ratio (KSMR) measure (Barrachina et al., 2009). This is calculated as the number of keystrokes plus the number of mouse movements plus one more count per sentence (aimed at simulating the user action needed to accept the final translation), the sum of which is divided by the total number of reference characters. In addition to this, we also used the well-known BLEU score (Papineni et al., 2001) to measure the translation quality of the first translation hypothesis produced by the IMT system for each source sentence (which is automatically generated without user intervention).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The evaluation of the techniques presented in this paper were carried out using the Key-stroke and mouse-action ratio (KSMR) measure (Barrachina et al., 2009).</text>
                  <doc_id>221</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This is calculated as the number of keystrokes plus the number of mouse movements plus one more count per sentence (aimed at simulating the user action needed to accept the final translation), the sum of which is divided by the total number of reference characters.</text>
                  <doc_id>222</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In addition to this, we also used the well-known BLEU score (Papineni et al., 2001) to measure the translation quality of the first translation hypothesis produced by the IMT system for each source sentence (which is automatically generated without user intervention).</text>
                  <doc_id>223</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>5.3 Online IMT results</title>
            <text>To test the techniques proposed in this work, we carried out experiments in two different scenarios. In the first one, the first 10 000 sentences extracted from the training corpora were interactively translated by means of an IMT system without any preexistent model stored in memory. Each time a new sentence pair was validated, it was used to incrementally train the system. Figures 3a, 3b and 3c show the evolution of the KSMR with respect to the number of sentence pairs processed by the IMT system; the results correspond to the translation from English to Spanish, French and German, respectively. In addi-
Train
Dev.
Test
tion, for each language pair we interactively translated the original portion of the training corpus and the same portion of the original corpus after being randomly shuffled.
As these figures show, the results clearly demonstrate that the IMT system is able to learn from scratch. The results were similar for the three languages. It is also worthy of note that the obtained results were better in all cases for the original corpora than for the shuffled ones. This is because, in the original corpora, similar sentences appear more or less contiguosly (due to the organization of the contents of the printer manuals). This circumstance increases the accuracy of the online learning, since with the original corpora the number of lateral effects ocurred between the translation of similar sentences is decreased. The online learning of a new sentence pair produces a lateral effect when the changes in the probability given by the models not only affect the newly trained sentence pair but also other sentence pairs. A lateral effect can cause that the system generates a wrong translation for a given source sentence due to undesired changes in the statistical models.
The accuracy were worse for shuffled corpora, since shuffling increases the number of lateral effects that may occur between the translation of similar sentences (because they no longer appear contiguously). A good way to compare the quality of different online IMT systems is to determine their robustness in relation to sentence ordering. However, it can generally be expected that the sentences to be translated in an interactive translation session will be in a non-random order.
Alternatively, we carried out experiments in a different learning scenario. Specifically, the XEROX
KSMR KSMR KSMR
30 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
#Sentences (a) English-Spanish
original shuffled
40 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
#Sentences (b) English-French
original shuffled
40 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
#Sentences
original shuffled
(c) English-German
test corpora were interactively translated from the English language to the other three languages, comparing the performance of a batch IMT system with
that of an online IMT system. The batch IMT system is a conventional IMT system which is not able to take advantage of user feedback after each translation while the online IMT system uses the new sentence pairs provided by the user to revise the statistical models. Both systems were initialized with a log-linear model trained in batch mode by means of the XEROX training corpora. The weights of the log-linear combination were adjusted for the development corpora by means of the downhill-simplex algorithm. Table 2 shows the obtained results. The table shows the BLEU score and the KSMR for the batch and the online IMT systems (95% confidence intervals are shown). The BLEU score was calculated from the first translation hypothesis produced by the IMT system for each source sentence. The table also shows the average online learning time (LT) for each new sample presented to the system 2 . All the improvements obtained with the online IMT system were statistically significant. Also, the average learning times clearly allow the system to be used in a real-time scenario.
Finally, in Table 3 a comparison of the KSMR results obtained by the online IMT system with stateof-the-art IMT systems is reported (95% confidence intervals are shown). We compared our system with those presented in (Barrachina et al., 2009): the alignment templates (AT), the stochastic finite-state transducer (SFST), and the phrase-based (PB) approaches to IMT. The results were obtained using the same Xerox training and test sets (see Table 1) for the four different IMT systems. Our system outperformed the results obtained by these systems.
2 All the experiments were executed on a PC with a 2.40 Ghz
Intel Xeon processor with 1GB of memory.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>To test the techniques proposed in this work, we carried out experiments in two different scenarios.</text>
                  <doc_id>224</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the first one, the first 10 000 sentences extracted from the training corpora were interactively translated by means of an IMT system without any preexistent model stored in memory.</text>
                  <doc_id>225</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Each time a new sentence pair was validated, it was used to incrementally train the system.</text>
                  <doc_id>226</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Figures 3a, 3b and 3c show the evolution of the KSMR with respect to the number of sentence pairs processed by the IMT system; the results correspond to the translation from English to Spanish, French and German, respectively.</text>
                  <doc_id>227</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In addi-</text>
                  <doc_id>228</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Train</text>
                  <doc_id>229</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Dev.</text>
                  <doc_id>230</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Test</text>
                  <doc_id>231</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>tion, for each language pair we interactively translated the original portion of the training corpus and the same portion of the original corpus after being randomly shuffled.</text>
                  <doc_id>232</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As these figures show, the results clearly demonstrate that the IMT system is able to learn from scratch.</text>
                  <doc_id>233</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The results were similar for the three languages.</text>
                  <doc_id>234</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It is also worthy of note that the obtained results were better in all cases for the original corpora than for the shuffled ones.</text>
                  <doc_id>235</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This is because, in the original corpora, similar sentences appear more or less contiguosly (due to the organization of the contents of the printer manuals).</text>
                  <doc_id>236</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This circumstance increases the accuracy of the online learning, since with the original corpora the number of lateral effects ocurred between the translation of similar sentences is decreased.</text>
                  <doc_id>237</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The online learning of a new sentence pair produces a lateral effect when the changes in the probability given by the models not only affect the newly trained sentence pair but also other sentence pairs.</text>
                  <doc_id>238</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>A lateral effect can cause that the system generates a wrong translation for a given source sentence due to undesired changes in the statistical models.</text>
                  <doc_id>239</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The accuracy were worse for shuffled corpora, since shuffling increases the number of lateral effects that may occur between the translation of similar sentences (because they no longer appear contiguously).</text>
                  <doc_id>240</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A good way to compare the quality of different online IMT systems is to determine their robustness in relation to sentence ordering.</text>
                  <doc_id>241</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, it can generally be expected that the sentences to be translated in an interactive translation session will be in a non-random order.</text>
                  <doc_id>242</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Alternatively, we carried out experiments in a different learning scenario.</text>
                  <doc_id>243</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Specifically, the XEROX</text>
                  <doc_id>244</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>KSMR KSMR KSMR</text>
                  <doc_id>245</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>30 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000</text>
                  <doc_id>246</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>#Sentences (a) English-Spanish</text>
                  <doc_id>247</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>original shuffled</text>
                  <doc_id>248</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>40 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000</text>
                  <doc_id>249</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>#Sentences (b) English-French</text>
                  <doc_id>250</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>original shuffled</text>
                  <doc_id>251</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>40 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000</text>
                  <doc_id>252</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>#Sentences</text>
                  <doc_id>253</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>original shuffled</text>
                  <doc_id>254</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(c) English-German</text>
                  <doc_id>255</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>test corpora were interactively translated from the English language to the other three languages, comparing the performance of a batch IMT system with</text>
                  <doc_id>256</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>that of an online IMT system.</text>
                  <doc_id>257</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The batch IMT system is a conventional IMT system which is not able to take advantage of user feedback after each translation while the online IMT system uses the new sentence pairs provided by the user to revise the statistical models.</text>
                  <doc_id>258</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Both systems were initialized with a log-linear model trained in batch mode by means of the XEROX training corpora.</text>
                  <doc_id>259</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The weights of the log-linear combination were adjusted for the development corpora by means of the downhill-simplex algorithm.</text>
                  <doc_id>260</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Table 2 shows the obtained results.</text>
                  <doc_id>261</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The table shows the BLEU score and the KSMR for the batch and the online IMT systems (95% confidence intervals are shown).</text>
                  <doc_id>262</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The BLEU score was calculated from the first translation hypothesis produced by the IMT system for each source sentence.</text>
                  <doc_id>263</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>The table also shows the average online learning time (LT) for each new sample presented to the system 2 .</text>
                  <doc_id>264</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>All the improvements obtained with the online IMT system were statistically significant.</text>
                  <doc_id>265</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>Also, the average learning times clearly allow the system to be used in a real-time scenario.</text>
                  <doc_id>266</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Finally, in Table 3 a comparison of the KSMR results obtained by the online IMT system with stateof-the-art IMT systems is reported (95% confidence intervals are shown).</text>
                  <doc_id>267</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We compared our system with those presented in (Barrachina et al., 2009): the alignment templates (AT), the stochastic finite-state transducer (SFST), and the phrase-based (PB) approaches to IMT.</text>
                  <doc_id>268</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The results were obtained using the same Xerox training and test sets (see Table 1) for the four different IMT systems.</text>
                  <doc_id>269</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Our system outperformed the results obtained by these systems.</text>
                  <doc_id>270</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 All the experiments were executed on a PC with a 2.40 Ghz</text>
                  <doc_id>271</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Intel Xeon processor with 1GB of memory.</text>
                  <doc_id>272</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>6 Conclusions</title>
        <text>We have presented an online IMT system. The proposed system is able to incrementally extend the statistical models involved in the translation process, breaking technical limitations encountered in other works. Empirical results show that our techniques allow the IMT system to learn from scratch or from previously estimated models. One key aspect of the proposed system is the use of HMM-based alignment models trained by means of the incremental EM algorithm. The incremental adjustment of the weights of the log-linear models and other parameters have not been tackled here. For the future we plan to incorporate this functionality into our IMT system. The incremental techniques proposed here can also be exploited to extend SMT systems (in fact, our proposed IMT system is based on an incrementally updateable SMT system). For the near future we plan to study possible aplications of our techniques in a fully automatic translation scenario. Finally, it is worthy of note that the main ideas presented here can be used in other interactive applications such as Computer Assisted Speech Transcription, Interactive Image Retrieval, etc (see (Vidal et al., 2007) for more information). In conclusion, we think that the online learning techniques proposed here can be the starting point for a new generation of interactive pattern recognition systems that are able to take advantage of user feedback.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have presented an online IMT system.</text>
              <doc_id>273</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The proposed system is able to incrementally extend the statistical models involved in the translation process, breaking technical limitations encountered in other works.</text>
              <doc_id>274</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Empirical results show that our techniques allow the IMT system to learn from scratch or from previously estimated models.</text>
              <doc_id>275</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>One key aspect of the proposed system is the use of HMM-based alignment models trained by means of the incremental EM algorithm.</text>
              <doc_id>276</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The incremental adjustment of the weights of the log-linear models and other parameters have not been tackled here.</text>
              <doc_id>277</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>For the future we plan to incorporate this functionality into our IMT system.</text>
              <doc_id>278</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The incremental techniques proposed here can also be exploited to extend SMT systems (in fact, our proposed IMT system is based on an incrementally updateable SMT system).</text>
              <doc_id>279</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>For the near future we plan to study possible aplications of our techniques in a fully automatic translation scenario.</text>
              <doc_id>280</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Finally, it is worthy of note that the main ideas presented here can be used in other interactive applications such as Computer Assisted Speech Transcription, Interactive Image Retrieval, etc (see (Vidal et al., 2007) for more information).</text>
              <doc_id>281</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>In conclusion, we think that the online learning techniques proposed here can be the starting point for a new generation of interactive pattern recognition systems that are able to take advantage of user feedback.</text>
              <doc_id>282</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>Acknowledgments</title>
        <text>Work supported by the EC (FEDER/FSE), the Spanish Government (MEC, MICINN, MITyC, MAEC, &#8221;Plan E&#8221;, under grants MIPRCV &#8221;Consolider Ingenio 2010&#8221; CSD2007-00018, iTrans2 TIN2009-14511, erudito.com TSI-020110-2009- 439), the Generalitat Valenciana (grant Prometeo/2009/014), the Univ. Polit&#233;cnica de Valencia (grant 20091027) and the Spanish JCCM (grant PBI08-0210-7127).
References
A. Arun and P. Koehn. 2007. Online learning methods for discriminative training of phrase based statistical machine translation. In Proc. of the MT Summit XI, pages 15&#8211;20, Copenhagen, Denmark, September.
S. Barrachina, O. Bender, F. Casacuberta, J. Civera, E. Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Tom&#225;s, and E. Vidal. 2009. Statistical approaches to computer-assisted translation. Computational Linguistics, 35(1):3&#8211;28.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263&#8211;311.
N. Cesa-Bianchi, G. Reverberi, and S. Szedmak. 2008. Online learning algorithms for computer-assisted translation. Deliverable D4.2, SMART: Stat. Multilingual Analysis for Retrieval and Translation, Mar.
S.F. Chen and J. Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proc. of the ACL, pages 310&#8211;318, San Francisco.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online largemargin training of syntactic and structural translation features. In Proc. of EMNLP. George Foster, Pierre Isabelle, and Pierre Plamondon.
1997. Target-text mediated interactive machine translation. Machine Translation, 12(1):175&#8211;194. P. Isabelle and K. Church. 1997. Special issue on
new tools for human translators. Machine Translation, 12(1&#8211;2).
D.E. Knuth. 1981. Seminumerical Algorithms, volume 2 of The Art of Computer Programming. Addison- Wesley, Massachusetts, 2nd edition. P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of the HLT/NAACL, pages 48&#8211;54, Edmonton, Canada, May.
P. Langlais, G. Lapalme, and M. Loranger. 2002.
Transtype: Development-evaluation cycles to boost translator&#8217;s productivity. Machine Translation, 15(4):77&#8211;98. P. Liang, A. Bouchard-C&#244;t&#233;, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to machine translation. In Proc. of the 44th ACL, pages 761&#8211; 768, Morristown, NJ, USA. R.M. Neal and G.E. Hinton. 1998. A view of the EM
algorithm that justifies incremental, sparse, and other variants. In Proceedings of the NATO-ASI on Learning in graphical models, pages 355&#8211;368, Norwell, MA, USA. L. Nepveu, G. Lapalme, P. Langlais, and G. Foster. 2004.
Adaptive language and translation models for interactive machine translation. In Proc. of EMNLP, pages 190&#8211;197, Barcelona, Spain, July.
Franz Josef Och and Hermann Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In Proc. of the 40th ACL, pages 295&#8211;302, Philadelphia, PA, July. Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19&#8211;51, March. D. Ortiz-Mart&#237;nez, I. Garc&#237;a-Varea, and Casacuberta F.
2008. The scaling problem in the pattern recognition approach to machine translation. Pattern Recognition Letters, 29:1145&#8211;1153. Daniel Ortiz-Mart&#237;nez, Ismael Garc&#237;a-Varea, and Francisco Casacuberta. 2009. Interactive machine translation based on partial statistical phrase-based alignments. In Proc. of RANLP, Borovets, Bulgaria, sep. Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY, September. SchlumbergerSema S.A., ITI Valencia, RWTH Aachen, RALI Montreal, Celer Soluciones, Soci&#233;t&#233; Gamma, and XRCE. 2001. TT2. TransType2 - computer assisted translation. Project Tech. Rep. Kristina Toutanova, H. Tolga Ilhan, and Christopher
Manning. 2002. Extensions to hmm-based statistical word alignment models. In Proc. of EMNLP. E. Vidal, L. Rodr&#237;guez, F. Casacuberta, and I. Garc&#237;a-
Varea. 2007. Interactive pattern recognition. In Proc. of the 4th MLMI, pages 60&#8211;71. Brno, Czech Republic, 28-30 June. Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical translation. In Proc. of COLING, pages 836&#8211;841, Copenhagen, Denmark, August. T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical machine translation. In Proc. of EMNLP and CoNLL, pages 764&#8211;733, Prage, Czeck Republic.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Work supported by the EC (FEDER/FSE), the Spanish Government (MEC, MICINN, MITyC, MAEC, &#8221;Plan E&#8221;, under grants MIPRCV &#8221;Consolider Ingenio 2010&#8221; CSD2007-00018, iTrans2 TIN2009-14511, erudito.com TSI-020110-2009- 439), the Generalitat Valenciana (grant Prometeo/2009/014), the Univ.</text>
              <doc_id>283</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Polit&#233;cnica de Valencia (grant 20091027) and the Spanish JCCM (grant PBI08-0210-7127).</text>
              <doc_id>284</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>References</text>
              <doc_id>285</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A. Arun and P. Koehn.</text>
              <doc_id>286</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>287</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Online learning methods for discriminative training of phrase based statistical machine translation.</text>
              <doc_id>288</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the MT Summit XI, pages 15&#8211;20, Copenhagen, Denmark, September.</text>
              <doc_id>289</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S. Barrachina, O. Bender, F. Casacuberta, J. Civera, E. Cubel, S. Khadivi, A. Lagarda, H. Ney, J. Tom&#225;s, and E. Vidal.</text>
              <doc_id>290</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>291</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Statistical approaches to computer-assisted translation.</text>
              <doc_id>292</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 35(1):3&#8211;28.</text>
              <doc_id>293</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and R.</text>
              <doc_id>294</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>L. Mercer.</text>
              <doc_id>295</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>1993.</text>
              <doc_id>296</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The mathematics of statistical machine translation: Parameter estimation.</text>
              <doc_id>297</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 19(2):263&#8211;311.</text>
              <doc_id>298</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>N. Cesa-Bianchi, G. Reverberi, and S. Szedmak.</text>
              <doc_id>299</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>300</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Online learning algorithms for computer-assisted translation.</text>
              <doc_id>301</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Deliverable D4.2, SMART: Stat.</text>
              <doc_id>302</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Multilingual Analysis for Retrieval and Translation, Mar.</text>
              <doc_id>303</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S.F.</text>
              <doc_id>304</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Chen and J. Goodman.</text>
              <doc_id>305</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>1996.</text>
              <doc_id>306</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>An empirical study of smoothing techniques for language modeling.</text>
              <doc_id>307</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the ACL, pages 310&#8211;318, San Francisco.</text>
              <doc_id>308</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>D. Chiang, Y. Marton, and P. Resnik.</text>
              <doc_id>309</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>310</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Online largemargin training of syntactic and structural translation features.</text>
              <doc_id>311</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of EMNLP.</text>
              <doc_id>312</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>George Foster, Pierre Isabelle, and Pierre Plamondon.</text>
              <doc_id>313</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1997.</text>
              <doc_id>314</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Target-text mediated interactive machine translation.</text>
              <doc_id>315</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Machine Translation, 12(1):175&#8211;194.</text>
              <doc_id>316</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>P. Isabelle and K. Church.</text>
              <doc_id>317</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>1997.</text>
              <doc_id>318</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Special issue on</text>
              <doc_id>319</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>new tools for human translators.</text>
              <doc_id>320</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Machine Translation, 12(1&#8211;2).</text>
              <doc_id>321</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>D.E.</text>
              <doc_id>322</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Knuth.</text>
              <doc_id>323</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>1981.</text>
              <doc_id>324</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Seminumerical Algorithms, volume 2 of The Art of Computer Programming.</text>
              <doc_id>325</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Addison- Wesley, Massachusetts, 2nd edition.</text>
              <doc_id>326</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>P. Koehn, F. J. Och, and D. Marcu.</text>
              <doc_id>327</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>328</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Statistical</text>
              <doc_id>329</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>phrase-based translation.</text>
              <doc_id>330</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the HLT/NAACL, pages 48&#8211;54, Edmonton, Canada, May.</text>
              <doc_id>331</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P. Langlais, G. Lapalme, and M. Loranger.</text>
              <doc_id>332</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>333</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Transtype: Development-evaluation cycles to boost translator&#8217;s productivity.</text>
              <doc_id>334</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Machine Translation, 15(4):77&#8211;98.</text>
              <doc_id>335</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>P. Liang, A. Bouchard-C&#244;t&#233;, D. Klein, and B. Taskar.</text>
              <doc_id>336</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2006.</text>
              <doc_id>337</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>An end-to-end discriminative approach to machine translation.</text>
              <doc_id>338</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the 44th ACL, pages 761&#8211; 768, Morristown, NJ, USA.</text>
              <doc_id>339</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>R.M.</text>
              <doc_id>340</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Neal and G.E.</text>
              <doc_id>341</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Hinton.</text>
              <doc_id>342</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>1998.</text>
              <doc_id>343</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>A view of the EM</text>
              <doc_id>344</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>algorithm that justifies incremental, sparse, and other variants.</text>
              <doc_id>345</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the NATO-ASI on Learning in graphical models, pages 355&#8211;368, Norwell, MA, USA.</text>
              <doc_id>346</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>L. Nepveu, G. Lapalme, P. Langlais, and G. Foster.</text>
              <doc_id>347</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2004.</text>
              <doc_id>348</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Adaptive language and translation models for interactive machine translation.</text>
              <doc_id>349</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of EMNLP, pages 190&#8211;197, Barcelona, Spain, July.</text>
              <doc_id>350</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Franz Josef Och and Hermann Ney.</text>
              <doc_id>351</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>352</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.</text>
              <doc_id>353</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the 40th ACL, pages 295&#8211;302, Philadelphia, PA, July. Franz Josef Och and Hermann Ney.</text>
              <doc_id>354</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>355</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>A systematic comparison of various statistical alignment models.</text>
              <doc_id>356</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 29(1):19&#8211;51, March.</text>
              <doc_id>357</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>D. Ortiz-Mart&#237;nez, I. Garc&#237;a-Varea, and Casacuberta F.</text>
              <doc_id>358</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2008.</text>
              <doc_id>359</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The scaling problem in the pattern recognition approach to machine translation.</text>
              <doc_id>360</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Pattern Recognition Letters, 29:1145&#8211;1153.</text>
              <doc_id>361</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Daniel Ortiz-Mart&#237;nez, Ismael Garc&#237;a-Varea, and Francisco Casacuberta.</text>
              <doc_id>362</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>363</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Interactive machine translation based on partial statistical phrase-based alignments.</text>
              <doc_id>364</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of RANLP, Borovets, Bulgaria, sep. Kishore A. Papineni, Salim Roukos, Todd Ward, and</text>
              <doc_id>365</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Wei-Jing Zhu.</text>
              <doc_id>366</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2001.</text>
              <doc_id>367</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Bleu: a method for automatic evaluation of machine translation.</text>
              <doc_id>368</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY, September.</text>
              <doc_id>369</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>SchlumbergerSema S.A., ITI Valencia, RWTH Aachen, RALI Montreal, Celer Soluciones, Soci&#233;t&#233; Gamma, and XRCE.</text>
              <doc_id>370</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2001.</text>
              <doc_id>371</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>TT2.</text>
              <doc_id>372</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>TransType2 - computer assisted translation.</text>
              <doc_id>373</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Project Tech.</text>
              <doc_id>374</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Rep.</text>
              <doc_id>375</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Kristina Toutanova, H. Tolga Ilhan, and Christopher</text>
              <doc_id>376</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Manning.</text>
              <doc_id>377</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>378</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Extensions to hmm-based statistical word alignment models.</text>
              <doc_id>379</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of EMNLP.</text>
              <doc_id>380</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>E. Vidal, L. Rodr&#237;guez, F. Casacuberta, and I. Garc&#237;a-</text>
              <doc_id>381</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Varea.</text>
              <doc_id>382</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>383</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Interactive pattern recognition.</text>
              <doc_id>384</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the 4th MLMI, pages 60&#8211;71.</text>
              <doc_id>385</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Brno, Czech Republic, 28-30 June.</text>
              <doc_id>386</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Stephan Vogel, Hermann Ney, and Christoph Tillmann.</text>
              <doc_id>387</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1996.</text>
              <doc_id>388</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>HMM-based word alignment in statistical translation.</text>
              <doc_id>389</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of COLING, pages 836&#8211;841, Copenhagen, Denmark, August.</text>
              <doc_id>390</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.</text>
              <doc_id>391</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2007.</text>
              <doc_id>392</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Online large-margin training for statistical machine translation.</text>
              <doc_id>393</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of EMNLP and CoNLL, pages 764&#8211;733, Prage, Czeck Republic.</text>
              <doc_id>394</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: XEROX corpus statistics for three different language pairs (from English (En) to Spanish (Sp), French (Fr) and German (Ge))</caption>
        <reference_text>In PAGE 6: ..., 2001), which consists of translations of Xe- rox printer manuals involving three different pairs of languages: French-English, Spanish-English, and German-English. The main features of these cor- pora are shown in  Table1 . Partitions into training, development and test were performed....</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>None</cell>
              <cell>Sent. pairs</cell>
              <cell>En   55761</cell>
              <cell>Sp   55761</cell>
              <cell>En   52844</cell>
              <cell>Fr   52844</cell>
              <cell>En   49376</cell>
              <cell>Ge    49376</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Train</cell>
              <cell>Running words</cell>
              <cell>571960</cell>
              <cell>657172</cell>
              <cell>542762</cell>
              <cell>573170</cell>
              <cell>506877</cell>
              <cell>440682</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Vocabulary</cell>
              <cell>25627</cell>
              <cell>29565</cell>
              <cell>24958</cell>
              <cell>27399</cell>
              <cell>24899</cell>
              <cell>37338</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Sent. pairs</cell>
              <cell>1012</cell>
              <cell>1012</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>964</cell>
              <cell>964</cell>
            </row>
            <row>
              <cell>Dev.</cell>
              <cell>Running words</cell>
              <cell>12111</cell>
              <cell>13808</cell>
              <cell>9480</cell>
              <cell>9801</cell>
              <cell>9162</cell>
              <cell>8283</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Perplexity (3-grams)</cell>
              <cell>46.2</cell>
              <cell>34.0</cell>
              <cell>96.2</cell>
              <cell>74.1</cell>
              <cell>68.4</cell>
              <cell>124.3</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Sent. pairs</cell>
              <cell>1125</cell>
              <cell>1125</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>996</cell>
              <cell>996</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Running words</cell>
              <cell>7634</cell>
              <cell>9358</cell>
              <cell>9572</cell>
              <cell>9805</cell>
              <cell>10792</cell>
              <cell>9823</cell>
            </row>
            <row>
              <cell>Test</cell>
              <cell>Perplexity (3-grams)</cell>
              <cell>107.0</cell>
              <cell>59.6</cell>
              <cell>192.6</cell>
              <cell>135.4</cell>
              <cell>92.8</cell>
              <cell>169.2</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>A Arun</author>
          <author>P Koehn</author>
        </authors>
        <title>Online learning methods for discriminative training of phrase based statistical machine translation.</title>
        <publication>In Proc. of the MT Summit XI,</publication>
        <pages>15--20</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>S Barrachina</author>
          <author>O Bender</author>
          <author>F Casacuberta</author>
          <author>J Civera</author>
          <author>E Cubel</author>
          <author>S Khadivi</author>
          <author>A Lagarda</author>
          <author>H Ney</author>
          <author>J Tom&#225;s</author>
          <author>E Vidal</author>
        </authors>
        <title>Statistical approaches to computer-assisted translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Peter F Brown</author>
          <author>Stephen A Della Pietra</author>
          <author>Vincent J Della Pietra</author>
          <author>R L Mercer</author>
        </authors>
        <title>The mathematics of statistical machine translation: Parameter estimation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1993</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>N Cesa-Bianchi</author>
          <author>G Reverberi</author>
          <author>S Szedmak</author>
        </authors>
        <title>Online learning algorithms for computer-assisted translation. Deliverable D4.2, SMART: Stat. Multilingual Analysis for Retrieval and Translation,</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>S F Chen</author>
          <author>J Goodman</author>
        </authors>
        <title>An empirical study of smoothing techniques for language modeling.</title>
        <publication>In Proc. of the ACL,</publication>
        <pages>310--318</pages>
        <date>1996</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>D Chiang</author>
          <author>Y Marton</author>
          <author>P Resnik</author>
        </authors>
        <title>Online largemargin training of syntactic and structural translation features.</title>
        <publication>In Proc. of EMNLP.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>George Foster</author>
          <author>Pierre Isabelle</author>
          <author>Pierre Plamondon</author>
        </authors>
        <title>Target-text mediated interactive machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>P Isabelle</author>
          <author>K Church</author>
        </authors>
        <title>Special issue on new tools for human translators.</title>
        <publication>None</publication>
        <pages>12--1</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>D E Knuth</author>
        </authors>
        <title>Seminumerical Algorithms,</title>
        <publication>of The Art of Computer Programming.</publication>
        <pages>None</pages>
        <date>1981</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>P Koehn</author>
          <author>F J Och</author>
          <author>D Marcu</author>
        </authors>
        <title>Statistical phrase-based translation.</title>
        <publication>In Proc. of the HLT/NAACL,</publication>
        <pages>48--54</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>P Langlais</author>
          <author>G Lapalme</author>
          <author>M Loranger</author>
        </authors>
        <title>Transtype: Development-evaluation cycles to boost translator&#8217;s productivity.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>P Liang</author>
          <author>A Bouchard-C&#244;t&#233;</author>
          <author>D Klein</author>
          <author>B Taskar</author>
        </authors>
        <title>An end-to-end discriminative approach to machine translation.</title>
        <publication>In Proc. of the 44th ACL,</publication>
        <pages>761--768</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>R M Neal</author>
          <author>G E Hinton</author>
        </authors>
        <title>A view of the EM algorithm that justifies incremental, sparse, and other variants.</title>
        <publication>In Proceedings of the NATO-ASI on Learning in graphical models,</publication>
        <pages>355--368</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>L Nepveu</author>
          <author>G Lapalme</author>
          <author>P Langlais</author>
          <author>G Foster</author>
        </authors>
        <title>Adaptive language and translation models for interactive machine translation.</title>
        <publication>In Proc. of EMNLP,</publication>
        <pages>190--197</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.</title>
        <publication>In Proc. of the 40th ACL,</publication>
        <pages>295--302</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A systematic comparison of various statistical alignment models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>D Ortiz-Mart&#237;nez</author>
          <author>I Garc&#237;a-Varea</author>
          <author>F Casacuberta</author>
        </authors>
        <title>The scaling problem in the pattern recognition approach to machine translation.</title>
        <publication>None</publication>
        <pages>29--1145</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Daniel Ortiz-Mart&#237;nez</author>
          <author>Ismael Garc&#237;a-Varea</author>
          <author>Francisco Casacuberta</author>
        </authors>
        <title>Interactive machine translation based on partial statistical phrase-based alignments.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>of RANLP</author>
          <author>Bulgaria Borovets</author>
          <author>sep Kishore A Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>Wei-Jing Zhu</author>
        </authors>
        <title>Bleu: a method for automatic evaluation of machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>S A SchlumbergerSema</author>
          <author>ITI Valencia</author>
          <author>RWTH Aachen</author>
          <author>RALI Montreal</author>
          <author>Celer Soluciones</author>
          <author>Soci&#233;t&#233; Gamma</author>
          <author>XRCE</author>
        </authors>
        <title>TT2. TransType2 - computer assisted translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Kristina Toutanova</author>
          <author>H Tolga Ilhan</author>
          <author>Christopher Manning</author>
        </authors>
        <title>Extensions to hmm-based statistical word alignment models.</title>
        <publication>In Proc. of EMNLP.</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>E Vidal</author>
          <author>L Rodr&#237;guez</author>
          <author>F Casacuberta</author>
          <author>I Garc&#237;aVarea</author>
        </authors>
        <title>Interactive pattern recognition.</title>
        <publication>In Proc. of the 4th MLMI,</publication>
        <pages>60--71</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Stephan Vogel</author>
          <author>Hermann Ney</author>
          <author>Christoph Tillmann</author>
        </authors>
        <title>HMM-based word alignment in statistical translation.</title>
        <publication>In Proc. of COLING,</publication>
        <pages>836--841</pages>
        <date>1996</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Arun and Koehn, 2007</string>
        <sentence_id>23351</sentence_id>
        <char_offset>116</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Barrachina et al. (2009)</string>
        <sentence_id>23276</sentence_id>
        <char_offset>31</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>Barrachina et al., 2009</string>
        <sentence_id>23338</sentence_id>
        <char_offset>200</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>1</reference_id>
        <string>Barrachina et al., 2009</string>
        <sentence_id>23345</sentence_id>
        <char_offset>131</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>1</reference_id>
        <string>Barrachina et al., 2009</string>
        <sentence_id>23491</sentence_id>
        <char_offset>134</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>1</reference_id>
        <string>Barrachina et al., 2009</string>
        <sentence_id>23538</sentence_id>
        <char_offset>48</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>2</reference_id>
        <string>Brown et al., 1993</string>
        <sentence_id>23303</sentence_id>
        <char_offset>143</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>3</reference_id>
        <string>Cesa-Bianchi et al., 2008</string>
        <sentence_id>23356</sentence_id>
        <char_offset>72</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>4</reference_id>
        <string>Chen and Goodman, 1996</string>
        <sentence_id>23371</sentence_id>
        <char_offset>136</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>5</reference_id>
        <string>Chiang et al., 2008</string>
        <sentence_id>23351</sentence_id>
        <char_offset>161</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>6</reference_id>
        <string>Foster et al., 1997</string>
        <sentence_id>23275</sentence_id>
        <char_offset>84</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>7</reference_id>
        <string>Isabelle and Church, 1997</string>
        <sentence_id>23274</sentence_id>
        <char_offset>194</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>8</reference_id>
        <string>Knuth, 1981</string>
        <sentence_id>23415</sentence_id>
        <char_offset>45</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>9</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>23319</sentence_id>
        <char_offset>64</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>9</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>23380</sentence_id>
        <char_offset>150</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>9</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>23451</sentence_id>
        <char_offset>223</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>10</reference_id>
        <string>Langlais et al., 2002</string>
        <sentence_id>23275</sentence_id>
        <char_offset>105</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>11</reference_id>
        <string>Liang et al., 2006</string>
        <sentence_id>23351</sentence_id>
        <char_offset>96</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>12</reference_id>
        <string>Neal and Hinton, 1998</string>
        <sentence_id>23460</sentence_id>
        <char_offset>87</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>13</reference_id>
        <string>Nepveu et al., 2004</string>
        <sentence_id>23353</sentence_id>
        <char_offset>4</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>15</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>23458</sentence_id>
        <char_offset>154</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>15</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>23461</sentence_id>
        <char_offset>14</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>15</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>23475</sentence_id>
        <char_offset>122</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>15</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>23476</sentence_id>
        <char_offset>64</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>17</reference_id>
        <string>Ortiz-Mart&#237;nez et al., 2009</string>
        <sentence_id>23346</sentence_id>
        <char_offset>4</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>17</reference_id>
        <string>Ortiz-Mart&#237;nez et al., 2009</string>
        <sentence_id>23385</sentence_id>
        <char_offset>100</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>17</reference_id>
        <string>Ortiz-Mart&#237;nez et al., 2009</string>
        <sentence_id>23405</sentence_id>
        <char_offset>221</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>20</reference_id>
        <string>Toutanova et al., 2002</string>
        <sentence_id>23476</sentence_id>
        <char_offset>88</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>21</reference_id>
        <string>Vidal et al., 2007</string>
        <sentence_id>23284</sentence_id>
        <char_offset>25</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>21</reference_id>
        <string>Vidal et al., 2007</string>
        <sentence_id>23552</sentence_id>
        <char_offset>198</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>22</reference_id>
        <string>Vogel et al., 1996</string>
        <sentence_id>23381</sentence_id>
        <char_offset>97</char_offset>
      </citation>
    </citations>
  </content>
</document>
