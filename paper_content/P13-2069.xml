<document>
  <filename>P13-2069</filename>
  <authors>
    <author>Fei Huang</author>
  </authors>
  <title>Generalized Reordering Rules for Improved SMT</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>We present a simple yet effective approach to syntactic reordering for Statistical Machine Translation (SMT). Instead of solely relying on the top-1 best-matching rule for source sentence preordering, we generalize fully lexicalized rules into partially lexicalized and unlexicalized rules to broaden the rule coverage. Furthermore, , we consider multiple permutations of all the matching rules, and select the final reordering path based on the weighed sum of reordering probabilities of these rules. Our experiments in English-Chinese and English-Japanese translations demonstrate the effectiveness of the proposed approach: we observe consistent and significant improvement in translation quality across multiple test sets in both language pairs judged by both humans and automatic metric.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We present a simple yet effective approach to syntactic reordering for Statistical Machine Translation (SMT).</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Instead of solely relying on the top-1 best-matching rule for source sentence preordering, we generalize fully lexicalized rules into partially lexicalized and unlexicalized rules to broaden the rule coverage.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Furthermore, , we consider multiple permutations of all the matching rules, and select the final reordering path based on the weighed sum of reordering probabilities of these rules.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Our experiments in English-Chinese and English-Japanese translations demonstrate the effectiveness of the proposed approach: we observe consistent and significant improvement in translation quality across multiple test sets in both language pairs judged by both humans and automatic metric.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Languages are structured data. The proper handling of linguistic structures (such as word order) has been one of the most important yet most challenging tasks in statistical machine translation (SMT). It is important because it has significant impact on human judgment of Machine Translation (MT) quality: an MT output without structure is just like a bag of words. It is also very challenging due to the lack of effective methods to model the structural difference between source and target languages.
A lot of research has been conducted in this area. Approaches include distance-based penalty function (Koehn et. al. 2003) and lexicalized distortion models such as (Tillman 2004), (Al- Onaizan and Papineni 2006). Because these models are relatively easy to compute, they are widely used in phrase-based SMT systems. Hierarchical phrase-based system (Hiero, Chiang, 2005) utilizes long range reordering information without syntax. Other models use more syntactic information (string-to-tree, treeto-string, tree-to-tree, string-to-dependency etc.) to capture the structural difference between language pairs, including (Yamada and Knight, 2001), (Zollmann and Venugopal, 2006), (Liu et. al. 2006), and (Shen et. al. 2008). These models demonstrate better handling of sentence structures, while the computation is more expensive compared with the distortion-based models.
In the middle of the spectrum, (Xia and McCord 2004), (Collins et. al 2005), (Wang et. al. 2007), and (Visweswariah et. al. 2010) combined the benefits of the above two strategies: their approaches reorder an input sentence based on a set of reordering rules defined over the source sentence&#8217;s syntax parse tree. As a result, the re-ordered source sentence resembles the word order of its target translation. The reordering rules are either hand-crafted or automatically learned from the training data (source parse trees and bitext word alignments). These rules can be unlexicalized (only including the constituent labels) or fully lexicalized (including both the constituent labels and their head words). The unlexicalized reordering rules are more general and can be applied broadly, but sometimes they are not discriminative enough. In the following English-Chinese reordering rules,
the NP and PP nodes are reordered with close to random probabilities. When the constituents are attached with their headwords, the reordering probability is much higher than that of the unlexicalized rules.
Unfortunately, the application of lexicalized reordering rules is constrained by data sparseness: it is unlikely to train the NP:&lt;noun&gt;
PP:&lt;prep&gt; reordering rules for every nounpreposition combination. Even for the learnt lexicalized rules, their counts are also relatively small, thus the reordering probabilities may not be estimated reliably, which could lead to incorrect reordering decisions. To alleviate this problem, we generalize fully lexicalized rules into partially lexicalized rules, which are further generalized into unlexicalized rules. Such generalization allows partial match when the fully lexicalized rules can not be found, thus achieving broader rule coverage.
Given a node of a source parse tree, we find all the matching rules and consider all their possible reorder permutations. Each permutation has a reordering score, which is the weighted sum of reordering probabilities of all the matching rules. We reorder the child nodes based on the permutation with the highest reordering score. Finally we translate the reordered sentence in a phrase-based SMT system. Our experiments in English to Chinese (EnZh) and English to Japanese (EnJa) translation demonstrate the effectiveness of the proposed approach: we observe consistent improvements across multiple test sets in multiple language pairs and significant gain in human judgment of the MT quality. This paper is organized as follows: in section 2 we briefly introduce the syntax-based reordering technique. In section 3, we describe our approach. In section 4, we show the experiment results, which is followed by conclusion in section 5.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Languages are structured data.</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The proper handling of linguistic structures (such as word order) has been one of the most important yet most challenging tasks in statistical machine translation (SMT).</text>
              <doc_id>5</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>It is important because it has significant impact on human judgment of Machine Translation (MT) quality: an MT output without structure is just like a bag of words.</text>
              <doc_id>6</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>It is also very challenging due to the lack of effective methods to model the structural difference between source and target languages.</text>
              <doc_id>7</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A lot of research has been conducted in this area.</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Approaches include distance-based penalty function (Koehn et.</text>
              <doc_id>9</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>al. 2003) and lexicalized distortion models such as (Tillman 2004), (Al- Onaizan and Papineni 2006).</text>
              <doc_id>10</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Because these models are relatively easy to compute, they are widely used in phrase-based SMT systems.</text>
              <doc_id>11</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Hierarchical phrase-based system (Hiero, Chiang, 2005) utilizes long range reordering information without syntax.</text>
              <doc_id>12</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Other models use more syntactic information (string-to-tree, treeto-string, tree-to-tree, string-to-dependency etc.) to capture the structural difference between language pairs, including (Yamada and Knight, 2001), (Zollmann and Venugopal, 2006), (Liu et.</text>
              <doc_id>13</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>al. 2006), and (Shen et.</text>
              <doc_id>14</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>al. 2008).</text>
              <doc_id>15</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>These models demonstrate better handling of sentence structures, while the computation is more expensive compared with the distortion-based models.</text>
              <doc_id>16</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the middle of the spectrum, (Xia and McCord 2004), (Collins et.</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>al 2005), (Wang et.</text>
              <doc_id>18</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>al. 2007), and (Visweswariah et.</text>
              <doc_id>19</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>al. 2010) combined the benefits of the above two strategies: their approaches reorder an input sentence based on a set of reordering rules defined over the source sentence&#8217;s syntax parse tree.</text>
              <doc_id>20</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>As a result, the re-ordered source sentence resembles the word order of its target translation.</text>
              <doc_id>21</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The reordering rules are either hand-crafted or automatically learned from the training data (source parse trees and bitext word alignments).</text>
              <doc_id>22</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>These rules can be unlexicalized (only including the constituent labels) or fully lexicalized (including both the constituent labels and their head words).</text>
              <doc_id>23</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The unlexicalized reordering rules are more general and can be applied broadly, but sometimes they are not discriminative enough.</text>
              <doc_id>24</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>In the following English-Chinese reordering rules,</text>
              <doc_id>25</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the NP and PP nodes are reordered with close to random probabilities.</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>When the constituents are attached with their headwords, the reordering probability is much higher than that of the unlexicalized rules.</text>
              <doc_id>27</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Unfortunately, the application of lexicalized reordering rules is constrained by data sparseness: it is unlikely to train the NP:&lt;noun&gt;</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>PP:&lt;prep&gt; reordering rules for every nounpreposition combination.</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Even for the learnt lexicalized rules, their counts are also relatively small, thus the reordering probabilities may not be estimated reliably, which could lead to incorrect reordering decisions.</text>
              <doc_id>30</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>To alleviate this problem, we generalize fully lexicalized rules into partially lexicalized rules, which are further generalized into unlexicalized rules.</text>
              <doc_id>31</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Such generalization allows partial match when the fully lexicalized rules can not be found, thus achieving broader rule coverage.</text>
              <doc_id>32</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Given a node of a source parse tree, we find all the matching rules and consider all their possible reorder permutations.</text>
              <doc_id>33</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each permutation has a reordering score, which is the weighted sum of reordering probabilities of all the matching rules.</text>
              <doc_id>34</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We reorder the child nodes based on the permutation with the highest reordering score.</text>
              <doc_id>35</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Finally we translate the reordered sentence in a phrase-based SMT system.</text>
              <doc_id>36</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Our experiments in English to Chinese (EnZh) and English to Japanese (EnJa) translation demonstrate the effectiveness of the proposed approach: we observe consistent improvements across multiple test sets in multiple language pairs and significant gain in human judgment of the MT quality.</text>
              <doc_id>37</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This paper is organized as follows: in section 2 we briefly introduce the syntax-based reordering technique.</text>
              <doc_id>38</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In section 3, we describe our approach.</text>
              <doc_id>39</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In section 4, we show the experiment results, which is followed by conclusion in section 5.</text>
              <doc_id>40</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Baseline Syntax-based Reordering</title>
        <text>In the general syntax-based reordering, reordering is achieved by permuting the children of any interior node in the source parse tree. Although there are cases where reordering is needed across multiple constituents, this still is a simple and effective technique.
Formally, the reordering rule is a triple {p, lhs, rhs}, where p is the reordering probability, lhs is the left hand side of the rule, i.e., the constituent label sequence of a parse tree node, and rhs is the reordering permutation derived either from handcrafted rules as in (Collins et. al 2005) and (Wang et. al. 2007), or from training data as in (Visweswariah et. al. 2010). The training data includes bilingual sentence pairs with word alignments, as well as the source sentences' parse trees. The children&#8217;s relative order of each node is decided according to their average alignment position in the target sentence. Such relative order is a permutation of the integer sequence [0, 1, &#8230; N-1], where N is the number of children of the given parse node. The counts of each permutation of each parse label sequence will be collected from the training data and converted to probabilities as shown in the examples in Section 1. Finally, only the permutation with the highest probability is selected to reorder the matching parse node. The SMT system is re-trained on reordered training data to translate reordered input sentences.
Following the above approach, only the reordering rule [0.56 NP PP &#8594; 1 0] is kept in the above example. In other words, all the NP PP phrases will be reordered, even though the reordering is only slightly preferred in all the training data.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In the general syntax-based reordering, reordering is achieved by permuting the children of any interior node in the source parse tree.</text>
              <doc_id>41</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Although there are cases where reordering is needed across multiple constituents, this still is a simple and effective technique.</text>
              <doc_id>42</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Formally, the reordering rule is a triple {p, lhs, rhs}, where p is the reordering probability, lhs is the left hand side of the rule, i.e., the constituent label sequence of a parse tree node, and rhs is the reordering permutation derived either from handcrafted rules as in (Collins et.</text>
              <doc_id>43</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>al 2005) and (Wang et.</text>
              <doc_id>44</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>al. 2007), or from training data as in (Visweswariah et.</text>
              <doc_id>45</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>al. 2010).</text>
              <doc_id>46</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The training data includes bilingual sentence pairs with word alignments, as well as the source sentences' parse trees.</text>
              <doc_id>47</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The children&#8217;s relative order of each node is decided according to their average alignment position in the target sentence.</text>
              <doc_id>48</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Such relative order is a permutation of the integer sequence [0, 1, &#8230; N-1], where N is the number of children of the given parse node.</text>
              <doc_id>49</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The counts of each permutation of each parse label sequence will be collected from the training data and converted to probabilities as shown in the examples in Section 1.</text>
              <doc_id>50</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Finally, only the permutation with the highest probability is selected to reorder the matching parse node.</text>
              <doc_id>51</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>The SMT system is re-trained on reordered training data to translate reordered input sentences.</text>
              <doc_id>52</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Following the above approach, only the reordering rule [0.56 NP PP &#8594; 1 0] is kept in the above example.</text>
              <doc_id>53</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In other words, all the NP PP phrases will be reordered, even though the reordering is only slightly preferred in all the training data.</text>
              <doc_id>54</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Generalized Syntactic Reordering</title>
        <text>As shown in the previous examples, reordering depends not only on the constituents&#8217; parse labels, but also on the headwords of the constituents. Such fully lexicalized rules suffer from data sparseness: there is either no matching lexicalized rule for a given parse node or the matching rule&#8217;s reordering probability is unreliable. We address the above issues with rule generalization, then consider all the permutations from multi-level rule matching.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>As shown in the previous examples, reordering depends not only on the constituents&#8217; parse labels, but also on the headwords of the constituents.</text>
              <doc_id>55</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Such fully lexicalized rules suffer from data sparseness: there is either no matching lexicalized rule for a given parse node or the matching rule&#8217;s reordering probability is unreliable.</text>
              <doc_id>56</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We address the above issues with rule generalization, then consider all the permutations from multi-level rule matching.</text>
              <doc_id>57</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Rule Generalization</title>
            <text>Lexicalized rules are applied only when both the constituent labels and headwords match. When only the labels match, these reordering rules are not used. To increase the rule coverage, we generalize the fully lexicalized rules into partially lexicalized and unlexicalized rules.
We notice that many lexicalized rules share similar reordering permutations, thus it is possible to merge them to form a partially lexicalized rule, where lexicalization only appears at selected constituent&#8217;s headword. Although it is possible to have multiple lexicalizations in a partially lexicalized rule (which will exponentially increase the total number of rules), we observe that most of the time reordering is triggered by a single constituent. Therefore we keep one lexicalization in the partially lexicalized rules. For example, the following lexicalized rule:
VB:appeal PP-MNR:by PP-DIR:to --&gt; 1 2 0
will be converted into the following 3 partially lexicalized rules:
The count of each rule will be the sum of the fully lexicalized rules which can derive the given partially lexicalized rule. In the above preordering rules, &#8220;MNR&#8221; and &#8220;DIR&#8221; are functional labels, indicating the semantic labels (&#8220;manner&#8221;, &#8220;direction&#8221;) of the parse node. We could go even further, converting the partially lexicalized rules into unlexicalized rules. This is similar to the baseline syntax reordering model, although we will keep all their possible permutations and counts for rule matching, as shown below.
5 VB PP-MNR PP-DIR --&gt; 2 0 1 22 VB PP-MNR PP-DIR --&gt; 2 1 0 21 VB PP-MNR PP-DIR --&gt; 0 1 2 41 VB PP-MNR PP-DIR --&gt; 1 2 0 35 VB PP-MNR PP-DIR --&gt; 1 0 2
Note that to reduce the noise from paring and word alignment errors, we only keep the reordering rules that appear at least 5 times. Then we convert the counts into probabilities:
p ( rhs | lhs ) =
i i
Ci ( rhs, lhsi ) C (*, lhs )
&#8721;
where i &#8712;{ f , p, u} represents the fully, partially and un-lexicalized rules, and C
i
( rhs, lhsi ) is the count of rule (lhs i &#8594; rhs) in type i rules.
When we convert the most specific fully lexicalized rules to the more general partially lexicalized rules and then to the most general unlexicalized rules, we increase the rule coverage while keep their discriminative power at different levels as much as possible.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Lexicalized rules are applied only when both the constituent labels and headwords match.</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>When only the labels match, these reordering rules are not used.</text>
                  <doc_id>59</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>To increase the rule coverage, we generalize the fully lexicalized rules into partially lexicalized and unlexicalized rules.</text>
                  <doc_id>60</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We notice that many lexicalized rules share similar reordering permutations, thus it is possible to merge them to form a partially lexicalized rule, where lexicalization only appears at selected constituent&#8217;s headword.</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Although it is possible to have multiple lexicalizations in a partially lexicalized rule (which will exponentially increase the total number of rules), we observe that most of the time reordering is triggered by a single constituent.</text>
                  <doc_id>62</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore we keep one lexicalization in the partially lexicalized rules.</text>
                  <doc_id>63</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For example, the following lexicalized rule:</text>
                  <doc_id>64</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VB:appeal PP-MNR:by PP-DIR:to --&gt; 1 2 0</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>will be converted into the following 3 partially lexicalized rules:</text>
                  <doc_id>66</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The count of each rule will be the sum of the fully lexicalized rules which can derive the given partially lexicalized rule.</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the above preordering rules, &#8220;MNR&#8221; and &#8220;DIR&#8221; are functional labels, indicating the semantic labels (&#8220;manner&#8221;, &#8220;direction&#8221;) of the parse node.</text>
                  <doc_id>68</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We could go even further, converting the partially lexicalized rules into unlexicalized rules.</text>
                  <doc_id>69</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This is similar to the baseline syntax reordering model, although we will keep all their possible permutations and counts for rule matching, as shown below.</text>
                  <doc_id>70</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5 VB PP-MNR PP-DIR --&gt; 2 0 1 22 VB PP-MNR PP-DIR --&gt; 2 1 0 21 VB PP-MNR PP-DIR --&gt; 0 1 2 41 VB PP-MNR PP-DIR --&gt; 1 2 0 35 VB PP-MNR PP-DIR --&gt; 1 0 2</text>
                  <doc_id>71</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Note that to reduce the noise from paring and word alignment errors, we only keep the reordering rules that appear at least 5 times.</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Then we convert the counts into probabilities:</text>
                  <doc_id>73</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p ( rhs | lhs ) =</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i i</text>
                  <doc_id>75</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Ci ( rhs, lhsi ) C (*, lhs )</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where i &#8712;{ f , p, u} represents the fully, partially and un-lexicalized rules, and C</text>
                  <doc_id>78</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>( rhs, lhsi ) is the count of rule (lhs i &#8594; rhs) in type i rules.</text>
                  <doc_id>80</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>When we convert the most specific fully lexicalized rules to the more general partially lexicalized rules and then to the most general unlexicalized rules, we increase the rule coverage while keep their discriminative power at different levels as much as possible.</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Multiple Permutation Multi-level Rule Matching</title>
            <text>When applying the three types of reordering rules to reorder a parse tree node, we find all the matching rules and consider all possible permutations. As multiple levels of rules can lead to the same permutation with different probabilities, we take the weighted sum of probabilities from all matching rules (with the same rhs). Therefore, the permutation decision is not based on any particular rule, but the combination of all the rules matching different
i i
levels of context. As opposed to the general syntax-based reordering approaches, this strategy achieves a desired balance between broad rule coverage and specific rule match: when a fully lexicalized rule matches, it has strong influence on the permutation decision given the richer context. If such specific rule is unavailable or has low probability, more general (partial and unlexicalized) rules will have higher weights. For each permutation we compute the weighted reordering probability, then select the permutation that has the highest score.
Formally, given a parse tree node T, let lhs f be the label:head_word sequence of the fully lexicalized rules matching T. Similarly, lhs p and lhs u are the sequences of the matching partially lexicalized and unlexicalized rules, respectively, and let rhs be their possible permutations. The top-score permutation is computed as:
*
rhs = arg max w p ( rhs | lhs )
&#8721;
rhs i i&#8712;{ f , p, u}
where w i &#8217;s are the weights of different kind of rules and p i is reordering probability of each rule. The weights are chosen empirically based on the performance on a held-out tuning set. In our experiments, w f =1.0, w p =0.5, and w u =0.2, where higher weights are assigned to more specific rules.
For each parse tree node, we identify the top permutation choice and reorder its children accordingly. The source parse tree is traversed breadth-first.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>When applying the three types of reordering rules to reorder a parse tree node, we find all the matching rules and consider all possible permutations.</text>
                  <doc_id>82</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As multiple levels of rules can lead to the same permutation with different probabilities, we take the weighted sum of probabilities from all matching rules (with the same rhs).</text>
                  <doc_id>83</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, the permutation decision is not based on any particular rule, but the combination of all the rules matching different</text>
                  <doc_id>84</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i i</text>
                  <doc_id>85</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>levels of context.</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As opposed to the general syntax-based reordering approaches, this strategy achieves a desired balance between broad rule coverage and specific rule match: when a fully lexicalized rule matches, it has strong influence on the permutation decision given the richer context.</text>
                  <doc_id>87</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>If such specific rule is unavailable or has low probability, more general (partial and unlexicalized) rules will have higher weights.</text>
                  <doc_id>88</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For each permutation we compute the weighted reordering probability, then select the permutation that has the highest score.</text>
                  <doc_id>89</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Formally, given a parse tree node T, let lhs f be the label:head_word sequence of the fully lexicalized rules matching T.</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Similarly, lhs p and lhs u are the sequences of the matching partially lexicalized and unlexicalized rules, respectively, and let rhs be their possible permutations.</text>
                  <doc_id>91</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The top-score permutation is computed as:</text>
                  <doc_id>92</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>*</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>rhs = arg max w p ( rhs | lhs )</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>rhs i i&#8712;{ f , p, u}</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where w i &#8217;s are the weights of different kind of rules and p i is reordering probability of each rule.</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The weights are chosen empirically based on the performance on a held-out tuning set.</text>
                  <doc_id>98</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In our experiments, w f =1.0, w p =0.5, and w u =0.2, where higher weights are assigned to more specific rules.</text>
                  <doc_id>99</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For each parse tree node, we identify the top permutation choice and reorder its children accordingly.</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The source parse tree is traversed breadth-first.</text>
                  <doc_id>101</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Experiments</title>
        <text>We applied the generalized syntax-based reordering on both English-Chinese (EnZh) and English-Japanese (EnJa) translations. Our English parser is IBM&#8217;s maximum entropy constituent parser (Ratnaparkhi 1999) trained on Penn Treebank. Experiments in (Visweswariah et. al. 2010) indicated that minimal difference was observed using Berkeley&#8217;s parser or IBM&#8217;s parser for reordering.
Our EnZh training data consists of 20 million sentence pairs (~250M words), half of which are from LDC released bilingual corpora and the other half are from technical domains (e.g., software manual). We first trained automatic word alignments (HMM alignments in both directions and a MaxEnt alignment (Ittycheriah and Roukos, 2005)), then parsed the English sentences with the IBM parser. We extracted different reordering rules from the word alignments and the English parse trees. After
i i
frequency-based pruning, we obtained 12M lexicalized rules, 13M partially lexicalized rules and 600K unlexicalized rules. Using these rules, we applied preordering on the English sentences and then built an SMT system with the reordered training data. Our decoder is a phrase-based decoder (Tillman 2006), where various features are combined within the log-linear framework. These features include source-to-target phrase translation score based on relative frequency, source-to-target and target-to-source word-toword translation scores, a 5-gram language model score, distortion model scores and word count.
We selected one tuning set from software manual domain (Tech1), and used PRO tuning (Hopkins and May 2011) to select decoder feature weights. Our test sets include one from the online technical support domain (Tech2) and one from the news domain: the NIST MT08 English-Chinese evaluation test data. The translation quality is measured by BLEU score (Papineni et. al., 2001). Table 1 shows the BLEU score of the baseline phrase-based system (PBMT) that uses lexicalized reordering at decoding time rather than preordering. Next, Table 1 shows the translation results with several preordered systems that use unlexicalized (UnLex), fully lexicalized (FullLex) and partially lexicalized (PartLex) rules, respectively. The lexicalized reordering model is still applicable for preordered systems so that some preordering errors can be recovered at run time.
First we observed that the UnLex preordering model on average does not improve over the typical phrase-based MT baseline due to its limited discriminative power. When the preordering decision is conditioned on the head word, the FullLex model shows some gains (~0.3 pt) thanks to the richer matching context, while the PartLex model improves further over the FullLex model because of its broader coverage. Combining all three with multipermutation, multi-level rule matching (MPML) brings the most gains, with consistent (~1.3 Bleu points) improvement over the baseline system on all the test sets. Note that the Bleu scores on the news domain (MT08) are higher than those on the tech domain. This is because the Tech1 and Tech2 have one reference translation while MT08 has 4 reference translations.
In addition to the automatic MT evaluation, we also used human judgment of quality of the MT translation on a set of randomly selected 125 sentences from the baseline and improved reordering systems. The human judgment score is 2.82 for the UnLex system output, and 3.04 for the improved MPML reordering output. The 0.2 point improvement on the 0-5 scale is considered significant.
We also apply the same generalized reordering technique on English-Japanese (EnJa) translation. As there is very limited publicly available English-Japanese parallel data, most our training data (20M sentence pairs) is from the in-house software manual domain. We use the same English parser and phrase-based decoder as in EnZh experiment. Table 2 shows the translation results on technical and news domain test sets. All the test sets have single reference translation.
First, we observe that the improvement from preordering is larger than that in EnZh MT (1.6-3 pts vs. 1 pt). This is because the word order difference between English and Japanese is larger than that between English and Chinese (Japanese is a SOV language while both English and Chinese are SVO languages). Without preordering, correct word orders are difficult to obtain given the typical skip-window beam search in the PBMT. Also, as in EnZh, the PartLex model outperforms the UnLex model, both of which being significantly better than the FullLex model due to the limited rule coverage in the later model: only 50% preordering rules
are applied in the FullLex model. Tech1 test set is a very close match to the training data thus its BLEU score is much higher.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We applied the generalized syntax-based reordering on both English-Chinese (EnZh) and English-Japanese (EnJa) translations.</text>
              <doc_id>102</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our English parser is IBM&#8217;s maximum entropy constituent parser (Ratnaparkhi 1999) trained on Penn Treebank.</text>
              <doc_id>103</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Experiments in (Visweswariah et.</text>
              <doc_id>104</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>al. 2010) indicated that minimal difference was observed using Berkeley&#8217;s parser or IBM&#8217;s parser for reordering.</text>
              <doc_id>105</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Our EnZh training data consists of 20 million sentence pairs (~250M words), half of which are from LDC released bilingual corpora and the other half are from technical domains (e.g., software manual).</text>
              <doc_id>106</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We first trained automatic word alignments (HMM alignments in both directions and a MaxEnt alignment (Ittycheriah and Roukos, 2005)), then parsed the English sentences with the IBM parser.</text>
              <doc_id>107</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We extracted different reordering rules from the word alignments and the English parse trees.</text>
              <doc_id>108</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>After</text>
              <doc_id>109</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i i</text>
              <doc_id>110</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>frequency-based pruning, we obtained 12M lexicalized rules, 13M partially lexicalized rules and 600K unlexicalized rules.</text>
              <doc_id>111</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Using these rules, we applied preordering on the English sentences and then built an SMT system with the reordered training data.</text>
              <doc_id>112</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Our decoder is a phrase-based decoder (Tillman 2006), where various features are combined within the log-linear framework.</text>
              <doc_id>113</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>These features include source-to-target phrase translation score based on relative frequency, source-to-target and target-to-source word-toword translation scores, a 5-gram language model score, distortion model scores and word count.</text>
              <doc_id>114</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We selected one tuning set from software manual domain (Tech1), and used PRO tuning (Hopkins and May 2011) to select decoder feature weights.</text>
              <doc_id>115</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our test sets include one from the online technical support domain (Tech2) and one from the news domain: the NIST MT08 English-Chinese evaluation test data.</text>
              <doc_id>116</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The translation quality is measured by BLEU score (Papineni et.</text>
              <doc_id>117</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>al., 2001).</text>
              <doc_id>118</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Table 1 shows the BLEU score of the baseline phrase-based system (PBMT) that uses lexicalized reordering at decoding time rather than preordering.</text>
              <doc_id>119</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Next, Table 1 shows the translation results with several preordered systems that use unlexicalized (UnLex), fully lexicalized (FullLex) and partially lexicalized (PartLex) rules, respectively.</text>
              <doc_id>120</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The lexicalized reordering model is still applicable for preordered systems so that some preordering errors can be recovered at run time.</text>
              <doc_id>121</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>First we observed that the UnLex preordering model on average does not improve over the typical phrase-based MT baseline due to its limited discriminative power.</text>
              <doc_id>122</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>When the preordering decision is conditioned on the head word, the FullLex model shows some gains (~0.3 pt) thanks to the richer matching context, while the PartLex model improves further over the FullLex model because of its broader coverage.</text>
              <doc_id>123</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Combining all three with multipermutation, multi-level rule matching (MPML) brings the most gains, with consistent (~1.3 Bleu points) improvement over the baseline system on all the test sets.</text>
              <doc_id>124</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Note that the Bleu scores on the news domain (MT08) are higher than those on the tech domain.</text>
              <doc_id>125</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This is because the Tech1 and Tech2 have one reference translation while MT08 has 4 reference translations.</text>
              <doc_id>126</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In addition to the automatic MT evaluation, we also used human judgment of quality of the MT translation on a set of randomly selected 125 sentences from the baseline and improved reordering systems.</text>
              <doc_id>127</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The human judgment score is 2.82 for the UnLex system output, and 3.04 for the improved MPML reordering output.</text>
              <doc_id>128</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The 0.2 point improvement on the 0-5 scale is considered significant.</text>
              <doc_id>129</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We also apply the same generalized reordering technique on English-Japanese (EnJa) translation.</text>
              <doc_id>130</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As there is very limited publicly available English-Japanese parallel data, most our training data (20M sentence pairs) is from the in-house software manual domain.</text>
              <doc_id>131</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We use the same English parser and phrase-based decoder as in EnZh experiment.</text>
              <doc_id>132</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Table 2 shows the translation results on technical and news domain test sets.</text>
              <doc_id>133</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>All the test sets have single reference translation.</text>
              <doc_id>134</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>First, we observe that the improvement from preordering is larger than that in EnZh MT (1.6-3 pts vs. 1 pt).</text>
              <doc_id>135</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This is because the word order difference between English and Japanese is larger than that between English and Chinese (Japanese is a SOV language while both English and Chinese are SVO languages).</text>
              <doc_id>136</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Without preordering, correct word orders are difficult to obtain given the typical skip-window beam search in the PBMT.</text>
              <doc_id>137</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Also, as in EnZh, the PartLex model outperforms the UnLex model, both of which being significantly better than the FullLex model due to the limited rule coverage in the later model: only 50% preordering rules</text>
              <doc_id>138</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>are applied in the FullLex model.</text>
              <doc_id>139</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Tech1 test set is a very close match to the training data thus its BLEU score is much higher.</text>
              <doc_id>140</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>5 Conclusion and Future Work</title>
        <text>To summarize, we made the following improvements: 1. We generalized fully lexicalized reordering rules to partially lexicalized and unlexicalized rules for broader rule coverage and reduced data sparseness. 2. We allowed multiple permutation, multilevel rule matching to select the best reordering path. Experiment results show consistent and significant improvements on multiple English- Chinese and English-Japanese test sets judged by both automatic and human judgments. In future work we would like to explore new methods to prune the phrase table without degrading MT performance and to make rule extraction and reordering more robust to parsing errors.
The authors appreciate helpful comments from anonymous reviewers as well as fruitful discussions with Karthik Visweswariah and Salim Roukos.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>To summarize, we made the following improvements: 1.</text>
              <doc_id>141</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We generalized fully lexicalized reordering rules to partially lexicalized and unlexicalized rules for broader rule coverage and reduced data sparseness.</text>
              <doc_id>142</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>2.</text>
              <doc_id>143</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We allowed multiple permutation, multilevel rule matching to select the best reordering path.</text>
              <doc_id>144</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Experiment results show consistent and significant improvements on multiple English- Chinese and English-Japanese test sets judged by both automatic and human judgments.</text>
              <doc_id>145</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In future work we would like to explore new methods to prune the phrase table without degrading MT performance and to make rule extraction and reordering more robust to parsing errors.</text>
              <doc_id>146</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The authors appreciate helpful comments from anonymous reviewers as well as fruitful discussions with Karthik Visweswariah and Salim Roukos.</text>
              <doc_id>147</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables/>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Yaser Al-Onaizan</author>
        </authors>
        <title>Distortion models for statistical machine translation,</title>
        <publication>Proceedings of the 21st International Conference on Computational Linguistics and the 44th</publication>
        <pages>529--536</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Abraham Ittycheriah</author>
        </authors>
        <title>A maximum entropy word aligner for Arabic-English machine translation,</title>
        <publication>Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</publication>
        <pages>89--96</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Libin Shen</author>
          <author>Jinxi Xu</author>
          <author>Ralph Weischedel</author>
        </authors>
        <title>A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model.</title>
        <publication>in Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL).</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Christoph Tillmann</author>
        </authors>
        <title>A unigram orientation model for statistical machine translation,</title>
        <publication>Proceedings of HLT-NAACL 2004: Short Papers,</publication>
        <pages>101--104</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Chao Wang</author>
          <author>Michael Collins</author>
          <author>Philipp Koehn</author>
        </authors>
        <title>Chinese syntactic reordering for statistical machine translation.</title>
        <publication>In Proceedings of EMNLPCoNLL.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Jeffrey Sorensen</author>
        </authors>
        <title>Syntax based reordering with automatically derived rules for improved statistical machine translation,</title>
        <publication>Proceedings of the 23rd International Conference on Computational Linguistics,</publication>
        <pages>1119--1127</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Fei Xia</author>
        </authors>
        <title>Improving a statistical MT system with automatically learned rewrite patterns,</title>
        <publication>Proceedings of the 20th international conference on Computational Linguistics, p.508-es,</publication>
        <pages>523--530</pages>
        <date>2004</date>
      </reference>
    </references>
    <citations/>
  </content>
</document>
