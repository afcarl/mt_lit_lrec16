<document>
  <filename>W12-0111</filename>
  <authors/>
  <title>Tree-based Hybrid Machine Translation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>I present an automatic post-editing approach that combines translation systems which produce syntactic trees as output. The nodes in the generation tree and targetside SCFG tree are aligned and form the basis for computing structural similarity. Structural similarity computation aligns subtrees and based on this alignment, subtrees are substituted to create more accurate translations. Two different techniques have been implemented to compute structural similarity: leaves and tree-edit distance. I report on the translation quality of a machine translation (MT) system where both techniques are implemented. The approach shows significant improvement over the baseline for MT systems with limited training data and structural improvement for MT systems trained on Europarl.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>I present an automatic post-editing approach that combines translation systems which produce syntactic trees as output.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The nodes in the generation tree and targetside SCFG tree are aligned and form the basis for computing structural similarity.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Structural similarity computation aligns subtrees and based on this alignment, subtrees are substituted to create more accurate translations.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Two different techniques have been implemented to compute structural similarity: leaves and tree-edit distance.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>I report on the translation quality of a machine translation (MT) system where both techniques are implemented.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The approach shows significant improvement over the baseline for MT systems with limited training data and structural improvement for MT systems trained on Europarl.</text>
              <doc_id>5</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Statistical MT (SMT) and rule-based MT (RBMT) have complimentary strengths and combining their output can improve translation quality. The underlying models in SMT lack linguistic sophistication when compared to RBMT systems and there is a trend towards incorporating more linguistic knowledge by creating hybrid systems that can exploit the linguistic knowledge contained in hand-crafted rules and the knowledge extracted from large amounts of text. Hierarchical phrases (Chiang, 2005) are encoded in a tree structure just as linguistic trees. Most RBMT systems also encode the analysis of a sentence in a tree. The rules generating hierarchical trees are inferred from unlabeled corpora and RBMT systems use hand-crafted rules based in linguistic knowledge. While the trees are generated differently, alignments between nodes and subtrees in the generation phase can be computed. Based on the computed alignments, substitution can be performed between the trees.
The automatic post-editing approach proposed in this paper is based on structural similarity. The tree structures are aligned and subtree substitution based on the similarity of subtrees performed. This knowledge-poor approach is compatible with the surface-near nature of SMT systems, does not require other information than what is available in the output, and ensures that the approach is generic so it can, in principle, be applied to any language pair.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Statistical MT (SMT) and rule-based MT (RBMT) have complimentary strengths and combining their output can improve translation quality.</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The underlying models in SMT lack linguistic sophistication when compared to RBMT systems and there is a trend towards incorporating more linguistic knowledge by creating hybrid systems that can exploit the linguistic knowledge contained in hand-crafted rules and the knowledge extracted from large amounts of text.</text>
              <doc_id>7</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Hierarchical phrases (Chiang, 2005) are encoded in a tree structure just as linguistic trees.</text>
              <doc_id>8</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Most RBMT systems also encode the analysis of a sentence in a tree.</text>
              <doc_id>9</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The rules generating hierarchical trees are inferred from unlabeled corpora and RBMT systems use hand-crafted rules based in linguistic knowledge.</text>
              <doc_id>10</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>While the trees are generated differently, alignments between nodes and subtrees in the generation phase can be computed.</text>
              <doc_id>11</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Based on the computed alignments, substitution can be performed between the trees.</text>
              <doc_id>12</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The automatic post-editing approach proposed in this paper is based on structural similarity.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The tree structures are aligned and subtree substitution based on the similarity of subtrees performed.</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This knowledge-poor approach is compatible with the surface-near nature of SMT systems, does not require other information than what is available in the output, and ensures that the approach is generic so it can, in principle, be applied to any language pair.</text>
              <doc_id>15</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Hybrid Machine Translation</title>
        <text>Hybrid machine translation (HMT) is a paradigm that seeks to combine the strengths of SMT and RBMT. The different approaches have complementary strengths and weaknesses (Thurmair, 2009) which have led to the emergence of HMT as a subfield in machine translation research.
The strength of SMT is robustness - i.e. it will always produce an output - and fluency due to the use of language models. A weakness of SMT is the lack of explicit linguistic knowledge, which make translation phenomena requiring such information, e.g. long-distance dependencies, difficult to handle. RBMT systems translate more accurately in cases without parse failure, since they can take more information into account e.g. morphological, syntactic or semantic information, where SMT only uses surface forms. RBMT often suffer from lack of robustness when parsing fails and 77
Jeg [jeg] 1S NOM @SUBJ #1-&gt;2 arbejder [arbejde] &lt;mv&gt; V PR AKT @FS-STA #2-&gt;0 hjemme [hjemme] &lt;aloc&gt; ADV LOC @&lt;ADVL #3-&gt;2 . [.] PU @PU #4-&gt;0
in lexical selection in transfer. RBMT systems are also very costly to build, and maintenance and development can be very complex e.g. due to the interdependency of rules.
The post-editing approach attempts to incorporate the linguistic knowledge encoded in targetside dependency trees into hierarchical trees produced by an SMT system.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Hybrid machine translation (HMT) is a paradigm that seeks to combine the strengths of SMT and RBMT.</text>
              <doc_id>16</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The different approaches have complementary strengths and weaknesses (Thurmair, 2009) which have led to the emergence of HMT as a subfield in machine translation research.</text>
              <doc_id>17</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The strength of SMT is robustness - i.e. it will always produce an output - and fluency due to the use of language models.</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A weakness of SMT is the lack of explicit linguistic knowledge, which make translation phenomena requiring such information, e.g. long-distance dependencies, difficult to handle.</text>
              <doc_id>19</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>RBMT systems translate more accurately in cases without parse failure, since they can take more information into account e.g. morphological, syntactic or semantic information, where SMT only uses surface forms.</text>
              <doc_id>20</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>RBMT often suffer from lack of robustness when parsing fails and 77</text>
              <doc_id>21</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Jeg [jeg] 1S NOM @SUBJ #1-&gt;2 arbejder [arbejde] &lt;mv&gt; V PR AKT @FS-STA #2-&gt;0 hjemme [hjemme] &lt;aloc&gt; ADV LOC @&lt;ADVL #3-&gt;2 .</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>[.</text>
              <doc_id>23</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>] PU @PU #4-&gt;0</text>
              <doc_id>24</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>in lexical selection in transfer.</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>RBMT systems are also very costly to build, and maintenance and development can be very complex e.g. due to the interdependency of rules.</text>
              <doc_id>26</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The post-editing approach attempts to incorporate the linguistic knowledge encoded in targetside dependency trees into hierarchical trees produced by an SMT system.</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Related work</title>
            <text>System combinations by coupling MT systems serially or in parallel have been attempted before e.g. via hypothesis selection (Hildebrand and Vogel, 2008), by combining translation hypotheses locally using POS tags (Federmann et al., 2010) or by statistical post-editing (SPE) (Simard et al., 2007). In hypothesis selection approaches, a number of MT systems produce translations for an n- best list and use a re-ranking module to rescore the translations. Using this approach, the best improvements are achieved with a large number of systems running in parallel and this is not feasible in a practical application, mostly due to the computational resources required by the component systems. The translations will also not be better than the one produced by the best component system. Tighter integration of rule-based and statistical approaches have also been proposed: Adding probabilities to parse trees, pre-translation word reordering, enriching the phrase table with output phrases from a rule-based system (Eisele et al., Figure 2: Disambiguated CG representation for I work at home. Dependency annotation is indicated by the #-character.
2008), creating training data from RBMT systems etc. The factored translation models also present a way to integrate rule-based parsing systems.
The automatic post-editing approach proposed here does not exactly fit the classification of parallel coupling approaches in Thurmair (2009). Other coupling architectures with post-editing work on words or phrases and generate confusion networks or add more information to identify substitution candidates, while the units focused on here are graphs and no additional information is added to the MT output. This approach does select a skeleton upon which transformations are conducted as in Rosti et al. (2007) and requires the RBMT system to generate a target side language analysis which must be available to the post-editing systems, but does not require a new syntactic analysis of noisy MT output. The architecture of the hybrid system used in this paper is parallel coupling with post-editing. A diagram of the implemented systems can be seen in Figure 1. The dark grey boxes represent pre-existing modules and open source software and the light grey boxes represent the additional modules developed to implement the post-editing approach.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>System combinations by coupling MT systems serially or in parallel have been attempted before e.g. via hypothesis selection (Hildebrand and Vogel, 2008), by combining translation hypotheses locally using POS tags (Federmann et al., 2010) or by statistical post-editing (SPE) (Simard et al., 2007).</text>
                  <doc_id>28</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In hypothesis selection approaches, a number of MT systems produce translations for an n- best list and use a re-ranking module to rescore the translations.</text>
                  <doc_id>29</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Using this approach, the best improvements are achieved with a large number of systems running in parallel and this is not feasible in a practical application, mostly due to the computational resources required by the component systems.</text>
                  <doc_id>30</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The translations will also not be better than the one produced by the best component system.</text>
                  <doc_id>31</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Tighter integration of rule-based and statistical approaches have also been proposed: Adding probabilities to parse trees, pre-translation word reordering, enriching the phrase table with output phrases from a rule-based system (Eisele et al., Figure 2: Disambiguated CG representation for I work at home.</text>
                  <doc_id>32</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Dependency annotation is indicated by the #-character.</text>
                  <doc_id>33</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2008), creating training data from RBMT systems etc.</text>
                  <doc_id>34</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The factored translation models also present a way to integrate rule-based parsing systems.</text>
                  <doc_id>35</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The automatic post-editing approach proposed here does not exactly fit the classification of parallel coupling approaches in Thurmair (2009).</text>
                  <doc_id>36</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Other coupling architectures with post-editing work on words or phrases and generate confusion networks or add more information to identify substitution candidates, while the units focused on here are graphs and no additional information is added to the MT output.</text>
                  <doc_id>37</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This approach does select a skeleton upon which transformations are conducted as in Rosti et al. (2007) and requires the RBMT system to generate a target side language analysis which must be available to the post-editing systems, but does not require a new syntactic analysis of noisy MT output.</text>
                  <doc_id>38</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The architecture of the hybrid system used in this paper is parallel coupling with post-editing.</text>
                  <doc_id>39</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>A diagram of the implemented systems can be seen in Figure 1.</text>
                  <doc_id>40</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The dark grey boxes represent pre-existing modules and open source software and the light grey boxes represent the additional modules developed to implement the post-editing approach.</text>
                  <doc_id>41</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 RBMT Component</title>
            <text>The Danish to English translation engine in GramTrans (Bick, 2007) is called through an API. The output is a constraint grammar (CG) analysis on the target language side after all transfer and target side transformation rules have been applied. Example output is shown in Figure 2. In the analysis, dependency information is provided and they form the basis for creating the tree used for structural similarity computation. Part-of-speech tags, source and target surface structure, sentence position and dependency information are extracted from the CG analysis.
GramTrans is created to be robust and produce as many dependency markings as possible to be used in later translation stages. Errors in the assignment of functional tags propagate to the dependency level and can result in markings that will produce a dependency tree and a number of 78
unconnected subgraphs with circularities. This presents a problem if the dependency markings are the basis for creating a dependency tree because it is not straight-forward to reattach a subgraph correctly, when the grammatical tags cannot be relied upon.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The Danish to English translation engine in GramTrans (Bick, 2007) is called through an API.</text>
                  <doc_id>42</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The output is a constraint grammar (CG) analysis on the target language side after all transfer and target side transformation rules have been applied.</text>
                  <doc_id>43</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Example output is shown in Figure 2.</text>
                  <doc_id>44</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In the analysis, dependency information is provided and they form the basis for creating the tree used for structural similarity computation.</text>
                  <doc_id>45</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Part-of-speech tags, source and target surface structure, sentence position and dependency information are extracted from the CG analysis.</text>
                  <doc_id>46</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>GramTrans is created to be robust and produce as many dependency markings as possible to be used in later translation stages.</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Errors in the assignment of functional tags propagate to the dependency level and can result in markings that will produce a dependency tree and a number of 78</text>
                  <doc_id>48</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>unconnected subgraphs with circularities.</text>
                  <doc_id>49</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This presents a problem if the dependency markings are the basis for creating a dependency tree because it is not straight-forward to reattach a subgraph correctly, when the grammatical tags cannot be relied upon.</text>
                  <doc_id>50</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>2.3 SMT Component</title>
            <text>A CKY+ algorithm for chart decoding is implemented in Moses (Koehn et al., 2007) for treebased models and is used as the SMT component system in this paper.
Hierarchical phrases are phrases that can contain subphrases, i.e. a hierarchical phrase contains non-terminal symbols. An example rule from Danish to English:
X 1 i &#248;vrigt X 2 &#8722;&#8594; moreover, X 1 X 2
X n is a nonterminal and the subscript identifies how the nonterminals are aligned. The hierarchical phrases are learned from bitext with unannotated data and are formally productions from a synchronous context-free grammar (SCFG) and can be viewed as a move towards syntax-based SMT (Chiang, 2005). Since hierarchical phrases are not linguistic, Chiang makes a distinction between linguistically syntax-based MT and formally syntax-based MT where hierarchical models fall in the latter category because the structures they are defined over are not linguistically informed, i.e. unannotated bitexts.
A hierarchical model is based on a SCFG and the elementary structures are rewrite rules:
X &#8722;&#8594; &#12296;&#947;, &#945;, &#8764;&#12297;
As above, X is a nonterminal, &#947; and &#945; are both strings of terminals and nonterminals and &#8764; is a 1-to-1 correspondence between nonterminals in &#947; and &#945;. As in shown previously, the convention is to use subscripts to represent &#8764;.
To maintain the advantage of the phrase-based approach, glue rules are added to the rules that are otherwise learned from raw data:
S &#8722;&#8594; &#12296;S 1 X 2 , S 1 X 2 &#12297;
S &#8722;&#8594; &#12296;X 1 , X 1 &#12297;
Only these rewrite rules contain the nonterminal S. These rules are added to give the model
the option of combining partial hypotheses serially and they make the hierarchical model as robust as the traditional phrase-based approaches.
The Moses chart decoder was modified to output trace information from which the n-best hierarchical trees can be reconstructed. The trace information contains the derivations which produce the translation hypotheses. The sentence&#8211;aligned Danish-English part of Europarl (Koehn, 2005) was used for training, and to tune parameters with MERT, the test set from the NAACL WMT 2006 was used (Koehn and Monz, 2006). GIZA++ aligns hierarchical phrases which were extracted by Moses to train a translation model and a language model was trained with SRILM (Stolcke, 2002). Moses was trained using the Experimental Management System (EMS) (Koehn, 2010) and the configuration followed the standard guidelines in the syntax tutorial. 1 To train SRILM, the English side of Europarl was used.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>A CKY+ algorithm for chart decoding is implemented in Moses (Koehn et al., 2007) for treebased models and is used as the SMT component system in this paper.</text>
                  <doc_id>51</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Hierarchical phrases are phrases that can contain subphrases, i.e. a hierarchical phrase contains non-terminal symbols.</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>An example rule from Danish to English:</text>
                  <doc_id>53</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>X 1 i &#248;vrigt X 2 &#8722;&#8594; moreover, X 1 X 2</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>X n is a nonterminal and the subscript identifies how the nonterminals are aligned.</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The hierarchical phrases are learned from bitext with unannotated data and are formally productions from a synchronous context-free grammar (SCFG) and can be viewed as a move towards syntax-based SMT (Chiang, 2005).</text>
                  <doc_id>56</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Since hierarchical phrases are not linguistic, Chiang makes a distinction between linguistically syntax-based MT and formally syntax-based MT where hierarchical models fall in the latter category because the structures they are defined over are not linguistically informed, i.e. unannotated bitexts.</text>
                  <doc_id>57</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A hierarchical model is based on a SCFG and the elementary structures are rewrite rules:</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>X &#8722;&#8594; &#12296;&#947;, &#945;, &#8764;&#12297;</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As above, X is a nonterminal, &#947; and &#945; are both strings of terminals and nonterminals and &#8764; is a 1-to-1 correspondence between nonterminals in &#947; and &#945;.</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As in shown previously, the convention is to use subscripts to represent &#8764;.</text>
                  <doc_id>61</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To maintain the advantage of the phrase-based approach, glue rules are added to the rules that are otherwise learned from raw data:</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>S &#8722;&#8594; &#12296;S 1 X 2 , S 1 X 2 &#12297;</text>
                  <doc_id>63</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>S &#8722;&#8594; &#12296;X 1 , X 1 &#12297;</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Only these rewrite rules contain the nonterminal S.</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These rules are added to give the model</text>
                  <doc_id>66</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the option of combining partial hypotheses serially and they make the hierarchical model as robust as the traditional phrase-based approaches.</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The Moses chart decoder was modified to output trace information from which the n-best hierarchical trees can be reconstructed.</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The trace information contains the derivations which produce the translation hypotheses.</text>
                  <doc_id>69</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The sentence&#8211;aligned Danish-English part of Europarl (Koehn, 2005) was used for training, and to tune parameters with MERT, the test set from the NAACL WMT 2006 was used (Koehn and Monz, 2006).</text>
                  <doc_id>70</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>GIZA++ aligns hierarchical phrases which were extracted by Moses to train a translation model and a language model was trained with SRILM (Stolcke, 2002).</text>
                  <doc_id>71</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Moses was trained using the Experimental Management System (EMS) (Koehn, 2010) and the configuration followed the standard guidelines in the syntax tutorial.</text>
                  <doc_id>72</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>1 To train SRILM, the English side of Europarl was used.</text>
                  <doc_id>73</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Matching Approach</title>
        <text>The post-editing approach relies on structures output by the component systems. It is necessary to find similar structures to perform subtree substitution. Matching structures is a problem in several application areas such as semantic web, schema and ontology integration, query mediation etc. Structures include database schemas, directories, diagrams and graphs. Shvaiko and Euzenat (2005) provide a comprehensive survey of matching techniques. The matching operation determines an alignment between two structures and an alignment is a set of matching elements. A matching element is a quintuple: &#12296;id, e, e &#8242; , n, R&#12297;:
id Unique id.
e, e &#8242; Elements from different structures.
n Confidence measure. 1 http://www.statmt.org/moses/?n=Moses.
SyntaxTutorial
S
sat S X
the the
X
cat
R The relation holding between the elements.
The resources that can be used in the matching process are shown in Figure 3. o and o &#8242; are the structures to be matched, A is an optional existing alignment, r is external resources, p is parameters, weights and thresholds and A &#8242; is the set of matching elements created by the process. In this paper, only matching elements with an equivalence relation (=) are used.
The returned alignment can be a new alignment or a refinement of A. o will be a dependency tree and o &#8242; the hierachical trees from the SMT component system. To compute the initial alignment A between hierarchical and dependency trees, the source to target language phrase alignment output by the component systems is used. So the initial alignment between leaf nodes in target-side trees are computed over the alignment to the source language.
An important decision regarding this hybrid approach is how to compute the alignment and the size of the substituted subtrees. Irrespective of which technique is chosen to compute structural similarity, the resulting alignment should be refined to contain matching elements between internal nodes as shown in Figure 4.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The post-editing approach relies on structures output by the component systems.</text>
              <doc_id>74</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It is necessary to find similar structures to perform subtree substitution.</text>
              <doc_id>75</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Matching structures is a problem in several application areas such as semantic web, schema and ontology integration, query mediation etc. Structures include database schemas, directories, diagrams and graphs.</text>
              <doc_id>76</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Shvaiko and Euzenat (2005) provide a comprehensive survey of matching techniques.</text>
              <doc_id>77</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The matching operation determines an alignment between two structures and an alignment is a set of matching elements.</text>
              <doc_id>78</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>A matching element is a quintuple: &#12296;id, e, e &#8242; , n, R&#12297;:</text>
              <doc_id>79</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>id Unique id.</text>
              <doc_id>80</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e, e &#8242; Elements from different structures.</text>
              <doc_id>81</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>n Confidence measure.</text>
              <doc_id>82</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1 http://www.statmt.org/moses/?n=Moses.</text>
              <doc_id>83</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>SyntaxTutorial</text>
              <doc_id>84</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S</text>
              <doc_id>85</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>sat S X</text>
              <doc_id>86</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the the</text>
              <doc_id>87</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>X</text>
              <doc_id>88</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>cat</text>
              <doc_id>89</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>R The relation holding between the elements.</text>
              <doc_id>90</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The resources that can be used in the matching process are shown in Figure 3. o and o &#8242; are the structures to be matched, A is an optional existing alignment, r is external resources, p is parameters, weights and thresholds and A &#8242; is the set of matching elements created by the process.</text>
              <doc_id>91</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, only matching elements with an equivalence relation (=) are used.</text>
              <doc_id>92</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The returned alignment can be a new alignment or a refinement of A. o will be a dependency tree and o &#8242; the hierachical trees from the SMT component system.</text>
              <doc_id>93</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To compute the initial alignment A between hierarchical and dependency trees, the source to target language phrase alignment output by the component systems is used.</text>
              <doc_id>94</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>So the initial alignment between leaf nodes in target-side trees are computed over the alignment to the source language.</text>
              <doc_id>95</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>An important decision regarding this hybrid approach is how to compute the alignment and the size of the substituted subtrees.</text>
              <doc_id>96</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Irrespective of which technique is chosen to compute structural similarity, the resulting alignment should be refined to contain matching elements between internal nodes as shown in Figure 4.</text>
              <doc_id>97</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Alignment Challenges</title>
            <text>The change made to the chart decoder to output the n-best trace information is simple and does not output the alignment information. Currently, the tree extraction module computes an alignment between the source and target language phrases. The segmentation of words into phrases done by Moses does not always correspond to the word-based segmentation required by the CG parser; phrases recognised by the CG parser rarely correspond to phrases in Moses and the hierarchical phrase alignment is not easy to handle.
Aligning hierarchical phrases like (a) in Figure 5 is not complicated. The ordering is identical and the Danish word offentligg&#248;res is aligned to will be published. The numbers 1-3 refer to the alignment of non-terminal nodes based on phrase positions.
It is more complicated to align (b) in Figure 5. There are two methods of handling this type of alignment appropriate for the component systems. Because there are an equal number of tokens in the English phrase and Danish phrase, aligning the tokens 1-1 monotonically would be a solution that, in this case, results in a correct alignment.
Another approach relies on weak word reordering between Danish and English and would align findes with there are. This reduces the alignment problem to aligning vi der with we. In this case, the alignment is noisy, but usable for creating matching elements. Both approaches are implemented in the hybrid system and the first approach supercedes the second due to the advantage of correlating with the CG approach.
An initial element-level alignment between nodes in a dependency tree and a hierarchical tree is computed over the source language and creates a set of matching elements containing aligned nodes.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The change made to the chart decoder to output the n-best trace information is simple and does not output the alignment information.</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Currently, the tree extraction module computes an alignment between the source and target language phrases.</text>
                  <doc_id>99</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The segmentation of words into phrases done by Moses does not always correspond to the word-based segmentation required by the CG parser; phrases recognised by the CG parser rarely correspond to phrases in Moses and the hierarchical phrase alignment is not easy to handle.</text>
                  <doc_id>100</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Aligning hierarchical phrases like (a) in Figure 5 is not complicated.</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The ordering is identical and the Danish word offentligg&#248;res is aligned to will be published.</text>
                  <doc_id>102</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The numbers 1-3 refer to the alignment of non-terminal nodes based on phrase positions.</text>
                  <doc_id>103</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It is more complicated to align (b) in Figure 5.</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>There are two methods of handling this type of alignment appropriate for the component systems.</text>
                  <doc_id>105</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Because there are an equal number of tokens in the English phrase and Danish phrase, aligning the tokens 1-1 monotonically would be a solution that, in this case, results in a correct alignment.</text>
                  <doc_id>106</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Another approach relies on weak word reordering between Danish and English and would align findes with there are.</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This reduces the alignment problem to aligning vi der with we.</text>
                  <doc_id>108</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In this case, the alignment is noisy, but usable for creating matching elements.</text>
                  <doc_id>109</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Both approaches are implemented in the hybrid system and the first approach supercedes the second due to the advantage of correlating with the CG approach.</text>
                  <doc_id>110</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>An initial element-level alignment between nodes in a dependency tree and a hierarchical tree is computed over the source language and creates a set of matching elements containing aligned nodes.</text>
                  <doc_id>111</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Alignment Refinement</title>
            <text>Between a dependency and an hierarchical tree, an element-level alignment needs to be refined to 80
(a) offentligg&#248;res X : X -&gt; will be published X : 1-3
(b) vi X der X findes : X -&gt; X, we X there are : 1-3 3-0
a structure-level alignment similar to the one in Figure 4. Not all matching elements in an initial alignment should be refined e.g. if both nodes in a matching element are leaf nodes, no refinement is needed. Criteria for selecting initial matching elements for refinement are needed.
In the RBMT output, there are no indications of where the parser encountered problems. If a surface form is an out-of-vocabulary (OOV) word, the morphological analyser is used to assign a lexical category based on the word form, hypothesise additional tags based on the analysis and proceed with parsing. In the SMT output, an OOV marker is appended to a surface form to indicate that the word has not been translated. The marker gives an indication of where enriching a hierarchical tree with RBMT output can result in improvement of translation quality.
Based on these observations, hierarchical trees are chosen to function as skeletons. Substituting dependency subtrees into a hierarchical tree is more straightforward than using dependency trees as skeletons. It was not possible to identify head-dependent relations based solely on the information contained in hierarchical subtrees while removing subtrees from hierarchical trees and inserting dependency subtrees does not destroy linguistic information in the tree and dependency subtrees can easily be transformed into a hierarchical-style subtree.
Leaves Based on the OOV marker, a matching technique based on leaf nodes is implemented to refine matching elements and based on this alignment, substitute hierarchical subtrees with dependency subtrees.
The dependency subtree is identified by collecting all descendants of a node. The descendants are handled as leaf nodes because both leaf and nonterminal nodes contain surface forms in a dependecy tree.
The dependency trees provided by GramTrans are not always projective. Subtrees may not represent a continuous surface structure and a continuous subtree must be isolated before an alignment between subtrees can be found because the hierarchical trees resemble phrase structure trees and discontinuous phrases are handled using glue rules.
To identify the corresponding subtree in the hierarchical tree, the matching elements that contain the nodes in the dependency subtree are collected and a path from each leaf node to the root node is computed. The intersection of nodes is retrieved and the root node of the subtree identified as the lowest node present in all paths. It is not always possible to find a common root node besides the root node of the entire tree. To prevent the loss of a high amount of structural information, the root node cannot be replaced or deleted.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Between a dependency and an hierarchical tree, an element-level alignment needs to be refined to 80</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(a) offentligg&#248;res X : X -&gt; will be published X : 1-3</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(b) vi X der X findes : X -&gt; X, we X there are : 1-3 3-0</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>a structure-level alignment similar to the one in Figure 4.</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Not all matching elements in an initial alignment should be refined e.g. if both nodes in a matching element are leaf nodes, no refinement is needed.</text>
                  <doc_id>116</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Criteria for selecting initial matching elements for refinement are needed.</text>
                  <doc_id>117</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In the RBMT output, there are no indications of where the parser encountered problems.</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If a surface form is an out-of-vocabulary (OOV) word, the morphological analyser is used to assign a lexical category based on the word form, hypothesise additional tags based on the analysis and proceed with parsing.</text>
                  <doc_id>119</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In the SMT output, an OOV marker is appended to a surface form to indicate that the word has not been translated.</text>
                  <doc_id>120</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The marker gives an indication of where enriching a hierarchical tree with RBMT output can result in improvement of translation quality.</text>
                  <doc_id>121</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Based on these observations, hierarchical trees are chosen to function as skeletons.</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Substituting dependency subtrees into a hierarchical tree is more straightforward than using dependency trees as skeletons.</text>
                  <doc_id>123</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It was not possible to identify head-dependent relations based solely on the information contained in hierarchical subtrees while removing subtrees from hierarchical trees and inserting dependency subtrees does not destroy linguistic information in the tree and dependency subtrees can easily be transformed into a hierarchical-style subtree.</text>
                  <doc_id>124</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Leaves Based on the OOV marker, a matching technique based on leaf nodes is implemented to refine matching elements and based on this alignment, substitute hierarchical subtrees with dependency subtrees.</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The dependency subtree is identified by collecting all descendants of a node.</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The descendants are handled as leaf nodes because both leaf and nonterminal nodes contain surface forms in a dependecy tree.</text>
                  <doc_id>127</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The dependency trees provided by GramTrans are not always projective.</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Subtrees may not represent a continuous surface structure and a continuous subtree must be isolated before an alignment between subtrees can be found because the hierarchical trees resemble phrase structure trees and discontinuous phrases are handled using glue rules.</text>
                  <doc_id>129</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To identify the corresponding subtree in the hierarchical tree, the matching elements that contain the nodes in the dependency subtree are collected and a path from each leaf node to the root node is computed.</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The intersection of nodes is retrieved and the root node of the subtree identified as the lowest node present in all paths.</text>
                  <doc_id>131</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It is not always possible to find a common root node besides the root node of the entire tree.</text>
                  <doc_id>132</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>To prevent the loss of a high amount of structural information, the root node cannot be replaced or deleted.</text>
                  <doc_id>133</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Substitution based on an edit script</title>
            <text>An algorithm for computing structural similarity is the Tree Edit Distance (TED) algorithm, which computes how many operations are necessary for transforming one tree into another tree. Following Zhang and Shasha (1989) and Bille (2005), the operations are defined on nodes and the trees are ordered, labelled trees. There are 3 different edit operations:
rename Change the label of a node in a tree.
delete Remove a node n from a tree. Insert the children of n as children of the parent of n so the sequence of children are preserved. The deleted node may not be the root node.
insert Insert a node as the child of a node n in a tree. A subsequence of children of n are inserted as children of the new node so the sequence of children are preserved. An insertion is the inverse operation of a deletion.
A cost function is defined for each operation. The goal is to find the sequence of edit operations that turns a tree T 1 into another tree T 2 with minimum cost. The sequence of edit operations is called an edit script and the cost of the optimal edit script is the tree edit distance.
The cost functions should return a distance metric and satisfy the following conditions:
1. &#947;(i &#8594; j) &#8805; 0 and &#947;(i &#8594; i) = 0 81
2. &#947;(i &#8594; j) = &#947;(j &#8594; i)
3. &#947;(i &#8594; k) &#8804; &#947;(i &#8594; j) + &#947;(j &#8594; k)
&#947; is the cost of an edit operation. The edit distance mapping is a representation of an edit script. A rename operation is represented as (i 1 &#8594; j 2 ) where the subscript denotes that the nodes i and j belong to different trees. (i 1 &#8594; &#603;) represents a deletion and (&#603; &#8594; j 2 ) an insertion.
The cost of an edit distance mapping is given by:
&#947;(M) = &#8721; &#947;(i &#8594; j)+ &#8721; &#947;(i &#8594; &#603;)+ &#8721; &#947;(&#603; &#8594; j)
i&#8712;T 1 j&#8712;T 2 (i,j)&#8712;M
j &#8712; T 2 means j is in the set of nodes in T 2 .
It is important to note that the trees are ordered trees. The unordered version of the tree edit distance problem is NP-hard, while polynomial algorithms based on dynamic programming exist for ordered trees.
The algorithm does not require an input alignment or external resources. The cost functions for deletion, insertion and renaming must be defined on the information present in the nodes and a unique id must be assigned to the nodes. This id is assigned by traversing the tree depth-first and assigning an integer as id. The algorithm visits each node in the trees in post order and determines based on the cost assigned by the cost functions, which edit operation should be performed.
To generate matching elements that align dependency nodes to nonterminal hierarchical nodes, cost functions for edit operations are modified to assign a lower cost to rename operations where one of the nodes is a hierarchical nonterminal node. If two nodes have the same target and source phrase, a rename operation does not incur any cost and neither does the renaming of untranslated phrases. This ensures that matching elements from the initial alignment that does not require refinement are not altered. Also, if the source is the same and the difference in sentence position is no more than five, the renaming cost is reduced. Experiments showed that a window of five words was necessary to account for differences in sentence position and prevent alignment to nodes later in the sentence with the same source phrase.
This technique is independent of the OOV marker and creates a structure-level alignment. The substitutions performed can be of very high quality but some untranslated words might not be handled. If the system finds any OOV words in the hierachical tree after substitution, a rename operation is carried out on the node.
The extracted matching elements are noisy because they rely on the noisy source to target language alignment and the RBMT engine can also produce an inaccurate translation making the substitution counter-productive. Further limitations on the cost functions become too restrictive and produce too few matching elements. To avoid some of the noise, all permutations of applying substitutions based on the edit script are generated, re-ranked and the highest scoring hypothesis chosen as the translation.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>An algorithm for computing structural similarity is the Tree Edit Distance (TED) algorithm, which computes how many operations are necessary for transforming one tree into another tree.</text>
                  <doc_id>134</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Following Zhang and Shasha (1989) and Bille (2005), the operations are defined on nodes and the trees are ordered, labelled trees.</text>
                  <doc_id>135</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>There are 3 different edit operations:</text>
                  <doc_id>136</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>rename Change the label of a node in a tree.</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>delete Remove a node n from a tree.</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Insert the children of n as children of the parent of n so the sequence of children are preserved.</text>
                  <doc_id>139</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The deleted node may not be the root node.</text>
                  <doc_id>140</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>insert Insert a node as the child of a node n in a tree.</text>
                  <doc_id>141</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A subsequence of children of n are inserted as children of the new node so the sequence of children are preserved.</text>
                  <doc_id>142</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>An insertion is the inverse operation of a deletion.</text>
                  <doc_id>143</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A cost function is defined for each operation.</text>
                  <doc_id>144</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The goal is to find the sequence of edit operations that turns a tree T 1 into another tree T 2 with minimum cost.</text>
                  <doc_id>145</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The sequence of edit operations is called an edit script and the cost of the optimal edit script is the tree edit distance.</text>
                  <doc_id>146</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The cost functions should return a distance metric and satisfy the following conditions:</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1. &#947;(i &#8594; j) &#8805; 0 and &#947;(i &#8594; i) = 0 81</text>
                  <doc_id>148</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2. &#947;(i &#8594; j) = &#947;(j &#8594; i)</text>
                  <doc_id>149</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3. &#947;(i &#8594; k) &#8804; &#947;(i &#8594; j) + &#947;(j &#8594; k)</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#947; is the cost of an edit operation.</text>
                  <doc_id>151</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The edit distance mapping is a representation of an edit script.</text>
                  <doc_id>152</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>A rename operation is represented as (i 1 &#8594; j 2 ) where the subscript denotes that the nodes i and j belong to different trees.</text>
                  <doc_id>153</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>(i 1 &#8594; &#603;) represents a deletion and (&#603; &#8594; j 2 ) an insertion.</text>
                  <doc_id>154</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The cost of an edit distance mapping is given by:</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#947;(M) = &#8721; &#947;(i &#8594; j)+ &#8721; &#947;(i &#8594; &#603;)+ &#8721; &#947;(&#603; &#8594; j)</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i&#8712;T 1 j&#8712;T 2 (i,j)&#8712;M</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j &#8712; T 2 means j is in the set of nodes in T 2 .</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It is important to note that the trees are ordered trees.</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The unordered version of the tree edit distance problem is NP-hard, while polynomial algorithms based on dynamic programming exist for ordered trees.</text>
                  <doc_id>160</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The algorithm does not require an input alignment or external resources.</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The cost functions for deletion, insertion and renaming must be defined on the information present in the nodes and a unique id must be assigned to the nodes.</text>
                  <doc_id>162</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This id is assigned by traversing the tree depth-first and assigning an integer as id.</text>
                  <doc_id>163</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The algorithm visits each node in the trees in post order and determines based on the cost assigned by the cost functions, which edit operation should be performed.</text>
                  <doc_id>164</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To generate matching elements that align dependency nodes to nonterminal hierarchical nodes, cost functions for edit operations are modified to assign a lower cost to rename operations where one of the nodes is a hierarchical nonterminal node.</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If two nodes have the same target and source phrase, a rename operation does not incur any cost and neither does the renaming of untranslated phrases.</text>
                  <doc_id>166</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This ensures that matching elements from the initial alignment that does not require refinement are not altered.</text>
                  <doc_id>167</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Also, if the source is the same and the difference in sentence position is no more than five, the renaming cost is reduced.</text>
                  <doc_id>168</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Experiments showed that a window of five words was necessary to account for differences in sentence position and prevent alignment to nodes later in the sentence with the same source phrase.</text>
                  <doc_id>169</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>This technique is independent of the OOV marker and creates a structure-level alignment.</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The substitutions performed can be of very high quality but some untranslated words might not be handled.</text>
                  <doc_id>171</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>If the system finds any OOV words in the hierachical tree after substitution, a rename operation is carried out on the node.</text>
                  <doc_id>172</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The extracted matching elements are noisy because they rely on the noisy source to target language alignment and the RBMT engine can also produce an inaccurate translation making the substitution counter-productive.</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Further limitations on the cost functions become too restrictive and produce too few matching elements.</text>
                  <doc_id>174</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>To avoid some of the noise, all permutations of applying substitutions based on the edit script are generated, re-ranked and the highest scoring hypothesis chosen as the translation.</text>
                  <doc_id>175</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>3.4 Generation</title>
            <text>To ensure that the surface string generated from the newly created tree will have the correct word ordering, the dependency subtree is transformed before being inserted into the hierarchical tree. To create the insertion tree, the dependency nodes are inserted as leaf nodes of a dummy node. The dummy node is inserted before the root node of the aligned hierarchical subtree and the information on the root node copied to the new node. Subsequently, the hierarchical nodes are removed from the tree. If both nodes in a matching element are leaf nodes, the hierarchical node is relabeled with information from the dependency node.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>To ensure that the surface string generated from the newly created tree will have the correct word ordering, the dependency subtree is transformed before being inserted into the hierarchical tree.</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To create the insertion tree, the dependency nodes are inserted as leaf nodes of a dummy node.</text>
                  <doc_id>177</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The dummy node is inserted before the root node of the aligned hierarchical subtree and the information on the root node copied to the new node.</text>
                  <doc_id>178</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Subsequently, the hierarchical nodes are removed from the tree.</text>
                  <doc_id>179</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>If both nodes in a matching element are leaf nodes, the hierarchical node is relabeled with information from the dependency node.</text>
                  <doc_id>180</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Experiments</title>
        <text>The experiments have been conducted between Danish and English. The language model trained with EMS is used to re-rank translation alternatives. BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) scores will be reported.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The experiments have been conducted between Danish and English.</text>
              <doc_id>181</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The language model trained with EMS is used to re-rank translation alternatives.</text>
              <doc_id>182</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) scores will be reported.</text>
              <doc_id>183</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Experimental Setup</title>
            <text>Two sets of five experiments have been conducted. The first set of experiments use the initial 100,000 lines from Europarl for training Moses and the second set of experiments use the full Europarl corpus of ca. 1.8 mio sentences. The SMT baseline is the hierarchical version of Moses.
TED Skeleton Selection The impact of choosing the translation hypothesis with a minimal edit 82
distance to the dependency tree from the rulebased system is investigated. In one setting, the cost functions adhere to the constrictions of computing a distance metric. Two settings test the impact of biasing the insertion and deletion cost functions to assign a lower cost to inserting/deleting nonterminals, i.e. turning the dependency tree into the hierarchical tree and vice versa.
TED is computed for 20 translation hypotheses and the best performing setting reported.
Leaves An experiment using the leaves technique has been conducted. The experiment is performed using the best hypothesis from Moses and also using TED to chose the most structurally similar skeleton. The best setting will be reported.
Lexical substitution To be able to compare a more naive approach, subtree substitution based on the initial element-level alignment between leaf nodes is used. In this approach, a subtree is one node. The technique is identical to using the RBMT lexicon to lookup untranslated words and inserting them in the translation.
TED-R An experiment where the mappings that represent a rename operation, which are produced during TED computation, are extracted and used as matching elements is conducted. Mapping elements containing only punctuation or the root node of either tree are discarded. All combinations of substitutions based on the extracted matching elements are performed and the highest ranking hypothesis according to a language model is chosen as the final translation. The extracted matching elements may not incorporate all the untranslated nodes. All untranslated nodes are subsequently translated using lexical substitution as mentioned above. The subtrees inserted into the hierarchical tree will undergo the same transformation as the subtrees inserted using the leaves technique.
This experiment is evaluated using both the 1- best hierarchical tree as skeleton and choosing the skeleton using TED. All three settings are tested and the best performing experiment reported.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Two sets of five experiments have been conducted.</text>
                  <doc_id>184</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The first set of experiments use the initial 100,000 lines from Europarl for training Moses and the second set of experiments use the full Europarl corpus of ca.</text>
                  <doc_id>185</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>1.8 mio sentences.</text>
                  <doc_id>186</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The SMT baseline is the hierarchical version of Moses.</text>
                  <doc_id>187</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>TED Skeleton Selection The impact of choosing the translation hypothesis with a minimal edit 82</text>
                  <doc_id>188</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>distance to the dependency tree from the rulebased system is investigated.</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In one setting, the cost functions adhere to the constrictions of computing a distance metric.</text>
                  <doc_id>190</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Two settings test the impact of biasing the insertion and deletion cost functions to assign a lower cost to inserting/deleting nonterminals, i.e. turning the dependency tree into the hierarchical tree and vice versa.</text>
                  <doc_id>191</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>TED is computed for 20 translation hypotheses and the best performing setting reported.</text>
                  <doc_id>192</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Leaves An experiment using the leaves technique has been conducted.</text>
                  <doc_id>193</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The experiment is performed using the best hypothesis from Moses and also using TED to chose the most structurally similar skeleton.</text>
                  <doc_id>194</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The best setting will be reported.</text>
                  <doc_id>195</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Lexical substitution To be able to compare a more naive approach, subtree substitution based on the initial element-level alignment between leaf nodes is used.</text>
                  <doc_id>196</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this approach, a subtree is one node.</text>
                  <doc_id>197</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The technique is identical to using the RBMT lexicon to lookup untranslated words and inserting them in the translation.</text>
                  <doc_id>198</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>TED-R An experiment where the mappings that represent a rename operation, which are produced during TED computation, are extracted and used as matching elements is conducted.</text>
                  <doc_id>199</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Mapping elements containing only punctuation or the root node of either tree are discarded.</text>
                  <doc_id>200</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>All combinations of substitutions based on the extracted matching elements are performed and the highest ranking hypothesis according to a language model is chosen as the final translation.</text>
                  <doc_id>201</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The extracted matching elements may not incorporate all the untranslated nodes.</text>
                  <doc_id>202</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>All untranslated nodes are subsequently translated using lexical substitution as mentioned above.</text>
                  <doc_id>203</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The subtrees inserted into the hierarchical tree will undergo the same transformation as the subtrees inserted using the leaves technique.</text>
                  <doc_id>204</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>This experiment is evaluated using both the 1- best hierarchical tree as skeleton and choosing the skeleton using TED.</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>All three settings are tested and the best performing experiment reported.</text>
                  <doc_id>206</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Evaluation</title>
            <text>The results of the automatic evaluation can be seen in Table 1. Skeleton indicates that TED was used to pick the hierarchical tree. The best evaluations are in bold.
100k The RBMT baseline is outperformed by all hybrid configurations, though it does have a higher METEOR score than the SMT baseline and skeleton selection. Lexical substitution and TED-R obtains an increase of ca. 2.5 BLEU, 4 TER and 4 METEOR points over the best baseline scores. The leaves technique decreases the metrics except for METEOR and the skeleton selection only shows an insignificant improvement.
Europarl Only lexical substitution improve all metrics over the baseline. Using the leaves technique again results in a decrease in BLEU and TER, but improves METEOR. The impact of skeleton selection is similar to previous experiments, but the use of skeleton selection in TED-R has become larger.
Manual Evaluation The evaluators rank 20 sentences randomly extracted from the test set on a scale from 1-5 with 5 being the best and it is possible to assign the same score to multiple translation alternatives. This evaluation was inspired by the sentence ranking evaluation in Callison-Burch et al. (2007). The five sentences to be evaluated will come from the RBMT and SMT baselines, lexical substitution, leaves technique and TED- R skeleton and the evaluators are 5 Danes who have studied translation with English as second language and 3 native English speakers.
The baseline systems make up 85% of the lowest ranking. The distribution between systems is more even for the second lowest ranking with the baselines only accounting for 52.6%. In the middle ranking, the top scorer is lexical substitution 83
System 1 2 3 4 5 Avg. rank
with a small margin to the RBMT baseline and the leaves technique. The many assignments of rank 3 could indicate that many of the translations produced can be used for gisting, i.e. get an impression of what information the source text conveys, but not enough to give a complete understanding, but can also be a result of being the middle value and chosen when the evaluators are in doubt. Lexical substitution is also the top scorer in the second-best ranking, followed closely by the other hybrid configurations and the hybrid systems account for 80.3% of the second-best rankings. TED-R recieves more top rankings than the other systems combined (55.3%). The RBMT baseline achieves second-most top-rankings. This can be attributed to the cases where the rules did not encounter unknown words and created very accurate translations, as is the hallmark of RBMT.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The results of the automatic evaluation can be seen in Table 1.</text>
                  <doc_id>207</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Skeleton indicates that TED was used to pick the hierarchical tree.</text>
                  <doc_id>208</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The best evaluations are in bold.</text>
                  <doc_id>209</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>100k The RBMT baseline is outperformed by all hybrid configurations, though it does have a higher METEOR score than the SMT baseline and skeleton selection.</text>
                  <doc_id>210</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Lexical substitution and TED-R obtains an increase of ca.</text>
                  <doc_id>211</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>2.5 BLEU, 4 TER and 4 METEOR points over the best baseline scores.</text>
                  <doc_id>212</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The leaves technique decreases the metrics except for METEOR and the skeleton selection only shows an insignificant improvement.</text>
                  <doc_id>213</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Europarl Only lexical substitution improve all metrics over the baseline.</text>
                  <doc_id>214</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Using the leaves technique again results in a decrease in BLEU and TER, but improves METEOR.</text>
                  <doc_id>215</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The impact of skeleton selection is similar to previous experiments, but the use of skeleton selection in TED-R has become larger.</text>
                  <doc_id>216</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Manual Evaluation The evaluators rank 20 sentences randomly extracted from the test set on a scale from 1-5 with 5 being the best and it is possible to assign the same score to multiple translation alternatives.</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This evaluation was inspired by the sentence ranking evaluation in Callison-Burch et al. (2007).</text>
                  <doc_id>218</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The five sentences to be evaluated will come from the RBMT and SMT baselines, lexical substitution, leaves technique and TED- R skeleton and the evaluators are 5 Danes who have studied translation with English as second language and 3 native English speakers.</text>
                  <doc_id>219</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The baseline systems make up 85% of the lowest ranking.</text>
                  <doc_id>220</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The distribution between systems is more even for the second lowest ranking with the baselines only accounting for 52.6%.</text>
                  <doc_id>221</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In the middle ranking, the top scorer is lexical substitution 83</text>
                  <doc_id>222</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>System 1 2 3 4 5 Avg.</text>
                  <doc_id>223</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>rank</text>
                  <doc_id>224</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>with a small margin to the RBMT baseline and the leaves technique.</text>
                  <doc_id>225</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The many assignments of rank 3 could indicate that many of the translations produced can be used for gisting, i.e. get an impression of what information the source text conveys, but not enough to give a complete understanding, but can also be a result of being the middle value and chosen when the evaluators are in doubt.</text>
                  <doc_id>226</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Lexical substitution is also the top scorer in the second-best ranking, followed closely by the other hybrid configurations and the hybrid systems account for 80.3% of the second-best rankings.</text>
                  <doc_id>227</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>TED-R recieves more top rankings than the other systems combined (55.3%).</text>
                  <doc_id>228</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The RBMT baseline achieves second-most top-rankings.</text>
                  <doc_id>229</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This can be attributed to the cases where the rules did not encounter unknown words and created very accurate translations, as is the hallmark of RBMT.</text>
                  <doc_id>230</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Discussion</title>
        <text>It is not surprising that lexical substitution achieves a significant increase in all metrics. The approach only translates untranslated words using the RBMT lexicon. This can improve the translation or, because of noisy matching elements, introduce wrong words but the penalty incurred for untranslated words and wrongly translated words is the same if the number of tokens is similar. Further, lexical substitution does not rely on structural similarity and can avoid the potential sources of errors encountered at a later processing stage.
Skeleton selection has little impact on the metrics and distinct derivations can result in the same surface structure, giving the same scores, but it is evident that finding the most similar tree improves substitution.
The improvements observed in the 100k experiments are not evident in the metrics when the full Europarl data is used. The more powerful SMT system is able to handle more translations but manual evaluation reveals a distribution where the majority of rankings for the baseline systems SMT ( COM ( 1999 ) 493 - C5-0320 baseline / 1999 - 1999 / 2208 ( COS ) ) Leaves ( came ( 1999 ) 493 - C5-0320/1999-1999/2208 ( COM COS ) ) - C5-0320 / 1999 - 1999 / 2208 ( TED-R ( COM ( 1999 ) 493 -
C5-0320/1999-1999/2208 / 1999 - 1999 / 2208 ( COS ) )
are in the lower half and rankings for the hybrid systems tend more towards the mid-to-upper rankings, with TED-R having more distribution around the second-best and highest score. This indicates that the approach creates more accurate translations.
The leaves technique consistently underperforms lexical substitution, but manual evaluation shows a high correlation between the two methods and their average ranks are similar. TED-R is ranked higher than the leaves technique in the metrics and manual evaluation also ranks TED- R higher than lexical substitution. This suggests that the extra surface structure removed is not present in the reference translation and that TED- R is a better implementation of the post-editing approach. Subtree substitution, whether using leaves or TED, does not handle parentheses, hyphens and numbers well. The structure severely degrades when performing substitution near these environments. The example in Table 3 shows the errors made by the substitution algorithm. An entire subphrase is duplicated using the leaves technique which introduces an opening parenthesis with no closing counterpart and includes the erroneous translation came, while TED-R duplicates / 1999 - 1999 / 2208.
The reason for these wayward substitutions can be found in the dependency tree. The matching parentheses are not part of the same subtree and this is the root cause of the problem. The leaves technique is very sensitive to these errors and there is no easy way to prevent spurious parentheses from being introduced. Re-ranking in TED- R could filter these hypotheses out, but because the re-ranking module cannot model this dependency, the sentences with these errors are not always discarded. In the manual evaluation campaign, the sentence from Table 3 was included in the sample sentences. It would seem that the many evaluators did not view this error as impor- 84
tant or it was ignored. It would be impossible to find the referenced Council decision based on the translations and dates or monetary amounts might change drastically, which would not be acceptable if the translated text should be ready for publishing after translation. For gisting, where the user knows that the translation is not perfect, this may constitute less of a problem.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>It is not surprising that lexical substitution achieves a significant increase in all metrics.</text>
              <doc_id>231</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The approach only translates untranslated words using the RBMT lexicon.</text>
              <doc_id>232</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This can improve the translation or, because of noisy matching elements, introduce wrong words but the penalty incurred for untranslated words and wrongly translated words is the same if the number of tokens is similar.</text>
              <doc_id>233</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Further, lexical substitution does not rely on structural similarity and can avoid the potential sources of errors encountered at a later processing stage.</text>
              <doc_id>234</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Skeleton selection has little impact on the metrics and distinct derivations can result in the same surface structure, giving the same scores, but it is evident that finding the most similar tree improves substitution.</text>
              <doc_id>235</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The improvements observed in the 100k experiments are not evident in the metrics when the full Europarl data is used.</text>
              <doc_id>236</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The more powerful SMT system is able to handle more translations but manual evaluation reveals a distribution where the majority of rankings for the baseline systems SMT ( COM ( 1999 ) 493 - C5-0320 baseline / 1999 - 1999 / 2208 ( COS ) ) Leaves ( came ( 1999 ) 493 - C5-0320/1999-1999/2208 ( COM COS ) ) - C5-0320 / 1999 - 1999 / 2208 ( TED-R ( COM ( 1999 ) 493 -</text>
              <doc_id>237</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>C5-0320/1999-1999/2208 / 1999 - 1999 / 2208 ( COS ) )</text>
              <doc_id>238</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>are in the lower half and rankings for the hybrid systems tend more towards the mid-to-upper rankings, with TED-R having more distribution around the second-best and highest score.</text>
              <doc_id>239</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This indicates that the approach creates more accurate translations.</text>
              <doc_id>240</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The leaves technique consistently underperforms lexical substitution, but manual evaluation shows a high correlation between the two methods and their average ranks are similar.</text>
              <doc_id>241</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>TED-R is ranked higher than the leaves technique in the metrics and manual evaluation also ranks TED- R higher than lexical substitution.</text>
              <doc_id>242</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This suggests that the extra surface structure removed is not present in the reference translation and that TED- R is a better implementation of the post-editing approach.</text>
              <doc_id>243</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Subtree substitution, whether using leaves or TED, does not handle parentheses, hyphens and numbers well.</text>
              <doc_id>244</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The structure severely degrades when performing substitution near these environments.</text>
              <doc_id>245</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The example in Table 3 shows the errors made by the substitution algorithm.</text>
              <doc_id>246</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>An entire subphrase is duplicated using the leaves technique which introduces an opening parenthesis with no closing counterpart and includes the erroneous translation came, while TED-R duplicates / 1999 - 1999 / 2208.</text>
              <doc_id>247</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The reason for these wayward substitutions can be found in the dependency tree.</text>
              <doc_id>248</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The matching parentheses are not part of the same subtree and this is the root cause of the problem.</text>
              <doc_id>249</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The leaves technique is very sensitive to these errors and there is no easy way to prevent spurious parentheses from being introduced.</text>
              <doc_id>250</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Re-ranking in TED- R could filter these hypotheses out, but because the re-ranking module cannot model this dependency, the sentences with these errors are not always discarded.</text>
              <doc_id>251</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In the manual evaluation campaign, the sentence from Table 3 was included in the sample sentences.</text>
              <doc_id>252</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>It would seem that the many evaluators did not view this error as impor- 84</text>
              <doc_id>253</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tant or it was ignored.</text>
              <doc_id>254</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It would be impossible to find the referenced Council decision based on the translations and dates or monetary amounts might change drastically, which would not be acceptable if the translated text should be ready for publishing after translation.</text>
              <doc_id>255</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For gisting, where the user knows that the translation is not perfect, this may constitute less of a problem.</text>
              <doc_id>256</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>6 Future work</title>
        <text>The initial alignment is based on the source to target language alignment. In the RBMT module, it is mostly word-based while in Moses, the alignment must be recomputed due to the simplicity of the modification and that the Moses chart decoder cannot output word alignment. The modelling only handles alignment crossing one nonterminal and reduces alignment problems to these cases by assuming a weak reordering.
Future work should include extracting the word alignment from the SMT system to improve source to target language alignment. The MT decoder Joshua can output complete derivations including word-based alignment which would eliminate the need to recompute source to target language alignment which currently produces noisy matching elements. Experiments using a different RBMT engine should also be conducted. The RBMT module does not always produce one complete tree structure for a sentence and the reattachment algorithm handles this by adding any additional graphs to the root node of the tree structure. A RBMT engine that produces complete derivations is likely to improve the translation quality. This will require different tree extraction modules for Joshua and the RBMT engine, but otherwise the system can be reused as is.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The initial alignment is based on the source to target language alignment.</text>
              <doc_id>257</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In the RBMT module, it is mostly word-based while in Moses, the alignment must be recomputed due to the simplicity of the modification and that the Moses chart decoder cannot output word alignment.</text>
              <doc_id>258</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The modelling only handles alignment crossing one nonterminal and reduces alignment problems to these cases by assuming a weak reordering.</text>
              <doc_id>259</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Future work should include extracting the word alignment from the SMT system to improve source to target language alignment.</text>
              <doc_id>260</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The MT decoder Joshua can output complete derivations including word-based alignment which would eliminate the need to recompute source to target language alignment which currently produces noisy matching elements.</text>
              <doc_id>261</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Experiments using a different RBMT engine should also be conducted.</text>
              <doc_id>262</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The RBMT module does not always produce one complete tree structure for a sentence and the reattachment algorithm handles this by adding any additional graphs to the root node of the tree structure.</text>
              <doc_id>263</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>A RBMT engine that produces complete derivations is likely to improve the translation quality.</text>
              <doc_id>264</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This will require different tree extraction modules for Joshua and the RBMT engine, but otherwise the system can be reused as is.</text>
              <doc_id>265</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>6.1 Languages and formalisms</title>
            <text>The chosen languages are closely related Germanic languages. While the results seem promising, the applicability of the approach should be tested on a more distant language pair, e.g. Chinese-English or Russian-English if you wish to preserve the possibility of using METEOR for evaluation, but any distant pair for which an RBMT system exists can be used &#8212; provided a tree output is available.
The implementation substitutes dependency subtrees into a hierarchical CFG-style tree. A second test of the hybridisation approach is to combine systems where the structures are not as diverse. Hierarchical systems are derived from a SCFG so a RBMT system based on a CFG formalism such as LUCY, could be used to test the generality of the hybridisation approach.
As the TED-R approach does not rely on markers for OOV words, an implementation where hierarchical subtrees are inserted into the RBMT output should also be conducted. The problem of inserting CFG-style subtrees into a dependency tree and generating the correct surface structure must be resolved or a different RBMT system which produce CFG-style trees implemented.
The implementation of the leaves technique relies on the diversity of the tree structures, i.e. that there are element-level similarities between hierarchical leaf nodes and both terminal and nonterminal dependency nodes and that the subtree rooted in a dependency node can be aligned to a hierarchical subtree. The refinement method would have to be altered. The relations and children techniques (Shvaiko and Euzenat, 2005) are good candidates for similar tree structures.
A change of formalism would not require alterations of the tree edit distance approach, as long as the structures are in fact tree structures.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The chosen languages are closely related Germanic languages.</text>
                  <doc_id>266</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>While the results seem promising, the applicability of the approach should be tested on a more distant language pair, e.g. Chinese-English or Russian-English if you wish to preserve the possibility of using METEOR for evaluation, but any distant pair for which an RBMT system exists can be used &#8212; provided a tree output is available.</text>
                  <doc_id>267</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The implementation substitutes dependency subtrees into a hierarchical CFG-style tree.</text>
                  <doc_id>268</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A second test of the hybridisation approach is to combine systems where the structures are not as diverse.</text>
                  <doc_id>269</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Hierarchical systems are derived from a SCFG so a RBMT system based on a CFG formalism such as LUCY, could be used to test the generality of the hybridisation approach.</text>
                  <doc_id>270</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As the TED-R approach does not rely on markers for OOV words, an implementation where hierarchical subtrees are inserted into the RBMT output should also be conducted.</text>
                  <doc_id>271</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The problem of inserting CFG-style subtrees into a dependency tree and generating the correct surface structure must be resolved or a different RBMT system which produce CFG-style trees implemented.</text>
                  <doc_id>272</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The implementation of the leaves technique relies on the diversity of the tree structures, i.e. that there are element-level similarities between hierarchical leaf nodes and both terminal and nonterminal dependency nodes and that the subtree rooted in a dependency node can be aligned to a hierarchical subtree.</text>
                  <doc_id>273</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The refinement method would have to be altered.</text>
                  <doc_id>274</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The relations and children techniques (Shvaiko and Euzenat, 2005) are good candidates for similar tree structures.</text>
                  <doc_id>275</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A change of formalism would not require alterations of the tree edit distance approach, as long as the structures are in fact tree structures.</text>
                  <doc_id>276</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>7</index>
        <title>7 Conclusion</title>
        <text>The post-editing approach proposed in this paper combines the strengths of statistical and rulebased machine translation and improve translation quality, especially for the least accurate translations. The structural and knowledge-poor approach is novel and has not been attempted before. It exploits structural output to create hybrid translations and uses the linguistic knowledge encoded in structure and on nodes to improve the translation candidates of hierarchical phrase-based MT systems. Automatic evaluation shows a significant increase over the baselines when training data is limited and also improvement in TER and ME- TEOR for lexical substitution and TED-R with a SMT system trained on the Europarl corpus. Manual evaluation on test data shows that hybrid translations were generally ranked higher, indicating that the hybrid approach produces more accurate translations.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The post-editing approach proposed in this paper combines the strengths of statistical and rulebased machine translation and improve translation quality, especially for the least accurate translations.</text>
              <doc_id>277</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The structural and knowledge-poor approach is novel and has not been attempted before.</text>
              <doc_id>278</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>It exploits structural output to create hybrid translations and uses the linguistic knowledge encoded in structure and on nodes to improve the translation candidates of hierarchical phrase-based MT systems.</text>
              <doc_id>279</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Automatic evaluation shows a significant increase over the baselines when training data is limited and also improvement in TER and ME- TEOR for lexical substitution and TED-R with a SMT system trained on the Europarl corpus.</text>
              <doc_id>280</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Manual evaluation on test data shows that hybrid translations were generally ranked higher, indicating that the hybrid approach produces more accurate translations.</text>
              <doc_id>281</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Automatic evaluation. 100k experiments in parentheses</caption>
        <reference_text>In PAGE 7: ... 4.2 Evaluation The results of the automatic evaluation can be seen in  Table1 . Skeleton indicates that TED was used to pick the hierarchical tree....</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>Metrics:</cell>
              <cell>BLEU</cell>
              <cell>TER</cell>
              <cell>METEOR</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>RBMT baseline</cell>
              <cell>19.35</cell>
              <cell>64.54</cell>
              <cell>53.19</cell>
            </row>
            <row>
              <cell>SMT baseline</cell>
              <cell>30.16 (22.63)</cell>
              <cell>57.16 (63.10)</cell>
              <cell>59.51 (50.72)</cell>
            </row>
            <row>
              <cell>Lexical substitution</cell>
              <cell>30.53 (25.28)</cell>
              <cell>56.40 (60.56)</cell>
              <cell>61.22 (57.24)</cell>
            </row>
            <row>
              <cell>Leaves technique</cell>
              <cell>29.06 (21.96)</cell>
              <cell>57.96 (64.80)</cell>
              <cell>60.09 (54.32)</cell>
            </row>
            <row>
              <cell>TED skeleton(any bias)</cell>
              <cell>30.16 (22.63)</cell>
              <cell>57.08 (62.98)</cell>
              <cell>59.46 (50.75)</cell>
            </row>
            <row>
              <cell>TED-R 1-best</cell>
              <cell>29.78 (25.16)</cell>
              <cell>57.25 (60.51)</cell>
              <cell>59.87 (57.31)</cell>
            </row>
            <row>
              <cell>TED-R skeleton(any bias)</cell>
              <cell>29.99 (25.18)</cell>
              <cell>56.72 (60.44)</cell>
              <cell>60.79 (57.34)</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Rankings from the manual evaluation of the second set of experiments.</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>SMT</cell>
              <cell>53</cell>
              <cell>64</cell>
              <cell>30</cell>
              <cell>12</cell>
              <cell>1</cell>
              <cell>2.025</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>RBMT</cell>
              <cell>14</cell>
              <cell>48</cell>
              <cell>61</cell>
              <cell>29</cell>
              <cell>8</cell>
              <cell>2.806</cell>
            </row>
            <row>
              <cell>Lex. sub.</cell>
              <cell>3</cell>
              <cell>33</cell>
              <cell>63</cell>
              <cell>58</cell>
              <cell>3</cell>
              <cell>3.156</cell>
            </row>
            <row>
              <cell>Leaves</cell>
              <cell>6</cell>
              <cell>33</cell>
              <cell>61</cell>
              <cell>55</cell>
              <cell>5</cell>
              <cell>3.125</cell>
            </row>
            <row>
              <cell>TED-R</cell>
              <cell>3</cell>
              <cell>35</cell>
              <cell>46</cell>
              <cell>55</cell>
              <cell>21</cell>
              <cell>3.35</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>S Banerjee</author>
          <author>A Lavie</author>
        </authors>
        <title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</title>
        <publication>None</publication>
        <pages>65</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>E Bick</author>
        </authors>
        <title>Dan2eng: Wide-coverage danishenglish machine translation.</title>
        <publication>Proceedings of Machine Translation Summit XI,</publication>
        <pages>37--43</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>P Bille</author>
        </authors>
        <title>A survey on tree edit distance and related problems. Theoretical computer science,</title>
        <publication>None</publication>
        <pages>337--1</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>C Callison-Burch</author>
          <author>C Fordyce</author>
          <author>P Koehn</author>
          <author>C Monz</author>
          <author>J Schroeder</author>
        </authors>
        <title>(Meta-) evaluation of machine translation.</title>
        <publication>In Proceedings of the Second Workshop on Statistical Machine Translation,</publication>
        <pages>136--158</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>D Chiang</author>
        </authors>
        <title>A hierarchical phrase-based model for statistical machine translation.</title>
        <publication>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</publication>
        <pages>263--270</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>A Eisele</author>
          <author>C Federmann</author>
          <author>H Uszkoreit</author>
          <author>H SaintAmand</author>
          <author>M Kay</author>
          <author>M Jellinghaus</author>
          <author>S Hunsicker</author>
          <author>T Herrmann</author>
          <author>Y Chen</author>
        </authors>
        <title>Hybrid machine translation architectures within and beyond the EuroMatrix project.</title>
        <publication>In Proceedings of the 12th annual conference of the European Association for Machine Translation (EAMT</publication>
        <pages>27--34</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>C Federmann</author>
          <author>A Eisele</author>
          <author>H Uszkoreit</author>
          <author>Y Chen</author>
          <author>S Hunsicker</author>
          <author>J Xu</author>
        </authors>
        <title>Further experiments with shallow hybrid mt systems.</title>
        <publication>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</publication>
        <pages>77--81</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>A S Hildebrand</author>
          <author>S Vogel</author>
        </authors>
        <title>Combination of machine translation systems via hypothesis selection from combined n-best lists.</title>
        <publication>In MT at work: Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas,</publication>
        <pages>254--261</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>P Koehn</author>
          <author>C Monz</author>
        </authors>
        <title>Manual and automatic evaluation of machine translation between european languages.</title>
        <publication>In Proceedings of the Workshop on Statistical Machine Translation,</publication>
        <pages>102--121</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>P Koehn</author>
          <author>H Hoang</author>
          <author>A Birch</author>
          <author>C Callison-Burch</author>
          <author>M Federico</author>
          <author>N Bertoldi</author>
          <author>B Cowan</author>
          <author>W Shen</author>
          <author>C Moran</author>
          <author>R Zens</author>
        </authors>
        <title>Moses: Open source toolkit for statistical machine translation.</title>
        <publication>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</publication>
        <pages>177--180</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>P Koehn</author>
        </authors>
        <title>Europarl: A parallel corpus for statistical machine translation.</title>
        <publication>In MT summit,</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>P Koehn</author>
        </authors>
        <title>An experimental management system.</title>
        <publication>The Prague Bulletin of Mathematical Linguistics,</publication>
        <pages>94--1</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>K Papineni</author>
          <author>S Roukos</author>
          <author>T Ward</author>
          <author>W J Zhu</author>
        </authors>
        <title>BLEU: a method for automatic evaluation of machine translation.</title>
        <publication>In Proceedings of the 40th annual meeting on association for computational linguistics,</publication>
        <pages>311--318</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>A V I Rosti</author>
          <author>N F Ayan</author>
          <author>B Xiang</author>
          <author>S Matsoukas</author>
          <author>R Schwartz</author>
          <author>B Dorr</author>
        </authors>
        <title>Combining outputs from multiple machine translation systems.</title>
        <publication>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</publication>
        <pages>228--235</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>P Shvaiko</author>
          <author>J Euzenat</author>
        </authors>
        <title>A survey of schemabased matching approaches.</title>
        <publication>Journal on Data Semantics IV,</publication>
        <pages>146--171</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>M Simard</author>
          <author>N Ueffing</author>
          <author>P Isabelle</author>
          <author>R Kuhn</author>
        </authors>
        <title>Rule-based translation with statistical phrase-based post-editing.</title>
        <publication>In ACL 2007 Second Workshop on Statistical Machine Translation.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>M Snover</author>
          <author>B Dorr</author>
          <author>R Schwartz</author>
          <author>L Micciulla</author>
          <author>J Makhoul</author>
        </authors>
        <title>A study of translation edit rate with targeted human annotation.</title>
        <publication>In Proceedings of Association for Machine Translation in the Americas,</publication>
        <pages>223--231</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>A Stolcke</author>
        </authors>
        <title>SRILM - an extensible language modeling toolkit.</title>
        <publication>In Proceedings of the international conference on spoken language processing,</publication>
        <pages>901--904</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Gregor Thurmair</author>
        </authors>
        <title>Comparing different architectures of Hybrid Machine Translation systems.</title>
        <publication>In Proceedings of the MT Summit XII,</publication>
        <pages>340--347</pages>
        <date>2009</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Banerjee and Lavie, 2005</string>
        <sentence_id>48731</sentence_id>
        <char_offset>68</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Bick, 2007</string>
        <sentence_id>48531</sentence_id>
        <char_offset>55</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Bille (2005)</string>
        <sentence_id>48612</sentence_id>
        <char_offset>38</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>3</reference_id>
        <string>Callison-Burch et al. (2007)</string>
        <sentence_id>48716</sentence_id>
        <char_offset>67</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>4</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>48509</sentence_id>
        <char_offset>22</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>4</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>48545</sentence_id>
        <char_offset>201</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>6</reference_id>
        <string>Federmann et al., 2010</string>
        <sentence_id>48517</sentence_id>
        <char_offset>214</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>7</reference_id>
        <string>Hildebrand and Vogel, 2008</string>
        <sentence_id>48517</sentence_id>
        <char_offset>125</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>8</reference_id>
        <string>Koehn and Monz, 2006</string>
        <sentence_id>48559</sentence_id>
        <char_offset>171</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>9</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>48540</sentence_id>
        <char_offset>61</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>10</reference_id>
        <string>Koehn, 2005</string>
        <sentence_id>48559</sentence_id>
        <char_offset>54</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>11</reference_id>
        <string>Koehn, 2010</string>
        <sentence_id>48561</sentence_id>
        <char_offset>66</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>12</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>48731</sentence_id>
        <char_offset>6</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>13</reference_id>
        <string>Rosti et al. (2007)</string>
        <sentence_id>48527</sentence_id>
        <char_offset>84</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>14</reference_id>
        <string>Shvaiko and Euzenat, 2005</string>
        <sentence_id>48767</sentence_id>
        <char_offset>39</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>14</reference_id>
        <string>Shvaiko and Euzenat (2005)</string>
        <sentence_id>48661</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>15</reference_id>
        <string>Simard et al., 2007</string>
        <sentence_id>48517</sentence_id>
        <char_offset>276</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>16</reference_id>
        <string>Snover et al., 2006</string>
        <sentence_id>48731</sentence_id>
        <char_offset>35</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>17</reference_id>
        <string>Stolcke, 2002</string>
        <sentence_id>48560</sentence_id>
        <char_offset>139</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>18</reference_id>
        <string>Thurmair (2009)</string>
        <sentence_id>48525</sentence_id>
        <char_offset>125</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>18</reference_id>
        <string>Thurmair, 2009</string>
        <sentence_id>48564</sentence_id>
        <char_offset>70</char_offset>
      </citation>
    </citations>
  </content>
</document>
