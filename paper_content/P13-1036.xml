<document>
  <filename>P13-1036</filename>
  <authors/>
  <title>Scalable Decipherment for Machine Translation via Hash Sampling</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>In this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models. In order to perform fast, efficient Bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of Ahmed et al. (2012). The new translation hash sampler enables us to scale elegantly to complex models (for the first time) and large vocabulary/corpora sizes. We show empirical results on the OPUS data&#8212;our method yields the best BLEU scores compared to existing approaches, while achieving significant computational speedups (several orders faster). We also report for the first time&#8212;BLEU score results for a largescale MT task using only non-parallel data (EMEA corpus).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In order to perform fast, efficient Bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of Ahmed et al. (2012).</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The new translation hash sampler enables us to scale elegantly to complex models (for the first time) and large vocabulary/corpora sizes.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We show empirical results on the OPUS data&#8212;our method yields the best BLEU scores compared to existing approaches, while achieving significant computational speedups (several orders faster).</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We also report for the first time&#8212;BLEU score results for a largescale MT task using only non-parallel data (EMEA corpus).</text>
              <doc_id>5</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Statistical machine translation (SMT) systems these days are built using large amounts of bilingual parallel corpora. The parallel corpora are used to estimate translation model parameters involving word-to-word translation tables, fertilities, distortion, phrase translations, syntactic transformations, etc. But obtaining parallel data is an expensive process and not available for all language pairs or domains. On the other hand, monolingual data (in written form) exists and is easier to obtain for many languages. Learning translation models from monolingual corpora could help address the challenges faced by modern-day MT systems, especially for low resource language pairs. Recently, this topic has been receiving increasing attention from researchers and new methods have been proposed to train statistical machine translation models using only monolingual data in the source and target language. The underlying motivation behind most of these methods is that statistical properties for linguistic elements are shared across different languages and some of these similarities (mappings) could be automatically identified from large amounts of monolingual data.
The MT literature does cover some prior work on extracting or augmenting partial lexicons using non-parallel corpora (Rapp, 1995; Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008). However, none of these methods attempt to train end-to-end MT models, instead they focus on mining bilingual lexicons from monolingual corpora and often they require parallel seed lexicons as a starting point. Some of them (Haghighi et al., 2008) also rely on additional linguistic knowledge such as orthography, etc. to mine word translation pairs across related languages (e.g., Spanish/English). Unsupervised training methods have also been proposed in the past for related problems in decipherment (Knight and Yamada, 1999; Snyder et al., 2010; Ravi and Knight, 2011a) where the goal is to decode unknown scripts or ciphers.
The body of work that is more closely related to ours include that of Ravi and Knight (2011b) who introduced a decipherment approach for training translation models using only monolingual cor-
pora. Their best performing method uses an EM algorithm to train a word translation model and they show results on a Spanish/English task. Nuhn et al. (2012) extend the former approach and improve training efficiency by pruning translation candidates prior to EM training with the help of context similarities computed from monolingual corpora.
In this work we propose a new Bayesian inference method for estimating translation models from scratch using only monolingual corpora. Secondly, we introduce a new feature-based representation for sampling translation candidates that allows one to incorporate any amount of additional features (beyond simple bag-of-words) as sideinformation during decipherment training. Finally, we also derive a new accelerated sampling mechanism using locality sensitive hashing inspired by recent work on fast, probabilistic inference for unsupervised clustering (Ahmed et al., 2012). The new sampler allows us to perform fast, efficient inference with more complex translation models (than previously used) and scale better to large vocabulary and corpora sizes compared to existing methods as evidenced by our experimental results on two different corpora.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Statistical machine translation (SMT) systems these days are built using large amounts of bilingual parallel corpora.</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The parallel corpora are used to estimate translation model parameters involving word-to-word translation tables, fertilities, distortion, phrase translations, syntactic transformations, etc.</text>
              <doc_id>7</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>But obtaining parallel data is an expensive process and not available for all language pairs or domains.</text>
              <doc_id>8</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>On the other hand, monolingual data (in written form) exists and is easier to obtain for many languages.</text>
              <doc_id>9</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Learning translation models from monolingual corpora could help address the challenges faced by modern-day MT systems, especially for low resource language pairs.</text>
              <doc_id>10</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Recently, this topic has been receiving increasing attention from researchers and new methods have been proposed to train statistical machine translation models using only monolingual data in the source and target language.</text>
              <doc_id>11</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The underlying motivation behind most of these methods is that statistical properties for linguistic elements are shared across different languages and some of these similarities (mappings) could be automatically identified from large amounts of monolingual data.</text>
              <doc_id>12</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The MT literature does cover some prior work on extracting or augmenting partial lexicons using non-parallel corpora (Rapp, 1995; Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008).</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, none of these methods attempt to train end-to-end MT models, instead they focus on mining bilingual lexicons from monolingual corpora and often they require parallel seed lexicons as a starting point.</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Some of them (Haghighi et al., 2008) also rely on additional linguistic knowledge such as orthography, etc. to mine word translation pairs across related languages (e.g., Spanish/English).</text>
              <doc_id>15</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Unsupervised training methods have also been proposed in the past for related problems in decipherment (Knight and Yamada, 1999; Snyder et al., 2010; Ravi and Knight, 2011a) where the goal is to decode unknown scripts or ciphers.</text>
              <doc_id>16</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The body of work that is more closely related to ours include that of Ravi and Knight (2011b) who introduced a decipherment approach for training translation models using only monolingual cor-</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>pora.</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Their best performing method uses an EM algorithm to train a word translation model and they show results on a Spanish/English task.</text>
              <doc_id>19</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Nuhn et al. (2012) extend the former approach and improve training efficiency by pruning translation candidates prior to EM training with the help of context similarities computed from monolingual corpora.</text>
              <doc_id>20</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this work we propose a new Bayesian inference method for estimating translation models from scratch using only monolingual corpora.</text>
              <doc_id>21</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Secondly, we introduce a new feature-based representation for sampling translation candidates that allows one to incorporate any amount of additional features (beyond simple bag-of-words) as sideinformation during decipherment training.</text>
              <doc_id>22</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we also derive a new accelerated sampling mechanism using locality sensitive hashing inspired by recent work on fast, probabilistic inference for unsupervised clustering (Ahmed et al., 2012).</text>
              <doc_id>23</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The new sampler allows us to perform fast, efficient inference with more complex translation models (than previously used) and scale better to large vocabulary and corpora sizes compared to existing methods as evidenced by our experimental results on two different corpora.</text>
              <doc_id>24</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Decipherment Model for Machine Translation</title>
        <text>We now describe the decipherment problem formulation for machine translation. Problem Formulation: Given a source text f (i.e., source word sequences f 1 ...f m ) and a monolingual target language corpus, our goal is to decipher the source text and produce a target translation.
Contrary to standard machine translation training scenarios, here we have to estimate the translation model P &#952; (f|e) parameters using only monolingual data. During decipherment training, our objective is to estimate the model parameters in order to maximize the probability of the source text f as suggested by Ravi and Knight (2011b).
arg max
&#952;
&#8719; &#8721; P (e) &#183; P &#952; (f|e) (1)
f
e
For P (e), we use a word n-gram language model (LM) trained on monolingual target text. We then estimate the parameters of the translation model P &#952; (f|e) during training.
Translation Model: Machine translation is a much more complex task than solving other decipherment tasks such as word substitution ciphers (Ravi and Knight, 2011b; Dou and Knight, 2012). The mappings between languages involve non-determinism (i.e., words can have multiple translations), re-ordering of words can occur as grammar and syntax varies with language, and in addition word insertion and deletion operations are also involved. Ideally, for the translation model P (f |e) we would like to use well-known statistical models such as IBM Model 3 and estimate its parameters &#952; using the EM algorithm (Dempster et al., 1977). But training becomes intractable with complex translation models and scalability is also an issue when large corpora sizes are involved and the translation tables become huge to fit in memory. So, instead we use a simplified generative process for the translation model as proposed by Ravi and Knight (2011b) and used by others (Nuhn et al., 2012) for this task:
1. Generate a target (e.g., English) string e = e 1 ...e l , with probability P (e) according to an n-gram language model.
2. Insert a NULL word at any position in the English string, with uniform probability.
3. For each target word token e i (including NULLs), choose a source word translation f i , with probability P &#952; (f i |e i ). The source word may be NULL.
4. Swap any pair of adjacent source words f i&#8722;1 , f i , with probability P (swap); set to 0.1.
5. Output the foreign string f = f 1 ...f m , skipping over NULLs.
Previous approaches (Ravi and Knight, 2011b; Nuhn et al., 2012) use the EM algorithm to estimate all the parameters &#952; in order to maximize likelihood of the foreign corpus. Instead, we propose a new Bayesian inference framework to estimate the translation model parameters. In spite of using Bayesian inference which is typically slow in practice (with standard Gibbs sampling), we show later that our method is scalable and permits decipherment training using more complex translation models (with several additional parameters).
2.1 Adding Phrases, Flexible Reordering and Fertility to Translation Model
We now extend the generative process (described earlier) to more complex translation models. Non-local Re-ordering: The generative process described earlier limits re-ordering to local or adjacent word pairs in a source sentence. We extend this to allow re-ordering between any pair of words in the sentence. Fertility: We also add a fertility model P &#952;fert to the translation model using the formula:
P &#952;fert = &#8719; i n &#952; (&#966; i |e i ) &#183; p &#966; 0
1 (2)
n &#952; (&#966; i |e i ) = &#945; fert &#183; P 0 (&#966; i |e i ) + C &#8722;i (e i , &#966; i ) &#945; fert + C &#8722;i (3) (e i )
where, P 0 represents the base distribution (which is set to uniform) in a Chinese Restaurant Process (CRP) 1 for the fertility model and C &#8722;i represents the count of events occurring in the history excluding the observation at position i. &#966; i is the number of source words aligned to (i.e., generated by) the target word e i . We use sparse Dirichlet priors for all the translation model components. 2 &#966; 0 represents the target NULL word fertility and p 1 is the insertion probability which is fixed to 0.1. In addition, we set a maximum threshold for fertility values &#966; i &#8804; &#947; &#183; m, where m is the length of the source sentence. This discourages a particular target word (e.g., NULL word) from generating too many source words in the same sentence. In our experiments, we set &#947; = 0.3. We enforce this constraint in the training process during sampling. 3 Modeling Phrases: Finally, we extend the translation candidate set in P &#952; (f i |e i ) to model phrases in addition to words for the target side (i.e., e i can now be a word or a phrase 4 previously seen in the monolingual target corpus). This greatly increases the training time since in each sampling step, we now have many more e i candidates to choose from. In Section 4, we describe how we deal
1 Each component in the translation model (word/phrase
translations P &#952; (f i|e i), fertility P &#952;fert , etc.) is modeled using a CRP formulation. 2 i.e., All the concentration parameters are set to low values; &#945; f|e = &#945; fert = 0.01. 3 We only apply this constraint when training on source
text/corpora made of long sentences (&gt;10 words) where the sampler might converge very slowly. For short sentences, a sparse prior on fertility &#945; fert typically discourages a target word from being aligned to too many different source words. 4 Phrase size is limited to two words in our experiments.
with this problem by using a fast, efficient sampler based on hashing that allows us to speed up the Bayesian inference significantly whereas standard Gibbs sampling would be extremely slow.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We now describe the decipherment problem formulation for machine translation.</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Problem Formulation: Given a source text f (i.e., source word sequences f 1 ...f m ) and a monolingual target language corpus, our goal is to decipher the source text and produce a target translation.</text>
              <doc_id>26</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Contrary to standard machine translation training scenarios, here we have to estimate the translation model P &#952; (f|e) parameters using only monolingual data.</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>During decipherment training, our objective is to estimate the model parameters in order to maximize the probability of the source text f as suggested by Ravi and Knight (2011b).</text>
              <doc_id>28</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>arg max</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#952;</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8719; &#8721; P (e) &#183; P &#952; (f|e) (1)</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>f</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e</text>
              <doc_id>33</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For P (e), we use a word n-gram language model (LM) trained on monolingual target text.</text>
              <doc_id>34</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We then estimate the parameters of the translation model P &#952; (f|e) during training.</text>
              <doc_id>35</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Translation Model: Machine translation is a much more complex task than solving other decipherment tasks such as word substitution ciphers (Ravi and Knight, 2011b; Dou and Knight, 2012).</text>
              <doc_id>36</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The mappings between languages involve non-determinism (i.e., words can have multiple translations), re-ordering of words can occur as grammar and syntax varies with language, and in addition word insertion and deletion operations are also involved.</text>
              <doc_id>37</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Ideally, for the translation model P (f |e) we would like to use well-known statistical models such as IBM Model 3 and estimate its parameters &#952; using the EM algorithm (Dempster et al., 1977).</text>
              <doc_id>38</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>But training becomes intractable with complex translation models and scalability is also an issue when large corpora sizes are involved and the translation tables become huge to fit in memory.</text>
              <doc_id>39</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>So, instead we use a simplified generative process for the translation model as proposed by Ravi and Knight (2011b) and used by others (Nuhn et al., 2012) for this task:</text>
              <doc_id>40</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1.</text>
              <doc_id>41</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Generate a target (e.g., English) string e = e 1 ...e l , with probability P (e) according to an n-gram language model.</text>
              <doc_id>42</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.</text>
              <doc_id>43</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Insert a NULL word at any position in the English string, with uniform probability.</text>
              <doc_id>44</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3.</text>
              <doc_id>45</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For each target word token e i (including NULLs), choose a source word translation f i , with probability P &#952; (f i |e i ).</text>
              <doc_id>46</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The source word may be NULL.</text>
              <doc_id>47</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>4.</text>
              <doc_id>48</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Swap any pair of adjacent source words f i&#8722;1 , f i , with probability P (swap); set to 0.1.</text>
              <doc_id>49</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>5.</text>
              <doc_id>50</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Output the foreign string f = f 1 ...f m , skipping over NULLs.</text>
              <doc_id>51</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Previous approaches (Ravi and Knight, 2011b; Nuhn et al., 2012) use the EM algorithm to estimate all the parameters &#952; in order to maximize likelihood of the foreign corpus.</text>
              <doc_id>52</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Instead, we propose a new Bayesian inference framework to estimate the translation model parameters.</text>
              <doc_id>53</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In spite of using Bayesian inference which is typically slow in practice (with standard Gibbs sampling), we show later that our method is scalable and permits decipherment training using more complex translation models (with several additional parameters).</text>
              <doc_id>54</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.1 Adding Phrases, Flexible Reordering and Fertility to Translation Model</text>
              <doc_id>55</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We now extend the generative process (described earlier) to more complex translation models.</text>
              <doc_id>56</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Non-local Re-ordering: The generative process described earlier limits re-ordering to local or adjacent word pairs in a source sentence.</text>
              <doc_id>57</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We extend this to allow re-ordering between any pair of words in the sentence.</text>
              <doc_id>58</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Fertility: We also add a fertility model P &#952;fert to the translation model using the formula:</text>
              <doc_id>59</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P &#952;fert = &#8719; i n &#952; (&#966; i |e i ) &#183; p &#966; 0</text>
              <doc_id>60</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 (2)</text>
              <doc_id>61</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>n &#952; (&#966; i |e i ) = &#945; fert &#183; P 0 (&#966; i |e i ) + C &#8722;i (e i , &#966; i ) &#945; fert + C &#8722;i (3) (e i )</text>
              <doc_id>62</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where, P 0 represents the base distribution (which is set to uniform) in a Chinese Restaurant Process (CRP) 1 for the fertility model and C &#8722;i represents the count of events occurring in the history excluding the observation at position i. &#966; i is the number of source words aligned to (i.e., generated by) the target word e i .</text>
              <doc_id>63</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We use sparse Dirichlet priors for all the translation model components.</text>
              <doc_id>64</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>2 &#966; 0 represents the target NULL word fertility and p 1 is the insertion probability which is fixed to 0.1.</text>
              <doc_id>65</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In addition, we set a maximum threshold for fertility values &#966; i &#8804; &#947; &#183; m, where m is the length of the source sentence.</text>
              <doc_id>66</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This discourages a particular target word (e.g., NULL word) from generating too many source words in the same sentence.</text>
              <doc_id>67</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In our experiments, we set &#947; = 0.3.</text>
              <doc_id>68</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We enforce this constraint in the training process during sampling.</text>
              <doc_id>69</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>3 Modeling Phrases: Finally, we extend the translation candidate set in P &#952; (f i |e i ) to model phrases in addition to words for the target side (i.e., e i can now be a word or a phrase 4 previously seen in the monolingual target corpus).</text>
              <doc_id>70</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>This greatly increases the training time since in each sampling step, we now have many more e i candidates to choose from.</text>
              <doc_id>71</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>In Section 4, we describe how we deal</text>
              <doc_id>72</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 Each component in the translation model (word/phrase</text>
              <doc_id>73</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>translations P &#952; (f i|e i), fertility P &#952;fert , etc.) is modeled using a CRP formulation.</text>
              <doc_id>74</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2 i.e., All the concentration parameters are set to low values; &#945; f|e = &#945; fert = 0.01.</text>
              <doc_id>75</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>3 We only apply this constraint when training on source</text>
              <doc_id>76</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>text/corpora made of long sentences (&gt;10 words) where the sampler might converge very slowly.</text>
              <doc_id>77</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For short sentences, a sparse prior on fertility &#945; fert typically discourages a target word from being aligned to too many different source words.</text>
              <doc_id>78</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>4 Phrase size is limited to two words in our experiments.</text>
              <doc_id>79</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>with this problem by using a fast, efficient sampler based on hashing that allows us to speed up the Bayesian inference significantly whereas standard Gibbs sampling would be extremely slow.</text>
              <doc_id>80</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Feature-based representation for Source and Target</title>
        <text>The model described in the previous section while being flexible in describing the translation process, poses several challenges for training. As the source and target vocabulary sizes increase the size of the translation table (|V f | &#183; |V e |) increases significantly and often becomes too huge to fit in memory. Additionally, performing Bayesian inference with such a complex model using standard Gibbs sampling can be very slow in practice. Here, we describe a new method for doing Bayesian inference by first introducing a featurebased representation for the source and target words (or phrases) from which we then derive a novel proposal distribution for sampling translation candidates. We represent both source and target words in a vector space similar to how documents are represented in typical information retrieval settings. But unlike documents, here each word w is associated with a feature vector w 1 ...w d (where w i represents the weight for the feature indexed by i) which is constructed from monolingual corpora. For instance, context features for word w may include other words (or phrases) that appear in the immediate context (n-gram window) surrounding w in the monolingual corpus. Similarly, we can add other features based on topic models, orthography (Haghighi et al., 2008), temporal (Klementiev et al., 2012), etc. to our representation all of which can be extracted from monolingual corpora.
Next, given two high dimensional vectors u and v it is possible to calculate the similarity between the two words denoted by s(u, v). The feature construction process is described in more detail below: Target Language: We represent each word (or phrase) e i with the following contextual features along with their counts: (a) f &#8722;context : every (word n-gram, position) pair immediately preceding e i in the monolingual corpus (n=1, position=&#8722;1), (b) similar features f +context to model the context following e i , and (c) we also throw in generic context features f scontext without position information&#8212; every word that co-occurs with e i in the same sen-
tence. While the two position-features provide specific context information (may be sparse for large monolingual corpora), this feature is more generic and captures long-distance co-occurrence statistics. Source Language: Words appearing in a source sentence f are represented using the corresponding target translation e = e 1 ...e m generated for f in the current sample during training. For each source word f j &#8712; f, we look at the corresponding word e j in the target translation. We then extract all the context features of e j in the target translation sample sentence e and add these features (f &#8722;context , f +context , f scontext ) with weights to the feature representation for f j . Unlike the target word feature vectors (which can be pre-computed from the monolingual target corpus), the feature vector for every source word f j is dynamically constructed from the target translation sampled in each training iteration. This is a key distinction of our framework compared to previous approaches that use contextual similarity (or any other) features constructed from static monolingual corpora (Rapp, 1995; Koehn and Knight, 2000; Nuhn et al., 2012).
Note that as we add more and more features for a particular word (by training on larger monolingual corpora or adding new types of features, etc.), it results in the feature representation becoming more sparse (especially for source feature vectors) which can cause problems in efficiency as well as robustness when computing similarity against other vectors. In the next section, we will describe how we mitigate this problem by projecting into a low-dimensional space by computing hash signatures.
In all our experiments, we only use the features described above for representing source and target words. We note that the new sampling framework is easily extensible to many additional feature types (for example, monolingual topic model features, etc.) which can be efficiently handled by our inference algorithm and could further improve translation performance but we leave this for future work.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The model described in the previous section while being flexible in describing the translation process, poses several challenges for training.</text>
              <doc_id>81</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As the source and target vocabulary sizes increase the size of the translation table (|V f | &#183; |V e |) increases significantly and often becomes too huge to fit in memory.</text>
              <doc_id>82</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Additionally, performing Bayesian inference with such a complex model using standard Gibbs sampling can be very slow in practice.</text>
              <doc_id>83</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Here, we describe a new method for doing Bayesian inference by first introducing a featurebased representation for the source and target words (or phrases) from which we then derive a novel proposal distribution for sampling translation candidates.</text>
              <doc_id>84</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We represent both source and target words in a vector space similar to how documents are represented in typical information retrieval settings.</text>
              <doc_id>85</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>But unlike documents, here each word w is associated with a feature vector w 1 ...w d (where w i represents the weight for the feature indexed by i) which is constructed from monolingual corpora.</text>
              <doc_id>86</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>For instance, context features for word w may include other words (or phrases) that appear in the immediate context (n-gram window) surrounding w in the monolingual corpus.</text>
              <doc_id>87</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Similarly, we can add other features based on topic models, orthography (Haghighi et al., 2008), temporal (Klementiev et al., 2012), etc. to our representation all of which can be extracted from monolingual corpora.</text>
              <doc_id>88</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Next, given two high dimensional vectors u and v it is possible to calculate the similarity between the two words denoted by s(u, v).</text>
              <doc_id>89</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The feature construction process is described in more detail below: Target Language: We represent each word (or phrase) e i with the following contextual features along with their counts: (a) f &#8722;context : every (word n-gram, position) pair immediately preceding e i in the monolingual corpus (n=1, position=&#8722;1), (b) similar features f +context to model the context following e i , and (c) we also throw in generic context features f scontext without position information&#8212; every word that co-occurs with e i in the same sen-</text>
              <doc_id>90</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tence.</text>
              <doc_id>91</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>While the two position-features provide specific context information (may be sparse for large monolingual corpora), this feature is more generic and captures long-distance co-occurrence statistics.</text>
              <doc_id>92</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Source Language: Words appearing in a source sentence f are represented using the corresponding target translation e = e 1 ...e m generated for f in the current sample during training.</text>
              <doc_id>93</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>For each source word f j &#8712; f, we look at the corresponding word e j in the target translation.</text>
              <doc_id>94</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We then extract all the context features of e j in the target translation sample sentence e and add these features (f &#8722;context , f +context , f scontext ) with weights to the feature representation for f j .</text>
              <doc_id>95</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Unlike the target word feature vectors (which can be pre-computed from the monolingual target corpus), the feature vector for every source word f j is dynamically constructed from the target translation sampled in each training iteration.</text>
              <doc_id>96</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>This is a key distinction of our framework compared to previous approaches that use contextual similarity (or any other) features constructed from static monolingual corpora (Rapp, 1995; Koehn and Knight, 2000; Nuhn et al., 2012).</text>
              <doc_id>97</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Note that as we add more and more features for a particular word (by training on larger monolingual corpora or adding new types of features, etc.), it results in the feature representation becoming more sparse (especially for source feature vectors) which can cause problems in efficiency as well as robustness when computing similarity against other vectors.</text>
              <doc_id>98</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In the next section, we will describe how we mitigate this problem by projecting into a low-dimensional space by computing hash signatures.</text>
              <doc_id>99</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In all our experiments, we only use the features described above for representing source and target words.</text>
              <doc_id>100</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We note that the new sampling framework is easily extensible to many additional feature types (for example, monolingual topic model features, etc.) which can be efficiently handled by our inference algorithm and could further improve translation performance but we leave this for future work.</text>
              <doc_id>101</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Bayesian MT Decipherment via Hash Sampling</title>
        <text>The next step is to use the feature representations described earlier and iteratively sample a target word (or phrase) translation candidate e i for every word f i in the source text f. This involves choosing from |V e | possible target candidates in every step which can be highly inefficient (and infeasible for large vocabulary sizes). One possible strategy is to compute similarity scores s(w fi , w e &#8242;) between the current source word feature vector w fi and feature vectors w e &#8242; &#8712;V e for all possible candidates in the target vocabulary. Following this, we can prune the translation candidate set by keeping only the top candidates e &#8727; according to the similarity scores. Nuhn et al. (2012) use a similar strategy to obtain a more compact translation table that improves runtime efficiency for EM training. Their approach requires calculating and sorting all |V e | &#183; |V f | distances in time O(V 2 &#183; log(V )), where V = max(|V e |, |V f |).
Challenges: Unfortunately, there are several additional challenges which makes inference very hard in our case. Firstly, we would like to include as many features as possible to represent the source/target words in our framework besides simple bag-of-words context similarity (for example, left-context, right-context, and other generalpurpose features based on topic models, etc.). This makes the complexity far worse (in practice) since the dimensionality of the feature vectors d is a much higher value than |V e |. Computing similarity scores alone (na&#239;vely) would incur O(|V e | &#183; d) time which is prohibitively huge since we have to do this for every token in the source language corpus. Secondly, for Bayesian inference we need to sample from a distribution that involves computing probabilities for all the components (language model, translation model, fertility, etc.) described in Equation 1. This distribution needs to be computed for every source word token f i in the corpus, for all possible candidates e i &#8712; V e and the process has to be repeated for multiple sampling iterations (typically more than 1000). Doing standard collapsed Gibbs sampling in this scenario would be very slow and intractable.
We now present an alternative fast, efficient inference strategy that overcomes many of the challenges described above and helps accelerate the sampling process significantly. First, we set our translation models within the context of a more generic and widely known family of distributions&#8212;mixtures of exponential families. Then we derive a novel proposal distribution for sampling translation candidates and introduce a new sampler for decipherment training that
is based on locality sensitive hashing (LSH).
Hashing methods such as LSH have been widely used in the past in several scenarios including NLP applications (Ravichandran et al., 2005). Most of these approaches employ LSH within heuristic methods for speeding up nearestneighbor look up and similarity computation techniques. However, we use LSH hashing within a probabilistic framework which is very different from the typical use of LSH.
Our work is inspired by some recent work by Ahmed et al. (2012) on speeding up Bayesian inference for unsupervised clustering. We use a similar technique as theirs but a different approximate distribution for the proposal, one that is bettersuited for machine translation models and without some of the additional overhead required for computing certain terms in the original formulation. Mixtures of Exponential Families: The translation models described earlier (Section 2) can be represented as mixtures of exponential families, specifically mixtures of multinomials. In exponential families, distributions over random variables are given by:
p(x; &#952;) = exp(&#12296;&#966;(x), &#952;&#12297;) &#8722; g(&#952;) (4)
where, &#966; : X &#8594; F is a map from x to the space of sufficient statistics and &#952; &#8712; F. The term g(&#952;) ensures that p(x; &#952;) is properly normalized. X is the domain of observations X = x 1 , ..., x m drawn from some distribution p. Our goal is to estimate p. In our case, this refers to the translation model from Equation 1.
We also choose corresponding conjugate Dirichlet distributions for priors which have the property that the posterior distribution p(&#952;|X) over &#952; remains in the same family as p(&#952;).
Note that the (translation) model in our case consists of multiple exponential families components&#8212;a multinomial pertaining to the language model (which remains fixed 5 ), and other components pertaining to translation probabilities P &#952; (f i |e i ), fertility P &#952;fert , etc. To do collapsed Gibbs sampling under this model, we would perform the following steps during sampling: 1. For a given source word token f i draw target
5 A high value for the LM concentration parameter &#945; ensures that the LM probabilities do not deviate too far from the original fixed base distribution during sampling.
translation
e i &#8764; p(e i |F, E &#8722;i )
&#8733; p(e) &#183; p(f i |e i , F &#8722;i , E &#8722;i )
&#183; p fert (&#183;|e i , F &#8722;i , E &#8722;i ) &#183; ... (5)
where, F is the full source text and E the full target translation generated during sampling. 2. Update the sufficient statistics for the changed target translation assignments.
For large target vocabularies, computing p(f i |e i , F &#8722;i , E &#8722;i ) dominates the inference procedure. We can accelerate this step significantly using a good proposal distribution via hashing. Locality Sensitive Hash Sampling: For general exponential families, here is a Taylor approximation for the data likelihood term (Ahmed et al., 2012):
p(x|&#183;) &#8776; exp(&#12296;&#966;(x), &#952; &#8727; &#12297;) &#8722; g(&#952; &#8727; ) (6)
where, &#952; &#8727; is the expected parameter (sufficient statistics). For sampling the translation model, this involves computing an expensive inner product &#12296;&#966;(f i ), &#952;e &#8727; &#12297; &#8242; for each source word f i which has to be repeated for every translation candidate e &#8242; , including candidates that have very low probabilities and are unlikely to be chosen as the translation for f j .
So, during decipherment training a standard collapsed Gibbs sampler will waste most of its time on expensive computations that will be discarded in the end anyways. Also, unlike some standard generative models used in other unsupervised learning scenarios (e.g., clustering) that model only observed features (namely words appearing in the document), here we would like to enrich the translation model with a lot more features (side-information). Instead, we can accelerate the computation of the inner product &#12296;&#966;(f i ), &#952;e &#8727; &#12297; using a hash sampling strategy similar to (Ahmed et al., 2012). &#8242;
The underlying idea here is to use binary hashing (Charikar, 2002) to explore only those candidates e &#8242; that are sufficiently close to the best matching translation via a proposal distribution. Next, we briefly introduce some notations and existing theoretical results related to binary hashing before describing the hash sampling procedure.
For any two vectors u, v &#8712; R n ,
&#12296;u, v&#12297; = &#8214;u&#8214; &#183; &#8214;v&#8214; &#183; cos &#8737;(u, v) (7)
&#8737;(u, v) = &#960;P r{sgn[&#12296;u, w&#12297;] &#8800; sgn[&#12296;v, w&#12297;]} (8)
where, w is a random vector drawn from a symmetric spherical distribution and the term inside P r{&#183;} represents the relation between the signs of the two inner products.
Let h l (v) &#8712; {0, 1} l be an l-bit binary hash of v where: [h l (v)] i := sgn[&#12296;v, w i &#12297;]; w i &#8764; U m . Then the probability of matching signs is given by:
z l (u, v) := 1 l &#8214;h(u) &#8722; h(v)&#8214; 1 (9)
So, z l (u, v) measures how many bits differ between the hash vectors h(u) and h(v) associated with u, v. Combining this with Equations 6 and 7 we can estimate the unnormalized log-likelihood of a source word f i being translated as target e &#8242; via:
s l (f i , e &#8242; ) &#8733; &#8214;&#952; e &#8242;&#8214; &#183; &#8214;&#966;(f i )&#8214; &#183; cos &#960;z l (&#966;(f i ), &#952; e &#8242;) (10)
For each source word f i , we now sample from this new distribution (after normalization) instead of the original one. The binary hash representation for the two vectors yield significant speedups during sampling since Hamming distance computation between h(u) and h(v) is highly optimized on modern CPUs. Hence, we can compute an estimate for the inner product quite efficiently. 6 Updating the hash signatures: During training, we compute the target candidate projection h(&#952; e &#8242;) and corresponding norm only once 7 which is different from the setup of Ahmed et al. (2012). The source word projection &#966;(f i ) is dynamically updated in every sampling step. Note that doing this na&#239;vely would scale slowly as O(Dl) where D is the total number of features but instead we can update the hash signatures in a more efficient manner that scales as O(D i&gt;0 l) where D i&gt;0 is the number of non-zero entries in the feature representation for the source word &#966;(f i ). Also, we do not need to store the random vectors w in practice since these can be computed on the fly using hash functions. The inner product approximation also yields some theoretical guarantees for the hash sampler. 8
6 We set l = 32 bits in our experiments. 7 In practice, we can ignore the norm terms to further
speed up sampling since this is only an estimate for the proposal distribution and we follow this with the Metropolis Hastings step. 8 For further details, please refer to (Ahmed et al., 2012).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The next step is to use the feature representations described earlier and iteratively sample a target word (or phrase) translation candidate e i for every word f i in the source text f. This involves choosing from |V e | possible target candidates in every step which can be highly inefficient (and infeasible for large vocabulary sizes).</text>
              <doc_id>102</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>One possible strategy is to compute similarity scores s(w fi , w e &#8242;) between the current source word feature vector w fi and feature vectors w e &#8242; &#8712;V e for all possible candidates in the target vocabulary.</text>
              <doc_id>103</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Following this, we can prune the translation candidate set by keeping only the top candidates e &#8727; according to the similarity scores.</text>
              <doc_id>104</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Nuhn et al. (2012) use a similar strategy to obtain a more compact translation table that improves runtime efficiency for EM training.</text>
              <doc_id>105</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Their approach requires calculating and sorting all |V e | &#183; |V f | distances in time O(V 2 &#183; log(V )), where V = max(|V e |, |V f |).</text>
              <doc_id>106</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Challenges: Unfortunately, there are several additional challenges which makes inference very hard in our case.</text>
              <doc_id>107</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Firstly, we would like to include as many features as possible to represent the source/target words in our framework besides simple bag-of-words context similarity (for example, left-context, right-context, and other generalpurpose features based on topic models, etc.).</text>
              <doc_id>108</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This makes the complexity far worse (in practice) since the dimensionality of the feature vectors d is a much higher value than |V e |.</text>
              <doc_id>109</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Computing similarity scores alone (na&#239;vely) would incur O(|V e | &#183; d) time which is prohibitively huge since we have to do this for every token in the source language corpus.</text>
              <doc_id>110</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Secondly, for Bayesian inference we need to sample from a distribution that involves computing probabilities for all the components (language model, translation model, fertility, etc.) described in Equation 1.</text>
              <doc_id>111</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This distribution needs to be computed for every source word token f i in the corpus, for all possible candidates e i &#8712; V e and the process has to be repeated for multiple sampling iterations (typically more than 1000).</text>
              <doc_id>112</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Doing standard collapsed Gibbs sampling in this scenario would be very slow and intractable.</text>
              <doc_id>113</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We now present an alternative fast, efficient inference strategy that overcomes many of the challenges described above and helps accelerate the sampling process significantly.</text>
              <doc_id>114</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>First, we set our translation models within the context of a more generic and widely known family of distributions&#8212;mixtures of exponential families.</text>
              <doc_id>115</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Then we derive a novel proposal distribution for sampling translation candidates and introduce a new sampler for decipherment training that</text>
              <doc_id>116</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>is based on locality sensitive hashing (LSH).</text>
              <doc_id>117</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Hashing methods such as LSH have been widely used in the past in several scenarios including NLP applications (Ravichandran et al., 2005).</text>
              <doc_id>118</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Most of these approaches employ LSH within heuristic methods for speeding up nearestneighbor look up and similarity computation techniques.</text>
              <doc_id>119</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, we use LSH hashing within a probabilistic framework which is very different from the typical use of LSH.</text>
              <doc_id>120</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Our work is inspired by some recent work by Ahmed et al. (2012) on speeding up Bayesian inference for unsupervised clustering.</text>
              <doc_id>121</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We use a similar technique as theirs but a different approximate distribution for the proposal, one that is bettersuited for machine translation models and without some of the additional overhead required for computing certain terms in the original formulation.</text>
              <doc_id>122</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Mixtures of Exponential Families: The translation models described earlier (Section 2) can be represented as mixtures of exponential families, specifically mixtures of multinomials.</text>
              <doc_id>123</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In exponential families, distributions over random variables are given by:</text>
              <doc_id>124</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>p(x; &#952;) = exp(&#12296;&#966;(x), &#952;&#12297;) &#8722; g(&#952;) (4)</text>
              <doc_id>125</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where, &#966; : X &#8594; F is a map from x to the space of sufficient statistics and &#952; &#8712; F. The term g(&#952;) ensures that p(x; &#952;) is properly normalized.</text>
              <doc_id>126</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>X is the domain of observations X = x 1 , ..., x m drawn from some distribution p.</text>
              <doc_id>127</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Our goal is to estimate p.</text>
              <doc_id>128</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In our case, this refers to the translation model from Equation 1.</text>
              <doc_id>129</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We also choose corresponding conjugate Dirichlet distributions for priors which have the property that the posterior distribution p(&#952;|X) over &#952; remains in the same family as p(&#952;).</text>
              <doc_id>130</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Note that the (translation) model in our case consists of multiple exponential families components&#8212;a multinomial pertaining to the language model (which remains fixed 5 ), and other components pertaining to translation probabilities P &#952; (f i |e i ), fertility P &#952;fert , etc. To do collapsed Gibbs sampling under this model, we would perform the following steps during sampling: 1.</text>
              <doc_id>131</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For a given source word token f i draw target</text>
              <doc_id>132</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>5 A high value for the LM concentration parameter &#945; ensures that the LM probabilities do not deviate too far from the original fixed base distribution during sampling.</text>
              <doc_id>133</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>translation</text>
              <doc_id>134</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e i &#8764; p(e i |F, E &#8722;i )</text>
              <doc_id>135</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8733; p(e) &#183; p(f i |e i , F &#8722;i , E &#8722;i )</text>
              <doc_id>136</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#183; p fert (&#183;|e i , F &#8722;i , E &#8722;i ) &#183; ... (5)</text>
              <doc_id>137</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where, F is the full source text and E the full target translation generated during sampling.</text>
              <doc_id>138</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2.</text>
              <doc_id>139</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Update the sufficient statistics for the changed target translation assignments.</text>
              <doc_id>140</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For large target vocabularies, computing p(f i |e i , F &#8722;i , E &#8722;i ) dominates the inference procedure.</text>
              <doc_id>141</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We can accelerate this step significantly using a good proposal distribution via hashing.</text>
              <doc_id>142</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Locality Sensitive Hash Sampling: For general exponential families, here is a Taylor approximation for the data likelihood term (Ahmed et al., 2012):</text>
              <doc_id>143</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>p(x|&#183;) &#8776; exp(&#12296;&#966;(x), &#952; &#8727; &#12297;) &#8722; g(&#952; &#8727; ) (6)</text>
              <doc_id>144</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where, &#952; &#8727; is the expected parameter (sufficient statistics).</text>
              <doc_id>145</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For sampling the translation model, this involves computing an expensive inner product &#12296;&#966;(f i ), &#952;e &#8727; &#12297; &#8242; for each source word f i which has to be repeated for every translation candidate e &#8242; , including candidates that have very low probabilities and are unlikely to be chosen as the translation for f j .</text>
              <doc_id>146</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>So, during decipherment training a standard collapsed Gibbs sampler will waste most of its time on expensive computations that will be discarded in the end anyways.</text>
              <doc_id>147</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Also, unlike some standard generative models used in other unsupervised learning scenarios (e.g., clustering) that model only observed features (namely words appearing in the document), here we would like to enrich the translation model with a lot more features (side-information).</text>
              <doc_id>148</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Instead, we can accelerate the computation of the inner product &#12296;&#966;(f i ), &#952;e &#8727; &#12297; using a hash sampling strategy similar to (Ahmed et al., 2012).</text>
              <doc_id>149</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>&#8242;</text>
              <doc_id>150</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The underlying idea here is to use binary hashing (Charikar, 2002) to explore only those candidates e &#8242; that are sufficiently close to the best matching translation via a proposal distribution.</text>
              <doc_id>151</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Next, we briefly introduce some notations and existing theoretical results related to binary hashing before describing the hash sampling procedure.</text>
              <doc_id>152</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For any two vectors u, v &#8712; R n ,</text>
              <doc_id>153</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#12296;u, v&#12297; = &#8214;u&#8214; &#183; &#8214;v&#8214; &#183; cos &#8737;(u, v) (7)</text>
              <doc_id>154</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8737;(u, v) = &#960;P r{sgn[&#12296;u, w&#12297;] &#8800; sgn[&#12296;v, w&#12297;]} (8)</text>
              <doc_id>155</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where, w is a random vector drawn from a symmetric spherical distribution and the term inside P r{&#183;} represents the relation between the signs of the two inner products.</text>
              <doc_id>156</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Let h l (v) &#8712; {0, 1} l be an l-bit binary hash of v where: [h l (v)] i := sgn[&#12296;v, w i &#12297;]; w i &#8764; U m .</text>
              <doc_id>157</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Then the probability of matching signs is given by:</text>
              <doc_id>158</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>z l (u, v) := 1 l &#8214;h(u) &#8722; h(v)&#8214; 1 (9)</text>
              <doc_id>159</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>So, z l (u, v) measures how many bits differ between the hash vectors h(u) and h(v) associated with u, v.</text>
              <doc_id>160</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Combining this with Equations 6 and 7 we can estimate the unnormalized log-likelihood of a source word f i being translated as target e &#8242; via:</text>
              <doc_id>161</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>s l (f i , e &#8242; ) &#8733; &#8214;&#952; e &#8242;&#8214; &#183; &#8214;&#966;(f i )&#8214; &#183; cos &#960;z l (&#966;(f i ), &#952; e &#8242;) (10)</text>
              <doc_id>162</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For each source word f i , we now sample from this new distribution (after normalization) instead of the original one.</text>
              <doc_id>163</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The binary hash representation for the two vectors yield significant speedups during sampling since Hamming distance computation between h(u) and h(v) is highly optimized on modern CPUs.</text>
              <doc_id>164</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Hence, we can compute an estimate for the inner product quite efficiently.</text>
              <doc_id>165</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>6 Updating the hash signatures: During training, we compute the target candidate projection h(&#952; e &#8242;) and corresponding norm only once 7 which is different from the setup of Ahmed et al. (2012).</text>
              <doc_id>166</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The source word projection &#966;(f i ) is dynamically updated in every sampling step.</text>
              <doc_id>167</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Note that doing this na&#239;vely would scale slowly as O(Dl) where D is the total number of features but instead we can update the hash signatures in a more efficient manner that scales as O(D i&gt;0 l) where D i&gt;0 is the number of non-zero entries in the feature representation for the source word &#966;(f i ).</text>
              <doc_id>168</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Also, we do not need to store the random vectors w in practice since these can be computed on the fly using hash functions.</text>
              <doc_id>169</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The inner product approximation also yields some theoretical guarantees for the hash sampler.</text>
              <doc_id>170</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>8</text>
              <doc_id>171</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>6 We set l = 32 bits in our experiments.</text>
              <doc_id>172</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>7 In practice, we can ignore the norm terms to further</text>
              <doc_id>173</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>speed up sampling since this is only an estimate for the proposal distribution and we follow this with the Metropolis Hastings step.</text>
              <doc_id>174</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>8 For further details, please refer to (Ahmed et al., 2012).</text>
              <doc_id>175</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Metropolis Hastings</title>
            <text>In each sampling step, we use the distribution from Equation 10 as a proposal distribution in a Metropolis Hastings scheme to sample target translations for each source word.
Once a new target translation e &#8242; is sampled for source word f i from the proposal distribution q(&#183;) &#8733; exp sl (f i ,e &#8242;) , we accept the proposal (and update the corresponding hash signatures) according to the probability r
r = q(eold i ) &#183; p new (&#183;) q(e new i ) &#183; p old (&#183;) (11)
where, p old (&#183;), p new (&#183;) are the true conditional likelihood probabilities according to our model (including the language model component) for the old, new sample respectively.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In each sampling step, we use the distribution from Equation 10 as a proposal distribution in a Metropolis Hastings scheme to sample target translations for each source word.</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Once a new target translation e &#8242; is sampled for source word f i from the proposal distribution q(&#183;) &#8733; exp sl (f i ,e &#8242;) , we accept the proposal (and update the corresponding hash signatures) according to the probability r</text>
                  <doc_id>177</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r = q(eold i ) &#183; p new (&#183;) q(e new i ) &#183; p old (&#183;) (11)</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where, p old (&#183;), p new (&#183;) are the true conditional likelihood probabilities according to our model (including the language model component) for the old, new sample respectively.</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Training Algorithm</title>
        <text>Putting together all the pieces described in the previous section, we perform the following steps: 1. Initialization: We initialize the starting sample as follows: for each source word token, randomly sample a target word. If the source word also exists in the target vocabulary, then choose identity translation instead of the random one. 9 2. Hash Sampling Steps: For each source word token f i , run the hash sampler:
(a) Generate a proposal distribution by computing the hamming distance between the feature vectors for the source word and each target translation candidate. Sample a new target translation e i for f i from this distribution.
(b) Compute the acceptance probability for the chosen translation using a Metropolis Hastings scheme and accept (or reject) the sample. In practice, computation of the acceptance probability only needs to be done every r iterations (where r can be anywhere from 5 or 100). Iterate through steps (2a) and (2b) for every word in the source text and then repeat this process for multiple iterations (usually 1000). 3. Other Sampling Operators: After every k iterations, 10 perform the following sampling operations:
(a) Re-ordering: For each source word token f i at position i, randomly choose another position j
9 Initializing with identity translation rather than random
choice helps in some cases, especially for unknown words that involve named entities, etc. 10 We set k = 3 in our experiments.
Corpus Language Sent. Words Vocab.
in the source sentence and swap the translations e i with e j . During the sampling process, we compute the probabilities for the two samples&#8212;the original and the swapped versions, and then sample an alignment from this distribution.
(b) Deletion: For each source word token, delete the current target translation (i.e., align it with the target NULL token). As with the reordering operation, we sample from a distribution consisting of the original and the deleted versions. 4. Decoding the foreign sentence: Finally, once the training is done (i.e., after all sampling iterations) we choose the final sample as our target translation output for the source text.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Putting together all the pieces described in the previous section, we perform the following steps: 1.</text>
              <doc_id>180</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Initialization: We initialize the starting sample as follows: for each source word token, randomly sample a target word.</text>
              <doc_id>181</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>If the source word also exists in the target vocabulary, then choose identity translation instead of the random one.</text>
              <doc_id>182</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>9 2.</text>
              <doc_id>183</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Hash Sampling Steps: For each source word token f i , run the hash sampler:</text>
              <doc_id>184</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(a) Generate a proposal distribution by computing the hamming distance between the feature vectors for the source word and each target translation candidate.</text>
              <doc_id>185</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Sample a new target translation e i for f i from this distribution.</text>
              <doc_id>186</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(b) Compute the acceptance probability for the chosen translation using a Metropolis Hastings scheme and accept (or reject) the sample.</text>
              <doc_id>187</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In practice, computation of the acceptance probability only needs to be done every r iterations (where r can be anywhere from 5 or 100).</text>
              <doc_id>188</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Iterate through steps (2a) and (2b) for every word in the source text and then repeat this process for multiple iterations (usually 1000).</text>
              <doc_id>189</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>3.</text>
              <doc_id>190</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Other Sampling Operators: After every k iterations, 10 perform the following sampling operations:</text>
              <doc_id>191</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(a) Re-ordering: For each source word token f i at position i, randomly choose another position j</text>
              <doc_id>192</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>9 Initializing with identity translation rather than random</text>
              <doc_id>193</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>choice helps in some cases, especially for unknown words that involve named entities, etc. 10 We set k = 3 in our experiments.</text>
              <doc_id>194</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Corpus Language Sent.</text>
              <doc_id>195</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Words Vocab.</text>
              <doc_id>196</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>in the source sentence and swap the translations e i with e j .</text>
              <doc_id>197</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>During the sampling process, we compute the probabilities for the two samples&#8212;the original and the swapped versions, and then sample an alignment from this distribution.</text>
              <doc_id>198</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(b) Deletion: For each source word token, delete the current target translation (i.e., align it with the target NULL token).</text>
              <doc_id>199</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As with the reordering operation, we sample from a distribution consisting of the original and the deleted versions.</text>
              <doc_id>200</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>4.</text>
              <doc_id>201</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Decoding the foreign sentence: Finally, once the training is done (i.e., after all sampling iterations) we choose the final sample as our target translation output for the source text.</text>
              <doc_id>202</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>6 Experiments and Results</title>
        <text>We test our method on two different corpora. To evaluate translation quality, we use BLEU score (Papineni et al., 2002), a standard evaluation measure used in machine translation. First, we present MT results on non-parallel Spanish/English data from the OPUS corpus (Tiedemann, 2009) which was used by Ravi and Knight (2011b) and Nuhn et al. (2012). We show that our method achieves the best performance (BLEU scores) on this task while being significantly faster than both the previous approaches. We then apply our method to a much larger non-parallel French/Spanish corpus constructed from the EMEA corpus (Tiedemann, 2009). Here the vocabulary sizes are much larger and we show how our new Bayesian decipherment method scales well to this task inspite of using complex translation models. We also report the first BLEU results on such a large-scale MT task under truly non-parallel settings (without using any parallel data or seed lexicon).
For both the MT tasks, we also report BLEU scores for a baseline system using identity translations for common words (words appearing in both source/target vocabularies) and random translations for other words.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We test our method on two different corpora.</text>
              <doc_id>203</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To evaluate translation quality, we use BLEU score (Papineni et al., 2002), a standard evaluation measure used in machine translation.</text>
              <doc_id>204</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>First, we present MT results on non-parallel Spanish/English data from the OPUS corpus (Tiedemann, 2009) which was used by Ravi and Knight (2011b) and Nuhn et al. (2012).</text>
              <doc_id>205</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We show that our method achieves the best performance (BLEU scores) on this task while being significantly faster than both the previous approaches.</text>
              <doc_id>206</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We then apply our method to a much larger non-parallel French/Spanish corpus constructed from the EMEA corpus (Tiedemann, 2009).</text>
              <doc_id>207</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Here the vocabulary sizes are much larger and we show how our new Bayesian decipherment method scales well to this task inspite of using complex translation models.</text>
              <doc_id>208</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We also report the first BLEU results on such a large-scale MT task under truly non-parallel settings (without using any parallel data or seed lexicon).</text>
              <doc_id>209</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For both the MT tasks, we also report BLEU scores for a baseline system using identity translations for common words (words appearing in both source/target vocabularies) and random translations for other words.</text>
              <doc_id>210</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>6.1 MT Task and Data</title>
            <text>OPUS movie subtitle corpus (Tiedemann, 2009): This is a large open source collection of parallel corpora available for multiple language pairs. We use the same non-parallel Spanish/English corpus used in previous works (Ravi and Knight, 2011b; Nuhn et al., 2012). The details of the corpus are listed in Table 1. We use the entire Spanish source text for decipherment training and evaluate the final English output to report BLEU scores. EMEA corpus (Tiedemann, 2009): This is a parallel corpus made out of PDF documents (articles from the medical domain) from the European Medicines Agency. We reserve the first 1k sentences in French as our source text (also used in decipherment training). To construct a nonparallel corpus, we split the remaining 1.1M lines as follows: first 550k sentences in French, last 550k sentences in Spanish. The latter is used to construct a target language model used for decipherment training. The corpus statistics are shown in Table 1.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>OPUS movie subtitle corpus (Tiedemann, 2009): This is a large open source collection of parallel corpora available for multiple language pairs.</text>
                  <doc_id>211</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We use the same non-parallel Spanish/English corpus used in previous works (Ravi and Knight, 2011b; Nuhn et al., 2012).</text>
                  <doc_id>212</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The details of the corpus are listed in Table 1.</text>
                  <doc_id>213</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We use the entire Spanish source text for decipherment training and evaluate the final English output to report BLEU scores.</text>
                  <doc_id>214</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>EMEA corpus (Tiedemann, 2009): This is a parallel corpus made out of PDF documents (articles from the medical domain) from the European Medicines Agency.</text>
                  <doc_id>215</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We reserve the first 1k sentences in French as our source text (also used in decipherment training).</text>
                  <doc_id>216</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>To construct a nonparallel corpus, we split the remaining 1.1M lines as follows: first 550k sentences in French, last 550k sentences in Spanish.</text>
                  <doc_id>217</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>The latter is used to construct a target language model used for decipherment training.</text>
                  <doc_id>218</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>The corpus statistics are shown in Table 1.</text>
                  <doc_id>219</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>6.2 Results</title>
            <text>OPUS: We compare the MT results (BLEU scores) from different systems on the OPUS corpus in Table 2. The first row displays baseline performance. The next three rows 1a&#8211;1c display performance achieved by two methods from Ravi and Knight (2011b). Rows 2a, 2b show results from the of Nuhn et al. (2012). The last two rows display results for the new method using Bayesian hash sampling. Overall, using a 3-gram language model (instead of 2-gram) for decipherment training improves the performance for all methods. We observe that our method produces much better results than the others even with a 2-gram LM. With a 3-gram LM, the new method achieves the best performance; the highest BLEU score reported on this task. It is also interesting to note that the hash sampling method yields much better results than the Bayesian inference method presented in (Ravi and Knight, 2011b). This is due to the accelerated sampling scheme introduced earlier which helps it converge to better solutions faster. Table 2 (last column) also compares the efficiency of different methods in terms of CPU time required for training. Both our 2-gram and 3-gram based methods are significantly faster than those previously reported for EM based training methods presented in (Ravi and Knight, 2011b; Nuhn
et al., 2012). This is very encouraging since Nuhn et al. (2012) reported obtaining a speedup by pruning translation candidates (to &#8764;1/8th the original size) prior to EM training. On the other hand, we sample from the full set of translation candidates including additional target phrase (of size 2) candidates which results in a much larger vocabulary consisting of 1600 candidates (&#8764;4 times the original size), yet our method runs much faster and yields better results. The table also demonstrates the siginificant speedup achieved by the hash sampler over a standard Gibbs sampler for the same model (&#8764;85 times faster when using a 2-gram LM). We also compare the results against MT performance from parallel training&#8212;MOSES system (Koehn et al., 2007) trained on 20k sentence pairs. The comparable number for Table 2 is 63.6 BLEU.
Spanish (e) French (f)
EMEA Results Table 3 shows the results achieved by our method on the larger task involving EMEA corpus. Here, the target vocabulary V e is much higher (67k). In spite of this challenge and the model complexity, we can still perform decipherment training using Bayesian inference. We report the first BLEU score results on such a large-scale task using a 2-gram LM. This is achieved without using any seed lexicon or parallel corpora. The results are encouraging and demonstrates the ability of the method to scale to large-scale settings while performing efficient inference with complex models, which we believe will be especially useful for future MT application in scenarios where parallel data is hard to obtain. Table 4 displays some sample 1-best translations learned using this method.
For comparison purposes, we also evaluate MT performance on this task using parallel training (MOSES trained with hundred sentence pairs) and observe a BLEU score of 11.7.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>OPUS: We compare the MT results (BLEU scores) from different systems on the OPUS corpus in Table 2.</text>
                  <doc_id>220</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The first row displays baseline performance.</text>
                  <doc_id>221</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The next three rows 1a&#8211;1c display performance achieved by two methods from Ravi and Knight (2011b).</text>
                  <doc_id>222</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Rows 2a, 2b show results from the of Nuhn et al. (2012).</text>
                  <doc_id>223</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The last two rows display results for the new method using Bayesian hash sampling.</text>
                  <doc_id>224</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Overall, using a 3-gram language model (instead of 2-gram) for decipherment training improves the performance for all methods.</text>
                  <doc_id>225</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>We observe that our method produces much better results than the others even with a 2-gram LM.</text>
                  <doc_id>226</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>With a 3-gram LM, the new method achieves the best performance; the highest BLEU score reported on this task.</text>
                  <doc_id>227</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>It is also interesting to note that the hash sampling method yields much better results than the Bayesian inference method presented in (Ravi and Knight, 2011b).</text>
                  <doc_id>228</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>This is due to the accelerated sampling scheme introduced earlier which helps it converge to better solutions faster.</text>
                  <doc_id>229</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>Table 2 (last column) also compares the efficiency of different methods in terms of CPU time required for training.</text>
                  <doc_id>230</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>Both our 2-gram and 3-gram based methods are significantly faster than those previously reported for EM based training methods presented in (Ravi and Knight, 2011b; Nuhn</text>
                  <doc_id>231</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>et al., 2012).</text>
                  <doc_id>232</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This is very encouraging since Nuhn et al. (2012) reported obtaining a speedup by pruning translation candidates (to &#8764;1/8th the original size) prior to EM training.</text>
                  <doc_id>233</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>On the other hand, we sample from the full set of translation candidates including additional target phrase (of size 2) candidates which results in a much larger vocabulary consisting of 1600 candidates (&#8764;4 times the original size), yet our method runs much faster and yields better results.</text>
                  <doc_id>234</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The table also demonstrates the siginificant speedup achieved by the hash sampler over a standard Gibbs sampler for the same model (&#8764;85 times faster when using a 2-gram LM).</text>
                  <doc_id>235</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We also compare the results against MT performance from parallel training&#8212;MOSES system (Koehn et al., 2007) trained on 20k sentence pairs.</text>
                  <doc_id>236</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The comparable number for Table 2 is 63.6 BLEU.</text>
                  <doc_id>237</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Spanish (e) French (f)</text>
                  <doc_id>238</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>EMEA Results Table 3 shows the results achieved by our method on the larger task involving EMEA corpus.</text>
                  <doc_id>239</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Here, the target vocabulary V e is much higher (67k).</text>
                  <doc_id>240</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In spite of this challenge and the model complexity, we can still perform decipherment training using Bayesian inference.</text>
                  <doc_id>241</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We report the first BLEU score results on such a large-scale task using a 2-gram LM.</text>
                  <doc_id>242</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This is achieved without using any seed lexicon or parallel corpora.</text>
                  <doc_id>243</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The results are encouraging and demonstrates the ability of the method to scale to large-scale settings while performing efficient inference with complex models, which we believe will be especially useful for future MT application in scenarios where parallel data is hard to obtain.</text>
                  <doc_id>244</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Table 4 displays some sample 1-best translations learned using this method.</text>
                  <doc_id>245</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For comparison purposes, we also evaluate MT performance on this task using parallel training (MOSES trained with hundred sentence pairs) and observe a BLEU score of 11.7.</text>
                  <doc_id>246</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>7</index>
        <title>7 Discussion and Future Work</title>
        <text>There exists some work (Dou and Knight, 2012; Klementiev et al., 2012) that uses monolingual corpora to induce phrase tables, etc. These when combined with standard MT systems such as Moses (Koehn et al., 2007) trained on parallel corpora, have been shown to yield some BLEU score improvements. Nuhn et al. (2012) show some sample English/French lexicon entries learnt using EM algorithm with a pruned translation candidate set on a portion of the Gigaword corpus 11 but do not report any actual MT results. In addition, as we showed earlier our method can use Bayesian inference (which has a lot of nice properties compared to EM for unsupervised natural language tasks (Johnson, 2007; Goldwater and Griffiths, 2007)) and still scale easily to large vocabulary, data sizes while allowing the models to grow in complexity. Most importantly, our method produces better translation results (as demonstrated on the OPUS MT task). And to our knowledge, this is the first time that anyone has reported MT results under truly non-parallel settings on such a large-scale task (EMEA). Our method is also easily extensible to outof-domain translation scenarios similar to (Dou and Knight, 2012). While their work also uses Bayesian inference with a slice sampling scheme, our new approach uses a novel hash sampling scheme for decipherment that can easily scale to more complex models. The new decipherment framework also allows one to easily incorporate additional information (besides standard word translations) as features (e.g., context features, topic features, etc.) for unsupervised machine translation which can help further improve the performance in addition to accelerating the sampling process. We already demonstrated the utility of this system by going beyond words and incorporating phrase translations in a decipherment model for the first time. In the future, we can obtain further speedups
(especially for large-scale tasks) by parallelizing the sampling scheme seamlessly across multiple machines and CPU cores. The new framework can also be stacked with complementary techniques such as slice sampling, blocked (and type) sampling to further improve inference efficiency.
11 http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?
catalogId=LDC2003T05</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>There exists some work (Dou and Knight, 2012; Klementiev et al., 2012) that uses monolingual corpora to induce phrase tables, etc.</text>
              <doc_id>247</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>These when combined with standard MT systems such as Moses (Koehn et al., 2007) trained on parallel corpora, have been shown to yield some BLEU score improvements.</text>
              <doc_id>248</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Nuhn et al. (2012) show some sample English/French lexicon entries learnt using EM algorithm with a pruned translation candidate set on a portion of the Gigaword corpus 11 but do not report any actual MT results.</text>
              <doc_id>249</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In addition, as we showed earlier our method can use Bayesian inference (which has a lot of nice properties compared to EM for unsupervised natural language tasks (Johnson, 2007; Goldwater and Griffiths, 2007)) and still scale easily to large vocabulary, data sizes while allowing the models to grow in complexity.</text>
              <doc_id>250</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Most importantly, our method produces better translation results (as demonstrated on the OPUS MT task).</text>
              <doc_id>251</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>And to our knowledge, this is the first time that anyone has reported MT results under truly non-parallel settings on such a large-scale task (EMEA).</text>
              <doc_id>252</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Our method is also easily extensible to outof-domain translation scenarios similar to (Dou and Knight, 2012).</text>
              <doc_id>253</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>While their work also uses Bayesian inference with a slice sampling scheme, our new approach uses a novel hash sampling scheme for decipherment that can easily scale to more complex models.</text>
              <doc_id>254</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>The new decipherment framework also allows one to easily incorporate additional information (besides standard word translations) as features (e.g., context features, topic features, etc.) for unsupervised machine translation which can help further improve the performance in addition to accelerating the sampling process.</text>
              <doc_id>255</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>We already demonstrated the utility of this system by going beyond words and incorporating phrase translations in a decipherment model for the first time.</text>
              <doc_id>256</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>In the future, we can obtain further speedups</text>
              <doc_id>257</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(especially for large-scale tasks) by parallelizing the sampling scheme seamlessly across multiple machines and CPU cores.</text>
              <doc_id>258</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The new framework can also be stacked with complementary techniques such as slice sampling, blocked (and type) sampling to further improve inference efficiency.</text>
              <doc_id>259</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>11 http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?</text>
              <doc_id>260</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>catalogId=LDC2003T05</text>
              <doc_id>261</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>8</index>
        <title>8 Conclusion</title>
        <text>To summarize, our method is significantly faster than previous methods based on EM or Bayesian with standard Gibbs sampling and obtains better results than any previously published methods for the same task. The new framework also allows performing Bayesian inference for decipherment applications with more complex models than previously shown. We believe this framework will be useful for further extending MT models in the future to improve translation performance and for many other unsupervised decipherment application scenarios.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>To summarize, our method is significantly faster than previous methods based on EM or Bayesian with standard Gibbs sampling and obtains better results than any previously published methods for the same task.</text>
              <doc_id>262</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The new framework also allows performing Bayesian inference for decipherment applications with more complex models than previously shown.</text>
              <doc_id>263</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We believe this framework will be useful for further extending MT models in the future to improve translation performance and for many other unsupervised decipherment application scenarios.</text>
              <doc_id>264</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Statistics of non-parallel corpora used here.</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>OPUS</cell>
              <cell>Spanish</cell>
              <cell>13,181</cell>
              <cell>39,185</cell>
              <cell>562</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell></cell>
              <cell>English</cell>
              <cell>19,770</cell>
              <cell>61,835</cell>
              <cell>411</cell>
            </row>
            <row>
              <cell>EMEA</cell>
              <cell>French</cell>
              <cell>550,000</cell>
              <cell>8,566,321</cell>
              <cell>41,733</cell>
            </row>
            <row>
              <cell></cell>
              <cell>Spanish</cell>
              <cell>550,000</cell>
              <cell>7,245,672</cell>
              <cell>67,446</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Comparison of MT performance (BLEU scores) and efficiency (running time in CPU hours) on the Spanish/English OPUS corpus using only non-parallel corpora for training. For the Bayesian methods 4a and 4b, the samplers were run for 1000 iterations each on a single machine (1.8GHz Intel processor). For 1a, 2a, 2b, we list the training times as reported by Nuhn et al. (2012) based on their EM implementation for different settings.</caption>
        <reference_text>In PAGE 7: ... This is due to the accelerated sampling scheme introduced earlier which helps it converge to better solutions faster.  Table2  (last column) also compares the effi- ciency of different methods in terms of CPU time required for training. Both our 2-gram and 3-gram based methods are significantly faster than those previously reported for EM based training meth- ods presented in (Ravi and Knight, 2011b; Nuhn...  In PAGE 8: ..., 2007) trained on 20k sentence pairs. The comparable number for  Table2  is 63.6 BLEU....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>4a. Bayesian Hash Sampling &#8727; with 2-gram LM (this work)</cell>
              <cell>20.3</cell>
              <cell>2.6h</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>4b. Bayesian Hash Sampling &#8727; with 3-gram LM (this work)</cell>
              <cell>21.2</cell>
              <cell>2.7h</cell>
            </row>
            <row>
              <cell>( &#8727; sampler was run for 1000 iterations)</cell>
              <cell></cell>
              <cell></cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: MT results on the French/Spanish EMEA corpus using the new hash sampling method. &#8727; The last row displays results when we sample target translations from a pruned candidate set (most frequent 1k Spanish words + identity translation candidates) which enables the sampler to run much faster when using more complex models.</caption>
        <reference_text>In PAGE 8: ... Spanish (e) French (f) el ? les la ? la por ? des secci? on ? rubrique administraci? on ? administration Table 4: Sample (1-best) Spanish/French transla- tions produced by the new method on the EMEA corpus using word translation models trained with non-parallel corpora. EMEA Results  Table3  shows the results achieved by our method on the larger task involving EMEA corpus. Here, the target vocabulary Ve is much higher (67k)....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>Bayesian Hash Sampling with 2-gram LM</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>vocab=full (V e ), add fertility=no</cell>
              <cell>4.2</cell>
            </row>
            <row>
              <cell>vocab=pruned &#8727; , add fertility=yes</cell>
              <cell>5.3</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: Sample (1-best) Spanish/French translations produced by the new method on the EMEA corpus using word translation models trained with non-parallel corpora.</caption>
        <reference_text>In PAGE 8: ... The re- sults are encouraging and demonstrates the ability of the method to scale to large-scale settings while performing efficient inference with complex mod- els, which we believe will be especially useful for future MT application in scenarios where parallel data is hard to obtain.  Table4  displays some sam- ple 1-best translations learned using this method. For comparison purposes, we also evaluate MT performance on this task using parallel training (MOSES trained with hundred sentence pairs) and observe a BLEU score of 11....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>Spanish (e)</cell>
              <cell>None</cell>
              <cell>French (f)</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>el</cell>
              <cell>?</cell>
              <cell>les</cell>
            </row>
            <row>
              <cell>la</cell>
              <cell>?</cell>
              <cell>la</cell>
            </row>
            <row>
              <cell>por</cell>
              <cell>?</cell>
              <cell>des</cell>
            </row>
            <row>
              <cell>secci? on</cell>
              <cell>?</cell>
              <cell>rubrique</cell>
            </row>
            <row>
              <cell>administraci? on</cell>
              <cell>?</cell>
              <cell>administration</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Amr Ahmed</author>
          <author>Sujith Ravi</author>
          <author>Shravan Narayanamurthy</author>
          <author>Alex Smola</author>
        </authors>
        <title>Fastex: Hash clustering with exponential families.</title>
        <publication>In Proceedings of the 26th Conference on Neural Information Processing Systems (NIPS).</publication>
        <pages>None</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Moses S Charikar</author>
        </authors>
        <title>Similarity estimation techniques from rounding algorithms.</title>
        <publication>In Proceedings of the thiry-fourth annual ACM Symposium on Theory of Computing,</publication>
        <pages>380--388</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>A P Dempster</author>
          <author>N M Laird</author>
          <author>D B Rubin</author>
        </authors>
        <title>Maximum likelihood from incomplete data via the em algorithm.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1977</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Qing Dou</author>
          <author>Kevin Knight</author>
        </authors>
        <title>Large scale decipherment for out-of-domain machine translation.</title>
        <publication>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</publication>
        <pages>266--275</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Pascale Fung</author>
          <author>Kathleen McKeown</author>
        </authors>
        <title>Finding terminology translations from non-parallel corpora.</title>
        <publication>In Proceedings of the 5th Annual Workshop on Very Large Corpora,</publication>
        <pages>192--202</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Sharon Goldwater</author>
          <author>Tom Griffiths</author>
        </authors>
        <title>A fully bayesian approach to unsupervised part-of-speech tagging.</title>
        <publication>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</publication>
        <pages>744--751</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Aria Haghighi</author>
          <author>Percy Liang</author>
          <author>Taylor Berg-Kirkpatrick</author>
          <author>Dan Klein</author>
        </authors>
        <title>Learning bilingual lexicons from monolingual corpora.</title>
        <publication>In Proceedings of ACL: HLT,</publication>
        <pages>771--779</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Mark Johnson</author>
        </authors>
        <title>Why doesn&#8217;t EM find good HMM POS-taggers?</title>
        <publication>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</publication>
        <pages>296--305</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Ann Irvine Klementiev</author>
          <author>Chris Callison-Burch</author>
          <author>David Yarowsky</author>
        </authors>
        <title>Toward statistical machine translation without parallel corpora.</title>
        <publication>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.</publication>
        <pages>None</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Kevin Knight</author>
          <author>Kenji Yamada</author>
        </authors>
        <title>A computational approach to deciphering unknown scripts.</title>
        <publication>In Proceedings of the ACL Workshop on Unsupervised Learning in Natural Language Processing,</publication>
        <pages>37--44</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Benjamin Snyder</author>
          <author>Regina Barzilay</author>
          <author>Kevin Knight</author>
        </authors>
        <title>A statistical model for lost language decipherment.</title>
        <publication>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>1048--1057</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>J&#246;rg Tiedemann</author>
        </authors>
        <title>News from opus - a collection of multilingual parallel corpora with tools and interfaces.</title>
        <publication>Recent Advances in Natural Language Processing,</publication>
        <pages>237--248</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Kevin Knight</author>
        </authors>
        <title>Estimating word translation probabilities from unrelated monolingual corpora using the em algorithm.</title>
        <publication>In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence,</publication>
        <pages>711--715</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Hieu Hoang</author>
          <author>Alexandra Birch</author>
          <author>Chris Callison-Burch</author>
          <author>Marcello Federico</author>
          <author>Nicola Bertoldi</author>
          <author>Brooke Cowan</author>
          <author>Wade Shen</author>
          <author>Christine Moran</author>
          <author>Richard Zens</author>
        </authors>
        <title>Chris Dyer, Ond&#345;ej Bojar,</title>
        <publication>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</publication>
        <pages>177--180</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Malte Nuhn</author>
          <author>Arne Mauser</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Deciphering foreign language by combining language models and context vectors.</title>
        <publication>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>156--164</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>Bleu: a method for automatic evaluation of machine translation.</title>
        <publication>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</publication>
        <pages>311--318</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Reinhard Rapp</author>
        </authors>
        <title>Identifying word translations in non-parallel texts.</title>
        <publication>In Proceedings of the 33rd Annual Meeting on Association for Computational Linguistics,</publication>
        <pages>320--322</pages>
        <date>1995</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Sujith Ravi</author>
          <author>Kevin Knight</author>
        </authors>
        <title>Bayesian inference for zodiac and other homophonic ciphers.</title>
        <publication>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies -</publication>
        <pages>239--247</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Sujith Ravi</author>
          <author>Kevin Knight</author>
        </authors>
        <title>Deciphering foreign language.</title>
        <publication>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</publication>
        <pages>12--21</pages>
        <date>2011</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Ahmed et al., 2012</string>
        <sentence_id>36939</sentence_id>
        <char_offset>180</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>0</reference_id>
        <string>Ahmed et al., 2012</string>
        <sentence_id>37063</sentence_id>
        <char_offset>129</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>0</reference_id>
        <string>Ahmed et al., 2012</string>
        <sentence_id>37069</sentence_id>
        <char_offset>124</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>0</reference_id>
        <string>Ahmed et al., 2012</string>
        <sentence_id>37095</sentence_id>
        <char_offset>40</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>0</reference_id>
        <string>Ahmed et al. (2012)</string>
        <sentence_id>36918</sentence_id>
        <char_offset>146</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>0</reference_id>
        <string>Ahmed et al. (2012)</string>
        <sentence_id>37041</sentence_id>
        <char_offset>44</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>0</reference_id>
        <string>Ahmed et al. (2012)</string>
        <sentence_id>37086</sentence_id>
        <char_offset>173</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>1</reference_id>
        <string>Charikar, 2002</string>
        <sentence_id>37071</sentence_id>
        <char_offset>51</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>2</reference_id>
        <string>Dempster et al., 1977</string>
        <sentence_id>36954</sentence_id>
        <char_offset>169</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>3</reference_id>
        <string>Dou and Knight, 2012</string>
        <sentence_id>36952</sentence_id>
        <char_offset>164</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>3</reference_id>
        <string>Dou and Knight, 2012</string>
        <sentence_id>37163</sentence_id>
        <char_offset>24</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>3</reference_id>
        <string>Dou and Knight, 2012</string>
        <sentence_id>37169</sentence_id>
        <char_offset>87</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>4</reference_id>
        <string>Fung and McKeown, 1997</string>
        <sentence_id>36929</sentence_id>
        <char_offset>130</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>5</reference_id>
        <string>Goldwater and Griffiths, 2007</string>
        <sentence_id>37166</sentence_id>
        <char_offset>179</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>6</reference_id>
        <string>Haghighi et al., 2008</string>
        <sentence_id>36929</sentence_id>
        <char_offset>178</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>6</reference_id>
        <string>Haghighi et al., 2008</string>
        <sentence_id>36931</sentence_id>
        <char_offset>14</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>6</reference_id>
        <string>Haghighi et al., 2008</string>
        <sentence_id>37004</sentence_id>
        <char_offset>73</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>7</reference_id>
        <string>Johnson, 2007</string>
        <sentence_id>37166</sentence_id>
        <char_offset>164</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>8</reference_id>
        <string>Klementiev et al., 2012</string>
        <sentence_id>37004</sentence_id>
        <char_offset>107</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>8</reference_id>
        <string>Klementiev et al., 2012</string>
        <sentence_id>37163</sentence_id>
        <char_offset>46</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>9</reference_id>
        <string>Knight and Yamada, 1999</string>
        <sentence_id>36932</sentence_id>
        <char_offset>104</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>10</reference_id>
        <string>Snyder et al., 2010</string>
        <sentence_id>36932</sentence_id>
        <char_offset>129</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>11</reference_id>
        <string>Tiedemann, 2009</string>
        <sentence_id>37157</sentence_id>
        <char_offset>88</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>11</reference_id>
        <string>Tiedemann, 2009</string>
        <sentence_id>37159</sentence_id>
        <char_offset>111</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>11</reference_id>
        <string>Tiedemann, 2009</string>
        <sentence_id>37119</sentence_id>
        <char_offset>28</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>11</reference_id>
        <string>Tiedemann, 2009</string>
        <sentence_id>37123</sentence_id>
        <char_offset>13</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>12</reference_id>
        <string>Koehn and Knight, 2000</string>
        <sentence_id>36929</sentence_id>
        <char_offset>154</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>12</reference_id>
        <string>Koehn and Knight, 2000</string>
        <sentence_id>37013</sentence_id>
        <char_offset>187</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>13</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>37144</sentence_id>
        <char_offset>88</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>13</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>37164</sentence_id>
        <char_offset>60</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>14</reference_id>
        <string>Nuhn et al., 2012</string>
        <sentence_id>36956</sentence_id>
        <char_offset>136</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>14</reference_id>
        <string>Nuhn et al., 2012</string>
        <sentence_id>36968</sentence_id>
        <char_offset>45</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>14</reference_id>
        <string>Nuhn et al., 2012</string>
        <sentence_id>37013</sentence_id>
        <char_offset>211</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>14</reference_id>
        <string>Nuhn et al., 2012</string>
        <sentence_id>37120</sentence_id>
        <char_offset>100</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>14</reference_id>
        <string>Nuhn et al. (2012)</string>
        <sentence_id>36936</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>14</reference_id>
        <string>Nuhn et al. (2012)</string>
        <sentence_id>37025</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>14</reference_id>
        <string>Nuhn et al. (2012)</string>
        <sentence_id>37157</sentence_id>
        <char_offset>151</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>14</reference_id>
        <string>Nuhn et al. (2012)</string>
        <sentence_id>37131</sentence_id>
        <char_offset>37</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>14</reference_id>
        <string>Nuhn et al. (2012)</string>
        <sentence_id>37141</sentence_id>
        <char_offset>31</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>14</reference_id>
        <string>Nuhn et al. (2012)</string>
        <sentence_id>37165</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>15</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>37156</sentence_id>
        <char_offset>52</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>16</reference_id>
        <string>Rapp, 1995</string>
        <sentence_id>36929</sentence_id>
        <char_offset>118</char_offset>
      </citation>
      <citation>
        <id>42</id>
        <reference_id>16</reference_id>
        <string>Rapp, 1995</string>
        <sentence_id>37013</sentence_id>
        <char_offset>175</char_offset>
      </citation>
      <citation>
        <id>43</id>
        <reference_id>17</reference_id>
        <string>Ravi and Knight (2011</string>
        <sentence_id>36933</sentence_id>
        <char_offset>70</char_offset>
      </citation>
      <citation>
        <id>44</id>
        <reference_id>17</reference_id>
        <string>Ravi and Knight (2011</string>
        <sentence_id>36944</sentence_id>
        <char_offset>154</char_offset>
      </citation>
      <citation>
        <id>45</id>
        <reference_id>17</reference_id>
        <string>Ravi and Knight (2011</string>
        <sentence_id>36956</sentence_id>
        <char_offset>92</char_offset>
      </citation>
      <citation>
        <id>46</id>
        <reference_id>17</reference_id>
        <string>Ravi and Knight (2011</string>
        <sentence_id>37157</sentence_id>
        <char_offset>123</char_offset>
      </citation>
      <citation>
        <id>47</id>
        <reference_id>17</reference_id>
        <string>Ravi and Knight (2011</string>
        <sentence_id>37130</sentence_id>
        <char_offset>75</char_offset>
      </citation>
      <citation>
        <id>48</id>
        <reference_id>17</reference_id>
        <string>Ravi and Knight, 2011</string>
        <sentence_id>36932</sentence_id>
        <char_offset>150</char_offset>
      </citation>
      <citation>
        <id>49</id>
        <reference_id>17</reference_id>
        <string>Ravi and Knight, 2011</string>
        <sentence_id>36952</sentence_id>
        <char_offset>140</char_offset>
      </citation>
      <citation>
        <id>50</id>
        <reference_id>17</reference_id>
        <string>Ravi and Knight, 2011</string>
        <sentence_id>36968</sentence_id>
        <char_offset>21</char_offset>
      </citation>
      <citation>
        <id>51</id>
        <reference_id>17</reference_id>
        <string>Ravi and Knight, 2011</string>
        <sentence_id>37120</sentence_id>
        <char_offset>76</char_offset>
      </citation>
      <citation>
        <id>52</id>
        <reference_id>17</reference_id>
        <string>Ravi and Knight, 2011</string>
        <sentence_id>37136</sentence_id>
        <char_offset>137</char_offset>
      </citation>
      <citation>
        <id>53</id>
        <reference_id>17</reference_id>
        <string>Ravi and Knight, 2011</string>
        <sentence_id>37139</sentence_id>
        <char_offset>141</char_offset>
      </citation>
      <citation>
        <id>54</id>
        <reference_id>18</reference_id>
        <string>Ravi and Knight (2011</string>
        <sentence_id>36933</sentence_id>
        <char_offset>70</char_offset>
      </citation>
      <citation>
        <id>55</id>
        <reference_id>18</reference_id>
        <string>Ravi and Knight (2011</string>
        <sentence_id>36944</sentence_id>
        <char_offset>154</char_offset>
      </citation>
      <citation>
        <id>56</id>
        <reference_id>18</reference_id>
        <string>Ravi and Knight (2011</string>
        <sentence_id>36956</sentence_id>
        <char_offset>92</char_offset>
      </citation>
      <citation>
        <id>57</id>
        <reference_id>18</reference_id>
        <string>Ravi and Knight (2011</string>
        <sentence_id>37157</sentence_id>
        <char_offset>123</char_offset>
      </citation>
      <citation>
        <id>58</id>
        <reference_id>18</reference_id>
        <string>Ravi and Knight (2011</string>
        <sentence_id>37130</sentence_id>
        <char_offset>75</char_offset>
      </citation>
      <citation>
        <id>59</id>
        <reference_id>18</reference_id>
        <string>Ravi and Knight, 2011</string>
        <sentence_id>36932</sentence_id>
        <char_offset>150</char_offset>
      </citation>
      <citation>
        <id>60</id>
        <reference_id>18</reference_id>
        <string>Ravi and Knight, 2011</string>
        <sentence_id>36952</sentence_id>
        <char_offset>140</char_offset>
      </citation>
      <citation>
        <id>61</id>
        <reference_id>18</reference_id>
        <string>Ravi and Knight, 2011</string>
        <sentence_id>36968</sentence_id>
        <char_offset>21</char_offset>
      </citation>
      <citation>
        <id>62</id>
        <reference_id>18</reference_id>
        <string>Ravi and Knight, 2011</string>
        <sentence_id>37120</sentence_id>
        <char_offset>76</char_offset>
      </citation>
      <citation>
        <id>63</id>
        <reference_id>18</reference_id>
        <string>Ravi and Knight, 2011</string>
        <sentence_id>37136</sentence_id>
        <char_offset>137</char_offset>
      </citation>
      <citation>
        <id>64</id>
        <reference_id>18</reference_id>
        <string>Ravi and Knight, 2011</string>
        <sentence_id>37139</sentence_id>
        <char_offset>141</char_offset>
      </citation>
    </citations>
  </content>
</document>
