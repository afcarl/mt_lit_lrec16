<document>
  <filename>W12-3144</filename>
  <authors>
    <author>Jan Niehues</author>
    <author>Yuqi Zhang</author>
    <author>Mohammed Mediani</author>
    <author>Teresa Herrmann</author>
    <author>Eunah Cho</author>
  </authors>
  <title>The Karlsruhe Institute of Technology Translation Systems for the WMT 2012</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>This paper describes the phrase-based SMT systems developed for our participation in the WMT12 Shared Translation Task. Translations for English&#8596;German and English&#8596;French were generated using a phrase-based translation system which is extended by additional models such as bilingual, fine-grained part-of-speech (POS) and automatic cluster language models and discriminative word lexica. In addition, we explicitly handle out-of-vocabulary (OOV) words in German, if we have translations for other morphological forms of the same stem. Furthermore, we extended the POS-based reordering approach to also use information from syntactic trees.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This paper describes the phrase-based SMT systems developed for our participation in the WMT12 Shared Translation Task.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Translations for English&#8596;German and English&#8596;French were generated using a phrase-based translation system which is extended by additional models such as bilingual, fine-grained part-of-speech (POS) and automatic cluster language models and discriminative word lexica.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In addition, we explicitly handle out-of-vocabulary (OOV) words in German, if we have translations for other morphological forms of the same stem.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Furthermore, we extended the POS-based reordering approach to also use information from syntactic trees.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>In this paper, we describe our systems for the NAACL 2012 Seventh Workshop on Statistical Machine Translation. We participated in the Shared Translation Task and submitted translations for English&#8596;German and English&#8596;French. We use a phrase-based decoder that can use lattices as input and developed several models that extend the standard log-linear model combination of phrase-based MT. In addition to the POS-based reordering model used in past years, for German-English we extended it to also use rules learned using syntax trees.
The translation model was extended by the bilingual language model and a discriminative word lexicon using a maximum entropy classifier. For the French-English and English-French translation systems, we also used phrase table adaptation to avoid overestimation of the probabilities of the huge, but noisy Giga corpus. In the German-English system, we tried to learn translations for OOV words by exploring different morphological forms of the OOVs with the same lemma.
Furthermore, we combined different language models in the log-linear model. We used wordbased language models trained on different parts of the training corpus as well as POS-based language models using fine-grained POS information and language models trained on automatic word clusters.
The paper is organized as follows: The next section gives a detailed description of our systems including all the models. The translation results for all directions are presented afterwards and we close with a conclusion.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper, we describe our systems for the NAACL 2012 Seventh Workshop on Statistical Machine Translation.</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We participated in the Shared Translation Task and submitted translations for English&#8596;German and English&#8596;French.</text>
              <doc_id>5</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We use a phrase-based decoder that can use lattices as input and developed several models that extend the standard log-linear model combination of phrase-based MT.</text>
              <doc_id>6</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In addition to the POS-based reordering model used in past years, for German-English we extended it to also use rules learned using syntax trees.</text>
              <doc_id>7</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The translation model was extended by the bilingual language model and a discriminative word lexicon using a maximum entropy classifier.</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For the French-English and English-French translation systems, we also used phrase table adaptation to avoid overestimation of the probabilities of the huge, but noisy Giga corpus.</text>
              <doc_id>9</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In the German-English system, we tried to learn translations for OOV words by exploring different morphological forms of the OOVs with the same lemma.</text>
              <doc_id>10</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Furthermore, we combined different language models in the log-linear model.</text>
              <doc_id>11</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We used wordbased language models trained on different parts of the training corpus as well as POS-based language models using fine-grained POS information and language models trained on automatic word clusters.</text>
              <doc_id>12</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The paper is organized as follows: The next section gives a detailed description of our systems including all the models.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The translation results for all directions are presented afterwards and we close with a conclusion.</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 System Description</title>
        <text>For the French&#8596;English systems the phrase table is based on a GIZA++ word alignment, while the systems for German&#8596;English use a discriminative word alignment as described in Niehues and Vogel (2008). The language models are 4-gram SRI language models using Kneser-Ney smoothing trained by the SRILM Toolkit (Stolcke, 2002).
The problem of word reordering is addressed with POS-based and tree-based reordering models as described in Section 2.3. The POS tags used in the reordering model are obtained using the TreeTagger (Schmid, 1994). The syntactic parse trees are generated using the Stanford Parser (Rafferty and Manning, 2008).
An in-house phrase-based decoder (Vogel, 2003) is used to perform translation. Optimization with
regard to the BLEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). During decoding only the top 10 translation options for every source phrase are considered.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>For the French&#8596;English systems the phrase table is based on a GIZA++ word alignment, while the systems for German&#8596;English use a discriminative word alignment as described in Niehues and Vogel (2008).</text>
              <doc_id>15</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The language models are 4-gram SRI language models using Kneser-Ney smoothing trained by the SRILM Toolkit (Stolcke, 2002).</text>
              <doc_id>16</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The problem of word reordering is addressed with POS-based and tree-based reordering models as described in Section 2.3.</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The POS tags used in the reordering model are obtained using the TreeTagger (Schmid, 1994).</text>
              <doc_id>18</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The syntactic parse trees are generated using the Stanford Parser (Rafferty and Manning, 2008).</text>
              <doc_id>19</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>An in-house phrase-based decoder (Vogel, 2003) is used to perform translation.</text>
              <doc_id>20</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Optimization with</text>
              <doc_id>21</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>regard to the BLEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005).</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>During decoding only the top 10 translation options for every source phrase are considered.</text>
              <doc_id>23</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Data</title>
            <text>Our translation models were trained on the EPPS and News Commentary (NC) corpora. Furthermore, the additional available data for French and English (i.e. UN and Giga corpora) were exploited in the corresponding systems. The systems were tuned with the news-test2011 data, while news-test2011 was used for testing in all our systems. We trained language models for each language on the monolingual part of the training corpora as well as the News Shuffle and the Gigaword (version 4) corpora. The discriminative word alignment model was trained on 500 hand-aligned sentences selected from the EPPS corpus.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Our translation models were trained on the EPPS and News Commentary (NC) corpora.</text>
                  <doc_id>24</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Furthermore, the additional available data for French and English (i.e. UN and Giga corpora) were exploited in the corresponding systems.</text>
                  <doc_id>25</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The systems were tuned with the news-test2011 data, while news-test2011 was used for testing in all our systems.</text>
                  <doc_id>26</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We trained language models for each language on the monolingual part of the training corpora as well as the News Shuffle and the Gigaword (version 4) corpora.</text>
                  <doc_id>27</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The discriminative word alignment model was trained on 500 hand-aligned sentences selected from the EPPS corpus.</text>
                  <doc_id>28</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Preprocessing</title>
            <text>The training data is preprocessed prior to training the system. This includes normalizing special symbols, smart-casing the first word of each sentence and removing long sentences and sentences with length mismatch.
For the German parts of the training corpus, in order to obtain a homogenous spelling, we use the hunspell 1 lexicon to map words written according to old German spelling rules to new German spelling rules.
In order to reduce the OOV problem of German compound words, Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system.
The Giga corpus received a special preprocessing by removing noisy pairs using an SVM classifier as described in Mediani et al. (2011). The SVM classifier training and test sets consist of randomly selected sentence pairs from the corpora of EPPS, NC, tuning, and test sets. Giving at the end around 16 million sentence pairs.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The training data is preprocessed prior to training the system.</text>
                  <doc_id>29</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This includes normalizing special symbols, smart-casing the first word of each sentence and removing long sentences and sentences with length mismatch.</text>
                  <doc_id>30</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For the German parts of the training corpus, in order to obtain a homogenous spelling, we use the hunspell 1 lexicon to map words written according to old German spelling rules to new German spelling rules.</text>
                  <doc_id>31</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In order to reduce the OOV problem of German compound words, Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system.</text>
                  <doc_id>32</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The Giga corpus received a special preprocessing by removing noisy pairs using an SVM classifier as described in Mediani et al. (2011).</text>
                  <doc_id>33</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The SVM classifier training and test sets consist of randomly selected sentence pairs from the corpora of EPPS, NC, tuning, and test sets.</text>
                  <doc_id>34</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Giving at the end around 16 million sentence pairs.</text>
                  <doc_id>35</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>2.3 Word Reordering</title>
            <text>In contrast to modeling the reordering by a distancebased reordering model and/or a lexicalized distor-
1 http://hunspell.sourceforge.net/
tion model, we use a different approach that relies on POS sequences. By abstracting from surface words to POS, we expect to model the reordering more accurately. For German-to-English, we additionally apply reordering rules learned from syntactic parse trees.
2.3.1 POS-based Reordering Model
In order to build the POS-based reordering model, we first learn probabilistic rules from the POS tags of the training corpus and the alignment. Continuous reordering rules are extracted as described in Rottmann and Vogel (2007) to model short-range reorderings. When translating between German and English, we apply a modified reordering model with non-continuous rules to cover also long-range reorderings (Niehues and Kolss, 2009).
2.3.2 Tree-based Reordering Model
Word order is quite different between German and English. And during translation especially verbs or verb particles need to be shifted over a long distance in a sentence. Using discontinuous POS rules already improves the translation tremendously. In addition, we apply a tree-based reordering model for the German-English translation. Syntactic parse trees provide information about the words in a sentence that form constituents and should therefore be treated as inseparable units by the reordering model. For the tree-based reordering model, syntactic parse trees are generated for the whole training corpus. Then the word alignment between the source and target language part of the corpus is used to learn rules on how to reorder the constituents in a German source sentence to make it matches the English target sentence word order better. In order to apply the rules to the source text, POS tags and a parse tree are generated for each sentence. Then the POSbased and tree-based reordering rules are applied. The original order of words as well as the reordered sentence variants generated by the rules are encoded in a word lattice. The lattice is then used as input to the decoder. For the test sentences, the reordering based on POS and trees allows us to change the word order in the source sentence so that the sentence can be translated more easily. In addition, we build reordering lattices for all training sentences and then extract
phrase pairs from the monotone source path as well as from the reordered paths.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In contrast to modeling the reordering by a distancebased reordering model and/or a lexicalized distor-</text>
                  <doc_id>36</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 http://hunspell.sourceforge.net/</text>
                  <doc_id>37</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>tion model, we use a different approach that relies on POS sequences.</text>
                  <doc_id>38</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>By abstracting from surface words to POS, we expect to model the reordering more accurately.</text>
                  <doc_id>39</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For German-to-English, we additionally apply reordering rules learned from syntactic parse trees.</text>
                  <doc_id>40</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.3.1 POS-based Reordering Model</text>
                  <doc_id>41</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In order to build the POS-based reordering model, we first learn probabilistic rules from the POS tags of the training corpus and the alignment.</text>
                  <doc_id>42</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Continuous reordering rules are extracted as described in Rottmann and Vogel (2007) to model short-range reorderings.</text>
                  <doc_id>43</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>When translating between German and English, we apply a modified reordering model with non-continuous rules to cover also long-range reorderings (Niehues and Kolss, 2009).</text>
                  <doc_id>44</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.3.2 Tree-based Reordering Model</text>
                  <doc_id>45</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Word order is quite different between German and English.</text>
                  <doc_id>46</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>And during translation especially verbs or verb particles need to be shifted over a long distance in a sentence.</text>
                  <doc_id>47</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Using discontinuous POS rules already improves the translation tremendously.</text>
                  <doc_id>48</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, we apply a tree-based reordering model for the German-English translation.</text>
                  <doc_id>49</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Syntactic parse trees provide information about the words in a sentence that form constituents and should therefore be treated as inseparable units by the reordering model.</text>
                  <doc_id>50</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For the tree-based reordering model, syntactic parse trees are generated for the whole training corpus.</text>
                  <doc_id>51</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Then the word alignment between the source and target language part of the corpus is used to learn rules on how to reorder the constituents in a German source sentence to make it matches the English target sentence word order better.</text>
                  <doc_id>52</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>In order to apply the rules to the source text, POS tags and a parse tree are generated for each sentence.</text>
                  <doc_id>53</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>Then the POSbased and tree-based reordering rules are applied.</text>
                  <doc_id>54</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>The original order of words as well as the reordered sentence variants generated by the rules are encoded in a word lattice.</text>
                  <doc_id>55</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>The lattice is then used as input to the decoder.</text>
                  <doc_id>56</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>For the test sentences, the reordering based on POS and trees allows us to change the word order in the source sentence so that the sentence can be translated more easily.</text>
                  <doc_id>57</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, we build reordering lattices for all training sentences and then extract</text>
                  <doc_id>58</doc_id>
                  <sec_id>12</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>phrase pairs from the monotone source path as well as from the reordered paths.</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>2.4 Translation Models</title>
            <text>In addition to the models used in the baseline system described above, we conducted experiments including additional models that enhance translation quality by introducing alternative or additional information into the translation modeling process.
2.4.1 Phrase table adaptation
Since the Giga corpus is huge, but noisy, it is advantageous to also use the translation probabilities of the phrase pair extracted only from the more reliable EPPS and News commentary corpus. Therefore, we build two phrase tables for the French&#8596;English system. One trained on all data and the other only trained on the EPPS and News commentary corpus. The two models are then combined using a log-linear combination to achieve the adaptation towards the cleaner corpora as described in (Niehues et al., 2010). The newly created translation model uses the four scores from the general model as well as the two smoothed relative frequencies of both directions from the smaller, but cleaner model. If a phrase pair does not occur in the indomain part, a default score is used instead of a relative frequency. In our case, we used the lowest probability.
2.4.2 Bilingual Language Model
In phrase-based systems the source sentence is segmented by the decoder according to the best combination of phrases that maximize the translation and language model scores. This segmentation into phrases leads to the loss of context information at the phrase boundaries. Although more target side context is available to the language model, source side context would also be valuable for the decoder when searching for the best translation hypothesis. To make also source language context available we use a bilingual language model, in which each token consists of a target word and all source words it is aligned to. The bilingual tokens enter the translation process as an additional target factor and the bilingual language model is applied to the additional factor like a normal language model. For more details see Niehues et al. (2011).
2.4.3 Discriminative Word Lexica
Mauser et al. (2009) have shown that the use of discriminative word lexica (DWL) can improve the translation quality. For every target word, they trained a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per one source word.
When applying DWL in our experiments, we would like to have the same conditions for the training and test case. For this we would need to change the score of the feature only if a new word is added to the hypothesis. If a word is added the second time, we do not want to change the feature value. In order to keep track of this, additional bookkeeping would be required. Also the other models in our translation system will prevent us from using a word too often.
Therefore, we ignore this problem and can calculate the score for every phrase pair before starting with the translation. This leads to the following definition of the model: J&#8719; p(e|f) = p(e j |f) (1)
j=1
In this definition, p(e j |f) is calculated using a maximum likelihood classifier. Each classifier is trained independently on the parallel training data. All sentences pairs where the target word e occurs in the target sentence are used as positive examples. We could now use all other sentences as negative examples. But in many of these sentences, we would anyway not generate the target word, since there is no phrase pair that translates any of the source words into the target word.
Therefore, we build a target vocabulary for every training sentence. This vocabulary consists of all target side words of phrase pairs matching a source phrase in the source part of the training sentence. Then we use all sentence pairs where e is in the target vocabulary but not in the target sentences as negative examples. This has shown to have a postive influence on the translation quality (Mediani et al., 2011) and also reduces training time.
2.4.4 Quasi-Morphological Operations for OOV words
Since German is a highly inflected language, there will be always some word forms of a given Ger-
man lemma that did not occur in the training data. In order to be able to also translate unseen word forms, we try to learn quasi-morphological operations that change the lexical entry of a known word form to the unknown word form. These have shown to be beneficial in Niehues and Waibel (2011) using Wikipedia 2 titles. The idea is illustrated in Figure 1.
If we look at the data, our system is able to translate a German word Kamin (engl. chimney), but not the dative plural form Kaminen. To address this problem, we try to automatically learn rules how words can be modified. If we look at the example, we would like the system to learn the following rule. If an &#8220;en&#8221; is appended to a German word, as it is done when creating the dative plural form of Kaminen, we need to add an &#8220;s&#8221; to the end of the English word in order to perform the same morphological word transformation. We use only rules where the ending of the word has at most 3 letters.
Depending on the POS, number, gender or case of the involved words, the same operation on the source side does not necessarily correspond to the same operation on the target side.
To account for this ambiguity, we rank the different target operation using the following four features and use the best ranked one. Firstly, we should not generate target words that do not exist. Here, we have an advantage that we can use monolingual data to determine whether the word exists. In addition, a target operation that often coincides with a given source operation should be better than one that is rarely used together with the source operation. We therefore look at pairs of entries in the lexicon and count in how many of them the source operation can be applied to the source side and the target operation can be applied to the target side. We then use only operations that occur at least ten times. Furthermore,
2 http://www.wikipedia.org/
we use the ending of the source and target word to determine which pair of operations should be used.
Integration We only use the proposed method for OOVs and do not try to improve translations of words that the baseline system already covers. We look for phrase pairs, for which a source operation op s exists that changes one of the source words f 1 into the OOV word f 2 . Since we need to apply a target operation to one word on the target side of the phrase pair, we only consider phrase pairs where f 1 is aligned to one of the target words of the phrase containing e 1 . If a target operation exists given f 1 and op s , we select the one with the highest rank. Then we generate a new phrase pair by applying op s to f 1 and op t to e 1 keeping the original scores from the phrase pairs, since the original and synthesized phrase pair are not directly competing anyway. We do not add several phrase pairs generated by different operations, since we would then need to add the features used for ranking the operations into the MERT. This is problematic, since the operations were only used for very few words and therefore a good estimation of the weights is not possible.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In addition to the models used in the baseline system described above, we conducted experiments including additional models that enhance translation quality by introducing alternative or additional information into the translation modeling process.</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.4.1 Phrase table adaptation</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Since the Giga corpus is huge, but noisy, it is advantageous to also use the translation probabilities of the phrase pair extracted only from the more reliable EPPS and News commentary corpus.</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, we build two phrase tables for the French&#8596;English system.</text>
                  <doc_id>63</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>One trained on all data and the other only trained on the EPPS and News commentary corpus.</text>
                  <doc_id>64</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The two models are then combined using a log-linear combination to achieve the adaptation towards the cleaner corpora as described in (Niehues et al., 2010).</text>
                  <doc_id>65</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The newly created translation model uses the four scores from the general model as well as the two smoothed relative frequencies of both directions from the smaller, but cleaner model.</text>
                  <doc_id>66</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>If a phrase pair does not occur in the indomain part, a default score is used instead of a relative frequency.</text>
                  <doc_id>67</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>In our case, we used the lowest probability.</text>
                  <doc_id>68</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.4.2 Bilingual Language Model</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In phrase-based systems the source sentence is segmented by the decoder according to the best combination of phrases that maximize the translation and language model scores.</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This segmentation into phrases leads to the loss of context information at the phrase boundaries.</text>
                  <doc_id>71</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Although more target side context is available to the language model, source side context would also be valuable for the decoder when searching for the best translation hypothesis.</text>
                  <doc_id>72</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>To make also source language context available we use a bilingual language model, in which each token consists of a target word and all source words it is aligned to.</text>
                  <doc_id>73</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The bilingual tokens enter the translation process as an additional target factor and the bilingual language model is applied to the additional factor like a normal language model.</text>
                  <doc_id>74</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For more details see Niehues et al. (2011).</text>
                  <doc_id>75</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.4.3 Discriminative Word Lexica</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Mauser et al. (2009) have shown that the use of discriminative word lexica (DWL) can improve the translation quality.</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For every target word, they trained a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per one source word.</text>
                  <doc_id>78</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>When applying DWL in our experiments, we would like to have the same conditions for the training and test case.</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For this we would need to change the score of the feature only if a new word is added to the hypothesis.</text>
                  <doc_id>80</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>If a word is added the second time, we do not want to change the feature value.</text>
                  <doc_id>81</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In order to keep track of this, additional bookkeeping would be required.</text>
                  <doc_id>82</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Also the other models in our translation system will prevent us from using a word too often.</text>
                  <doc_id>83</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Therefore, we ignore this problem and can calculate the score for every phrase pair before starting with the translation.</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This leads to the following definition of the model: J&#8719; p(e|f) = p(e j |f) (1)</text>
                  <doc_id>85</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=1</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In this definition, p(e j |f) is calculated using a maximum likelihood classifier.</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Each classifier is trained independently on the parallel training data.</text>
                  <doc_id>88</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>All sentences pairs where the target word e occurs in the target sentence are used as positive examples.</text>
                  <doc_id>89</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We could now use all other sentences as negative examples.</text>
                  <doc_id>90</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>But in many of these sentences, we would anyway not generate the target word, since there is no phrase pair that translates any of the source words into the target word.</text>
                  <doc_id>91</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Therefore, we build a target vocabulary for every training sentence.</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This vocabulary consists of all target side words of phrase pairs matching a source phrase in the source part of the training sentence.</text>
                  <doc_id>93</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Then we use all sentence pairs where e is in the target vocabulary but not in the target sentences as negative examples.</text>
                  <doc_id>94</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This has shown to have a postive influence on the translation quality (Mediani et al., 2011) and also reduces training time.</text>
                  <doc_id>95</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.4.4 Quasi-Morphological Operations for OOV words</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Since German is a highly inflected language, there will be always some word forms of a given Ger-</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>man lemma that did not occur in the training data.</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In order to be able to also translate unseen word forms, we try to learn quasi-morphological operations that change the lexical entry of a known word form to the unknown word form.</text>
                  <doc_id>99</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>These have shown to be beneficial in Niehues and Waibel (2011) using Wikipedia 2 titles.</text>
                  <doc_id>100</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The idea is illustrated in Figure 1.</text>
                  <doc_id>101</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>If we look at the data, our system is able to translate a German word Kamin (engl.</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>chimney), but not the dative plural form Kaminen.</text>
                  <doc_id>103</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>To address this problem, we try to automatically learn rules how words can be modified.</text>
                  <doc_id>104</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>If we look at the example, we would like the system to learn the following rule.</text>
                  <doc_id>105</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>If an &#8220;en&#8221; is appended to a German word, as it is done when creating the dative plural form of Kaminen, we need to add an &#8220;s&#8221; to the end of the English word in order to perform the same morphological word transformation.</text>
                  <doc_id>106</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We use only rules where the ending of the word has at most 3 letters.</text>
                  <doc_id>107</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Depending on the POS, number, gender or case of the involved words, the same operation on the source side does not necessarily correspond to the same operation on the target side.</text>
                  <doc_id>108</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To account for this ambiguity, we rank the different target operation using the following four features and use the best ranked one.</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Firstly, we should not generate target words that do not exist.</text>
                  <doc_id>110</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Here, we have an advantage that we can use monolingual data to determine whether the word exists.</text>
                  <doc_id>111</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, a target operation that often coincides with a given source operation should be better than one that is rarely used together with the source operation.</text>
                  <doc_id>112</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We therefore look at pairs of entries in the lexicon and count in how many of them the source operation can be applied to the source side and the target operation can be applied to the target side.</text>
                  <doc_id>113</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We then use only operations that occur at least ten times.</text>
                  <doc_id>114</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Furthermore,</text>
                  <doc_id>115</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 http://www.wikipedia.org/</text>
                  <doc_id>116</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>we use the ending of the source and target word to determine which pair of operations should be used.</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Integration We only use the proposed method for OOVs and do not try to improve translations of words that the baseline system already covers.</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We look for phrase pairs, for which a source operation op s exists that changes one of the source words f 1 into the OOV word f 2 .</text>
                  <doc_id>119</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Since we need to apply a target operation to one word on the target side of the phrase pair, we only consider phrase pairs where f 1 is aligned to one of the target words of the phrase containing e 1 .</text>
                  <doc_id>120</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>If a target operation exists given f 1 and op s , we select the one with the highest rank.</text>
                  <doc_id>121</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Then we generate a new phrase pair by applying op s to f 1 and op t to e 1 keeping the original scores from the phrase pairs, since the original and synthesized phrase pair are not directly competing anyway.</text>
                  <doc_id>122</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We do not add several phrase pairs generated by different operations, since we would then need to add the features used for ranking the operations into the MERT.</text>
                  <doc_id>123</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>This is problematic, since the operations were only used for very few words and therefore a good estimation of the weights is not possible.</text>
                  <doc_id>124</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>4</index>
            <title>2.5 Language Models</title>
            <text>The 4-gram language models generated by the SRILM toolkit are used as the main language models for all of our systems. For English-French and French-English systems, we use a good quality corpus as in-domain data to train in-domain language models. Additionally, we apply the POS and cluster language models in different systems. All language models are integrated into the translation system by a log-linear combination and received optimal weights during tuning by the MERT.
2.5.1 POS Language Models
The POS language model is trained on the POS sequences of the target language. In this evaluation, the POS language model is applied for the English-German system. We expect that having additional information in form of probabilities of POS sequences should help especially in case of the rich morphology of German. The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information. We
use a 9-gram language model on the News Shuffle corpus and the German side of all parallel corpora. More details and discussions about the POS language model can be found in Herrmann et al. (2011).
2.5.2 Cluster Language Models
The cluster language model follows a similar idea as the POS language model. Since there is a data sparsity problem when we substitute words with the word classes, it is possible to make use of larger context information. In the POS language model, POS tags are the word classes. Here, we generated word classes in a different way. First, we cluster the words in the corpus using the MKCLS algorithm (Och, 1999) given a number of classes. Second, we replace the words in the corpus by their cluster IDs. Finally, we train an n-gram language model on this corpus consisting of cluster IDs. Generally, all cluster language models used in our systems are 5-gram.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The 4-gram language models generated by the SRILM toolkit are used as the main language models for all of our systems.</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For English-French and French-English systems, we use a good quality corpus as in-domain data to train in-domain language models.</text>
                  <doc_id>126</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Additionally, we apply the POS and cluster language models in different systems.</text>
                  <doc_id>127</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>All language models are integrated into the translation system by a log-linear combination and received optimal weights during tuning by the MERT.</text>
                  <doc_id>128</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.5.1 POS Language Models</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The POS language model is trained on the POS sequences of the target language.</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this evaluation, the POS language model is applied for the English-German system.</text>
                  <doc_id>131</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We expect that having additional information in form of probabilities of POS sequences should help especially in case of the rich morphology of German.</text>
                  <doc_id>132</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.</text>
                  <doc_id>133</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We</text>
                  <doc_id>134</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>use a 9-gram language model on the News Shuffle corpus and the German side of all parallel corpora.</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>More details and discussions about the POS language model can be found in Herrmann et al. (2011).</text>
                  <doc_id>136</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.5.2 Cluster Language Models</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The cluster language model follows a similar idea as the POS language model.</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Since there is a data sparsity problem when we substitute words with the word classes, it is possible to make use of larger context information.</text>
                  <doc_id>139</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In the POS language model, POS tags are the word classes.</text>
                  <doc_id>140</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Here, we generated word classes in a different way.</text>
                  <doc_id>141</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>First, we cluster the words in the corpus using the MKCLS algorithm (Och, 1999) given a number of classes.</text>
                  <doc_id>142</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Second, we replace the words in the corpus by their cluster IDs.</text>
                  <doc_id>143</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, we train an n-gram language model on this corpus consisting of cluster IDs.</text>
                  <doc_id>144</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Generally, all cluster language models used in our systems are 5-gram.</text>
                  <doc_id>145</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Results</title>
        <text>Using the models described above we performed several experiments leading finally to the systems used for generating the translations submitted to the workshop. The following sections describe the experiments for the individual language pairs and show the translation results. The results are reported as case-sensitive BLEU scores (Papineni et al., 2002) on one reference translation.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Using the models described above we performed several experiments leading finally to the systems used for generating the translations submitted to the workshop.</text>
              <doc_id>146</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The following sections describe the experiments for the individual language pairs and show the translation results.</text>
              <doc_id>147</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The results are reported as case-sensitive BLEU scores (Papineni et al., 2002) on one reference translation.</text>
              <doc_id>148</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 German-English</title>
            <text>The experiments for the German-English translation system are summarized in Table 1. The Baseline system uses POS-based reordering, discriminative word alignment and a language model trained on the News Shuffle corpus. By adding lattice phrase extraction small improvements of the translation quality could be gained.
Further improvements could be gained by adding a language model trained on the Gigaword corpus and adding a bilingual and cluster-based language model. We used 50 word classes and trained a 5- gram language model. Afterwards, the translation quality was improved by also using a discriminative word lexicon. Finally, the best system was achieved by using Tree-based reordering and using special treatment for the OOVs. This system generates a BLEU score of 22.31 on the test data. For the last two systems, we did not perform new optimization runs.
System Dev Test</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The experiments for the German-English translation system are summarized in Table 1.</text>
                  <doc_id>149</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The Baseline system uses POS-based reordering, discriminative word alignment and a language model trained on the News Shuffle corpus.</text>
                  <doc_id>150</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>By adding lattice phrase extraction small improvements of the translation quality could be gained.</text>
                  <doc_id>151</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Further improvements could be gained by adding a language model trained on the Gigaword corpus and adding a bilingual and cluster-based language model.</text>
                  <doc_id>152</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We used 50 word classes and trained a 5- gram language model.</text>
                  <doc_id>153</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Afterwards, the translation quality was improved by also using a discriminative word lexicon.</text>
                  <doc_id>154</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, the best system was achieved by using Tree-based reordering and using special treatment for the OOVs.</text>
                  <doc_id>155</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This system generates a BLEU score of 22.31 on the test data.</text>
                  <doc_id>156</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For the last two systems, we did not perform new optimization runs.</text>
                  <doc_id>157</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>System Dev Test</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 English-German</title>
            <text>The English-German baseline system uses also POS-based reordering, discriminative word alignment and a language model based on EPPS, NC and News Shuffle. A small gain could be achieved by the POS-based language model and the bilingual language model. Further gain was achieved by using also a cluster-based language model. For this language model, we use 100 word classes and trained a 5-gram language model. Finally, the best system uses the discriminative word lexicon.
System Dev Test</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The English-German baseline system uses also POS-based reordering, discriminative word alignment and a language model based on EPPS, NC and News Shuffle.</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A small gain could be achieved by the POS-based language model and the bilingual language model.</text>
                  <doc_id>160</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Further gain was achieved by using also a cluster-based language model.</text>
                  <doc_id>161</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For this language model, we use 100 word classes and trained a 5-gram language model.</text>
                  <doc_id>162</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, the best system uses the discriminative word lexicon.</text>
                  <doc_id>163</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>System Dev Test</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 English-French</title>
            <text>Table 3 summarizes how our English-French system evolved. The baseline system here was trained on the EPPS, NC, and UN corpora, while the language model was trained on all the French part of the parallel corpora (including the Giga corpus). It also uses short-range reordering trained on EPPS and NC. This system had a BLEU score of around 26.7. The Giga parallel data turned out to be quite
beneficial for this task. It improves the scores by more than 1 BLEU point. More importantly, additional language models boosted the system quality: around 1.8 points. In fact, three language models were log-linearly combined: In addition to the aforementioned, two additional language models were trained on the monolingual sets (one for News and one for Gigaword). We could get an improvement of around 0.2 by retraining the reordering rules on EPPS and NC only, but using Giza alignment from the whole data. Adapting the translation model by using EPPS and NC as in-domain data improves the BLEU score by only 0.1. This small improvement might be due to the fact that the news domain is very broad and that the Giga corpus has already been carefully cleaned and filtered. Furthermore, using a bilingual language model enhances the BLEU score by almost 0.3. Finally, incorporating a cluster language model adds an additional 0.1 to the score. This leads to a system with 30.58.
System Dev Test</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Table 3 summarizes how our English-French system evolved.</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The baseline system here was trained on the EPPS, NC, and UN corpora, while the language model was trained on all the French part of the parallel corpora (including the Giga corpus).</text>
                  <doc_id>166</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It also uses short-range reordering trained on EPPS and NC.</text>
                  <doc_id>167</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This system had a BLEU score of around 26.7.</text>
                  <doc_id>168</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The Giga parallel data turned out to be quite</text>
                  <doc_id>169</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>beneficial for this task.</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It improves the scores by more than 1 BLEU point.</text>
                  <doc_id>171</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>More importantly, additional language models boosted the system quality: around 1.8 points.</text>
                  <doc_id>172</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In fact, three language models were log-linearly combined: In addition to the aforementioned, two additional language models were trained on the monolingual sets (one for News and one for Gigaword).</text>
                  <doc_id>173</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We could get an improvement of around 0.2 by retraining the reordering rules on EPPS and NC only, but using Giza alignment from the whole data.</text>
                  <doc_id>174</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Adapting the translation model by using EPPS and NC as in-domain data improves the BLEU score by only 0.1.</text>
                  <doc_id>175</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>This small improvement might be due to the fact that the news domain is very broad and that the Giga corpus has already been carefully cleaned and filtered.</text>
                  <doc_id>176</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Furthermore, using a bilingual language model enhances the BLEU score by almost 0.3.</text>
                  <doc_id>177</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, incorporating a cluster language model adds an additional 0.1 to the score.</text>
                  <doc_id>178</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>This leads to a system with 30.58.</text>
                  <doc_id>179</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>System Dev Test</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>3.4 French-English</title>
            <text>The development of our system for the French- English direction is summarized in Table 4. The baseline system for this direction was trained on the EPPS, NC, UN and Giga parallel corpora, while the language model was trained on the French part of the parallel training corpora. The baseline system includes the POS-based reordering model with shortrange rules. The largest improvement of 1.7 BLEU score was achieved by the integration of the bigger language models which are trained on the English version of News Shuffle and the Gigaword corpus (v4). We did not add the language models from the monolingual English version of EPPS and NC data, since the experiments have shown that they did not provide improvement in our system. The second largest improvement came from the domain adaptation that includes an in-domain language model and adaptations to the phrase extraction. The BLEU score has improved about 1 BLEU in total. The indomain data we used here are parallel EPPS and NC corpus. Further gains were obtained by augmenting the system with a bilingual language model adding around 0.2 BLEU to the previous score. The submitted system was obtained by adding the cluster 5-gram language model trained on the News Shuffle corpus with 100 clusters and thus giving 30.25 as the final score.
System Dev Test</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The development of our system for the French- English direction is summarized in Table 4.</text>
                  <doc_id>181</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The baseline system for this direction was trained on the EPPS, NC, UN and Giga parallel corpora, while the language model was trained on the French part of the parallel training corpora.</text>
                  <doc_id>182</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The baseline system includes the POS-based reordering model with shortrange rules.</text>
                  <doc_id>183</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The largest improvement of 1.7 BLEU score was achieved by the integration of the bigger language models which are trained on the English version of News Shuffle and the Gigaword corpus (v4).</text>
                  <doc_id>184</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We did not add the language models from the monolingual English version of EPPS and NC data, since the experiments have shown that they did not provide improvement in our system.</text>
                  <doc_id>185</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The second largest improvement came from the domain adaptation that includes an in-domain language model and adaptations to the phrase extraction.</text>
                  <doc_id>186</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The BLEU score has improved about 1 BLEU in total.</text>
                  <doc_id>187</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>The indomain data we used here are parallel EPPS and NC corpus.</text>
                  <doc_id>188</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>Further gains were obtained by augmenting the system with a bilingual language model adding around 0.2 BLEU to the previous score.</text>
                  <doc_id>189</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>The submitted system was obtained by adding the cluster 5-gram language model trained on the News Shuffle corpus with 100 clusters and thus giving 30.25 as the final score.</text>
                  <doc_id>190</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>System Dev Test</text>
                  <doc_id>191</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Conclusions</title>
        <text>We have presented the systems for our participation in the WMT 2012 Evaluation for English&#8596;German and English&#8596;French. In all systems we could improve by using a class-based language model. Furthermore, the translation quality could be improved by using a discriminative word lexicon. Therefore, we trained a maximum entropy classifier for every target word. For English&#8596;French, adapting the phrase table helps to avoid using wrong parts of the noisy Giga corpus. For the German-to-English system, we could improve the translation quality additionally by using a tree-based reordering model and by special handling of OOV words. For the inverse direction we could improve the translation quality by using a 9-gram language model trained on the fine-grained POS tags.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have presented the systems for our participation in the WMT 2012 Evaluation for English&#8596;German and English&#8596;French.</text>
              <doc_id>192</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In all systems we could improve by using a class-based language model.</text>
              <doc_id>193</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Furthermore, the translation quality could be improved by using a discriminative word lexicon.</text>
              <doc_id>194</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, we trained a maximum entropy classifier for every target word.</text>
              <doc_id>195</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>For English&#8596;French, adapting the phrase table helps to avoid using wrong parts of the noisy Giga corpus.</text>
              <doc_id>196</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>For the German-to-English system, we could improve the translation quality additionally by using a tree-based reordering model and by special handling of OOV words.</text>
              <doc_id>197</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>For the inverse direction we could improve the translation quality by using a 9-gram language model trained on the fine-grained POS tags.</text>
              <doc_id>198</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>Acknowledgments</title>
        <text>This work was realized as part of the Quaero Programme, funded by OSEO, French State agency for innovation.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This work was realized as part of the Quaero Programme, funded by OSEO, French State agency for innovation.</text>
              <doc_id>199</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Translation results for German-English</caption>
        <reference_text>None</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>runs.</cell>
              <cell>None</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Dev</cell>
              <cell>Test</cell>
            </row>
            <row>
              <cell>Baseline</cell>
              <cell>23.64</cell>
              <cell>21.32</cell>
            </row>
            <row>
              <cell>+ Lattice Phrase Extraction</cell>
              <cell>23.76</cell>
              <cell>21.36</cell>
            </row>
            <row>
              <cell>+ Gigaward Language Model</cell>
              <cell>24.01</cell>
              <cell>21.73</cell>
            </row>
            <row>
              <cell>+ Bilingual LM</cell>
              <cell>24.19</cell>
              <cell>21.91</cell>
            </row>
            <row>
              <cell>+ Cluster LM</cell>
              <cell>24.16</cell>
              <cell>22.09</cell>
            </row>
            <row>
              <cell>+ DWL</cell>
              <cell>24.19</cell>
              <cell>22.19</cell>
            </row>
            <row>
              <cell>+ Tree-based Reordering</cell>
              <cell>-</cell>
              <cell>22.26</cell>
            </row>
            <row>
              <cell>+ OOV</cell>
              <cell>-</cell>
              <cell>22.31</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Translation results for English-German</caption>
        <reference_text>None</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Dev</cell>
              <cell>Test</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Baseline</cell>
              <cell>17.06</cell>
              <cell>15.57</cell>
            </row>
            <row>
              <cell>+ POSLM</cell>
              <cell>17.27</cell>
              <cell>15.63</cell>
            </row>
            <row>
              <cell>+ Bilingual LM</cell>
              <cell>17.40</cell>
              <cell>15.78</cell>
            </row>
            <row>
              <cell>+ Cluster LM</cell>
              <cell>17.77</cell>
              <cell>16.06</cell>
            </row>
            <row>
              <cell>+ DWL</cell>
              <cell>17.75</cell>
              <cell>16.28</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Translation results for English-French</caption>
        <reference_text>In PAGE 5: ...28 Table 2: Translation results for English-German 3.3 English-French  Table3  summarizes how our English-French sys- tem evolved. The baseline system here was trained on the EPPS, NC, and UN corpora, while the lan- guage model was trained on all the French part of the parallel corpora (including the Giga corpus)....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>This leads to a system with 30.58.</cell>
              <cell>This leads to a system with 30.58.</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Dev</cell>
              <cell>Test</cell>
            </row>
            <row>
              <cell>Baseline</cell>
              <cell>24.96</cell>
              <cell>26.67</cell>
            </row>
            <row>
              <cell>+ GigParData</cell>
              <cell>26.12</cell>
              <cell>28.16</cell>
            </row>
            <row>
              <cell>+ Big LMs</cell>
              <cell>29.22</cell>
              <cell>29.92</cell>
            </row>
            <row>
              <cell>+ All Reo</cell>
              <cell>29.14</cell>
              <cell>30.10</cell>
            </row>
            <row>
              <cell>+ PT Adaptation</cell>
              <cell>29.15</cell>
              <cell>30.22</cell>
            </row>
            <row>
              <cell>+ Bilingual LM</cell>
              <cell>29.17</cell>
              <cell>30.49</cell>
            </row>
            <row>
              <cell>+ Cluster LM</cell>
              <cell>29.08</cell>
              <cell>30.58</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: Translation results for French-English</caption>
        <reference_text>None</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>the final score.</cell>
              <cell>None</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Dev</cell>
              <cell>Test</cell>
            </row>
            <row>
              <cell>Baseline</cell>
              <cell>25.81</cell>
              <cell>27.15</cell>
            </row>
            <row>
              <cell>+ Indomain LM</cell>
              <cell>26.17</cell>
              <cell>27.91</cell>
            </row>
            <row>
              <cell>+ PT Adaptation</cell>
              <cell>26.33</cell>
              <cell>28.11</cell>
            </row>
            <row>
              <cell>+ Big LMs</cell>
              <cell>28.90</cell>
              <cell>29.82</cell>
            </row>
            <row>
              <cell>+ Bilingual LM</cell>
              <cell>29.14</cell>
              <cell>30.09</cell>
            </row>
            <row>
              <cell>+ Cluster LM</cell>
              <cell>29.31</cell>
              <cell>30.25</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Teresa Herrmann</author>
          <author>Mohammed Mediani</author>
          <author>Jan Niehues</author>
          <author>Alex Waibel</author>
        </authors>
        <title>The karlsruhe institute of technology translation systems for the wmt 2011.</title>
        <publication>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</publication>
        <pages>379--385</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Kevin Knight</author>
        </authors>
        <title>Empirical Methods for Compound Splitting. In EACL,</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Arne Mauser</author>
          <author>Sa&#353;a Hasan</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Extending Statistical Machine Translation with Discriminative and Trigger-based Lexicon Models.</title>
        <publication>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP &#8217;09,</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Mohammed Mediani</author>
          <author>Eunah Cho</author>
          <author>Jan Niehues</author>
          <author>Teresa Herrmann</author>
          <author>Alex Waibel</author>
        </authors>
        <title>The kit englishfrench translation systems for iwslt</title>
        <publication>In Proceedings of the eight International Workshop on Spoken Language Translation (IWSLT).</publication>
        <pages>None</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Jan Niehues</author>
          <author>Muntsin Kolss</author>
        </authors>
        <title>A POS-Based Model for Long-Range Reorderings in SMT.</title>
        <publication>In Fourth Workshop on Statistical Machine Translation (WMT</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Jan Niehues</author>
          <author>Stephan Vogel</author>
        </authors>
        <title>Discriminative Word Alignment via Alignment Matrix Modeling.</title>
        <publication>In Proc. of Third ACL Workshop on Statistical Machine Translation,</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Jan Niehues</author>
          <author>Alex Waibel</author>
        </authors>
        <title>Using wikipedia to translate domain-specific terms in smt.</title>
        <publication>In Proceedings of the eight International Workshop on Spoken Language Translation (IWSLT).</publication>
        <pages>None</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Jan Niehues</author>
          <author>Mohammed Mediani</author>
          <author>Teresa Herrmann</author>
          <author>Michael Heck</author>
          <author>Christian Herff</author>
          <author>Alex Waibel</author>
        </authors>
        <title>The KIT Translation system for IWSLT</title>
        <publication>Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT),</publication>
        <pages>93--98</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Jan Niehues</author>
          <author>Teresa Herrmann</author>
          <author>Stephan Vogel</author>
          <author>Alex Waibel</author>
        </authors>
        <title>Wider Context by Using Bilingual Language Models in Machine Translation.</title>
        <publication>In Sixth Workshop on Statistical Machine Translation (WMT 2011),</publication>
        <pages>None</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>An efficient method for determining bilingual word classes.</title>
        <publication>In Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics, EACL &#8217;99,</publication>
        <pages>71--76</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Anna N Rafferty</author>
          <author>Christopher D Manning</author>
        </authors>
        <title>Parsing three german treebanks: lexicalized and unlexicalized baselines.</title>
        <publication>In Proceedings of the Workshop on Parsing German.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Kay Rottmann</author>
          <author>Stephan Vogel</author>
        </authors>
        <title>Word Reordering in Statistical Machine Translation with a POSBased Distortion Model. In TMI,</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Helmut Schmid</author>
          <author>Florian Laws</author>
        </authors>
        <title>Estimation of Conditional Probabilities with Decision Trees and an Application to Fine-Grained POS Tagging.</title>
        <publication>In COLING 2008,</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Helmut Schmid</author>
        </authors>
        <title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
        <publication>In International Conference on New Methods in Language Processing,</publication>
        <pages>None</pages>
        <date>1994</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Andreas Stolcke</author>
        </authors>
        <title>SRILM &#8211; An Extensible Language Modeling Toolkit.</title>
        <publication>In Proc. of ICSLP,</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Ashish Venugopal</author>
          <author>Andreas Zollman</author>
          <author>Alex Waibel</author>
        </authors>
        <title>Training and Evaluation Error Minimization Rules for Statistical Machine Translation.</title>
        <publication>In Workshop on Data-drive Machine Translation and Beyond (WPT-05),</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Herrmann et al. (2011)</string>
        <sentence_id>51558</sentence_id>
        <char_offset>74</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Koehn and Knight (2003)</string>
        <sentence_id>51454</sentence_id>
        <char_offset>96</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Mauser et al. (2009)</string>
        <sentence_id>51499</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>3</reference_id>
        <string>Mediani et al., 2011</string>
        <sentence_id>51517</sentence_id>
        <char_offset>71</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>3</reference_id>
        <string>Mediani et al. (2011)</string>
        <sentence_id>51455</sentence_id>
        <char_offset>113</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>4</reference_id>
        <string>Niehues and Kolss, 2009</string>
        <sentence_id>51466</sentence_id>
        <char_offset>146</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>5</reference_id>
        <string>Niehues and Vogel (2008)</string>
        <sentence_id>51568</sentence_id>
        <char_offset>174</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>6</reference_id>
        <string>Niehues and Waibel (2011)</string>
        <sentence_id>51522</sentence_id>
        <char_offset>37</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>7</reference_id>
        <string>Niehues et al., 2010</string>
        <sentence_id>51487</sentence_id>
        <char_offset>135</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>8</reference_id>
        <string>Niehues et al. (2011)</string>
        <sentence_id>51497</sentence_id>
        <char_offset>21</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>9</reference_id>
        <string>Och, 1999</string>
        <sentence_id>51564</sentence_id>
        <char_offset>69</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>10</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>51622</sentence_id>
        <char_offset>56</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>11</reference_id>
        <string>Rafferty and Manning, 2008</string>
        <sentence_id>51572</sentence_id>
        <char_offset>67</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>12</reference_id>
        <string>Rottmann and Vogel (2007)</string>
        <sentence_id>51465</sentence_id>
        <char_offset>58</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>13</reference_id>
        <string>Schmid and Laws, 2008</string>
        <sentence_id>51555</sentence_id>
        <char_offset>46</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>14</reference_id>
        <string>Schmid, 1994</string>
        <sentence_id>51571</sentence_id>
        <char_offset>77</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>15</reference_id>
        <string>Stolcke, 2002</string>
        <sentence_id>51569</sentence_id>
        <char_offset>108</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>16</reference_id>
        <string>Venugopal et al. (2005)</string>
        <sentence_id>51575</sentence_id>
        <char_offset>83</char_offset>
      </citation>
    </citations>
  </content>
</document>
