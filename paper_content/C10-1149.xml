<document>
  <filename>C10-1149</filename>
  <authors/>
  <title>Leveraging Multiple MT Engines for Paraphrase Generation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>This paper proposes a method that leverages multiple machine translation (MT) engines for paraphrase generation (PG). The method includes two stages. Firstly, we use a multi-pivot approach to acquire a set of candidate paraphrases for a source sentence S. Then, we employ two kinds of techniques, namely the selection-based technique and the decoding-based technique, to produce a best paraphrase T for S using the candidates acquired in the first stage. Experimental results show that: (1) The multi-pivot approach is effective for obtaining plenty of valuable candidate paraphrases. (2) Both the selectionbased and decoding-based techniques can make good use of the candidates and produce high-quality paraphrases. Moreover, these two techniques are complementary. (3) The proposed method outperforms a state-of-the-art paraphrase generation approach.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This paper proposes a method that leverages multiple machine translation (MT) engines for paraphrase generation (PG).</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The method includes two stages.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Firstly, we use a multi-pivot approach to acquire a set of candidate paraphrases for a source sentence S.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Then, we employ two kinds of techniques, namely the selection-based technique and the decoding-based technique, to produce a best paraphrase T for S using the candidates acquired in the first stage.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Experimental results show that: (1) The multi-pivot approach is effective for obtaining plenty of valuable candidate paraphrases.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>(2) Both the selectionbased and decoding-based techniques can make good use of the candidates and produce high-quality paraphrases.</text>
              <doc_id>5</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, these two techniques are complementary.</text>
              <doc_id>6</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>(3) The proposed method outperforms a state-of-the-art paraphrase generation approach.</text>
              <doc_id>7</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>This paper addresses the problem of paraphrase generation (PG), which seeks to generate paraphrases for sentences. PG is important in many natural language processing (NLP) applications. For example, in machine translation (MT), a sentence can be paraphrased so as to make it more translatable (Zhang and Yamamoto, 2002; Callison-Burch et al., 2006). In question answering (QA), a question can be paraphrased to improve the coverage of answer extraction (Duboue and Chu-Carroll, 2006; Riezler et al., 2007). In natural language generation (NLG), paraphrasing can help to increase the expressive power of the NLG systems (Iordanskaja et al., 1991).
In this paper, we propose a novel PG method. For an English sentence S, the method first acquires a set of candidate paraphrases with a multipivot approach, which uses MT engines to automatically translate S into multiple pivot languages and then translate them back into English. Furthermore, the method employs two kinds of techniques to produce a best paraphrase T for S using the candidates, i.e., the selection-based and decoding-based techniques. The former selects a best paraphrase from the candidates based on Minimum Bayes Risk (MBR), while the latter trains a MT model using the candidates and generates paraphrases with a MT decoder.
We evaluate our method on a set of 1182 English sentences. The results show that: (1) although the candidate paraphrases acquired by MT engines are noisy, they provide good raw materials for further paraphrase generation; (2) the selection-based technique is effective, which results in the best performance; (3) the decodingbased technique is promising, which can generate paraphrases that are different from the candidates; (4) both the selection-based and decoding-based techniques outperform a state-of-the-art approach SPG (Zhao et al., 2009).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This paper addresses the problem of paraphrase generation (PG), which seeks to generate paraphrases for sentences.</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>PG is important in many natural language processing (NLP) applications.</text>
              <doc_id>9</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For example, in machine translation (MT), a sentence can be paraphrased so as to make it more translatable (Zhang and Yamamoto, 2002; Callison-Burch et al., 2006).</text>
              <doc_id>10</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In question answering (QA), a question can be paraphrased to improve the coverage of answer extraction (Duboue and Chu-Carroll, 2006; Riezler et al., 2007).</text>
              <doc_id>11</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In natural language generation (NLG), paraphrasing can help to increase the expressive power of the NLG systems (Iordanskaja et al., 1991).</text>
              <doc_id>12</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this paper, we propose a novel PG method.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For an English sentence S, the method first acquires a set of candidate paraphrases with a multipivot approach, which uses MT engines to automatically translate S into multiple pivot languages and then translate them back into English.</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Furthermore, the method employs two kinds of techniques to produce a best paraphrase T for S using the candidates, i.e., the selection-based and decoding-based techniques.</text>
              <doc_id>15</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The former selects a best paraphrase from the candidates based on Minimum Bayes Risk (MBR), while the latter trains a MT model using the candidates and generates paraphrases with a MT decoder.</text>
              <doc_id>16</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We evaluate our method on a set of 1182 English sentences.</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The results show that: (1) although the candidate paraphrases acquired by MT engines are noisy, they provide good raw materials for further paraphrase generation; (2) the selection-based technique is effective, which results in the best performance; (3) the decodingbased technique is promising, which can generate paraphrases that are different from the candidates; (4) both the selection-based and decoding-based techniques outperform a state-of-the-art approach SPG (Zhao et al., 2009).</text>
              <doc_id>18</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Related Work</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>19</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Methods for Paraphrase Generation</title>
            <text>MT-based method is the mainstream method on PG. It regards PG as a monolingual machine translation problem, i.e., &#8220;translating&#8221; a sentence S into another sentence T in the same language.
Quirk et al. (2004) first presented the MT-based method. They trained a statistical MT (SMT) model on a monolingual parallel corpus extracted from comparable news articles and applied the model to generate paraphrases. Their work shows that SMT techniques can be extended to PG. However, its usefulness is limited by the scarcity of monolingual parallel data.
To overcome the data sparseness problem, Zhao et al. (2008a) improved the MT-based PG method by training the paraphrase model using multiple resources, including monolingual parallel corpora, monolingual comparable corpora, bilingual parallel corpora, etc. Their results show that bilingual parallel corpora are the most useful among the exploited resources. Zhao et al. (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications.
The main disadvantage of the MT-based method is that its performance heavily depends on the fine-grained paraphrases, such as paraphrase phrases and patterns, which provide paraphrase options in decoding. Hence one has to first extract fine-grained paraphrases from various corpora with different methods (Zhao et al., 2008a; Zhao et al., 2009), which is difficult and timeconsuming.
In addition to the MT-based method, researchers have also investigated other methods for paraphrase generation, such as the pattern-based methods (Barzilay and Lee, 2003; Pang et al., 2003), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and NLG-based methods (Kozlowski et al., 2003; Power and Scott, 2005).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>MT-based method is the mainstream method on PG.</text>
                  <doc_id>20</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It regards PG as a monolingual machine translation problem, i.e., &#8220;translating&#8221; a sentence S into another sentence T in the same language.</text>
                  <doc_id>21</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Quirk et al. (2004) first presented the MT-based method.</text>
                  <doc_id>22</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>They trained a statistical MT (SMT) model on a monolingual parallel corpus extracted from comparable news articles and applied the model to generate paraphrases.</text>
                  <doc_id>23</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Their work shows that SMT techniques can be extended to PG.</text>
                  <doc_id>24</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>However, its usefulness is limited by the scarcity of monolingual parallel data.</text>
                  <doc_id>25</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To overcome the data sparseness problem, Zhao et al. (2008a) improved the MT-based PG method by training the paraphrase model using multiple resources, including monolingual parallel corpora, monolingual comparable corpora, bilingual parallel corpora, etc. Their results show that bilingual parallel corpora are the most useful among the exploited resources.</text>
                  <doc_id>26</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Zhao et al. (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications.</text>
                  <doc_id>27</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The main disadvantage of the MT-based method is that its performance heavily depends on the fine-grained paraphrases, such as paraphrase phrases and patterns, which provide paraphrase options in decoding.</text>
                  <doc_id>28</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Hence one has to first extract fine-grained paraphrases from various corpora with different methods (Zhao et al., 2008a; Zhao et al., 2009), which is difficult and timeconsuming.</text>
                  <doc_id>29</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In addition to the MT-based method, researchers have also investigated other methods for paraphrase generation, such as the pattern-based methods (Barzilay and Lee, 2003; Pang et al., 2003), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and NLG-based methods (Kozlowski et al., 2003; Power and Scott, 2005).</text>
                  <doc_id>30</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Pivot Approach for Paraphrasing</title>
            <text>Bannard and Callison-Burch (2005) introduced the pivot approach to extracting paraphrase phrases from bilingual parallel corpora. Their basic assumption is that two English phrases aligned with the same phrase in a foreign language (also called a pivot language) are potential paraphrases. Zhao et al. (2008b) extended the approach and used it to extract paraphrase patterns. Both of the above works have proved the effectiveness of the pivot approach in paraphrase extraction.
Pivot approach can also be used in paraphrase generation. It generates paraphrases by translating sentences from a source language to one (singlepivot) or more (multi-pivot) pivot languages and then translating them back to the source language. Duboue et al. (2006) first proposed the multipivot approach for paraphrase generation, which was specially designed for question expansion in QA. In addition, Max (2009) presented a singlepivot approach for generating sub-sentential paraphrases. A clear difference between our method and the above works is that we propose selectionbased and decoding-based techniques to generate high-quality paraphrases using the candidates yielded from the pivot approach.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Bannard and Callison-Burch (2005) introduced the pivot approach to extracting paraphrase phrases from bilingual parallel corpora.</text>
                  <doc_id>31</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Their basic assumption is that two English phrases aligned with the same phrase in a foreign language (also called a pivot language) are potential paraphrases.</text>
                  <doc_id>32</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Zhao et al. (2008b) extended the approach and used it to extract paraphrase patterns.</text>
                  <doc_id>33</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Both of the above works have proved the effectiveness of the pivot approach in paraphrase extraction.</text>
                  <doc_id>34</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Pivot approach can also be used in paraphrase generation.</text>
                  <doc_id>35</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It generates paraphrases by translating sentences from a source language to one (singlepivot) or more (multi-pivot) pivot languages and then translating them back to the source language.</text>
                  <doc_id>36</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Duboue et al. (2006) first proposed the multipivot approach for paraphrase generation, which was specially designed for question expansion in QA.</text>
                  <doc_id>37</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, Max (2009) presented a singlepivot approach for generating sub-sentential paraphrases.</text>
                  <doc_id>38</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>A clear difference between our method and the above works is that we propose selectionbased and decoding-based techniques to generate high-quality paraphrases using the candidates yielded from the pivot approach.</text>
                  <doc_id>39</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Multi-pivot Approach for Acquiring Candidate Paraphrases</title>
        <text>A single-pivot PG approach paraphrases a sentence S by translating it into a pivot language P L with a MT engine MT 1 and then translating it back into the source language with MT 2 . In this paper, a single-pivot PG system is represented as a triple (MT 1 , P L, MT 2 ). A multipivot PG system is made up of a set of single-pivot systems with various pivot languages and MT engines. Given m pivot languages and n MT engines, we can build a multi-pivot PG system consisting of N (N &#8804; n &#8727; m &#8727; n) single-pivot ones, where N = n &#8727; m &#8727; n iff all the n MT engines can perform bidirectional translation between the source and each pivot language.
In this work, we experiment with 6 pivot languages (Table 1) and 3 MT engines (Table 2) in the multi-pivot approach. All the 3 MT engines are off-the-shelf systems, in which Google and Microsoft translators are SMT engines, while Systran translator is a rule-based MT engine. Each MT engine can translate English to all the 6 pivot languages and back to English. We thereby construct a multi-pivot PG system consisting of 54 (3*6*3) single-pivot systems.
The advantages of the multi-pivot PG approach lie in two aspects. First, it effectively makes use of the vast bilingual data and translation rules underlying the MT engines. Second, the approach is simple, which just sends sentences to the online MT engines and gets the translations back.
1 Google Translate (GG)
(translate.google.com) 2 Microsoft Translator (MS)
(www.microsofttranslator.com) 3 Systran Online Translation (ST)
(www.systransoft.com)
4 Producing High-quality Paraphrases using the Candidates
Table 3 shows some examples of candidate paraphrases for a sentence. As can be seen, the candidates do provide some correct and useful paraphrase substitutes (in bold) for the source sentence. However, they also contain quite a few errors (in italic) due to the limited translation quality of the MT engines. The problem is even worse when the source sentences get longer and more complicated. Therefore, we need to combine the outputs of the multiple single-pivot PG systems and produce high-quality paraphrases out of them. To this end, we investigate two techniques, namely, the selection-based and decodingbased techniques.
4.1 Selection-based Technique
Given a source sentence S along with a set D of candidate paraphrases {T 1 , T 2 , ..., T i , ...T N }, the goal of the selection-based technique is to select the best paraphrase &#710;T i for S from D. The paraphrase selection technique we propose is based on Minimum Bayes Risk (MBR). In detail, the MBR based technique first measures the quality of each candidate paraphrase T i &#8712; D in terms of Bayes risk (BR), and then selects the one with the minimum BR as the best paraphrase. In detail, given S, a candidate T i &#8712; D, a reference paraphrase T 1 , and a loss function L(T, T i ) that measures the quality of T i relative to T , we define the Bayes risk as follows:
BR(T i ) = E P (T,S) [L(T, T i )], (1)
where the expectation is taken under the true distribution P (T, S) of the paraphrases. According to (Bickel and Doksum, 1977), the candidate paraphrase that minimizes the Bayes risk can be found as follows:
&#710;T i = arg min
T i &#8712;D T &#8712;T
&#8721; L(T, T i )P (T |S), (2)
where T represents the space of reference paraphrases. In practice, however, the collection of reference paraphrases is not available. We thus construct a set D &#8242; = D &#8746; {S} to approximate T 2 . In addition, we cannot estimate P (T |S) in Equation (2), either. Therefore, we make a simplification by assigning a constant c to P (T |S) for each T &#8712; D &#8242; , which can then be removed:
&#710;T i = arg min
T i &#8712;D
&#8721;
T &#8712;D &#8242; L(T, T i ). (3)
Equation (3) can be further rewritten using a gain function G(T, T i ) instead of the loss function:
1 Here we assume that we have the collection of all possible paraphrases of S, which are used as references. 2 The source sentence S is included in D &#8242; based on the
consideration that a sentence is allowed to keep unchanged during paraphrasing.
&#8721; &#710;T i = arg max G(T, T i ). (4)
T i &#8712;D T &#8712;D &#8242;
We define the gain function based on BLEU: G(T, T i ) = BLEU(T, T i ). BLEU is a widely used metric in the automatic evaluation of MT (Papineni et al., 2002). It measures the similarity of two sentences by counting the overlapping n-grams (n=1,2,3,4 in our experiments):
BLEU(T, T i ) = BP &#183;exp( 4&#8721;
w n log p n (T, T i )),
n=1
where p n (T, T i ) is the n-gram precision of T i and w n = 1/4. BP (&#8804; 1) is a brevity penalty that penalizes T i if it is shorter than T . In summary, for each sentence S, the MBR based technique selects a paraphrase that is the most similar to all candidates and the source sentence. The underlying assumption is that correct paraphrase substitutes should be common among the candidates, while errors committed by the single-pivot PG systems should be all different. We denote this approach as S-1 hereafter.
Approaches for comparison. In the experiments, we also design another two paraphrase selection approaches S-2 and S-3 for comparison with S-1.
S-2: S-2 selects the best single-pivot PG system from all the 54 ones. The selection is also based on MBR and BLEU. For each single-pivot PG system, we sum up its gain function
&#8721; values &#8721;
over a set of source sentences (i.e.,
S T S &#8712;D S &#8242; G(T S , T Si )). Then we select the one with the maximum gain value as the best single-pivot system. In our experiments, the selected best single-pivot PG system is (ST, P, GG), the candidate paraphrases acquired by which are then returned as the best paraphrases in S-2.
S-3: S-3 is a simple baseline, which just randomly selects a paraphrase from the 54 candidates for each source sentence S.
4.2 Decoding-based Technique
The selection-based technique introduced above has an inherent limitation that it can only select a paraphrase from the candidates. That is to say, it major cuts significant cuts major cuts* important cuts big cuts great cuts
high-level civil servants senior officials high-level officials senior civil servants
can never produce a perfect paraphrase if all the candidates have some tiny flaws. To solve this problem, we propose the decoding-based technique, which trains a MT model using the candidate paraphrases of each source sentence S and generates a new paraphrase T for S with a MT decoder.
In this work, we implement the decoding-based technique using Giza++ (Och and Ney, 2000) and Moses (Hoang and Koehn, 2008), both of which are commonly used SMT tools. For a sentence S, we first construct a set of parallel sentences by pairing S with each of its candidate paraphrases: {(S,T 1 ),(S,T 2 ),...,(S,T N )} (N = 54). We then run word alignment on the set using Giza++ and extract aligned phrase pairs as described in (Koehn, 2004). Here we only keep the phrase pairs that are aligned &#8805;3 times on the set, so as to filter errors brought by the noisy sentence pairs. The extracted phrase pairs are stored in a phrase table. Table 4 shows some extracted phrase pairs.
Note that Giza++ is sensitive to the data size. Hence it is interesting to examine if the alignment can be improved by augmenting the parallel sentence pairs. To this end, we have tried augmenting the parallel set for each sentence S by pairing any two candidate paraphrases. In this manner, C 2 N sentence pairs are augmented for each S. We conduct word alignment using the (N +C 2 N ) sentence pairs and extract aligned phrases from the original N pairs. However, we have not found clear improvement after observing the results. Therefore, we do not adopt the augmentation strategy in our experiments.
Using the extracted phrasal paraphrases, we conduct decoding for the sentence S with Moses, which is based on a log-linear model. The default setting of Moses is used, except that the distortion model for phrase reordering is turned off 3 . The language model in Moses is trained using a 9 GB English corpus. We denote the above approach as D-1 in what follows.
Approach for comparison. The main advantage of the decoding-based technique is that it allows us to customize the paraphrases for different requirements through tailoring the phrase table or tuning the model parameters. As a case study, this paper shows how to generate paraphrases with varied paraphrase rates 4 .
D-2: The extracted phrasal paraphrases (including self-paraphrases) are stored in a phrase table, in which each phrase pair has 4 scores measuring their alignment confidence (Koehn et al., 2003). Our basic idea is to control the paraphrase rate by tuning the scores of the self-paraphrases. We thus extend D-1 to D-2, which assigns a weight &#955; (&#955; &gt; 0) to the scores of the selfparaphrase pairs. Obviously, if we set &#955; &lt; 1, the self-paraphrases will be penalized and the decoder will prefer to generate a paraphrase with more changes. If we set &#955; &gt; 1, the decoder will tend to generate a paraphrase that is more similar to the source sentence. In our experiments, we set &#955; = 0.1 in D-2.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>A single-pivot PG approach paraphrases a sentence S by translating it into a pivot language P L with a MT engine MT 1 and then translating it back into the source language with MT 2 .</text>
              <doc_id>40</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, a single-pivot PG system is represented as a triple (MT 1 , P L, MT 2 ).</text>
              <doc_id>41</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A multipivot PG system is made up of a set of single-pivot systems with various pivot languages and MT engines.</text>
              <doc_id>42</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Given m pivot languages and n MT engines, we can build a multi-pivot PG system consisting of N (N &#8804; n &#8727; m &#8727; n) single-pivot ones, where N = n &#8727; m &#8727; n iff all the n MT engines can perform bidirectional translation between the source and each pivot language.</text>
              <doc_id>43</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this work, we experiment with 6 pivot languages (Table 1) and 3 MT engines (Table 2) in the multi-pivot approach.</text>
              <doc_id>44</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>All the 3 MT engines are off-the-shelf systems, in which Google and Microsoft translators are SMT engines, while Systran translator is a rule-based MT engine.</text>
              <doc_id>45</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Each MT engine can translate English to all the 6 pivot languages and back to English.</text>
              <doc_id>46</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We thereby construct a multi-pivot PG system consisting of 54 (3*6*3) single-pivot systems.</text>
              <doc_id>47</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The advantages of the multi-pivot PG approach lie in two aspects.</text>
              <doc_id>48</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>First, it effectively makes use of the vast bilingual data and translation rules underlying the MT engines.</text>
              <doc_id>49</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Second, the approach is simple, which just sends sentences to the online MT engines and gets the translations back.</text>
              <doc_id>50</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 Google Translate (GG)</text>
              <doc_id>51</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(translate.google.com) 2 Microsoft Translator (MS)</text>
              <doc_id>52</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(www.microsofttranslator.com) 3 Systran Online Translation (ST)</text>
              <doc_id>53</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(www.systransoft.com)</text>
              <doc_id>54</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>4 Producing High-quality Paraphrases using the Candidates</text>
              <doc_id>55</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Table 3 shows some examples of candidate paraphrases for a sentence.</text>
              <doc_id>56</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As can be seen, the candidates do provide some correct and useful paraphrase substitutes (in bold) for the source sentence.</text>
              <doc_id>57</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, they also contain quite a few errors (in italic) due to the limited translation quality of the MT engines.</text>
              <doc_id>58</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The problem is even worse when the source sentences get longer and more complicated.</text>
              <doc_id>59</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, we need to combine the outputs of the multiple single-pivot PG systems and produce high-quality paraphrases out of them.</text>
              <doc_id>60</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>To this end, we investigate two techniques, namely, the selection-based and decodingbased techniques.</text>
              <doc_id>61</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>4.1 Selection-based Technique</text>
              <doc_id>62</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Given a source sentence S along with a set D of candidate paraphrases {T 1 , T 2 , ..., T i , ...T N }, the goal of the selection-based technique is to select the best paraphrase &#710;T i for S from D.</text>
              <doc_id>63</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The paraphrase selection technique we propose is based on Minimum Bayes Risk (MBR).</text>
              <doc_id>64</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In detail, the MBR based technique first measures the quality of each candidate paraphrase T i &#8712; D in terms of Bayes risk (BR), and then selects the one with the minimum BR as the best paraphrase.</text>
              <doc_id>65</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In detail, given S, a candidate T i &#8712; D, a reference paraphrase T 1 , and a loss function L(T, T i ) that measures the quality of T i relative to T , we define the Bayes risk as follows:</text>
              <doc_id>66</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>BR(T i ) = E P (T,S) [L(T, T i )], (1)</text>
              <doc_id>67</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where the expectation is taken under the true distribution P (T, S) of the paraphrases.</text>
              <doc_id>68</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>According to (Bickel and Doksum, 1977), the candidate paraphrase that minimizes the Bayes risk can be found as follows:</text>
              <doc_id>69</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#710;T i = arg min</text>
              <doc_id>70</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>T i &#8712;D T &#8712;T</text>
              <doc_id>71</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8721; L(T, T i )P (T |S), (2)</text>
              <doc_id>72</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where T represents the space of reference paraphrases.</text>
              <doc_id>73</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In practice, however, the collection of reference paraphrases is not available.</text>
              <doc_id>74</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We thus construct a set D &#8242; = D &#8746; {S} to approximate T 2 .</text>
              <doc_id>75</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In addition, we cannot estimate P (T |S) in Equation (2), either.</text>
              <doc_id>76</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, we make a simplification by assigning a constant c to P (T |S) for each T &#8712; D &#8242; , which can then be removed:</text>
              <doc_id>77</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#710;T i = arg min</text>
              <doc_id>78</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>T i &#8712;D</text>
              <doc_id>79</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8721;</text>
              <doc_id>80</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>T &#8712;D &#8242; L(T, T i ).</text>
              <doc_id>81</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>(3)</text>
              <doc_id>82</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Equation (3) can be further rewritten using a gain function G(T, T i ) instead of the loss function:</text>
              <doc_id>83</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 Here we assume that we have the collection of all possible paraphrases of S, which are used as references.</text>
              <doc_id>84</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2 The source sentence S is included in D &#8242; based on the</text>
              <doc_id>85</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>consideration that a sentence is allowed to keep unchanged during paraphrasing.</text>
              <doc_id>86</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8721; &#710;T i = arg max G(T, T i ).</text>
              <doc_id>87</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>(4)</text>
              <doc_id>88</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>T i &#8712;D T &#8712;D &#8242;</text>
              <doc_id>89</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We define the gain function based on BLEU: G(T, T i ) = BLEU(T, T i ).</text>
              <doc_id>90</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>BLEU is a widely used metric in the automatic evaluation of MT (Papineni et al., 2002).</text>
              <doc_id>91</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>It measures the similarity of two sentences by counting the overlapping n-grams (n=1,2,3,4 in our experiments):</text>
              <doc_id>92</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>BLEU(T, T i ) = BP &#183;exp( 4&#8721;</text>
              <doc_id>93</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>w n log p n (T, T i )),</text>
              <doc_id>94</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>n=1</text>
              <doc_id>95</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where p n (T, T i ) is the n-gram precision of T i and w n = 1/4.</text>
              <doc_id>96</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>BP (&#8804; 1) is a brevity penalty that penalizes T i if it is shorter than T .</text>
              <doc_id>97</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In summary, for each sentence S, the MBR based technique selects a paraphrase that is the most similar to all candidates and the source sentence.</text>
              <doc_id>98</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The underlying assumption is that correct paraphrase substitutes should be common among the candidates, while errors committed by the single-pivot PG systems should be all different.</text>
              <doc_id>99</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We denote this approach as S-1 hereafter.</text>
              <doc_id>100</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Approaches for comparison.</text>
              <doc_id>101</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In the experiments, we also design another two paraphrase selection approaches S-2 and S-3 for comparison with S-1.</text>
              <doc_id>102</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S-2: S-2 selects the best single-pivot PG system from all the 54 ones.</text>
              <doc_id>103</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The selection is also based on MBR and BLEU.</text>
              <doc_id>104</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For each single-pivot PG system, we sum up its gain function</text>
              <doc_id>105</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8721; values &#8721;</text>
              <doc_id>106</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>over a set of source sentences (i.e.,</text>
              <doc_id>107</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S T S &#8712;D S &#8242; G(T S , T Si )).</text>
              <doc_id>108</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Then we select the one with the maximum gain value as the best single-pivot system.</text>
              <doc_id>109</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In our experiments, the selected best single-pivot PG system is (ST, P, GG), the candidate paraphrases acquired by which are then returned as the best paraphrases in S-2.</text>
              <doc_id>110</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S-3: S-3 is a simple baseline, which just randomly selects a paraphrase from the 54 candidates for each source sentence S.</text>
              <doc_id>111</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>4.2 Decoding-based Technique</text>
              <doc_id>112</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The selection-based technique introduced above has an inherent limitation that it can only select a paraphrase from the candidates.</text>
              <doc_id>113</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>That is to say, it major cuts significant cuts major cuts* important cuts big cuts great cuts</text>
              <doc_id>114</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>high-level civil servants senior officials high-level officials senior civil servants</text>
              <doc_id>115</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>can never produce a perfect paraphrase if all the candidates have some tiny flaws.</text>
              <doc_id>116</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To solve this problem, we propose the decoding-based technique, which trains a MT model using the candidate paraphrases of each source sentence S and generates a new paraphrase T for S with a MT decoder.</text>
              <doc_id>117</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this work, we implement the decoding-based technique using Giza++ (Och and Ney, 2000) and Moses (Hoang and Koehn, 2008), both of which are commonly used SMT tools.</text>
              <doc_id>118</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For a sentence S, we first construct a set of parallel sentences by pairing S with each of its candidate paraphrases: {(S,T 1 ),(S,T 2 ),...,(S,T N )} (N = 54).</text>
              <doc_id>119</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We then run word alignment on the set using Giza++ and extract aligned phrase pairs as described in (Koehn, 2004).</text>
              <doc_id>120</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Here we only keep the phrase pairs that are aligned &#8805;3 times on the set, so as to filter errors brought by the noisy sentence pairs.</text>
              <doc_id>121</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The extracted phrase pairs are stored in a phrase table.</text>
              <doc_id>122</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Table 4 shows some extracted phrase pairs.</text>
              <doc_id>123</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Note that Giza++ is sensitive to the data size.</text>
              <doc_id>124</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Hence it is interesting to examine if the alignment can be improved by augmenting the parallel sentence pairs.</text>
              <doc_id>125</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>To this end, we have tried augmenting the parallel set for each sentence S by pairing any two candidate paraphrases.</text>
              <doc_id>126</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In this manner, C 2 N sentence pairs are augmented for each S.</text>
              <doc_id>127</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We conduct word alignment using the (N +C 2 N ) sentence pairs and extract aligned phrases from the original N pairs.</text>
              <doc_id>128</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>However, we have not found clear improvement after observing the results.</text>
              <doc_id>129</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, we do not adopt the augmentation strategy in our experiments.</text>
              <doc_id>130</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Using the extracted phrasal paraphrases, we conduct decoding for the sentence S with Moses, which is based on a log-linear model.</text>
              <doc_id>131</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The default setting of Moses is used, except that the distortion model for phrase reordering is turned off 3 .</text>
              <doc_id>132</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The language model in Moses is trained using a 9 GB English corpus.</text>
              <doc_id>133</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We denote the above approach as D-1 in what follows.</text>
              <doc_id>134</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Approach for comparison.</text>
              <doc_id>135</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The main advantage of the decoding-based technique is that it allows us to customize the paraphrases for different requirements through tailoring the phrase table or tuning the model parameters.</text>
              <doc_id>136</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>As a case study, this paper shows how to generate paraphrases with varied paraphrase rates 4 .</text>
              <doc_id>137</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>D-2: The extracted phrasal paraphrases (including self-paraphrases) are stored in a phrase table, in which each phrase pair has 4 scores measuring their alignment confidence (Koehn et al., 2003).</text>
              <doc_id>138</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our basic idea is to control the paraphrase rate by tuning the scores of the self-paraphrases.</text>
              <doc_id>139</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We thus extend D-1 to D-2, which assigns a weight &#955; (&#955; &gt; 0) to the scores of the selfparaphrase pairs.</text>
              <doc_id>140</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Obviously, if we set &#955; &lt; 1, the self-paraphrases will be penalized and the decoder will prefer to generate a paraphrase with more changes.</text>
              <doc_id>141</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>If we set &#955; &gt; 1, the decoder will tend to generate a paraphrase that is more similar to the source sentence.</text>
              <doc_id>142</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In our experiments, we set &#955; = 0.1 in D-2.</text>
              <doc_id>143</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>5 Experimental Setup</title>
        <text>Our test sentences are extracted from the parallel reference translations of a Chinese-to-English MT evaluation 5 , in which each Chinese sentence c has 4 English reference translations, namely e 1 , e 2 , e 3 , and e 4 . We use e 1 as a test sentence to paraphrase and e 2 , e 3 , e 4 as human paraphrases of e 1 for comparison with the automatically generated paraphrases. We process the test set by manually filtering ill-formed sentences, such as the ungrammatical or incomplete ones. 1182 out of 1357
3 We conduct monotone decoding as previous work
(Quirk et al., 2004; Zhao et al., 2008a, Zhao et al., 2009). 4 The paraphrase rate reflects how different a paraphrase
is from the source sentence. 5 2008 NIST Open Machine Translation Evaluation: Chinese to English Task.
test sentences are retained after filtering. Statistics show that about half of the test sentences are from news and the other half are from essays. The average length of the test sentences is 34.12 (words).
Manual evaluation is used in this work. A paraphrase T of a sentence S is manually scored based on a five point scale, which measures both the &#8220;adequacy&#8221; (i.e., how much of the meaning of S is preserved in T ) and &#8220;fluency&#8221; of T (See Table 5). The five point scale used here is similar to that in the human evaluation of MT (Callison-Burch et al., 2007). In MT, adequacy and fluency are evaluated separately. However, we find that there is a high correlation between the two aspects, which makes it difficult to separate them. Thus we combine them in this paper.
We compare our method with a state-of-theart approach SPG 6 (Zhao et al., 2009), which is a statistical approach specially designed for PG. The approach first collects a large volume of fine-grained paraphrase resources, including paraphrase phrases, patterns, and collocations, from various corpora using different methods. Then it generates paraphrases using these resources with a statistical model 7 .</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Our test sentences are extracted from the parallel reference translations of a Chinese-to-English MT evaluation 5 , in which each Chinese sentence c has 4 English reference translations, namely e 1 , e 2 , e 3 , and e 4 .</text>
              <doc_id>144</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We use e 1 as a test sentence to paraphrase and e 2 , e 3 , e 4 as human paraphrases of e 1 for comparison with the automatically generated paraphrases.</text>
              <doc_id>145</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We process the test set by manually filtering ill-formed sentences, such as the ungrammatical or incomplete ones.</text>
              <doc_id>146</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>1182 out of 1357</text>
              <doc_id>147</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3 We conduct monotone decoding as previous work</text>
              <doc_id>148</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(Quirk et al., 2004; Zhao et al., 2008a, Zhao et al., 2009).</text>
              <doc_id>149</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>4 The paraphrase rate reflects how different a paraphrase</text>
              <doc_id>150</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>is from the source sentence.</text>
              <doc_id>151</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>5 2008 NIST Open Machine Translation Evaluation: Chinese to English Task.</text>
              <doc_id>152</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>test sentences are retained after filtering.</text>
              <doc_id>153</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Statistics show that about half of the test sentences are from news and the other half are from essays.</text>
              <doc_id>154</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The average length of the test sentences is 34.12 (words).</text>
              <doc_id>155</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Manual evaluation is used in this work.</text>
              <doc_id>156</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A paraphrase T of a sentence S is manually scored based on a five point scale, which measures both the &#8220;adequacy&#8221; (i.e., how much of the meaning of S is preserved in T ) and &#8220;fluency&#8221; of T (See Table 5).</text>
              <doc_id>157</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The five point scale used here is similar to that in the human evaluation of MT (Callison-Burch et al., 2007).</text>
              <doc_id>158</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In MT, adequacy and fluency are evaluated separately.</text>
              <doc_id>159</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>However, we find that there is a high correlation between the two aspects, which makes it difficult to separate them.</text>
              <doc_id>160</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Thus we combine them in this paper.</text>
              <doc_id>161</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We compare our method with a state-of-theart approach SPG 6 (Zhao et al., 2009), which is a statistical approach specially designed for PG.</text>
              <doc_id>162</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The approach first collects a large volume of fine-grained paraphrase resources, including paraphrase phrases, patterns, and collocations, from various corpora using different methods.</text>
              <doc_id>163</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Then it generates paraphrases using these resources with a statistical model 7 .</text>
              <doc_id>164</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>6 Experimental Results</title>
        <text>We evaluate six approaches, i.e., S-1, S-2, S-3, D- 1, D-2 and SPG, in the experiments. Each approach generates a 1-best paraphrase for a test sentence S. We randomize the order of the 6 paraphrases of each S to avoid bias of the raters.
6 SPG: Statistical Paraphrase Generation. 7 We ran SPG under the setting of baseline-2 as described
in (Zhao et al., 2009).
4.5
3.5
2.5
1.5
0.5
0
S-1 S-2 S-3 D-1 D-2 SPG
score 3.92 3.52 2.78 3.62 3.36 3.47
4.5
3.5
2.5
1.5
0.5
0
S-1 S-2 S-3 D-1 D-2 SPG
r1 r2 r3 r4 r5 r6</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We evaluate six approaches, i.e., S-1, S-2, S-3, D- 1, D-2 and SPG, in the experiments.</text>
              <doc_id>165</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each approach generates a 1-best paraphrase for a test sentence S.</text>
              <doc_id>166</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We randomize the order of the 6 paraphrases of each S to avoid bias of the raters.</text>
              <doc_id>167</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>6 SPG: Statistical Paraphrase Generation.</text>
              <doc_id>168</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>7 We ran SPG under the setting of baseline-2 as described</text>
              <doc_id>169</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>in (Zhao et al., 2009).</text>
              <doc_id>170</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>4.5</text>
              <doc_id>171</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3.5</text>
              <doc_id>172</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.5</text>
              <doc_id>173</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1.5</text>
              <doc_id>174</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.5</text>
              <doc_id>175</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0</text>
              <doc_id>176</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S-1 S-2 S-3 D-1 D-2 SPG</text>
              <doc_id>177</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>score 3.92 3.52 2.78 3.62 3.36 3.47</text>
              <doc_id>178</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>4.5</text>
              <doc_id>179</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3.5</text>
              <doc_id>180</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.5</text>
              <doc_id>181</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1.5</text>
              <doc_id>182</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.5</text>
              <doc_id>183</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0</text>
              <doc_id>184</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S-1 S-2 S-3 D-1 D-2 SPG</text>
              <doc_id>185</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>r1 r2 r3 r4 r5 r6</text>
              <doc_id>186</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>6.1 Human Evaluation Results</title>
            <text>We have 6 raters in the evaluation, all of whom are postgraduate students. In particular, 3 raters major in English, while the other 3 major in computer science. Each rater scores the paraphrases of 1/6 test sentences, whose results are then combined to form the final scoring result. The average scores of the six approaches are shown in Figure 1. We can find that among the selectionbased approaches, the performance of S-3 is the worst, which indicates that randomly selecting a paraphrase from the candidates works badly. S- 2 performs much better than S-3, suggesting that the quality of the paraphrases acquired with the best single-pivot PG system are much higher than the randomly selected ones. S-1 performs the best in all the six approaches, which demonstrates the effectiveness of the MBR-based selection technique. Additionally, the fact that S-1 evidently outperforms S-2 suggests that it is necessary to extend a single-pivot approach to a multi-pivot one.
To get a deeper insight of S-1, we randomly sample 100 test sentences and manually score all of their candidates. We find that S-1 successfully picks out a paraphrase with the highest score for 72 test sentences. We further analyze the remaining 28 sentences for which S-1 fails and find that the failures are mainly due to the BLEU-based gain function. For example, S-1 sometimes selects paraphrases that have correct phrases but incorrect phrase orders, since BLEU is weak in evaluating phrase orders and sentence structures. In the next step we shall improve the gain function by investigating other features besides BLEU. In the decoding-based approaches, D-1 ranks the second in the six approaches only behind S-1. Figure 2: Evaluation results from each rater.
We will further improve D-1 in the future rather than simply use Moses in decoding with the default setting. However, the value of D-1 lies in that it enables us to break down the candidates and generate new paraphrases flexibly. The performance decreases when we extend D-1 to D-2 to achieve a larger paraphrase rate. This is mainly because more errors are brought in when more parts of a sentence are paraphrased.
We can also find from Figure 1 that S-1, S-2, and D-1 all get higher scores than SPG, which shows that our method outperforms this state-ofthe-art approach. This is more important if we consider that our method is lightweight, which makes no effort to collect fine-grained paraphrase resources beforehand. After observing the results, we believe that the outperformance of our method can be mainly ascribed to the selection-based and decoding-based techniques, since we avoid many errors by voting among the candidates. For instance, an ambiguous phrase may be incorrectly paraphrased by some of the single-pivot PG systems or the SPG approach. However, our method may obtain the correct paraphrase through statistics over all candidates and selecting the most credible one.
The human evaluation of paraphrases is subjective. Hence it is necessary to examine the coherence among the raters. The scoring results from the six raters are depicted in Figure 2. As it can be seen, they show similar trends though the raters have different degrees of strictness.
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
S-1 S-2 S-3 D-1 D-2 SPG HP1 HP2 HP3</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We have 6 raters in the evaluation, all of whom are postgraduate students.</text>
                  <doc_id>187</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In particular, 3 raters major in English, while the other 3 major in computer science.</text>
                  <doc_id>188</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Each rater scores the paraphrases of 1/6 test sentences, whose results are then combined to form the final scoring result.</text>
                  <doc_id>189</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The average scores of the six approaches are shown in Figure 1.</text>
                  <doc_id>190</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We can find that among the selectionbased approaches, the performance of S-3 is the worst, which indicates that randomly selecting a paraphrase from the candidates works badly.</text>
                  <doc_id>191</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>S- 2 performs much better than S-3, suggesting that the quality of the paraphrases acquired with the best single-pivot PG system are much higher than the randomly selected ones.</text>
                  <doc_id>192</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>S-1 performs the best in all the six approaches, which demonstrates the effectiveness of the MBR-based selection technique.</text>
                  <doc_id>193</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Additionally, the fact that S-1 evidently outperforms S-2 suggests that it is necessary to extend a single-pivot approach to a multi-pivot one.</text>
                  <doc_id>194</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To get a deeper insight of S-1, we randomly sample 100 test sentences and manually score all of their candidates.</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We find that S-1 successfully picks out a paraphrase with the highest score for 72 test sentences.</text>
                  <doc_id>196</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We further analyze the remaining 28 sentences for which S-1 fails and find that the failures are mainly due to the BLEU-based gain function.</text>
                  <doc_id>197</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For example, S-1 sometimes selects paraphrases that have correct phrases but incorrect phrase orders, since BLEU is weak in evaluating phrase orders and sentence structures.</text>
                  <doc_id>198</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In the next step we shall improve the gain function by investigating other features besides BLEU.</text>
                  <doc_id>199</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In the decoding-based approaches, D-1 ranks the second in the six approaches only behind S-1.</text>
                  <doc_id>200</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 2: Evaluation results from each rater.</text>
                  <doc_id>201</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We will further improve D-1 in the future rather than simply use Moses in decoding with the default setting.</text>
                  <doc_id>202</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, the value of D-1 lies in that it enables us to break down the candidates and generate new paraphrases flexibly.</text>
                  <doc_id>203</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The performance decreases when we extend D-1 to D-2 to achieve a larger paraphrase rate.</text>
                  <doc_id>204</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This is mainly because more errors are brought in when more parts of a sentence are paraphrased.</text>
                  <doc_id>205</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We can also find from Figure 1 that S-1, S-2, and D-1 all get higher scores than SPG, which shows that our method outperforms this state-ofthe-art approach.</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This is more important if we consider that our method is lightweight, which makes no effort to collect fine-grained paraphrase resources beforehand.</text>
                  <doc_id>207</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>After observing the results, we believe that the outperformance of our method can be mainly ascribed to the selection-based and decoding-based techniques, since we avoid many errors by voting among the candidates.</text>
                  <doc_id>208</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For instance, an ambiguous phrase may be incorrectly paraphrased by some of the single-pivot PG systems or the SPG approach.</text>
                  <doc_id>209</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>However, our method may obtain the correct paraphrase through statistics over all candidates and selecting the most credible one.</text>
                  <doc_id>210</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The human evaluation of paraphrases is subjective.</text>
                  <doc_id>211</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Hence it is necessary to examine the coherence among the raters.</text>
                  <doc_id>212</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The scoring results from the six raters are depicted in Figure 2.</text>
                  <doc_id>213</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>As it can be seen, they show similar trends though the raters have different degrees of strictness.</text>
                  <doc_id>214</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.8</text>
                  <doc_id>215</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.7</text>
                  <doc_id>216</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.6</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.5</text>
                  <doc_id>218</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.4</text>
                  <doc_id>219</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.3</text>
                  <doc_id>220</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.2</text>
                  <doc_id>221</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.1</text>
                  <doc_id>222</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0</text>
                  <doc_id>223</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>S-1 S-2 S-3 D-1 D-2 SPG HP1 HP2 HP3</text>
                  <doc_id>224</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>6.2 Paraphrase Rate</title>
            <text>Human evaluation assesses the quality of paraphrases. However, the paraphrase rates cannot be reflected. A paraphrase that is totally transformed from the source sentence and another that is almost unchanged may get the same score. Therefore, we propose two strategies, i.e., PR1 and PR2, to compute the paraphrase rate:
P R1(T ) = 1 &#8722; OL(S, T ) L(S) ; P R2(T ) = ED(S, T ) . L(S)
Here, PR1 is defined based on word overlapping rate, in which OL(S, T ) denotes the number of overlapping words between a paraphrase T and its source sentence S, L(S) denotes the number of words in S. PR2 is defined based on edit distance, in which ED(S, T ) denotes the edit distance between T and S. Obviously, PR1 only measures the percentage of words that are changed from S to T , whereas PR2 further takes word order changes into consideration. It should be noted that PR1 and PR2 not only count the correct changes between S and T , but also count the incorrect ones. We compute the paraphrase rate for each of the six approaches by averaging the paraphrase rates over the whole test set. The results are shown in the left part of Figure 3.
On the whole, the paraphrase rates of the approaches are not high. In particular, we can see that the paraphrase rate of D-2 is clearly higher than D-1, which is in line with our intention of designing D-2. We can also see that the paraphrase rate of S-3 is the highest among the approaches. We find it is mainly because the paraphrases generated with S-3 contain quite a lot of errors, which contribute most of the changes.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Human evaluation assesses the quality of paraphrases.</text>
                  <doc_id>225</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, the paraphrase rates cannot be reflected.</text>
                  <doc_id>226</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>A paraphrase that is totally transformed from the source sentence and another that is almost unchanged may get the same score.</text>
                  <doc_id>227</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, we propose two strategies, i.e., PR1 and PR2, to compute the paraphrase rate:</text>
                  <doc_id>228</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P R1(T ) = 1 &#8722; OL(S, T ) L(S) ; P R2(T ) = ED(S, T ) .</text>
                  <doc_id>229</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>L(S)</text>
                  <doc_id>230</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Here, PR1 is defined based on word overlapping rate, in which OL(S, T ) denotes the number of overlapping words between a paraphrase T and its source sentence S, L(S) denotes the number of words in S. PR2 is defined based on edit distance, in which ED(S, T ) denotes the edit distance between T and S.</text>
                  <doc_id>231</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Obviously, PR1 only measures the percentage of words that are changed from S to T , whereas PR2 further takes word order changes into consideration.</text>
                  <doc_id>232</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It should be noted that PR1 and PR2 not only count the correct changes between S and T , but also count the incorrect ones.</text>
                  <doc_id>233</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We compute the paraphrase rate for each of the six approaches by averaging the paraphrase rates over the whole test set.</text>
                  <doc_id>234</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The results are shown in the left part of Figure 3.</text>
                  <doc_id>235</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>On the whole, the paraphrase rates of the approaches are not high.</text>
                  <doc_id>236</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In particular, we can see that the paraphrase rate of D-2 is clearly higher than D-1, which is in line with our intention of designing D-2.</text>
                  <doc_id>237</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We can also see that the paraphrase rate of S-3 is the highest among the approaches.</text>
                  <doc_id>238</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We find it is mainly because the paraphrases generated with S-3 contain quite a lot of errors, which contribute most of the changes.</text>
                  <doc_id>239</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>6</index>
        <title>7 Analysis</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>240</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>7.1 Effectiveness of the Proposed Method</title>
            <text>Our analysis starts from the candidate paraphrases acquired with the multi-pivot approach. Actually, the results of S-3 reflect the average quality of the candidate paraphrases. A score of 2.78 (See Figure 1) indicates that the candidates are unacceptable according to the human evaluation metrics. This is in line with our expectation that the automatically acquired paraphrases through a two-way translation are noisy. However, the results of S-1 and D-1 demonstrate that, using the selection-based and decoding-based techniques, we can produce paraphrases of good quality. Especially, S-1 gets a score of nearly 4, which suggests that the paraphrases are pretty good according to our metrics. Moreover, our method outperforms SPG built on pre-extracted fine-grained paraphrases. It shows that our method makes good use of the paraphrase knowledge from the large volume of bilingual data underlying the multiple MT engines.
7.2 How to Choose Pivot Languages and MT Engines in the Multi-pivot Approach
In our experiments, besides the six pivot languages used in the multi-pivot system, we have also tried another five pivot languages, including Arabic, Japanese, Korean, Russian, and Dutch. They are finally abandoned since we find that they perform badly. Our experience on choosing pivot languages is that: (1) a pivot language should be a language whose translation quality can be well guaranteed by the MT engines; (2) it is better to choose a pivot language similar to the source language (e.g., French - English), which is easier to translate; (3) the translation quality of a pivot language should not vary a lot among the MT engines. On the other hand, it is better to choose MT engines built on diverse models and corpora, which can provide different paraphrase options. We plan to employ a syntax-based MT engine in our further experiments besides the currently used phrase-based SMT and rule-based MT engines.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Our analysis starts from the candidate paraphrases acquired with the multi-pivot approach.</text>
                  <doc_id>241</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Actually, the results of S-3 reflect the average quality of the candidate paraphrases.</text>
                  <doc_id>242</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>A score of 2.78 (See Figure 1) indicates that the candidates are unacceptable according to the human evaluation metrics.</text>
                  <doc_id>243</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This is in line with our expectation that the automatically acquired paraphrases through a two-way translation are noisy.</text>
                  <doc_id>244</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>However, the results of S-1 and D-1 demonstrate that, using the selection-based and decoding-based techniques, we can produce paraphrases of good quality.</text>
                  <doc_id>245</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Especially, S-1 gets a score of nearly 4, which suggests that the paraphrases are pretty good according to our metrics.</text>
                  <doc_id>246</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Moreover, our method outperforms SPG built on pre-extracted fine-grained paraphrases.</text>
                  <doc_id>247</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>It shows that our method makes good use of the paraphrase knowledge from the large volume of bilingual data underlying the multiple MT engines.</text>
                  <doc_id>248</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>7.2 How to Choose Pivot Languages and MT Engines in the Multi-pivot Approach</text>
                  <doc_id>249</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In our experiments, besides the six pivot languages used in the multi-pivot system, we have also tried another five pivot languages, including Arabic, Japanese, Korean, Russian, and Dutch.</text>
                  <doc_id>250</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>They are finally abandoned since we find that they perform badly.</text>
                  <doc_id>251</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Our experience on choosing pivot languages is that: (1) a pivot language should be a language whose translation quality can be well guaranteed by the MT engines; (2) it is better to choose a pivot language similar to the source language (e.g., French - English), which is easier to translate; (3) the translation quality of a pivot language should not vary a lot among the MT engines.</text>
                  <doc_id>252</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>On the other hand, it is better to choose MT engines built on diverse models and corpora, which can provide different paraphrase options.</text>
                  <doc_id>253</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We plan to employ a syntax-based MT engine in our further experiments besides the currently used phrase-based SMT and rule-based MT engines.</text>
                  <doc_id>254</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>7.3 Comparing the Selection-based and Decoding-based Techniques</title>
            <text>It is necessary to compare the paraphrases generated via the selection-based and decoding-based techniques. As stated above, the selection-based technique can only select a paraphrase from the candidates, while the decoding-based technique can generate a paraphrase different from all candidates. In our experiments, we find that for about 90% test sentences, the paraphrases generated by the decoding-based approach D-1 are outside the candidates. In particular, we compare the paraphrases generated by S-1 and D-1 and find that, for about 40% test sentences, S-1 gets higher scores than D-1, while for another 21% test sentences, D-1 gets higher scores than S-1 8 . This indicates that the selection-based and decodingbased techniques are complementary. In addition, we find examples in which the decoding-based technique can generate a perfect paraphrase for the source sentence, even if all the candidate paraphrases have obvious errors. This also shows that the decoding-based technique is promising.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>It is necessary to compare the paraphrases generated via the selection-based and decoding-based techniques.</text>
                  <doc_id>255</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As stated above, the selection-based technique can only select a paraphrase from the candidates, while the decoding-based technique can generate a paraphrase different from all candidates.</text>
                  <doc_id>256</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In our experiments, we find that for about 90% test sentences, the paraphrases generated by the decoding-based approach D-1 are outside the candidates.</text>
                  <doc_id>257</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In particular, we compare the paraphrases generated by S-1 and D-1 and find that, for about 40% test sentences, S-1 gets higher scores than D-1, while for another 21% test sentences, D-1 gets higher scores than S-1 8 .</text>
                  <doc_id>258</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This indicates that the selection-based and decodingbased techniques are complementary.</text>
                  <doc_id>259</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, we find examples in which the decoding-based technique can generate a perfect paraphrase for the source sentence, even if all the candidate paraphrases have obvious errors.</text>
                  <doc_id>260</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>This also shows that the decoding-based technique is promising.</text>
                  <doc_id>261</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>7.4 Comparing Automatically Generated Paraphrases with Human Paraphrases</title>
            <text>We also analyze the characteristics of the generated paraphrases and compare them with the human paraphrases (i.e., the other 3 reference translations in the MT evaluation, see Section 5, which are denoted as HP1, HP2, and HP3). We find that, compared with the automatically generated paraphrases, the human paraphrases are more com-
8 For the rest 39%, S-1 and D-1 get identical scores.
plicated, which involve not only phrase replacements, but also structure reformulations and even inferences. Their paraphrase rates are also much higher, which can be seen in the right part of Figure 3. We show the automatic and human paraphrases for the example sentence of this paper in Table 6. To narrow the gap between the automatic and human paraphrases, it is necessary to learn structural paraphrase knowledge from the candidates in the future work.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We also analyze the characteristics of the generated paraphrases and compare them with the human paraphrases (i.e., the other 3 reference translations in the MT evaluation, see Section 5, which are denoted as HP1, HP2, and HP3).</text>
                  <doc_id>262</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We find that, compared with the automatically generated paraphrases, the human paraphrases are more com-</text>
                  <doc_id>263</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>8 For the rest 39%, S-1 and D-1 get identical scores.</text>
                  <doc_id>264</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>plicated, which involve not only phrase replacements, but also structure reformulations and even inferences.</text>
                  <doc_id>265</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Their paraphrase rates are also much higher, which can be seen in the right part of Figure 3.</text>
                  <doc_id>266</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We show the automatic and human paraphrases for the example sentence of this paper in Table 6.</text>
                  <doc_id>267</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>To narrow the gap between the automatic and human paraphrases, it is necessary to learn structural paraphrase knowledge from the candidates in the future work.</text>
                  <doc_id>268</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>7</index>
        <title>8 Conclusions and Future Work</title>
        <text>We put forward an effective method for paraphrase generation, which has the following contributions. First, it acquires a rich fund of paraphrase knowledge through the use of multiple MT engines and pivot languages. Second, it presents a MBR-based technique that effectively selects high-quality paraphrases from the noisy candidates. Third, it proposes a decoding-based technique, which can generate paraphrases that are different from the candidates. Experimental results show that the proposed method outperforms a state-of-the-art approach SPG. In the future work, we plan to improve the selection-based and decoding-based techniques. We will try some standard system combination strategies, like confusion networks and consensus decoding. In addition, we will refine our evaluation metrics. In the current experiments, paraphrase correctness (adequacy and fluency) and paraphrase rate are evaluated separately, which seem to be incompatible. We plan to combine them together and propose a uniform metric.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We put forward an effective method for paraphrase generation, which has the following contributions.</text>
              <doc_id>269</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>First, it acquires a rich fund of paraphrase knowledge through the use of multiple MT engines and pivot languages.</text>
              <doc_id>270</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Second, it presents a MBR-based technique that effectively selects high-quality paraphrases from the noisy candidates.</text>
              <doc_id>271</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Third, it proposes a decoding-based technique, which can generate paraphrases that are different from the candidates.</text>
              <doc_id>272</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Experimental results show that the proposed method outperforms a state-of-the-art approach SPG.</text>
              <doc_id>273</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In the future work, we plan to improve the selection-based and decoding-based techniques.</text>
              <doc_id>274</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We will try some standard system combination strategies, like confusion networks and consensus decoding.</text>
              <doc_id>275</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In addition, we will refine our evaluation metrics.</text>
              <doc_id>276</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>In the current experiments, paraphrase correctness (adequacy and fluency) and paraphrase rate are evaluated separately, which seem to be incompatible.</text>
              <doc_id>277</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>We plan to combine them together and propose a uniform metric.</text>
              <doc_id>278</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 3: Examples of candidate paraphrases obtained using the multi-pivot approach.#@#@Table 1: Pivot languages used in the approach.</caption>
        <reference_text>In PAGE 3: ...www.systransoft.com) Table 2: MT engines utilized in the approach. 4 Producing High-quality Paraphrases using the Candidates  Table3  shows some examples of candidate para- phrases for a sentence. As can be seen, the can- didates do provide some correct and useful para- phrase substitutes (in bold) for the source sen- tence....</reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>1</cell>
              <cell>French (F)</cell>
              <cell>4</cell>
              <cell>Italian (I)</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>2</cell>
              <cell>German (G)</cell>
              <cell>5</cell>
              <cell>Portuguese (P)</cell>
            </row>
            <row>
              <cell>3</cell>
              <cell>Spanish (S)</cell>
              <cell>6</cell>
              <cell>Chinese (C)</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: MT engines utilized in the approach.</caption>
        <reference_text>In PAGE 2: ... Given m pivot languages and n MT en- gines, we can build a multi-pivot PG system con- sisting of N (N ? n ? m ? n) single-pivot ones, where N = n ? m ? n iff all the n MT engines can perform bidirectional translation between the source and each pivot language. In this work, we experiment with 6 pivot lan- guages (Table 1) and 3 MT engines ( Table2 ) in the multi-pivot approach. All the 3 MT engines are off-the-shelf systems, in which Google and Microsoft translators are SMT engines, while Sys- tran translator is a rule-based MT engine....</reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>1</cell>
              <cell>Google Translate (GG)</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>(translate.google.com)</cell>
            </row>
            <row>
              <cell>2</cell>
              <cell>Microsoft Translator (MS)</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>(www.microsofttranslator.com)</cell>
            </row>
            <row>
              <cell>3</cell>
              <cell>Systran Online Translation (ST)</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>(www.systransoft.com)</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TET</source>
        <caption>Table 3: Examples of candidate paraphrases obtained using the multi-pivot approach.</caption>
        <reference_text></reference_text>
        <page_num>2</page_num>
        <head>
          <rows>
            <row>
              <cell>Source Sentence</cell>
              <cell>he said there will be major cuts in the salaries of high-level civil servants .</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>(GG, G, MS)</cell>
              <cell>he said there are significant cuts in the salaries of high-level officials .</cell>
            </row>
            <row>
              <cell>(GG, F , GG)</cell>
              <cell>he said there will be significant cuts in the salaries of top civil level .</cell>
            </row>
            <row>
              <cell>(MS, C, MS)</cell>
              <cell>he said that there will be a major senior civil service pay cut .</cell>
            </row>
            <row>
              <cell>(MS, F , ST )</cell>
              <cell>he said there will be great cuts in the wages of the high level civils servant .</cell>
            </row>
            <row>
              <cell>(ST , G, GG)</cell>
              <cell>he said that there are major cuts in the salaries of senior government officials .</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TET</source>
        <caption>Table 5: Five point scale for human evaluation.</caption>
        <reference_text></reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>Score</cell>
              <cell>Adequacy</cell>
              <cell>Fluency</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>5</cell>
              <cell>All</cell>
              <cell>Flawless English</cell>
            </row>
            <row>
              <cell>4</cell>
              <cell>Most</cell>
              <cell>Good English</cell>
            </row>
            <row>
              <cell>3</cell>
              <cell>Much</cell>
              <cell>Non-native English</cell>
            </row>
            <row>
              <cell>2</cell>
              <cell>Little</cell>
              <cell>Disfluent English</cell>
            </row>
            <row>
              <cell>1</cell>
              <cell>None</cell>
              <cell>Incomprehensible</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>6</id>
        <source>TET</source>
        <caption>Table 6: Comparing the automatically generated paraphrases with the human paraphrases.</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>S</cell>
              <cell>he said there will be major cuts in the salaries of high-level civil servants .</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>S-1</cell>
              <cell>he said that there will be significant cuts in the salaries of senior officials .</cell>
            </row>
            <row>
              <cell>S-2</cell>
              <cell>he said there will be major cuts in salaries of civil servants high level .</cell>
            </row>
            <row>
              <cell>S-3</cell>
              <cell>he said that there will be significant cuts in the salaries of senior officials .</cell>
            </row>
            <row>
              <cell>D-1</cell>
              <cell>he said , there will be significant cuts in salaries of senior civil servants .</cell>
            </row>
            <row>
              <cell>D-2</cell>
              <cell>he said , there will be significant cuts in salaries of senior officials .</cell>
            </row>
            <row>
              <cell>SPG</cell>
              <cell>he said that there will be the main cuts in the wages of high-level civil servants .</cell>
            </row>
            <row>
              <cell>HP1</cell>
              <cell>he said there will be a big salary cut for high-level government employees .</cell>
            </row>
            <row>
              <cell>HP2</cell>
              <cell>he said salaries of senior public servants would be slashed .</cell>
            </row>
            <row>
              <cell>HP3</cell>
              <cell>he claimed to implement huge salary cut to senior civil servants .</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Colin Bannard</author>
          <author>Chris Callison-Burch</author>
        </authors>
        <title>Paraphrasing with Bilingual Parallel Corpora.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>597--604</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Regina Barzilay</author>
          <author>Lillian Lee</author>
        </authors>
        <title>Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment.</title>
        <publication>In Proceedings of HLT-NAACL,</publication>
        <pages>16--23</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Peter J Bickel</author>
          <author>Kjell A Doksum</author>
        </authors>
        <title>Mathematical Statistics: Basic Ideas and Selected Topics.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1977</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Igor A Bolshakov</author>
          <author>Alexander Gelbukh</author>
        </authors>
        <title>Synonymous Paraphrasing Using WordNet and Internet.</title>
        <publication>In Proceedings of NLDB,</publication>
        <pages>312--323</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Chris Callison-Burch</author>
          <author>Cameron Fordyce</author>
          <author>Philipp Koehn</author>
          <author>Christof Monz</author>
          <author>Josh Schroeder</author>
        </authors>
        <title>Evaluation of Machine Translation.</title>
        <publication>In Proceedings of ACL-2007 Workshop on Statistical Machine Translation,</publication>
        <pages>136--158</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Chris Callison-Burch</author>
          <author>Philipp Koehn</author>
          <author>Miles Osborne</author>
        </authors>
        <title>Improved Statistical Machine Translation Using Paraphrases.</title>
        <publication>In Proceedings of HLTNAACL,</publication>
        <pages>17--24</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Pablo Ariel Duboue</author>
          <author>Jennifer Chu-Carroll</author>
        </authors>
        <title>Answering the Question You Wish They Had Asked: The Impact of Paraphrasing for Question Answering.</title>
        <publication>In Proceedings of HLT-NAACL,</publication>
        <pages>33--36</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Hieu Hoang</author>
          <author>Philipp Koehn</author>
        </authors>
        <title>Design of the Moses Decoder for Statistical Machine Translation.</title>
        <publication>In Proceedings of ACL Workshop on Software engineering, testing, and quality assurance for NLP,</publication>
        <pages>58--65</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Lidija Iordanskaja</author>
          <author>Richard Kittredge</author>
          <author>Alain Polgu&#232;re</author>
        </authors>
        <title>Lexical Selection and Paraphrase in a Meaning-Text Generation Model. In C&#233;cile</title>
        <publication>Mann (Eds.): Natural Language Generation in Artificial Intelligence and Computational Linguistics,</publication>
        <pages>293--312</pages>
        <date>1991</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>David Kauchak</author>
          <author>Regina Barzilay</author>
        </authors>
        <title>Paraphrasing for Automatic Evaluation.</title>
        <publication>In Proceedings of HLT-NAACL,</publication>
        <pages>455--462</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Philipp Koehn</author>
        </authors>
        <title>Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models: User Manual and Description for Version 1.2. Philipp Koehn,</title>
        <publication>In Proceedings of HLT-NAACL,</publication>
        <pages>127--133</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Vijay-Shanker</author>
        </authors>
        <title>Generation of singlesentence paraphrases from predicate/argument structure using lexico-grammatical resources.</title>
        <publication>In Proceedings of IWP,</publication>
        <pages>1--8</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Aur&#233;lien Max</author>
        </authors>
        <title>Sub-sentential Paraphrasing by Contextual Pivot Translation.</title>
        <publication>In Proceedings of the 2009 Workshop on Applied Textual Inference, ACLIJCNLP</publication>
        <pages>18--26</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Improved Statistical Alignment Models.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>440--447</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Bo Pang</author>
          <author>Kevin Knight</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Syntax-based Alignment of Multiple Translations: Extracting Paraphrases and Generating New Sentences.</title>
        <publication>In Proceedings of HLT-NAACL,</publication>
        <pages>102--109</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>Wei-Jing Zhu</author>
        </authors>
        <title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>311--318</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Richard Power</author>
          <author>Donia Scott</author>
        </authors>
        <title>Automatic generation of large-scale paraphrases.</title>
        <publication>In Proceedings of IWP,</publication>
        <pages>73--79</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Chris Quirk</author>
          <author>Chris Brockett</author>
          <author>William Dolan</author>
        </authors>
        <title>Monolingual Machine Translation for Paraphrase Generation.</title>
        <publication>In Proceedings of EMNLP,</publication>
        <pages>142--149</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Stefan Riezler</author>
        </authors>
        <title>Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>464--471</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Yujie Zhang</author>
          <author>Kazuhide Yamamoto</author>
        </authors>
        <title>Paraphrasing of Chinese Utterances.</title>
        <publication>In Proceedings of COLING,</publication>
        <pages>1163--1169</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Shiqi Zhao</author>
          <author>Xiang Lan</author>
          <author>Ting Liu</author>
          <author>Sheng Li</author>
        </authors>
        <title>Application-driven Statistical Paraphrase Generation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>21</id>
        <authors/>
        <title>None</title>
        <publication>In Proceedings of ACL-IJCNLP</publication>
        <pages>834--842</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Shiqi Zhao</author>
          <author>Cheng Niu</author>
          <author>Ming Zhou</author>
          <author>Ting Liu</author>
          <author>Sheng Li</author>
        </authors>
        <title>Combining Multiple Resources to Improve SMT-based Paraphrasing Model.</title>
        <publication>In Proceedings of ACL-08:HLT,</publication>
        <pages>1021--1029</pages>
        <date>2008</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Bannard and Callison-Burch (2005)</string>
        <sentence_id>1861</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Barzilay and Lee, 2003</string>
        <sentence_id>1860</sentence_id>
        <char_offset>147</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Bickel and Doksum, 1977</string>
        <sentence_id>1900</sentence_id>
        <char_offset>14</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>3</reference_id>
        <string>Bolshakov and Gelbukh, 2004</string>
        <sentence_id>1860</sentence_id>
        <char_offset>216</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>4</reference_id>
        <string>Callison-Burch et al., 2007</string>
        <sentence_id>1989</sentence_id>
        <char_offset>81</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>5</reference_id>
        <string>Callison-Burch et al., 2006</string>
        <sentence_id>1841</sentence_id>
        <char_offset>134</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>6</reference_id>
        <string>Duboue and Chu-Carroll, 2006</string>
        <sentence_id>1842</sentence_id>
        <char_offset>104</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>7</reference_id>
        <string>Hoang and Koehn, 2008</string>
        <sentence_id>1949</sentence_id>
        <char_offset>100</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>8</reference_id>
        <string>Iordanskaja et al., 1991</string>
        <sentence_id>1843</sentence_id>
        <char_offset>113</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>9</reference_id>
        <string>Kauchak and Barzilay, 2006</string>
        <sentence_id>1860</sentence_id>
        <char_offset>245</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>10</reference_id>
        <string>Koehn, 2004</string>
        <sentence_id>1951</sentence_id>
        <char_offset>101</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>12</reference_id>
        <string>Max (2009)</string>
        <sentence_id>1868</sentence_id>
        <char_offset>13</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>13</reference_id>
        <string>Och and Ney, 2000</string>
        <sentence_id>1949</sentence_id>
        <char_offset>70</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>14</reference_id>
        <string>Pang et al., 2003</string>
        <sentence_id>1860</sentence_id>
        <char_offset>171</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>15</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>1922</sentence_id>
        <char_offset>64</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>16</reference_id>
        <string>Power and Scott, 2005</string>
        <sentence_id>1860</sentence_id>
        <char_offset>321</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>17</reference_id>
        <string>Quirk et al. (2004)</string>
        <sentence_id>1852</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>17</reference_id>
        <string>Quirk et al., 2004</string>
        <sentence_id>1980</sentence_id>
        <char_offset>1</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>19</reference_id>
        <string>Zhang and Yamamoto, 2002</string>
        <sentence_id>1841</sentence_id>
        <char_offset>108</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>20</reference_id>
        <string>Zhao et al., 2009</string>
        <sentence_id>1849</sentence_id>
        <char_offset>470</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>20</reference_id>
        <string>Zhao et al., 2009</string>
        <sentence_id>1859</sentence_id>
        <char_offset>121</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>20</reference_id>
        <string>Zhao et al., 2009</string>
        <sentence_id>1980</sentence_id>
        <char_offset>41</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>20</reference_id>
        <string>Zhao et al., 2009</string>
        <sentence_id>1993</sentence_id>
        <char_offset>61</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>20</reference_id>
        <string>Zhao et al., 2009</string>
        <sentence_id>2054</sentence_id>
        <char_offset>4</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>21</reference_id>
        <string>(2009)</string>
        <sentence_id>1857</sentence_id>
        <char_offset>12</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>21</reference_id>
        <string>(2009)</string>
        <sentence_id>1868</sentence_id>
        <char_offset>17</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>22</reference_id>
        <string>Zhao et al. (2008</string>
        <sentence_id>1856</sentence_id>
        <char_offset>41</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>22</reference_id>
        <string>Zhao et al. (2008</string>
        <sentence_id>1863</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>22</reference_id>
        <string>Zhao et al., 2008</string>
        <sentence_id>1859</sentence_id>
        <char_offset>101</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>22</reference_id>
        <string>Zhao et al., 2008</string>
        <sentence_id>1980</sentence_id>
        <char_offset>21</char_offset>
      </citation>
    </citations>
  </content>
</document>
