<PAPER>
  <FILENO/>
  <TITLE>Improving Alignment of System Combination by Using Multi-objective Optimization</TITLE>
  <AUTHORS/>
  <ABSTRACT>
    <A-S ID="S-14899">This paper proposes a multi-objective optimization framework which supports heterogeneous information sources to improve alignment in machine translation system combination techniques.</A-S>
    <A-S ID="S-14900">In this area, most of techniques usually utilize confusion networks (CN) as their central data structure to compact an exponential number of an potential hypotheses, and because better hypothesis alignment may benefit constructing better quality confusion networks, it is natural to add more useful information to improve alignment results.</A-S>
    <A-S ID="S-14901">However, these information may be heterogeneous, so the widely-used Viterbi algorithm for searching the best alignment may not apply here.</A-S>
    <A-S ID="S-14902">In the multi-objective optimization framework, each information source is viewed as an independent objective, and a new goal of improving all objectives can be searched by mature algorithms.</A-S>
    <A-S ID="S-14903">The solutions from this framework, termed Pareto optimal solutions, are then combined to construct confusion networks.</A-S>
    <A-S ID="S-14904">Experiments on two Chinese-to-English translation datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-14905">System combination (SC) techniques have the power of boosting translation quality in BLEU by several percent over the best among all input machine translation systems (<REF ID="R-00" RPTR="0">Bangalore et al., 2001</REF>; <REF ID="R-21" RPTR="42">Matusov et al., 2006</REF>; <REF ID="R-30" RPTR="61">Sim et al., 2007</REF>; <REF ID="R-26" RPTR="47">Rosti et al., 2007</REF><REF ID="R-27" RPTR="52">Rosti et al., 2007</REF>b; <REF ID="R-26" RPTR="48">Rosti et al., 2007</REF><REF ID="R-27" RPTR="53">Rosti et al., 2007</REF>a; <REF ID="R-14" RPTR="31">Huang and Papineni, 2007</REF>; <REF ID="R-12" RPTR="20">He et al., 2008</REF>; <REF ID="R-28" RPTR="57">Rosti et al., 2008</REF>; <REF ID="R-11" RPTR="19">He and Toutanova, 2009</REF>; <REF ID="R-19" RPTR="37">Li et al., 2009</REF>; <REF ID="R-08" RPTR="11">Feng et al., 2009</REF>; <REF ID="R-25" RPTR="46">Pauls et al., 2009</REF>).</S>
        <S ID="S-14906">A central data structure in the SC is the confusion network, and its quality greatly affects the final performance.</S>
        <S ID="S-14907">He et al. (2008) proposed a new hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm (<REF ID="R-26" RPTR="49">Rosti et al., 2007</REF><REF ID="R-27" RPTR="54">Rosti et al., 2007</REF>b; <REF ID="R-28" RPTR="58">Rosti et al., 2008</REF>; <REF ID="R-30" RPTR="62">Sim et al., 2007</REF>).</S>
        <S ID="S-14908">Now, current state-of-the-art SC systems have been using IHMM or variants in their alignment algorithms more or less (<REF ID="R-19" RPTR="38">Li et al., 2009</REF>; <REF ID="R-08" RPTR="12">Feng et al., 2009</REF>).</S>
      </P>
      <P>
        <S ID="S-14909">Our motivation derives from an observation that in an ideal alignment of a pair of sentences, many-tomany alignments often exist.</S>
        <S ID="S-14910">For instance, &#8220;be about to&#8221; has the same meaning with &#8220;be on the point of&#8221;.</S>
        <S ID="S-14911">Because Hidden Markov Model based alignment algorithms, e.g. IHMM for system combination, HMM in GIZA++ software for statistical machine translation (SMT) (<REF ID="R-23" RPTR="43">Och and Ney, 2000</REF>; <REF ID="R-15" RPTR="32">Koehn et al., 2003</REF>), are designed for one-to-many alignment, and running GIZA++ from two directions to gain better performance turns into a standard operation in SMT, therefore we are seeking a way to empower IHMM by introducing bi-directional information.</S>
      </P>
      <P>
        <S ID="S-14912">However, it appears to be intractable in an IHMM model to search the optimal solution by simply defining a new goal as a product of probabilities</S>
      </P>
      <P>
        <S ID="S-14913">from two directions.</S>
        <S ID="S-14914">To bypass this problem, <REF ID="R-20" RPTR="40">Liang et al. (2006)</REF> adopts a simple and effective variational inference algorithm.</S>
        <S ID="S-14915">Further, different alignment algorithms capture different information and linguistic phenomena for a pair of sentences, hence more information would be expected to benefit the final alignment.</S>
        <S ID="S-14916">Liang&#8217;s method may not be suitable for this expected outcome.</S>
        <S ID="S-14917">We propose to adopt multi-objective optimization framework to support heterogeneous information sources which may induce difficulties in a conventional search algorithm.</S>
        <S ID="S-14918">In this framework, there exist a variety of matured multi-objective optimization algorithms, e.g. evolutionary algorithm (<REF ID="R-02" RPTR="3">Deb et al., 2000</REF>; <REF ID="R-03" RPTR="5">Deb et al., 2002</REF>), Tabu search (<REF ID="R-10" RPTR="16">Hansen, 1997</REF>), ants colony (<REF ID="R-07" RPTR="9">Engelbrecht, 2005</REF>), and simulated annealing (<REF ID="R-29" RPTR="60">Serafini, 1994</REF>).</S>
        <S ID="S-14919">In this work, we select the multi-objective evolutionary algorithm because of its public open source software (http://www.iitk.ac.in/kangal/codes.shtml).</S>
        <S ID="S-14920">On the other hand, this framework is also totally unsupervised.</S>
        <S ID="S-14921">It prevents weights of a linearly combined goal from training even if all information is homogeneous and applicable in a Viterbi search (Forney <REF ID="R-09" RPTR="15">Jr, 1973</REF>).</S>
        <S ID="S-14922">This framework views any useful information benefiting alignment as an independent objective, and researchers just need to write short codes for objective definitions.</S>
        <S ID="S-14923">The search algorithm seeks for potentially better solutions which are no worse than the current solution set.</S>
        <S ID="S-14924">The output from multiobjective optimization algorithms includes a set of solutions, called Pareto optimal solutions, each one being a many-to-many alignment.</S>
        <S ID="S-14925">We then combine and normalize them into a unique one-to-one alignment to perform confusion network construction (Section 3.3).</S>
        <S ID="S-14926">Our work is conducted on the classic pipeline</S>
      </P>
      <P>
        <S ID="S-14927">which has three modules, pair-wise hypothesis alignment, confusion network construction, and training.</S>
        <S ID="S-14928">Now many work integrates neighboring modules to avoid propagated errors to gain improved performance.</S>
        <S ID="S-14929">For example, <REF ID="R-28" RPTR="59">Rosti et al. (2008)</REF>, and <REF ID="R-19" RPTR="36">Li et al. (2009)</REF> combine the first and the second module, and <REF ID="R-11" RPTR="17">He and Toutanova (2009)</REF> combine all modules into one directly.</S>
        <S ID="S-14930">Nevertheless, the classic structure also owns its merits.</S>
        <S ID="S-14931">Because of the independence between modules, a system is relatively</S>
      </P>
      <P>
        <S ID="S-14932">simple to maintain, and improvements on each module might contribute to final performance additively.</S>
        <S ID="S-14933">Based on our work, lattice-based minimum error rate training (lattice-MERT) and minimum bayes risk training techniques (<REF ID="R-17" RPTR="35">Kumar et al., 2009</REF>) could be adopted on the third module.</S>
        <S ID="S-14934">And <REF ID="R-08" RPTR="10">Feng et al. (2009)</REF> in the second module adopts a different data structure called lattice which could directly use our better many-to-many alignment for construction.</S>
        <S ID="S-14935">Experiments on the Chinese-to-English task on two datasets use four objectives, IHMM probability (Section 3.2.1), and alignment probability from GIZA++ (Section 3.2.2) from two directions.</S>
        <S ID="S-14936">Results show multi-objective optimization framework efficiently integrates different information to gain approximately 1 BLEU point improvement over a strong baseline.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Background</HEADER>
      <P>
        <S ID="S-14991">We briefly give an introduction to confusion networks, and because the IHMM based alignment is an important objective in our multi-objective framework, here we also provide detailed definition of formulas for completeness of content.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>2.1 Confusion Network</HEADER>
        <P>
          <S ID="S-14937">Table 1 shows hypotheses h 1 and h 2 are aligned to selected backbone h 0 .</S>
          <S ID="S-14938">When alignment algorithm obtains good enough results, the expected output &#8220;he prefers apples&#8221; is included in its corresponding confusion network in Figure 1.</S>
          <S ID="S-14939">This suggests developing better alignment algorithm may help creating high-quality confusion networks.</S>
          <S ID="S-14940">This also motivates us to use the BLEU of oracle hypotheses to approximately measure the quality of a set of CNs.</S>
          <S ID="S-14941">We hereafter call it an oracle BLEU of a CN.</S>
          <S ID="S-14942">See more in Section 5.1.</S>
        </P>
        <P>
          <S ID="S-14943">A confusion network G = (V, E) is a directed acyclic graph with a unique source and sink vertex,</S>
        </P>
        <P>
          <S ID="S-14944">&#8226;</S>
        </P>
        <P>
          <S ID="S-14945">formally a weighted finite state automation (FSA), where V is the set of nodes and E is the set of edges.</S>
          <S ID="S-14946">Each edge is restricted to attach to a single word as well as an associated probability.</S>
          <S ID="S-14947">A special mark &#949; is a place-holder denoting no word here.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.2 IHMM-based Alignment</HEADER>
        <P>
          <S ID="S-14948">Indirected Hidden Markov Model (IHMM) was firstly proposed by He et.</S>
          <S ID="S-14949">al (2008).</S>
          <S ID="S-14950">Compared with TER-based alignment performing literal matching, IHMM supports synonym comparison in redefining emission probabilities in an IHMM model.</S>
        </P>
        <P>
          <S ID="S-14951">Let f I = (f 1 , .</S>
          <S ID="S-14952">.</S>
          <S ID="S-14953">.</S>
          <S ID="S-14954">f I ) be a backbone hypothesis, and e J = (e 1 , .</S>
          <S ID="S-14955">.</S>
          <S ID="S-14956">.</S>
          <S ID="S-14957">e J ) be a hypothesis aligned to the backbone, both being English sentences in our experiments.</S>
          <S ID="S-14958">Let a J = {a 1 , .</S>
          <S ID="S-14959">.</S>
          <S ID="S-14960">.</S>
          <S ID="S-14961">a j } be an alignment.</S>
          <S ID="S-14962">Suppose the a j th word in f I is aligned to jth word in e J , and the conditional probability that the hypothesis is generated by the backbone, shown in the upper graph of Figure 3, is given by</S>
        </P>
        <P>
          <S ID="S-14963">J &#8719;</S>
        </P>
        <P>
          <S ID="S-14964">p(f I , e J ) = &#8721; {p t (a j |a j&#8722;1 , I)p o (e j |f aj )}</S>
        </P>
        <P>
          <S ID="S-14965">a J j=1</S>
        </P>
        <P>
          <S ID="S-14966">(1)</S>
        </P>
        <P>
          <S ID="S-14967">The distortion probability p t (a j |a j&#8722;1 , I) from position a j&#8722;1 to a j , relies on jumped distance, which is computed as follows:</S>
        </P>
        <P>
          <S ID="S-14968">p t (i &#8242; |i, I) = c(i&#8242; &#8722; i) &#8721; I</S>
        </P>
        <P>
          <S ID="S-14969">t=1 c(t &#8722; i) (2)</S>
        </P>
        <P>
          <S ID="S-14970">The distortion parameters c(d) are grouped into 11 buckets, c(&#8804; &#8722;4),c(&#8722;3),c(&#8722;2).</S>
          <S ID="S-14971">.</S>
          <S ID="S-14972">.c(5),c(&#8805; 6).</S>
          <S ID="S-14973">Because all the hypotheses in system combination are in the same language, the IHMM model would support more monotonic alignments, and non-monotonic alignments will be penalized.</S>
        </P>
        <P>
          <S ID="S-14974">c(d) = (1 + |d &#8722; 1|) &#8722;K , d = &#8722;4 .</S>
          <S ID="S-14975">.</S>
          <S ID="S-14976">.</S>
          <S ID="S-14977">6 (3)</S>
        </P>
        <P>
          <S ID="S-14978">where K is tuned on held-out data.</S>
        </P>
        <P>
          <S ID="S-14979">Let p 0 be the probability of jumping to a null word state, which is also tuned on held-out data, and the accurate transition probability becomes:</S>
        </P>
        <P>
          <S ID="S-14980">{</S>
        </P>
        <P>
          <S ID="S-14981">p t (i &#8242; p 0 if i &#8242; = null |i, I) = (1 &#8722; p 0 )p t (i &#8242; |i, I) otherwise (4)</S>
        </P>
        <P>
          <S ID="S-14982">The output probability p o (e|f) from the state word f to the observation word e, also called translation probability, is a linear interpolation of semantic similarity p sem (e|f) and surface similarity p sur (e|f), and &#945; is the interpolation factor:</S>
        </P>
        <P>
          <S ID="S-14983">p o (e|f) = &#945;p sem (e|f) + (1 &#8722; &#945;)p sur (e|f) (5)</S>
        </P>
        <P>
          <S ID="S-14984">When calculating semantic similarity p sem (e|f), source sentence src is needed, and a bilingual probabilistic dictionary p dic (w 1 |w 2 ) is necessary.</S>
        </P>
        <P>
          <S ID="S-14985">p sem (e|f) &#8776; &#8721; p dic (c|f) &#183; p dic (e|c) (6)</S>
        </P>
        <P>
          <S ID="S-14986">c&#8712;src</S>
        </P>
        <P>
          <S ID="S-14987">Note that p sem (e|f) has been updated with different source sentences.</S>
        </P>
        <P>
          <S ID="S-14988">The surface similarity p sur (e|f) is measured by the literal matching rate:</S>
        </P>
        <P>
          <S ID="S-14989">p sur LMP(f, e) (e, f) = exp{&#961;[ &#8722; 1]} (7) max(|f|, |e|)</S>
        </P>
        <P>
          <S ID="S-14990">where LMP(f, e) is the length of the longest matched prefix, and &#961; is a smoothing parameter.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Multi-objective Optimization</HEADER>
      <P>
        <S ID="S-15110">Many decision making problems in the real world consider more than one objective.</S>
        <S ID="S-15111">One natural way is to scalarize multiple objectives into one by assigning it with a weight vector.</S>
        <S ID="S-15112">This method allows a simple optimization algorithm in many cases, while in system combination, it would cause problems.</S>
      </P>
      <P>
        <S ID="S-15113">In the first module, in order to train suitable weights of objectives, extra labeled data is needed, besides that, the efficient Viterbi algorithm for searching the optimal alignment would not work for</S>
      </P>
      <P>
        <S ID="S-15114">&#8226;</S>
      </P>
      <P>
        <S ID="S-15115">&#8226;</S>
      </P>
      <P>
        <S ID="S-15116">&#8226;</S>
      </P>
      <P>
        <S ID="S-15117">the alignment objectives in this work.</S>
        <S ID="S-15118">More, the parameter training in the third module relies on the CNs constructed from the output of the first module, which increases the instability of the whole system.</S>
        <S ID="S-15119">Therefore, an unsupervised multi-objective algorithm may be a good choice allowing for more alignment information.</S>
        <S ID="S-15120">There exist other alternative optimization algorithms in the multi-objective optimization framework, though the evolutionary algorithm is adopted here, we only introduce some general concepts.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>3.1 Pareto Optimal Solutions</HEADER>
        <P>
          <S ID="S-14992">A general multi-objective optimization problem consists of a number of objectives and is associated with a number of constraints.</S>
          <S ID="S-14993">Mathematically, the problem can be written as follows (<REF ID="R-04" RPTR="7">Deb, 2001</REF>)</S>
        </P>
        <P>
          <S ID="S-14994">Maximize f i (x) i = 1 .</S>
          <S ID="S-14995">.</S>
          <S ID="S-14996">.</S>
          <S ID="S-14997">M</S>
        </P>
        <P>
          <S ID="S-14998">s.t.</S>
          <S ID="S-14999">g j (x) &#8804; 0 j = 1 .</S>
          <S ID="S-15000">.</S>
          <S ID="S-15001">.</S>
          <S ID="S-15002">N</S>
        </P>
        <P>
          <S ID="S-15003">h k (x) = 0 k = 1 .</S>
          <S ID="S-15004">.</S>
          <S ID="S-15005">.</S>
          <S ID="S-15006">K</S>
        </P>
        <P>
          <S ID="S-15007">where x denotes a potential solution, its structure relying on different problems, and the number of constraints M, N, K depend on different problems.</S>
          <S ID="S-15008">All the functions f i , g j , h k map a solution x into a scalar.</S>
          <S ID="S-15009">We will explain them in terms of system combination.</S>
        </P>
        <P>
          <S ID="S-15010">In this work, we refer to x = {x i,j |x i,j &#8712; {0, 1}} as a potential alignment of a pair of hypotheses, where x i,j is a boolean value to denote whether the ith word in the first hypothesis is aligned to the jth word in the second hypothesis.</S>
          <S ID="S-15011">Here the definition of x seems different from that of a in Formula 1, and they could convert to each other.</S>
          <S ID="S-15012">Using a line-based access style, a matrix can be unfolded as a vector.</S>
          <S ID="S-15013">We refer to f as IHMM alignment probability (<REF ID="R-12" RPTR="21">He et al., 2008</REF>) and GIZA++ alignment probability (<REF ID="R-01" RPTR="1">Chen et al., 2009</REF>), total four objectives from two directions, and the larger the objectives, the better.</S>
          <S ID="S-15014">The g j s and h k s serve as the role of checking if x represents a legal alignment.</S>
          <S ID="S-15015">For instance, the subscripts of x i,j are not in bounds.</S>
        </P>
        <P>
          <S ID="S-15016">Definition 1.</S>
          <S ID="S-15017">Let x, x &#8242; be two potential alignments.</S>
          <S ID="S-15018">If f i (x) &#8805; f i (x &#8242; ) holds for all i, we call the alignment x dominates the alignment x &#8242; .</S>
          <S ID="S-15019">If there</S>
        </P>
        <P>
          <S ID="S-15020">Y: Direct IHMM Probability (1e-8) 5</S>
        </P>
        <P>
          <S ID="S-15021">p1</S>
        </P>
        <P>
          <S ID="S-15022">&#8853; p2</S>
        </P>
        <P>
          <S ID="S-15023">&#8853; p4</S>
        </P>
        <P>
          <S ID="S-15024">p3</S>
        </P>
        <P>
          <S ID="S-15025">&#8853; p6</S>
        </P>
        <P>
          <S ID="S-15026">p5</S>
        </P>
        <P>
          <S ID="S-15027">&#8226; p7</S>
        </P>
        <P>
          <S ID="S-15028">0 0 1 2 3 4 5</S>
        </P>
        <P>
          <S ID="S-15029">X: Reversed IHMM Probability (1e-8)</S>
        </P>
        <P>
          <S ID="S-15030">does not exist any alignment x&#8242;&#8242; to dominate x, we call the alignment x to be non-dominated.</S>
        </P>
        <P>
          <S ID="S-15031">Definition 2.</S>
          <S ID="S-15032">A alignment x is said to be Pareto optimal if there is no other alignment x &#8242; found to dominate x.</S>
        </P>
        <P>
          <S ID="S-15033">In Figure 2, p 1 dominates p 2 , and p 2 dominates p 4 .</S>
          <S ID="S-15034">To summarize, a point is dominated by the ones on its upper and right side with ties.</S>
          <S ID="S-15035">In this example, p 1 , p 3 , p 5 , p 7 are Pareto optimal.</S>
          <S ID="S-15036">In some cases, Pareto optimal solutions can be used for good candidate solutions.</S>
          <S ID="S-15037">Considering the IHMM model, maximizing Y axis, the top-4 best alignments are p 1 , p 2 , p 3 , p 4 .</S>
          <S ID="S-15038">But from the view of Pareto optimal, the top-4 alignments would be p 1 , p 3 , p 5 , p 7 without order, which considers a greater range than a single optimization model.</S>
          <S ID="S-15039">In our method, we just combine these Pareto optimal solutions equally into a unique alignment (Section 3.3).</S>
        </P>
        <P>
          <S ID="S-15040">Our adopted multi-objective optimization searching algorithm is the non-dominated sorting genetic algorithm II (NSGA-II) (<REF ID="R-02" RPTR="4">Deb et al., 2000</REF>; <REF ID="R-03" RPTR="6">Deb et al., 2002</REF>) with an open source software (http://www.iitk.ac.in/kangal/codes.shtml).</S>
          <S ID="S-15041">NSGA- II has a complexity of O(mn 2 ), where m is the number of objectives and n is the population size in an evolutionary algorithm.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.2 Objectives in Evolutionary Algorithm</HEADER>
        <P>
          <S ID="S-15042">The optimization objectives in our experiments can be categorized as an IHMM alignment probability (<REF ID="R-12" RPTR="22">He et al., 2008</REF>) and GIZA++ alignment probability</S>
        </P>
        <P>
          <S ID="S-15043">S:</S>
        </P>
        <P>
          <S ID="S-15044">O:</S>
        </P>
        <P>
          <S ID="S-15045">S:</S>
        </P>
        <P>
          <S ID="S-15046">O: f 1 f 2 f 3</S>
        </P>
        <P>
          <S ID="S-15047">&#8226; &#8226; &#8226;</S>
        </P>
        <P>
          <S ID="S-15048">&#8226; &#8226; &#8226; e 1 e 2 e 3</S>
        </P>
        <P>
          <S ID="S-15049">e 1 e 2 e 3</S>
        </P>
        <P>
          <S ID="S-15050">&#8226; &#8226; &#8226;</S>
        </P>
        <P>
          <S ID="S-15051">Backbone</S>
        </P>
        <P>
          <S ID="S-15052">(<REF ID="R-01" RPTR="2">Chen et al., 2009</REF>), total four from two directions.</S>
        </P>
        <P>
          <S ID="S-15053">3.2.1 IHMM Probability</S>
        </P>
        <P>
          <S ID="S-15054">A typical IHMM alignment is demonstrated in the upper graph of Figure 3, where a backbone is acting the role of a status sequence.</S>
          <S ID="S-15055">The unnormalized conditional alignment probability is [p t (1|null)] &#183; [p t (1|1)p t (2|1)] &#183; [p o (e 1 |f 1 )p o (e 2 |f 1 )p o (e 3 |f 2 )].</S>
          <S ID="S-15056">However, the same alignment (f 1 , e 1 )(f 1 , e 2 )(f 2 , e 3 ), if we change the alignment direction, the backbone being observations, would be a bit different.</S>
          <S ID="S-15057">We offer a minor modification to Formula 1.</S>
        </P>
        <P>
          <S ID="S-15058">Look at the bottom graph of Figure 3, the observation f 1 has two statuses, e 1 and e 2 at the same time, it becomes ambiguous to compute the transitional probability between p t (3|1) and p t (3|2).</S>
          <S ID="S-15059">This is because IHMM algorithm deals with oneto-many alignments, and MOEA permits many-tomany alignments.</S>
          <S ID="S-15060">We hence empirically modify the IHMM model to support many-to-many alignments.</S>
          <S ID="S-15061">A new status is defined, rather than a single position p t (j|i), but as a set of positions p t ({j}|{i}).</S>
          <S ID="S-15062">The positions in one status need not to be adjacent to each other.</S>
        </P>
        <P>
          <S ID="S-15063">The redefined transitional probability</S>
        </P>
        <P>
          <S ID="S-15064">p t ({j}|{i}) =</S>
        </P>
        <P>
          <S ID="S-15065">1 |{j}| &#183; |{i}| &#8721;</S>
        </P>
        <P>
          <S ID="S-15066">p t (j|i)</S>
        </P>
        <P>
          <S ID="S-15067">i,j</S>
        </P>
        <P>
          <S ID="S-15068">The redefined emission probability</S>
        </P>
        <P>
          <S ID="S-15069">p o (j|{i}) = &#8719; p o (j|i)</S>
        </P>
        <P>
          <S ID="S-15070">i</S>
        </P>
        <P>
          <S ID="S-15071">We need to note that there is no guarantee on the closed property of probabilities, though these approximations prove to be effective in a practical sense.</S>
          <S ID="S-15072">Straightforwardly, when there is only one position in a new status, the expanded IHMM degenerates to the standard IHMM.</S>
        </P>
        <P>
          <S ID="S-15073">Let us return to the second IHMM example.</S>
          <S ID="S-15074">The new probability becomes [p t (1|null)p t (2|null)] &#183; [ 1 2 pt (3|1)p t (3|2) &#183; p t (null|3)] &#183; [p o (f 1 |e 1 )p o (f 1 |e 2 )p o (f 2 |e 3 )p o (f 3 |null)].</S>
        </P>
        <P>
          <S ID="S-15075">3.2.2 Alignment Probability</S>
        </P>
        <P>
          <S ID="S-15076">GIZA++ considers very different and more information in alignment, we attempt to utilize them.</S>
          <S ID="S-15077">All probabilities appearing in below formulas can be looked up in GIZA++.</S>
        </P>
        <P>
          <S ID="S-15078">Given a pair of hypotheses f I = (f 1 , .</S>
          <S ID="S-15079">.</S>
          <S ID="S-15080">.</S>
          <S ID="S-15081">f I ), e J = (e 1 , .</S>
          <S ID="S-15082">.</S>
          <S ID="S-15083">.</S>
          <S ID="S-15084">e J ), and their alignment a, the alignment probability could be calculated as follows</S>
        </P>
        <P>
          <S ID="S-15085">T (e i|f I , a) =</S>
        </P>
        <P>
          <S ID="S-15086">p Giza (e J |f I , a) = &#8719; e i T (e i|f I , a)</S>
        </P>
        <P>
          <S ID="S-15087">&#966; i = |{j|(i, j) &#8712; a}|</S>
        </P>
        <P>
          <S ID="S-15088">where &#966; i is the fertility number, t(e|c) the translation probability for the word pair, z(j|i) alignment probability to show how likely a target word at position i could be translated into a source word at position j, and n(&#966;|e) is the fertility probability to show how likely a given target word e is translated into &#966; source words.</S>
        </P>
        <P>
          <S ID="S-15089">In order to increase the coverage of words, we collect all the hypothesis pairs in both the tuning set and the test set and feed them into GIZA++.</S>
          <S ID="S-15090">This is an off-line operation, which makes it not suitable for an online translation system.</S>
          <S ID="S-15091">In some circumstances, users submit a pile of documents in the hope of high-quality translations, thus more useful knowledge sources would be helpful.</S>
          <S ID="S-15092">In our experiments, a pure GIZA++ based system combination does not perform as well as IHMM based, but does benefit the final translation quality if combined in our multiobjective optimization framework.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.3 Configuration of Evolutionary Algorithm</HEADER>
        <P>
          <S ID="S-15093">3.3.1 Encoding</S>
        </P>
        <P>
          <S ID="S-15094">Given a sentence pair &lt;f I , e J &gt;, we define a twodimensional matrix x = {z i,j |z ij &#8712; {0, 1}} to encode a set of possible alignments.</S>
          <S ID="S-15095">Using a line-based access style, the matrix could be unfolded as a vector with |I| &#183; |J| bits of length.</S>
        </P>
        <P>
          <S ID="S-15096">3.3.2 Initialization</S>
        </P>
        <P>
          <S ID="S-15097">Because in NSGA-II software the initial population are generated at random.</S>
          <S ID="S-15098">In order to make NSGA-II more consistent and flexible, better initial seeds should be fed with, thus we combine an existing word alignment results as input.</S>
          <S ID="S-15099">Here we use together two N-best lists generated from directional HMM and reversed HMM respectively for initialization.</S>
        </P>
        <P>
          <S ID="S-15100">3.3.3 Normalization of Pareto Optimal Solutions</S>
        </P>
        <P>
          <S ID="S-15101">Multi-objective optimization algorithms do not pose weights on objectives, thus they output a set of so-called Pareto optimal solutions, each of which is a many-to-many alignment.</S>
          <S ID="S-15102">We can understand them as an N-best alignment list without explicit preferences.</S>
          <S ID="S-15103">We also empirically compare it with the idea that directly cuts an N-best list from the IHMM based alignment.</S>
          <S ID="S-15104">We describe a two-stage strategy for normalization.</S>
          <S ID="S-15105">Firstly, we use a simple and effective voting strategy to combine a set of many-to-many alignments into a single many-to-many alignment, and Secondly we normalize it into a one-to-one alignment for confusion network construction.</S>
          <S ID="S-15106">In the first stage, we count the number of word-to-word alignments on each position pair (i, j).</S>
          <S ID="S-15107">If there is more than a half number of alignments, then we output 1, otherwise 0.</S>
          <S ID="S-15108">In the second stage, if any word relates to more than one word alignment, the one with the highest posterior probability is selected (<REF ID="R-12" RPTR="23">He et al., 2008</REF>; <REF ID="R-08" RPTR="13">Feng et al., 2009</REF>).</S>
          <S ID="S-15109">The posterior probabilities can be computed in a classic forward-backward procedure in IHMM (<REF ID="R-12" RPTR="24">He et al., 2008</REF>).</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 Training and Decoding</HEADER>
      <P>
        <S ID="S-15121">Our work does not change the classic pipeline, thus the model and features are nearly identical to the ones in (<REF ID="R-26" RPTR="50">Rosti et al., 2007</REF><REF ID="R-27" RPTR="55">Rosti et al., 2007</REF>b; <REF ID="R-12" RPTR="25">He et al., 2008</REF>), which are modeled in a log-linear fashion in Eq.</S>
        <S ID="S-15122">8.</S>
        <S ID="S-15123">Translation on a CN is just a concatenation of edges traversed, on which 4 categories of features are defined.</S>
      </P>
      <P>
        <S ID="S-15124">1. word posterior probabilities.</S>
        <S ID="S-15125">In Eq.</S>
        <S ID="S-15126">8, p(w|sys, span) are word confidence scores.</S>
        <S ID="S-15127">If the word w comes from the kth hypothesis of thesys-th system, the raw score should be 1 k+1 , and then it would be normalized by the same sys and span.</S>
        <S ID="S-15128">The same word coming from different systems owns a different score, so there are sys system weights &#955; sys .</S>
      </P>
      <P>
        <S ID="S-15129">2. logarithm of language model score, L(h).</S>
      </P>
      <P>
        <S ID="S-15130">3. number of null edge, Num null .</S>
      </P>
      <P>
        <S ID="S-15131">4. number of words, Num w .</S>
      </P>
      <P>
        <S ID="S-15132">log(h) = &#8721; span log(&#8721; sys &#955; sysp(w|sys, span)) + w 0 L(h) + w 1 Num null + w 2 Num w (8)</S>
      </P>
      <P>
        <S ID="S-15133">Decoding a confusion network is straightforward, traversing each node from left to right, and the beam search algorithm will retain for each node an N- best list.</S>
        <S ID="S-15134">The final N-best can be acquired following (<REF ID="R-13" RPTR="30">Huang and Chiang, 2005</REF>).</S>
      </P>
      <P>
        <S ID="S-15135">The training process follows minimum error rate training (MERT) described in (<REF ID="R-24" RPTR="45">Och, 2003</REF>; <REF ID="R-15" RPTR="33">Koehn et al., 2003</REF>).</S>
        <S ID="S-15136">In each iteration, the Powell algorithm would attempt to predict the optimal parameters on the cumulative N-best list.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Experiments</HEADER>
      <P>
        <S ID="S-15211">We evaluate our method in two datasets in the Chinese-to-English task.</S>
        <S ID="S-15212">In the first one, NIST MT 2002 and 2005 are used for tuning and testing respectively, and in the second, the newswire part of MT 2006 and 2008 are for tuning and testing.</S>
        <S ID="S-15213">A 5- gram language model is trained on the Xinhua portion of the Gigaword corpus.</S>
        <S ID="S-15214">We report the casesensitive NIST-BLEU score.</S>
        <S ID="S-15215">Four single machine translation systems participating in the system combination consist of a BTGbased system using a Max-Entropy based reordering model, a hierarchical phrase-based system, a Moses decoder and a syntax-based system.</S>
        <S ID="S-15216">10-best unique hypotheses from a single system on the development</S>
      </P>
      <P>
        <S ID="S-15217">SYSTEM MT 2005 MT 2008(news)</S>
      </P>
      <P>
        <S ID="S-15218">best single 0.3207 0.3016</S>
      </P>
      <P>
        <S ID="S-15219">IHMM* 0.3585(+3.78%) 0.3263(+2.47%)</S>
      </P>
      <P>
        <S ID="S-15220">IncIHMM 0.3639(+4.32%) 0.3320(+3.04%)</S>
      </P>
      <P>
        <S ID="S-15221">GIZA++ 0.3438(+2.31%) 0.3166(+1.50%)</S>
      </P>
      <P>
        <S ID="S-15222">PPBD 0.3619(+4.10%) 0.3306(+2.90%)</S>
      </P>
      <P>
        <S ID="S-15223">N-best IHMM 0.3590(+3.83%) 0.3270(+2.54%)</S>
      </P>
      <P>
        <S ID="S-15224">dH+rH 0.3604 0.3284</S>
      </P>
      <P>
        <S ID="S-15225">dH+dT 0.3610 0.3290</S>
      </P>
      <P>
        <S ID="S-15226">dH+rH+dT 0.3609 0.3289</S>
      </P>
      <P>
        <S ID="S-15227">dH+rH+rT 0.3630 &#8727; (+4.27%) 0.3320&#8727;(+3.04%)</S>
      </P>
      <P>
        <S ID="S-15228">dH+rH+dT+rT 0.3682 &#8727;&#8727; (+4.75%) 0.3369 &#8727;&#8727; (+3.53%)</S>
      </P>
      <P>
        <S ID="S-15229">and test sets are collected as the input of the system combination.</S>
        <S ID="S-15230">Our baseline systems are described as follows.</S>
        <S ID="S-15231">Two main baseline systems are IHMM based and incremental IHMM (<REF ID="R-19" RPTR="39">Li et al., 2009</REF>).</S>
        <S ID="S-15232">The first system differs from our method just in hypothesis alignment algorithm, and the second combines the first and second module of the system combination pipeline.</S>
      </P>
      <P>
        <S ID="S-15233">Because our method utilizes bidirectional information, we also provide another two alternative systems for comparison, which are GIZA++ based alignment and the posterior probability based alignment (<REF ID="R-20" RPTR="41">Liang et al., 2006</REF>).</S>
        <S ID="S-15234">Finally, we also provide an N-best alignment IHMM system, which combines an N-best alignment list to simulate the Pareto optimal solutions in our method.</S>
      </P>
      <P>
        <S ID="S-15235">The method that linearly combines all objectives is not listed as our baseline like (<REF ID="R-06" RPTR="8">Duh et al., 2012</REF>) does, because their algorithm finds the best weighted solution in a fixed and small solution set, while in our problem, the solution space is a trellis-style structure consisting of an exponential number of solutions, and no efficient algorithms apply here.</S>
      </P>
      <P>
        <S ID="S-15236">The IHMM based alignment utilizes typical settings (<REF ID="R-12" RPTR="26">He et al., 2008</REF>; <REF ID="R-08" RPTR="14">Feng et al., 2009</REF>).</S>
        <S ID="S-15237">The smoothing factor for the surface similarity model, and &#961; = 3 the controlling factor for the distortion model, K = 2.</S>
        <S ID="S-15238">The bilingual probabilistic dictionary is trained in the FBIS corpus which includes about 230k parallel sentence pairs.</S>
        <S ID="S-15239">GIZA++ based system is to run GIZA++ from two directions to align all the hypotheses, and make the intersection using grow-diag-final heuristics (<REF ID="R-15" RPTR="34">Koehn et al., 2003</REF>).</S>
        <S ID="S-15240">The many-to-many alignments are normalized with the same method with ours.</S>
        <S ID="S-15241">Our system employs NSGA-II software to realize the MOEA algorithm.</S>
        <S ID="S-15242">The main parameters, generation number, cross probability and mutation probability, and population size, are empirically set as 100, 0.9, 0.001 and 40, and we examine the influence of difference populations sizes in the full system combination.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>5.1 The Quality of Confusion Networks</HEADER>
        <P>
          <S ID="S-15137">This experiment shows the relationship between hypothesis alignment and confusion network.</S>
          <S ID="S-15138">Intuitively, we expect a better hypothesis alignment would reduce the error in constructing confusion networks, and then improve the final translation quality.</S>
        </P>
        <P>
          <S ID="S-15139">We first use the alignment error rate (AER) (<REF ID="R-23" RPTR="44">Och and Ney, 2000</REF>), which is widely used to measure the quality of hypothesis alignment.</S>
          <S ID="S-15140">The smaller, the better.</S>
          <S ID="S-15141">For convenience, we only examine exact literal matching.</S>
          <S ID="S-15142">IHMM based alignment reaches around 0.15 in AER, and our method 0.145.</S>
        </P>
        <P>
          <S ID="S-15143">As the AER may not vividly reflect the relations between alignment and the final BLEU of systems, and the quality of confusion network is hard to measure directly, we assume that the quality of confusion networks could be measured by the oracle hypotheses that could be generated from them.</S>
          <S ID="S-15144">We test the BLEU of the oracle hypotheses.</S>
          <S ID="S-15145">From this angle, we demonstrate several oracle BLEU of CNs generated from some conventional alignment algorithms.</S>
          <S ID="S-15146">The results are shown in Table 3.</S>
        </P>
        <P>
          <S ID="S-15147">We find the confusion network from IHMM based alignment (<REF ID="R-12" RPTR="27">He et al., 2008</REF>) is better than that from TER based alignment (<REF ID="R-26" RPTR="51">Rosti et al., 2007</REF><REF ID="R-27" RPTR="56">Rosti et al., 2007</REF>b) by about 1 point in both two datasets.</S>
          <S ID="S-15148">These quantities agree with the final improvements in the BLEU score in (<REF ID="R-12" RPTR="28">He et al., 2008</REF>).</S>
          <S ID="S-15149">As confusion networks from MOEA based alignment also show superiority over</S>
        </P>
        <P>
          <S ID="S-15150">alignment MT02 MT05</S>
        </P>
        <P>
          <S ID="S-15151">GIZA++ 0.5690 0.5228</S>
        </P>
        <P>
          <S ID="S-15152">TER 0.5720 0.5270</S>
        </P>
        <P>
          <S ID="S-15153">IHMM 0.5883 0.5382</S>
        </P>
        <P>
          <S ID="S-15154">IncIHMM 0.5931 0.5453</S>
        </P>
        <P>
          <S ID="S-15155">MOEA 0.6017 0.5526</S>
        </P>
        <P>
          <S ID="S-15156">that from IHMM based in the oracle BLEU, we expect our final translation quality would be improved.</S>
        </P>
        <P>
          <S ID="S-15157">In Table 3, GIZA++ and TER perform similarly, because the former is more capable of tackling many-to-many alignments over the latter, while latter based might obtain relatively more precise alignment information.</S>
          <S ID="S-15158">Both of the two do not consider synonym matching compared to IHMM.</S>
          <S ID="S-15159">Our method and IncIHMM overpass IHMM on this metric due to different strategies.</S>
          <S ID="S-15160">Obtaining better hypothesis alignment or better construction of confusion networks benefit the quality of CNs.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>5.2 Different Objective Combinations</HEADER>
        <P>
          <S ID="S-15161">As our framework is convenient to support different alignment information, we test the influence of different objective combinations to the final translation quality.</S>
          <S ID="S-15162">We adopt four objectives to depict the candidate alignment, directed IHMM probability (dH), reversed IHMM probability (rH), directed alignment probability (dT), and reversed alignment probability (rT).</S>
          <S ID="S-15163">Table 2 demonstrates all the results.</S>
        </P>
        <P>
          <S ID="S-15164">We can see that the IHMM based system outperforms the GIZA++ based system by about 1-1.5 points in BLEU, which agrees with the difference of oracle BLEU in Table 1.</S>
          <S ID="S-15165">From (<REF ID="R-12" RPTR="29">He et al., 2008</REF>), the IHMM based system outperforms the TER based by 1 point, which also agrees with our results in Table 1.</S>
          <S ID="S-15166">Our system, using dH + rH + dT + rT, improves BLEU score by about 1 points over the IHMM based system.</S>
          <S ID="S-15167">This comparison verifies our assumption, improving the quality of the confusion network does improve system performance.</S>
          <S ID="S-15168">The different feature combinations exhibit interesting results.</S>
          <S ID="S-15169">The system with dH + rH + dT is 0.05 point better than the system with dH + rH, and the system dH + rH + rT is 0.3 point better than system with dH + rH, so the contributions of feature dT and rT are 0.05 and 0.3 respectively.</S>
          <S ID="S-15170">While the two features are used together in the fourth system, the contribution is about 0.8 point, rather than 0.35.</S>
          <S ID="S-15171">This phenomenon also proves the correlations between different features.</S>
        </P>
        <P>
          <S ID="S-15172">Our method explores a way to integrate GIZA++ and IHMM, and is supportive of useful features.</S>
          <S ID="S-15173">Compared to the classic and powerful IHMM based system, we obtained an improvement of 0.97 points on MT 05 and 1.06 points on news of MT 2008, and equivalently over the best single system by 4.75 points and 3.53 points respectively.</S>
          <S ID="S-15174">More, compared with the incremental IHMM, our system also shows moderate improvement, though not much.</S>
          <S ID="S-15175">We hope these two ideas could be effectively combined in the future work.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>5.3 Comparison with Other Bi-directional Alignment Methods</HEADER>
        <P>
          <S ID="S-15176">Our method introduces multiple alignment information into system combination to obtain improvements, thus it would be interesting to explore other alternative methods for utilizing this information.</S>
          <S ID="S-15177">We provide three alternative methods similar to our motivations, and they fall into two categories.</S>
        </P>
        <P>
          <S ID="S-15178">The first category is from the angle of bidirectional alignment.</S>
          <S ID="S-15179">We use GiZA++ alignment and the posterior probability decoding-based alignment for comparison.</S>
          <S ID="S-15180">The basic idea for the latter is setting a word-to-word alignment x i,j as 1, if its approximate posterior marginal probability q(x i,j , x) = p d (x i,j |x, &#952; d ) &#183; p r (x i,j |x, &#952; r ) is greater than a threshold &#948;, where p d and p r are posterior marginal probabilities from directed and reversed IHMM models, which could be conveniently computed with a forward-backward algorithm, and the &#948; is tuned on a validation-set optimized data.</S>
          <S ID="S-15181">We just list some &#948; values to examine its best performance shown in Table 4.</S>
        </P>
        <P>
          <S ID="S-15182">The second class is because our method combines the Pareto optimal solutions that consist of several candidate alignments, thus for fairness we also use a 100-best outputs from the directed IHMM model and conduct the same normalization technique.</S>
        </P>
        <P>
          <S ID="S-15183">The general results are shown in Table 2.</S>
          <S ID="S-15184">We can</S>
        </P>
        <P>
          <S ID="S-15185">&#948; MT 2005 MT 2008</S>
        </P>
        <P>
          <S ID="S-15186">IHMM 0.3585 0.3263</S>
        </P>
        <P>
          <S ID="S-15187">0.15 0.3556 0.3391</S>
        </P>
        <P>
          <S ID="S-15188">0.2 0.3619 0.3306</S>
        </P>
        <P>
          <S ID="S-15189">0.25 0.3575 0.3278</S>
        </P>
        <P>
          <S ID="S-15190">0.3 0.3608 0.3259</S>
        </P>
        <P>
          <S ID="S-15191">see that, GIZA++ leads to the worst performance, which can be explained as GIZA++ does not support synonym matching like IHMM.</S>
          <S ID="S-15192">The N-best IHMM has a minor improvement over the IHMM method.</S>
          <S ID="S-15193">We found differences in the N-best list are not obvious enough.</S>
          <S ID="S-15194">In comparison, the posterior decoding method brings relatively significant improvements on both datasets.</S>
          <S ID="S-15195">However, the threshold &#948; must be selected suitably.</S>
          <S ID="S-15196">Table 4 lists the ideal results, which will be hampered when tuning on a validation set.</S>
        </P>
        <P>
          <S ID="S-15197">All of the three candidate methods can not conveniently support extra alignment information, and a linear model poses restrictions on features to get an efficient decoding, the multi-objective optimization may be a good selection as an inference algorithm in many circumstances.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>5.4 Population Size</HEADER>
        <P>
          <S ID="S-15198">We test the influence of final translation quality and time consumed by different population size.</S>
        </P>
        <P>
          <S ID="S-15199">population BLEU</S>
        </P>
        <P>
          <S ID="S-15200">size MT 2005</S>
        </P>
        <P>
          <S ID="S-15201">20 0.3597</S>
        </P>
        <P>
          <S ID="S-15202">40 0.3682</S>
        </P>
        <P>
          <S ID="S-15203">60 0.3655</S>
        </P>
        <P>
          <S ID="S-15204">We expect enlarging the population size would improve the translation quality, but the BLEU in population size set as 60 does not overpass when set as 40.</S>
          <S ID="S-15205">We conjecture that, in our code, if the N-best size from IHMM (we set as 50-best) does not reach the population size, we would use randomly generated seeds, which may hamper the performance of MOEA.</S>
          <S ID="S-15206">We also tried a larger population in MOEA, but did not receive obvious improvement on performance.</S>
        </P>
        <P>
          <S ID="S-15207">We exerted a hard restriction on the genes in evolutionary algorithm, that is many-to-many discontiguous alignment is forbidden.</S>
          <S ID="S-15208">This trick speeds up running by about 20 times, and does not harm system performance.</S>
          <S ID="S-15209">Now our method runs about 0.9 seconds to align a pair of hypotheses.</S>
          <S ID="S-15210">In practice, we utilize multi-thread to speed up.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>6 Conclusion</HEADER>
      <P>
        <S ID="S-15243">In this paper, we explore a multi-objective framework to conveniently support more useful alignment objectives to improve the hypothesis alignment.</S>
        <S ID="S-15244">By a minor modification of the first module in the classic pipeline, we successfully combine GIZA++ and IHMM to obtain significant improvement over a powerful and state-of-the-art IHMM based system.</S>
        <S ID="S-15245">In comparison with another genre of improving system combination by combing adjacent modules of the pipeline, more powerful incremental IHMM here, our system also show moderate improvement.</S>
        <S ID="S-15246">Though, our best system may not overpass <REF ID="R-11" RPTR="18">He and Toutanova (2009)</REF> who combine all the modules into a unified training procedure, we believe our method could boost many work on the higher modules of the pipeline to obtain a further improvement to match their work.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>7 Acknowledgement</HEADER>
      <P>
        <S ID="S-15247">This research is partially supported by Air Force Office of Scientific Research under grant FA9550- 10-1-0335, the National Science Foundation under grant IIS RI-small 1218863 and a Google research award.</S>
        <S ID="S-15248">We thank the anonymous reviewers for their insightful comments.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS/>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>B Bangalore</RAUTHOR>
      <REFTITLE>Computing consensus translation from multiple machine translation systems. In Automatic Speech Recognition and Understanding.</REFTITLE>
      <DATE>2001</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>Yidong Chen</RAUTHOR>
      <REFTITLE>A word alignment 543 based on multiobjective evolutionary algorithms.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>Kalyanmoy Deb</RAUTHOR>
      <REFTITLE>A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: Nsga-ii. Lecture notes in computer science,</REFTITLE>
      <DATE>2000</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>Kalyanmoy Deb</RAUTHOR>
      <REFTITLE>A fast and elitist multiobjective genetic algorithm: Nsga-ii. Evolutionary Computation,</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>Kalyanmoy Deb</RAUTHOR>
      <REFTITLE>Multi-objective optimization. Multi-objective optimization using evolutionary algorithms,</REFTITLE>
      <DATE>2001</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>John DeNero</RAUTHOR>
      <REFTITLE>Model combination for machine translation.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>Kevin Duh</RAUTHOR>
      <REFTITLE>Learning to translate with multiple objectives.</REFTITLE>
      <DATE>2012</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>Andries P Engelbrecht</RAUTHOR>
      <REFTITLE>Fundamentals of computational swarm intelligence, volume 1.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>Yang Feng</RAUTHOR>
      <REFTITLE>Lattice-based system combination for statistical machine translation.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>G David Forney Jr</RAUTHOR>
      <REFTITLE>The viterbi algorithm.</REFTITLE>
      <DATE>1973</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>Michael Pilegaard Hansen</RAUTHOR>
      <REFTITLE>Tabu search for multiobjective optimization: Mots.</REFTITLE>
      <DATE>1997</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>Xiaodong He</RAUTHOR>
      <REFTITLE>Joint optimization for machine translation system combination.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>Xiaodong He</RAUTHOR>
      <REFTITLE>Indirect-hmm-based hypothesis alignment for combining outputs from machine translation systems.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>Liang Huang</RAUTHOR>
      <REFTITLE>Better k-best parsing.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="14">
      <RAUTHOR>Fei Huang</RAUTHOR>
      <REFTITLE>Hierarchical system combination for machine translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="15">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Statistical phrase-based translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="16">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Moses: Open source toolkit for statistical machine translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="17">
      <RAUTHOR>Shankar Kumar</RAUTHOR>
      <REFTITLE>Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="18">
      <RAUTHOR>Zhifei Li</RAUTHOR>
      <REFTITLE>Forest reranking for machine translation with the perceptron algorithm.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="19">
      <RAUTHOR>Chi-Ho Li</RAUTHOR>
      <REFTITLE>Incremental hmm alignment for mt system combination.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="20">
      <RAUTHOR>Percy Liang</RAUTHOR>
      <REFTITLE>Alignment by agreement.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="21">
      <RAUTHOR>Evgeny Matusov</RAUTHOR>
      <REFTITLE>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="22">
      <RAUTHOR>Haitao Mi</RAUTHOR>
      <REFTITLE>Forestbased translation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="23">
      <RAUTHOR>F J Och</RAUTHOR>
      <REFTITLE>Improved statistical alignment models.</REFTITLE>
      <DATE>2000</DATE>
    </REFERENCE>
    <REFERENCE ID="24">
      <RAUTHOR>Franz Josef Och</RAUTHOR>
      <REFTITLE>Minimum error rate training in statistical machine translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="25">
      <RAUTHOR>Adam Pauls</RAUTHOR>
      <REFTITLE>Consensus training for consensus decoding in machine translation.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="26">
      <RAUTHOR>Antti-Veikko I Rosti</RAUTHOR>
      <REFTITLE>Combining outputs from multiple machine translation systems.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="27">
      <RAUTHOR>Antti-Veikko I Rosti</RAUTHOR>
      <REFTITLE>Improved word-level system combination for machine translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="28">
      <RAUTHOR>Antti-Veikko I Rosti</RAUTHOR>
      <REFTITLE>Incremental hypothesis alignment for building confusion networks with application to machine translation system combination.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="29">
      <RAUTHOR>Paolo Serafini</RAUTHOR>
      <REFTITLE>Simulated annealing for multi objective optimization problems.</REFTITLE>
      <DATE>1994</DATE>
    </REFERENCE>
    <REFERENCE ID="30">
      <RAUTHOR>Khe Chai Sim</RAUTHOR>
      <REFTITLE>Consensus network decoding for statistical machine translation system combination.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
