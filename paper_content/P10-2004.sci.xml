<PAPER>
  <FILENO/>
  <TITLE>Filtering Syntactic Constraints for Statistical Machine Translation</TITLE>
  <AUTHORS>
    <AUTHOR>Hailong Cao</AUTHOR>
  </AUTHORS>
  <ABSTRACT>
    <A-S ID="S-31222">Source language parse trees offer very useful but imperfect reordering constraints for statistical machine translation.</A-S>
    <A-S ID="S-31223">A lot of effort has been made for soft applications of syntactic constraints.</A-S>
    <A-S ID="S-31224">We alternatively propose the selective use of syntactic constraints.</A-S>
    <A-S ID="S-31225">A classifier is built automatically to decide whether a node in the parse trees should be used as a reordering constraint or not.</A-S>
    <A-S ID="S-31226">Using this information yields a 0.8 BLEU point improvement over a full constraint-based system.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-31227">In statistical machine translation (SMT), the search problem is NP-hard if arbitrary reordering is allowed (<REF ID="R-07" RPTR="8">Knight, 1999</REF>).</S>
        <S ID="S-31228">Therefore, we need to restrict the possible reordering in an appropriate way for both efficiency and translation quality.</S>
        <S ID="S-31229">The most widely used reordering constraints are IBM constraints (<REF ID="R-00" RPTR="0">Berger et al., 1996</REF>), ITG constraints (<REF ID="R-15" RPTR="15">Wu, 1995</REF>) and syntactic constraints (Yamada et al., 2000; <REF ID="R-06" RPTR="7">Galley et al., 2004</REF>; <REF ID="R-10" RPTR="9">Liu et al., 2006</REF>; <REF ID="R-11" RPTR="10">Marcu et al., 2006</REF>; Zollmann and Venugopal 2006; and numerous others).</S>
        <S ID="S-31230">Syntactic constraints can be imposed from the source side or target side.</S>
        <S ID="S-31231">This work will focus on syntactic constraints from source parse trees.</S>
      </P>
      <P>
        <S ID="S-31232">Linguistic parse trees can provide very useful reordering constraints for SMT.</S>
        <S ID="S-31233">However, they are far from perfect because of both parsing errors and the crossing of the constituents and formal phrases extracted from parallel training data.</S>
        <S ID="S-31234">The key challenge is how to take advantage of the prior knowledge in the linguistic parse trees without affecting the strengths of formal phrases.</S>
        <S ID="S-31235">Recent efforts attack this problem by using the constraints softly (<REF ID="R-02" RPTR="2">Cherry, 2008</REF>; <REF ID="R-12" RPTR="12">Marton and Resnik, 2008</REF>).</S>
        <S ID="S-31236">In their methods, a candidate translation gets an extra credit if it respects the parse tree but may incur a cost if it violates a constituent boundary.</S>
      </P>
      <P>
        <S ID="S-31237">In this paper, we address this challenge from a less explored direction.</S>
        <S ID="S-31238">Rather than use all constraints offered by the parse trees, we propose using them selectively.</S>
        <S ID="S-31239">Based on parallel training data, a classifier is built automatically to decide whether a node in the parse trees should be used as a reordering constraint or not.</S>
        <S ID="S-31240">As a result, we obtain a 0.8 BLEU point improvement over a full constraint-based system.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Reordering Constraints from Source Parse Trees</HEADER>
      <P>
        <S ID="S-31241">In this section we briefly review a constraintbased system named IST-ITG (Imposing Source Tree on Inversion Transduction Grammar, Yamamoto et al., 2008) upon which this work builds.</S>
      </P>
      <P>
        <S ID="S-31242">When using ITG constraints during decoding, the source-side parse tree structure is not considered.</S>
        <S ID="S-31243">The reordering process can be more tightly constrained if constraints from the source parse tree are integrated with the ITG constraints.</S>
        <S ID="S-31244">IST- ITG constraints directly apply source sentence tree structure to generate the target with the following constraint: the target sentence is obtained by rotating any node of the source sentence tree structure.</S>
      </P>
      <P>
        <S ID="S-31245">After parsing the source sentence, a bracketed sentence is obtained by removing the node syntactic labels; this bracketed sentence can then be directly expressed as a tree structure.</S>
        <S ID="S-31246">For example 1 , the parse tree &#8220;(S1 (S (NP (DT This)) (VP (AUX is) (NP (DT a) (NN pen)))))&#8221; is obtained from the source sentence &#8220;This is a pen&#8221;, which consists of four words.</S>
        <S ID="S-31247">By removing</S>
      </P>
      <P>
        <S ID="S-31248">1 We use English examples for the sake of readability.</S>
      </P>
      <P>
        <S ID="S-31249">the node syntactic labels, the bracketed sentence &#8220;((This) ((is) ((a) (pen))))&#8221; is obtained.</S>
        <S ID="S-31250">Such a bracketed sentence can be used to produce constraints.</S>
      </P>
      <P>
        <S ID="S-31251">For example, for the source-side bracketed tree &#8220;((f1 f2) (f3 f4)) &#8221;, eight target sequences [e1, e2, e3, e4], [e2, e1, e3, e4], [e1, e2, e4, e3], [e2, e1, e4, e3], [e3, e4, e1, e2], [e3, e4, e2, e1], [e4, e3, e1, e2], and [e4, e3, e2, e1] are possible.</S>
        <S ID="S-31252">For the source-side bracketed tree &#8220;(((f1f2) f3) f4),&#8221; eight sequences [e1, e2, e3, e4], [e2, e1, e3, e4], [e3, e1, e2, e4], [e3, e2, e1, e4], [e4, e1, e2, e3], [e4, e2, e1, e3], [e4, e3, e1, e2], and [e4, e3, e2, e1] are possible.</S>
        <S ID="S-31253">When the source sentence tree structure is a binary tree, the number of word orderings is reduced to 2 N-1 where N is the length of the source sentence.</S>
      </P>
      <P>
        <S ID="S-31254">The parsing results sometimes do not produce binary trees.</S>
        <S ID="S-31255">In this case, some subtrees have more than two child nodes.</S>
        <S ID="S-31256">For a non-binary subtree, any reordering of child nodes is allowed.</S>
        <S ID="S-31257">For example, if a subtree has three child nodes, six reorderings of the nodes are possible.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Learning to Classify Parse Tree Nodes</HEADER>
      <P>
        <S ID="S-31274">In IST-ITG and many other methods which use syntactic constraints, all of the nodes in the parse trees are utilized.</S>
        <S ID="S-31275">Though many nodes in the parse trees are useful, we would argue that some nodes are not trustworthy.</S>
        <S ID="S-31276">For example, if we constrain the translation of &#8220;f1 f2 f3 f4&#8221; with node N2 illustrated in Figure 1, then word &#8220;e1&#8221; will never be put in the middle the other three words.</S>
        <S ID="S-31277">If we want to obtain the translation &#8220;e2 e1 e4 e3&#8221;, node N3 can offer a good constraint while node N2 should be filtered out.</S>
        <S ID="S-31278">In real corpora, cases such as node N2 are frequent enough to be noticeable (see <REF ID="R-05" RPTR="5">Fox (2002)</REF> or section 4.1 in this paper).</S>
      </P>
      <P>
        <S ID="S-31279">Therefore, we use the definitions in <REF ID="R-06" RPTR="6">Galley et al. (2004)</REF> to classify the nodes in parse trees into two types: frontier nodes and interior nodes.</S>
        <S ID="S-31280">Though the definitions were originally made for target language parse trees, they can be straightforwardly applied to the source side.</S>
        <S ID="S-31281">A node which satisfies both of the following two conditions is referred as a frontier node:</S>
      </P>
      <P>
        <S ID="S-31282">&#8226; All the words covered by the node can be translated separately.</S>
        <S ID="S-31283">That is to say, these words do not share a translation with any word outside the coverage of the node.</S>
      </P>
      <P>
        <S ID="S-31284">&#8226; All the words covered by the node remain contiguous after translation.</S>
      </P>
      <P>
        <S ID="S-31285">Otherwise the node is an interior node.</S>
      </P>
      <P>
        <S ID="S-31286">For example, in Figure 1, both node N1 and node N3 are frontier nodes.</S>
        <S ID="S-31287">Node N2 is an interior node because the source words f2, f3 and f4 are translated into e2, e3 and e4, which are not contiguous in the target side.</S>
      </P>
      <P>
        <S ID="S-31288">Clearly, only frontier nodes should be used as reordering constraints while interior nodes are not suitable for this.</S>
        <S ID="S-31289">However, little work has been done on how to explicitly distinguish these two kinds of nodes in the source parse trees.</S>
        <S ID="S-31290">In this section, we will explore building a classifier which can label the nodes in the parse trees as frontier nodes or interior nodes.</S>
      </P>
      <P>
        <S ID="S-31291">N1</S>
      </P>
      <P>
        <S ID="S-31292">N2</S>
      </P>
      <P>
        <S ID="S-31293">N3</S>
      </P>
      <P>
        <S ID="S-31294">f1 f2 f3 f4</S>
      </P>
      <P>
        <S ID="S-31295">e2 e1 e4 e3</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>3.1 Training</HEADER>
        <P>
          <S ID="S-31258">Ideally, we would have a human-annotated corpus in which each sentence is parsed and each node in the parse trees is labeled as a frontier node or an interior node.</S>
          <S ID="S-31259">But such a target language specific corpus is hard to come by, and never in the quantity we would like.</S>
        </P>
        <P>
          <S ID="S-31260">Instead, we generate such a corpus automatically.</S>
          <S ID="S-31261">We begin with a parallel corpus which will be used to train our SMT model.</S>
          <S ID="S-31262">In our case, it is the FBIS Chinese-English corpus.</S>
        </P>
        <P>
          <S ID="S-31263">Firstly, the Chinese sentences are segmented, POS tagged and parsed by the tools described in Kruengkrai et al. (2009) and Cao et al. (2007), both of which are trained on the Penn Chinese Treebank 6.0.</S>
        </P>
        <P>
          <S ID="S-31264">Secondly, we use GIZA++ to align the sentences in both the Chinese-English and English- Chinese directions.</S>
          <S ID="S-31265">We combine the alignments using the &#8220;grow-diag-final-and&#8221; procedure provided with MOSES (Koehn, 2007).</S>
          <S ID="S-31266">Because there are many errors in the alignment, we remove the links if the alignment count is less than three for the source or the target word.</S>
          <S ID="S-31267">Additionally, we also remove notoriously bad links in</S>
        </P>
        <P>
          <S ID="S-31268">{de, le} &#215; {the, a, an} following <REF ID="R-04" RPTR="4">Fossum and Knight (2008)</REF>.</S>
        </P>
        <P>
          <S ID="S-31269">Thirdly, given the parse trees and the alignment information, we label each node as a frontier node or an interior node according to the definition introduced in this section.</S>
          <S ID="S-31270">Using the labeled nodes as training data, we can build a classifier.</S>
          <S ID="S-31271">In theory, a broad class of machine learning tools can be used; however, due to the scale of the task (see section 4), we utilize the Pegasos 2 which is a very fast SVM solver (<REF ID="R-14" RPTR="14">Shalev-Shwartz et al, 2007</REF>).</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.2 Features</HEADER>
        <P>
          <S ID="S-31272">For each node in the parse trees, we use the following feature templates: &#8226; A context-free grammar rule which rewrites the current node (In this and all the following grammar based features, a mark is used to indicate which non terminal is the current node.</S>
          <S ID="S-31273">) &#8226; A context-free grammar rule which rewrites the current node&#8217;s father &#8226; The combination of the above two rules &#8226; A lexicalized context-free grammar rule which rewrites the current node &#8226; A lexicalized context-free grammar rule which rewrites the current node&#8217;s father &#8226; Syntactic label, head word, and head POS tag of the current node &#8226; Syntactic label, head word, and head POS tag of the current node&#8217;s left child &#8226; Syntactic label, head word, and head POS tag of the current node&#8217;s right child &#8226; Syntactic label, head word, and head POS tag of the current node&#8217;s left brother &#8226; Syntactic label, head word, and head POS tag of the current node&#8217;s right brother &#8226; Syntactic label, head word, and head POS tag of the current node&#8217;s father &#8226; The leftmost word covered by the current node and the word before it &#8226; The rightmost word covered by the current node and the word after it</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 Experiments</HEADER>
      <P>
        <S ID="S-31319">Our SMT system is based on a fairly typical phrase-based model (<REF ID="R-03" RPTR="3">Finch and Sumita, 2008</REF>).</S>
        <S ID="S-31320">For the training of our SMT model, we use a modified training toolkit adapted from the</S>
      </P>
      <P>
        <S ID="S-31321">2 http://www.cs.huji.ac.il/~shais/code/index.html</S>
      </P>
      <P>
        <S ID="S-31322">MOSES decoder.</S>
        <S ID="S-31323">Our decoder can operate on the same principles as the MOSES decoder.</S>
        <S ID="S-31324">Minimum error rate training (MERT) with respect to BLEU score is used to tune the decoder&#8217;s parameters, and it is performed using the standard technique of <REF ID="R-13" RPTR="13">Och (2003)</REF>.</S>
        <S ID="S-31325">A lexical reordering model was used in our experiments.</S>
      </P>
      <P>
        <S ID="S-31326">The translation model was created from the FBIS corpus.</S>
        <S ID="S-31327">We used a 5-gram language model trained with modified Knesser-Ney smoothing.</S>
        <S ID="S-31328">The language model was trained on the target side of FBIS corpus and the Xinhua news in GI- GAWORD corpus.</S>
        <S ID="S-31329">The development and test sets are from NIST MT08 evaluation campaign.</S>
        <S ID="S-31330">Table 1 shows the statistics of the corpora used in our experiments.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>4.1 Experiments on Nodes Classification</HEADER>
        <P>
          <S ID="S-31296">We extracted about 3.9 million example nodes from the training data, i.e. the FBIS corpus.</S>
          <S ID="S-31297">There were 2.37 million frontier nodes and 1.59 million interior nodes in these examples, give rise to about 4.4 million features.</S>
          <S ID="S-31298">To test the performance of our classifier, we simply use the last ten thousand examples as a test set, and the rest being used as Pegasos training data.</S>
          <S ID="S-31299">All the parameters in Pegasos were set as default values.</S>
          <S ID="S-31300">In this way, the accuracy of the classifier was 71.59%.</S>
        </P>
        <P>
          <S ID="S-31301">Then we retrained our classifier by using all of the examples.</S>
          <S ID="S-31302">The nodes in the automatically parsed NIST MT08 test set were labeled by the classifier.</S>
          <S ID="S-31303">As a result, 17,240 nodes were labeled as frontier nodes and 5,736 nodes were labeled as interior nodes.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>4.2 Experiments on Chinese-English SMT</HEADER>
        <P>
          <S ID="S-31304">In order to confirm that it is advantageous to distinguish between frontier nodes and interior nodes, we performed four translation experiments.</S>
        </P>
        <P>
          <S ID="S-31305">The first one was a typical beam search decoding without any syntactic constraints.</S>
        </P>
        <P>
          <S ID="S-31306">All the other three experiments were based on the IST-ITG method which makes use of syntac-</S>
        </P>
        <P>
          <S ID="S-31307">tic constraints.</S>
          <S ID="S-31308">The difference between these three experiments lies in what constraints are used.</S>
          <S ID="S-31309">In detail, the second one used all nodes recognized by the parser; the third one only used frontier nodes labeled by the classifier; the fourth one only used interior nodes labeled by the classifier.</S>
        </P>
        <P>
          <S ID="S-31310">With the exception of the above differences, all the other settings were the same in the four experiments.</S>
          <S ID="S-31311">Table 2 summarizes the SMT performance.</S>
        </P>
        <P>
          <S ID="S-31312">Clearly, we obtain the best performance if we constrain the search with only frontier nodes.</S>
          <S ID="S-31313">Using just frontier yields a 0.8 BLEU point improvement over the baseline constraint-based system which uses all the constraints.</S>
        </P>
        <P>
          <S ID="S-31314">On the other hand, constraints from interior nodes result in the worst performance.</S>
          <S ID="S-31315">This comparison shows it is necessary to explicitly distinguish nodes in the source parse trees when they are used as reordering constraints.</S>
        </P>
        <P>
          <S ID="S-31316">The improvement over the system without constraints is only modest.</S>
          <S ID="S-31317">It may be too coarse to use pare trees as hard constraints.</S>
          <S ID="S-31318">We believe a greater improvement can be expected if we apply our idea to finer-grained approaches that use constraints softly (<REF ID="R-12" RPTR="11">Marton and Resnik (2008)</REF> and <REF ID="R-02" RPTR="1">Cherry (2008)</REF>).</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Conclusion and Future Work</HEADER>
      <P>
        <S ID="S-31331">We propose a selectively approach to syntactic constraints during decoding.</S>
        <S ID="S-31332">A classifier is built automatically to decide whether a node in the parse trees should be used as a reordering constraint or not.</S>
        <S ID="S-31333">Preliminary results show that it is not only advantageous but necessary to explicitly distinguish between frontier nodes and interior nodes.</S>
        <S ID="S-31334">The idea of selecting syntactic constraints is compatible with the idea of using constraints softly; we plan to combine the two ideas and obtain further improvements in future work.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS>
    <P>
      <S ID="S-31335">We would like to thank Taro Watanabe and Andrew Finch for insightful discussions.</S>
      <S ID="S-31336">We also would like to thank the anonymous reviewers for their constructive comments.</S>
    </P>
  </ACKNOWLEDGMENTS>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>A L Berger</RAUTHOR>
      <REFTITLE>Language translation apparatus and method of using context-based translation models. United States patent, patent number 5510981,</REFTITLE>
      <DATE>1996</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>Hailong Cao</RAUTHOR>
      <REFTITLE>Yujie Zhang and Hitoshi Isahara. Empirical study on parsing Chinese based on Collins&amp;apos; model.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>Colin Cherry</RAUTHOR>
      <REFTITLE>Cohesive phrase-Based decoding for statistical machine translation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>Andrew Finch</RAUTHOR>
      <REFTITLE>Dynamic model interpolation for statistical machine translation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>Victoria Fossum</RAUTHOR>
      <REFTITLE>Using bilingual Chinese-English word alignments to resolve PP attachment ambiguity in English.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>Heidi J Fox</RAUTHOR>
      <REFTITLE>Phrasal cohesion and statistical machine translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>Michel Galley</RAUTHOR>
      <REFTITLE>What&amp;apos;s in a translation rule?</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>Kevin Knight</RAUTHOR>
      <REFTITLE>Decoding complexity in word replacement translation models.</REFTITLE>
      <DATE>1999</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Moses: Open Source Toolkit for Statistical Machine Translation. In ACL demo and poster sessions.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>Canasai Kruengkrai</RAUTHOR>
      <REFTITLE>Kiyotaka Uchimoto, Jun&amp;apos;ichi Kazama, Yiou Wang, Kentaro Torisawa and Hitoshi Isahara.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>Yang Liu</RAUTHOR>
      <REFTITLE>Tree-tostring alignment template for statistical machine translation.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>Daniel Marcu</RAUTHOR>
      <REFTITLE>SPMT: Statistical machine translation with syntactified target language phrases.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>Marton</RAUTHOR>
      <REFTITLE>Soft syntactic constraints for hierarchical phrased-based translation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>Franz Och</RAUTHOR>
      <REFTITLE>Minimum error rate training in statistical machine translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="14">
      <RAUTHOR>Shai Shalev-Shwartz</RAUTHOR>
      <REFTITLE>Pegasos: Primal estimated sub-gradient solver for SVM.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="15">
      <RAUTHOR>Dekai Wu</RAUTHOR>
      <REFTITLE>Stochastic inversion transduction grammars with application to segmentation, bracketing, and alignment of parallel corpora.</REFTITLE>
      <DATE>1995</DATE>
    </REFERENCE>
    <REFERENCE ID="16">
      <RAUTHOR>Kenji Yamada</RAUTHOR>
      <REFTITLE>A syntaxbased statistical translation model.</REFTITLE>
      <DATE>2000</DATE>
    </REFERENCE>
    <REFERENCE ID="17">
      <RAUTHOR>Hirofumi Yamamoto</RAUTHOR>
      <REFTITLE>Hideo Okuma and Eiichiro Sumita.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
