<PAPER>
  <FILENO/>
  <TITLE>The RWTH Aachen Machine Translation System for WMT 2010</TITLE>
  <AUTHORS>
    <AUTHOR>Carmen Heger</AUTHOR>
    <AUTHOR>Joern Wuebker</AUTHOR>
    <AUTHOR>Matthias Huck</AUTHOR>
    <AUTHOR>Gregor Leusch</AUTHOR>
    <AUTHOR>Saab Mansour</AUTHOR>
    <AUTHOR>Daniel Stein</AUTHOR>
  </AUTHORS>
  <ABSTRACT>
    <A-S ID="S-43073">In this paper we describe the statistical machine translation system of the RWTH Aachen University developed for the translation task of the Fifth Workshop on Statistical Machine Translation.</A-S>
    <A-S ID="S-43074">Stateof-the-art phrase-based and hierarchical statistical MT systems are augmented with appropriate morpho-syntactic enhancements, as well as alternative phrase training methods and extended lexicon models.</A-S>
    <A-S ID="S-43075">For some tasks, a system combination of the best systems was used to generate a final hypothesis.</A-S>
    <A-S ID="S-43076">We participated in the constrained condition of German- English and French-English in each translation direction.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-43077">This paper describes the statistical MT system used for our participation in the WMT 2010 shared translation task.</S>
        <S ID="S-43078">We used it as an opportunity to incorporate novel methods which have been investigated at RWTH over the last year and which have proven to be successful in other evaluations.</S>
        <S ID="S-43079">For all tasks we used standard alignment and training tools as well as our in-house phrasebased and hierarchical statistical MT decoders.</S>
        <S ID="S-43080">When German was involved, morpho-syntactic preprocessing was applied.</S>
        <S ID="S-43081">An alternative phrasetraining method and additional models were tested and investigated with respect to their effect for the different language pairs.</S>
        <S ID="S-43082">For two of the language pairs we could improve performance by system combination.</S>
      </P>
      <P>
        <S ID="S-43083">An overview of the systems and models will follow in Section 2 and 3, which describe the baseline architecture, followed by descriptions of the additional system components.</S>
        <S ID="S-43084">Morpho-syntactic analysis and other preprocessing issues are covered by Section 4.</S>
        <S ID="S-43085">Finally, translation results for the different languages and system variants are presented in Section 5.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Translation Systems</HEADER>
      <P>
        <S ID="S-43110">For the WMT 2010 Evaluation we used standard phrase-based and hierarchical translation systems.</S>
        <S ID="S-43111">Alignments were trained with a variant of GIZA++.</S>
        <S ID="S-43112">Target language models are 4-gram language models trained with the SRI toolkit, using Kneser-Ney discounting with interpolation.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>2.1 Phrase-Based System</HEADER>
        <P>
          <S ID="S-43086">Our phrase-based translation system is similar to the one described in (Zens and Ney, 2008).</S>
          <S ID="S-43087">Phrase pairs are extracted from a word-aligned bilingual corpus and their translation probability in both directions is estimated by relative frequencies.</S>
          <S ID="S-43088">Additional models include a standard n-gram language model, phrase-level IBM1, word-, phraseand distortion-penalties and a discriminative reordering model as described in (<REF ID="R-13" RPTR="12">Zens and Ney, 2006</REF>).</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.2 Hierarchical System</HEADER>
        <P>
          <S ID="S-43089">Our hierarchical phrase-based system is similar to the one described in (<REF ID="R-00" RPTR="0">Chiang, 2007</REF>).</S>
          <S ID="S-43090">It allows for gaps in the phrases by employing a context-free grammar and a CYK-like parsing during the decoding step.</S>
          <S ID="S-43091">It has similar features as the phrasebased system mentioned above.</S>
          <S ID="S-43092">For some systems, we only allowed the non-terminals in hierarchical phrases to be substituted with initial phrases as in (Iglesias et al., 2009), which gave better results on some language pairs.</S>
          <S ID="S-43093">We will refer to this as &#8220;shallow rules&#8221;.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.3 System Combination</HEADER>
        <P>
          <S ID="S-43094">The RWTH approach to MT system combination of the French&#8594;English systems as well as the German&#8594;English systems is a refined version of the ROVER approach in ASR (<REF ID="R-02" RPTR="2">Fiscus, 1997</REF>) with</S>
        </P>
        <P>
          <S ID="S-43095">additional steps to cope with reordering between different hypotheses, and to use true casing information from the input hypotheses.</S>
          <S ID="S-43096">The basic concept of the approach has been described by <REF ID="R-06" RPTR="5">Matusov et al. (2006)</REF>.</S>
          <S ID="S-43097">Several improvements have been added later (<REF ID="R-07" RPTR="6">Matusov et al., 2008</REF>).</S>
          <S ID="S-43098">This approach includes an enhanced alignment and reordering framework.</S>
          <S ID="S-43099">Alignments between the systems are learned by GIZA++, a one-to-one alignment is generated from the learned state occupation probabilities.</S>
          <S ID="S-43100">From these alignments, a confusion network (CN) is then built using one of the hypotheses as &#8220;skeleton&#8221; or &#8220;primary&#8221; hypothesis.</S>
          <S ID="S-43101">We do not make a hard decision on which of the hypotheses to use for that, but instead combine all possible CNs into a single lattice.</S>
          <S ID="S-43102">Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statistical models such as a special trigram language model.</S>
          <S ID="S-43103">This language model is also learned on the input hypotheses.</S>
          <S ID="S-43104">The intention is to favor longer phrases contained in individual hypotheses.</S>
          <S ID="S-43105">The translation with the best total score within this lattice is selected as consensus translation.</S>
          <S ID="S-43106">Scaling factors of these models are optimized similar to MERT using the Downhill Simplex algorithm.</S>
          <S ID="S-43107">As the objective function for this optimization, we selected a linear combination of BLEU and TER with a weight of 2 on the former; a combination that has proven to deliver stable results on several MT evaluation measures in preceding experiments.</S>
        </P>
        <P>
          <S ID="S-43108">In contrast to previous years, we now include a separate consensus true casing step to exploit the true casing capabilities of some of the input systems: After generating a (lower cased) consensus translation from the CN, we sum up the counts of different casing variants of each word in a sentence over the input hypotheses, and use the majority casing over those.</S>
          <S ID="S-43109">In previous experiments, this showed to work significantly better than using a fixed non-consensus true caser, and maintains flexibility on the input systems.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 New Additional Models</HEADER>
      <P>
        <S ID="S-43148"></S>
      </P>
      <DIV DEPTH="1">
        <HEADER>3.1 Forced Alignment</HEADER>
        <P>
          <S ID="S-43113">For the German&#8594;English, French&#8594;English and English&#8594;French language tasks we applied a forced alignment procedure to train the phrase translation model with the EM algorithm, similar to the one described in (<REF ID="R-01" RPTR="1">DeNero et al., 2006</REF>).</S>
          <S ID="S-43114">Here, the phrase translation probabilities are estimated from their relative frequencies in the phrase-aligned training data.</S>
          <S ID="S-43115">The phrase alignment is produced by a modified version of the translation decoder.</S>
          <S ID="S-43116">In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid experiments.</S>
          <S ID="S-43117">For the language pairs German&#8594;English and English&#8594;French the best results were achieved by log-linear interpolation of the standard phrase table with the generative model.</S>
          <S ID="S-43118">For French&#8594;English we directly used the model trained by forced alignment.</S>
          <S ID="S-43119">A detailed description of the training procedure is given in (<REF ID="R-12" RPTR="11">Wuebker et al., 2010</REF>).</S>
          <S ID="S-43120">Table 1 shows the system performances and phrase table sizes with the standard phrase table and the one trained with forced alignment after the first EM iteration.</S>
          <S ID="S-43121">We can see that the generative model reduces the phrase table size by 85-90% while increasing performance by 0.3% to 0.4% BLEU.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.2 Extended Lexicon Models</HEADER>
        <P>
          <S ID="S-43122">In previous work, RWTH was able to show the positive impact of extended lexicon models that cope with lexical context beyond the limited horizon of phrase pairs and n-gram language models.</S>
          <S ID="S-43123"><REF ID="R-08" RPTR="7">Mauser et al. (2009)</REF> report improvements of up to +1% in BLEU on large-scale systems for Chinese&#8594;English and Arabic&#8594;English by incorporating discriminative and trigger-based lexicon models into a state-of-the-art phrase-based decoder.</S>
          <S ID="S-43124">They discuss how the two types of lexicon</S>
        </P>
        <P>
          <S ID="S-43125">models help to select content words by capturing long-distance effects.</S>
        </P>
        <P>
          <S ID="S-43126">The triplet model is a straightforward extension of the IBM model 1 with a second trigger, and like the former is trained iteratively using the EM algorithm.</S>
          <S ID="S-43127">In search, the triggers are usually on the source side, i.e., p(e|f, f &#8242; ) is modeled.</S>
          <S ID="S-43128">The pathconstrained triplet model restricts the first source trigger to the aligned target word, whereas the second trigger can move along the whole source sentence.</S>
          <S ID="S-43129">See (<REF ID="R-03" RPTR="3">Hasan et al., 2008</REF>) for a detailed description and variants of the model and its training.</S>
          <S ID="S-43130">For the WMT 2010 evaluation, triplets modeling p(e|f, f &#8242; ) were trained and applied directly in search for all relevant language pairs.</S>
          <S ID="S-43131">Path-constrained models were trained on the indomain news-commentary data only and on the news-commentary plus the Europarl data.</S>
          <S ID="S-43132">Although experience from similar setups indicates that triplet lexicon models can be beneficial for machine translation between the languages English, French, and German, on this year&#8217;s WMT translation tasks slight improvements on the development sets did not or only partially carry over to the held-out test sets.</S>
          <S ID="S-43133">Nevertheless, systems with triplets were used for system combination, as extended lexicon models often help to predict content words and to capture long-range dependencies.</S>
          <S ID="S-43134">Thus they can help to find a strong consensus hypothesis.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.3 Unsupervised Training</HEADER>
        <P>
          <S ID="S-43135">Due to the small size of the English&#8594;German resources available for language modeling as well as for lexicon extraction, we decided to apply the unsupervised adaptation suggested in (<REF ID="R-11" RPTR="10">Schwenk and Senellart, 2009</REF>).</S>
          <S ID="S-43136">We use a baseline SMT system to translate in-domain monolingual source data, filter the translations according to a decoder score normalized by sentence length, add this synthetic bilingual data to the original one and rebuild the SMT system from scratch.</S>
          <S ID="S-43137">The motivation behind the method is that the phrase table will adapt to the genre, and thus let phrases which are domain related have higher probabilities.</S>
          <S ID="S-43138">Two phenomena are observed from phrase tables and the corresponding translations:</S>
        </P>
        <P>
          <S ID="S-43139">&#8226; Phrase translation probabilities are changed, making the system choose better phrase translation candidates.</S>
        </P>
        <P>
          <S ID="S-43140">&#8226; Phrases which appear repeatedly in the domain get higher probabilities, so that the decoder can better segment the sentence.</S>
        </P>
        <P>
          <S ID="S-43141">To implement this idea, we translate the AFP part of the English LDC Gigaword v4.0 and obtain the synthetic data.</S>
        </P>
        <P>
          <S ID="S-43142">To decrease the number of OOV words, we use dictionaries from the stardict directory as additional bilingual data to translate the AFP corpus.</S>
          <S ID="S-43143">We filter sentences with OOV words and sentences longer than 100 tokens.</S>
          <S ID="S-43144">A summary of the additional data used is shown in Table 2.</S>
        </P>
        <P>
          <S ID="S-43145">We tried to use the best 10%, 20% and 40% of the synthetic data, where the 40% option worked best.</S>
          <S ID="S-43146">A summary of the results is given in Table 3.</S>
        </P>
        <P>
          <S ID="S-43147">Although this is our best result for the English&#8594;German task, it was not submitted, because the use of the dictionary is not allowed in the constrained track.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 Preprocessing</HEADER>
      <P>
        <S ID="S-43167"></S>
      </P>
      <DIV DEPTH="1">
        <HEADER>4.1 Large Parallel Data</HEADER>
        <P>
          <S ID="S-43149">In addition to the provided parallel Europarl and news-commentary corpora, also the large French- English news corpus (about 22.5 Mio.</S>
          <S ID="S-43150">sentence pairs) and the French-English UN corpus (about 7.2 Mio.</S>
          <S ID="S-43151">sentence pairs) were available.</S>
          <S ID="S-43152">Since model training and tuning with such large corpora takes a very long time, we extracted about 2 Mio.</S>
          <S ID="S-43153">sentence pairs of both of these corpora.</S>
          <S ID="S-43154">We filter sentences with the following properties:</S>
        </P>
        <P>
          <S ID="S-43155">&#8226; Only sentences of minimum length of 4 tokens were considered.</S>
        </P>
        <P>
          <S ID="S-43156">&#8226; At least 92% of the vocabulary of each sentence occur in the development set.</S>
        </P>
        <P>
          <S ID="S-43157">&#8226; The ratio of the vocabulary size of a sentence and the number of its tokens is minimum 80%.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>4.2 Morpho-Syntactic Analysis</HEADER>
        <P>
          <S ID="S-43158">German, as a flexible and morphologically rich language, raises a couple of problems in machine translation.</S>
          <S ID="S-43159">We picked two major problems and tackled them with morpho-syntactic pre- and postprocessing: compound splitting and long-range verb reordering.</S>
          <S ID="S-43160">For the translation from German into English, German compound words were split using the frequency-based method described in (<REF ID="R-05" RPTR="4">Koehn and Knight, 2003</REF>).</S>
          <S ID="S-43161">Thereby, we forbid certain words and syllables to be split.</S>
          <S ID="S-43162">For the other translation direction, the English text was first translated into the modified German language with split compounds.</S>
          <S ID="S-43163">The generated output was then postprocessed by re-merging the previously generated components using the method described in (<REF ID="R-10" RPTR="9">Popovi&#263; et al., 2006</REF>).</S>
        </P>
        <P>
          <S ID="S-43164">Additionally, for the German&#8594;English phrasebased system, the long-range POS-based reordering rules described in (<REF ID="R-09" RPTR="8">Popovi&#263; and Ney, 2006</REF>) were applied on the training and test corpora as a preprocessing step.</S>
          <S ID="S-43165">Thereby, German verbs which occur at the end of a clause, like infinitives and past participles, are moved towards the beginning of that clause.</S>
          <S ID="S-43166">With this, we improved our baseline phrase-based system by 0.6% BLEU.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Experimental Results</HEADER>
      <P>
        <S ID="S-43168">For all translation directions, we used the provided parallel corpora (Europarl, news) to train the translation models and the monolingual corpora to train</S>
      </P>
      <P>
        <S ID="S-43169">the language models.</S>
        <S ID="S-43170">We improved the French- English systems by enriching the data with parts of the large addional data, extracted with the method described in Section 4.1.</S>
        <S ID="S-43171">Depending on the system this gave an improvement of 0.2-0.7% BLEU.</S>
        <S ID="S-43172">We also made use of the large giga-news as well as the LDC Gigaword corpora for the French and English language models.</S>
        <S ID="S-43173">All systems were optimized for BLEU score on the development data, newstest2008.</S>
        <S ID="S-43174">The newstest2009 data is used as a blind test set.</S>
      </P>
      <P>
        <S ID="S-43175">In the following, we will give the BLEU scores for all language tasks of the baseline system and the best setup for both, the phrase-based and the hierarchical system.</S>
        <S ID="S-43176">We will use the following notations to indicate the several methods we used:</S>
      </P>
      <P>
        <S ID="S-43177">(+POS) (+mero) (+giga)</S>
      </P>
      <P>
        <S ID="S-43178">(fa) (shallow) POS-based verb reordering maximum entropy reordering including giga-news and LDC Gigaword in LM trained by forced alignment allow only shallow rules</S>
      </P>
      <P>
        <S ID="S-43179">We applied system combination of up to 6 systems with several setups.</S>
        <S ID="S-43180">The submitted systems are marked in tables 4-7.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>6 Conclusion</HEADER>
      <P>
        <S ID="S-43181">For the participation in the WMT 2010 shared translation task, RWTH used state-of-the-art phrase-based and hierarchical translation systems.</S>
        <S ID="S-43182">To deal with the rich morphology and word order differences in German, compound splitting and long range verb reordering were applied in a preprocessing step.</S>
        <S ID="S-43183">For the French-English language pairs, RWTH extracted parts of the large news corpus and the UN corpus as additional training data.</S>
        <S ID="S-43184">Further, training the phrase translation model with forced alignment yielded improvements in BLEU.</S>
        <S ID="S-43185">To obtain the final hypothesis for the French&#8594;English and German&#8594;English</S>
      </P>
      <P>
        <S ID="S-43186">language pairs, RWTH applied system combination.</S>
        <S ID="S-43187">Altogether, by application of these methods RWTH was able to increase performance in BLEU by 0.8% for German&#8594;English, 0.2% for English&#8594;German, 1.0% for French&#8594;English and 1.4% for English&#8594;French on the test set over the respective baseline systems.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS>
    <P>
      <S ID="S-43188">This work was realized as part of the Quaero Programme, funded by OSEO, French State agency for innovation.</S>
    </P>
  </ACKNOWLEDGMENTS>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>D Chiang</RAUTHOR>
      <REFTITLE>Hierarchical Phrase-Based Translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>J DeNero</RAUTHOR>
      <REFTITLE>Why Generative Phrase Models Underperform Surface Heuristics.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>J G Fiscus</RAUTHOR>
      <REFTITLE>A Post-Processing System to Yield Reduced Word Error Rates: Recognizer Output Voting Error Reduction (ROVER).</REFTITLE>
      <DATE>1997</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>S Hasan</RAUTHOR>
      <REFTITLE>Triplet Lexicon Models for Statistical Machine Translation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>G Iglesias</RAUTHOR>
      <REFTITLE>Rule Filtering by Pattern for Efficient Hierarchical Translation.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>P Koehn</RAUTHOR>
      <REFTITLE>Empirical Methods for Compound Splitting.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>E Matusov</RAUTHOR>
      <REFTITLE>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>E Matusov</RAUTHOR>
      <REFTITLE>System Combination for Machine Translation of Spoken and Written Language.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>A Mauser</RAUTHOR>
      <REFTITLE>Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>M Popovi&#263;</RAUTHOR>
      <REFTITLE>POS-based Word Reorderings for Statistical Machine Translation.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>M Popovi&#263;</RAUTHOR>
      <REFTITLE>Statistical Machine Translation of German Compound Words.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>H Schwenk</RAUTHOR>
      <REFTITLE>Translation Model Adaptation for an Arabic/French News Translation System by Lightly-Supervised Training.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>J Wuebker</RAUTHOR>
      <REFTITLE>Training Phrase Translation Models with Leaving-One-Out.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>R Zens</RAUTHOR>
      <REFTITLE>Discriminative Reordering Models for Statistical Machine Translation.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
