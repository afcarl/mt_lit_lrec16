<document>
  <filename>P13-2061</filename>
  <authors/>
  <title>Bilingual Data Cleaning for SMT using Graph-based Random Walk &#8727;</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>The quality of bilingual data is a key factor in Statistical Machine Translation (SMT). Low-quality bilingual data tends to produce incorrect translation knowledge and also degrades translation modeling performance. Previous work often used supervised learning methods to filter lowquality data, but a fair amount of human labeled examples are needed which are not easy to obtain. To reduce the reliance on labeled examples, we propose an unsupervised method to clean bilingual data. The method leverages the mutual reinforcement between the sentence pairs and the extracted phrase pairs, based on the observation that better sentence pairs often lead to better phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The quality of bilingual data is a key factor in Statistical Machine Translation (SMT).</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Low-quality bilingual data tends to produce incorrect translation knowledge and also degrades translation modeling performance.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Previous work often used supervised learning methods to filter lowquality data, but a fair amount of human labeled examples are needed which are not easy to obtain.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>To reduce the reliance on labeled examples, we propose an unsupervised method to clean bilingual data.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The method leverages the mutual reinforcement between the sentence pairs and the extracted phrase pairs, based on the observation that better sentence pairs often lead to better phrase extraction and vice versa.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks.</text>
              <doc_id>5</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Statistical machine translation (SMT) depends on the amount of bilingual data and its quality. In real-world SMT systems, bilingual data is often mined from the web where low-quality data is inevitable. The low-quality bilingual data degrades the quality of word alignment and leads to the incorrect phrase pairs, which will hurt the translation performance of phrase-based SMT systems (Koehn et al., 2003; Och and Ney, 2004). Therefore, it is very important to exploit data quality information to improve the translation modeling.
Previous work on bilingual data cleaning often involves some supervised learning methods. Several bilingual data mining systems (Resnik and
&#8727;
This work has been done while the first author was visiting Microsoft Research Asia.
Smith, 2003; Shi et al., 2006; Munteanu and Marcu, 2005; Jiang et al., 2009) have a postprocessing step for data cleaning. Maximum entropy or SVM based classifiers are built to filter some non-parallel data or partial-parallel data. Although these methods can filter some low-quality bilingual data, they need sufficient human labeled training instances to build the model, which may not be easy to acquire. To this end, we propose an unsupervised approach to clean the bilingual data. It is intuitive that high-quality parallel data tends to produce better phrase pairs than low-quality data. Meanwhile, it is also observed that the phrase pairs that appear frequently in the bilingual corpus are more reliable than less frequent ones because they are more reusable, hence most good sentence pairs are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010). This kind of mutual reinforcement fits well into the framework of graph-based random walk. When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p. The higher the number of votes a phrase pair has, the more reliable of the phrase pair. Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s.
In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance score of each sentence pair that indicates its quality: the higher the better. Unlike other data filtering methods, our proposed method utilizes the importance scores of sentence pairs as fractional counts to calculate the phrase translation probabilities based on Maximum Likelihood Estimation (MLE), thereby none of the bilingual data is filtered out. Experimental results show that our proposed approach substantially improves the performance in large-scale Chinese-to-English translation tasks.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Statistical machine translation (SMT) depends on the amount of bilingual data and its quality.</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In real-world SMT systems, bilingual data is often mined from the web where low-quality data is inevitable.</text>
              <doc_id>7</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The low-quality bilingual data degrades the quality of word alignment and leads to the incorrect phrase pairs, which will hurt the translation performance of phrase-based SMT systems (Koehn et al., 2003; Och and Ney, 2004).</text>
              <doc_id>8</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, it is very important to exploit data quality information to improve the translation modeling.</text>
              <doc_id>9</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Previous work on bilingual data cleaning often involves some supervised learning methods.</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Several bilingual data mining systems (Resnik and</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8727;</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>This work has been done while the first author was visiting Microsoft Research Asia.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Smith, 2003; Shi et al., 2006; Munteanu and Marcu, 2005; Jiang et al., 2009) have a postprocessing step for data cleaning.</text>
              <doc_id>14</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Maximum entropy or SVM based classifiers are built to filter some non-parallel data or partial-parallel data.</text>
              <doc_id>15</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Although these methods can filter some low-quality bilingual data, they need sufficient human labeled training instances to build the model, which may not be easy to acquire.</text>
              <doc_id>16</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>To this end, we propose an unsupervised approach to clean the bilingual data.</text>
              <doc_id>17</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>It is intuitive that high-quality parallel data tends to produce better phrase pairs than low-quality data.</text>
              <doc_id>18</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Meanwhile, it is also observed that the phrase pairs that appear frequently in the bilingual corpus are more reliable than less frequent ones because they are more reusable, hence most good sentence pairs are prone to contain more frequent phrase pairs (Foster et al., 2006; Wuebker et al., 2010).</text>
              <doc_id>19</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>This kind of mutual reinforcement fits well into the framework of graph-based random walk.</text>
              <doc_id>20</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>When a phrase pair p is extracted from a sentence pair s, s is considered casting a vote for p.</text>
              <doc_id>21</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>The higher the number of votes a phrase pair has, the more reliable of the phrase pair.</text>
              <doc_id>22</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Similarly, the quality of the sentence pair s is determined by the number of votes casted by the extracted phrase pairs from s.</text>
              <doc_id>23</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this paper, a PageRank-style random walk algorithm (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007) is conducted to iteratively compute the importance score of each sentence pair that indicates its quality: the higher the better.</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Unlike other data filtering methods, our proposed method utilizes the importance scores of sentence pairs as fractional counts to calculate the phrase translation probabilities based on Maximum Likelihood Estimation (MLE), thereby none of the bilingual data is filtered out.</text>
              <doc_id>25</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Experimental results show that our proposed approach substantially improves the performance in large-scale Chinese-to-English translation tasks.</text>
              <doc_id>26</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 The Proposed Approach</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Graph-based random walk</title>
            <text>Graph-based random walk is a general algorithm to approximate the importance of a vertex within the graph in a global view. In our method, the vertices denote the sentence pairs and phrase pairs. The importance of each vertex is propagated to other vertices along the edges. Depending on different scenarios, the graph can take directed or undirected, weighted or un-weighted forms. Starting from the initial scores assigned in the graph, the algorithm is applied to recursively compute the importance scores of vertices until it converges, or the difference between two consecutive iterations falls below a pre-defined threshold.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Graph-based random walk is a general algorithm to approximate the importance of a vertex within the graph in a global view.</text>
                  <doc_id>28</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In our method, the vertices denote the sentence pairs and phrase pairs.</text>
                  <doc_id>29</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The importance of each vertex is propagated to other vertices along the edges.</text>
                  <doc_id>30</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Depending on different scenarios, the graph can take directed or undirected, weighted or un-weighted forms.</text>
                  <doc_id>31</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Starting from the initial scores assigned in the graph, the algorithm is applied to recursively compute the importance scores of vertices until it converges, or the difference between two consecutive iterations falls below a pre-defined threshold.</text>
                  <doc_id>32</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Graph construction</title>
            <text>Given the sentence pairs that are word-aligned automatically, an undirected, weighted bipartite graph is constructed which maps the sentence pairs and the extracted phrase pairs to the vertices. An edge between a sentence pair vertex and a phrase pair vertex is added if the phrase pair can be extracted from the sentence pair. Mutual reinforcement scores are defined on edges, through which the importance scores are propagated between vertices. Figure 1 illustrates the graph structure. Formally, the bipartite graph is defined as:
G = (V, E)
where V = S &#8746; P is the vertex set, S = {s i |1 &#8804; i &#8804; n} is the set of all sentence pairs. P = {p j |1 &#8804; j &#8804; m} is the set of all phrase pairs which are extracted from S based on the word alignment. E is the edge set in which the edges are between S and P , thereby E = {&#12296;s i , p j &#12297;|s i &#8712; S, p j &#8712; P, &#966;(s i , p j ) = 1}. {
1 if p j can be extracted from s i &#966;(s i , p j ) = 0 otherwise</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Given the sentence pairs that are word-aligned automatically, an undirected, weighted bipartite graph is constructed which maps the sentence pairs and the extracted phrase pairs to the vertices.</text>
                  <doc_id>33</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>An edge between a sentence pair vertex and a phrase pair vertex is added if the phrase pair can be extracted from the sentence pair.</text>
                  <doc_id>34</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Mutual reinforcement scores are defined on edges, through which the importance scores are propagated between vertices.</text>
                  <doc_id>35</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 1 illustrates the graph structure.</text>
                  <doc_id>36</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Formally, the bipartite graph is defined as:</text>
                  <doc_id>37</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>G = (V, E)</text>
                  <doc_id>38</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where V = S &#8746; P is the vertex set, S = {s i |1 &#8804; i &#8804; n} is the set of all sentence pairs.</text>
                  <doc_id>39</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>P = {p j |1 &#8804; j &#8804; m} is the set of all phrase pairs which are extracted from S based on the word alignment.</text>
                  <doc_id>40</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>E is the edge set in which the edges are between S and P , thereby E = {&#12296;s i , p j &#12297;|s i &#8712; S, p j &#8712; P, &#966;(s i , p j ) = 1}.</text>
                  <doc_id>41</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>{</text>
                  <doc_id>42</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 if p j can be extracted from s i &#966;(s i , p j ) = 0 otherwise</text>
                  <doc_id>43</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>2.3 Graph parameters</title>
            <text>For sentence-phrase mutual reinforcement, a nonnegative score r(s i , p j ) is defined using the standard TF-IDF formula:
Sentence Pair Vertices
Phrase Pair Vertices
When the random walk runs on some large bilingual corpora, even filtering phrase pairs that ap- r(s i , p j ) = {
P F (s i ,p j )&#215;IP F (p j )
&#8721;p &#8242; &#8712;{p|&#966;(s i ,p)=1} P F (s i,p &#8242; )&#215;IP F (p &#8242; )
if &#966;(s i , p j ) = 1 pear only once would still require several days of CPU time for a number of iterations. To overcome this problem, we use a distributed
0 otherwise algorithm
s1
s2
s3
where P F (s i , p j ) is the phrase pair frequency in a sentence pair and IP F (p j ) is the inverse phrase pair frequency of p j in the whole bilingual corpus. r(s i , p j ) is abbreviated as r ij . Inspired by (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007), we compute the importance scores of sentence pairs and phrase pairs using a PageRank-style algorithm. The weights r ij are leveraged to reflect the relationships between two types of vertices. Let u(s i ) and v(p j ) denote the scores of a sentence pair vertex and a phrase pair vertex. They are computed iteratively by:
u(s i ) = (1 &#8722; d) + d &#215;
v(p j ) = (1 &#8722; d) + d &#215;
&#8721;
j&#8712;N(s i )
&#8721;
j&#8712;M(p j )
p1
p2
p3
p4
p5
p6
r ij &#8721;
k&#8712;M(p j ) r kj
r ij
v(p j ) &#8721;
k&#8712;N(s i ) r u(s i ) ik
where d is empirically set to the default value 0.85 that is same as the original PageRank, N(s i ) = {j|&#12296;s i , p j &#12297; &#8712; E}, M(p j ) = {i|&#12296;s i , p j &#12297; &#8712; E}. The detailed process is illustrated in Algorithm 1. Algorithm 1 iteratively updates the scores of sentence pairs and phrase pairs (lines 10-26). The computation ends when difference between two consecutive iterations is lower than a pre-defined threshold &#948; (10 &#8722;12 in this study).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>For sentence-phrase mutual reinforcement, a nonnegative score r(s i , p j ) is defined using the standard TF-IDF formula:</text>
                  <doc_id>44</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Sentence Pair Vertices</text>
                  <doc_id>45</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Phrase Pair Vertices</text>
                  <doc_id>46</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>When the random walk runs on some large bilingual corpora, even filtering phrase pairs that ap- r(s i , p j ) = {</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P F (s i ,p j )&#215;IP F (p j )</text>
                  <doc_id>48</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;p &#8242; &#8712;{p|&#966;(s i ,p)=1} P F (s i,p &#8242; )&#215;IP F (p &#8242; )</text>
                  <doc_id>49</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>if &#966;(s i , p j ) = 1 pear only once would still require several days of CPU time for a number of iterations.</text>
                  <doc_id>50</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To overcome this problem, we use a distributed</text>
                  <doc_id>51</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0 otherwise algorithm</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>s1</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>s2</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>s3</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where P F (s i , p j ) is the phrase pair frequency in a sentence pair and IP F (p j ) is the inverse phrase pair frequency of p j in the whole bilingual corpus.</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>r(s i , p j ) is abbreviated as r ij .</text>
                  <doc_id>57</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Inspired by (Brin and Page, 1998; Mihalcea and Tarau, 2004; Wan et al., 2007), we compute the importance scores of sentence pairs and phrase pairs using a PageRank-style algorithm.</text>
                  <doc_id>58</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The weights r ij are leveraged to reflect the relationships between two types of vertices.</text>
                  <doc_id>59</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Let u(s i ) and v(p j ) denote the scores of a sentence pair vertex and a phrase pair vertex.</text>
                  <doc_id>60</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>They are computed iteratively by:</text>
                  <doc_id>61</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u(s i ) = (1 &#8722; d) + d &#215;</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>v(p j ) = (1 &#8722; d) + d &#215;</text>
                  <doc_id>63</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j&#8712;N(s i )</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;</text>
                  <doc_id>66</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j&#8712;M(p j )</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p1</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p2</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p3</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p4</text>
                  <doc_id>71</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p5</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p6</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r ij &#8721;</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k&#8712;M(p j ) r kj</text>
                  <doc_id>75</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r ij</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>v(p j ) &#8721;</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k&#8712;N(s i ) r u(s i ) ik</text>
                  <doc_id>78</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where d is empirically set to the default value 0.85 that is same as the original PageRank, N(s i ) = {j|&#12296;s i , p j &#12297; &#8712; E}, M(p j ) = {i|&#12296;s i , p j &#12297; &#8712; E}.</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The detailed process is illustrated in Algorithm 1.</text>
                  <doc_id>80</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Algorithm 1 iteratively updates the scores of sentence pairs and phrase pairs (lines 10-26).</text>
                  <doc_id>81</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The computation ends when difference between two consecutive iterations is lower than a pre-defined threshold &#948; (10 &#8722;12 in this study).</text>
                  <doc_id>82</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>2.4 Parallelization</title>
            <text>Algorithm 1 Modified Random Walk
1: for all i &#8712; {0 . . . |S| &#8722; 1} do 2: u(s i) (0) &#8592; 1 3: end for 4: for all j &#8712; {0 . . . |P | &#8722; 1} do 5: v(p j) (0) &#8592; 1 6: end for 7: &#948; &#8592; Infinity 8: &#603; &#8592; threshold 9: n &#8592; 1 10: while &#948; &gt; &#603; do 11: for all i &#8712; {0 . . . |S| &#8722; 1} do 12: F (s i) &#8592; 0
22: end for 23: v(p j) (n) &#8592; (1 &#8722; d) + d &#183; G(p j) 24: end for 25: &#948; &#8592; max(&#9651;u(s i)| |S|&#8722;1 i=1 , &#9651;v(p j)| 26: n &#8592; n + 1 27: end while 28: return u(s i) (n) | |S|&#8722;1 i=0
r ij &#8721;k&#8712;M(p j ) r kj &#183; v(pj)(n&#8722;1)
r ij &#8721;k&#8712;N(s i ) r ik &#183; u(si)(n&#8722;1)
|P |&#8722;1 j=1 )
based on the iterative computation in the Section 2.3. Before the iterative computation starts, the sum of the outlink weights for each vertex is computed first. The edges are randomly partitioned into sets of roughly equal size. Each edge &#12296;s i , p j &#12297; can generate two key-value pairs in the format &#12296;s i , r ij &#12297; and &#12296;p j , r ij &#12297;. The pairs with the same key are summed locally and accumulated across different machines. Then, in each iteration, the score of each vertex is updated according to the sum of the normalized inlink weights. The key-value pairs are generated in the format &#12296;s i , ij &#183; v(p j)&#12297;
r
and
&#12296;p j ,
&#8721;
k&#8712;M(p j ) r kj
r ij &#8721;
k&#8712;N(s i ) r ik &#183; u(s i)&#12297;. These key-value pairs
are also randomly partitioned and summed across different machines. Since long sentence pairs usually extract more phrase pairs, we need to normalize the importance scores based on the sentence length. The algorithm fits well into the MapReduce programming model (Dean and Ghemawat, 2008) and we use it as our implementation.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Algorithm 1 Modified Random Walk</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1: for all i &#8712; {0 .</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>85</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>86</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>|S| &#8722; 1} do 2: u(s i) (0) &#8592; 1 3: end for 4: for all j &#8712; {0 .</text>
                  <doc_id>87</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>88</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>89</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>|P | &#8722; 1} do 5: v(p j) (0) &#8592; 1 6: end for 7: &#948; &#8592; Infinity 8: &#603; &#8592; threshold 9: n &#8592; 1 10: while &#948; &gt; &#603; do 11: for all i &#8712; {0 .</text>
                  <doc_id>90</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>91</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>92</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>|S| &#8722; 1} do 12: F (s i) &#8592; 0</text>
                  <doc_id>93</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>22: end for 23: v(p j) (n) &#8592; (1 &#8722; d) + d &#183; G(p j) 24: end for 25: &#948; &#8592; max(&#9651;u(s i)| |S|&#8722;1 i=1 , &#9651;v(p j)| 26: n &#8592; n + 1 27: end while 28: return u(s i) (n) | |S|&#8722;1 i=0</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r ij &#8721;k&#8712;M(p j ) r kj &#183; v(pj)(n&#8722;1)</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r ij &#8721;k&#8712;N(s i ) r ik &#183; u(si)(n&#8722;1)</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>|P |&#8722;1 j=1 )</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>based on the iterative computation in the Section 2.3.</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Before the iterative computation starts, the sum of the outlink weights for each vertex is computed first.</text>
                  <doc_id>99</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The edges are randomly partitioned into sets of roughly equal size.</text>
                  <doc_id>100</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Each edge &#12296;s i , p j &#12297; can generate two key-value pairs in the format &#12296;s i , r ij &#12297; and &#12296;p j , r ij &#12297;.</text>
                  <doc_id>101</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The pairs with the same key are summed locally and accumulated across different machines.</text>
                  <doc_id>102</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Then, in each iteration, the score of each vertex is updated according to the sum of the normalized inlink weights.</text>
                  <doc_id>103</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The key-value pairs are generated in the format &#12296;s i , ij &#183; v(p j)&#12297;</text>
                  <doc_id>104</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#12296;p j ,</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;</text>
                  <doc_id>108</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k&#8712;M(p j ) r kj</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r ij &#8721;</text>
                  <doc_id>110</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k&#8712;N(s i ) r ik &#183; u(s i)&#12297;.</text>
                  <doc_id>111</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These key-value pairs</text>
                  <doc_id>112</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>are also randomly partitioned and summed across different machines.</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Since long sentence pairs usually extract more phrase pairs, we need to normalize the importance scores based on the sentence length.</text>
                  <doc_id>114</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The algorithm fits well into the MapReduce programming model (Dean and Ghemawat, 2008) and we use it as our implementation.</text>
                  <doc_id>115</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>4</index>
            <title>2.5 Integration into translation modeling</title>
            <text>After sufficient number of iterations, the importance scores of sentence pairs (i.e., u(s i )) are obtained. Instead of simple filtering, we use the scores of sentence pairs as the fractional counts to re-estimate the translation probabilities of phrase pairs. Given a phrase pair p = &#12296; &#175;f, &#275;&#12297;, A( &#175;f) and B(&#275;) indicate the sets of sentences that &#175;f and &#275; appear. Then the translation probability is defined as: &#8721;
P CW ( &#175;f|&#275;) i&#8712;A( = &#175;f)&#8745;B(&#275;) u(s i) &#215; c i ( &#175;f, &#275;) &#8721;
j&#8712;B(&#275;) u(s j) &#215; c j (&#275;)
where c i (&#183;) denotes the count of the phrase or phrase pair in s i . P CW ( &#175;f|&#275;) and P CW (&#275;| &#175;f) are named as Corpus Weighting (CW) based translation probability, which are integrated into the loglinear model in addition to the conventional phrase translation probabilities (Koehn et al., 2003).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>After sufficient number of iterations, the importance scores of sentence pairs (i.e., u(s i )) are obtained.</text>
                  <doc_id>116</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Instead of simple filtering, we use the scores of sentence pairs as the fractional counts to re-estimate the translation probabilities of phrase pairs.</text>
                  <doc_id>117</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Given a phrase pair p = &#12296; &#175;f, &#275;&#12297;, A( &#175;f) and B(&#275;) indicate the sets of sentences that &#175;f and &#275; appear.</text>
                  <doc_id>118</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Then the translation probability is defined as: &#8721;</text>
                  <doc_id>119</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P CW ( &#175;f|&#275;) i&#8712;A( = &#175;f)&#8745;B(&#275;) u(s i) &#215; c i ( &#175;f, &#275;) &#8721;</text>
                  <doc_id>120</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j&#8712;B(&#275;) u(s j) &#215; c j (&#275;)</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where c i (&#183;) denotes the count of the phrase or phrase pair in s i .</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>P CW ( &#175;f|&#275;) and P CW (&#275;| &#175;f) are named as Corpus Weighting (CW) based translation probability, which are integrated into the loglinear model in addition to the conventional phrase translation probabilities (Koehn et al., 2003).</text>
                  <doc_id>123</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Experiments</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>124</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Setup</title>
            <text>We evaluated our bilingual data cleaning approach on large-scale Chinese-to-English machine translation tasks. The bilingual data we used was mainly mined from the web (Jiang et al., 2009) 1 , as well as the United Nations parallel corpus released by LDC and the parallel corpus released by China Workshop on Machine Translation (CWMT), which contain around 30 million sentence pairs in total after removing duplicated ones. The development data and testing data is shown in Table 1.
A phrase-based decoder was implemented based on inversion transduction grammar (Wu, 1997). The performance of this decoder is similar to the state-of-the-art phrase-based decoder in Moses, but the implementation is more straightforward. We use the following feature functions in the log-linear model:
1 Although supervised data cleaning has been done in the
post-processing, the corpus still contains a fair amount of noisy data based on our random sampling.
&#8226; phrase translation probabilities and lexical weights in both directions (4 features);
weijing tansuo de xin lingyu
&#26410; &#32463; &#25506; &#32034; &#30340; &#26032; &#39046; &#22495; &#26410; &#32463; &#25506; &#32034; &#30340; &#26032; &#39046; &#22495;
&#8226; 5-gram language model with Kneser-Ney smoothing (1 feature);
&#8226; lexicalized reordering model (1 feature);
&#8226; phrase count and word count (2 features).
The translation model was trained over the word-aligned bilingual corpus conducted by GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment. The language model was trained on the LDC English Gigaword Version 4.0 plus the English part of the bilingual corpus. The lexicalized reordering model (Xiong et al., 2006) was trained over the 40% randomly sampled sentence pairs from our parallel data. Case-insensitive BLEU4 (Papineni et al., 2002) was used as the evaluation metric. The parameters of the log-linear model are tuned by optimizing BLEU on the development data using MERT (Och, 2003). Statistical significance test was performed using the bootstrap re-sampling method proposed by Koehn (2004).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We evaluated our bilingual data cleaning approach on large-scale Chinese-to-English machine translation tasks.</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The bilingual data we used was mainly mined from the web (Jiang et al., 2009) 1 , as well as the United Nations parallel corpus released by LDC and the parallel corpus released by China Workshop on Machine Translation (CWMT), which contain around 30 million sentence pairs in total after removing duplicated ones.</text>
                  <doc_id>126</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The development data and testing data is shown in Table 1.</text>
                  <doc_id>127</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A phrase-based decoder was implemented based on inversion transduction grammar (Wu, 1997).</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The performance of this decoder is similar to the state-of-the-art phrase-based decoder in Moses, but the implementation is more straightforward.</text>
                  <doc_id>129</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We use the following feature functions in the log-linear model:</text>
                  <doc_id>130</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 Although supervised data cleaning has been done in the</text>
                  <doc_id>131</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>post-processing, the corpus still contains a fair amount of noisy data based on our random sampling.</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; phrase translation probabilities and lexical weights in both directions (4 features);</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>weijing tansuo de xin lingyu</text>
                  <doc_id>134</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#26410; &#32463; &#25506; &#32034; &#30340; &#26032; &#39046; &#22495; &#26410; &#32463; &#25506; &#32034; &#30340; &#26032; &#39046; &#22495;</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; 5-gram language model with Kneser-Ney smoothing (1 feature);</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; lexicalized reordering model (1 feature);</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; phrase count and word count (2 features).</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The translation model was trained over the word-aligned bilingual corpus conducted by GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to refine the symmetric word alignment.</text>
                  <doc_id>139</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The language model was trained on the LDC English Gigaword Version 4.0 plus the English part of the bilingual corpus.</text>
                  <doc_id>140</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The lexicalized reordering model (Xiong et al., 2006) was trained over the 40% randomly sampled sentence pairs from our parallel data.</text>
                  <doc_id>141</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Case-insensitive BLEU4 (Papineni et al., 2002) was used as the evaluation metric.</text>
                  <doc_id>142</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The parameters of the log-linear model are tuned by optimizing BLEU on the development data using MERT (Och, 2003).</text>
                  <doc_id>143</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Statistical significance test was performed using the bootstrap re-sampling method proposed by Koehn (2004).</text>
                  <doc_id>144</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Baseline</title>
            <text>The experimental results are shown in Table 2. In the baseline system, the phrase pairs that appear only once in the bilingual data are simply discarded because most of them are noisy. In addition, the fix-discount method in (Foster et al., 2006) for phrase table smoothing is also used. This implementation makes the baseline system perform much better and the model size is much smaller. In fact, the basic idea of our &#8221;one count&#8221; cutoff is very similar to the idea of &#8221;leaving-oneout&#8221; in (Wuebker et al., 2010). The results show
uncharted waters unexplored new areas
that the &#8221;leaving-one-out&#8221; method performs almost the same as our baseline, thereby cannot bring other benefits to the system.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The experimental results are shown in Table 2.</text>
                  <doc_id>145</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the baseline system, the phrase pairs that appear only once in the bilingual data are simply discarded because most of them are noisy.</text>
                  <doc_id>146</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, the fix-discount method in (Foster et al., 2006) for phrase table smoothing is also used.</text>
                  <doc_id>147</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This implementation makes the baseline system perform much better and the model size is much smaller.</text>
                  <doc_id>148</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In fact, the basic idea of our &#8221;one count&#8221; cutoff is very similar to the idea of &#8221;leaving-oneout&#8221; in (Wuebker et al., 2010).</text>
                  <doc_id>149</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The results show</text>
                  <doc_id>150</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>uncharted waters unexplored new areas</text>
                  <doc_id>151</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>that the &#8221;leaving-one-out&#8221; method performs almost the same as our baseline, thereby cannot bring other benefits to the system.</text>
                  <doc_id>152</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Results</title>
            <text>We evaluate the proposed bilingual data cleaning method by incorporating sentence scores into translation modeling. In addition, we also compare with several settings that filtering low-quality sentence pairs from the bilingual data based on the importance scores. The last N = { 0.25M, 0.5M, 1M } sentence pairs are filtered before the modeling process. Although the simple bilingual data filtering can improve the performance on some datasets, it is difficult to determine the border line and translation performance is fluctuated. One main reason is in the proposed random walk approach, the bilingual sentence pairs with nonliteral translations may get lower scores because they appear less frequently compared with those literal translations. Crudely filtering out these data may degrade the translation performance. For example, we have a sentence pair in the bilingual corpus shown in the left part of Figure 2. Although the translation is correct in this situation, translating the Chinese word &#8221;lingyu&#8221; to &#8221;waters&#8221; appears very few times since the common translations are &#8221;areas&#8221; or &#8221;fields&#8221;. However, simply filtering out this kind of sentence pairs may lead to some loss of native English expressions, thereby the trans-
lation performance is unstable since both nonparallel sentence pairs and non-literal but parallel sentence pairs are filtered. Therefore, we use the importance score of each sentence pair to estimate the phrase translation probabilities. It consistently brings substantial improvements compared to the baseline, which demonstrates graph-based random walk indeed improves the translation modeling performance for our SMT system.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We evaluate the proposed bilingual data cleaning method by incorporating sentence scores into translation modeling.</text>
                  <doc_id>153</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, we also compare with several settings that filtering low-quality sentence pairs from the bilingual data based on the importance scores.</text>
                  <doc_id>154</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The last N = { 0.25M, 0.5M, 1M } sentence pairs are filtered before the modeling process.</text>
                  <doc_id>155</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Although the simple bilingual data filtering can improve the performance on some datasets, it is difficult to determine the border line and translation performance is fluctuated.</text>
                  <doc_id>156</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>One main reason is in the proposed random walk approach, the bilingual sentence pairs with nonliteral translations may get lower scores because they appear less frequently compared with those literal translations.</text>
                  <doc_id>157</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Crudely filtering out these data may degrade the translation performance.</text>
                  <doc_id>158</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>For example, we have a sentence pair in the bilingual corpus shown in the left part of Figure 2.</text>
                  <doc_id>159</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Although the translation is correct in this situation, translating the Chinese word &#8221;lingyu&#8221; to &#8221;waters&#8221; appears very few times since the common translations are &#8221;areas&#8221; or &#8221;fields&#8221;.</text>
                  <doc_id>160</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>However, simply filtering out this kind of sentence pairs may lead to some loss of native English expressions, thereby the trans-</text>
                  <doc_id>161</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>lation performance is unstable since both nonparallel sentence pairs and non-literal but parallel sentence pairs are filtered.</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, we use the importance score of each sentence pair to estimate the phrase translation probabilities.</text>
                  <doc_id>163</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It consistently brings substantial improvements compared to the baseline, which demonstrates graph-based random walk indeed improves the translation modeling performance for our SMT system.</text>
                  <doc_id>164</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>3.4 Discussion</title>
            <text>In (Goutte et al., 2012), they evaluated phrasebased SMT systems trained on parallel data with different proportions of synthetic noisy data. They suggested that when collecting larger, noisy parallel data for training phrase-based SMT, cleaning up by trying to detect and remove incorrect alignments can actually degrade performance. Our experimental results confirm their findings on some datasets. Based on our method, sometimes filtering noisy data leads to unexpected results. The reason is two-fold: on the one hand, the non-literal parallel data makes false positive in noisy data detection; on the other hand, large-scale SMT systems is relatively robust and tolerant to noisy data, especially when we remove frequency- 1 phrase pairs. Therefore, we propose to integrate the importance scores when re-estimating phrase pair probabilities in this paper. The importance scores can be considered as a kind of contribution constraint, thereby high-quality parallel data contributes more while noisy parallel data contributes less.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In (Goutte et al., 2012), they evaluated phrasebased SMT systems trained on parallel data with different proportions of synthetic noisy data.</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>They suggested that when collecting larger, noisy parallel data for training phrase-based SMT, cleaning up by trying to detect and remove incorrect alignments can actually degrade performance.</text>
                  <doc_id>166</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Our experimental results confirm their findings on some datasets.</text>
                  <doc_id>167</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Based on our method, sometimes filtering noisy data leads to unexpected results.</text>
                  <doc_id>168</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The reason is two-fold: on the one hand, the non-literal parallel data makes false positive in noisy data detection; on the other hand, large-scale SMT systems is relatively robust and tolerant to noisy data, especially when we remove frequency- 1 phrase pairs.</text>
                  <doc_id>169</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, we propose to integrate the importance scores when re-estimating phrase pair probabilities in this paper.</text>
                  <doc_id>170</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The importance scores can be considered as a kind of contribution constraint, thereby high-quality parallel data contributes more while noisy parallel data contributes less.</text>
                  <doc_id>171</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Conclusion and Future Work</title>
        <text>In this paper, we develop an effective approach to clean the bilingual data using graph-based random walk. Significant improvements on several datasets are achieved in our experiments. For future work, we will extend our method to explore the relationships of sentence-to-sentence and phrase-to-phrase, which is beyond the existing sentence-to-phrase mutual reinforcement.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper, we develop an effective approach to clean the bilingual data using graph-based random walk.</text>
              <doc_id>172</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Significant improvements on several datasets are achieved in our experiments.</text>
              <doc_id>173</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For future work, we will extend our method to explore the relationships of sentence-to-sentence and phrase-to-phrase, which is beyond the existing sentence-to-phrase mutual reinforcement.</text>
              <doc_id>174</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>Acknowledgments</title>
        <text>We are especially grateful to Yajuan Duan, Hong Sun, Nan Yang and Xilun Chen for the helpful discussions. We also thank the anonymous reviewers for their insightful comments.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We are especially grateful to Yajuan Duan, Hong Sun, Nan Yang and Xilun Chen for the helpful discussions.</text>
              <doc_id>175</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We also thank the anonymous reviewers for their insightful comments.</text>
              <doc_id>176</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Development and testing data used in the experiments.</caption>
        <reference_text>None</reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>ones.</cell>
              <cell>The development data and testing data is</cell>
              <cell>The development data and testing data is</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>s</cell>
              <cell>h</cell>
              <cell>o</cell>
              <cell>w</cell>
              <cell>n</cell>
              <cell></cell>
              <cell>i</cell>
              <cell>n</cell>
              <cell></cell>
              <cell>T</cell>
              <cell>a</cell>
              <cell>b</cell>
              <cell>l</cell>
              <cell>e</cell>
              <cell></cell>
              <cell>1</cell>
              <cell>.</cell>
            </row>
            <row>
              <cell>Data Set</cell>
              <cell>#Sentences</cell>
              <cell>Source</cell>
            </row>
            <row>
              <cell>NIST 2003 (dev)</cell>
              <cell>919</cell>
              <cell>open test</cell>
            </row>
            <row>
              <cell>NIST 2005 (test)</cell>
              <cell>1,082</cell>
              <cell>open test</cell>
            </row>
            <row>
              <cell>NIST 2006 (test)</cell>
              <cell>1,664</cell>
              <cell>open test</cell>
            </row>
            <row>
              <cell>NIST 2008 (test)</cell>
              <cell>1,357</cell>
              <cell>open test</cell>
            </row>
            <row>
              <cell>CWMT 2008 (test)</cell>
              <cell>1,006</cell>
              <cell>open test</cell>
            </row>
            <row>
              <cell>In-house dataset 1 (test)</cell>
              <cell>1,002</cell>
              <cell>web data</cell>
            </row>
            <row>
              <cell>In-house dataset 2 (test)</cell>
              <cell>5,000</cell>
              <cell>web data</cell>
            </row>
            <row>
              <cell>In-house dataset 3 (test)</cell>
              <cell>2,999</cell>
              <cell>web data</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: BLEU(%) of Chinese-to-English translation tasks on multiple testing datasets (p &lt; 0.05), where &#8221;-numberM&#8221; denotes we simply filter number million low scored sentence pairs from the bilingual data and use others to extract the phrase table. &#8221;CW&#8221; means the corpus weighting feature, which incorporates sentence scores from random walk as fractional counts to re-estimate the phrase translation probabilities.</caption>
        <reference_text>In PAGE 4: ... 3.2 Baseline The experimental results are shown in  Table2 . In the baseline system, the phrase pairs that appear only once in the bilingual data are simply dis- carded because most of them are noisy....</reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell>dev</cell>
              <cell>NIST 2005</cell>
              <cell>NIST 2006</cell>
              <cell>NIST 2008</cell>
              <cell>CWMT 2008</cell>
              <cell>IH 1</cell>
              <cell>IH 2</cell>
              <cell>IH 3</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>baseline</cell>
              <cell>41.24</cell>
              <cell>37.34</cell>
              <cell>35.20</cell>
              <cell>29.38</cell>
              <cell>31.14</cell>
              <cell>24.29</cell>
              <cell>22.61</cell>
              <cell>24.19</cell>
            </row>
            <row>
              <cell>(Wuebker et al., 2010)</cell>
              <cell>41.20</cell>
              <cell>37.48</cell>
              <cell>35.30</cell>
              <cell>29.33</cell>
              <cell>31.10</cell>
              <cell>24.33</cell>
              <cell>22.52</cell>
              <cell>24.18</cell>
            </row>
            <row>
              <cell>-0.25M</cell>
              <cell>41.28</cell>
              <cell>37.62</cell>
              <cell>35.31</cell>
              <cell>29.70</cell>
              <cell>31.40</cell>
              <cell>24.52</cell>
              <cell>22.69</cell>
              <cell>24.64</cell>
            </row>
            <row>
              <cell>-0.5M</cell>
              <cell>41.45</cell>
              <cell>37.71</cell>
              <cell>35.52</cell>
              <cell>29.76</cell>
              <cell>31.77</cell>
              <cell>24.64</cell>
              <cell>22.68</cell>
              <cell>24.69</cell>
            </row>
            <row>
              <cell>-1M</cell>
              <cell>41.28</cell>
              <cell>37.41</cell>
              <cell>35.28</cell>
              <cell>29.65</cell>
              <cell>31.73</cell>
              <cell>24.23</cell>
              <cell>23.06</cell>
              <cell>24.20</cell>
            </row>
            <row>
              <cell>+CW</cell>
              <cell>41.75</cell>
              <cell>38.08</cell>
              <cell>35.84</cell>
              <cell>30.03</cell>
              <cell>31.82</cell>
              <cell>25.23</cell>
              <cell>23.18</cell>
              <cell>24.80</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Sergey Brin</author>
          <author>Lawrence Page</author>
        </authors>
        <title>The anatomy of a large-scale hypertextual web search engine.</title>
        <publication>None</publication>
        <pages>117</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Jeffrey Dean</author>
          <author>Sanjay Ghemawat</author>
        </authors>
        <title>Mapreduce: simplified data processing on large clusters.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>George Foster</author>
          <author>Roland Kuhn</author>
          <author>Howard Johnson</author>
        </authors>
        <title>Phrasetable smoothing for statistical machine translation.</title>
        <publication>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>53--61</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Cyril Goutte</author>
          <author>Marine Carpuat</author>
          <author>George Foster</author>
        </authors>
        <title>The impact of sentence alignment errors on phrase-based machine translation performance.</title>
        <publication>In Proceedings of AMTA 2012,</publication>
        <pages>None</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Long Jiang</author>
          <author>Shiquan Yang</author>
          <author>Ming Zhou</author>
          <author>Xiaohua Liu</author>
          <author>Qingsheng Zhu</author>
        </authors>
        <title>Mining bilingual data from the web with adaptively learnt patterns.</title>
        <publication>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</publication>
        <pages>870--878</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz Josef Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical phrase-based translation.</title>
        <publication>In Proceedings of HLT-NAACL 2003 Main Papers,</publication>
        <pages>48--54</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Philipp Koehn</author>
        </authors>
        <title>Statistical significance tests for machine translation evaluation.</title>
        <publication>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</publication>
        <pages>388--395</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Rada Mihalcea</author>
          <author>Paul Tarau</author>
        </authors>
        <title>Textrank: Bringing order into texts.</title>
        <publication>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</publication>
        <pages>404--411</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Dragos Stefan Munteanu</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Improving machine translation performance by exploiting non-parallel corpora.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A systematic comparison of various statistical alignment models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>The alignment template approach to statistical machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Josef Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>160--167</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>Bleu: a method for automatic evaluation of machine translation.</title>
        <publication>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>311--318</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Philip Resnik</author>
          <author>Noah A Smith</author>
        </authors>
        <title>The web as a parallel corpus.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Lei Shi</author>
          <author>Cheng Niu</author>
          <author>Ming Zhou</author>
          <author>Jianfeng Gao</author>
        </authors>
        <title>A dom tree alignment model for mining parallel data from the web.</title>
        <publication>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>489--496</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Xiaojun Wan</author>
          <author>Jianwu Yang</author>
          <author>Jianguo Xiao</author>
        </authors>
        <title>Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction.</title>
        <publication>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</publication>
        <pages>552--559</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Dekai Wu</author>
        </authors>
        <title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Joern Wuebker</author>
          <author>Arne Mauser</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Training phrase translation models with leaving-one-out.</title>
        <publication>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>475--484</pages>
        <date>2010</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Brin and Page, 1998</string>
        <sentence_id>38103</sentence_id>
        <char_offset>55</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>0</reference_id>
        <string>Brin and Page, 1998</string>
        <sentence_id>38136</sentence_id>
        <char_offset>13</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>Dean and Ghemawat, 2008</string>
        <sentence_id>38193</sentence_id>
        <char_offset>62</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>2</reference_id>
        <string>Foster et al., 2006</string>
        <sentence_id>38098</sentence_id>
        <char_offset>254</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>2</reference_id>
        <string>Foster et al., 2006</string>
        <sentence_id>38225</sentence_id>
        <char_offset>41</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>3</reference_id>
        <string>Goutte et al., 2012</string>
        <sentence_id>38243</sentence_id>
        <char_offset>4</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>4</reference_id>
        <string>Jiang et al., 2009</string>
        <sentence_id>38093</sentence_id>
        <char_offset>57</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>4</reference_id>
        <string>Jiang et al., 2009</string>
        <sentence_id>38204</sentence_id>
        <char_offset>58</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>5</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>38087</sentence_id>
        <char_offset>184</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>5</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>38201</sentence_id>
        <char_offset>208</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>6</reference_id>
        <string>Koehn (2004)</string>
        <sentence_id>38222</sentence_id>
        <char_offset>95</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>7</reference_id>
        <string>Mihalcea and Tarau, 2004</string>
        <sentence_id>38103</sentence_id>
        <char_offset>76</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>7</reference_id>
        <string>Mihalcea and Tarau, 2004</string>
        <sentence_id>38136</sentence_id>
        <char_offset>34</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>8</reference_id>
        <string>Munteanu and Marcu, 2005</string>
        <sentence_id>38093</sentence_id>
        <char_offset>31</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>9</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>38217</sentence_id>
        <char_offset>94</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>10</reference_id>
        <string>Och and Ney, 2004</string>
        <sentence_id>38087</sentence_id>
        <char_offset>204</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>11</reference_id>
        <string>Och, 2003</string>
        <sentence_id>38221</sentence_id>
        <char_offset>104</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>12</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>38220</sentence_id>
        <char_offset>24</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>14</reference_id>
        <string>Shi et al., 2006</string>
        <sentence_id>38093</sentence_id>
        <char_offset>13</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>15</reference_id>
        <string>Wan et al., 2007</string>
        <sentence_id>38103</sentence_id>
        <char_offset>102</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>15</reference_id>
        <string>Wan et al., 2007</string>
        <sentence_id>38136</sentence_id>
        <char_offset>60</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>16</reference_id>
        <string>Wu, 1997</string>
        <sentence_id>38206</sentence_id>
        <char_offset>80</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>17</reference_id>
        <string>Wuebker et al., 2010</string>
        <sentence_id>38098</sentence_id>
        <char_offset>275</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>17</reference_id>
        <string>Wuebker et al., 2010</string>
        <sentence_id>38227</sentence_id>
        <char_offset>102</char_offset>
      </citation>
    </citations>
  </content>
</document>
