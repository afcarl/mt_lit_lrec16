<document>
  <filename>E09-1044</filename>
  <authors/>
  <title>Rule Filtering by Pattern for Efficient Hierarchical Translation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>We describe refinements to hierarchical translation search procedures intended to reduce both search errors and memory usage through modifications to hypothesis expansion in cube pruning and reductions in the size of the rule sets used in translation. Rules are put into syntactic classes based on the number of non-terminals and the pattern, and various filtering strategies are then applied to assess the impact on translation speed and quality. Results are reported on the 2008 NIST Arabic-to- English evaluation task.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We describe refinements to hierarchical translation search procedures intended to reduce both search errors and memory usage through modifications to hypothesis expansion in cube pruning and reductions in the size of the rule sets used in translation.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Rules are put into syntactic classes based on the number of non-terminals and the pattern, and various filtering strategies are then applied to assess the impact on translation speed and quality.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Results are reported on the 2008 NIST Arabic-to- English evaluation task.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Hierarchical phrase-based translation (Chiang, 2005) has emerged as one of the dominant current approaches to statistical machine translation. Hiero translation systems incorporate many of the strengths of phrase-based translation systems, such as feature-based translation and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned parallel text. The approach has been widely adopted and reported to be competitive with other large-scale data driven approaches, e.g. (Zollmann et al., 2008).
Large-scale hierarchical SMT involves automatic rule extraction from aligned parallel text, model parameter estimation, and the use of cube pruning k-best list generation in hierarchical translation. The number of hierarchical rules extracted far exceeds the number of phrase translations typically found in aligned text. While this may lead to improved translation quality, there is also the risk of lengthened translation times and increased memory usage, along with possible search errors due to the pruning procedures needed in search.
We describe several techniques to reduce memory usage and search errors in hierarchical translation. Memory usage can be reduced in cube pruning (Chiang, 2007) through smart memoization, and spreading neighborhood exploration can be used to reduce search errors. However, search errors can still remain even when implementing simple phrase-based translation. We describe a &#8216;shallow&#8217; search through hierarchical rules which greatly speeds translation without any effect on quality. We then describe techniques to analyze and reduce the set of hierarchical rules. We do this based on the structural properties of rules and develop strategies to identify and remove redundant or harmful rules. We identify groupings of rules based on non-terminals and their patterns and assess the impact on translation quality and computational requirements for each given rule group. We find that with appropriate filtering strategies rule sets can be greatly reduced in size without impact on translation performance.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Hierarchical phrase-based translation (Chiang, 2005) has emerged as one of the dominant current approaches to statistical machine translation.</text>
              <doc_id>3</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Hiero translation systems incorporate many of the strengths of phrase-based translation systems, such as feature-based translation and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned parallel text.</text>
              <doc_id>4</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The approach has been widely adopted and reported to be competitive with other large-scale data driven approaches, e.g. (Zollmann et al., 2008).</text>
              <doc_id>5</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Large-scale hierarchical SMT involves automatic rule extraction from aligned parallel text, model parameter estimation, and the use of cube pruning k-best list generation in hierarchical translation.</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The number of hierarchical rules extracted far exceeds the number of phrase translations typically found in aligned text.</text>
              <doc_id>7</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>While this may lead to improved translation quality, there is also the risk of lengthened translation times and increased memory usage, along with possible search errors due to the pruning procedures needed in search.</text>
              <doc_id>8</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We describe several techniques to reduce memory usage and search errors in hierarchical translation.</text>
              <doc_id>9</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Memory usage can be reduced in cube pruning (Chiang, 2007) through smart memoization, and spreading neighborhood exploration can be used to reduce search errors.</text>
              <doc_id>10</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, search errors can still remain even when implementing simple phrase-based translation.</text>
              <doc_id>11</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We describe a &#8216;shallow&#8217; search through hierarchical rules which greatly speeds translation without any effect on quality.</text>
              <doc_id>12</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We then describe techniques to analyze and reduce the set of hierarchical rules.</text>
              <doc_id>13</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We do this based on the structural properties of rules and develop strategies to identify and remove redundant or harmful rules.</text>
              <doc_id>14</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We identify groupings of rules based on non-terminals and their patterns and assess the impact on translation quality and computational requirements for each given rule group.</text>
              <doc_id>15</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>We find that with appropriate filtering strategies rule sets can be greatly reduced in size without impact on translation performance.</text>
              <doc_id>16</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>1.1 Related Work</title>
            <text>The search and rule pruning techniques described in the following sections add to a growing literature of refinements to the hierarchical phrasebased SMT systems originally described by Chiang (2005; 2007). Subsequent work has addressed improvements and extensions to the search procedure itself, the extraction of the hierarchical rules needed for translation, and has also reported contrastive experiments with other SMT architectures. Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n- grams into account within cube pruning to minimize language model requests. Dyer et al. (2008)
extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999).
Extensions to Hiero Blunsom et al. (2008) discuss procedures to combine discriminative latent models with hierarchical SMT. The Syntax- Augmented Machine Translation system (Zollmann and Venugopal, 2006) incorporates target language syntactic constituents in addition to the synchronous grammars used in translation. Shen at al. (2008) make use of target dependency trees and a target dependency language model during decoding. Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). Zhang and Gildea (2006) propose binarization for synchronous grammars as a means to control search complexity arising from more complex, syntactic, hierarchical rules sets.
Hierarchical rule extraction Zhang et al. (2008) describe a linear algorithm, a modified version of shift-reduce, to extract phrase pairs organized into a tree from which hierarchical rules can be directly extracted. Lopez (2007) extracts rules on-the-fly from the training bitext during decoding, searching efficiently for rule patterns using suffix arrays.
Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English, and they find that attempts to expedite translation by simple schemes which discard rules also degrade translation performance. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures (e.g. (Sim et al., 2007; Rosti et al., 2007)).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The search and rule pruning techniques described in the following sections add to a growing literature of refinements to the hierarchical phrasebased SMT systems originally described by Chiang (2005; 2007).</text>
                  <doc_id>17</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Subsequent work has addressed improvements and extensions to the search procedure itself, the extraction of the hierarchical rules needed for translation, and has also reported contrastive experiments with other SMT architectures.</text>
                  <doc_id>18</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed.</text>
                  <doc_id>19</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007).</text>
                  <doc_id>20</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n- grams into account within cube pruning to minimize language model requests.</text>
                  <doc_id>21</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Dyer et al. (2008)</text>
                  <doc_id>22</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999).</text>
                  <doc_id>23</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Extensions to Hiero Blunsom et al. (2008) discuss procedures to combine discriminative latent models with hierarchical SMT.</text>
                  <doc_id>24</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The Syntax- Augmented Machine Translation system (Zollmann and Venugopal, 2006) incorporates target language syntactic constituents in addition to the synchronous grammars used in translation.</text>
                  <doc_id>25</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Shen at al. (2008) make use of target dependency trees and a target dependency language model during decoding.</text>
                  <doc_id>26</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005).</text>
                  <doc_id>27</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Zhang and Gildea (2006) propose binarization for synchronous grammars as a means to control search complexity arising from more complex, syntactic, hierarchical rules sets.</text>
                  <doc_id>28</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Hierarchical rule extraction Zhang et al. (2008) describe a linear algorithm, a modified version of shift-reduce, to extract phrase pairs organized into a tree from which hierarchical rules can be directly extracted.</text>
                  <doc_id>29</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Lopez (2007) extracts rules on-the-fly from the training bitext during decoding, searching efficiently for rule patterns using suffix arrays.</text>
                  <doc_id>30</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English, and they find that attempts to expedite translation by simple schemes which discard rules also degrade translation performance.</text>
                  <doc_id>31</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems.</text>
                  <doc_id>32</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Hierarchical translation has also been used to great effect in combination with other translation architectures (e.g. (Sim et al., 2007; Rosti et al., 2007)).</text>
                  <doc_id>33</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>1.2 Outline</title>
            <text>The paper proceeds as follows. Section 2 describes memoization and spreading neighborhood exploration in cube pruning intended to reduce memory usage and search errors, respectively. A detailed comparison with a simple phrase-based system is presented. Section 3 describes patternbased rule filtering and various procedures to select rule sets for use in translation with an aim to improving translation quality while minimizing rule set size. Finally, Section 4 concludes.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The paper proceeds as follows.</text>
                  <doc_id>34</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Section 2 describes memoization and spreading neighborhood exploration in cube pruning intended to reduce memory usage and search errors, respectively.</text>
                  <doc_id>35</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>A detailed comparison with a simple phrase-based system is presented.</text>
                  <doc_id>36</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Section 3 describes patternbased rule filtering and various procedures to select rule sets for use in translation with an aim to improving translation quality while minimizing rule set size.</text>
                  <doc_id>37</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, Section 4 concludes.</text>
                  <doc_id>38</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>2</index>
        <title>2 Two Refinements in Cube Pruning</title>
        <text>Chiang (2007) introduced cube pruning to apply language models in pruning during the generation of k-best translation hypotheses via the application of hierarchical rules in the CYK algorithm. In the implementation of Hiero described here, there is the parser itself, for which we use a variant of the CYK algorithm closely related to CYK+ (Chappelier and Rajman, 1998); it employs hypothesis recombination, without pruning, while maintaining back pointers. Before k-best list generation with cube pruning, we apply a smart memoization procedure intended to reduce memory consumption during k-best list expansion. Within the cube pruning algorithm we use spreading neighborhood exploration to improve robustness in the face of search errors.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Chiang (2007) introduced cube pruning to apply language models in pruning during the generation of k-best translation hypotheses via the application of hierarchical rules in the CYK algorithm.</text>
              <doc_id>39</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In the implementation of Hiero described here, there is the parser itself, for which we use a variant of the CYK algorithm closely related to CYK+ (Chappelier and Rajman, 1998); it employs hypothesis recombination, without pruning, while maintaining back pointers.</text>
              <doc_id>40</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Before k-best list generation with cube pruning, we apply a smart memoization procedure intended to reduce memory consumption during k-best list expansion.</text>
              <doc_id>41</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Within the cube pruning algorithm we use spreading neighborhood exploration to improve robustness in the face of search errors.</text>
              <doc_id>42</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Smart Memoization</title>
            <text>Each cell in the chart built by the CYK algorithm contains all possible derivations of a span of the source sentence being translated. After the parsing stage is completed, it is possible to make a very efficient sweep through the backpointers of the CYK grid to count how many times each cell will be accessed by the k-best generation algorithm. When k-best list generation is running, the number of times each cell is visited is logged so that, as each cell is visited for the last time, the k-best list associated with each cell is deleted. This continues until the one k-best list remaining at the top of the chart spans the entire sentence. Memory reductions are substantial for longer sentences: for the longest sentence in the tuning set described later (105 words in length), smart memoization reduces memory usage during the cube pruning stage from 2.1GB to 0.7GB. For average length sentences of approx. 30 words, memory reductions of 30% are typical.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Each cell in the chart built by the CYK algorithm contains all possible derivations of a span of the source sentence being translated.</text>
                  <doc_id>43</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>After the parsing stage is completed, it is possible to make a very efficient sweep through the backpointers of the CYK grid to count how many times each cell will be accessed by the k-best generation algorithm.</text>
                  <doc_id>44</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>When k-best list generation is running, the number of times each cell is visited is logged so that, as each cell is visited for the last time, the k-best list associated with each cell is deleted.</text>
                  <doc_id>45</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This continues until the one k-best list remaining at the top of the chart spans the entire sentence.</text>
                  <doc_id>46</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Memory reductions are substantial for longer sentences: for the longest sentence in the tuning set described later (105 words in length), smart memoization reduces memory usage during the cube pruning stage from 2.1GB to 0.7GB.</text>
                  <doc_id>47</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For average length sentences of approx.</text>
                  <doc_id>48</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>30 words, memory reductions of 30% are typical.</text>
                  <doc_id>49</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Spreading Neighborhood Exploration</title>
            <text>In generation of a k-best list of translations for a source sentence span, every derivation is transformed into a cube containing the possible translations arising from that derivation, along with their translation and language model scores (Chiang, 2007). These derivations may contain nonterminals which must be expanded based on hypotheses generated by lower cells, which them-
selves may contain non-terminals. For efficiency each cube maintains a queue of hypotheses, called here the frontier queue, ranked by translation and language model score; it is from these frontier queues that hypotheses are removed to create the k-best list for each cell. When a hypothesis is extracted from a frontier queue, that queue is updated by searching through the neighborhood of the extracted item to find novel hypotheses to add; if no novel hypotheses are found, that queue necessarily shrinks. This shrinkage can lead to search errors. We therefore require that, when a hypothesis is removed, new candidates must be added by exploring a neighborhood which spreads from the last extracted hypothesis. Each axis of the cube is searched (here, to a depth of 20) until a novel hypothesis is found. In this way, up to three new candidates are added for each entry extracted from a frontier queue. Chiang (2007) describes an initialization procedure in which these frontier queues are seeded with a single candidate per axis; we initialize each frontier queue to a depth of b Nnt+1 , where N nt is the number of non-terminals in the derivation and b is a search parameter set throughout to 10. By starting with deep frontier queues and by forcing them to grow during search we attempt to avoid search errors by ensuring that the universe of items within the frontier queues does not decrease as the k-best lists are filled.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In generation of a k-best list of translations for a source sentence span, every derivation is transformed into a cube containing the possible translations arising from that derivation, along with their translation and language model scores (Chiang, 2007).</text>
                  <doc_id>50</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These derivations may contain nonterminals which must be expanded based on hypotheses generated by lower cells, which them-</text>
                  <doc_id>51</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>selves may contain non-terminals.</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For efficiency each cube maintains a queue of hypotheses, called here the frontier queue, ranked by translation and language model score; it is from these frontier queues that hypotheses are removed to create the k-best list for each cell.</text>
                  <doc_id>53</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>When a hypothesis is extracted from a frontier queue, that queue is updated by searching through the neighborhood of the extracted item to find novel hypotheses to add; if no novel hypotheses are found, that queue necessarily shrinks.</text>
                  <doc_id>54</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This shrinkage can lead to search errors.</text>
                  <doc_id>55</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We therefore require that, when a hypothesis is removed, new candidates must be added by exploring a neighborhood which spreads from the last extracted hypothesis.</text>
                  <doc_id>56</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Each axis of the cube is searched (here, to a depth of 20) until a novel hypothesis is found.</text>
                  <doc_id>57</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>In this way, up to three new candidates are added for each entry extracted from a frontier queue.</text>
                  <doc_id>58</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Chiang (2007) describes an initialization procedure in which these frontier queues are seeded with a single candidate per axis; we initialize each frontier queue to a depth of b Nnt+1 , where N nt is the number of non-terminals in the derivation and b is a search parameter set throughout to 10.</text>
                  <doc_id>59</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>By starting with deep frontier queues and by forcing them to grow during search we attempt to avoid search errors by ensuring that the universe of items within the frontier queues does not decrease as the k-best lists are filled.</text>
                  <doc_id>60</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>2.3 A Study of Hiero Search Errors in Phrase-Based Translation</title>
            <text>Experiments reported in this paper are based on the NIST MT08 Arabic-to-English translation task. Alignments are generated over all allowed parallel data, (&#8764;150M words per language). Features extracted from the alignments and used in translation are in common use: target language model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule Figure 1: Spreading neighborhood exploration within a cube, just before and after extraction of the item C. Grey squares represent the frontier queue; black squares are candidates already extracted. Chiang (2007) would only consider adding items X to the frontier queue, so the queue would shrink. Spreading neighborhood exploration adds candidates S to the frontier queue.
count features inspired by Bender et al. (2007). MET (Och, 2003) iterative parameter estimation under IBM BLEU is performed on the development set. The English language used model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition. In addition to the MT08 set itself, we use a development set mt02- 05-tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form the validation set mt02-05-test. The mt02-05-tune set has 2,075 sentences. We first compare the cube pruning decoder to the TTM (Kumar et al., 2006), a phrase-based SMT system implemented with Weighted Finite- State Tansducers (Allauzen et al., 2007). The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005). Relative to the complex movement and translation allowed by Hiero and other models, MJ1 is clearly inferior (Dreyer et al., 2007); MJ1 was developed with efficiency in mind so as to run with a minimum of search errors in translation and to be easily and exactly realized via WFSTs. Even for the
large models used in an evaluation task, the TTM system is reported to run largely without pruning (Blackwood et al., 2008).
The Hiero decoder can easily be made to implement MJ1 reordering by allowing only a restricted set of reordering rules in addition to the usual glue rule, as shown in left-hand column of Table 1, where T is the set of terminals. Constraining Hiero in this way makes it possible to compare its performance to the exact WFST TTM implementation and to identify any search errors made by Hiero.
Table 2 shows the lowercased IBM BLEU scores obtained by the systems for mt02-05-tune with monotone and reordered search, and with MET-optimised parameters for MJ1 reordering. For Hiero, an N-best list depth of 10,000 is used throughout. In the monotone case, all phrasebased systems perform similarly although Hiero does make search errors. For simple MJ1 reordering, the basic Hiero search procedure makes many search errors and these lead to degradations in BLEU. Spreading neighborhood expansion reduces the search errors and improves BLEU score significantly but search errors remain a problem. Search errors are even more apparent after MET. This is not surprising, given that mt02-05-tune is the set over which MET is run: MET drives up the likelihood of good hypotheses at the expense of poor hypotheses, but search errors often increase due to the expanded dynamic range of the hypothesis scores.
Our aim in these experiments was to demonstrate that spreading neighborhood exploration can aid in avoiding search errors. We emphasize that we are not proposing that Hiero should be used to implement reordering models such as MJ1 which were created for completely different search procedures (e.g. WFST composition). However these experiments do suggest that search errors may be an issue, particularly as the search space grows to include the complex long-range movement allowed by the hierarchical rules. We next study various filtering procedures to reduce hierarchical rule sets to find a balance between translation speed, memory usage, and performance.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Experiments reported in this paper are based on the NIST MT08 Arabic-to-English translation task.</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Alignments are generated over all allowed parallel data, (&#8764;150M words per language).</text>
                  <doc_id>62</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Features extracted from the alignments and used in translation are in common use: target language model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule Figure 1: Spreading neighborhood exploration within a cube, just before and after extraction of the item C.</text>
                  <doc_id>63</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Grey squares represent the frontier queue; black squares are candidates already extracted.</text>
                  <doc_id>64</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Chiang (2007) would only consider adding items X to the frontier queue, so the queue would shrink.</text>
                  <doc_id>65</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Spreading neighborhood exploration adds candidates S to the frontier queue.</text>
                  <doc_id>66</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>count features inspired by Bender et al. (2007).</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>MET (Och, 2003) iterative parameter estimation under IBM BLEU is performed on the development set.</text>
                  <doc_id>68</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The English language used model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition.</text>
                  <doc_id>69</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In addition to the MT08 set itself, we use a development set mt02- 05-tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form the validation set mt02-05-test.</text>
                  <doc_id>70</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The mt02-05-tune set has 2,075 sentences.</text>
                  <doc_id>71</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We first compare the cube pruning decoder to the TTM (Kumar et al., 2006), a phrase-based SMT system implemented with Weighted Finite- State Tansducers (Allauzen et al., 2007).</text>
                  <doc_id>72</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005).</text>
                  <doc_id>73</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Relative to the complex movement and translation allowed by Hiero and other models, MJ1 is clearly inferior (Dreyer et al., 2007); MJ1 was developed with efficiency in mind so as to run with a minimum of search errors in translation and to be easily and exactly realized via WFSTs.</text>
                  <doc_id>74</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>Even for the</text>
                  <doc_id>75</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>large models used in an evaluation task, the TTM system is reported to run largely without pruning (Blackwood et al., 2008).</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The Hiero decoder can easily be made to implement MJ1 reordering by allowing only a restricted set of reordering rules in addition to the usual glue rule, as shown in left-hand column of Table 1, where T is the set of terminals.</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Constraining Hiero in this way makes it possible to compare its performance to the exact WFST TTM implementation and to identify any search errors made by Hiero.</text>
                  <doc_id>78</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 2 shows the lowercased IBM BLEU scores obtained by the systems for mt02-05-tune with monotone and reordered search, and with MET-optimised parameters for MJ1 reordering.</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For Hiero, an N-best list depth of 10,000 is used throughout.</text>
                  <doc_id>80</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In the monotone case, all phrasebased systems perform similarly although Hiero does make search errors.</text>
                  <doc_id>81</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For simple MJ1 reordering, the basic Hiero search procedure makes many search errors and these lead to degradations in BLEU.</text>
                  <doc_id>82</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Spreading neighborhood expansion reduces the search errors and improves BLEU score significantly but search errors remain a problem.</text>
                  <doc_id>83</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Search errors are even more apparent after MET.</text>
                  <doc_id>84</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>This is not surprising, given that mt02-05-tune is the set over which MET is run: MET drives up the likelihood of good hypotheses at the expense of poor hypotheses, but search errors often increase due to the expanded dynamic range of the hypothesis scores.</text>
                  <doc_id>85</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Our aim in these experiments was to demonstrate that spreading neighborhood exploration can aid in avoiding search errors.</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We emphasize that we are not proposing that Hiero should be used to implement reordering models such as MJ1 which were created for completely different search procedures (e.g. WFST composition).</text>
                  <doc_id>87</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However these experiments do suggest that search errors may be an issue, particularly as the search space grows to include the complex long-range movement allowed by the hierarchical rules.</text>
                  <doc_id>88</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We next study various filtering procedures to reduce hierarchical rule sets to find a balance between translation speed, memory usage, and performance.</text>
                  <doc_id>89</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Rule Filtering by Pattern</title>
        <text>Hierarchical rules X &#8594; &#12296;&#947;,&#945;&#12297; are composed of sequences of terminals and non-terminals, which
we call elements. In the source, a maximum of two non-adjacent non-terminals is allowed (Chiang, 2007). Leaving aside rules without nonterminals (i.e. phrase pairs as used in phrasebased translation), rules can be classed by their number of non-terminals, N nt , and their number of elements, N e . There are 5 possible classes: N nt .N e = 1.2,1.3,2.3,2.4, 2.5.
During rule extraction we search each class separately to control memory usage. Furthermore, we extract from alignments only those rules which are relevant to our given test set; for computation of backward translation probabilities we log general counts of target-side rules but discard unneeded rules. Even with this restriction, our initial ruleset for mt02-05-tune exceeds 175M rules, of which only 0.62M are simple phrase pairs.
The question is whether all these rules are needed for translation. If the rule set can be reduced without reducing translation quality, both memory efficiency and translation speed can be increased. Previously published approaches to reducing the rule set include: enforcing a minimum span of two words per non-terminal (Lopez, 2008), which would reduce our set to 115M rules; or a minimum count (mincount) threshold (Zollmann et al., 2008), which would reduce our set to 78M (mincount=2) or 57M (mincount=3) rules. Shen et al. (2008) describe the result of filtering rules by insisting that target-side rules are well-formed dependency trees. This reduces their rule set from 140M to 26M rules. This filtering leads to a degradation in translation performance (see Table 2 of Shen et al. (2008)), which they counter by adding a dependency LM in translation. As another reference point, Chiang (2007) reports Chinese-to-English translation experiments based on 5.5M rules.
Zollmann et al. (2008) report that filtering rules
en masse leads to degradation in translation performance. Rather than apply a coarse filtering, such as a mincount for all rules, we follow a more syntactic approach and further classify our rules according to their pattern and apply different filters to each pattern depending on its value in translation. The premise is that some patterns are more important than others.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Hierarchical rules X &#8594; &#12296;&#947;,&#945;&#12297; are composed of sequences of terminals and non-terminals, which</text>
              <doc_id>90</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>we call elements.</text>
              <doc_id>91</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In the source, a maximum of two non-adjacent non-terminals is allowed (Chiang, 2007).</text>
              <doc_id>92</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Leaving aside rules without nonterminals (i.e. phrase pairs as used in phrasebased translation), rules can be classed by their number of non-terminals, N nt , and their number of elements, N e .</text>
              <doc_id>93</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>There are 5 possible classes: N nt .N e = 1.2,1.3,2.3,2.4, 2.5.</text>
              <doc_id>94</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>During rule extraction we search each class separately to control memory usage.</text>
              <doc_id>95</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Furthermore, we extract from alignments only those rules which are relevant to our given test set; for computation of backward translation probabilities we log general counts of target-side rules but discard unneeded rules.</text>
              <doc_id>96</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Even with this restriction, our initial ruleset for mt02-05-tune exceeds 175M rules, of which only 0.62M are simple phrase pairs.</text>
              <doc_id>97</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The question is whether all these rules are needed for translation.</text>
              <doc_id>98</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>If the rule set can be reduced without reducing translation quality, both memory efficiency and translation speed can be increased.</text>
              <doc_id>99</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Previously published approaches to reducing the rule set include: enforcing a minimum span of two words per non-terminal (Lopez, 2008), which would reduce our set to 115M rules; or a minimum count (mincount) threshold (Zollmann et al., 2008), which would reduce our set to 78M (mincount=2) or 57M (mincount=3) rules.</text>
              <doc_id>100</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Shen et al. (2008) describe the result of filtering rules by insisting that target-side rules are well-formed dependency trees.</text>
              <doc_id>101</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This reduces their rule set from 140M to 26M rules.</text>
              <doc_id>102</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This filtering leads to a degradation in translation performance (see Table 2 of Shen et al. (2008)), which they counter by adding a dependency LM in translation.</text>
              <doc_id>103</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>As another reference point, Chiang (2007) reports Chinese-to-English translation experiments based on 5.5M rules.</text>
              <doc_id>104</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Zollmann et al. (2008) report that filtering rules</text>
              <doc_id>105</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>en masse leads to degradation in translation performance.</text>
              <doc_id>106</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Rather than apply a coarse filtering, such as a mincount for all rules, we follow a more syntactic approach and further classify our rules according to their pattern and apply different filters to each pattern depending on its value in translation.</text>
              <doc_id>107</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The premise is that some patterns are more important than others.</text>
              <doc_id>108</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Rule Patterns</title>
            <text>Given a rule set, we define source patterns and target patterns by replacing every sequence of non-terminals by a single symbol &#8216;w&#8217; (indicating word, i.e. terminal string, w &#8712; T + ). Each hierarchical rule has a unique source and target pattern which together define the rule pattern.
By ignoring the identity and the number of adjacent terminals, the rule pattern represents a natural generalization of any rule, capturing its structure and the type of reordering it encodes. In total, there are 66 possible rule patterns. Table 3 presents a few examples extracted for mt02-05- tune, showing that some patterns are much more diverse than others. For example, patterns with two non-terminals (N nt =2) are richer than patterns with N nt =1, as they cover many more distinct rules. Additionally, patterns with two nonterminals which also have a monotonic relationship between source and target non-terminals are much more diverse than their reordered counterparts.
Some examples of extracted rules and their corresponding pattern follow, where Arabic is shown in Buckwalter encoding.
Pattern &#12296;wX 1 , wX 1 w&#12297; :
&#12296;w+ qAl X 1 , the X 1 said&#12297; Pattern &#12296;wX 1 w , wX 1 &#12297; :
&#12296;fy X 1 kAnwn Al&gt;wl , on december X 1 &#12297; Pattern &#12296;wX 1 wX 2 , wX 1 wX 2 w&#12297; :
&#12296;Hl X 1 lAzmp X 2 , a X 1 solution to the X 2 crisis&#12297;</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Given a rule set, we define source patterns and target patterns by replacing every sequence of non-terminals by a single symbol &#8216;w&#8217; (indicating word, i.e. terminal string, w &#8712; T + ).</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Each hierarchical rule has a unique source and target pattern which together define the rule pattern.</text>
                  <doc_id>110</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>By ignoring the identity and the number of adjacent terminals, the rule pattern represents a natural generalization of any rule, capturing its structure and the type of reordering it encodes.</text>
                  <doc_id>111</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In total, there are 66 possible rule patterns.</text>
                  <doc_id>112</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Table 3 presents a few examples extracted for mt02-05- tune, showing that some patterns are much more diverse than others.</text>
                  <doc_id>113</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For example, patterns with two non-terminals (N nt =2) are richer than patterns with N nt =1, as they cover many more distinct rules.</text>
                  <doc_id>114</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Additionally, patterns with two nonterminals which also have a monotonic relationship between source and target non-terminals are much more diverse than their reordered counterparts.</text>
                  <doc_id>115</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Some examples of extracted rules and their corresponding pattern follow, where Arabic is shown in Buckwalter encoding.</text>
                  <doc_id>116</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Pattern &#12296;wX 1 , wX 1 w&#12297; :</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#12296;w+ qAl X 1 , the X 1 said&#12297; Pattern &#12296;wX 1 w , wX 1 &#12297; :</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#12296;fy X 1 kAnwn Al&gt;wl , on december X 1 &#12297; Pattern &#12296;wX 1 wX 2 , wX 1 wX 2 w&#12297; :</text>
                  <doc_id>119</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#12296;Hl X 1 lAzmp X 2 , a X 1 solution to the X 2 crisis&#12297;</text>
                  <doc_id>120</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Building an Initial Rule Set</title>
            <text>We describe a greedy approach to building a rule set in which rules belonging to a pattern are added to the rule set guided by the improvements they yield on mt02-05-tune relative to the monotone Hiero system described in the previous section. We find that certain patterns seem not to contribute to any improvement. This is particularly significant as these patterns often encompass large numbers of rules, as with patterns with matching source and target patterns. For instance, we found no improvement when adding the pattern &#12296;X 1 w,X 1 w&#12297;, of which there were 1.2M instances (Table 3). Since concatenation is already possible under the general glue rule, rules with this pattern are redundant. By contrast, the much less frequent reordered counterpart, i.e. the &#12296;wX 1 ,X 1 w&#12297; pattern (0.01M instances), provides substantial gains. The situation is analogous for rules with two nonterminals (N nt =2).
Based on exploratory analyses (not reported here, for space) an initial rule set was built by excluding patterns reported in Table 4. In total, 171.5M rules are excluded, for a remaining set of 4.2M rules, 3.5M of which are hierarchical. We acknowledge that adding rules in this way, by greedy search, is less than ideal and inevitably raises questions with respect to generality and repeatability. However in our experience this is a robust approach, mainly because the initial translation system runs very fast; it is possible to run many exploratory experiments in a short time.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We describe a greedy approach to building a rule set in which rules belonging to a pattern are added to the rule set guided by the improvements they yield on mt02-05-tune relative to the monotone Hiero system described in the previous section.</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We find that certain patterns seem not to contribute to any improvement.</text>
                  <doc_id>122</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This is particularly significant as these patterns often encompass large numbers of rules, as with patterns with matching source and target patterns.</text>
                  <doc_id>123</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For instance, we found no improvement when adding the pattern &#12296;X 1 w,X 1 w&#12297;, of which there were 1.2M instances (Table 3).</text>
                  <doc_id>124</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Since concatenation is already possible under the general glue rule, rules with this pattern are redundant.</text>
                  <doc_id>125</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>By contrast, the much less frequent reordered counterpart, i.e. the &#12296;wX 1 ,X 1 w&#12297; pattern (0.01M instances), provides substantial gains.</text>
                  <doc_id>126</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The situation is analogous for rules with two nonterminals (N nt =2).</text>
                  <doc_id>127</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Based on exploratory analyses (not reported here, for space) an initial rule set was built by excluding patterns reported in Table 4.</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In total, 171.5M rules are excluded, for a remaining set of 4.2M rules, 3.5M of which are hierarchical.</text>
                  <doc_id>129</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We acknowledge that adding rules in this way, by greedy search, is less than ideal and inevitably raises questions with respect to generality and repeatability.</text>
                  <doc_id>130</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>However in our experience this is a robust approach, mainly because the initial translation system runs very fast; it is possible to run many exploratory experiments in a short time.</text>
                  <doc_id>131</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Shallow versus Fully Hierarchical Translation</title>
            <text>In measuring the effectiveness of rules in translation, we also investigate whether a &#8216;fully hierarchical&#8217; search is needed or whether a shallow search is also effective. In constrast to full Hiero, in the shallow search, only phrases are allowed to be substituted into non-terminals. The rules used in each case can be expressed as shown in the 2nd and 3rd columns of Table 1. Shallow search can be considered (loosely) to be a form of rule filtering.
As can be seen in Table 5 there is no impact on BLEU, while translation speed increases by a factor of 7. Of course, these results are specific to this Arabic-to-English translation task, and need not be expected to carry over to other language pairs, such as Chinese-to-English translation. However, the impact of this search simplification is easy to measure, and the gains can be significant enough, that it may be worth investigation even for languages with complex long distance movement.
mt02-05- -tune -test
&#8226; Number of translations (NT). We keep the NT most frequent &#945;, i.e. each &#947; is allowed to have at most NT rules.
&#8226; Number of reordered translations (NRT). We keep the NRT most frequent &#945; with monotonic non-terminals and the NRT most frequent &#945; with reordered non-terminals.
&#8226; Count percentage (CP). We keep the most frequent &#945; until their aggregated number of counts reaches a certain percentage CP of the total counts of X &#8594; &#12296;&#947;,&#8727;&#12297;. Some &#947;&#8217;s are allowed to have more &#945;&#8217;s than others, depending on their count distribution.
Results applying these filters with various thresholds are given in Table 6, including number of rules and decoding time. As shown, all filters achieve at least a 50% speed-up in decoding time by discarding 15% to 25% of the baseline rules. Remarkably, performance is unaffected when applying the simple NT and NRT filters with a threshold of 20 translations. Finally, the CM filter behaves slightly worse for thresholds of 90% for the same decoding time. For this reason, we select NRT=20 as our general filter.
mt02-05- -tune -test</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In measuring the effectiveness of rules in translation, we also investigate whether a &#8216;fully hierarchical&#8217; search is needed or whether a shallow search is also effective.</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In constrast to full Hiero, in the shallow search, only phrases are allowed to be substituted into non-terminals.</text>
                  <doc_id>133</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The rules used in each case can be expressed as shown in the 2nd and 3rd columns of Table 1.</text>
                  <doc_id>134</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Shallow search can be considered (loosely) to be a form of rule filtering.</text>
                  <doc_id>135</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As can be seen in Table 5 there is no impact on BLEU, while translation speed increases by a factor of 7.</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Of course, these results are specific to this Arabic-to-English translation task, and need not be expected to carry over to other language pairs, such as Chinese-to-English translation.</text>
                  <doc_id>137</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, the impact of this search simplification is easy to measure, and the gains can be significant enough, that it may be worth investigation even for languages with complex long distance movement.</text>
                  <doc_id>138</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>mt02-05- -tune -test</text>
                  <doc_id>139</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Number of translations (NT).</text>
                  <doc_id>140</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We keep the NT most frequent &#945;, i.e. each &#947; is allowed to have at most NT rules.</text>
                  <doc_id>141</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Number of reordered translations (NRT).</text>
                  <doc_id>142</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We keep the NRT most frequent &#945; with monotonic non-terminals and the NRT most frequent &#945; with reordered non-terminals.</text>
                  <doc_id>143</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Count percentage (CP).</text>
                  <doc_id>144</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We keep the most frequent &#945; until their aggregated number of counts reaches a certain percentage CP of the total counts of X &#8594; &#12296;&#947;,&#8727;&#12297;.</text>
                  <doc_id>145</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Some &#947;&#8217;s are allowed to have more &#945;&#8217;s than others, depending on their count distribution.</text>
                  <doc_id>146</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Results applying these filters with various thresholds are given in Table 6, including number of rules and decoding time.</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As shown, all filters achieve at least a 50% speed-up in decoding time by discarding 15% to 25% of the baseline rules.</text>
                  <doc_id>148</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Remarkably, performance is unaffected when applying the simple NT and NRT filters with a threshold of 20 translations.</text>
                  <doc_id>149</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, the CM filter behaves slightly worse for thresholds of 90% for the same decoding time.</text>
                  <doc_id>150</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For this reason, we select NRT=20 as our general filter.</text>
                  <doc_id>151</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>mt02-05- -tune -test</text>
                  <doc_id>152</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>3.4 Individual Rule Filters</title>
            <text>We now filter rules individually (not by class) according to their number of translations. For each fixed &#947; /&#8712; T + (i.e. with at least 1 non-terminal), we define the following filters over rules X &#8594; &#12296;&#947;,&#945;&#12297;:</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We now filter rules individually (not by class) according to their number of translations.</text>
                  <doc_id>153</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For each fixed &#947; /&#8712; T + (i.e. with at least 1 non-terminal), we define the following filters over rules X &#8594; &#12296;&#947;,&#945;&#12297;:</text>
                  <doc_id>154</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>4</index>
            <title>3.5 Pattern-based Rule Filters</title>
            <text>In this section we first reconsider whether reintroducing the monotonic rules (originally excluded as described in rows &#8217;b&#8217;, &#8217;c&#8217;, &#8217;d&#8217; in Table 4) affects performance. Results are given in the upper rows of Table 7. For all classes, we find that reintroducing these rules increases the total number of rules
mt02-05- -tune -test N nt .N e Filter Time Rules BLEU BLEU
substantially, despite the NRT=20 filter, but leads to degradation in translation performance.
We next reconsider the mincount threshold values for N nt .N e classes 1.3, 2.3, 2.4 and 2.5 originally described in Table 4 (rows &#8217;e&#8217; to &#8217;h&#8217;). Results under various mincount cutoffs for each class are given in Table 7 (middle five rows). For classes 2.3 and 2.5, the mincount cutoff can be reduced to 1 (i.e. all rules are kept) with slight translation improvements. In contrast, reducing the cutoff for classes 1.3 and 2.4 to 3 and 5, respectively, adds many more rules with no increase in performance. We also find that increasing the cutoff to 15 for class 2.4 yields the same results with a smaller rule set. Finally, we consider further filtering applied to class 1.2 with mincount 5 and 10 (final two rows in Table 7). The number of rules is largely unchanged, but translation performance drops consistently as more rules are removed.
Based on these experiments, we conclude that it is better to apply separate mincount thresholds to the classes to obtain optimal performance with a minimum size rule set.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In this section we first reconsider whether reintroducing the monotonic rules (originally excluded as described in rows &#8217;b&#8217;, &#8217;c&#8217;, &#8217;d&#8217; in Table 4) affects performance.</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Results are given in the upper rows of Table 7.</text>
                  <doc_id>156</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For all classes, we find that reintroducing these rules increases the total number of rules</text>
                  <doc_id>157</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>mt02-05- -tune -test N nt .N e Filter Time Rules BLEU BLEU</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>substantially, despite the NRT=20 filter, but leads to degradation in translation performance.</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We next reconsider the mincount threshold values for N nt .N e classes 1.3, 2.3, 2.4 and 2.5 originally described in Table 4 (rows &#8217;e&#8217; to &#8217;h&#8217;).</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Results under various mincount cutoffs for each class are given in Table 7 (middle five rows).</text>
                  <doc_id>161</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For classes 2.3 and 2.5, the mincount cutoff can be reduced to 1 (i.e. all rules are kept) with slight translation improvements.</text>
                  <doc_id>162</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast, reducing the cutoff for classes 1.3 and 2.4 to 3 and 5, respectively, adds many more rules with no increase in performance.</text>
                  <doc_id>163</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We also find that increasing the cutoff to 15 for class 2.4 yields the same results with a smaller rule set.</text>
                  <doc_id>164</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, we consider further filtering applied to class 1.2 with mincount 5 and 10 (final two rows in Table 7).</text>
                  <doc_id>165</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The number of rules is largely unchanged, but translation performance drops consistently as more rules are removed.</text>
                  <doc_id>166</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Based on these experiments, we conclude that it is better to apply separate mincount thresholds to the classes to obtain optimal performance with a minimum size rule set.</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>5</index>
            <title>3.6 Large Language Models and Evaluation</title>
            <text>Finally, in this section we report results of our shallow hierarchical system with the 2.5 mincount=1 configuration from Table 7, after including the following N-best list rescoring steps.
&#8226; Large-LM rescoring. We build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using &#8764;4.7B words of English newswire text, and apply them to rescore each 10000-best list.
&#8226; Minimum Bayes Risk (MBR). We then rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function to minimise (Kumar and Byrne, 2004).
Table 8 shows results for mt02-05-tune, mt02- 05-test, the NIST subsets from the MT06 evaluation (mt06-nist-nw for newswire data and mt06- nist-ng for newsgroup) and mt08, as measured by lowercased IBM BLEU and TER (Snover et al., 2006). Mixed case NIST BLEU for this system on mt08 is 42.5. This is directly comparable to official MT08 evaluation results 1 .</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Finally, in this section we report results of our shallow hierarchical system with the 2.5 mincount=1 configuration from Table 7, after including the following N-best list rescoring steps.</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Large-LM rescoring.</text>
                  <doc_id>169</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using &#8764;4.7B words of English newswire text, and apply them to rescore each 10000-best list.</text>
                  <doc_id>170</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Minimum Bayes Risk (MBR).</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We then rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function to minimise (Kumar and Byrne, 2004).</text>
                  <doc_id>172</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 8 shows results for mt02-05-tune, mt02- 05-test, the NIST subsets from the MT06 evaluation (mt06-nist-nw for newswire data and mt06- nist-ng for newsgroup) and mt08, as measured by lowercased IBM BLEU and TER (Snover et al., 2006).</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Mixed case NIST BLEU for this system on mt08 is 42.5.</text>
                  <doc_id>174</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This is directly comparable to official MT08 evaluation results 1 .</text>
                  <doc_id>175</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Conclusions</title>
        <text>This paper focuses on efficient large-scale hierarchical translation while maintaining good translation quality. Smart memoization and spreading neighborhood exploration during cube pruning are described and shown to reduce memory consumption and Hiero search errors using a simple phrasebased system as a contrast. We then define a general classification of hierarchical rules, based on their number of nonterminals, elements and their patterns, for refined extraction and filtering. For a large-scale Arabic-to-English task, we show that shallow hierarchical decoding is as good
and MBR decoding. as fully hierarchical search and that decoding time is dramatically decreased. In addition, we describe individual rule filters based on the distribution of translations with further time reductions at no cost in translation scores. This is in direct contrast to recent reported results in which other filtering strategies lead to degraded performance (Shen et al., 2008; Zollmann et al., 2008). We find that certain patterns are of much greater value in translation than others and that separate minimum count filters should be applied accordingly. Some patterns were found to be redundant or harmful, in particular those with two monotonic non-terminals. Moreover, we show that the value of a pattern is not directly related to the number of rules it encompasses, which can lead to discarding large numbers of rules as well as to dramatic speed improvements. Although reported experiments are only for Arabic-to-English translation, we believe the approach will prove to be general. Pattern relevance will vary for other language pairs, but we expect filtering strategies to be equally worth pursuing.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This paper focuses on efficient large-scale hierarchical translation while maintaining good translation quality.</text>
              <doc_id>176</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Smart memoization and spreading neighborhood exploration during cube pruning are described and shown to reduce memory consumption and Hiero search errors using a simple phrasebased system as a contrast.</text>
              <doc_id>177</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We then define a general classification of hierarchical rules, based on their number of nonterminals, elements and their patterns, for refined extraction and filtering.</text>
              <doc_id>178</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>For a large-scale Arabic-to-English task, we show that shallow hierarchical decoding is as good</text>
              <doc_id>179</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and MBR decoding.</text>
              <doc_id>180</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>as fully hierarchical search and that decoding time is dramatically decreased.</text>
              <doc_id>181</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In addition, we describe individual rule filters based on the distribution of translations with further time reductions at no cost in translation scores.</text>
              <doc_id>182</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This is in direct contrast to recent reported results in which other filtering strategies lead to degraded performance (Shen et al., 2008; Zollmann et al., 2008).</text>
              <doc_id>183</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We find that certain patterns are of much greater value in translation than others and that separate minimum count filters should be applied accordingly.</text>
              <doc_id>184</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Some patterns were found to be redundant or harmful, in particular those with two monotonic non-terminals.</text>
              <doc_id>185</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, we show that the value of a pattern is not directly related to the number of rules it encompasses, which can lead to discarding large numbers of rules as well as to dramatic speed improvements.</text>
              <doc_id>186</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Although reported experiments are only for Arabic-to-English translation, we believe the approach will prove to be general.</text>
              <doc_id>187</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Pattern relevance will vary for other language pairs, but we expect filtering strategies to be equally worth pursuing.</text>
              <doc_id>188</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>Acknowledgments</title>
        <text>This work was supported in part by the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011- 06-C-0022. G. Iglesias supported by Spanish Government research grant BES-2007-15956 (project TEC2006- 13694-C03-03).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This work was supported in part by the GALE program of the Defense Advanced Research Projects Agency, Contract No.</text>
              <doc_id>189</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>HR0011- 06-C-0022.</text>
              <doc_id>190</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>G. Iglesias supported by Spanish Government research grant BES-2007-15956 (project TEC2006- 13694-C03-03).</text>
              <doc_id>191</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Hierarchical grammars (not including glue rules). T is the set of terminals.</caption>
        <reference_text></reference_text>
        <page_num>2</page_num>
        <head>
          <rows>
            <row>
              <cell>HIERO MJ1</cell>
              <cell>HIERO</cell>
              <cell>HIERO SHALLOW</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>X &#8594; &#12296;V 2 V 1 ,V 1 V 2 &#12297;</cell>
              <cell>X &#8594; &#12296;&#947;,&#945;&#12297;</cell>
              <cell>X &#8594; &#12296;&#947; s ,&#945; s &#12297;</cell>
            </row>
            <row>
              <cell>X &#8594; &#12296;V ,V &#12297;</cell>
              <cell>&#947;,&#945; &#8712; ({X} &#8746; T) +</cell>
              <cell>X &#8594; &#12296;V ,V &#12297;</cell>
            </row>
            <row>
              <cell>V &#8594; &#12296;s,t&#12297;</cell>
              <cell></cell>
              <cell>V &#8594; &#12296;s,t&#12297;</cell>
            </row>
            <row>
              <cell>s,t &#8712; T +</cell>
              <cell></cell>
              <cell>s,t &#8712; T + ; &#947; s ,&#945; s &#8712; ({V } &#8746; T) +</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Phrase-based TTM and Hiero performance on mt02-05-tune for TTM (a), Hiero (b), Hiero with spreading neighborhood exploration (c). SE is the number of Hiero hypotheses with search errors.</caption>
        <reference_text></reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>Monotone</cell>
              <cell>MJ1</cell>
              <cell>MJ1+MET</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell></cell>
              <cell>BLEU</cell>
              <cell>SE</cell>
              <cell>BLEU</cell>
              <cell>SE</cell>
              <cell>BLEU</cell>
              <cell>SE</cell>
            </row>
            <row>
              <cell>a</cell>
              <cell>44.7</cell>
              <cell>-</cell>
              <cell>47.2</cell>
              <cell>-</cell>
              <cell>49.1</cell>
              <cell>-</cell>
            </row>
            <row>
              <cell>b</cell>
              <cell>44.5</cell>
              <cell>342</cell>
              <cell>46.7</cell>
              <cell>555</cell>
              <cell>48.4</cell>
              <cell>822</cell>
            </row>
            <row>
              <cell>c</cell>
              <cell>44.7</cell>
              <cell>77</cell>
              <cell>47.1</cell>
              <cell>191</cell>
              <cell>48.9</cell>
              <cell>360</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Hierarchical rule patterns classed by number of non-terminals, N nt , number of elements N e , source and target patterns, and types in the rule set extracted for mt02-05-tune.</caption>
        <reference_text>In PAGE 5: ... For instance, we found no improvement when adding the pattern ?X1w,X1w?, of which there were 1.2M instances ( Table3 ). Since concatenation is already possible under the general glue rule, rules with this pattern are redundant....</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>important than others. 3.1  Class#@#@Class</cell>
              <cell>important than others.   Rule Patterns   Rule Pattern#@#@Rule Pattern</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Nnt.Ne#@#@N nt .N e</cell>
              <cell>?source , target?#@#@&#12296;source , target&#12297;</cell>
              <cell>Types</cell>
            </row>
            <row>
              <cell></cell>
              <cell>?wX1 , wX1?#@#@&#12296;wX 1 , wX 1 &#12297;</cell>
              <cell>1185028</cell>
            </row>
            <row>
              <cell>1.2</cell>
              <cell>?wX1 , wX1w?#@#@&#12296;wX 1 , wX 1 w&#12297;</cell>
              <cell>153130</cell>
            </row>
            <row>
              <cell></cell>
              <cell>?wX1 , X1w?#@#@&#12296;wX 1 , X 1 w&#12297;</cell>
              <cell>97889</cell>
            </row>
            <row>
              <cell>1.3</cell>
              <cell>?wX1w , wX1w?#@#@&#12296;wX 1 w , wX 1 w&#12297;</cell>
              <cell>32903522</cell>
            </row>
            <row>
              <cell></cell>
              <cell>?wX1w , wX1?#@#@&#12296;wX 1 w , wX 1 &#12297;</cell>
              <cell>989540</cell>
            </row>
            <row>
              <cell>2.3</cell>
              <cell>?X1wX2 , X1wX2?#@#@&#12296;X 1 wX 2 , X 1 wX 2 &#12297;</cell>
              <cell>1554656</cell>
            </row>
            <row>
              <cell></cell>
              <cell>?X2wX1 , X1wX2?#@#@&#12296;X 2 wX 1 , X 1 wX 2 &#12297;</cell>
              <cell>39163</cell>
            </row>
            <row>
              <cell></cell>
              <cell>?wX1wX2 , wX1wX2?#@#@&#12296;wX 1 wX 2 , wX 1 wX 2 &#12297;</cell>
              <cell>26901823</cell>
            </row>
            <row>
              <cell></cell>
              <cell>?X1wX2w , X1wX2w?#@#@&#12296;X 1 wX 2 w , X 1 wX 2 w&#12297;</cell>
              <cell>26053969</cell>
            </row>
            <row>
              <cell>2.4</cell>
              <cell>?wX1wX2 , wX1wX2w?#@#@&#12296;wX 1 wX 2 , wX 1 wX 2 w&#12297;</cell>
              <cell>2534510</cell>
            </row>
            <row>
              <cell></cell>
              <cell>?wX2wX1 , wX1wX2?#@#@&#12296;wX 2 wX 1 , wX 1 wX 2 &#12297;</cell>
              <cell>349176</cell>
            </row>
            <row>
              <cell></cell>
              <cell>?X2wX1w , X1wX2w?#@#@&#12296;X 2 wX 1 w , X 1 wX 2 w&#12297;</cell>
              <cell>259459</cell>
            </row>
            <row>
              <cell></cell>
              <cell>?wX1wX2w , wX1wX2w?#@#@&#12296;wX 1 wX 2 w , wX 1 wX 2 w&#12297;</cell>
              <cell>61704299</cell>
            </row>
            <row>
              <cell></cell>
              <cell>?wX1wX2w , wX1X2w?#@#@&#12296;wX 1 wX 2 w , wX 1 X 2 w&#12297;</cell>
              <cell>3149516</cell>
            </row>
            <row>
              <cell>2.5</cell>
              <cell>?wX1wX2w , X1wX2w?#@#@&#12296;wX 1 wX 2 w , X 1 wX 2 w&#12297;</cell>
              <cell>2330797</cell>
            </row>
            <row>
              <cell></cell>
              <cell>?wX2wX1w , wX1wX2w?#@#@&#12296;wX 2 wX 1 w , wX 1 wX 2 w&#12297;</cell>
              <cell>275810</cell>
            </row>
            <row>
              <cell></cell>
              <cell>?wX2wX1w , wX1X2w?#@#@&#12296;wX 2 wX 1 w , wX 1 X 2 w&#12297;</cell>
              <cell>205801</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: Rules excluded from the initial rule set.</caption>
        <reference_text>In PAGE 6: ... 3.5 Pattern-based Rule Filters In this section we first reconsider whether reintro- ducing the monotonic rules (originally excluded as described in rows ?b?, ?c?, ?d? in  Table4 ) affects performance. Results are given in the upper rows of Table 7....  In PAGE 7: ...es for Nnt.Ne classes 1.3, 2.3, 2.4 and 2.5 origi- nally described in  Table4  (rows ?e? to ?h?). Results under various mincount cutoffs for each class are given in Table 7 (middle five rows)....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>None</cell>
              <cell>Excluded Rules</cell>
              <cell>Types</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>a</cell>
              <cell>?X1w,X1w? , ?wX1,wX1?</cell>
              <cell>2332604</cell>
            </row>
            <row>
              <cell>b</cell>
              <cell>?X1wX2,??</cell>
              <cell>2121594</cell>
            </row>
            <row>
              <cell>c</cell>
              <cell>?X1wX2w,X1wX2w? ,</cell>
              <cell>52955792</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>?wX1wX2,wX1wX2?</cell>
            </row>
            <row>
              <cell>d</cell>
              <cell>?wX1wX2w,??</cell>
              <cell>69437146</cell>
            </row>
            <row>
              <cell>e</cell>
              <cell>Nnt.Ne= 1.3 w mincount=5</cell>
              <cell>32394578</cell>
            </row>
            <row>
              <cell>f</cell>
              <cell>Nnt.Ne= 2.3 w mincount=5</cell>
              <cell>166969</cell>
            </row>
            <row>
              <cell>g</cell>
              <cell>Nnt.Ne= 2.4 w mincount=10</cell>
              <cell>11465410</cell>
            </row>
            <row>
              <cell>h</cell>
              <cell>Nnt.Ne= 2.5 w mincount=5</cell>
              <cell>688804</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TableSeer</source>
        <caption>Table 5: Translation performance and time (in seconds per word) for full vs. shallow Hiero.</caption>
        <reference_text>In PAGE 6: ... Shallow search can be con- sidered (loosely) to be a form of rule filtering. As can be seen in  Table5  there is no impact on BLEU, while translation speed increases by a fac- tor of 7. Of course, these results are specific to this Arabic-to-English translation task, and need not be expected to carry over to other language pairs, such as Chinese-to-English translation....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>mt02-05-</cell>
              <cell>-tune</cell>
              <cell>-tune</cell>
              <cell>-test</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Time</cell>
              <cell>BLEU</cell>
              <cell>BLEU</cell>
            </row>
            <row>
              <cell>HIERO</cell>
              <cell>14.0</cell>
              <cell>52.1</cell>
              <cell>51.5</cell>
            </row>
            <row>
              <cell>HIERO - shallow</cell>
              <cell>2.0</cell>
              <cell>52.1</cell>
              <cell>51.4</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>6</id>
        <source>TableSeer</source>
        <caption>Table 6: Impact of general rule filters on translation (IBM BLEU), time (in seconds per word) and number of rules (in millions).</caption>
        <reference_text>In PAGE 6: ... Some ??s are al- lowed to have more ??s than others, depend- ing on their count distribution. Results applying these filters with various thresholds are given in  Table6 , including num- ber of rules and decoding time. As shown, all filters achieve at least a 50% speed-up in decod- ing time by discarding 15% to 25% of the base- line rules....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>mt02-05-</cell>
              <cell>None</cell>
              <cell>-tune</cell>
              <cell>None</cell>
              <cell>-test</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Filter</cell>
              <cell>Time</cell>
              <cell>Rules</cell>
              <cell>BLEU</cell>
              <cell>BLEU</cell>
            </row>
            <row>
              <cell>baseline</cell>
              <cell>2.0</cell>
              <cell>4.20</cell>
              <cell>52.1</cell>
              <cell>51.4</cell>
            </row>
            <row>
              <cell>NT=10</cell>
              <cell>0.8</cell>
              <cell>3.25</cell>
              <cell>52.0</cell>
              <cell>51.3</cell>
            </row>
            <row>
              <cell>NT=15</cell>
              <cell>0.8</cell>
              <cell>3.43</cell>
              <cell>52.0</cell>
              <cell>51.3</cell>
            </row>
            <row>
              <cell>NT=20</cell>
              <cell>0.8</cell>
              <cell>3.56</cell>
              <cell>52.1</cell>
              <cell>51.4</cell>
            </row>
            <row>
              <cell>NRT=10</cell>
              <cell>0.9</cell>
              <cell>3.29</cell>
              <cell>52.0</cell>
              <cell>51.3</cell>
            </row>
            <row>
              <cell>NRT=15</cell>
              <cell>1.0</cell>
              <cell>3.48</cell>
              <cell>52.0</cell>
              <cell>51.4</cell>
            </row>
            <row>
              <cell>NRT=20</cell>
              <cell>1.0</cell>
              <cell>3.59</cell>
              <cell>52.1</cell>
              <cell>51.4</cell>
            </row>
            <row>
              <cell>CP=50</cell>
              <cell>0.7</cell>
              <cell>2.56</cell>
              <cell>51.4</cell>
              <cell>50.9</cell>
            </row>
            <row>
              <cell>CP=90</cell>
              <cell>1.0</cell>
              <cell>3.60</cell>
              <cell>52.0</cell>
              <cell>51.3</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>7</id>
        <source>TableSeer</source>
        <caption>Table 7: Effect of pattern-based rule filters. Time in seconds per word. Rules in millions.</caption>
        <reference_text>In PAGE 6: ...5 Pattern-based Rule Filters In this section we first reconsider whether reintro- ducing the monotonic rules (originally excluded as described in rows ?b?, ?c?, ?d? in Table 4) affects performance. Results are given in the upper rows of  Table7 . For all classes, we find that reintroduc- ing these rules increases the total number of rules...  In PAGE 7: ...es for Nnt.Ne classes 1.3, 2.3, 2.4 and 2.5 origi- nally described in Table 4 (rows ?e? to ?h?). Results under various mincount cutoffs for each class are given in  Table7  (middle five rows). For classes 2....  In PAGE 7: ...6 Large Language Models and Evaluation Finally, in this section we report results of our shallow hierarchical system with the 2.5 min- count=1 configuration from  Table7 , after includ- ing the following N-best list rescoring steps. ? Large-LM rescoring....</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>mt02-05-</cell>
              <cell>mt02-05-</cell>
              <cell>None</cell>
              <cell>-tune</cell>
              <cell>None</cell>
              <cell>-test</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Nnt.Ne</cell>
              <cell>Filter</cell>
              <cell>Time</cell>
              <cell>Rules</cell>
              <cell>BLEU</cell>
              <cell>BLEU</cell>
            </row>
            <row>
              <cell>baseline NRT=20</cell>
              <cell>baseline NRT=20</cell>
              <cell>1.0</cell>
              <cell>3.59</cell>
              <cell>52.1</cell>
              <cell>51.4</cell>
            </row>
            <row>
              <cell>2.3</cell>
              <cell>+monotone</cell>
              <cell>1.1</cell>
              <cell>4.08</cell>
              <cell>51.5</cell>
              <cell>51.1</cell>
            </row>
            <row>
              <cell>2.4</cell>
              <cell>+monotone</cell>
              <cell>2.0</cell>
              <cell>11.52</cell>
              <cell>51.6</cell>
              <cell>51.0</cell>
            </row>
            <row>
              <cell>2.5</cell>
              <cell>+monotone</cell>
              <cell>1.8</cell>
              <cell>6.66</cell>
              <cell>51.7</cell>
              <cell>51.2</cell>
            </row>
            <row>
              <cell>1.3</cell>
              <cell>mincount=3</cell>
              <cell>1.0</cell>
              <cell>5.61</cell>
              <cell>52.1</cell>
              <cell>51.3</cell>
            </row>
            <row>
              <cell>2.3</cell>
              <cell>mincount=1</cell>
              <cell>1.2</cell>
              <cell>3.70</cell>
              <cell>52.1</cell>
              <cell>51.4</cell>
            </row>
            <row>
              <cell>2.4</cell>
              <cell>mincount=5</cell>
              <cell>1.8</cell>
              <cell>4.62</cell>
              <cell>52.0</cell>
              <cell>51.3</cell>
            </row>
            <row>
              <cell>2.4</cell>
              <cell>mincount=15</cell>
              <cell>1.0</cell>
              <cell>3.37</cell>
              <cell>52.0</cell>
              <cell>51.4</cell>
            </row>
            <row>
              <cell>2.5</cell>
              <cell>mincount=1</cell>
              <cell>1.1</cell>
              <cell>4.27</cell>
              <cell>52.2</cell>
              <cell>51.5</cell>
            </row>
            <row>
              <cell>1.2</cell>
              <cell>mincount=5</cell>
              <cell>1.0</cell>
              <cell>3.51</cell>
              <cell>51.8</cell>
              <cell>51.3</cell>
            </row>
            <row>
              <cell>1.2</cell>
              <cell>mincount=10</cell>
              <cell>1.0</cell>
              <cell>3.50</cell>
              <cell>51.7</cell>
              <cell>51.2</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>8</id>
        <source>TET</source>
        <caption>Table 8: Arabic-to-English translation results (lower-cased IBM BLEU / TER) with large language models and MBR decoding.</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell>mt02-05-tune</cell>
              <cell>mt02-05-test</cell>
              <cell>mt06-nist-nw</cell>
              <cell>mt06-nist-ng</cell>
              <cell>mt08</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>HIERO+MET</cell>
              <cell>52.2 / 41.6</cell>
              <cell>51.5 / 42.2</cell>
              <cell>48.4 / 43.6</cell>
              <cell>35.3 / 53.2</cell>
              <cell>42.5 / 48.6</cell>
            </row>
            <row>
              <cell>+rescoring</cell>
              <cell>53.2 / 40.8</cell>
              <cell>52.6 / 41.4</cell>
              <cell>49.4 / 42.9</cell>
              <cell>36.6 / 53.5</cell>
              <cell>43.4 / 48.1</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Cyril Allauzen</author>
          <author>Michael Riley</author>
          <author>Johan Schalkwyk</author>
          <author>Wojciech Skut</author>
          <author>Mehryar Mohri</author>
        </authors>
        <title>OpenFst: A general and efficient weighted finite-state transducer library.</title>
        <publication>In Proceedings of CIAA,</publication>
        <pages>11--23</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Oliver Bender</author>
          <author>Evgeny Matusov</author>
          <author>Stefan Hahn</author>
          <author>Sasa Hasan</author>
          <author>Shahram Khadivi</author>
          <author>Hermann Ney</author>
        </authors>
        <title>The RWTH Arabic-to-English spoken language translation system.</title>
        <publication>In Proceedings of ASRU,</publication>
        <pages>396--401</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Graeme Blackwood</author>
          <author>Adri&#224; de Gispert</author>
          <author>Jamie Brunning</author>
          <author>William Byrne</author>
        </authors>
        <title>Large-scale statistical machine translation with weighted finite state transducers.</title>
        <publication>In Proceedings of FSMNLP,</publication>
        <pages>27--35</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Phil Blunsom</author>
          <author>Trevor Cohn</author>
          <author>Miles Osborne</author>
        </authors>
        <title>A discriminative latent variable model for statistical machine translation.</title>
        <publication>In Proceedings of ACL-HLT,</publication>
        <pages>200--208</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Thorsten Brants</author>
          <author>Ashok C Popat</author>
          <author>Peng Xu</author>
          <author>Franz J Och</author>
          <author>Jeffrey Dean</author>
        </authors>
        <title>Large language models in machine translation.</title>
        <publication>In Proceedings of EMNLP-ACL,</publication>
        <pages>858--867</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Jean-C&#233;dric Chappelier</author>
          <author>Martin Rajman</author>
        </authors>
        <title>A generalized CYK algorithm for parsing stochastic CFG.</title>
        <publication>In Proceedings of TAPD,</publication>
        <pages>133--137</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Jean-C&#233;dric Chappelier</author>
          <author>Martin Rajman</author>
          <author>Ram&#243;n Arag&#252;&#233;s</author>
          <author>Antoine Rozenknop</author>
        </authors>
        <title>Lattice parsing for speech recognition.</title>
        <publication>In Proceedings of TALN,</publication>
        <pages>95--104</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>A hierarchical phrase-based model for statistical machine translation.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>263--270</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>Hierarchical phrase-based translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Markus Dreyer</author>
          <author>Keith Hall</author>
          <author>Sanjeev Khudanpur</author>
        </authors>
        <title>Comparing reordering constraints for SMT using efficient BLEU oracle computation.</title>
        <publication>In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Christopher Dyer</author>
          <author>Smaranda Muresan</author>
          <author>Philip Resnik</author>
        </authors>
        <title>Generalizing word lattice translation.</title>
        <publication>In Proceedings of ACL-HLT,</publication>
        <pages>1012--1020</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Liang Huang</author>
          <author>David Chiang</author>
        </authors>
        <title>Forest rescoring: Faster decoding with integrated language models.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>144--151</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Shankar Kumar</author>
          <author>William Byrne</author>
        </authors>
        <title>Minimum Bayes-risk decoding for statistical machine translation.</title>
        <publication>In Proceedings of HLT-NAACL,</publication>
        <pages>169--176</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Shankar Kumar</author>
          <author>William Byrne</author>
        </authors>
        <title>Local phrase reordering models for statistical machine translation.</title>
        <publication>In Proceedings of HLT-EMNLP,</publication>
        <pages>161--168</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Shankar Kumar</author>
          <author>Yonggang Deng</author>
          <author>William Byrne</author>
        </authors>
        <title>A weighted finite state transducer translation template model for statistical machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Li</author>
          <author>Sanjeev Khudanpur</author>
        </authors>
        <title>A scalable decoder for parsing-based machine translation with equivalent language model state maintenance.</title>
        <publication>In Proceedings of the ACL-HLT Second Workshop on Syntax and Structure in Statistical Translation,</publication>
        <pages>10--18</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Andreas Zollmann</author>
          <author>Ashish Venugopal</author>
          <author>Franz Och</author>
          <author>Jay Ponte</author>
        </authors>
        <title>A systematic comparison of phrase-based, hierarchical and syntax-augmented statistical MT.</title>
        <publication>In Proceedings of COLING,</publication>
        <pages>1145--1152</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Adam Lopez</author>
        </authors>
        <title>Hierarchical phrase-based translation with suffix arrays.</title>
        <publication>In Proceedings of EMNLPCONLL,</publication>
        <pages>976--985</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Adam Lopez</author>
        </authors>
        <title>Tera-scale translation models via pattern matching.</title>
        <publication>In Proceedings of COLING,</publication>
        <pages>505--512</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Yuval Marton</author>
          <author>Philip Resnik</author>
        </authors>
        <title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
        <publication>In Proceedings of ACL-HLT,</publication>
        <pages>1003--1011</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Franz J Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>160--167</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Antti-Veikko Rosti</author>
          <author>Necip Fazil Ayan</author>
          <author>Bing Xiang</author>
          <author>Spyros Matsoukas</author>
          <author>Richard Schwartz</author>
          <author>Bonnie Dorr</author>
        </authors>
        <title>Combining outputs from multiple machine translation systems.</title>
        <publication>In Proceedings of HLTNAACL,</publication>
        <pages>228--235</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Libin Shen</author>
          <author>Jinxi Xu</author>
          <author>Ralph Weischedel</author>
        </authors>
        <title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
        <publication>In Proceedings of ACL-HLT,</publication>
        <pages>577--585</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Khe Chai Sim</author>
          <author>William Byrne</author>
          <author>Mark Gales</author>
          <author>Hichem Sahbi</author>
          <author>Phil Woodland</author>
        </authors>
        <title>Consensus network decoding for statistical machine translation system combination.</title>
        <publication>In Proceedings of ICASSP,</publication>
        <pages>105--108</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>Matthew Snover</author>
          <author>Bonnie J Dorr</author>
          <author>Richard Schwartz</author>
          <author>Linnea Micciulla</author>
          <author>John Makhoul</author>
        </authors>
        <title>A study of translation edit rate with targeted human annotation.</title>
        <publication>In Proceedings of AMTA,</publication>
        <pages>223--231</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Ashish Venugopal</author>
          <author>Andreas Zollmann</author>
          <author>Vogel Stephan</author>
        </authors>
        <title>An efficient two-pass approach to synchronous-CFG driven statistical MT.</title>
        <publication>In Proceedings of HLT-NAACL,</publication>
        <pages>500--507</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>Hao Zhang</author>
          <author>Daniel Gildea</author>
        </authors>
        <title>Synchronous binarization for machine translation.</title>
        <publication>In Proceedings of HLT-NAACL,</publication>
        <pages>256--263</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>27</id>
        <authors>
          <author>Hao Zhang</author>
          <author>Daniel Gildea</author>
          <author>David Chiang</author>
        </authors>
        <title>Extracting synchronous grammar rules from wordlevel alignments in linear time.</title>
        <publication>In Proceedings of COLING,</publication>
        <pages>1081--1088</pages>
        <date>2008</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Allauzen et al., 2007</string>
        <sentence_id>18512</sentence_id>
        <char_offset>153</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Bender et al. (2007)</string>
        <sentence_id>18507</sentence_id>
        <char_offset>27</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>3</reference_id>
        <string>Blunsom et al. (2008)</string>
        <sentence_id>18454</sentence_id>
        <char_offset>20</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>4</reference_id>
        <string>Brants et al., 2007</string>
        <sentence_id>18595</sentence_id>
        <char_offset>54</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>5</reference_id>
        <string>Chappelier and Rajman, 1998</string>
        <sentence_id>18531</sentence_id>
        <char_offset>148</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>6</reference_id>
        <string>Chappelier et al. (1999)</string>
        <sentence_id>18453</sentence_id>
        <char_offset>86</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>7</reference_id>
        <string>Chiang (2005</string>
        <sentence_id>18447</sentence_id>
        <char_offset>186</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>7</reference_id>
        <string>Chiang (2005</string>
        <sentence_id>18457</sentence_id>
        <char_offset>177</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>7</reference_id>
        <string>Chiang (2005)</string>
        <sentence_id>18457</sentence_id>
        <char_offset>177</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>7</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>18469</sentence_id>
        <char_offset>39</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>8</reference_id>
        <string>Chiang (2007)</string>
        <sentence_id>18449</sentence_id>
        <char_offset>35</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>8</reference_id>
        <string>Chiang (2007)</string>
        <sentence_id>18450</sentence_id>
        <char_offset>195</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>8</reference_id>
        <string>Chiang (2007)</string>
        <sentence_id>18530</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>8</reference_id>
        <string>Chiang (2007)</string>
        <sentence_id>18499</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>8</reference_id>
        <string>Chiang (2007)</string>
        <sentence_id>18505</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>8</reference_id>
        <string>Chiang (2007)</string>
        <sentence_id>18615</sentence_id>
        <char_offset>28</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>8</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>18476</sentence_id>
        <char_offset>45</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>8</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>18490</sentence_id>
        <char_offset>242</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>8</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>18603</sentence_id>
        <char_offset>71</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>9</reference_id>
        <string>Dreyer et al., 2007</string>
        <sentence_id>18514</sentence_id>
        <char_offset>109</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>10</reference_id>
        <string>Dyer et al. (2008)</string>
        <sentence_id>18452</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>11</reference_id>
        <string>Huang and Chiang (2007)</string>
        <sentence_id>18449</sentence_id>
        <char_offset>25</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>12</reference_id>
        <string>Kumar and Byrne, 2004</string>
        <sentence_id>18597</sentence_id>
        <char_offset>137</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>13</reference_id>
        <string>Kumar and Byrne, 2005</string>
        <sentence_id>18513</sentence_id>
        <char_offset>121</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>14</reference_id>
        <string>Kumar et al., 2006</string>
        <sentence_id>18512</sentence_id>
        <char_offset>54</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>15</reference_id>
        <string>Li and Khudanpur (2008)</string>
        <sentence_id>18451</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>16</reference_id>
        <string>Zollmann et al., 2008</string>
        <sentence_id>18471</sentence_id>
        <char_offset>121</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>16</reference_id>
        <string>Zollmann et al., 2008</string>
        <sentence_id>18611</sentence_id>
        <char_offset>219</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>16</reference_id>
        <string>Zollmann et al., 2008</string>
        <sentence_id>18627</sentence_id>
        <char_offset>139</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>17</reference_id>
        <string>Lopez (2007)</string>
        <sentence_id>18460</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>18</reference_id>
        <string>Lopez, 2008</string>
        <sentence_id>18611</sentence_id>
        <char_offset>122</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>18</reference_id>
        <string>Lopez (2008)</string>
        <sentence_id>18462</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>19</reference_id>
        <string>Marton and Resnik (2008)</string>
        <sentence_id>18457</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>20</reference_id>
        <string>Och, 2003</string>
        <sentence_id>18508</sentence_id>
        <char_offset>5</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>21</reference_id>
        <string>Rosti et al., 2007</string>
        <sentence_id>18463</sentence_id>
        <char_offset>137</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>22</reference_id>
        <string>Shen et al. (2008)</string>
        <sentence_id>18612</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>22</reference_id>
        <string>Shen et al. (2008)</string>
        <sentence_id>18614</sentence_id>
        <char_offset>81</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>22</reference_id>
        <string>Shen et al., 2008</string>
        <sentence_id>18627</sentence_id>
        <char_offset>120</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>23</reference_id>
        <string>Sim et al., 2007</string>
        <sentence_id>18463</sentence_id>
        <char_offset>119</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>24</reference_id>
        <string>Snover et al., 2006</string>
        <sentence_id>18598</sentence_id>
        <char_offset>216</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>25</reference_id>
        <string>Venugopal et al. (2007)</string>
        <sentence_id>18450</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>26</reference_id>
        <string>Zhang and Gildea (2006)</string>
        <sentence_id>18458</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>42</id>
        <reference_id>27</reference_id>
        <string>Zhang et al. (2008)</string>
        <sentence_id>18459</sentence_id>
        <char_offset>29</char_offset>
      </citation>
    </citations>
  </content>
</document>
