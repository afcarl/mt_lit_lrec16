<document>
  <filename>C10-2084</filename>
  <authors/>
  <title>Improved Discriminative ITG Alignment using Hierarchical Phrase Pairs and Semi-supervised Training</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>While ITG has many desirable properties for word alignment, it still suffers from the limitation of one-to-one matching. While existing approaches relax this limitation using phrase pairs, we propose a ITG formalism, which even handles units of non-contiguous words, using both simple and hierarchical phrase pairs. We also propose a parameter estimation method, which combines the merits of both supervised and unsupervised learning, for the ITG formalism. The ITG alignment system achieves significant improvement in both word alignment quality and translation performance.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>While ITG has many desirable properties for word alignment, it still suffers from the limitation of one-to-one matching.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>While existing approaches relax this limitation using phrase pairs, we propose a ITG formalism, which even handles units of non-contiguous words, using both simple and hierarchical phrase pairs.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We also propose a parameter estimation method, which combines the merits of both supervised and unsupervised learning, for the ITG formalism.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The ITG alignment system achieves significant improvement in both word alignment quality and translation performance.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Inversion transduction grammar (ITG) (Wu, 1997) is an adaptation of CFG to bilingual parsing. It does synchronous parsing of two languages with phrasal and word-level alignment as by-product. One of the merits of ITG is that it is less biased towards short-distance reordering compared with other word alignment models such as HMM. For this reason ITG has gained more and more attention recently in the word alignment community (Zhang et al., 2005; Cherry et al., 2006; Haghighi et al., 2009) 1 . The basic ITG formalism suffers from the major drawback of one-to-one matching. This limitation renders ITG unable to produce certain alignment patterns (such as many-to-many * This work has been done while the first author was visiting Microsoft Research Asia. alignment for idiomatic expression). For this reason those recent approaches to ITG alignment introduce the notion of phrase (or block), defined as sequence of contiguous words, into the ITG formalism (Cherry and Lin, 2007; Haghighi et al., 2009; Zhang et al., 2008). However, there are still alignment patterns which cannot be captured by phrases. A simple example is connective in Chinese/English. In English, two clauses are connected by merely one connective (like "although", "because") but in Chinese we need two connectives (e.g. There is a sentence pattern " &#34429; &#28982; &#20294; &#26159; &#65533; although ", where and are variables for clauses). The English connective should then be aligned to two noncontiguous Chinese connectives, and such alignment pattern is not available in either wordlevel or phrase-level ITG. As hierarchical phrase-based SMT (Chiang, 2007) is proved to be superior to simple phrase-based SMT, it is natural to ask, why don&#8223;t we further incorporate hierarchical phrase pairs (henceforth h-phrase pairs) into ITG? In this paper we propose a ITG formalism and parsing algorithm using h-phrase pairs. The ITG model involves much more parameters. On the one hand, each phrase/h-phrase pair has its own probability or score. It is not feasible to learn these parameters through discriminative/supervised learning since the repertoire of phrase pairs is much larger than the size of human-annotated alignment set. On the other hand, there are also a few useful features which cannot be estimated merely by unsupervised learning like EM. Inspired by Fraser et al. (2006), we propose a semi-supervised learning algorithm which combines the merits of both discrimina-
Coling 2010: Poster Volume, pages 730&#8211;738, Beijing, August 2010 tive training (error minimization) and approximate EM (estimation of numerous parameters). The ITG model augmented with the learning algorithm is shown by experiment results to improve significantly both alignment quality and translation performance. In the following, we will explain, step-by-step, how to incorporate hierarchical phrase pairs into the ITG formalism (Section 2) and in ITG parsing (Section 3). The semi-supervised training method is elaborated in Section 4. The merits of the complete system are illustrated with the experiments described in Section 5.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Inversion transduction grammar (ITG) (Wu, 1997) is an adaptation of CFG to bilingual parsing.</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It does synchronous parsing of two languages with phrasal and word-level alignment as by-product.</text>
              <doc_id>5</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>One of the merits of ITG is that it is less biased towards short-distance reordering compared with other word alignment models such as HMM.</text>
              <doc_id>6</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>For this reason ITG has gained more and more attention recently in the word alignment community (Zhang et al., 2005; Cherry et al., 2006; Haghighi et al., 2009) 1 .</text>
              <doc_id>7</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The basic ITG formalism suffers from the major drawback of one-to-one matching.</text>
              <doc_id>8</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This limitation renders ITG unable to produce certain alignment patterns (such as many-to-many * This work has been done while the first author was visiting Microsoft Research Asia.</text>
              <doc_id>9</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>alignment for idiomatic expression).</text>
              <doc_id>10</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>For this reason those recent approaches to ITG alignment introduce the notion of phrase (or block), defined as sequence of contiguous words, into the ITG formalism (Cherry and Lin, 2007; Haghighi et al., 2009; Zhang et al., 2008).</text>
              <doc_id>11</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>However, there are still alignment patterns which cannot be captured by phrases.</text>
              <doc_id>12</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>A simple example is connective in Chinese/English.</text>
              <doc_id>13</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>In English, two clauses are connected by merely one connective (like "although", "because") but in Chinese we need two connectives (e.g. There is a sentence pattern " &#34429; &#28982; &#20294; &#26159; &#65533; although ", where and are variables for clauses).</text>
              <doc_id>14</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>The English connective should then be aligned to two noncontiguous Chinese connectives, and such alignment pattern is not available in either wordlevel or phrase-level ITG.</text>
              <doc_id>15</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>As hierarchical phrase-based SMT (Chiang, 2007) is proved to be superior to simple phrase-based SMT, it is natural to ask, why don&#8223;t we further incorporate hierarchical phrase pairs (henceforth h-phrase pairs) into ITG?</text>
              <doc_id>16</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>In this paper we propose a ITG formalism and parsing algorithm using h-phrase pairs.</text>
              <doc_id>17</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>The ITG model involves much more parameters.</text>
              <doc_id>18</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>On the one hand, each phrase/h-phrase pair has its own probability or score.</text>
              <doc_id>19</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>It is not feasible to learn these parameters through discriminative/supervised learning since the repertoire of phrase pairs is much larger than the size of human-annotated alignment set.</text>
              <doc_id>20</doc_id>
              <sec_id>16</sec_id>
            </sentence>
            <sentence>
              <text>On the other hand, there are also a few useful features which cannot be estimated merely by unsupervised learning like EM.</text>
              <doc_id>21</doc_id>
              <sec_id>17</sec_id>
            </sentence>
            <sentence>
              <text>Inspired by Fraser et al. (2006), we propose a semi-supervised learning algorithm which combines the merits of both discrimina-</text>
              <doc_id>22</doc_id>
              <sec_id>18</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Coling 2010: Poster Volume, pages 730&#8211;738, Beijing, August 2010 tive training (error minimization) and approximate EM (estimation of numerous parameters).</text>
              <doc_id>23</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The ITG model augmented with the learning algorithm is shown by experiment results to improve significantly both alignment quality and translation performance.</text>
              <doc_id>24</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In the following, we will explain, step-by-step, how to incorporate hierarchical phrase pairs into the ITG formalism (Section 2) and in ITG parsing (Section 3).</text>
              <doc_id>25</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The semi-supervised training method is elaborated in Section 4.</text>
              <doc_id>26</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The merits of the complete system are illustrated with the experiments described in Section 5.</text>
              <doc_id>27</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 ITG Formalisms 2.1 W-ITG : ITG with only word pairs</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 W-ITG : ITG with only word pairs</title>
            <text>The simplest formulation of ITG contains three types of rules: terminal unary rules &#65533; , where e and f represent words (possibly a null word, &#949;) in the English and foreign language respectively, and the binary rules &#65533; and &#65533; , which refer to that the component English and foreign phrases are combined in the same and inverted order respectively. From the viewpoint of word alignment, the terminal unary rules provide the links of word pairs, whereas the binary rules represent the reordering factor. Note also that the alignment between two phrase pairs is always composed of the alignment between word pairs (c.f. Figure 1(a) and (b)). The Figure 1 also shows ITG can handle the cases where two languages share the same (Figure 1(a)) and different (Figure 1(b)) word order
e2
e1
e2
e1
f1 f2 f1 f2 (a) X &#8594;[X,X] (b) X &#8594;&#65533; X, X &#65533;
f1 f2
(c) X &#8594; [e,f ] e3
e2
e1
e2
e1
f1 f2 f3
(d) X &#8594; [e1Xe3, f1Xf3]
Figure 1. Four ways in which ITG can analyze a multi-word span pair.
Such a formulation has two drawbacks. First of all, the simple ITG leads to redundancy if word alignment is the sole purpose of applying ITG. For instance, there are two parses for three consecutive word pairs, viz.
and . The problem of redundancy is fixed by adopting ITG normal form. The ITG normal form grammar as used in this paper is described in Appendix A.
The second drawback is that ITG fails to produce certain alignment patterns. Its constraint that a word is not allowed to align to more than one word is indeed a strong limitation as no idiom or multi-word expression is allowed to align to a single word on the other side. Moreover, its reordering constraint makes it unable to produce the &#8222;inside-out&#8223; alignment pattern (c.f. Figure 2).
f1 f2 f3 f4
e1 e2 e3 e4
Figure 2. An example of inside-out alignment.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The simplest formulation of ITG contains three types of rules: terminal unary rules &#65533; , where e and f represent words (possibly a null word, &#949;) in the English and foreign language respectively, and the binary rules &#65533; and &#65533; , which refer to that the component English and foreign phrases are combined in the same and inverted order respectively.</text>
                  <doc_id>29</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>From the viewpoint of word alignment, the terminal unary rules provide the links of word pairs, whereas the binary rules represent the reordering factor.</text>
                  <doc_id>30</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Note also that the alignment between two phrase pairs is always composed of the alignment between word pairs (c.f.</text>
                  <doc_id>31</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 1(a) and (b)).</text>
                  <doc_id>32</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The Figure 1 also shows ITG can handle the cases where two languages share the same (Figure 1(a)) and different (Figure 1(b)) word order</text>
                  <doc_id>33</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e2</text>
                  <doc_id>34</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e1</text>
                  <doc_id>35</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e2</text>
                  <doc_id>36</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e1</text>
                  <doc_id>37</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f1 f2 f1 f2 (a) X &#8594;[X,X] (b) X &#8594;&#65533; X, X &#65533;</text>
                  <doc_id>38</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f1 f2</text>
                  <doc_id>39</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(c) X &#8594; [e,f ] e3</text>
                  <doc_id>40</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e2</text>
                  <doc_id>41</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e1</text>
                  <doc_id>42</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e2</text>
                  <doc_id>43</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e1</text>
                  <doc_id>44</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f1 f2 f3</text>
                  <doc_id>45</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(d) X &#8594; [e1Xe3, f1Xf3]</text>
                  <doc_id>46</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 1.</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Four ways in which ITG can analyze a multi-word span pair.</text>
                  <doc_id>48</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Such a formulation has two drawbacks.</text>
                  <doc_id>49</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>First of all, the simple ITG leads to redundancy if word alignment is the sole purpose of applying ITG.</text>
                  <doc_id>50</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For instance, there are two parses for three consecutive word pairs, viz.</text>
                  <doc_id>51</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and .</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The problem of redundancy is fixed by adopting ITG normal form.</text>
                  <doc_id>53</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The ITG normal form grammar as used in this paper is described in Appendix A.</text>
                  <doc_id>54</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The second drawback is that ITG fails to produce certain alignment patterns.</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Its constraint that a word is not allowed to align to more than one word is indeed a strong limitation as no idiom or multi-word expression is allowed to align to a single word on the other side.</text>
                  <doc_id>56</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Moreover, its reordering constraint makes it unable to produce the &#8222;inside-out&#8223; alignment pattern (c.f.</text>
                  <doc_id>57</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 2).</text>
                  <doc_id>58</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f1 f2 f3 f4</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e1 e2 e3 e4</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 2.</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>An example of inside-out alignment.</text>
                  <doc_id>62</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 P-ITG : ITG with Phrase Pairs</title>
            <text>A single word in one language is not always on a par with a single word in another language. For example, the Chinese word " &#30333; &#23467; " is equivalent to two words in English ("white house"). This problem is even worsened by segmentation errors (i.e. splitting a single word into more than one word). The one-to-one constraint in W-ITG is a serious limitation as in reality there are always segmentation or tokenization errors as well as idiomatic expressions. Therefore, researches like Cherry and Lin (2007), Haghighi et al. (2009) and Zhang et al. (2009) tackle this problem by enriching ITG, in addition to word pairs, with pairs of phrases (or blocks). That is, a sequence of source language word can be aligned, as a whole, to one (or a sequence of more than one) target language word.
These methods can be subsumed under the term phrase-based ITG (P-ITG), which enhances W-ITG by altering the definition of a terminal production to include phrases: &#65533; (c.f. Figure 1(c)). stands for English phrase and stands for foreign phrase. As an example, if there is a simple phrase pair &lt;white house, &#30333;
&#23467; &gt;, then it is transformed into the ITG rule &#65533; white house &#30333; &#23467; . An important question is how these phrase pairs can be formulated. Marcu and Wong (2002) propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristic for phrase extraction. This method suffers from computational complexity because it considers all possible phrases and all their possible alignments. Birch et al. (2006) propose a better and more efficient method of constraining the search space which does not contradict a given high confidence word alignment for each sentence. Our P- ITG collects all phrase pairs which are consistent with a word alignment matrix produced by a simpler word alignment model.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>A single word in one language is not always on a par with a single word in another language.</text>
                  <doc_id>63</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For example, the Chinese word " &#30333; &#23467; " is equivalent to two words in English ("white house").</text>
                  <doc_id>64</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This problem is even worsened by segmentation errors (i.e. splitting a single word into more than one word).</text>
                  <doc_id>65</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The one-to-one constraint in W-ITG is a serious limitation as in reality there are always segmentation or tokenization errors as well as idiomatic expressions.</text>
                  <doc_id>66</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, researches like Cherry and Lin (2007), Haghighi et al. (2009) and Zhang et al. (2009) tackle this problem by enriching ITG, in addition to word pairs, with pairs of phrases (or blocks).</text>
                  <doc_id>67</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>That is, a sequence of source language word can be aligned, as a whole, to one (or a sequence of more than one) target language word.</text>
                  <doc_id>68</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>These methods can be subsumed under the term phrase-based ITG (P-ITG), which enhances W-ITG by altering the definition of a terminal production to include phrases: &#65533; (c.f.</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 1(c)).</text>
                  <doc_id>70</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>stands for English phrase and stands for foreign phrase.</text>
                  <doc_id>71</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>As an example, if there is a simple phrase pair &lt;white house, &#30333;</text>
                  <doc_id>72</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#23467; &gt;, then it is transformed into the ITG rule &#65533; white house &#30333; &#23467; .</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>An important question is how these phrase pairs can be formulated.</text>
                  <doc_id>74</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Marcu and Wong (2002) propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristic for phrase extraction.</text>
                  <doc_id>75</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This method suffers from computational complexity because it considers all possible phrases and all their possible alignments.</text>
                  <doc_id>76</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Birch et al. (2006) propose a better and more efficient method of constraining the search space which does not contradict a given high confidence word alignment for each sentence.</text>
                  <doc_id>77</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Our P- ITG collects all phrase pairs which are consistent with a word alignment matrix produced by a simpler word alignment model.</text>
                  <doc_id>78</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>2.3 HP-ITG : P-ITG with H-Phrase pairs</title>
            <text>P-ITG is the first enhancement of ITG to capture the linguistic phenomenon that more than one word of a language may function as a single unit, so that these words should be aligned to a single unit of another language. But P-ITG can only treat contiguous words as a single unit, and therefore cannot handle the single units of noncontiguous words. Apart from sentence connectives as mentioned in Section 1, there is also the example that the single word &#8220;since&#8221; in English corresponds to two non-adjacent words " &#33258; " and " &#20197; &#26469; " as shown the following sentence pair:
&#33258; &#19978; &#21608; &#26411; &#20197; &#26469; , &#25105; &#19968; &#30452; &#22312; &#29983; &#30149; .
I have been ill since last weekend .
No matter whether it is P-ITG or phrase-based SMT, the very notion of phrase pair is not helpful because this example is simply handled by enumerating all possible contiguous sequences involving the words " &#33258; " and " &#20197; &#26469; ", and thus subject to serious data sparseness. The lesson learned from hierarchical phrase-based SMT is that the modeling of non-contiguous word sequence can be very simple if we allow rules involving h-phrase pairs, like:
&#65533; since &#33258; &#20197; &#26469;
where is a placeholder for substituting a phrase pair like " &#19978; &#21608; &#26411; /last weekend".
H-phrase pairs can also perform reordering, as illustrated by the well-known example from Chiang (2007), &#65533; have with &#19982; &#26377; , for the following bilingual sentence fragment:
&#19982; &#21271; &#38889; &#26377; &#37030; &#20132;
have diplomatic relations with North Korea
The potential of intra-phrase reordering may also help us to capture those alignment patterns like the &#8222;inside-out&#8223; pattern.
All these merits of h-phrase pairs motivate a ITG formalism, viz. hierarchical phrase-based ITG (HP-ITG), which employs not only simple phrase pairs but also hierarchical ones. The ITG grammar is enriched with rules of the format: &#65533; where and refer to either a phrase or h-phrase (c.f. Figure 1(d)) pair in English and foreign language respectively 2 . Note that, although the format of HP-ITG is similar to P-ITG, it is much more difficult to handle rules with h- phrase pairs in ITG parsing, which will be elaborated in the next section.
It is again an important question how to formulate the h-phrase pairs. Similar to P-ITG, the h-phrase pairs are obtained by extracting the h- phrase pairs which are consistent with a word alignment matrix produced by some simpler word alignment model.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>P-ITG is the first enhancement of ITG to capture the linguistic phenomenon that more than one word of a language may function as a single unit, so that these words should be aligned to a single unit of another language.</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>But P-ITG can only treat contiguous words as a single unit, and therefore cannot handle the single units of noncontiguous words.</text>
                  <doc_id>80</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Apart from sentence connectives as mentioned in Section 1, there is also the example that the single word &#8220;since&#8221; in English corresponds to two non-adjacent words " &#33258; " and " &#20197; &#26469; " as shown the following sentence pair:</text>
                  <doc_id>81</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#33258; &#19978; &#21608; &#26411; &#20197; &#26469; , &#25105; &#19968; &#30452; &#22312; &#29983; &#30149; .</text>
                  <doc_id>82</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>I have been ill since last weekend .</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>No matter whether it is P-ITG or phrase-based SMT, the very notion of phrase pair is not helpful because this example is simply handled by enumerating all possible contiguous sequences involving the words " &#33258; " and " &#20197; &#26469; ", and thus subject to serious data sparseness.</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The lesson learned from hierarchical phrase-based SMT is that the modeling of non-contiguous word sequence can be very simple if we allow rules involving h-phrase pairs, like:</text>
                  <doc_id>85</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533; since &#33258; &#20197; &#26469;</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where is a placeholder for substituting a phrase pair like " &#19978; &#21608; &#26411; /last weekend".</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>H-phrase pairs can also perform reordering, as illustrated by the well-known example from Chiang (2007), &#65533; have with &#19982; &#26377; , for the following bilingual sentence fragment:</text>
                  <doc_id>88</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#19982; &#21271; &#38889; &#26377; &#37030; &#20132;</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>have diplomatic relations with North Korea</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The potential of intra-phrase reordering may also help us to capture those alignment patterns like the &#8222;inside-out&#8223; pattern.</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>All these merits of h-phrase pairs motivate a ITG formalism, viz.</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>hierarchical phrase-based ITG (HP-ITG), which employs not only simple phrase pairs but also hierarchical ones.</text>
                  <doc_id>93</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The ITG grammar is enriched with rules of the format: &#65533; where and refer to either a phrase or h-phrase (c.f.</text>
                  <doc_id>94</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 1(d)) pair in English and foreign language respectively 2 .</text>
                  <doc_id>95</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Note that, although the format of HP-ITG is similar to P-ITG, it is much more difficult to handle rules with h- phrase pairs in ITG parsing, which will be elaborated in the next section.</text>
                  <doc_id>96</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It is again an important question how to formulate the h-phrase pairs.</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Similar to P-ITG, the h-phrase pairs are obtained by extracting the h- phrase pairs which are consistent with a word alignment matrix produced by some simpler word alignment model.</text>
                  <doc_id>98</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 ITG Parsing</title>
        <text>Based on the rules, W-ITG word alignment is done in a similar way to chart parsing (Wu, 1997). The base step applies all relevant terminal unary rules to establish the links of word pairs. The word pairs are then combined into span pairs in all possible ways. Larger and larger span pairs are recursively built until the sentence pair is built.
Figure 3(a) shows one possible derivation for a toy example sentence pair with three words in each sentence. Each node (rectangle) represents a pair, marked with certain phrase category, of
2 Haghighi et al. (2009) impose some rules which look like
h-phrase pairs, but their rules are essentially h-phrase pairs with at most one &#8222; &#8223; only, added with the constraint that each &#8222; &#8223; covers only one word.
A:[e1,e3]/[f1,f3]
{e1/f2,e2/f1,e3/f3}
A&#8594;[B,C]
A:[e1,e3]/[f1,f3]
{e1/f2,e2/f1,e3/f3} , {e1/f1,e2/f2,e3,f3}
A&#8594;[A,C]
C:[e1,e1]/[f2,f2]
{e1/f2}
(a)
C:[e2,e2]/[f1,f1]
{e2/f1}
C:[e1,e1]/[f2,f2]
{e1/f2}
foreign span (F-span) and English span (E-span) (the upper half of the rectangle) and the associated alignment hypothesis (the lower half). Each graph like Figure 3(a) shows only one derivation and also only one alignment hypothesis.
The various derivations in ITG parsing can be compactly represented in hypergraph (Klein et al., 2001) like Figure 3(b). Each hypernode (rectangle) comprises both a span pair (upper half) and the list of possible alignment hypotheses (lower half) for that span pair. The hyperedges show how larger span pairs are derived from smaller span pairs. Note that hypernode may have more than one alignment hypothesis, since a hypernode may be derived through more than one hyperedge (e.g. the topmost hypernode in Figure 3(b)). Due to the use of normal form, the hypotheses of a span pair are different from each other.
In the case of P-ITG parsing, each span pair does not only examine all possible combinations of sub-span pairs using binary rules, but also checks if the yield of that span pair is exactly the same as that phrase pair. If so, then this span pair is treated as a valid leaf node in the parse tree. Moreover, in order to enable the parse tree produce a complete word aligned matrix as byproduct, the alignment links within the phrase pair (which are recorded when the phrase pair is extracted from a word aligned matrix produced by a simpler model) are taken as an alternative alignment hypothesis of that span pair.
In the case of HP-ITG parsing, an ITG rule like &#65533; have with &#19982; &#26377; (originated from the hierarchical rule like &#65533; &lt; &#19982;
&#26377; , have with &gt;), is processed in the following manner: 1) Each span pair checks if it
C:[e2,e2]/[f1,f1]
{e2/f1}
contains the lexical anchors: "have", "with"," &#19982; " and " &#26377; "; 2) each span pair checks if the remaining words in its yield can form two sub-span pairs which fit the reordering constraint among
and (Note that span pairs of any category in the ITG normal form grammar can substitute for or ). 3) If both conditions hold, then the span pair is assigned an alignment hypothesis which combines the alignment links among the lexical anchors and those links among the subspan pairs.
C:[e1,e2]/[f1,f2]
{e1/f2,e1/f1, e2/f1,e2/f2} C:[e3,e3]/[f3,f3]
e1 e2 e3
f1 f2 f3
(a)
A&#8594;[C,C]
{e3/f3}
C:[e3,e3]/[f3,f3]
{e3/f3}
C:[e1,e1]/[f1,f1]
Figure 3. Example ITG parses in graph (a) and hypergraph (b).
(b)
A:[e1,e3]/[f1,f3]
{e1/f2,e1/f1,e2/f1,e2/f2,e3/f3} , {e1/f1,e1/f3,e3/f1,e3/f3,e2,f2}
(c)
{e1/f1}
e1 e2 e3
f1 f2 f3
C:[e2,e2]/[f2,f2]
{e2/f2}
C:[e2,e2]/[f2,f2]
(b)
{e2/f2}
A&#8594;[e1Xe3/f1Xf3,C]
e1Xe3/f1Xf3: [e1Xe3]/[f1Xf3] {e1/f3,e1/f1, e3/f3,e3/f1}
Figure 4. Phrase/h-phrase in hypergraph.
Figure 4(c) shows an example how to use phrase pair and h-phrase pairs in hypergraph. Figure 4(a) and Figure 4(b) refer to alignment matrixes which cannot be generated by W-ITG, because of the one-to-one assumption. Figure 4(c) shows how the span pair [e1,e3]/[f1,f3] can be generated in two ways: one is combining a phrase pair and a word pair directly, and the other way is replacing the X in the h-phrase pair with a word pair. Here we only show how h- phrase pairs with one variable be used during the
parsing, and h-phrase pairs with more than one variable can be used in a similar way.
The original (unsupervised) ITG algorithm has complexity of O(n 6 ). When extended to supervised/discriminative framework, ITG runs even more slowly. Therefore all attempts to ITG alignment come with some pruning method. Zhang and Gildea (2005) show that Model 1 (Brown et al., 1993) probabilities of the word pairs inside and outside a span pair are useful. Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O(n 4 ). Tic-tac-toe pruning method is adopted in this paper.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Based on the rules, W-ITG word alignment is done in a similar way to chart parsing (Wu, 1997).</text>
              <doc_id>99</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The base step applies all relevant terminal unary rules to establish the links of word pairs.</text>
              <doc_id>100</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The word pairs are then combined into span pairs in all possible ways.</text>
              <doc_id>101</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Larger and larger span pairs are recursively built until the sentence pair is built.</text>
              <doc_id>102</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Figure 3(a) shows one possible derivation for a toy example sentence pair with three words in each sentence.</text>
              <doc_id>103</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each node (rectangle) represents a pair, marked with certain phrase category, of</text>
              <doc_id>104</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2 Haghighi et al. (2009) impose some rules which look like</text>
              <doc_id>105</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>h-phrase pairs, but their rules are essentially h-phrase pairs with at most one &#8222; &#8223; only, added with the constraint that each &#8222; &#8223; covers only one word.</text>
              <doc_id>106</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A:[e1,e3]/[f1,f3]</text>
              <doc_id>107</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{e1/f2,e2/f1,e3/f3}</text>
              <doc_id>108</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A&#8594;[B,C]</text>
              <doc_id>109</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A:[e1,e3]/[f1,f3]</text>
              <doc_id>110</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{e1/f2,e2/f1,e3/f3} , {e1/f1,e2/f2,e3,f3}</text>
              <doc_id>111</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A&#8594;[A,C]</text>
              <doc_id>112</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>C:[e1,e1]/[f2,f2]</text>
              <doc_id>113</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{e1/f2}</text>
              <doc_id>114</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(a)</text>
              <doc_id>115</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>C:[e2,e2]/[f1,f1]</text>
              <doc_id>116</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{e2/f1}</text>
              <doc_id>117</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>C:[e1,e1]/[f2,f2]</text>
              <doc_id>118</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{e1/f2}</text>
              <doc_id>119</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>foreign span (F-span) and English span (E-span) (the upper half of the rectangle) and the associated alignment hypothesis (the lower half).</text>
              <doc_id>120</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each graph like Figure 3(a) shows only one derivation and also only one alignment hypothesis.</text>
              <doc_id>121</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The various derivations in ITG parsing can be compactly represented in hypergraph (Klein et al., 2001) like Figure 3(b).</text>
              <doc_id>122</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each hypernode (rectangle) comprises both a span pair (upper half) and the list of possible alignment hypotheses (lower half) for that span pair.</text>
              <doc_id>123</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The hyperedges show how larger span pairs are derived from smaller span pairs.</text>
              <doc_id>124</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Note that hypernode may have more than one alignment hypothesis, since a hypernode may be derived through more than one hyperedge (e.g. the topmost hypernode in Figure 3(b)).</text>
              <doc_id>125</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Due to the use of normal form, the hypotheses of a span pair are different from each other.</text>
              <doc_id>126</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the case of P-ITG parsing, each span pair does not only examine all possible combinations of sub-span pairs using binary rules, but also checks if the yield of that span pair is exactly the same as that phrase pair.</text>
              <doc_id>127</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>If so, then this span pair is treated as a valid leaf node in the parse tree.</text>
              <doc_id>128</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, in order to enable the parse tree produce a complete word aligned matrix as byproduct, the alignment links within the phrase pair (which are recorded when the phrase pair is extracted from a word aligned matrix produced by a simpler model) are taken as an alternative alignment hypothesis of that span pair.</text>
              <doc_id>129</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the case of HP-ITG parsing, an ITG rule like &#65533; have with &#19982; &#26377; (originated from the hierarchical rule like &#65533; &lt; &#19982;</text>
              <doc_id>130</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#26377; , have with &gt;), is processed in the following manner: 1) Each span pair checks if it</text>
              <doc_id>131</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>C:[e2,e2]/[f1,f1]</text>
              <doc_id>132</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{e2/f1}</text>
              <doc_id>133</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>contains the lexical anchors: "have", "with"," &#19982; " and " &#26377; "; 2) each span pair checks if the remaining words in its yield can form two sub-span pairs which fit the reordering constraint among</text>
              <doc_id>134</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and (Note that span pairs of any category in the ITG normal form grammar can substitute for or ).</text>
              <doc_id>135</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>3) If both conditions hold, then the span pair is assigned an alignment hypothesis which combines the alignment links among the lexical anchors and those links among the subspan pairs.</text>
              <doc_id>136</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>C:[e1,e2]/[f1,f2]</text>
              <doc_id>137</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{e1/f2,e1/f1, e2/f1,e2/f2} C:[e3,e3]/[f3,f3]</text>
              <doc_id>138</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e1 e2 e3</text>
              <doc_id>139</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>f1 f2 f3</text>
              <doc_id>140</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(a)</text>
              <doc_id>141</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A&#8594;[C,C]</text>
              <doc_id>142</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{e3/f3}</text>
              <doc_id>143</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>C:[e3,e3]/[f3,f3]</text>
              <doc_id>144</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{e3/f3}</text>
              <doc_id>145</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>C:[e1,e1]/[f1,f1]</text>
              <doc_id>146</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Figure 3.</text>
              <doc_id>147</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Example ITG parses in graph (a) and hypergraph (b).</text>
              <doc_id>148</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(b)</text>
              <doc_id>149</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A:[e1,e3]/[f1,f3]</text>
              <doc_id>150</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{e1/f2,e1/f1,e2/f1,e2/f2,e3/f3} , {e1/f1,e1/f3,e3/f1,e3/f3,e2,f2}</text>
              <doc_id>151</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(c)</text>
              <doc_id>152</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{e1/f1}</text>
              <doc_id>153</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e1 e2 e3</text>
              <doc_id>154</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>f1 f2 f3</text>
              <doc_id>155</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>C:[e2,e2]/[f2,f2]</text>
              <doc_id>156</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{e2/f2}</text>
              <doc_id>157</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>C:[e2,e2]/[f2,f2]</text>
              <doc_id>158</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(b)</text>
              <doc_id>159</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>{e2/f2}</text>
              <doc_id>160</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A&#8594;[e1Xe3/f1Xf3,C]</text>
              <doc_id>161</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e1Xe3/f1Xf3: [e1Xe3]/[f1Xf3] {e1/f3,e1/f1, e3/f3,e3/f1}</text>
              <doc_id>162</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Figure 4.</text>
              <doc_id>163</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Phrase/h-phrase in hypergraph.</text>
              <doc_id>164</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Figure 4(c) shows an example how to use phrase pair and h-phrase pairs in hypergraph.</text>
              <doc_id>165</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Figure 4(a) and Figure 4(b) refer to alignment matrixes which cannot be generated by W-ITG, because of the one-to-one assumption.</text>
              <doc_id>166</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Figure 4(c) shows how the span pair [e1,e3]/[f1,f3] can be generated in two ways: one is combining a phrase pair and a word pair directly, and the other way is replacing the X in the h-phrase pair with a word pair.</text>
              <doc_id>167</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Here we only show how h- phrase pairs with one variable be used during the</text>
              <doc_id>168</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>parsing, and h-phrase pairs with more than one variable can be used in a similar way.</text>
              <doc_id>169</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The original (unsupervised) ITG algorithm has complexity of O(n 6 ).</text>
              <doc_id>170</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>When extended to supervised/discriminative framework, ITG runs even more slowly.</text>
              <doc_id>171</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Therefore all attempts to ITG alignment come with some pruning method.</text>
              <doc_id>172</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Zhang and Gildea (2005) show that Model 1 (Brown et al., 1993) probabilities of the word pairs inside and outside a span pair are useful.</text>
              <doc_id>173</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O(n 4 ).</text>
              <doc_id>174</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Tic-tac-toe pruning method is adopted in this paper.</text>
              <doc_id>175</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Semi-supervised Training</title>
        <text>The original formulation of ITG (W-ITG) is a generative model in which the ITG tree of a sentence pair is produced by a set of rules. The parameters of these rules are trained by EM. Certainly it is difficult to add more non-independent features in such a generative model, and therefore Cherry et al. (2006) and Haghighi et al. (2009) used a discriminative model to incorporate features to achieve state-of-art alignment performance.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The original formulation of ITG (W-ITG) is a generative model in which the ITG tree of a sentence pair is produced by a set of rules.</text>
              <doc_id>176</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The parameters of these rules are trained by EM.</text>
              <doc_id>177</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Certainly it is difficult to add more non-independent features in such a generative model, and therefore Cherry et al. (2006) and Haghighi et al. (2009) used a discriminative model to incorporate features to achieve state-of-art alignment performance.</text>
              <doc_id>178</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 HP-DITG : Discriminative HP-ITG</title>
            <text>We also use a discriminative model to assign score to an alignment candidate for a sentence pair ( ) as probability from a log-linear model (Liu et al., 2005; Moore, 2006):
(1)
where each is some feature about the alignment matrix, and each &#955; is the weight of the corresponding feature. The discriminative version of W-ITG, P-ITG, and HP-ITG are then called W-DITG, P-DITG, and HP-DITG respectively.
There are two kinds of parameters in (1) to be learned. The first is the values of the features &#936;. Most features are indeed about the probabilities of the phrase/h-phrase pairs and there are too many of them to be trained from a labeled data set of limited size. Thus the feature values are trained by approximate EM. The other kind of parameters is feature weights &#955;, which are trained by an error minimization method. The discriminative training of &#955; and the approximate EM training of &#936; are integrated into a semisupervised training framework similar to EMD3 (Fraser and Marcu, 2006).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We also use a discriminative model to assign score to an alignment candidate for a sentence pair ( ) as probability from a log-linear model (Liu et al., 2005; Moore, 2006):</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(1)</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where each is some feature about the alignment matrix, and each &#955; is the weight of the corresponding feature.</text>
                  <doc_id>181</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The discriminative version of W-ITG, P-ITG, and HP-ITG are then called W-DITG, P-DITG, and HP-DITG respectively.</text>
                  <doc_id>182</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>There are two kinds of parameters in (1) to be learned.</text>
                  <doc_id>183</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The first is the values of the features &#936;.</text>
                  <doc_id>184</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Most features are indeed about the probabilities of the phrase/h-phrase pairs and there are too many of them to be trained from a labeled data set of limited size.</text>
                  <doc_id>185</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Thus the feature values are trained by approximate EM.</text>
                  <doc_id>186</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The other kind of parameters is feature weights &#955;, which are trained by an error minimization method.</text>
                  <doc_id>187</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The discriminative training of &#955; and the approximate EM training of &#936; are integrated into a semisupervised training framework similar to EMD3 (Fraser and Marcu, 2006).</text>
                  <doc_id>188</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Discriminative Training of &#955;</title>
            <text>MERT (Och, 2003) is used to train feature weights &#955;. MERT estimates model parameters with the objective of minimizing certain measure of translation errors (or maximizing certain performance measure of translation quality) for a development corpus. Given an SMT system which produces, with model parameters , the K-best candidate translations for a source sentence , and an error measure of a particular candidate with respect to the reference translation , the optimal parameter values will be:
MERT for DITG applies the same equation for parameter tuning, with different interpretation of the components in the equation. Instead of a development corpus with reference translations, we have a collection of training samples, each of which is a sentence pair with annotated alignment result. The ITG parser outputs for each sentence pair a K-best list of alignment result based on the current parameter values . The MERT module for DITG takes alignment F-score of a sentence pair as the performance measure. Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>MERT (Och, 2003) is used to train feature weights &#955;. MERT estimates model parameters with the objective of minimizing certain measure of translation errors (or maximizing certain performance measure of translation quality) for a development corpus.</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Given an SMT system which produces, with model parameters , the K-best candidate translations for a source sentence , and an error measure of a particular candidate with respect to the reference translation , the optimal parameter values will be:</text>
                  <doc_id>190</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>MERT for DITG applies the same equation for parameter tuning, with different interpretation of the components in the equation.</text>
                  <doc_id>191</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Instead of a development corpus with reference translations, we have a collection of training samples, each of which is a sentence pair with annotated alignment result.</text>
                  <doc_id>192</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The ITG parser outputs for each sentence pair a K-best list of alignment result based on the current parameter values .</text>
                  <doc_id>193</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The MERT module for DITG takes alignment F-score of a sentence pair as the performance measure.</text>
                  <doc_id>194</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Given an input sentence pair and the reference annotated alignment, MERT aims to maximize the F-score of DITG-produced alignment.</text>
                  <doc_id>195</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>4.3 Approximate EM Training of &#936;</title>
            <text>Three kinds of features (introduced in section 4.5 and 4.6) are calculated from training corpus given some initial alignment result: conditional probability of word pairs and two types of conditional probabilities for phrase/h-phrase.
3 For simplicity, we will also call our semi-supervised
framework as EMD.
The initial alignment result is far from perfect and so the feature values thus obtained are not optimized. There are too many features to be trained in supervised way. So, unsupervised training like EM is the best solution.
When EM is applied to our model, the E-step corresponds to calculating the probability for all the ITG trees, and the M-step corresponds to reestimate the feature values. As it is intractable to handle all possible ITG trees, instead we use the Viterbi parse to update the feature values. In other words, the training is a kind of approximate EM rather than EM. Word pairs are collected over Viterbi alignment and their conditional probabilities are estimated by MLE. As to phrase/h-phrase, if they are handled in a similar way, then there will be data sparseness (as there are much fewer phrase/h-phrase pairs in Viterbi parse tree than needed for reliable parameter estimation). Thus, we collect all phrase/h-phrase pairs which are consistent with the alignment links. The conditional probabilities are then estimated by MLE.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Three kinds of features (introduced in section 4.5 and 4.6) are calculated from training corpus given some initial alignment result: conditional probability of word pairs and two types of conditional probabilities for phrase/h-phrase.</text>
                  <doc_id>196</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3 For simplicity, we will also call our semi-supervised</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>framework as EMD.</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The initial alignment result is far from perfect and so the feature values thus obtained are not optimized.</text>
                  <doc_id>199</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>There are too many features to be trained in supervised way.</text>
                  <doc_id>200</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>So, unsupervised training like EM is the best solution.</text>
                  <doc_id>201</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>When EM is applied to our model, the E-step corresponds to calculating the probability for all the ITG trees, and the M-step corresponds to reestimate the feature values.</text>
                  <doc_id>202</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As it is intractable to handle all possible ITG trees, instead we use the Viterbi parse to update the feature values.</text>
                  <doc_id>203</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In other words, the training is a kind of approximate EM rather than EM.</text>
                  <doc_id>204</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Word pairs are collected over Viterbi alignment and their conditional probabilities are estimated by MLE.</text>
                  <doc_id>205</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>As to phrase/h-phrase, if they are handled in a similar way, then there will be data sparseness (as there are much fewer phrase/h-phrase pairs in Viterbi parse tree than needed for reliable parameter estimation).</text>
                  <doc_id>206</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Thus, we collect all phrase/h-phrase pairs which are consistent with the alignment links.</text>
                  <doc_id>207</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The conditional probabilities are then estimated by MLE.</text>
                  <doc_id>208</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>4.4 Semi-supervised training</title>
            <text>Figure 5. Semi-supervised training for HP-DITG.
The discriminative training (error minimization) of feature weights and the approximate EM learning of feature values are integrated in a single semi-supervised framework. Given an initial estimation of (estimated from an initial alignment matrix by some simpler word alignment model) and an initial estimation of , the discriminative training process and the approximate EM learning process are alternatively iterated until there is no more improvement. The sketch of the semi-supervised training is shown in Figure 5.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Figure 5.</text>
                  <doc_id>209</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Semi-supervised training for HP-DITG.</text>
                  <doc_id>210</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The discriminative training (error minimization) of feature weights and the approximate EM learning of feature values are integrated in a single semi-supervised framework.</text>
                  <doc_id>211</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Given an initial estimation of (estimated from an initial alignment matrix by some simpler word alignment model) and an initial estimation of , the discriminative training process and the approximate EM learning process are alternatively iterated until there is no more improvement.</text>
                  <doc_id>212</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The sketch of the semi-supervised training is shown in Figure 5.</text>
                  <doc_id>213</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>4</index>
            <title>4.5 Features for word pairs</title>
            <text>The following features about alignment link are used in W-DITG:
1) Word pair translation probabilities trained from HMM model (Vogel et al., 1996) and IBM model 4 (Brown et al., 1993).
2) Conditional link probability (Moore,
2006). 3) Association score rank features (Moore et
al., 2006). 4) Distortion features: counts of inversion
and concatenation.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The following features about alignment link are used in W-DITG:</text>
                  <doc_id>214</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1) Word pair translation probabilities trained from HMM model (Vogel et al., 1996) and IBM model 4 (Brown et al., 1993).</text>
                  <doc_id>215</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2) Conditional link probability (Moore,</text>
                  <doc_id>216</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2006).</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>3) Association score rank features (Moore et</text>
                  <doc_id>218</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>al., 2006).</text>
                  <doc_id>219</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>4) Distortion features: counts of inversion</text>
                  <doc_id>220</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and concatenation.</text>
                  <doc_id>221</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>5</index>
            <title>4.6 Features for phrase/h-phrase pairs</title>
            <text>For our HP-DITG model, the rule probabilities in both English-to-foreign and foreign-to- English directions are estimated and taken as features, in addition to those features in W- DITG, in the discriminative model of alignment hypothesis selection:
1) : The conditional probability of English phrase/h-phrase given foreign phrase/h-phrase.
2) : The conditional probability of foreign phrase/h-phrase given English phrase/h-phrase.
The features are calculated as described in section 4.3.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>For our HP-DITG model, the rule probabilities in both English-to-foreign and foreign-to- English directions are estimated and taken as features, in addition to those features in W- DITG, in the discriminative model of alignment hypothesis selection:</text>
                  <doc_id>222</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1) : The conditional probability of English phrase/h-phrase given foreign phrase/h-phrase.</text>
                  <doc_id>223</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2) : The conditional probability of foreign phrase/h-phrase given English phrase/h-phrase.</text>
                  <doc_id>224</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The features are calculated as described in section 4.3.</text>
                  <doc_id>225</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Evaluation</title>
        <text>Our experiments evaluate the performance of HP-DITG in both word alignment and translation in a Chinese-English setting, taking GI- ZA++, BerkeleyAligner (henceforth BERK) (Haghighi, et al., 2009), W-ITG as baselines. Word alignment quality is evaluated by recall, precision, and F-measure, while translation performance is evaluated by case-insensitive BLEU4.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Our experiments evaluate the performance of HP-DITG in both word alignment and translation in a Chinese-English setting, taking GI- ZA++, BerkeleyAligner (henceforth BERK) (Haghighi, et al., 2009), W-ITG as baselines.</text>
              <doc_id>226</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Word alignment quality is evaluated by recall, precision, and F-measure, while translation performance is evaluated by case-insensitive BLEU4.</text>
              <doc_id>227</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Experiment Data</title>
            <text>The small human annotated alignment set for discriminative training of feature weights is the same as that in Haghighi et al. (2009). The 491
sentence pairs in this dataset are adapted to our own Chinese word segmentation standard. 250 sentence pairs are used as training data and the other 241 are test data. The large, un-annotated bilingual corpus for approximate EM learning of feature values is FBIS, which is also the training set for our SMT systems.
In SMT experiments, our 5-gram language model is trained from the Xinhua section of the Gigaword corpus. The NIST&#8223;03 test set is used as our development corpus and the NIST&#8223;05 and NIST&#8223;08 test sets are our test sets. We use two kinds of state-of-the-art SMT systems. One is a phrase-based decoder (PBSMT) with a MaxEntbased distortion model (Xiong, et al., 2006), and the other is an implementation of hierarchical phrase-based model (HPBSMT) (Chiang, 2007). The phrase/rule table for these two systems is not generated from the terminal node of HP- DITG tree directly, but extracted from word alignment matrix (HP-DITG generated) using the same criterion as most phrase-based systems (Chiang, 2007).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The small human annotated alignment set for discriminative training of feature weights is the same as that in Haghighi et al. (2009).</text>
                  <doc_id>228</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The 491</text>
                  <doc_id>229</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>sentence pairs in this dataset are adapted to our own Chinese word segmentation standard.</text>
                  <doc_id>230</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>250 sentence pairs are used as training data and the other 241 are test data.</text>
                  <doc_id>231</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The large, un-annotated bilingual corpus for approximate EM learning of feature values is FBIS, which is also the training set for our SMT systems.</text>
                  <doc_id>232</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In SMT experiments, our 5-gram language model is trained from the Xinhua section of the Gigaword corpus.</text>
                  <doc_id>233</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The NIST&#8223;03 test set is used as our development corpus and the NIST&#8223;05 and NIST&#8223;08 test sets are our test sets.</text>
                  <doc_id>234</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We use two kinds of state-of-the-art SMT systems.</text>
                  <doc_id>235</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>One is a phrase-based decoder (PBSMT) with a MaxEntbased distortion model (Xiong, et al., 2006), and the other is an implementation of hierarchical phrase-based model (HPBSMT) (Chiang, 2007).</text>
                  <doc_id>236</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The phrase/rule table for these two systems is not generated from the terminal node of HP- DITG tree directly, but extracted from word alignment matrix (HP-DITG generated) using the same criterion as most phrase-based systems (Chiang, 2007).</text>
                  <doc_id>237</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 HP-DITG without EMD</title>
            <text>Our first experiment isolates the contribution of the various DITG alignment models from that of semi-supervised training. The feature values of the DITG models are estimated simply from IBM Model 4 using GIZA++. Apart from DITG, P-ITG, and HP-ITG as introduced in Section 2, we also include a variation, known as H-DITG, which covers h-phrase pairs but no simple phrase pairs at all. The experiment results are shown in Table 1.
Table 1. Performance gains with features for HP-DITG.
It is obvious that any form of ITG achieves better F-Measure than GIZA++. Without semisupervised training, however, our various DITG models cannot compete with BERK. Among the DITG models, it can be seen that precision is roughly the same in all cases, while W-ITG has the lowest recall, due to the limitation of one-toone matching. The improvement by (simple) phrase pairs is roughly the same as that by h- phrase pairs. And it is not surprising that the combination of both kinds of phrases achieve the best result.
Even HP-DITG does not achieve as high recall as BERK, it does produce promising alignment patterns that BERK fails to produce. For instance, for the following sentence pair:
&#33258; &#19978; &#21608; &#26411; &#20197; &#26469; , &#25105; &#19968; &#30452; &#22312; &#29983; &#30149; .
I have been ill since last weekend .
Both GIZA++ and BERK produce the pattern in Figure 6(a), while HP-DITG produces the better pattern in Figure 6(b) as it learns the h-phrase pair since &#33258; &#20197; &#26469; .
&#33258; &#19978; &#21608; &#26411; &#20197; &#26469;
since last weekend
(a): BERK/Giza++
&#33258; &#19978; &#21608; &#26411; &#20197; &#26469;
since last weekend
(b): HP-DITG
Figure 6. Partial alignment results.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Our first experiment isolates the contribution of the various DITG alignment models from that of semi-supervised training.</text>
                  <doc_id>238</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The feature values of the DITG models are estimated simply from IBM Model 4 using GIZA++.</text>
                  <doc_id>239</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Apart from DITG, P-ITG, and HP-ITG as introduced in Section 2, we also include a variation, known as H-DITG, which covers h-phrase pairs but no simple phrase pairs at all.</text>
                  <doc_id>240</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The experiment results are shown in Table 1.</text>
                  <doc_id>241</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 1.</text>
                  <doc_id>242</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Performance gains with features for HP-DITG.</text>
                  <doc_id>243</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It is obvious that any form of ITG achieves better F-Measure than GIZA++.</text>
                  <doc_id>244</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Without semisupervised training, however, our various DITG models cannot compete with BERK.</text>
                  <doc_id>245</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Among the DITG models, it can be seen that precision is roughly the same in all cases, while W-ITG has the lowest recall, due to the limitation of one-toone matching.</text>
                  <doc_id>246</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The improvement by (simple) phrase pairs is roughly the same as that by h- phrase pairs.</text>
                  <doc_id>247</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>And it is not surprising that the combination of both kinds of phrases achieve the best result.</text>
                  <doc_id>248</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Even HP-DITG does not achieve as high recall as BERK, it does produce promising alignment patterns that BERK fails to produce.</text>
                  <doc_id>249</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For instance, for the following sentence pair:</text>
                  <doc_id>250</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#33258; &#19978; &#21608; &#26411; &#20197; &#26469; , &#25105; &#19968; &#30452; &#22312; &#29983; &#30149; .</text>
                  <doc_id>251</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>I have been ill since last weekend .</text>
                  <doc_id>252</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Both GIZA++ and BERK produce the pattern in Figure 6(a), while HP-DITG produces the better pattern in Figure 6(b) as it learns the h-phrase pair since &#33258; &#20197; &#26469; .</text>
                  <doc_id>253</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#33258; &#19978; &#21608; &#26411; &#20197; &#26469;</text>
                  <doc_id>254</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>since last weekend</text>
                  <doc_id>255</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(a): BERK/Giza++</text>
                  <doc_id>256</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#33258; &#19978; &#21608; &#26411; &#20197; &#26469;</text>
                  <doc_id>257</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>since last weekend</text>
                  <doc_id>258</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(b): HP-DITG</text>
                  <doc_id>259</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 6.</text>
                  <doc_id>260</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Partial alignment results.</text>
                  <doc_id>261</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>5.3 Alignment Quality of HP-DITG with EMD</title>
            <text>Table 2. Semi-supervised Training Task on F- Measure.
The second experiment evaluates how the semi-supervised method of EMD improves HP- DITG with respect to word alignment quality. The results are shown in Table 2. In the table, EMD0 refers to the HP-DITG model before any EMD training; EMD1 refers to the model after the first iteration of training, and so on. It is empirically found that F-Measure is not improved after the third EMD iteration.
It can be observed that EMD succeeds to help HP-DITG improves feature value and weight estimation iteratively. When semi-supervised
training converges, the new HP-DITG model is better than before training by 2%, and better than BERK by 1%.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Table 2.</text>
                  <doc_id>262</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Semi-supervised Training Task on F- Measure.</text>
                  <doc_id>263</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The second experiment evaluates how the semi-supervised method of EMD improves HP- DITG with respect to word alignment quality.</text>
                  <doc_id>264</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The results are shown in Table 2.</text>
                  <doc_id>265</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In the table, EMD0 refers to the HP-DITG model before any EMD training; EMD1 refers to the model after the first iteration of training, and so on.</text>
                  <doc_id>266</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>It is empirically found that F-Measure is not improved after the third EMD iteration.</text>
                  <doc_id>267</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It can be observed that EMD succeeds to help HP-DITG improves feature value and weight estimation iteratively.</text>
                  <doc_id>268</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>When semi-supervised</text>
                  <doc_id>269</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>training converges, the new HP-DITG model is better than before training by 2%, and better than BERK by 1%.</text>
                  <doc_id>270</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>5.4 Translation Quality of HP-DITG with EMD</title>
            <text>The third experiment evaluates the same alignment models in the last experiment but with respect to translation quality, measured by caseinsensitive BLEU4. The results are shown in Table 3. Note that the differences between EMD3 and the two baselines are statistically significant.
Table 3. Semi-supervised Training Task on BLEU.
It can be observed that EMD improves SMT performance in most iterations in most cases. EMD does not always improve BLEU score because the objective function of the discriminative training in EMD is about alignment F- Measure rather than BLEU. And it is well known that the correlation between F-Measure and BLEU (Fraser and Marcu, 2007) is itself an intriguing problem.
The best HP-DITG leads to more than 1 BLEU point gain compared with GIZA++ on all datasets/MT models. Compared with BERK, EMD3 improves SMT performance significantly on NIST05 and slightly on NIST08.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The third experiment evaluates the same alignment models in the last experiment but with respect to translation quality, measured by caseinsensitive BLEU4.</text>
                  <doc_id>271</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The results are shown in Table 3.</text>
                  <doc_id>272</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Note that the differences between EMD3 and the two baselines are statistically significant.</text>
                  <doc_id>273</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 3.</text>
                  <doc_id>274</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Semi-supervised Training Task on BLEU.</text>
                  <doc_id>275</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It can be observed that EMD improves SMT performance in most iterations in most cases.</text>
                  <doc_id>276</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>EMD does not always improve BLEU score because the objective function of the discriminative training in EMD is about alignment F- Measure rather than BLEU.</text>
                  <doc_id>277</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>And it is well known that the correlation between F-Measure and BLEU (Fraser and Marcu, 2007) is itself an intriguing problem.</text>
                  <doc_id>278</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The best HP-DITG leads to more than 1 BLEU point gain compared with GIZA++ on all datasets/MT models.</text>
                  <doc_id>279</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Compared with BERK, EMD3 improves SMT performance significantly on NIST05 and slightly on NIST08.</text>
                  <doc_id>280</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>6</index>
        <title>6 Conclusion and Future Work</title>
        <text>In this paper, we propose an ITG formalism which employs the notion of phrase/h-phrase pairs, in order to remove the limitation of one-toone matching. The formalism is proved to enable an alignment model to capture the linguistic fact that a single concept is expressed in several noncontiguous words. Based on the formalism, we also propose a semi-supervised training method to optimize feature values and feature weights, which does not only improve the alignment quality but also machine translation performance significantly. Combining the formalism and semi-supervised training, we obtain better alignment and translation than the baselines of GIZA++ and BERK. A fundamental problem of our current framework is that we fail to obtain monotonic increment of BLEU score during the course of semisupervised training. In the future, therefore, we will try to take the BLEU score as our objective function in discriminative training. That is to certain extent inspired by Deng et al. (2008). Appendix A. The Normal Form Grammar Table 4 lists the ITG rules in normal form as used in this paper, which extend the normal form in Wu (1997) so as to handle the case of alignment to null.
In these rules, is the Start symbol; is the category for concatenating combination whereas for inverted combination. Rules (2) and (3) are inherited from Wu (1997). Rules (4) divide the terminal category into subcategories. Rule schema (6) subsumes all terminal unary rules for some English word and foreign word , and rule schemas (7) are unary rules for alignment to null. Rules (8) ensure all words linked to null are combined in left branching manner, while rules (9) ensure those words linked to null combine with some following, rather than preceding, word pair. (Note: Accordingly, all sentences must be ended by a special token , otherwise the last word(s) of a sentence cannot be linked to null.) If there are both English and foreign words linked to null, rule (5) ensures that those English words linked to null precede those foreign words linked to null.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper, we propose an ITG formalism which employs the notion of phrase/h-phrase pairs, in order to remove the limitation of one-toone matching.</text>
              <doc_id>281</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The formalism is proved to enable an alignment model to capture the linguistic fact that a single concept is expressed in several noncontiguous words.</text>
              <doc_id>282</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Based on the formalism, we also propose a semi-supervised training method to optimize feature values and feature weights, which does not only improve the alignment quality but also machine translation performance significantly.</text>
              <doc_id>283</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Combining the formalism and semi-supervised training, we obtain better alignment and translation than the baselines of GIZA++ and BERK.</text>
              <doc_id>284</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>A fundamental problem of our current framework is that we fail to obtain monotonic increment of BLEU score during the course of semisupervised training.</text>
              <doc_id>285</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In the future, therefore, we will try to take the BLEU score as our objective function in discriminative training.</text>
              <doc_id>286</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>That is to certain extent inspired by Deng et al. (2008).</text>
              <doc_id>287</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Appendix A.</text>
              <doc_id>288</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>The Normal Form Grammar Table 4 lists the ITG rules in normal form as used in this paper, which extend the normal form in Wu (1997) so as to handle the case of alignment to null.</text>
              <doc_id>289</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In these rules, is the Start symbol; is the category for concatenating combination whereas for inverted combination.</text>
              <doc_id>290</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Rules (2) and (3) are inherited from Wu (1997).</text>
              <doc_id>291</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Rules (4) divide the terminal category into subcategories.</text>
              <doc_id>292</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Rule schema (6) subsumes all terminal unary rules for some English word and foreign word , and rule schemas (7) are unary rules for alignment to null.</text>
              <doc_id>293</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Rules (8) ensure all words linked to null are combined in left branching manner, while rules (9) ensure those words linked to null combine with some following, rather than preceding, word pair.</text>
              <doc_id>294</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>(Note: Accordingly, all sentences must be ended by a special token , otherwise the last word(s) of a sentence cannot be linked to null.</text>
              <doc_id>295</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>) If there are both English and foreign words linked to null, rule (5) ensures that those English words linked to null precede those foreign words linked to null.</text>
              <doc_id>296</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1. Performance gains with features for HP-DITG.</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Precision Recall</cell>
              <cell>F-Measure</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>GIZA++</cell>
              <cell>0.826</cell>
              <cell>0.807</cell>
              <cell>0.816</cell>
            </row>
            <row>
              <cell>BERK</cell>
              <cell>0.917</cell>
              <cell>0.814</cell>
              <cell>0.862</cell>
            </row>
            <row>
              <cell>W-DITG</cell>
              <cell>0.912</cell>
              <cell>0.745</cell>
              <cell>0.820</cell>
            </row>
            <row>
              <cell>P-DITG</cell>
              <cell>0.913</cell>
              <cell>0.788</cell>
              <cell>0.846</cell>
            </row>
            <row>
              <cell>H-DITG</cell>
              <cell>0.913</cell>
              <cell>0.781</cell>
              <cell>0.842</cell>
            </row>
            <row>
              <cell>HP-DITG</cell>
              <cell>0.915</cell>
              <cell>0.795</cell>
              <cell>0.851</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2. Semi-supervised Training Task on F- Measure.</caption>
        <reference_text>In PAGE 7: ...  The  second  experiment  evaluates  how  the  semi-supervised method of EMD improves HP- DITG  with  respect  to  word  alignment  quality.  The  results  are  shown  in   Table2 .  In  the  table,  EMD0 refers to the HP-DITG model before any  EMD  training;  EMD1  refers  to the  model  after  the first iteration of training, and so on....</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>5.3</cell>
              <cell>Alignment  Quality  of  HP-DITG  with</cell>
              <cell>Alignment  Quality  of  HP-DITG  with</cell>
              <cell>Alignment  Quality  of  HP-DITG  with</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>E</cell>
              <cell>M</cell>
              <cell>D</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Precision</cell>
              <cell>Recall</cell>
              <cell>F- Measure</cell>
            </row>
            <row>
              <cell>GIZA++</cell>
              <cell>0.826</cell>
              <cell>0.807</cell>
              <cell>0.816</cell>
            </row>
            <row>
              <cell>BERK</cell>
              <cell>0.917</cell>
              <cell>0.814</cell>
              <cell>0.862</cell>
            </row>
            <row>
              <cell>EMD0</cell>
              <cell>0.915</cell>
              <cell>0.795</cell>
              <cell>0.851</cell>
            </row>
            <row>
              <cell>EMD1</cell>
              <cell>0.923</cell>
              <cell>0.814</cell>
              <cell>0.865</cell>
            </row>
            <row>
              <cell>EMD2</cell>
              <cell>0.930</cell>
              <cell>0.821</cell>
              <cell>0.872</cell>
            </row>
            <row>
              <cell>EMD3</cell>
              <cell>0.935</cell>
              <cell>0.819</cell>
              <cell>0.873</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TET</source>
        <caption>Table 3. Semi-supervised Training Task on BLEU.</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>PBSMT</cell>
              <cell>HPBSMT</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell></cell>
              <cell>05</cell>
              <cell>08</cell>
              <cell>05</cell>
              <cell>08</cell>
            </row>
            <row>
              <cell>GIZA++</cell>
              <cell>33.43</cell>
              <cell>23.89</cell>
              <cell>33.59</cell>
              <cell>24.39</cell>
            </row>
            <row>
              <cell>BERK</cell>
              <cell>33.76</cell>
              <cell>24.92</cell>
              <cell>34.22</cell>
              <cell>25.18</cell>
            </row>
            <row>
              <cell>EMD0</cell>
              <cell>34.02</cell>
              <cell>24.50</cell>
              <cell>34.30</cell>
              <cell>24.90</cell>
            </row>
            <row>
              <cell>EMD1</cell>
              <cell>34.29</cell>
              <cell>24.80</cell>
              <cell>34.77</cell>
              <cell>25.25</cell>
            </row>
            <row>
              <cell>EMD2</cell>
              <cell>34.25</cell>
              <cell>25.01</cell>
              <cell>35.04</cell>
              <cell>25.43</cell>
            </row>
            <row>
              <cell>EMD3</cell>
              <cell>34.42</cell>
              <cell>25.19</cell>
              <cell>34.82</cell>
              <cell>25.56</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TET</source>
        <caption>Table 4. ITG Rules in Normal Form.</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>1</cell>
              <cell>&#65533;</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>2</cell>
              <cell>&#65533;</cell>
            </row>
            <row>
              <cell>3</cell>
              <cell>&#65533;</cell>
            </row>
            <row>
              <cell>&#65533;</cell>
            </row>
            <row>
              <cell>4</cell>
              <cell>&#65533;</cell>
              <cell></cell>
            </row>
            <row>
              <cell>5</cell>
              <cell>&#65533;</cell>
              <cell></cell>
            </row>
            <row>
              <cell>6</cell>
              <cell>&#65533;</cell>
              <cell></cell>
            </row>
            <row>
              <cell>7</cell>
              <cell>&#65533;</cell>
              <cell>&#65533;</cell>
            </row>
            <row>
              <cell>8</cell>
              <cell>&#65533;</cell>
              <cell>&#65533;</cell>
            </row>
            <row>
              <cell>9</cell>
              <cell>&#65533;</cell>
              <cell>&#65533;</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Alexandra Birch</author>
          <author>Chris Callison-Burch</author>
          <author>Miles Osborne</author>
          <author>Phillipp Koehn</author>
        </authors>
        <title>Constraining the Phrase-Based, Joint Probability Statistical Translation Model.</title>
        <publication>Proceedings of the Workshop on Statistical Machine Translation.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Peter F Brown Brown</author>
          <author>Stephen A Della Pietra</author>
          <author>Vincent J Della Peitra</author>
          <author>Robert L Mercer</author>
        </authors>
        <title>None</title>
        <publication>The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics,</publication>
        <pages>19--2</pages>
        <date>1993</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Colin Cherry</author>
          <author>Dekang Lin</author>
        </authors>
        <title>Soft Syntactic Constraints for Word Alignment through Discriminative Training.</title>
        <publication>Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Colin Cherry</author>
          <author>Dekang Lin</author>
        </authors>
        <title>Inversion Transduction Grammar for Joint Phrasal Translation Modeling.</title>
        <publication>Proceedings of the Second Workshop on Syntax and Structure in Statistical Translation,</publication>
        <pages>17--24</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>Hierarchical Phrase-based Translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Yonggang Deng</author>
        </authors>
        <title>Jia Xu and Yuqing</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>6</id>
        <authors/>
        <title>Phrase Table Training For Precision and Recall: What Makes a Good Phrase and a Good Phrase Pair?.</title>
        <publication>Proceedings of the 7th International Conference on Human Language Technology Research and 46th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>1017--1026</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Alexander Fraser</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>SemiSupervised Training for StatisticalWord Alignment.</title>
        <publication>Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>769--776</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Alexander Fraser</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Measuring Word Alignment Quality for Statistical Machine Translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Aria Haghighi</author>
          <author>John Blitzer</author>
          <author>John DeNero</author>
          <author>Dan Klein</author>
        </authors>
        <title>Better Word Alignments with Supervised ITG Models.</title>
        <publication>Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language,</publication>
        <pages>923--931</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Dan Klein</author>
          <author>Christopher D Manning</author>
        </authors>
        <title>Parsing and Hypergraphs.</title>
        <publication>Proceedings of the 7th International Workshop on Parsing Technologies, Pages:17-19</publication>
        <pages>81--88</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Daniel Marcu</author>
          <author>William Wong</author>
        </authors>
        <title>A Phrase-Based, Joint Probability Model for Statistical Machine Translation.</title>
        <publication>Proceedings of 2002 Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>133--139</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Robert Moore</author>
          <author>Wen-tau Yih</author>
          <author>Andreas Bode</author>
        </authors>
        <title>Improved Discriminative Bilingual Word Alignment.</title>
        <publication>Proceedings of the 44rd Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>513--520</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>Proceedings of the 41rd Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>160--167</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>The Alignment Template Approach to Statistical Machine Translation.</title>
        <publication>None</publication>
        <pages>417--449</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Stephan Vogel</author>
          <author>Hermann Ney</author>
          <author>Christoph Tillmann</author>
        </authors>
        <title>HMM-based word alignment in statistical translation.</title>
        <publication>Proceedings of 16th International Conference on Computational Linguistics, Pages:</publication>
        <pages>836--841</pages>
        <date>1996</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Dekai Wu</author>
        </authors>
        <title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Deyi Xiong</author>
          <author>Qun Liu</author>
          <author>Shouxun Lin</author>
        </authors>
        <title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
        <publication>Proceedings of the 44rd Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>521--528</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Hao Zhang</author>
          <author>Daniel Gildea</author>
        </authors>
        <title>Stochastic Lexicalized Inversion Transduction Grammar for Alignment.</title>
        <publication>Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Birch et al. (2006)</string>
        <sentence_id>3117</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Brown et al., 1993</string>
        <sentence_id>3214</sentence_id>
        <char_offset>43</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>Brown et al., 1993</string>
        <sentence_id>3253</sentence_id>
        <char_offset>100</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>3</reference_id>
        <string>Cherry and Lin (2007)</string>
        <sentence_id>3107</sentence_id>
        <char_offset>27</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>3</reference_id>
        <string>Cherry and Lin, 2007</string>
        <sentence_id>3052</sentence_id>
        <char_offset>165</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>4</reference_id>
        <string>Chiang (2007)</string>
        <sentence_id>3128</sentence_id>
        <char_offset>90</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>4</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>3057</sentence_id>
        <char_offset>34</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>4</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>3275</sentence_id>
        <char_offset>177</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>4</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>3276</sentence_id>
        <char_offset>227</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>7</reference_id>
        <string>Fraser and Marcu, 2006</string>
        <sentence_id>3226</sentence_id>
        <char_offset>143</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>8</reference_id>
        <string>Fraser and Marcu, 2007</string>
        <sentence_id>3317</sentence_id>
        <char_offset>70</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>9</reference_id>
        <string>Haghighi, et al., 2009</string>
        <sentence_id>3320</sentence_id>
        <char_offset>173</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>9</reference_id>
        <string>Haghighi et al., 2009</string>
        <sentence_id>3048</sentence_id>
        <char_offset>138</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>9</reference_id>
        <string>Haghighi et al., 2009</string>
        <sentence_id>3052</sentence_id>
        <char_offset>187</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>9</reference_id>
        <string>Haghighi et al. (2009)</string>
        <sentence_id>3107</sentence_id>
        <char_offset>50</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>9</reference_id>
        <string>Haghighi et al. (2009)</string>
        <sentence_id>3146</sentence_id>
        <char_offset>2</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>9</reference_id>
        <string>Haghighi et al. (2009)</string>
        <sentence_id>3266</sentence_id>
        <char_offset>130</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>9</reference_id>
        <string>Haghighi et al. (2009)</string>
        <sentence_id>3267</sentence_id>
        <char_offset>110</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>11</reference_id>
        <string>Marcu and Wong (2002)</string>
        <sentence_id>3115</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>13</reference_id>
        <string>Och, 2003</string>
        <sentence_id>3227</sentence_id>
        <char_offset>6</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>15</reference_id>
        <string>Vogel et al., 1996</string>
        <sentence_id>3253</sentence_id>
        <char_offset>63</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>16</reference_id>
        <string>Wu (1997)</string>
        <sentence_id>3330</sentence_id>
        <char_offset>122</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>16</reference_id>
        <string>Wu (1997)</string>
        <sentence_id>3332</sentence_id>
        <char_offset>37</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>16</reference_id>
        <string>Wu, 1997</string>
        <sentence_id>3045</sentence_id>
        <char_offset>38</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>16</reference_id>
        <string>Wu, 1997</string>
        <sentence_id>3140</sentence_id>
        <char_offset>84</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>17</reference_id>
        <string>Xiong, et al., 2006</string>
        <sentence_id>3275</sentence_id>
        <char_offset>75</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>18</reference_id>
        <string>Zhang and Gildea (2005)</string>
        <sentence_id>3214</sentence_id>
        <char_offset>0</char_offset>
      </citation>
    </citations>
  </content>
</document>
