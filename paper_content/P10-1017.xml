<document>
  <filename>P10-1017</filename>
  <authors>
    <author>Jason Riesa</author>
  </authors>
  <title>Hierarchical Search for Word Alignment</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1
BLEU score increase over a state-of-the-art
syntax-based machine translation system.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We present a simple yet powerful hierarchical search algorithm for automatic word alignment.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We report results on Arabic-English word alignment and translation tasks.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>BLEU score increase over a state-of-the-art</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>syntax-based machine translation system.</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Automatic word alignment is generally accepted as a first step in training any statistical machine translation system. It is a vital prerequisite for generating translation tables, phrase tables, or syntactic transformation rules. Generative alignment models like IBM Model-4 (Brown et al., 1993) have been in wide use for over 15 years, and while not perfect (see Figure 1), they are completely unsupervised, requiring no annotated training data to learn alignments that have powered many current state-of-the-art translation system.
Today, there exist human-annotated alignments and an abundance of other information for many language pairs potentially useful for inducing accurate alignments. How can we take advantage of all of this data at our fingertips? Using feature functions that encode extra information is one good way. Unfortunately, as Moore (2005) points out, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story. This difficulty
the five
have been previous tests
to the target missile and one
limited
body .
other
&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533; &#65533;&#65533;&#65533;&#65533;
&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;
&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533; &#65533;
&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533; &#65533;
has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Lacoste- Julien et al., 2006; Moore et al., 2006).
We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment.
Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of possible alignments. Our algorithm yields a forest of
the man
ate the
bread
&#1575;&#1603;&#1604;
&#1575;&#1604;&#1585;&#1580;&#1604;
&#1575;&#1604;&#1582;&#1576;&#1586;
S
VP
&#1575;&#1603;&#1604;
NP NP
&#1575;&#1604;&#1585;&#1575;&#1580;&#1604;
&#1575;&#1604;&#1582;&#1576;&#1586;
the man ate the bread
word alignments, from which we can efficiently extract the k-best. We handle an arbitrary number of features, compute them efficiently, and score alignments using a linear model. We train the parameters of the model using averaged perceptron (Collins, 2002) modified for structured outputs, but can easily fit into a max-margin or related framework. Finally, we use relatively little training data to achieve accurate word alignments. Our model can generate arbitrary alignments and learn from arbitrary gold alignments.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Automatic word alignment is generally accepted as a first step in training any statistical machine translation system.</text>
              <doc_id>7</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It is a vital prerequisite for generating translation tables, phrase tables, or syntactic transformation rules.</text>
              <doc_id>8</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Generative alignment models like IBM Model-4 (Brown et al., 1993) have been in wide use for over 15 years, and while not perfect (see Figure 1), they are completely unsupervised, requiring no annotated training data to learn alignments that have powered many current state-of-the-art translation system.</text>
              <doc_id>9</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Today, there exist human-annotated alignments and an abundance of other information for many language pairs potentially useful for inducing accurate alignments.</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>How can we take advantage of all of this data at our fingertips?</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Using feature functions that encode extra information is one good way.</text>
              <doc_id>12</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Unfortunately, as Moore (2005) points out, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story.</text>
              <doc_id>13</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This difficulty</text>
              <doc_id>14</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the five</text>
              <doc_id>15</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>have been previous tests</text>
              <doc_id>16</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>to the target missile and one</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>limited</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>body .</text>
              <doc_id>19</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>other</text>
              <doc_id>20</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533; &#65533;&#65533;&#65533;&#65533;</text>
              <doc_id>21</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533; &#65533;</text>
              <doc_id>23</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533; &#65533;</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al., 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Lacoste- Julien et al., 2006; Moore et al., 2006).</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure.</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We borrow ideas from both k-best parsing (Klein and Manning, 2001; Huang and Chiang, 2005; Huang, 2008) and forest-based, and hierarchical phrase-based translation (Huang and Chiang, 2007; Chiang, 2007), and apply them to word alignment.</text>
              <doc_id>27</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of possible alignments.</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our algorithm yields a forest of</text>
              <doc_id>29</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the man</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>ate the</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>bread</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#1575;&#1603;&#1604;</text>
              <doc_id>33</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#1575;&#1604;&#1585;&#1580;&#1604;</text>
              <doc_id>34</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#1575;&#1604;&#1582;&#1576;&#1586;</text>
              <doc_id>35</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S</text>
              <doc_id>36</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>VP</text>
              <doc_id>37</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#1575;&#1603;&#1604;</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>NP NP</text>
              <doc_id>39</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#1575;&#1604;&#1585;&#1575;&#1580;&#1604;</text>
              <doc_id>40</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#1575;&#1604;&#1582;&#1576;&#1586;</text>
              <doc_id>41</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the man ate the bread</text>
              <doc_id>42</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>word alignments, from which we can efficiently extract the k-best.</text>
              <doc_id>43</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We handle an arbitrary number of features, compute them efficiently, and score alignments using a linear model.</text>
              <doc_id>44</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We train the parameters of the model using averaged perceptron (Collins, 2002) modified for structured outputs, but can easily fit into a max-margin or related framework.</text>
              <doc_id>45</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we use relatively little training data to achieve accurate word alignments.</text>
              <doc_id>46</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Our model can generate arbitrary alignments and learn from arbitrary gold alignments.</text>
              <doc_id>47</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Word Alignment as a Hypergraph</title>
        <text>Algorithm input The input to our alignment algorithm is a sentence-pair (e n 1 , f 1 m ) and a parse tree over one of the input sentences. In this work, we parse our English data, and for each sentence E = e n 1 , let T be its syntactic parse. To generate parse trees, we use the Berkeley parser (Petrov et al., 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree.
Overview We present a brief overview here and delve deeper in Section 2.1. Word alignments are built bottom-up on the parse tree. Each node v in the tree holds partial alignments sorted by score.
u11 u11 u12 u12 u13 u13
(a) Score the left corner alignment first. Assume it is the 1- best. Numbers in the rest of the boxes are hidden at this point.
(b) Expand the frontier of alignments. We are now looking for the 2nd best.
(c) Expand the frontier further. After this step we have our top k alignments.
Each partial alignment comprises the columns of the alignment matrix for the e-words spanned by v, and each is scored by a linear combination of feature functions. See Figure 2 for a small example. Initial partial alignments are enumerated and scored at preterminal nodes, each spanning a single column of the word alignment matrix. To speed up search, we can prune at each node, keeping a beam of size k. In the diagram depicted in Figure 2, the beam is size k = 5.
From here, we traverse the tree nodes bottomup, combining partial alignments from child nodes until we have constructed a single full alignment at the root node of the tree. If we are interested in the k-best, we continue to populate the root node until we have k alignments. 1
We use one set of feature functions for preterminal nodes, and another set for nonterminal nodes. This is analogous to local and nonlocal feature functions for parse-reranking used by Huang (2008). Using nonlocal features at a nonterminal node emits a combination cost for composing a set of child partial alignments.
Because combination costs come into play, we use cube pruning (Chiang, 2007) to approximate the k-best combinations at some nonterminal node v. Inference is exact when only local features are used.
Assumptions There are certain assumptions related to our search algorithm that we must make:
1 We use approximate dynamic programming to store
alignments, keeping only scored lists of pointers to initial single-column spans. Each item in the list is a derivation that implies a partial alignment.
(1) that using the structure of 1-best English syntactic parse trees is a reasonable way to frame and drive our search, and (2) that F-measure approximately decomposes over hyperedges. We perform an oracle experiment to validate these assumptions. We find the oracle for a given (T,e, f ) triple by proceeding through our search algorithm, forcing ourselves to always select correct links with respect to the gold alignment when possible, breaking ties arbitrarily. The the F 1 score of our oracle alignment is 98.8%, given this &#8220;perfect&#8221; model.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Algorithm input The input to our alignment algorithm is a sentence-pair (e n 1 , f 1 m ) and a parse tree over one of the input sentences.</text>
              <doc_id>48</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this work, we parse our English data, and for each sentence E = e n 1 , let T be its syntactic parse.</text>
              <doc_id>49</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>To generate parse trees, we use the Berkeley parser (Petrov et al., 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree.</text>
              <doc_id>50</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Overview We present a brief overview here and delve deeper in Section 2.1.</text>
              <doc_id>51</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Word alignments are built bottom-up on the parse tree.</text>
              <doc_id>52</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Each node v in the tree holds partial alignments sorted by score.</text>
              <doc_id>53</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>u11 u11 u12 u12 u13 u13</text>
              <doc_id>54</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(a) Score the left corner alignment first.</text>
              <doc_id>55</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Assume it is the 1- best.</text>
              <doc_id>56</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Numbers in the rest of the boxes are hidden at this point.</text>
              <doc_id>57</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(b) Expand the frontier of alignments.</text>
              <doc_id>58</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We are now looking for the 2nd best.</text>
              <doc_id>59</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(c) Expand the frontier further.</text>
              <doc_id>60</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>After this step we have our top k alignments.</text>
              <doc_id>61</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Each partial alignment comprises the columns of the alignment matrix for the e-words spanned by v, and each is scored by a linear combination of feature functions.</text>
              <doc_id>62</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>See Figure 2 for a small example.</text>
              <doc_id>63</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Initial partial alignments are enumerated and scored at preterminal nodes, each spanning a single column of the word alignment matrix.</text>
              <doc_id>64</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>To speed up search, we can prune at each node, keeping a beam of size k.</text>
              <doc_id>65</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In the diagram depicted in Figure 2, the beam is size k = 5.</text>
              <doc_id>66</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>From here, we traverse the tree nodes bottomup, combining partial alignments from child nodes until we have constructed a single full alignment at the root node of the tree.</text>
              <doc_id>67</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>If we are interested in the k-best, we continue to populate the root node until we have k alignments.</text>
              <doc_id>68</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>1</text>
              <doc_id>69</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We use one set of feature functions for preterminal nodes, and another set for nonterminal nodes.</text>
              <doc_id>70</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This is analogous to local and nonlocal feature functions for parse-reranking used by Huang (2008).</text>
              <doc_id>71</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Using nonlocal features at a nonterminal node emits a combination cost for composing a set of child partial alignments.</text>
              <doc_id>72</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Because combination costs come into play, we use cube pruning (Chiang, 2007) to approximate the k-best combinations at some nonterminal node v.</text>
              <doc_id>73</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Inference is exact when only local features are used.</text>
              <doc_id>74</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Assumptions There are certain assumptions related to our search algorithm that we must make:</text>
              <doc_id>75</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 We use approximate dynamic programming to store</text>
              <doc_id>76</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>alignments, keeping only scored lists of pointers to initial single-column spans.</text>
              <doc_id>77</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each item in the list is a derivation that implies a partial alignment.</text>
              <doc_id>78</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(1) that using the structure of 1-best English syntactic parse trees is a reasonable way to frame and drive our search, and (2) that F-measure approximately decomposes over hyperedges.</text>
              <doc_id>79</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We perform an oracle experiment to validate these assumptions.</text>
              <doc_id>80</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We find the oracle for a given (T,e, f ) triple by proceeding through our search algorithm, forcing ourselves to always select correct links with respect to the gold alignment when possible, breaking ties arbitrarily.</text>
              <doc_id>81</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The the F 1 score of our oracle alignment is 98.8%, given this &#8220;perfect&#8221; model.</text>
              <doc_id>82</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Hierarchical search</title>
            <text>Initial alignments We can construct a word alignment hierarchically, bottom-up, by making use of the structure inherent in syntactic parse trees. We can think of building a word alignment as filling in an M&#215;N matrix (Figure 1), and we begin by visiting each preterminal node in the tree. Each of these nodes spans a single e word. (Line 2 in Algorithm 1).
From here we can assign links from each e word to zero or more f words (Lines 6&#8211;14). At this level of the tree the span size is 1, and the partial alignment we have made spans a single column of the matrix. We can make many such partial alignments depending on the links selected. Lines 5 through 9 of Algorithm 1 enumerate either the null alignment, single-link alignments, or two-link alignments. Each partial alignment is scored and stored in a sorted heap (Lines 9 and 13).
In practice enumerating all two-link alignments can be prohibitive for long sentence pairs; we set a practical limit and score only pairwise combina-
Sentence 1
TOP 1
Algorithm 1: Hypergraph Alignment Input:
Source sentence e n 1 Target sentence f m 1 Parse tree T over e n 1 Set of feature functions h Weight vector w Beam size k
Output: A k-best list of alignments over e n 1 and f m 1
1 function ALIGN(e n 1 , f m 1 , T)
2 for v &#8712; T in bottom-up order do
3 &#945; v &#8592; &#8709;
4 if IS-PRETERMINALNODE(v) then
5 i &#8592; index-of(v)
6 for j = 0 to m do
7 links &#8592; (i, j)
8 score &#8592; w &#183; h(links, v, e n 1 , f m 1 )
9 PUSH(&#945; v , &#12296;score, links&#12297;, k )
10 for k = j + 1 to m do
11 links &#8592; (i, j), (i, k)
12 score &#8592; w &#183; h(links, v, e n 1 , f m 1 )
13 PUSH(&#945; v , &#12296;score, links&#12297;, k )
14 end
15 end
16 else
17 &#945; v &#8592; GROWSPAN(children(v), k)
18 end
19 end
20 end
21 function GROWSPAN(&#12296;u 1 , u 2 &#12297;, k)
22 return CUBEPRUNING(&#12296;&#945; u1 , &#945; u2 &#12297;, k,w,h)
23 end
tions of the top n = max { | f |
2 , 10} scoring singlelink alignments.
We limit the number of total partial alignments &#945; v kept at each node to k. If at any time we wish to push onto the heap a new partial alignment when the heap is full, we pop the current worst off the heap and replace it with our new partial alignment if its score is better than the current worst.
Building the hypergraph We now visit internal nodes (Line 16) in the tree in bottom-up order. At each nonterminal node v we wish to combine the partial alignments of its children u 1 , . . . , u c . We use cube pruning (Chiang, 2007; Huang and Chiang, 2007) to select the k-best combinations of the partial alignments of u 1 , . . . , u c (Line 19). Note
DT
NP-C 1
NPB 2
NPB-BAR 2
CD
NPB-BAR 2
JJ
the five NNS
VBP
tests have been previous
S 2
VP 1
VBN
VP-C 1
VBN
limited
VP-C 1
IN
DT
S-BAR 1
PP 1
NP-C-BAR 1
NP 1
NPB 2
NPB-BAR 2
to the target missile and one other body .
NP-C 1
NN NN CC CD
NP 1
NPB 2
NPB-BAR 2
&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533; &#65533;&#65533;&#65533;&#65533;
&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;
&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533; &#65533;
&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533; &#65533;
JJ NN .
that Algorithm 1 assumes a binary tree 2 , but is not necessary. In the general case, cube pruning will operate on a d-dimensional hypercube, where d is the branching factor of node v.
We cannot enumerate and score every possibility; without the cube pruning approximation, we will have k c possible combinations at each node, exploding the search space exponentially. Figure 3 depicts how we select the top-k alignments at a node v from its children &#12296; u 1 , u 2 &#12297;.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Initial alignments We can construct a word alignment hierarchically, bottom-up, by making use of the structure inherent in syntactic parse trees.</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We can think of building a word alignment as filling in an M&#215;N matrix (Figure 1), and we begin by visiting each preterminal node in the tree.</text>
                  <doc_id>84</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Each of these nodes spans a single e word.</text>
                  <doc_id>85</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>(Line 2 in Algorithm 1).</text>
                  <doc_id>86</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>From here we can assign links from each e word to zero or more f words (Lines 6&#8211;14).</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>At this level of the tree the span size is 1, and the partial alignment we have made spans a single column of the matrix.</text>
                  <doc_id>88</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We can make many such partial alignments depending on the links selected.</text>
                  <doc_id>89</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Lines 5 through 9 of Algorithm 1 enumerate either the null alignment, single-link alignments, or two-link alignments.</text>
                  <doc_id>90</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Each partial alignment is scored and stored in a sorted heap (Lines 9 and 13).</text>
                  <doc_id>91</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In practice enumerating all two-link alignments can be prohibitive for long sentence pairs; we set a practical limit and score only pairwise combina-</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Sentence 1</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>TOP 1</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Algorithm 1: Hypergraph Alignment Input:</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Source sentence e n 1 Target sentence f m 1 Parse tree T over e n 1 Set of feature functions h Weight vector w Beam size k</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Output: A k-best list of alignments over e n 1 and f m 1</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 function ALIGN(e n 1 , f m 1 , T)</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 for v &#8712; T in bottom-up order do</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3 &#945; v &#8592; &#8709;</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4 if IS-PRETERMINALNODE(v) then</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5 i &#8592; index-of(v)</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>6 for j = 0 to m do</text>
                  <doc_id>103</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>7 links &#8592; (i, j)</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>8 score &#8592; w &#183; h(links, v, e n 1 , f m 1 )</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>9 PUSH(&#945; v , &#12296;score, links&#12297;, k )</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>10 for k = j + 1 to m do</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>11 links &#8592; (i, j), (i, k)</text>
                  <doc_id>108</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>12 score &#8592; w &#183; h(links, v, e n 1 , f m 1 )</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>13 PUSH(&#945; v , &#12296;score, links&#12297;, k )</text>
                  <doc_id>110</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>14 end</text>
                  <doc_id>111</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>15 end</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>16 else</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>17 &#945; v &#8592; GROWSPAN(children(v), k)</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>18 end</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>19 end</text>
                  <doc_id>116</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>20 end</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>21 function GROWSPAN(&#12296;u 1 , u 2 &#12297;, k)</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>22 return CUBEPRUNING(&#12296;&#945; u1 , &#945; u2 &#12297;, k,w,h)</text>
                  <doc_id>119</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>23 end</text>
                  <doc_id>120</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>tions of the top n = max { | f |</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 , 10} scoring singlelink alignments.</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We limit the number of total partial alignments &#945; v kept at each node to k.</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If at any time we wish to push onto the heap a new partial alignment when the heap is full, we pop the current worst off the heap and replace it with our new partial alignment if its score is better than the current worst.</text>
                  <doc_id>124</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Building the hypergraph We now visit internal nodes (Line 16) in the tree in bottom-up order.</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>At each nonterminal node v we wish to combine the partial alignments of its children u 1 , .</text>
                  <doc_id>126</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>127</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>128</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>, u c .</text>
                  <doc_id>129</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We use cube pruning (Chiang, 2007; Huang and Chiang, 2007) to select the k-best combinations of the partial alignments of u 1 , .</text>
                  <doc_id>130</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>131</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>132</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>, u c (Line 19).</text>
                  <doc_id>133</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>Note</text>
                  <doc_id>134</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>DT</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NP-C 1</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NPB 2</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NPB-BAR 2</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>CD</text>
                  <doc_id>139</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NPB-BAR 2</text>
                  <doc_id>140</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>JJ</text>
                  <doc_id>141</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the five NNS</text>
                  <doc_id>142</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VBP</text>
                  <doc_id>143</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>tests have been previous</text>
                  <doc_id>144</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>S 2</text>
                  <doc_id>145</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VP 1</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VBN</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VP-C 1</text>
                  <doc_id>148</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VBN</text>
                  <doc_id>149</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>limited</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VP-C 1</text>
                  <doc_id>151</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>IN</text>
                  <doc_id>152</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>DT</text>
                  <doc_id>153</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>S-BAR 1</text>
                  <doc_id>154</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>PP 1</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NP-C-BAR 1</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NP 1</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NPB 2</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NPB-BAR 2</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>to the target missile and one other body .</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NP-C 1</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NN NN CC CD</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NP 1</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NPB 2</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NPB-BAR 2</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533; &#65533;&#65533;&#65533;&#65533;</text>
                  <doc_id>166</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533; &#65533;</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533; &#65533;</text>
                  <doc_id>169</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>JJ NN .</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>that Algorithm 1 assumes a binary tree 2 , but is not necessary.</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the general case, cube pruning will operate on a d-dimensional hypercube, where d is the branching factor of node v.</text>
                  <doc_id>172</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We cannot enumerate and score every possibility; without the cube pruning approximation, we will have k c possible combinations at each node, exploding the search space exponentially.</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 3 depicts how we select the top-k alignments at a node v from its children &#12296; u 1 , u 2 &#12297;.</text>
                  <doc_id>174</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Discriminative training</title>
        <text>We incorporate all our new features into a linear model and learn weights for each using the online averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs inspired by Chiang et al. (2008). We define:
2 We find empirically that using binarized trees reduces
search errors in cube pruning.
in
. . .
in
...
!"
...
&#947;(y) = l(y i , y) + w &#183; (h(y i ) &#8722; h(y)) (1)
where l(y i ,y) is a loss function describing how bad it is to guess y when the correct answer is y i . In our case, we define l(y i ,y) as 1&#8722;F 1 (y i ,y). We select the oracle alignment according to:
y + = arg min &#947;(y) (2)
y&#8712;CAND(x)
where CAND(x) is a set of hypothesis alignments generated from input x. Instead of the traditional oracle, which is calculated solely with respect to the loss l(y i ,y), we choose the oracle that jointly minimizes the loss and the difference in model score to the true alignment. Note that Equation 2 is equivalent to maximizing the sum of the F- measure and model score of y:
y + = arg max (F 1 (y i , y) + w &#183; h(y)) (3)
y&#8712;CAND(x)
Let &#375; be the 1-best alignment according to our model: &#375; = arg max w &#183; h(y) (4)
y&#8712;CAND(x)
Then, at each iteration our weight update is:
w &#8592; w + &#951;(h(y + ) &#8722; h(&#375;)) (5)
where &#951; is a learning rate parameter. 3 We find that this more conservative update gives rise to a much more stable search. After each iteration, we expect y + to get closer and closer to the true y i .</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We incorporate all our new features into a linear model and learn weights for each using the online averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs inspired by Chiang et al. (2008).</text>
              <doc_id>175</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We define:</text>
              <doc_id>176</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2 We find empirically that using binarized trees reduces</text>
              <doc_id>177</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>search errors in cube pruning.</text>
              <doc_id>178</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>in</text>
              <doc_id>179</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>.</text>
              <doc_id>180</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>181</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>182</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>in</text>
              <doc_id>183</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>...</text>
              <doc_id>184</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>!</text>
              <doc_id>185</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>"</text>
              <doc_id>186</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>...</text>
              <doc_id>187</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#947;(y) = l(y i , y) + w &#183; (h(y i ) &#8722; h(y)) (1)</text>
              <doc_id>188</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where l(y i ,y) is a loss function describing how bad it is to guess y when the correct answer is y i .</text>
              <doc_id>189</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In our case, we define l(y i ,y) as 1&#8722;F 1 (y i ,y).</text>
              <doc_id>190</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We select the oracle alignment according to:</text>
              <doc_id>191</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>y + = arg min &#947;(y) (2)</text>
              <doc_id>192</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>y&#8712;CAND(x)</text>
              <doc_id>193</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where CAND(x) is a set of hypothesis alignments generated from input x.</text>
              <doc_id>194</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Instead of the traditional oracle, which is calculated solely with respect to the loss l(y i ,y), we choose the oracle that jointly minimizes the loss and the difference in model score to the true alignment.</text>
              <doc_id>195</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Note that Equation 2 is equivalent to maximizing the sum of the F- measure and model score of y:</text>
              <doc_id>196</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>y + = arg max (F 1 (y i , y) + w &#183; h(y)) (3)</text>
              <doc_id>197</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>y&#8712;CAND(x)</text>
              <doc_id>198</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Let &#375; be the 1-best alignment according to our model: &#375; = arg max w &#183; h(y) (4)</text>
              <doc_id>199</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>y&#8712;CAND(x)</text>
              <doc_id>200</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Then, at each iteration our weight update is:</text>
              <doc_id>201</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>w &#8592; w + &#951;(h(y + ) &#8722; h(&#375;)) (5)</text>
              <doc_id>202</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where &#951; is a learning rate parameter.</text>
              <doc_id>203</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>3 We find that this more conservative update gives rise to a much more stable search.</text>
              <doc_id>204</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>After each iteration, we expect y + to get closer and closer to the true y i .</text>
              <doc_id>205</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Features</title>
        <text>Our simple, flexible linear model makes it easy to throw in many features, mapping a given complex
3 We set &#951; to 0.05 in our experiments.
!"
alignment structure into a single high-dimensional feature vector. Our hierarchical search framework allows us to compute these features when needed, and affords us extra useful syntactic information.
We use two classes of features: local and nonlocal. Huang (2008) defines a feature h to be local if and only if it can be factored among the local productions in a tree, and non-local otherwise. Analogously for alignments, our class of local features are those that can be factored among the local partial alignments competing to comprise a larger span of the matrix, and non-local otherwise. These features score a set of links and the words connected by them.
Feature development Our features are inspired by analysis of patterns contained among our gold alignment data and automatically generated parse trees. We use both local lexical and nonlocal structural features as described below.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Our simple, flexible linear model makes it easy to throw in many features, mapping a given complex</text>
              <doc_id>206</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3 We set &#951; to 0.05 in our experiments.</text>
              <doc_id>207</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>!</text>
              <doc_id>208</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>"</text>
              <doc_id>209</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>alignment structure into a single high-dimensional feature vector.</text>
              <doc_id>210</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our hierarchical search framework allows us to compute these features when needed, and affords us extra useful syntactic information.</text>
              <doc_id>211</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We use two classes of features: local and nonlocal.</text>
              <doc_id>212</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Huang (2008) defines a feature h to be local if and only if it can be factored among the local productions in a tree, and non-local otherwise.</text>
              <doc_id>213</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Analogously for alignments, our class of local features are those that can be factored among the local partial alignments competing to comprise a larger span of the matrix, and non-local otherwise.</text>
              <doc_id>214</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>These features score a set of links and the words connected by them.</text>
              <doc_id>215</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Feature development Our features are inspired by analysis of patterns contained among our gold alignment data and automatically generated parse trees.</text>
              <doc_id>216</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We use both local lexical and nonlocal structural features as described below.</text>
              <doc_id>217</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Local features</title>
            <text>These features fire on single-column spans.
&#8226; From the output of GIZA++ Model 4, we compute lexical probabilities p(e | f ) and p( f | e), as well as a fertility table &#966;(e). From the fertility table, we fire features &#966; 0 (e), &#966; 1 (e), and &#966; 2+ (e) when a word e is aligned to zero, one, or two or more words, respectively. Lexical probability features p(e | f ) and p( f | e) fire when a word e is aligned to a word f .
&#8226; Based on these features, we include a binary lexical-zero feature that fires if both p(e | f ) and p( f | e) are equal to zero for a given word pair (e, f ). Negative weights essentially penalize alignments with links never seen before in the Model 4 alignment, and positive weights encourage such links. We employ a separate instance of this feature for each English part-of-speech tag: p( f | e, t).
We learn a different feature weight for each. Critically, this feature tells us how much to trust alignments involving nouns, verbs, adjectives, function words, punctuation, etc. from the Model 4 alignments from which our p(e | f ) and p( f | e) tables are built. Table 1 shows a sample of learned weights. Intuitively, alignments involving English partsof-speech more likely to be content words (e.g. NNPS, NNS, NN) are more trustworthy
PP NP VP
f f f
...
...
than those likely to be function words (e.g. TO, RP, EX), where the use of such words is often radically different across languages.
&#8226; We also include a measure of distortion. This feature returns the distance to the diagonal of the matrix for any link in a partial alignment. If there is more than one link, we return the distance of the link farthest from the diagonal.
&#8226; As a lexical backoff, we include a tag probability feature, p(t | f ) that fires for some link (e, f ) if the part-of-speech tag of e is t. The conditional probabilities in this table are computed from our parse trees and the baseline Model 4 alignments.
&#8226; In cases where the lexical probabilities are too strong for the distortion feature to overcome (see Figure 5), we develop the multiple-distortion feature. Although local features do not know the partial alignments at other spans, they do have access to the entire English sentence at every step because our input is constant. If some e exists more than once in e n 1 we fire this feature on all links containing word e, returning again the distance to the diagonal for that link. We learn a strong negative weight for this feature.
&#8226; We find that binary identity and punctuation-mismatch features are important. The binary identity feature fires if e = f , and proves useful for untranslated numbers, symbols, names, and punctuation in the data. Punctuation-mismatch fires on any link that causes nonpunctuation to be aligned to punctuation.
Additionally, we include fine-grained versions of the lexical probability, fertility, and distortion features. These fire for for each link (e, f ) and partof-speech tag. That is, we learn a separate weight for each feature for each part-of-speech tag in our data. Given the tag of e, this affords the model the ability to pay more or less attention to the features described above depending on the tag given to e.
Arabic-English specific features We describe here language specific features we implement to exploit shallow Arabic morphology.
IN
PP
NP
from ...
...
!"
of the sister NP are aligned to words following &#65533;&#65533;&#65533;. English preposition structure commonly matches that of Arabic in our gold data. This family of features captures these observations.
&#8226; We observe the Arabic prefix&#65533;, transliterated w- and generally meaning and, to prepend to most any word in the lexicon, so we define features p &#172;w (e | f ) and p &#172;w ( f | e). If f begins with w-, we strip off the prefix and return the values of p(e | f ) and p( f | e). Otherwise, these features return 0.
&#8226; We also include analogous feature functions for several functional and pronominal prefixes and suffixes. 4</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>These features fire on single-column spans.</text>
                  <doc_id>218</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; From the output of GIZA++ Model 4, we compute lexical probabilities p(e | f ) and p( f | e), as well as a fertility table &#966;(e).</text>
                  <doc_id>219</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>From the fertility table, we fire features &#966; 0 (e), &#966; 1 (e), and &#966; 2+ (e) when a word e is aligned to zero, one, or two or more words, respectively.</text>
                  <doc_id>220</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Lexical probability features p(e | f ) and p( f | e) fire when a word e is aligned to a word f .</text>
                  <doc_id>221</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Based on these features, we include a binary lexical-zero feature that fires if both p(e | f ) and p( f | e) are equal to zero for a given word pair (e, f ).</text>
                  <doc_id>222</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Negative weights essentially penalize alignments with links never seen before in the Model 4 alignment, and positive weights encourage such links.</text>
                  <doc_id>223</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We employ a separate instance of this feature for each English part-of-speech tag: p( f | e, t).</text>
                  <doc_id>224</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We learn a different feature weight for each.</text>
                  <doc_id>225</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Critically, this feature tells us how much to trust alignments involving nouns, verbs, adjectives, function words, punctuation, etc. from the Model 4 alignments from which our p(e | f ) and p( f | e) tables are built.</text>
                  <doc_id>226</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Table 1 shows a sample of learned weights.</text>
                  <doc_id>227</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Intuitively, alignments involving English partsof-speech more likely to be content words (e.g. NNPS, NNS, NN) are more trustworthy</text>
                  <doc_id>228</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>PP NP VP</text>
                  <doc_id>229</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f f f</text>
                  <doc_id>230</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>...</text>
                  <doc_id>231</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>...</text>
                  <doc_id>232</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>than those likely to be function words (e.g. TO, RP, EX), where the use of such words is often radically different across languages.</text>
                  <doc_id>233</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; We also include a measure of distortion.</text>
                  <doc_id>234</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This feature returns the distance to the diagonal of the matrix for any link in a partial alignment.</text>
                  <doc_id>235</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>If there is more than one link, we return the distance of the link farthest from the diagonal.</text>
                  <doc_id>236</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; As a lexical backoff, we include a tag probability feature, p(t | f ) that fires for some link (e, f ) if the part-of-speech tag of e is t.</text>
                  <doc_id>237</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The conditional probabilities in this table are computed from our parse trees and the baseline Model 4 alignments.</text>
                  <doc_id>238</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; In cases where the lexical probabilities are too strong for the distortion feature to overcome (see Figure 5), we develop the multiple-distortion feature.</text>
                  <doc_id>239</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Although local features do not know the partial alignments at other spans, they do have access to the entire English sentence at every step because our input is constant.</text>
                  <doc_id>240</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>If some e exists more than once in e n 1 we fire this feature on all links containing word e, returning again the distance to the diagonal for that link.</text>
                  <doc_id>241</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We learn a strong negative weight for this feature.</text>
                  <doc_id>242</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; We find that binary identity and punctuation-mismatch features are important.</text>
                  <doc_id>243</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The binary identity feature fires if e = f , and proves useful for untranslated numbers, symbols, names, and punctuation in the data.</text>
                  <doc_id>244</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Punctuation-mismatch fires on any link that causes nonpunctuation to be aligned to punctuation.</text>
                  <doc_id>245</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Additionally, we include fine-grained versions of the lexical probability, fertility, and distortion features.</text>
                  <doc_id>246</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These fire for for each link (e, f ) and partof-speech tag.</text>
                  <doc_id>247</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>That is, we learn a separate weight for each feature for each part-of-speech tag in our data.</text>
                  <doc_id>248</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Given the tag of e, this affords the model the ability to pay more or less attention to the features described above depending on the tag given to e.</text>
                  <doc_id>249</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Arabic-English specific features We describe here language specific features we implement to exploit shallow Arabic morphology.</text>
                  <doc_id>250</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>IN</text>
                  <doc_id>251</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>PP</text>
                  <doc_id>252</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NP</text>
                  <doc_id>253</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>from ...</text>
                  <doc_id>254</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>...</text>
                  <doc_id>255</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>!</text>
                  <doc_id>256</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>"</text>
                  <doc_id>257</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>of the sister NP are aligned to words following &#65533;&#65533;&#65533;.</text>
                  <doc_id>258</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>English preposition structure commonly matches that of Arabic in our gold data.</text>
                  <doc_id>259</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This family of features captures these observations.</text>
                  <doc_id>260</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; We observe the Arabic prefix&#65533;, transliterated w- and generally meaning and, to prepend to most any word in the lexicon, so we define features p &#172;w (e | f ) and p &#172;w ( f | e).</text>
                  <doc_id>261</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If f begins with w-, we strip off the prefix and return the values of p(e | f ) and p( f | e).</text>
                  <doc_id>262</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Otherwise, these features return 0.</text>
                  <doc_id>263</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; We also include analogous feature functions for several functional and pronominal prefixes and suffixes.</text>
                  <doc_id>264</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>4</text>
                  <doc_id>265</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Nonlocal features</title>
            <text>These features comprise the combination cost component of a partial alignment score and may fire when concatenating two partial alignments to create a larger span. Because these features can look into any two arbitrary subtrees, they are considered nonlocal features as defined by Huang (2008).
&#8226; Features PP-NP-head, NP-DT-head, and VP-VP-head (Figure 6) all exploit headwords on the parse tree. We observe English prepositions and determiners to often align to the headword of their sister. Likewise, we observe the head of a VP to align to the head of an immediate sister VP.
4 Affixes used by our model are currently: &#65533;&#65533;&#65533;, &#65533;&#65533;, &#65533;&#65533;&#65533;, &#65533;&#65533;&#65533;&#65533;&#65533;,
&#65533; &#65533;
&#65533;, &#65533;&#65533;&#65533;, &#65533;&#65533;&#65533;&#65533;, &#65533;&#65533;&#65533;, &#65533;&#65533;&#65533;&#65533;. Others either we did not experiment
with, or seemed to provide no significant benefit, and are not included.
In Figure 4, when the search arrives at the left-most NPB node, the NP-DT-head feature will fire given this structure and links over the span [the ... tests]. When search arrives at the second NPB node, it will fire given the structure and links over the span [the ... missle], but will not fire at the right-most NPB node.
&#8226; Local lexical preference features compete with the headword features described above. However, we also introduce nonlocal lexicalized features for the most common types of English and foreign prepositions to also compete with these general headword features.
PP features PP-of-prep, PP-from-prep, PPto-prep, PP-on-prep, and PP-in-prep fire at any PP whose left child is a preposition and right child is an NP. The head of the PP is one of the enumerated English prepositions and is aligned to any of the three most common foreign words to which it has also been observed aligned in the gold alignments. The last constraint on this pattern is that all words under the span of the sister NP, if aligned, must align to words following the foreign preposition. Figure 7 illustrates this pattern.
&#8226; Finally, we have a tree-distance feature to avoid making too many many-to-one (from many English words to a single foreign word) links. This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007). For any pair of links (e i , f ) and (e j , f ) in which the e words differ but the f word is the same token in each, return the tree height of first common ancestor of e i and e j .
This feature captures the intuition that it is much worse to align two English words at different ends of the tree to the same foreign word, than it is to align two English words under the same NP to the same foreign word.
To see why a string distance feature that counts only the flat horizontal distance from e i to e j is not the best strategy, consider the following. We wish to align a determiner to the same f word as its sister head noun under the same NP. Now suppose there are several intermediate adjectives separating the determiner and noun. A string distance met-
ric, with no knowledge of the relationship between determiner and noun will levy a much heavier penalty than its tree distance analog.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>These features comprise the combination cost component of a partial alignment score and may fire when concatenating two partial alignments to create a larger span.</text>
                  <doc_id>266</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Because these features can look into any two arbitrary subtrees, they are considered nonlocal features as defined by Huang (2008).</text>
                  <doc_id>267</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Features PP-NP-head, NP-DT-head, and VP-VP-head (Figure 6) all exploit headwords on the parse tree.</text>
                  <doc_id>268</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We observe English prepositions and determiners to often align to the headword of their sister.</text>
                  <doc_id>269</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Likewise, we observe the head of a VP to align to the head of an immediate sister VP.</text>
                  <doc_id>270</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4 Affixes used by our model are currently: &#65533;&#65533;&#65533;, &#65533;&#65533;, &#65533;&#65533;&#65533;, &#65533;&#65533;&#65533;&#65533;&#65533;,</text>
                  <doc_id>271</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533; &#65533;</text>
                  <doc_id>272</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533;, &#65533;&#65533;&#65533;, &#65533;&#65533;&#65533;&#65533;, &#65533;&#65533;&#65533;, &#65533;&#65533;&#65533;&#65533;.</text>
                  <doc_id>273</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Others either we did not experiment</text>
                  <doc_id>274</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>with, or seemed to provide no significant benefit, and are not included.</text>
                  <doc_id>275</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In Figure 4, when the search arrives at the left-most NPB node, the NP-DT-head feature will fire given this structure and links over the span [the ... tests].</text>
                  <doc_id>276</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>When search arrives at the second NPB node, it will fire given the structure and links over the span [the ... missle], but will not fire at the right-most NPB node.</text>
                  <doc_id>277</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Local lexical preference features compete with the headword features described above.</text>
                  <doc_id>278</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, we also introduce nonlocal lexicalized features for the most common types of English and foreign prepositions to also compete with these general headword features.</text>
                  <doc_id>279</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>PP features PP-of-prep, PP-from-prep, PPto-prep, PP-on-prep, and PP-in-prep fire at any PP whose left child is a preposition and right child is an NP.</text>
                  <doc_id>280</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The head of the PP is one of the enumerated English prepositions and is aligned to any of the three most common foreign words to which it has also been observed aligned in the gold alignments.</text>
                  <doc_id>281</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The last constraint on this pattern is that all words under the span of the sister NP, if aligned, must align to words following the foreign preposition.</text>
                  <doc_id>282</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 7 illustrates this pattern.</text>
                  <doc_id>283</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Finally, we have a tree-distance feature to avoid making too many many-to-one (from many English words to a single foreign word) links.</text>
                  <doc_id>284</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007).</text>
                  <doc_id>285</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For any pair of links (e i , f ) and (e j , f ) in which the e words differ but the f word is the same token in each, return the tree height of first common ancestor of e i and e j .</text>
                  <doc_id>286</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>This feature captures the intuition that it is much worse to align two English words at different ends of the tree to the same foreign word, than it is to align two English words under the same NP to the same foreign word.</text>
                  <doc_id>287</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To see why a string distance feature that counts only the flat horizontal distance from e i to e j is not the best strategy, consider the following.</text>
                  <doc_id>288</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We wish to align a determiner to the same f word as its sister head noun under the same NP.</text>
                  <doc_id>289</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Now suppose there are several intermediate adjectives separating the determiner and noun.</text>
                  <doc_id>290</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>A string distance met-</text>
                  <doc_id>291</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>ric, with no knowledge of the relationship between determiner and noun will levy a much heavier penalty than its tree distance analog.</text>
                  <doc_id>292</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Related Work</title>
        <text>Recent work has shown the potential for syntactic information encoded in various ways to support inference of superior word alignments. Very recent work in word alignment has also started to report downstream effects on BLEU score. Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. Haghighi et al. (2009) confirm and extend these results, showing BLEU improvement for a hierarchical phrasebased MT system on a small Chinese corpus. As opposed to ITG, we use a linguistically motivated phrase-structure tree to drive our search and inform our model. And, unlike ITG-style approaches, our model can generate arbitrary alignments and learn from arbitrary gold alignments.
DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. Fossum et al. (2008) start with the output from GIZA++ Model-4 union, and focus on increasing precision by deleting links based on a linear discriminative model exposed to syntactic and lexical information.
Fraser and Marcu (2007) take a semi-supervised approach to word alignment, using a small amount of gold data to further tune parameters of a headword-aware generative model. They show a significant improvement over a Model-4 union baseline on a very large corpus.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Recent work has shown the potential for syntactic information encoded in various ways to support inference of superior word alignments.</text>
              <doc_id>293</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Very recent work in word alignment has also started to report downstream effects on BLEU score.</text>
              <doc_id>294</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment.</text>
              <doc_id>295</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Haghighi et al. (2009) confirm and extend these results, showing BLEU improvement for a hierarchical phrasebased MT system on a small Chinese corpus.</text>
              <doc_id>296</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>As opposed to ITG, we use a linguistically motivated phrase-structure tree to drive our search and inform our model.</text>
              <doc_id>297</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>And, unlike ITG-style approaches, our model can generate arbitrary alignments and learn from arbitrary gold alignments.</text>
              <doc_id>298</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance.</text>
              <doc_id>299</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Fossum et al. (2008) start with the output from GIZA++ Model-4 union, and focus on increasing precision by deleting links based on a linear discriminative model exposed to syntactic and lexical information.</text>
              <doc_id>300</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Fraser and Marcu (2007) take a semi-supervised approach to word alignment, using a small amount of gold data to further tune parameters of a headword-aware generative model.</text>
              <doc_id>301</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>They show a significant improvement over a Model-4 union baseline on a very large corpus.</text>
              <doc_id>302</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>6 Experiments</title>
        <text>We evaluate our model and and resulting alignments on Arabic-English data against those induced by IBM Model-4 using GIZA++ (Och and Ney, 2003) with both the union and grow-diagfinal heuristics. We use 1,000 sentence pairs and gold alignments from LDC2006E86 to train model parameters: 800 sentences for training, 100 for testing, and 100 as a second held-out development set to decide when to stop perceptron training. We also align the test data using GIZA++ 5 along with 50 million words of English.
5 We use a standard training procedure: 5 iterations of
Model-1, 5 iterations of HMM, 3 iterations of Model-3, and 3 iterations of Model-4.
Training F&#8722;measure 0.775
0.77
0.765
0.76
0.755
0.75
0.745
0.74
0.735
F-measure 0.76 0.75 0.74 0.73 0.72 0.71 0.70 0.69 0.68 0.67 Model 1 HMM Model 4
Initial alignments</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We evaluate our model and and resulting alignments on Arabic-English data against those induced by IBM Model-4 using GIZA++ (Och and Ney, 2003) with both the union and grow-diagfinal heuristics.</text>
              <doc_id>303</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We use 1,000 sentence pairs and gold alignments from LDC2006E86 to train model parameters: 800 sentences for training, 100 for testing, and 100 as a second held-out development set to decide when to stop perceptron training.</text>
              <doc_id>304</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We also align the test data using GIZA++ 5 along with 50 million words of English.</text>
              <doc_id>305</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>5 We use a standard training procedure: 5 iterations of</text>
              <doc_id>306</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Model-1, 5 iterations of HMM, 3 iterations of Model-3, and 3 iterations of Model-4.</text>
              <doc_id>307</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Training F&#8722;measure 0.775</text>
              <doc_id>308</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.77</text>
              <doc_id>309</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.765</text>
              <doc_id>310</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.76</text>
              <doc_id>311</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.755</text>
              <doc_id>312</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.75</text>
              <doc_id>313</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.745</text>
              <doc_id>314</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.74</text>
              <doc_id>315</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.735</text>
              <doc_id>316</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>F-measure 0.76 0.75 0.74 0.73 0.72 0.71 0.70 0.69 0.68 0.67 Model 1 HMM Model 4</text>
              <doc_id>317</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Initial alignments</text>
              <doc_id>318</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>6.1 Alignment Quality</title>
            <text>We empirically choose our beam size k from the results of a series of experiments, setting k=1, 2, 4, 8, 16, 32, and 64. We find setting k = 16 to yield the highest accuracy on our held-out test data. Using wider beams results in higher F-measure on training data, but those gains do not translate into higher accuracy on held-out data.
The first three columns of Table 2 show the balanced F-measure, Precision, and Recall of our alignments versus the two GIZA++ Model-4 baselines. We report an F-measure 8.6 points over Model-4 union, and 6.3 points over Model-4 growdiag-final.
Figure 8 shows the stability of the search procedure over ten random restarts of parallel averaged perceptron training with 40 CPUs. Training examples are randomized at each epoch, leading to slight variations in learning curves over time but all converge into the same general neighborhood.
Figure 9 shows the robustness of the model to initial alignments used to derive lexical features p(e | f ) and p( f | e). In addition to IBM Model 4, we experiment with alignments from Model 1 and the HMM model. In each case, we significantly outperform the baseline GIZA++ Model 4 alignments on a heldout test set.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We empirically choose our beam size k from the results of a series of experiments, setting k=1, 2, 4, 8, 16, 32, and 64.</text>
                  <doc_id>319</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We find setting k = 16 to yield the highest accuracy on our held-out test data.</text>
                  <doc_id>320</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Using wider beams results in higher F-measure on training data, but those gains do not translate into higher accuracy on held-out data.</text>
                  <doc_id>321</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The first three columns of Table 2 show the balanced F-measure, Precision, and Recall of our alignments versus the two GIZA++ Model-4 baselines.</text>
                  <doc_id>322</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We report an F-measure 8.6 points over Model-4 union, and 6.3 points over Model-4 growdiag-final.</text>
                  <doc_id>323</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 8 shows the stability of the search procedure over ten random restarts of parallel averaged perceptron training with 40 CPUs.</text>
                  <doc_id>324</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Training examples are randomized at each epoch, leading to slight variations in learning curves over time but all converge into the same general neighborhood.</text>
                  <doc_id>325</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 9 shows the robustness of the model to initial alignments used to derive lexical features p(e | f ) and p( f | e).</text>
                  <doc_id>326</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In addition to IBM Model 4, we experiment with alignments from Model 1 and the HMM model.</text>
                  <doc_id>327</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In each case, we significantly outperform the baseline GIZA++ Model 4 alignments on a heldout test set.</text>
                  <doc_id>328</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>6.2 MT Experiments</title>
            <text>We align a corpus of 50 million words with GIZA++ Model-4, and extract translation rules from a 5.4 million word core subset. We align the same core subset with our trained hypergraph alignment model, and extract a second set of translation rules. For each set of translation rules, we train a machine translation system and decode a held-out test corpus for which we report results below. We use a syntax-based translation system for these experiments. This system transforms Arabic strings into target English syntax trees Translation rules are extracted from (e-tree, f -string, alignment) triples as in (Galley et al., 2004; Galley et al., 2006).
We use a randomized language model (similar to that of Talbot and Brants (2008)) of 472 million English words. We tune the the parameters of the MT system on a held-out development corpus of 1,172 parallel sentences, and test on a heldout parallel corpus of 746 parallel sentences. Both corpora are drawn from the NIST 2004 and 2006 evaluation data, with no overlap at the document or segment level with our training data.
Columns 4 and 5 in Table 2 show the results of our MT experiments. Our hypergraph alignment algorithm allows us a 1.1 BLEU increase over the best baseline system, Model-4 grow-diag-final. This is statistically significant at the p &lt; 0.01 level. We also report a 2.4 BLEU increase over a system trained with alignments from Model-4 union.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We align a corpus of 50 million words with GIZA++ Model-4, and extract translation rules from a 5.4 million word core subset.</text>
                  <doc_id>329</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We align the same core subset with our trained hypergraph alignment model, and extract a second set of translation rules.</text>
                  <doc_id>330</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For each set of translation rules, we train a machine translation system and decode a held-out test corpus for which we report results below.</text>
                  <doc_id>331</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We use a syntax-based translation system for these experiments.</text>
                  <doc_id>332</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This system transforms Arabic strings into target English syntax trees Translation rules are extracted from (e-tree, f -string, alignment) triples as in (Galley et al., 2004; Galley et al., 2006).</text>
                  <doc_id>333</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We use a randomized language model (similar to that of Talbot and Brants (2008)) of 472 million English words.</text>
                  <doc_id>334</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We tune the the parameters of the MT system on a held-out development corpus of 1,172 parallel sentences, and test on a heldout parallel corpus of 746 parallel sentences.</text>
                  <doc_id>335</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Both corpora are drawn from the NIST 2004 and 2006 evaluation data, with no overlap at the document or segment level with our training data.</text>
                  <doc_id>336</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Columns 4 and 5 in Table 2 show the results of our MT experiments.</text>
                  <doc_id>337</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Our hypergraph alignment algorithm allows us a 1.1 BLEU increase over the best baseline system, Model-4 grow-diag-final.</text>
                  <doc_id>338</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This is statistically significant at the p &lt; 0.01 level.</text>
                  <doc_id>339</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We also report a 2.4 BLEU increase over a system trained with alignments from Model-4 union.</text>
                  <doc_id>340</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>7</index>
        <title>7 Conclusion</title>
        <text>We have opened up the word alignment task to advances in hypergraph algorithms currently used in parsing and machine translation decoding. We treat word alignment as a parsing problem, and by taking advantage of English syntax and the hypergraph structure of our search algorithm, we report significant increases in both F-measure and BLEU score over standard baselines in use by most state-of-the-art MT systems today.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have opened up the word alignment task to advances in hypergraph algorithms currently used in parsing and machine translation decoding.</text>
              <doc_id>341</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We treat word alignment as a parsing problem, and by taking advantage of English syntax and the hypergraph structure of our search algorithm, we report significant increases in both F-measure and BLEU score over standard baselines in use by most state-of-the-art MT systems today.</text>
              <doc_id>342</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>8</index>
        <title>Acknowledgements</title>
        <text>We would like to thank our colleagues in the Natural Language Group at ISI for many meaningful discussions and the anonymous reviewers for their thoughtful suggestions. This research was supported by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies, and a</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We would like to thank our colleagues in the Natural Language Group at ISI for many meaningful discussions and the anonymous reviewers for their thoughtful suggestions.</text>
              <doc_id>343</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This research was supported by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies, and a</text>
              <doc_id>344</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>9</index>
        <title>USC CREATE Fellowship to the first author. References</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>345</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: A sampling of learned weights for the lexical zero feature. Negative weights penalize links never seen before in a baseline alignment used to initialize lexical p(e | f ) and p( f | e) tables. Positive weights outright reward such links.</caption>
        <reference_text>None</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>head of it?s sister NP.</cell>
              <cell>Penalty</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>NNPS</cell>
              <cell>?1.11</cell>
            </row>
            <row>
              <cell>NNS</cell>
              <cell>?1.03</cell>
            </row>
            <row>
              <cell>NN</cell>
              <cell>?0.80</cell>
            </row>
            <row>
              <cell>NNP</cell>
              <cell>?0.62</cell>
            </row>
            <row>
              <cell>VB</cell>
              <cell>?0.54</cell>
            </row>
            <row>
              <cell>VBG</cell>
              <cell>?0.52</cell>
            </row>
            <row>
              <cell>JJ</cell>
              <cell>?0.50</cell>
            </row>
            <row>
              <cell>JJS</cell>
              <cell>?0.46</cell>
            </row>
            <row>
              <cell>VBN</cell>
              <cell>?0.45</cell>
            </row>
            <row>
              <cell>...</cell>
              <cell>...</cell>
            </row>
            <row>
              <cell>POS</cell>
              <cell>?0.0093</cell>
            </row>
            <row>
              <cell>EX</cell>
              <cell>?0.0056</cell>
            </row>
            <row>
              <cell>RP</cell>
              <cell>?0.0037</cell>
            </row>
            <row>
              <cell>WP$</cell>
              <cell>?0.0011</cell>
            </row>
            <row>
              <cell>TO</cell>
              <cell>0.037</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Reward</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: F-measure, Precision, Recall, the resulting BLEU score, and number of unknown words on a held-out test corpus for three types of alignments. BLEU scores are case-insensitive IBM BLEU. We show a 1.1 BLEU increase over the strongest baseline, Model-4 grow-diag-final. This is statistically significant at the p &lt; 0.01 level.</caption>
        <reference_text></reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>M4 (union)</cell>
              <cell>.665</cell>
              <cell>.636</cell>
              <cell>.696</cell>
              <cell>45.1</cell>
              <cell>2,538</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>M4 (grow-diag-final)</cell>
              <cell>.688</cell>
              <cell>.702</cell>
              <cell>.674</cell>
              <cell>46.4</cell>
              <cell>2,262</cell>
            </row>
            <row>
              <cell>Hypergraph alignment</cell>
              <cell>.751</cell>
              <cell>.780</cell>
              <cell>.724</cell>
              <cell>47.5</cell>
              <cell>1,610</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Phil Blunsom</author>
          <author>Trevor Cohn</author>
        </authors>
        <title>Discriminative Word Alignment with Conditional Random Fields.</title>
        <publication>In Proceedings of the 44th Annual Meeting of the ACL.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Peter F Brown</author>
          <author>Stephen A Della Pietra</author>
          <author>Vincent Della J Pietra</author>
          <author>Robert L Mercer</author>
        </authors>
        <title>The mathematics of statistical machine translation: Parameter estimation.</title>
        <publication>None</publication>
        <pages>312</pages>
        <date>1993</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Cherry</author>
          <author>Dekang Lin</author>
        </authors>
        <title>Soft Syntactic Constraints for Word Alignment through Discriminative Training.</title>
        <publication>In Proceedings of the 44th Annual Meeting of the ACL.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>Hierarchical phrase-based translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>David Chiang</author>
          <author>Yuval Marton</author>
          <author>Philip Resnik</author>
        </authors>
        <title>Online Large-Margin Training of Syntactic and Structural Translation Features.</title>
        <publication>In Proceedings of EMNLP.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Michael Collins</author>
        </authors>
        <title>Head-Driven Statistical Models for Natural Language Parsing.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Michael Collins</author>
        </authors>
        <title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
        <publication>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>John DeNero</author>
          <author>Dan Klein</author>
        </authors>
        <title>Tailoring Word Alignments to Syntactic Machine Translation.</title>
        <publication>In Proceedings of the 45th Annual Meeting of the ACL.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Alexander Fraser</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Getting the Structure Right for Word Alignment: LEAF.</title>
        <publication>In Proceedings of EMNLP-CoNLL.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Victoria Fossum</author>
          <author>Kevin Knight</author>
          <author>Steven Abney</author>
        </authors>
        <title>Using Syntax to Improve Word Alignment Precision for Syntax-Based Machine Translation.</title>
        <publication>In Proceedings of the Third Workshop on Statistical Machine Translation.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Dan Klein</author>
          <author>Christopher D Manning</author>
        </authors>
        <title>Parsing and Hypergraphs.</title>
        <publication>In Proceedings of the 7th International Workshop on Parsing Technologies.</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Aria Haghighi</author>
          <author>John Blitzer</author>
          <author>Dan Klein</author>
        </authors>
        <title>Better Word Alignments with Supervised ITG Models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>12</id>
        <authors/>
        <title>None</title>
        <publication>In Proceedings of ACL-IJCNLP</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Liang Huang</author>
          <author>David Chiang</author>
        </authors>
        <title>Better k-best Parsing.</title>
        <publication>In Proceedings of the 9th International Workshop on Parsing Technologies.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Liang Huang</author>
          <author>David Chiang</author>
        </authors>
        <title>Forest Rescoring: Faster Decoding with Integrated Language Models.</title>
        <publication>In Proceedings of the 45th Annual Meeting of the ACL.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Liang Huang</author>
        </authors>
        <title>Forest Reranking: Discriminative Parsing with Non-Local Features.</title>
        <publication>In Proceedings of the 46th Annual Meeting of the ACL.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Michel Galley</author>
          <author>Mark Hopkins</author>
          <author>Kevin Knight</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>What&#8217;s in a Translation Rule? In</title>
        <publication>Proceedings of NAACL.</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Michel Galley</author>
          <author>Jonathan Graehl</author>
          <author>Kevin Knight</author>
          <author>Daniel Marcu</author>
          <author>Steve DeNeefe</author>
          <author>Wei Wang</author>
          <author>Ignacio Thayer</author>
        </authors>
        <title>Scalable Inference and Training of Context-Rich Syntactic Models</title>
        <publication>In Proceedings of the 44th Annual Meeting of the ACL.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Abraham Ittycheriah</author>
          <author>Salim Roukos</author>
        </authors>
        <title>A maximum entropy word aligner for Arabic-English machine translation.</title>
        <publication>In Proceedings of HLT-EMNLP.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Simon Lacoste-Julien</author>
          <author>Ben Taskar</author>
          <author>Dan Klein</author>
          <author>Michael I Jordan</author>
        </authors>
        <title>Word alignment via Quadratic Assignment.</title>
        <publication>In Proceedings of HLTEMNLP.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Yang Liu</author>
          <author>Qun Liu</author>
          <author>Shouxun Lin</author>
        </authors>
        <title>Loglinear Models for Word Alignment</title>
        <publication>In Proceedings of the 43rd Annual Meeting of the ACL.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Robert C Moore</author>
        </authors>
        <title>A Discriminative Framework for Word Alignment.</title>
        <publication>In Proceedings of EMNLP.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Robert C Moore</author>
          <author>Wen-tau Yih</author>
          <author>Andreas Bode</author>
        </authors>
        <title>Improved Discriminative Bilingual Word Alignment</title>
        <publication>In Proceedings of the 44th Annual Meeting of the ACL.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>Slav Petrov</author>
          <author>Leon Barrett</author>
          <author>Romain Thibaux</author>
          <author>Dan Klein</author>
        </authors>
        <title>Learning Accurate, Compact, and Interpretable Tree Annotation</title>
        <publication>In Proceedings of the 44th Annual Meeting of the ACL.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>T Ward</author>
          <author>W-J Zhu</author>
        </authors>
        <title>BLEU: A Method for Automatic Evaluation of Machine Translation</title>
        <publication>In Proceedings of the 40th Annual Meeting of the ACL.</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>Ben Taskar</author>
          <author>Simon Lacoste-Julien</author>
          <author>Dan Klein</author>
        </authors>
        <title>A Discriminative Matching Approach to Word Alignment.</title>
        <publication>In Proceedings of HLT-EMNLP.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>27</id>
        <authors>
          <author>David Talbot</author>
          <author>Thorsten Brants</author>
        </authors>
        <title>Randomized Language Models via Perfect Hash Functions.</title>
        <publication>In Proceedings of ACL-08: HLT.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Blunsom and Cohn, 2006</string>
        <sentence_id>413489</sentence_id>
        <char_offset>160</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Brown et al., 1993</string>
        <sentence_id>413473</sentence_id>
        <char_offset>46</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Cherry and Lin (2006)</string>
        <sentence_id>413759</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>3</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>413491</sentence_id>
        <char_offset>175</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>3</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>413491</sentence_id>
        <char_offset>189</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>3</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>413629</sentence_id>
        <char_offset>63</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>3</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>413559</sentence_id>
        <char_offset>21</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>3</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>413559</sentence_id>
        <char_offset>45</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>4</reference_id>
        <string>Chiang et al. (2008)</string>
        <sentence_id>413639</sentence_id>
        <char_offset>206</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>5</reference_id>
        <string>Collins, 2003</string>
        <sentence_id>413606</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>6</reference_id>
        <string>Collins, 2002</string>
        <sentence_id>413509</sentence_id>
        <char_offset>64</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>6</reference_id>
        <string>Collins, 2002</string>
        <sentence_id>413639</sentence_id>
        <char_offset>131</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>7</reference_id>
        <string>DeNero and Klein, 2007</string>
        <sentence_id>413737</sentence_id>
        <char_offset>91</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>7</reference_id>
        <string>DeNero and Klein (2007)</string>
        <sentence_id>413763</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>8</reference_id>
        <string>Fraser and Marcu (2007)</string>
        <sentence_id>413765</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>9</reference_id>
        <string>Fossum et al. (2008)</string>
        <sentence_id>413764</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>10</reference_id>
        <string>Klein and Manning, 2001</string>
        <sentence_id>413491</sentence_id>
        <char_offset>42</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>11</reference_id>
        <string>Haghighi et al. (2009)</string>
        <sentence_id>413760</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>12</reference_id>
        <string>(2009)</string>
        <sentence_id>413760</sentence_id>
        <char_offset>16</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>13</reference_id>
        <string>Huang and Chiang, 2005</string>
        <sentence_id>413491</sentence_id>
        <char_offset>67</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>14</reference_id>
        <string>Huang and Chiang, 2007</string>
        <sentence_id>413491</sentence_id>
        <char_offset>165</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>14</reference_id>
        <string>Huang and Chiang, 2007</string>
        <sentence_id>413559</sentence_id>
        <char_offset>35</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>15</reference_id>
        <string>Huang (2008)</string>
        <sentence_id>413627</sentence_id>
        <char_offset>86</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>15</reference_id>
        <string>Huang (2008)</string>
        <sentence_id>413752</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>15</reference_id>
        <string>Huang (2008)</string>
        <sentence_id>413719</sentence_id>
        <char_offset>117</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>15</reference_id>
        <string>Huang, 2008</string>
        <sentence_id>413491</sentence_id>
        <char_offset>91</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>16</reference_id>
        <string>Galley et al., 2004</string>
        <sentence_id>413781</sentence_id>
        <char_offset>154</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>17</reference_id>
        <string>Galley et al., 2006</string>
        <sentence_id>413781</sentence_id>
        <char_offset>175</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>18</reference_id>
        <string>Ittycheriah and Roukos, 2005</string>
        <sentence_id>413489</sentence_id>
        <char_offset>91</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>20</reference_id>
        <string>Liu et al., 2005</string>
        <sentence_id>413489</sentence_id>
        <char_offset>121</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>21</reference_id>
        <string>Moore (2005)</string>
        <sentence_id>413477</sentence_id>
        <char_offset>18</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>22</reference_id>
        <string>Moore et al., 2006</string>
        <sentence_id>413489</sentence_id>
        <char_offset>214</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>23</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>413789</sentence_id>
        <char_offset>125</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>24</reference_id>
        <string>Petrov et al., 2006</string>
        <sentence_id>413606</sentence_id>
        <char_offset>53</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>26</reference_id>
        <string>Taskar et al., 2005</string>
        <sentence_id>413489</sentence_id>
        <char_offset>139</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>27</reference_id>
        <string>Talbot and Brants (2008)</string>
        <sentence_id>413782</sentence_id>
        <char_offset>55</char_offset>
      </citation>
    </citations>
  </content>
</document>
