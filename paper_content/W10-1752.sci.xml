<PAPER>
  <FILENO/>
  <TITLE>Normalized Compression Distance Based Measures for MetricsMATR 2010</TITLE>
  <AUTHORS>
    <AUTHOR>Marcus Dobrinkat</AUTHOR>
    <AUTHOR>Jaakko V&#228;yrynen</AUTHOR>
    <AUTHOR>Tero Tapiovaara</AUTHOR>
  </AUTHORS>
  <ABSTRACT>
    <A-S ID="S-43518">We present the MT-NCD and MT-mNCD machine translation evaluation metrics as submission to the machine translation evaluation shared task (MetricsMATR 2010).</A-S>
    <A-S ID="S-43519">The metrics are based on normalized compression distance (NCD), a general information theoretic measure of string similarity, and evaluated against human judgments from the WMT08 shared task.</A-S>
    <A-S ID="S-43520">The experiments show that 1) our metric improves correlation to human judgments by using flexible matching, 2) segment replication is effective, and 3) our NCD-inspired method for multiple references indicates improved results.</A-S>
    <A-S ID="S-43521">Generally, the proposed MT-NCD and MT-mNCD methods correlate competitively with human judgments compared to commonly used machine translations evaluation metrics, for instance, BLEU.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-43522">The quality of automatic machine translation (MT) evaluation metrics plays an important role in the development of MT systems.</S>
        <S ID="S-43523">Human evaluation would no longer be necessary if automatic MT metrics correlated perfectly with manual judgments.</S>
        <S ID="S-43524">Besides high correlation with human judgments of translation quality, a good metric should be language independent, fast to compute and sensitive enough to reliably detect small improvements in MT systems.</S>
      </P>
      <P>
        <S ID="S-43525">Recently there have been some experiments with normalized compression distance (NCD) as a method for automatic evaluation of machine translation.</S>
        <S ID="S-43526">NCD is a general string similarity measure that has been useful for clustering in various tasks (<REF ID="R-03" RPTR="5">Cilibrasi and Vitanyi, 2005</REF>).</S>
      </P>
      <P>
        <S ID="S-43527"><REF ID="R-07" RPTR="8">Parker (2008)</REF> introduced BADGER, a machine translation evaluation metric that uses NCD together with a language independent word normalization method.</S>
        <S ID="S-43528"><REF ID="R-05" RPTR="6">Kettunen (2009)</REF> independently applied NCD to the direct evaluation of translations.</S>
        <S ID="S-43529">He showed with a small corpus of three language pairs that the scores of NCD and METEOR (v0.6) from translations of 10&#8211;12 MT systems were highly correlated.</S>
      </P>
      <P>
        <S ID="S-43530">V&#228;yrynen et al. (2010) have extended the work by showing that NCD can be used to rank translations of different MT systems so that the ranking order correlates with human rankings at the same level as BLEU (<REF ID="R-06" RPTR="7">Papineni et al., 2001</REF>).</S>
        <S ID="S-43531">For translations into English, NCD had an overall systemlevel correlation of 0.66 whereas the best method, ULC had an overall correlation of 0.76, and BLEU had an overall correlation of 0.65.</S>
        <S ID="S-43532">NCD presents a viable alternative to the de facto standard BLEU.</S>
        <S ID="S-43533">Both metrics are language independent, simple and efficient to compute.</S>
        <S ID="S-43534">However, NCD is a general measure of similarity that has been applied in many domains.</S>
        <S ID="S-43535">More advanced methods achieve better correlation with human judgments, but typically use additional language specific linguistic resources.</S>
        <S ID="S-43536">Dobrinkat et al. (2010) experimented with relaxed word matching, adding language specific resources to NCD.</S>
        <S ID="S-43537">The metric called mNCD, which works similarly to mBLEU (<REF ID="R-00" RPTR="0">Agarwal and Lavie, 2008</REF>), showed improved correlation to human judgments in English, the only language where a METEOR synonym module was used.</S>
      </P>
      <P>
        <S ID="S-43538">The motivation for this challenge submission is to evaluate the MT-NCD and MT-mNCD metric performance in an open competition with state-of-</S>
      </P>
      <P>
        <S ID="S-43539">the-art MT evaluation metrics.</S>
        <S ID="S-43540">Our experiments and submission build on NCD and mNCD.</S>
        <S ID="S-43541">We expand NCD to handle multiple references and report experimental results for replicating segments as a preprocessing step that improves the NCD as an MT evaluation metric.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 NCD-based MT evaluation metrics</HEADER>
      <P>
        <S ID="S-43589">NCD-based MT evaluation metrics build on the idea that a string x is similar to another string y, when both share common substrings.</S>
        <S ID="S-43590">When describing y, common substrings do not have to be repeated, but can be referenced to x.</S>
        <S ID="S-43591">This is done when compressing the concatenation of x and y, which results in smaller output when more information of y is already included in x.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>2.1 Normalized Compression Distance</HEADER>
        <P>
          <S ID="S-43542">The normalized compression distance, as defined by <REF ID="R-03" RPTR="4">Cilibrasi and Vitanyi (2005)</REF> is given in Equation 1, in which C(x) is the length of the compression of x and C(x, y) is the length of the compression of the concatenation of x and y.</S>
        </P>
        <P>
          <S ID="S-43543">NCD(x, y) = C(x, y) &#8722; min {C(x), C(y)}</S>
        </P>
        <P>
          <S ID="S-43544">max {C(x), C(y)} (1)</S>
        </P>
        <P>
          <S ID="S-43545">NCD computes the distance as a score closer to one for very different strings and closer to zero for more similar strings.</S>
          <S ID="S-43546">Most MT evaluation metrics are defined as similarity measures in contrast to NCD, which is a distance measure.</S>
          <S ID="S-43547">For easier comparison with other MT evaluation metrics, we define the NCD based MT evaluation similarity metric MT-NCD as 1 &#8722; NCD.</S>
        </P>
        <P>
          <S ID="S-43548">NCD is a practically usable form of the uncomputable normalized information distance (NID), a general metric for the similarity of two objects.</S>
          <S ID="S-43549">NID is based on the notion of Kolmogorov complexity K(x), a theoretical measure for the algorithmic information content of a string x.</S>
          <S ID="S-43550">It is defined as the shortest universal Turing machine that prints x and stops (<REF ID="R-08" RPTR="9">Solomonoff, 1964</REF>).</S>
          <S ID="S-43551">NCD approximates NID by the use of a compressor C(x) that presents a computable approximation of the Kolmogorov complexity K(x).</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.2 NCD with multiple references</HEADER>
        <P>
          <S ID="S-43552">Most ideas can be described with in different ways, therefore using only one reference translation for the evaluation of a candidate sentence is not ideal and the exploitation of knowledge in several different reference translations is helpful for automatic MT evaluation.</S>
          <S ID="S-43553">One simple way for handling multiple references is to evaluate against each reference individually and select the maximum score.</S>
          <S ID="S-43554">Although this works, it is clearly not optimal.</S>
          <S ID="S-43555">We developed the NCD m metric, which is inspired by NCD.</S>
          <S ID="S-43556">It considers all references simultaneously and the quality of a translation t against multiple references R = {r 1 , .</S>
          <S ID="S-43557">.</S>
          <S ID="S-43558">.</S>
          <S ID="S-43559">, r m } is assessed as</S>
        </P>
        <P>
          <S ID="S-43560">NCD m (t, R) = max{C(t|R), min C(r|t}</S>
        </P>
        <P>
          <S ID="S-43561">r&#8712;R</S>
        </P>
        <P>
          <S ID="S-43562">max{C(t), min C(r)} (2)</S>
        </P>
        <P>
          <S ID="S-43563">r&#8712;R</S>
        </P>
        <P>
          <S ID="S-43564">where C(x|y) = C(x, y) &#8722; C(y) approximates conditional algorithmic information with the compressor C.</S>
          <S ID="S-43565">The NCD m similarity metric with a single reference (m = 1) is equal to NCD in Equation 1.</S>
          <S ID="S-43566">Again, we define MT-NCD m as 1&#8722;NCD m .</S>
        </P>
        <P>
          <S ID="S-43567">Figure 1 shows how both, the MT-NCD m and the BLEU metric change with a different number of references when the translation is varied from correct to a random sequence of words.</S>
          <S ID="S-43568">The scores are computed with 249 sentences from the LDC2010E28Dev data set using the first reference as the correct translation.</S>
          <S ID="S-43569">A higher score with multiple references against the correct translation indicates that the measure is able to take into account information from multiple references at the same time.</S>
          <S ID="S-43570">The words in the candidate translation are replaced with probability p with a word randomly selected with uniform probability from a lexicon created from all reference translations.</S>
          <S ID="S-43571">This simulates partially correct translations.</S>
          <S ID="S-43572">The words are changed in a simple way without deletions, insertions or word order permutations.</S>
          <S ID="S-43573">The MT-NCD m score increases with more than one reference translation and random changes to the sentence reduce the score roughly proportional to the number of changed words.</S>
          <S ID="S-43574">With BLEU, the score is affected more by a small number of changes.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.3 mNCD</HEADER>
        <P>
          <S ID="S-43575">One enhancement to the basic NCD as automatic evaluation metric is mNCD (Dobrinkat et al., 2010), which provides relaxed word matching based on the flexible matching modules of ME- TEOR (<REF ID="R-00" RPTR="1">Agarwal and Lavie, 2008</REF>).</S>
        </P>
        <P>
          <S ID="S-43576">What mNCD does is that it changes the reference sentence to be more similar to the candi-</S>
        </P>
        <P>
          <S ID="S-43577">MT evaluation metric score</S>
        </P>
        <P>
          <S ID="S-43578">0.0 0.1 0.2 0.3 0.4 0.5 0.6</S>
        </P>
        <P>
          <S ID="S-43579">0.0 0.2 0.4 0.6 0.8 1.0</S>
        </P>
        <P>
          <S ID="S-43580">word change probability (p)</S>
        </P>
        <P>
          <S ID="S-43581">MT&#8722;NCD m with 3 references MT&#8722;NCD m with 2 references MT&#8722;NCD m with 1 reference BLEU with 3 references BLEU with 2 references BLEU with 1 reference</S>
        </P>
        <P>
          <S ID="S-43582">BLEU</S>
        </P>
        <P>
          <S ID="S-43583">MT&#8722;NCD m</S>
        </P>
        <P>
          <S ID="S-43584">date, given that some of the words are synonyms or share the same stem.</S>
          <S ID="S-43585">Subsequent analysis using any n-gram based automatic analysis should result in a larger similarity score in the hope that this reflects more than just the surface similarity between the candidate and the reference.</S>
        </P>
        <P>
          <S ID="S-43586">Given suitable Wordnet resources, mNCD should alleviate the problem of translation variability especially in absence of multiple reference translations.</S>
          <S ID="S-43587">Our submission uses the default METEOR exact stem synonym modules, which provide synonyms only for English.</S>
          <S ID="S-43588">We base our submission metric on the MT-NCD metric and therefore define MT-mNCD as 1 &#8722; mNCD.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 MT Evaluation System Description</HEADER>
      <P>
        <S ID="S-43677"></S>
      </P>
      <DIV DEPTH="1">
        <HEADER>3.1 System Parameters</HEADER>
        <P>
          <S ID="S-43592">The system parameters for the submission metrics include how candidates and references are preprocessed, the choice of compressor for the NCD itself, as well as the granularity of how large segments are evaluated by NCD and how they are combined into a final score.</S>
        </P>
        <P>
          <S ID="S-43593">Partly due to time constraints we decided not to introduce language specific parameters, therefore we chose those parameter values that perform well in overall and are simple to compute.</S>
        </P>
        <P>
          <S ID="S-43594">3.1.1 Preprocessing</S>
        </P>
        <P>
          <S ID="S-43595">Character casing For MT-NCD, we did experiments without preprocessing and with lowercasing candidates and references.</S>
          <S ID="S-43596">On average over all tasks for language pairs into English, lowercasing consistently decreased the RANK correlation scores but increased the CONST correlation scores.</S>
          <S ID="S-43597">No consistent effect could be found for the language pairs from English.</S>
          <S ID="S-43598">In our submission metrics we use no preprocessing.</S>
          <S ID="S-43599">For MT-mNCD the used METEOR matching module lower-cases the adapted words by default.</S>
          <S ID="S-43600">After adapting a synonym in a reference, we tried to keep the casing as it was in the candidate, which we called real-casing.</S>
          <S ID="S-43601">We use no real-casing for our submitted MT-mNCD metric as this did not improve results consistently over all task into English.</S>
        </P>
        <P>
          <S ID="S-43602">Segment Replication Compression algorithms may not work optimally with short strings, which would deteriorate the approximation of Kolmogorov complexity.</S>
          <S ID="S-43603">Our hypothesis was that a replication of a string (&#8221;abc&#8221;) multiple times (3 &#215; &#8221;abc&#8221; = &#8221;abcabcabc&#8221;) could help the compression algorithm to produce a better estimate of the algorithmic information.</S>
          <S ID="S-43604">This was tested in the MT evaluation framework, and correlation between MT-NCD and human judgments improved when the segments were replicated two times.</S>
          <S ID="S-43605">Further replication did not produce improvements.</S>
          <S ID="S-43606">Results for the MT-NCD metric with replications one, two and three times are shown in Table 1.</S>
          <S ID="S-43607">The results are averages over all used languages.</S>
          <S ID="S-43608">With two compared to one replication, the details for each language show that RANK correlation is improved for the target languages English and French, but degrades for German and Spanish.</S>
        </P>
        <P>
          <S ID="S-43609">CONST andYES/NO correlation improve for all</S>
        </P>
        <P>
          <S ID="S-43610">languages except German.</S>
          <S ID="S-43611">We did not use replication in our submissions.</S>
        </P>
        <P>
          <S ID="S-43612">3.1.2 Block size</S>
        </P>
        <P>
          <S ID="S-43613">The block size parameter governs the number of joined segments that are compared with NCD as a single string.</S>
          <S ID="S-43614">On one extreme, with block size one,</S>
        </P>
        <P>
          <S ID="S-43615">RANK</S>
        </P>
        <P>
          <S ID="S-43616">CONST</S>
        </P>
        <P>
          <S ID="S-43617">YES/NO</S>
        </P>
        <P>
          <S ID="S-43618">TOTAL</S>
        </P>
        <P>
          <S ID="S-43619">each segment is evaluated separately and the segment scores are aggregated to a document score.</S>
          <S ID="S-43620">This is similar to how other MT metrics, for example, BLEU, work.</S>
          <S ID="S-43621">The other extreme is to join all segments together, with block size equal to the number of segments, and evaluate it as a single string, which is similar to document comparison.</S>
          <S ID="S-43622">For block aggregation we experimented with arithmetic and geometric mean and obtained very similar results.</S>
          <S ID="S-43623">We selected arithmetic mean for the submission metrics.</S>
        </P>
        <P>
          <S ID="S-43624">Figure 2 shows the block size effect on the correlation between MT-NCD and human judgments for different target languages.</S>
          <S ID="S-43625">Except for Spanish, our experiments indicate that the block size value has little effect.</S>
          <S ID="S-43626">Therefore, and given how other evaluation metrics work, we chose a block size of one for our submission metrics.</S>
          <S ID="S-43627">We noticed inconsistencies with Spanish in other settings as well and will investigate these issues further.</S>
        </P>
        <P>
          <S ID="S-43628">3.1.3 Compressor There are several universal compressors that can be utilized with NCD, for instance, zlib/gzip, bz2 and PPMZ, which represent different approaches to compression.</S>
          <S ID="S-43629">In terms of compression rate, PPMZ is the best of the mentioned methods, but it is considerably slower to compute compared to the other methods.</S>
          <S ID="S-43630">In terms of correlation with human judgments, NCD using bz2 performs slightly worse than using PPMZ.</S>
          <S ID="S-43631">Given much shorter compression times for bz2 with very little correlation performance degradation, our choice for the submission is the more standard bz2 compressor.</S>
        </P>
        <P>
          <S ID="S-43632">3.1.4 Segment Interleaving Computation of NCD between longer texts (e.g. documents) may exceed the internal compressor window size that is present in some compression</S>
        </P>
        <P>
          <S ID="S-43633">system level correlation with human judgements</S>
        </P>
        <P>
          <S ID="S-43634">0.0 0.2 0.4 0.6 0.8 1.0</S>
        </P>
        <P>
          <S ID="S-43635">2 5 10 20 50 100 500 2000 5000</S>
        </P>
        <P>
          <S ID="S-43636">block size in lines</S>
        </P>
        <P>
          <S ID="S-43637">into fr into en into es into de</S>
        </P>
        <P>
          <S ID="S-43638">algorithms (<REF ID="R-02" RPTR="3">Cebrian et al., 2005</REF>).</S>
          <S ID="S-43639">In this case, only a part of the texts to be compared are visible at any time to the compressor and similarities to the text outside the window will be missed.</S>
          <S ID="S-43640">One solution for the MT evaluation task is to use utilize the known parallel segments of candidate and reference translations.</S>
          <S ID="S-43641">The two segment lists can be interleaved so that the corresponding segments are always adjacent and the compression window size is not exceeded for matching segments.</S>
        </P>
        <P>
          <S ID="S-43642">For our submission, we chose a block size of one, therefore every segment is evaluated individually.</S>
          <S ID="S-43643">As a result, segment interleaving does not have any effect.</S>
          <S ID="S-43644">Segment interleaving is affective in the block size evaluation and results shown in Figure 2.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.2 Evaluation Experiments</HEADER>
        <P>
          <S ID="S-43645">We chose parameters and evaluated our metrics using the WMT08 part of the MetricsMATR 2010 development data, which contains human judgments of the 2008 ACL Workshop on Statistical Machine Translation (<REF ID="R-01" RPTR="2">Callison-Burch et al., 2008</REF>) for translations from a total of 30 MT systems between English and five other European languages.</S>
          <S ID="S-43646">There are human evaluations and several automatic evaluations for the translations, divided into several tasks defined by the language pair and the domain of the translated sentences.</S>
          <S ID="S-43647">For each of these tasks, the WMT08 data contains about 2 000</S>
        </P>
        <P>
          <S ID="S-43648">fr</S>
        </P>
        <P>
          <S ID="S-43649">en</S>
        </P>
        <P>
          <S ID="S-43650">es</S>
        </P>
        <P>
          <S ID="S-43651">de</S>
        </P>
        <P>
          <S ID="S-43652">reference sentences (segments) plus their aligned translations for 12 to 17 different translation systems, depending on the language pair.</S>
        </P>
        <P>
          <S ID="S-43653">The human judgments include three categories which contain evaluations for at most one segment at a time, not whole documents.</S>
          <S ID="S-43654">In the RANK category, humans had to rank the output of five MT systems according to quality.</S>
          <S ID="S-43655">The CONST category contains rankings for short phrases (constituents), and the YES/NO category contains binary answers to judge if a short phrase is an acceptable translation or not.</S>
        </P>
        <P>
          <S ID="S-43656">We report RANK, CONST and YES/NO system level correlations to human judgments as results of our metrics for French, Spanish and German both from and to English.</S>
          <S ID="S-43657">The English&#8211;Spanish news task was left out as most metrics had negative correlation with human judgments.</S>
        </P>
        <P>
          <S ID="S-43658">The evaluation methodology used in Callison- Burch et al. (2008) allows us to measure how each MT evaluation metric correlates with human judgments on the system level, in which all translations from each MT system are aggregated into a single score.</S>
          <S ID="S-43659">The system rankings based on the scores are compared to human judgments.</S>
          <S ID="S-43660">Spearman&#8217;s rank correlation coefficient &#961; was calculated between each MT metric and human judgment category using the simplified equation:</S>
        </P>
        <P>
          <S ID="S-43661">&#961; = 1 &#8722; 6 &#8721; i d i n(n 2 &#8722; 1) (3)</S>
        </P>
        <P>
          <S ID="S-43662">where for each system i, d i is the difference between the rank derived from annotators&#8217; input and the rank obtained from the metric.</S>
          <S ID="S-43663">From the annotators&#8217; input, the n MT systems were ranked based on the number of times each system&#8217;s output was selected as the best translation divided by the number of times each system was part of a judgment.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.3 Results</HEADER>
        <P>
          <S ID="S-43664">The results for WMT08 data for our submitted metrics are shown in Table 2 and are sorted by the</S>
        </P>
        <P>
          <S ID="S-43665">RANK category separately for language pairs from</S>
        </P>
        <P>
          <S ID="S-43666">English and into English.</S>
          <S ID="S-43667">For tasks into English, the correlations show that MT-mNCD improves over the MT-NCD metric in all categories.</S>
          <S ID="S-43668">Also the flexible matching seems to work better for NCD-based metrics than for BLEU, where mBLEU only improves the CONST correlation scores.</S>
          <S ID="S-43669">For tasks from English, MT-mNCD shows slightly higher correlation compared to MT-NCD, except for the YES/NO category.</S>
          <S ID="S-43670">The standard BLEU correlation score is best of the shown evaluation metrics.</S>
          <S ID="S-43671">Relaxed matching using mBLEU does not improve BLEU&#8217;s RANK correlation scores here either, but CONST and YES/NO correlation performs better relative to BLEU than MT-mNCD compared to MT-NCD.</S>
        </P>
        <P>
          <S ID="S-43672">INTO EN FROM EN</S>
        </P>
        <P>
          <S ID="S-43673">RANK</S>
        </P>
        <P>
          <S ID="S-43674">CONST</S>
        </P>
        <P>
          <S ID="S-43675">YES/NO</S>
        </P>
        <P>
          <S ID="S-43676">TOTAL</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 Conclusions</HEADER>
      <P>
        <S ID="S-43678">In our submissions, we applied MT-NCD and MT-mNCD metrics and extended the NCD MT evaluation metric to handle multiple references.</S>
        <S ID="S-43679">The reported experiment indicate a possible improvement for the multiple references.</S>
        <S ID="S-43680">We showed that a replication of segments as a preprocessing step improves the correlation to human judgments.</S>
        <S ID="S-43681">The string replication might alleviate problems in the compressor for short strings and thus could provide better estimates of the algorithmic information.</S>
        <S ID="S-43682">The results of our experiments show that relaxed matching in MT-mNCD works well with proper synonym dictionaries, but is less effective for tasks from English, which only use stemming.</S>
        <S ID="S-43683">MT-mNCD and MT-NCD are reasonably simple to compute and utilize standard and widely used resources, such as the bz2 compression algorithm and WordNet.</S>
        <S ID="S-43684">The metrics perform comparable to the de facto standard BLEU.</S>
        <S ID="S-43685">Improvements with language dependent resources, in particular relaxed matching using synonym dictionaries proved to be useful.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS/>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>Abhaya Agarwal</RAUTHOR>
      <REFTITLE>METEOR, M-BLEU and M-TER: evaluation metrics for highcorrelation with human rankings of machine translation output.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>Chris Callison-Burch</RAUTHOR>
      <REFTITLE>Further meta-evaluation of machine translation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>Manuel Cebrian</RAUTHOR>
      <REFTITLE>Common pitfalls using the normalized compression distance: What to watch out for in a compressor.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>Rudi Cilibrasi</RAUTHOR>
      <REFTITLE>Clustering by compression.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>V&#228;yrynen</RAUTHOR>
      <REFTITLE>Evaluating machine translations using mNCD.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>Kimmo Kettunen</RAUTHOR>
      <REFTITLE>Packing it all up in search for a language independent MT quality measure tool.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>K Papineni</RAUTHOR>
      <REFTITLE>BLEU: a method for automatic evaluation of machine translation.</REFTITLE>
      <DATE>2001</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>Steven Parker</RAUTHOR>
      <REFTITLE>BADGER: A new machine translation metric.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>Ray Solomonoff</RAUTHOR>
      <REFTITLE>Formal theory of inductive inference.</REFTITLE>
      <DATE>1964</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
