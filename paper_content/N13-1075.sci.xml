<PAPER>
  <FILENO/>
  <TITLE>Translation Acquisition Using Synonym Sets</TITLE>
  <AUTHORS>
    <AUTHOR>Daniel Andrade Masaaki Tsuchida Takashi Onishi Kai Ishikawa Knowledge Discovery Research Laboratories</AUTHOR>
    <AUTHOR>NEC Corporation</AUTHOR>
  </AUTHORS>
  <ABSTRACT>
    <A-S ID="S-25439">We propose a new method for translation acquisition which uses a set of synonyms to acquire translations from comparable corpora.</A-S>
    <A-S ID="S-25440">The motivation is that, given a certain query term, it is often possible for a user to specify one or more synonyms.</A-S>
    <A-S ID="S-25441">Using the resulting set of query terms has the advantage that we can overcome the problem that a single query term&#8217;s context vector does not always reliably represent a terms meaning due to the context vector&#8217;s sparsity.</A-S>
    <A-S ID="S-25442">Our proposed method uses a weighted average of the synonyms&#8217; context vectors, that is derived by inferring the mean vector of the von Mises-Fisher distribution.</A-S>
    <A-S ID="S-25443">We evaluate our method, using the synsets from the cross-lingually aligned Japanese and English WordNet.</A-S>
    <A-S ID="S-25444">The experiments show that our proposed method significantly improves translation accuracy when compared to a previous method for smoothing context vectors.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-25445">Automatic translation acquisition is an important task for various applications.</S>
        <S ID="S-25446">For example, finding term translations can be used to automatically update existing bilingual dictionaries, which are an indispensable resource for tasks such as cross-lingual information retrieval and text mining.</S>
      </P>
      <P>
        <S ID="S-25447">Various previous research like (<REF ID="R-15" RPTR="20">Rapp, 1999</REF>; <REF ID="R-05" RPTR="3">Fung, 1998</REF>) has shown that it is possible to acquire word translations from comparable corpora.</S>
        <S ID="S-25448">We suggest here an extension of this approach which uses several query terms instead of a single query term.</S>
        <S ID="S-25449">A user who searches a translation for a query term that is not listed in an existing bilingual dictionary, might first try to find a synonym of that term.</S>
        <S ID="S-25450">For example, the user might look up a synonym in a thesaurus 1 or might use methods for automatic synonym acquisition like described in (<REF ID="R-06" RPTR="5">Grefenstette, 1994</REF>).</S>
        <S ID="S-25451">If the synonym is listed in the bilingual dictionary, we can consider the synonym&#8217;s translations as the translations of the query term.</S>
        <S ID="S-25452">Otherwise, if the synonym is not listed in the dictionary either, we use the synonym together with the original query term to find a translation.</S>
      </P>
      <P>
        <S ID="S-25453">We claim that using a set of synonymous query terms to find a translation is better than using a single query term.</S>
        <S ID="S-25454">The reason is that a single query term&#8217;s context vector is, in general, unreliable due to sparsity.</S>
        <S ID="S-25455">For example, a low frequent query term tends to have many zero entries in its context vector.</S>
        <S ID="S-25456">To mitigate this problem it has been proposed to smooth a query&#8217;s context vector by its nearest neighbors (<REF ID="R-14" RPTR="16">Pekar et al., 2006</REF>).</S>
        <S ID="S-25457">However, nearest neighbors, which context vectors are close the query&#8217;s context vector, can have different meanings and therefore might introduce noise.</S>
      </P>
      <P>
        <S ID="S-25458">The contributions of this paper are two-fold.</S>
        <S ID="S-25459">First, we confirm experimentally that smoothing a query&#8217;s context vector with its synonyms leads in deed to higher translation accuracy, compared to smoothing with nearest neighbors.</S>
        <S ID="S-25460">Second, we propose a simple method to combine a set of context vectors that performs in this setting better than a method previously proposed by (<REF ID="R-14" RPTR="17">Pekar et al., 2006</REF>).</S>
      </P>
      <P>
        <S ID="S-25461">Our approach to combine a set of context vec-</S>
      </P>
      <P>
        <S ID="S-25462">1 Monolingual thesauri are, arguably, easier to construct than</S>
      </P>
      <P>
        <S ID="S-25463">bilingual dictionaries.</S>
      </P>
      <P>
        <S ID="S-25464">tors is derived by learning the mean vector of a von Mises-Fisher distribution.</S>
        <S ID="S-25465">The combined context vector is a weighted-average of the original contextvectors, where the weights are determined by the word occurrence frequencies.</S>
      </P>
      <P>
        <S ID="S-25466">In the following section we briefly show the relation to other previous work.</S>
        <S ID="S-25467">In Section 3, we explain our method in detail, followed by an empirical evaluation in Section 4.</S>
        <S ID="S-25468">We summarize our results in Section 6.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Related Work</HEADER>
      <P>
        <S ID="S-25469">There are several previous works on extracting translations from comparable corpora ranging from (<REF ID="R-15" RPTR="21">Rapp, 1999</REF>; <REF ID="R-05" RPTR="4">Fung, 1998</REF>), and more recently (<REF ID="R-07" RPTR="6">Haghighi et al., 2008</REF>; <REF ID="R-12" RPTR="13">Laroche and Langlais, 2010</REF>), among others.</S>
        <S ID="S-25470">Essentially, all these methods calculate the similarity of a query term&#8217;s context vector with each translation candidate&#8217;s context vector.</S>
        <S ID="S-25471">The context vectors are extracted from the comparable corpora, and mapped to a common vector space with the help of an existing bilingual dictionary.</S>
        <S ID="S-25472">The work in (<REF ID="R-02" RPTR="1">D&#233;jean et al., 2002</REF>) uses crosslingually aligned classes in a multilingual thesaurus to improve the translation accuracy.</S>
        <S ID="S-25473">Their method uses the probability that the query term and a translation candidate are assigned to the same class.</S>
        <S ID="S-25474">In contrast, our method does not need cross-lingually aligned classes.</S>
      </P>
      <P>
        <S ID="S-25475"><REF ID="R-08" RPTR="8">Ismail and Manandhar (2010)</REF> proposes a method that tries to improve a query&#8217;s context vector by using in-domain terms.</S>
        <S ID="S-25476">In-domain terms are the terms that are highly associated to the query, as well as highly associated to one of the query&#8217;s highly associated terms.</S>
        <S ID="S-25477">Their method makes it necessary that the query term has enough highly associated context terms.</S>
        <S ID="S-25478">2 However, a low-frequent query term might not have enough highly associated terms.</S>
      </P>
      <P>
        <S ID="S-25479">In general if a query term has a low-frequency in the corpus, then its context vector is sparse.</S>
        <S ID="S-25480">In that case, the chance of finding a correct translation is reduced (<REF ID="R-14" RPTR="18">Pekar et al., 2006</REF>).</S>
        <S ID="S-25481">Therefore, Pekar et al. (2006) suggest to use distance-based averaging to smooth the context vector of a low-frequent query</S>
      </P>
      <P>
        <S ID="S-25482">2 In their experiments, they require that a query word has at</S>
      </P>
      <P>
        <S ID="S-25483">least 100 associated terms.</S>
      </P>
      <P>
        <S ID="S-25484">term.</S>
        <S ID="S-25485">Their smoothing strategy is dependent on the occurrence frequency of a query term and its close neighbors.</S>
        <S ID="S-25486">Let us denote q the context vector of the query word, and K be the set of its close neighbors.</S>
        <S ID="S-25487">The smoothed context vector q &#8242; is then derived by using:</S>
      </P>
      <P>
        <S ID="S-25488">q &#8242; = &#947; &#183; q + (1 &#8722; &#947;) &#183; &#8721; w x &#183; x , (1)</S>
      </P>
      <P>
        <S ID="S-25489">x&#8712;K</S>
      </P>
      <P>
        <S ID="S-25490">where w x is the weight of neighbor x, and all weights sum to one.</S>
        <S ID="S-25491">The context vectors q and x are interpreted as probability vectors and therefore L1-normalized.</S>
        <S ID="S-25492">The weight w x is a function of the distance between neighbor x and query q.</S>
        <S ID="S-25493">The parameter &#947; determines the degree of smoothing, and is a function of the frequency of the query term and its neighbors:</S>
      </P>
      <P>
        <S ID="S-25494">&#947; = log f(q) log max x&#8712;K&#8746;{q} f(x) (2)</S>
      </P>
      <P>
        <S ID="S-25495">where f(x) is the frequency of term x.</S>
        <S ID="S-25496">Their method forms the baseline for our proposed method.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Proposed Method</HEADER>
      <P>
        <S ID="S-25497">Our goal is to combine the context vectors to one context vector which is less sparse and more reliable than the original context vector of query word q.</S>
        <S ID="S-25498">We assume that for each occurrence of a word, its corresponding context vector was generated by a probabilistic model.</S>
        <S ID="S-25499">Furthermore, we assume that synonyms are generated by the same probability distribution.</S>
        <S ID="S-25500">Finally we use the mean vector of that distribution to represent the combined context vector.</S>
        <S ID="S-25501">By using the assumption that each occurrence of a word corresponds to one sample of the probability distribution, our model places more weight on synonyms that are highly-frequent than synonyms that occur infrequently.</S>
        <S ID="S-25502">This is motivated by the assumption that context vectors of synonyms that occur with high frequency in the corpus, are more reliable than the ones of low-frequency synonyms.</S>
      </P>
      <P>
        <S ID="S-25503">When comparing context vectors, work like <REF ID="R-12" RPTR="12">Laroche and Langlais (2010)</REF> observed that often the cosine similarity performs superior to other distance-measures, like, for example, the euclidean distance.</S>
        <S ID="S-25504">This suggests that context vectors tend to lie in the spherical vector space,</S>
      </P>
      <P>
        <S ID="S-25505">and therefore the von Mises-Fisher distribution is a natural choice for our probabilistic model.</S>
        <S ID="S-25506">The von Mises-Fisher distribution was also successfully used in the work of (<REF ID="R-00" RPTR="0">Basu et al., 2004</REF>) to cluster text data.</S>
        <S ID="S-25507">The von Mises-Fisher distribution with location parameter &#181;, and concentration parameter &#954; is defined as:</S>
      </P>
      <P>
        <S ID="S-25508">p(x|&#181;, &#954;) = c(&#954;) &#183; e &#954;&#183;x&#183;&#181;T ,</S>
      </P>
      <P>
        <S ID="S-25509">where c(&#954;) is a normalization constant, and ||x|| = ||&#181;|| = 1, and &#954; &#8805; 0.</S>
        <S ID="S-25510">|| denotes here the L2-norm.</S>
        <S ID="S-25511">The cosine-similarity measures the angle between two vectors, and the von Mises distribution defines a probability distribution over the possible angles.</S>
        <S ID="S-25512">The parameter &#181; of the von Mises distribution is estimated as follows (<REF ID="R-09" RPTR="9">Jammalamadaka and Sengupta, 2001</REF>): Given the words x 1 , ..., x n , we denote the corresponding context vectors as x 1 , ..., x n , and assume that each context vector is L2-normalized.</S>
        <S ID="S-25513">Then, the mean vector &#181; is calculated as:</S>
      </P>
      <P>
        <S ID="S-25514">&#181; = 1 Z n&#8721;</S>
      </P>
      <P>
        <S ID="S-25515">i=1</S>
      </P>
      <P>
        <S ID="S-25516">where Z ensures that the resulting context vector is L2-normalized, i.e.</S>
        <S ID="S-25517">Z is || &#8721; n</S>
      </P>
      <P>
        <S ID="S-25518">i=1 x i n</S>
      </P>
      <P>
        <S ID="S-25519">||.</S>
        <S ID="S-25520">For our purpose, &#954; is irrelevant and is assumed to be any fixed positive constant.</S>
      </P>
      <P>
        <S ID="S-25521">Since we assume that each occurrence of a word x in the corpus corresponds to one observation of the corresponding word&#8217;s context vector x, we get the following formula:</S>
      </P>
      <P>
        <S ID="S-25522">&#181; = 1 Z &#8242; &#183; n&#8721;</S>
      </P>
      <P>
        <S ID="S-25523">i=1</S>
      </P>
      <P>
        <S ID="S-25524">where Z &#8242; is now || &#8721; n</S>
      </P>
      <P>
        <S ID="S-25525">i=1</S>
      </P>
      <P>
        <S ID="S-25526">x i n</S>
      </P>
      <P>
        <S ID="S-25527">f(x i ) &#8721; n</S>
      </P>
      <P>
        <S ID="S-25528">j=1 f(x j) &#183; x i</S>
      </P>
      <P>
        <S ID="S-25529">f(x i ) &#8721; n</S>
      </P>
      <P>
        <S ID="S-25530">j=1 f(x j) &#183; x i||.</S>
        <S ID="S-25531">We then</S>
      </P>
      <P>
        <S ID="S-25532">use the vector &#181; as the combined vector of the words&#8217; context vectors x i .</S>
        <S ID="S-25533">Our proposed procedure to combine the context vector of query word q and its synonyms can be summarized as follows:</S>
      </P>
      <P>
        <S ID="S-25534">1.</S>
        <S ID="S-25535">Denote the context vectors of q and its synonyms as x 1 , ..., x n , and L2-normalize each context vector.</S>
      </P>
      <P>
        <S ID="S-25536">2.</S>
        <S ID="S-25537">Calculate the weighted average of the vectors x 1 , ..., x n , whereas the weights correspond to the frequencies of each word x i .</S>
      </P>
      <P>
        <S ID="S-25538">3.</S>
        <S ID="S-25539">L2-normalize the weighted average.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 Experiments</HEADER>
      <P>
        <S ID="S-25540">As source and target language corpora we use a corpus extracted from a collection of complaints concerning automobiles compiled by the Japanese Ministry of Land, Infrastructure, Transport and Tourism (MLIT) 3 and the USA National Highway Traffic Safety Administration (NHTSA) 4 , respectively.</S>
        <S ID="S-25541">The Japanese corpus contains 24090 sentences that were POS tagged using MeCab (<REF ID="R-11" RPTR="11">Kudo et al., 2004</REF>).</S>
        <S ID="S-25542">The English corpus contains 47613 sentences, that were POS tagged using Stepp Tagger (Tsuruoka et al., 2005), and use the Lemmatizer (<REF ID="R-13" RPTR="15">Okazaki et al., 2008</REF>) to extract and stem content words (nouns, verbs, adjectives, adverbs).</S>
      </P>
      <P>
        <S ID="S-25543">For creating the context vectors, we calculate the association between two content words occurring in the same sentence, using the log-odds-ratio (<REF ID="R-03" RPTR="2">Evert, 2004</REF>).</S>
        <S ID="S-25544">It was shown in (<REF ID="R-12" RPTR="14">Laroche and Langlais, 2010</REF>) that the log-odds-ratio in combination with the cosine-similarity performs superior to several other methods like PMI 5 and LLR 6 .</S>
        <S ID="S-25545">For comparing two context vectors we use the cosine similarity.</S>
        <S ID="S-25546">To transform the Japanese and English context vectors into the same vector space, we use a bilingual dictionary with around 1.6 million entries.</S>
        <S ID="S-25547">7 To express all context vectors in the same vector space, we map the context vectors in English to context vectors in Japanese.</S>
        <S ID="S-25548">8 First, for all the words which are listed in the bilingual dictionary we calculate word translation probabilities.</S>
        <S ID="S-25549">These translation probabilities are calculated using the EM-algorithm described in (<REF ID="R-10" RPTR="10">Koehn and Knight, 2000</REF>).</S>
        <S ID="S-25550">We then create a translation matrix T which contains in each</S>
      </P>
      <P>
        <S ID="S-25551">3 http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html 4 http://www-odi.nhtsa.dot.gov/downloads/index.cfm 5 point-wise mutual information 6 log-likelihood ratio 7 The bilingual dictionary was developed in the course of our</S>
      </P>
      <P>
        <S ID="S-25552">Japanese language processing efforts described in (<REF ID="R-16" RPTR="22">Sato et al., 2003</REF>).</S>
        <S ID="S-25553">8 Alternatively, we could, for example, use canonical correlation analysis to match the vectors to a common latent vector space, like described in (<REF ID="R-07" RPTR="7">Haghighi et al., 2008</REF>).</S>
      </P>
      <P>
        <S ID="S-25554">column the translation probabilities for a word in English into any word in Japanese.</S>
        <S ID="S-25555">Each context vector in English is then mapped into Japanese using the linear transformation described by the translation matrix T .</S>
        <S ID="S-25556">For word x with context vector x in English, let x &#8242; be its context vector after transformation into Japanese, i.e. x &#8242; = T &#183; x.</S>
      </P>
      <P>
        <S ID="S-25557">The gold-standard was created by considering all nouns in the Japanese and English WordNet where synsets are aligned cross-lingually.</S>
        <S ID="S-25558">This way we were able to create a gold-standard with 215 Japanese nouns, and their respective English translations that occur in our comparable corpora.</S>
        <S ID="S-25559">9 Note that the cross-lingual alignment is needed only for evaluation.</S>
        <S ID="S-25560">For evaluation, we consider only the translations that occur in the corresponding English synset as correct.</S>
      </P>
      <P>
        <S ID="S-25561">Because all methods return a ranked list of translation candidates, the accuracy is measured using the rank of the translation listed in the gold-standard.</S>
        <S ID="S-25562">The inverse rank is the sum of the inverse ranks of each translation in the gold-standard.</S>
      </P>
      <P>
        <S ID="S-25563">In Table 1, the first row shows the results when using no smoothing.</S>
        <S ID="S-25564">Next, we smooth the query&#8217;s context vector by using Equation (1) and (2).</S>
        <S ID="S-25565">The set of neighbors K is defined as the k-terms in the source language that are closest to the query word, with respect to the cosine similarity (sim).</S>
        <S ID="S-25566">The weight w x for a neighbor x is set to w x = 10 0.13&#183;sim(x,q) in accordance to (<REF ID="R-14" RPTR="19">Pekar et al., 2006</REF>).</S>
        <S ID="S-25567">For k we tried values between 1 and 100, and got the best inverse rank when using k=19.</S>
        <S ID="S-25568">The resulting method (Topk Smoothing) performs consistently better than the method using no smoothing, see Table 1, second row.</S>
        <S ID="S-25569">Next, instead of smoothing the query word with its nearest neighbors, we use as the set K the set of synonyms of the query word (Syn Smoothing).</S>
        <S ID="S-25570">Table 1 shows a clear improvement over the method that uses nearest neighbor-smoothing.</S>
        <S ID="S-25571">This confirms our claim that using synonyms for smoothing can lead to better translation accuracy than using nearest neighbors.</S>
        <S ID="S-25572">In the last row of Table 1, we compare our proposed method to combine context vectors of synonyms (Syn Mises-Combination), with the pre-</S>
      </P>
      <P>
        <S ID="S-25573">9 The resulting synsets in Japanese and English, contain in</S>
      </P>
      <P>
        <S ID="S-25574">average 2.2 and 2.8 words, respectively.</S>
        <S ID="S-25575">The ambiguity of a query term in our gold-standard is low, since, in average, a query term belongs to only 1.2 different synsets.</S>
      </P>
      <P>
        <S ID="S-25576">vious method (Syn Smoothing).</S>
        <S ID="S-25577">A pair-wise comparison of our proposed method with Syn Smoothing shows a statistically significant improvement (p &lt; 0.01).</S>
        <S ID="S-25578">10</S>
      </P>
      <P>
        <S ID="S-25579">Finally, we also show the result when simply adding each synonym vector to the query&#8217;s context vector to form a new combined context vector (Syn Sum).</S>
        <S ID="S-25580">11 Even though, this approach does not use the frequency information of a word, it performs better than Syn Smoothing.</S>
        <S ID="S-25581">We suppose that this is due to the fact that it actually indirectly uses frequency information, since the log-odds-ratio tends to be higher for words which occur with high frequency in the corpus.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Discussion</HEADER>
      <P>
        <S ID="S-25582">We first discuss an example where the query terms are &#65533;&#65533;&#65533;&#65533; (cruise) and &#65533;&#65533; (cruise).</S>
        <S ID="S-25583">Both words can have the same meaning.</S>
        <S ID="S-25584">The resulting translation candidates suggested by the baseline methods and the proposed method is shown in Table 2.</S>
        <S ID="S-25585">Using no smoothing, the baseline method outputs the correct translation for &#65533;&#65533;&#65533;&#65533; (cruise) and &#65533; &#65533; (cruise) at rank 10 and 15, respectively.</S>
        <S ID="S-25586">When combining both queries to form one context vector our proposed method (Syn Mises-Combination) retrieves the correct translation at rank 2.</S>
        <S ID="S-25587">Note that we considered all nouns that occur three or more times as possible translation candidates.</S>
        <S ID="S-25588">As can be seen in Table 2, this also includes spelling mistakes like &#8221;sevice&#8221; and &#8221;infromation&#8221;.</S>
      </P>
      <P>
        <S ID="S-25589">10 We use the sign-test (Wilcox, 2009) to test the hypothesis</S>
      </P>
      <P>
        <S ID="S-25590">that the proposed method ranks higher than the baseline.</S>
        <S ID="S-25591">11 No normalization is performed before adding the context</S>
      </P>
      <P>
        <S ID="S-25592">vectors.</S>
      </P>
      <P>
        <S ID="S-25593">Finally, we note that some terms in our test set are ambiguous, and the ambiguity is not resolved by using the synonyms of only one synset.</S>
        <S ID="S-25594">For example, the term &#65533;&#65533; (steering, guidance) belongs to the synset &#8221;steering, guidance&#8221; which includes the terms &#65533;&#65533;&#65533; (steering, guidance) and &#65533;&#65533;&#65533; (guidance), &#65533;&#65533; (guidance).</S>
        <S ID="S-25595">Despite this conflation of senses in one synset, our proposed method can improve the finding of (one) correct translation.</S>
        <S ID="S-25596">The baseline system using only &#65533;&#65533; (steering, guidance) outputs the correct translation &#8221;steering&#8221; at rank 4, whereas our method using all four terms outputs it at rank 2.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>6 Conclusions</HEADER>
      <P>
        <S ID="S-25597">We proposed a new method for translation acquisition which uses a set of synonyms to acquire translations.</S>
        <S ID="S-25598">Our approach combines the query term&#8217;s context vector with all the context vectors of its synonyms.</S>
        <S ID="S-25599">In order to combine the vectors we use a weighted average of each context vector, where the weights are determined by a term&#8217;s occurrence frequency.</S>
        <S ID="S-25600">Our experiments, using the Japanese and English WordNet (Bond et al., 2009; Fellbaum, 1998), show that our proposed method can increase the translation accuracy, when compared to using only a single query term, or smoothing with nearest neighbours.</S>
        <S ID="S-25601">Our results suggest that instead of directly searching for a translation, it is worth first looking for synonyms, for example by considering spelling variations or monolingual resources.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS/>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>S Basu</RAUTHOR>
      <REFTITLE>A probabilistic framework for semi-supervised clustering.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>F Bond</RAUTHOR>
      <REFTITLE>Enhancing the japanese wordnet.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>H D&#233;jean</RAUTHOR>
      <REFTITLE>An approach based on multilingual thesauri and model combination for bilingual lexicon extraction.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>S Evert</RAUTHOR>
      <REFTITLE>The statistics of word cooccurrences: word pairs and collocations. Doctoral dissertation, Institut f&#252;r maschinelle Sprachverarbeitung,</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>C Fellbaum</RAUTHOR>
      <REFTITLE>Wordnet: an electronic lexical database. Cambrige,</REFTITLE>
      <DATE>1998</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>P Fung</RAUTHOR>
      <REFTITLE>A statistical view on bilingual lexicon extraction: from parallel corpora to non-parallel corpora.</REFTITLE>
      <DATE>1998</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>G Grefenstette</RAUTHOR>
      <REFTITLE>Explorations in automatic thesaurus discovery.</REFTITLE>
      <DATE>1994</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>A Haghighi</RAUTHOR>
      <REFTITLE>Learning bilingual lexicons from monolingual corpora.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>A Ismail</RAUTHOR>
      <REFTITLE>Bilingual lexicon extraction from comparable corpora using in-domain terms.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>Jammalamadaka</RAUTHOR>
      <REFTITLE>Topics in circular statistics, volume 5. World Scientific Pub Co Inc.</REFTITLE>
      <DATE>2001</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>P Koehn</RAUTHOR>
      <REFTITLE>Estimating word translation probabilities from unrelated monolingual corpora using the em algorithm.</REFTITLE>
      <DATE>2000</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>T Kudo</RAUTHOR>
      <REFTITLE>Applying conditional random fields to Japanese morphological analysis.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>A Laroche</RAUTHOR>
      <REFTITLE>Revisiting contextbased projection methods for term-translation spotting in comparable corpora.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>N Okazaki</RAUTHOR>
      <REFTITLE>A discriminative candidate generator for string transformations.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="14">
      <RAUTHOR>V Pekar</RAUTHOR>
      <REFTITLE>Finding translations for low-frequency words in comparable corpora.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="15">
      <RAUTHOR>R Rapp</RAUTHOR>
      <REFTITLE>Automatic identification of word translations from unrelated English and German corpora.</REFTITLE>
      <DATE>1999</DATE>
    </REFERENCE>
    <REFERENCE ID="16">
      <RAUTHOR>K Sato</RAUTHOR>
      <REFTITLE>Introduction of a Japanese language processing middleware used for CRM.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
