<PAPER>
  <FILENO/>
  <TITLE>Is Machine Translation Ripe for Cross-lingual Sentiment Classification?</TITLE>
  <AUTHORS>
    <AUTHOR>Kevin Duh</AUTHOR>
    <AUTHOR>Akinori Fujino</AUTHOR>
  </AUTHORS>
  <ABSTRACT>
    <A-S ID="S-33763">Recent advances in Machine Translation (MT) have brought forth a new paradigm for building NLP applications in low-resource scenarios.</A-S>
    <A-S ID="S-33764">To build a sentiment classifier for a language with no labeled resources, one can translate labeled data from another language, then train a classifier on the translated text.</A-S>
    <A-S ID="S-33765">This can be viewed as a domain adaptation problem, where labeled translations and test data have some mismatch.</A-S>
    <A-S ID="S-33766">Various prior work have achieved positive results using this approach.</A-S>
    <A-S ID="S-33767">In this opinion piece, we take a step back and make some general statements about crosslingual adaptation problems.</A-S>
    <A-S ID="S-33768">First, we claim that domain mismatch is not caused by MT errors, and accuracy degradation will occur even in the case of perfect MT.</A-S>
    <A-S ID="S-33769">Second, we argue that the cross-lingual adaptation problem is qualitatively different from other (monolingual) adaptation problems in NLP; thus new adaptation algorithms ought to be considered.</A-S>
    <A-S ID="S-33770">This paper will describe a series of carefullydesigned experiments that led us to these conclusions.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Summary</HEADER>
      <P>
        <S ID="S-33771">Question 1: If MT gave perfect translations (semantically), do we still have a domain adaptation challenge in cross-lingual sentiment classification?</S>
      </P>
      <P>
        <S ID="S-33772">Answer: Yes.</S>
        <S ID="S-33773">The reason is that while many translations of a word may be valid, the MT system might have a systematic bias.</S>
        <S ID="S-33774">For example, the word &#8220;awesome&#8221; might be prevalent in English reviews, but in 429 translated reviews, the word &#8220;excellent&#8221; is generated instead.</S>
        <S ID="S-33775">From the perspective of MT, this translation is correct and preserves sentiment polarity.</S>
        <S ID="S-33776">But from the perspective of a classifier, there is a domain mismatch due to differences in word distributions.</S>
      </P>
      <P>
        <S ID="S-33777">Question 2: Can we apply standard adaptation algorithms developed for other (monolingual) adaptation problems to cross-lingual adaptation?</S>
      </P>
      <P>
        <S ID="S-33778">Answer: No.</S>
        <S ID="S-33779">It appears that the interaction between target unlabeled data and source data can be rather unexpected in the case of cross-lingual adaptation.</S>
        <S ID="S-33780">We do not know the reason, but our experiments show that the accuracy of adaptation algorithms in cross-lingual scenarios have much higher variance than monolingual scenarios.</S>
      </P>
      <P>
        <S ID="S-33781">The goal of this opinion piece is to argue the need to better understand the characteristics of domain adaptation in cross-lingual problems.</S>
        <S ID="S-33782">We invite the reader to disagree with our conclusion (that the true barrier to good performance is not insufficient MT quality, but inappropriate domain adaptation methods).</S>
        <S ID="S-33783">Here we present a series of experiments that led us to this conclusion.</S>
        <S ID="S-33784">First we describe the experiment design (&#167;2) and baselines (&#167;3), before answering Question 1 (&#167;4) and Question 2 (&#167;5).</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Experiment Design</HEADER>
      <P>
        <S ID="S-33785">The cross-lingual setup is this: we have labeled data from source domain S and wish to build a sentiment classifier for target domain T .</S>
        <S ID="S-33786">Domain mismatch can arise from language differences (e.g. English vs. translated text) or market differences (e.g. DVD vs.</S>
        <S ID="S-33787">Book reviews).</S>
        <S ID="S-33788">Our experiments will involve fixing</S>
      </P>
      <P>
        <S ID="S-33789">T to a common testset and varying S.</S>
        <S ID="S-33790">This allows us to experiment with different settings for adaptation.</S>
        <S ID="S-33791">We use the Amazon review dataset of Prettenhofer (2010) 1 , due to its wide range of languages (English [EN], Japanese [JP], French [FR], German [DE]) and markets (music, DVD, books).</S>
        <S ID="S-33792">Unlike Prettenhofer (2010), we reverse the direction of cross-lingual adaptation and consider English as target.</S>
        <S ID="S-33793">English is not a low-resource language, but this setting allows for more comparisons.</S>
        <S ID="S-33794">Each source dataset has 2000 reviews, equally balanced between positive and negative.</S>
        <S ID="S-33795">The target has 2000 test samples, large unlabeled data (25k, 30k, 50k samples respectively for Music, DVD, and Books), and an additional 2000 labeled data reserved for oracle experiments.</S>
        <S ID="S-33796">Texts in JP, FR, and DE are translated word-by-word into English with Google Translate.</S>
        <S ID="S-33797">2 We perform three sets of experiments, shown in Table 1.</S>
        <S ID="S-33798">Table 2 lists all the results; we will interpret them in the following sections.</S>
      </P>
      <P>
        <S ID="S-33799">Target (T ) Source (S) 1 Music-EN Music-JP, Music-FR, Music-DE, DVD-EN, Book-EN 2 DVD-EN DVD-JP, DVD-FR, DVD-DE, Music-EN, Book-EN 3 Book-EN Book-JP, Book-FR, Book-DE, Music-EN, DVD-EN</S>
      </P>
      <P>
        <S ID="S-33800">3 How much performance degradation occurs in cross-lingual adaptation?</S>
      </P>
      <P>
        <S ID="S-33801">First, we need to quantify the accuracy degradation under different source data, without consideration of domain adaptation methods.</S>
        <S ID="S-33802">So we train a SVM classifier on labeled source data 3 , and directly apply it on test data.</S>
        <S ID="S-33803">The oracle setting, which has no domain-mismatch (e.g. train on Music-EN, test on Music-EN), achieves an average test accuracy of (81.6 + 80.9 + 80.0)/3 = 80.8% 4 .</S>
        <S ID="S-33804">Aver-</S>
      </P>
      <P>
        <S ID="S-33805">1 http://www.webis.de/research/corpora/webis-cls-10 2 This is done by querying foreign words to build a bilingual</S>
      </P>
      <P>
        <S ID="S-33806">dictionary.</S>
        <S ID="S-33807">The words are converted to tfidf unigram features.</S>
        <S ID="S-33808">3 For all methods we try here, 5% of the 2000 labeled source</S>
      </P>
      <P>
        <S ID="S-33809">samples are held-out for parameter tuning.</S>
        <S ID="S-33810">4 See column EN of Table 2, Supervised SVM results.</S>
      </P>
      <P>
        <S ID="S-33811">age cross-lingual accuracies are: 69.4% (JP), 75.6% (FR), 77.0% (DE), so degradations compared to oracle are: -11% (JP), -5% (FR), -4% (DE).</S>
        <S ID="S-33812">5 Crossmarket degradations are around -6% 6 .</S>
        <S ID="S-33813">Observation 1: Degradations due to market and language mismatch are comparable in several cases (e.g. MUSIC-DE and DVD-EN perform similarly for target MUSIC-EN).</S>
        <S ID="S-33814">Observation 2: The ranking of source language by decreasing accuracy is DE &gt; FR &gt; JP.</S>
        <S ID="S-33815">Does this mean JP-EN is a more difficult language pair for MT?</S>
        <S ID="S-33816">The next section will show that this is not necessarily the case.</S>
        <S ID="S-33817">Certainly, the domain mismatch for JP is larger than DE, but this could be due to phenomenon other than MT errors.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 Where exactly is the domain mismatch?</HEADER>
      <P>
        <S ID="S-33882"></S>
      </P>
      <DIV DEPTH="1">
        <HEADER>4.1 Theory of Domain Adaptation</HEADER>
        <P>
          <S ID="S-33818">We analyze domain adaptation by the concepts of labeling and instance mismatch (<REF ID="R-04" RPTR="3">Jiang and Zhai, 2007</REF>).</S>
          <S ID="S-33819">Let p t (x,y) = p t (y|x)p t (x) be the target distribution of samples x (e.g. unigram feature vector) and labels y (positive / negative).</S>
          <S ID="S-33820">Let p s (x,y) = p s (y|x)p s (x) be the corresponding source distribution.</S>
          <S ID="S-33821">We assume that one (or both) of the following distributions differ between source and target:</S>
        </P>
        <P>
          <S ID="S-33822">&#8226; Instance mismatch: p s (x) &#8800; p t (x).</S>
        </P>
        <P>
          <S ID="S-33823">&#8226; Labeling mismatch: p s (y|x) &#8800; p t (y|x).</S>
        </P>
        <P>
          <S ID="S-33824">Instance mismatch implies that the input feature vectors have different distribution (e.g. one dataset uses the word &#8220;excellent&#8221; often, while the other uses the word &#8220;awesome&#8221;).</S>
          <S ID="S-33825">This degrades performance because classifiers trained on &#8220;excellent&#8221; might not know how to classify texts with the word &#8220;awesome.&#8221; The solution is to tie together these features (<REF ID="R-02" RPTR="1">Blitzer et al., 2006</REF>) or re-weight the input distribution (Sugiyama et al., 2008).</S>
          <S ID="S-33826">Under some assumptions (i.e. covariate shift), oracle accuracy can be achieved theoretically (<REF ID="R-06" RPTR="5">Shimodaira, 2000</REF>).</S>
          <S ID="S-33827">Labeling mismatch implies the same input has different labels in different domains.</S>
          <S ID="S-33828">For example, the JP word meaning &#8220;excellent&#8221; may be mistranslated as &#8220;bad&#8221; in English.</S>
          <S ID="S-33829">Then, positive JP</S>
        </P>
        <P>
          <S ID="S-33830">5 See &#8220;Adapt by Language&#8221; columns of Table 2.</S>
          <S ID="S-33831">Note</S>
        </P>
        <P>
          <S ID="S-33832">JP+FR+DE condition has 6000 labeled samples, so is not directly comparable to other adaptation scenarios (2000 samples).</S>
          <S ID="S-33833">Nevertheless, mixing languages seem to give good results.</S>
          <S ID="S-33834">6 See &#8220;Adapt by Market&#8221; columns of Table 2.</S>
        </P>
        <P>
          <S ID="S-33835">Target Classifier Oracle Adapt by Language Adapt by Market EN JP FR DE JP+FR+DE MUSIC DVD BOOK</S>
        </P>
        <P>
          <S ID="S-33836">reviews will be associated with the word &#8220;bad&#8221;: p s (y = +1|x = bad) will be high, whereas the true conditional distribution should have high p t (y = &#8722;1|x = bad) instead.</S>
          <S ID="S-33837">There are several cases for labeling mismatch, depending on how the polarity changes (Table 3).</S>
          <S ID="S-33838">The solution is to filter out these noisy samples (<REF ID="R-04" RPTR="4">Jiang and Zhai, 2007</REF>) or optimize loosely-linked objectives through shared parameters or Bayesian priors (<REF ID="R-03" RPTR="2">Finkel and Manning, 2009</REF>).</S>
        </P>
        <P>
          <S ID="S-33839">Which mismatch is responsible for accuracy degradations in cross-lingual adaptation?</S>
        </P>
        <P>
          <S ID="S-33840">&#8226; Instance mismatch: Systematic MT bias generates word distributions different from naturallyoccurring English.</S>
          <S ID="S-33841">(Translation may be valid.</S>
          <S ID="S-33842">)</S>
        </P>
        <P>
          <S ID="S-33843">&#8226; Label mismatch: MT error mis-translates a word into something with different polarity.</S>
        </P>
        <P>
          <S ID="S-33844">Conclusion from &#167;4.2 and &#167;4.3: Instance mismatch occurs often; MT error appears minimal.</S>
        </P>
        <P>
          <S ID="S-33845">Mis-translated polarity Effect</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>4.2 Analysis of Instance Mismatch</HEADER>
        <P>
          <S ID="S-33846">To measure instance mismatch, we compute statistics between p s (x) and p t (x), or approximations thereof: First, we calculate a (normalized) average feature from all samples of source S, which represents the unigram distribution of MT output.</S>
          <S ID="S-33847">Similarly, the average feature vector for target T approximates the unigram distribution of English reviews p t (x).</S>
          <S ID="S-33848">Then we measure:</S>
        </P>
        <P>
          <S ID="S-33849">&#8226; KL Divergence between Avg(S) and Avg(T ), where Avg() is the average vector.</S>
        </P>
        <P>
          <S ID="S-33850">&#8226; Set Coverage of Avg(T ) on Avg(S): how many word (type) in T appears at least once in S.</S>
        </P>
        <P>
          <S ID="S-33851">Both measures correlate strongly with final accuracy, as seen in Figure 1.</S>
          <S ID="S-33852">The correlation coefficients are r = &#8722;0.78 for KL Divergence and r = 0.71 for Coverage, both statistically significant (p &lt; 0.05).</S>
          <S ID="S-33853">This implies that instance mismatch is an important reason for the degradations seen in Section 3.</S>
          <S ID="S-33854">7</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>4.3 Analysis of Labeling Mismatch</HEADER>
        <P>
          <S ID="S-33855">We measure labeling mismatch by looking at differences in the weight vectors of oracle SVM and adapted SVM.</S>
          <S ID="S-33856">Intuitively, if a feature has positive weight in the oracle SVM, but negative weight in the adapted SVM, then it is likely a MT mis-translation</S>
        </P>
        <P>
          <S ID="S-33857">7 The observant reader may notice that cross-market points</S>
        </P>
        <P>
          <S ID="S-33858">exhibit higher coverage but equal accuracy (74-78%) to some cross-lingual points.</S>
          <S ID="S-33859">This suggests that MT output may be more constrained in vocabulary than naturally-occurring English.</S>
        </P>
        <P>
          <S ID="S-33860">KL Divergence Test Coverage 0.35</S>
        </P>
        <P>
          <S ID="S-33861">0.3</S>
        </P>
        <P>
          <S ID="S-33862">0.25</S>
        </P>
        <P>
          <S ID="S-33863">0.2</S>
        </P>
        <P>
          <S ID="S-33864">0.15</S>
        </P>
        <P>
          <S ID="S-33865">0.1</S>
        </P>
        <P>
          <S ID="S-33866">0.05</S>
        </P>
        <P>
          <S ID="S-33867">0.9</S>
        </P>
        <P>
          <S ID="S-33868">0.8</S>
        </P>
        <P>
          <S ID="S-33869">0.7</S>
        </P>
        <P>
          <S ID="S-33870">0.6</S>
        </P>
        <P>
          <S ID="S-33871">Algorithm 1 Measuring labeling mismatch Input: Weight vectors for source w s and target w t Input: Target data average sample vector avg(T ) Output: Polarity flip rate f 1: Normalize: w s = avg(T ) * w s ; w t = avg(T ) * w t 2: Set S + = { K most positive features in w s } 3: Set S &#8722; = { K most negative features in w s } 4: Set T + = { K most positive features in w t } 5: Set T &#8722; = { K most negative features in w t } 6: for each feature i &#8712; T + do 7: if i &#8712; S &#8722; then f = f + 1 8: end for 9: for each feature j &#8712; T &#8722; do 10: if j &#8712; S + then f = f + 1 11: end for 12: f = f 2K</S>
        </P>
        <P>
          <S ID="S-33872">0.5</S>
        </P>
        <P>
          <S ID="S-33873">is causing the polarity flip.</S>
          <S ID="S-33874">Algorithm 1 (with K=2000) shows how we compute polarity flip rate.</S>
          <S ID="S-33875">8</S>
        </P>
        <P>
          <S ID="S-33876">We found that the polarity flip rate does not correlate well with accuracy at all (r = 0.04).</S>
          <S ID="S-33877">Conclusion: Labeling mismatch is not a factor in performance degradation.</S>
          <S ID="S-33878">Nevertheless, we note there is a surprising large number of flips (24% on average).</S>
          <S ID="S-33879">A manual check of the flipped words in BOOK-JP revealed few MT mistakes.</S>
          <S ID="S-33880">Only 3.7% of 450 random EN-JP word pairs checked can be judged as blatantly incorrect (without sentence context).</S>
          <S ID="S-33881">The majority of flipped words do not have a clear sentiment orientation (e.g. &#8220;amazon&#8221;, &#8220;human&#8221;, &#8220;moreover&#8221;).</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Are standard adaptation algorithms applicable to cross-lingual problems?</HEADER>
      <P>
        <S ID="S-33883">One of the breakthroughs in cross-lingual text classification is the realization that it can be cast as domain adaptation.</S>
        <S ID="S-33884">This makes available a host of preexisting adaptation algorithms for improving over supervised results.</S>
        <S ID="S-33885">However, we argue that it may be</S>
      </P>
      <P>
        <S ID="S-33886">8 The feature normalization in Step 1 is important to ensure</S>
      </P>
      <P>
        <S ID="S-33887">that the weight magnitudes are comparable.</S>
      </P>
      <P>
        <S ID="S-33888">better to &#8220;adapt&#8221; the standard adaptation algorithm to the cross-lingual setting.</S>
        <S ID="S-33889">We arrived at this conclusion by trying the adapted counterpart of SVMs off-the-shelf.</S>
        <S ID="S-33890">Recently, (<REF ID="R-01" RPTR="0">Bergamo and Torresani, 2010</REF>) showed that Transductive SVMs (TSVM), originally developed for semi-supervised learning, are also strong adaptation methods.</S>
        <S ID="S-33891">The idea is to train on source data like a SVM, but encourage the classification boundary to divide through low density regions in the unlabeled target data.</S>
      </P>
      <P>
        <S ID="S-33892">Table 2 shows that TSVM outperforms SVM in all but one case for cross-market adaptation, but gives mixed results for cross-lingual adaptation.</S>
        <S ID="S-33893">This is a puzzling result considering that both use the same unlabeled data.</S>
        <S ID="S-33894">Why does TSVM exhibit such a large variance on cross-lingual problems, but not on cross-market problems?</S>
        <S ID="S-33895">Is unlabeled target data interacting with source data in some unexpected way?</S>
      </P>
      <P>
        <S ID="S-33896">Certainly there are several successful studies (Wan, 2009; Wei and Pal, 2010; Banea et al., 2008), but we think it is important to consider the possibility that cross-lingual adaptation has some fundamental differences.</S>
        <S ID="S-33897">We conjecture that adapting from artificially-generated text (e.g. MT output) is a different story than adapting from naturallyoccurring text (e.g. cross-market).</S>
        <S ID="S-33898">In short, MT is ripe for cross-lingual adaptation; what is not ripe is probably our understanding of the special characteristics of the adaptation problem.</S>
      </P>
      <P>
        <S ID="S-33899">References</S>
      </P>
      <P>
        <S ID="S-33900">Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer Hassan.</S>
        <S ID="S-33901">2008.</S>
        <S ID="S-33902">Multilingual subjectivity analysis using machine translation.</S>
        <S ID="S-33903">In Proc. of Conference on Empirical Methods in Natural Language Processing (EMNLP).</S>
        <S ID="S-33904">Alessandro Bergamo and Lorenzo Torresani.</S>
        <S ID="S-33905">2010.</S>
        <S ID="S-33906">Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach.</S>
        <S ID="S-33907">In Advances in Neural Information Processing Systems (NIPS).</S>
        <S ID="S-33908">John Blitzer, Ryan McDonald, and Fernando Pereira.</S>
      </P>
      <P>
        <S ID="S-33909">2006.</S>
        <S ID="S-33910">Domain adaptation with structural correspondence learning.</S>
        <S ID="S-33911">In Proc. of Conference on Empirical Methods in Natural Language Processing (EMNLP).</S>
        <S ID="S-33912">Jenny Rose Finkel and Chris Manning.</S>
        <S ID="S-33913">2009.</S>
        <S ID="S-33914">Hierarchical Bayesian domain adaptation.</S>
        <S ID="S-33915">In Proc. of NAACL Human Language Technologies (HLT).</S>
      </P>
      <P>
        <S ID="S-33916">Jing Jiang and ChengXiang Zhai.</S>
        <S ID="S-33917">2007.</S>
        <S ID="S-33918">Instance weighting for domain adaptation in NLP.</S>
        <S ID="S-33919">In Proc. of the Association for Computational Linguistics (ACL).</S>
        <S ID="S-33920">Peter Prettenhofer and Benno Stein.</S>
        <S ID="S-33921">2010.</S>
        <S ID="S-33922">Crosslanguage text classification using structural correspondence learning.</S>
        <S ID="S-33923">In Proc. of the Association for Computational Linguistics (ACL).</S>
        <S ID="S-33924">Hidetoshi Shimodaira.</S>
        <S ID="S-33925">2000.</S>
        <S ID="S-33926">Improving predictive inference under covariate shift by weighting the loglikelihood function.</S>
        <S ID="S-33927">Journal of Statistical Planning and Inferenc, 90.</S>
        <S ID="S-33928">Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima,</S>
      </P>
      <P>
        <S ID="S-33929">Hisashi Kashima, Paul von B&#252;nau, and Motoaki Kawanabe.</S>
        <S ID="S-33930">2008.</S>
        <S ID="S-33931">Direct importance estimation for covariate shift adaptation.</S>
        <S ID="S-33932">Annals of the Institute of Statistical Mathematics, 60(4).</S>
        <S ID="S-33933">Xiaojun Wan.</S>
        <S ID="S-33934">2009.</S>
        <S ID="S-33935">Co-training for cross-lingual sentiment classification.</S>
        <S ID="S-33936">In Proc. of the Association for Computational Linguistics (ACL).</S>
        <S ID="S-33937">Bin Wei and Chris Pal.</S>
        <S ID="S-33938">2010.</S>
        <S ID="S-33939">Cross lingual adaptation:</S>
      </P>
      <P>
        <S ID="S-33940">an experiment on sentiment classification.</S>
        <S ID="S-33941">In Proceedings of the ACL 2010 Conference Short Papers.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS/>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>Carmen Banea</RAUTHOR>
      <REFTITLE>Multilingual subjectivity analysis using machine translation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>Alessandro Bergamo</RAUTHOR>
      <REFTITLE>Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>John Blitzer</RAUTHOR>
      <REFTITLE>Domain adaptation with structural correspondence learning.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>Jenny Rose Finkel</RAUTHOR>
      <REFTITLE>Hierarchical Bayesian domain adaptation.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>Jing Jiang</RAUTHOR>
      <REFTITLE>Instance weighting for domain adaptation in NLP.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>Peter Prettenhofer</RAUTHOR>
      <REFTITLE>Crosslanguage text classification using structural correspondence learning.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>Hidetoshi Shimodaira</RAUTHOR>
      <REFTITLE>Improving predictive inference under covariate shift by weighting the loglikelihood function.</REFTITLE>
      <DATE>2000</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>Masashi Sugiyama</RAUTHOR>
      <REFTITLE>Direct importance estimation for covariate shift adaptation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>Xiaojun Wan</RAUTHOR>
      <REFTITLE>Co-training for cross-lingual sentiment classification.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
