<document>
  <filename>N09-1015</filename>
  <authors/>
  <title>Intersecting multilingual data for faster and better statistical translations</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>In current phrase-based SMT systems, more training data is generally better than less. However, a larger data set eventually introduces a larger model that enlarges the search space for the translation problem, and consequently requires more time and more resources to translate. We argue redundant information in a SMT system may not only delay the computations but also affect the quality of the outputs. This paper proposes an approach to reduce the model size by filtering out the less probable entries based on compatible data in an intermediate language, a novel use of triangulation, without sacrificing the translation quality. Comprehensive experiments were conducted on standard data sets. We achieved significant quality improvements (up to 2.3 BLEU points) while translating with reduced models. In addition, we demonstrate a straightforward combination method for more progressive filtering. The reduction of the model size can be up to 94% with the translation quality being preserved.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In current phrase-based SMT systems, more training data is generally better than less.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, a larger data set eventually introduces a larger model that enlarges the search space for the translation problem, and consequently requires more time and more resources to translate.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We argue redundant information in a SMT system may not only delay the computations but also affect the quality of the outputs.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This paper proposes an approach to reduce the model size by filtering out the less probable entries based on compatible data in an intermediate language, a novel use of triangulation, without sacrificing the translation quality.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Comprehensive experiments were conducted on standard data sets.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We achieved significant quality improvements (up to 2.3 BLEU points) while translating with reduced models.</text>
              <doc_id>5</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In addition, we demonstrate a straightforward combination method for more progressive filtering.</text>
              <doc_id>6</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The reduction of the model size can be up to 94% with the translation quality being preserved.</text>
              <doc_id>7</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Statistical machine translation (SMT) applies machine learning techniques to a bilingual corpus to produce a translation system entirely automatically. Such a scheme has many potential advantages over earlier systems which relied on carefully crafted rules. The most obvious is that it at dramatically reduces cost in human labor and it is able to reach many critical translation rules that are easily overlooked by human being.
SMT systems generally assemble translations by selecting phrases from a large candidate set. Unsupervised learning often introduces a considerable amount of noise into this set as a result of which the selection process becomes more longer and less effective. This paper provides one approach to these problems. Various filtering techniques, such as (Johnson et al., 2007) and (Chen et al., 2008), have been applied to eliminate a large portion of the translation rules that were judged unlikely to be of value for the current translation. However, these approaches were only able to improve the translation quality slightly. In this paper, we describe a triangulation approach (Kay, 1997) that incorporates multilingual data to improve system efficiency and translation quality at the same time. Most of the previous triangulation approaches (Kumar et al., 2007; Cohn and Lapata, 2007; Filali and Bilmes, 2005; Simard, 1999; Och and Ney, 2001) add information obtained from a third language. In other words, they work with the union of the data from the different languages. In contrast, we work with the intersection of information acquired through a third language. The hope is that the intersection will be more precise and more compact than the union, so that a better result will be obtained more efficiently.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Statistical machine translation (SMT) applies machine learning techniques to a bilingual corpus to produce a translation system entirely automatically.</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Such a scheme has many potential advantages over earlier systems which relied on carefully crafted rules.</text>
              <doc_id>9</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The most obvious is that it at dramatically reduces cost in human labor and it is able to reach many critical translation rules that are easily overlooked by human being.</text>
              <doc_id>10</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>SMT systems generally assemble translations by selecting phrases from a large candidate set.</text>
              <doc_id>11</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Unsupervised learning often introduces a considerable amount of noise into this set as a result of which the selection process becomes more longer and less effective.</text>
              <doc_id>12</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This paper provides one approach to these problems.</text>
              <doc_id>13</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Various filtering techniques, such as (Johnson et al., 2007) and (Chen et al., 2008), have been applied to eliminate a large portion of the translation rules that were judged unlikely to be of value for the current translation.</text>
              <doc_id>14</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>However, these approaches were only able to improve the translation quality slightly.</text>
              <doc_id>15</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, we describe a triangulation approach (Kay, 1997) that incorporates multilingual data to improve system efficiency and translation quality at the same time.</text>
              <doc_id>16</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Most of the previous triangulation approaches (Kumar et al., 2007; Cohn and Lapata, 2007; Filali and Bilmes, 2005; Simard, 1999; Och and Ney, 2001) add information obtained from a third language.</text>
              <doc_id>17</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In other words, they work with the union of the data from the different languages.</text>
              <doc_id>18</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>In contrast, we work with the intersection of information acquired through a third language.</text>
              <doc_id>19</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>The hope is that the intersection will be more precise and more compact than the union, so that a better result will be obtained more efficiently.</text>
              <doc_id>20</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Noise in a phrase-based SMT system</title>
        <text>The phrases in a translation model are extracted heuristically from a word alignment between the parallel texts in two languages using machine learning techniques. The translation model feature values are stored in the form of a so-called phrase-table,
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 128&#8211;136, Boulder, Colorado, June 2009. c&#9675;2009 Association for Computational Linguistics
while the distortion model is in the reordering-table. As we have said models built in this way tend to contain a contains a considerable amount of noise. The phrase-table entries are far less reliable than the lexicons and grammar rules handcrafted for rule-based systems.
The main source of noise in the phrase table is errors from the word alignment process. For example, many function words occur so frequently that they are incorrectly mapped to translations of many function words in the other language to which they are, in fact, unrelated. On the other hand, many words remain unaligned on account of their very low frequency. Another source noise comes from the phrase extraction algorithm itself. The unaligned words are usually attached to aligned sequences In order to achieve longer phrase pairs.
The final selection of entries from the phrase table is based not only on the values assigned to them there, but also to values coming from the language and reordering models, so that entries that receive an initially high value may end up not being preferred.
(1) Sie lieben ihre Kinder nicht. they love their children not
They don&#8217;t love their children.
The frequently occurring German negative &#8220;nicht&#8221; in (1). is sometimes difficult for SMT systems to translate into English because it may appear in many positions of a sentence. For instance, it occurs at the end of the sentence in (1). The phrase pairs &#8220;ihre kinder nicht &#8594; their children are not&#8221; and &#8220;ihre kinder nicht &#8594; their children&#8221; are both likely also to appear in the phrase table and the former has greater estimated probability. However, the language model would preferred the latter in this example because the sentence &#8220;They love their children are not.&#8221; is unlikely to be attested. Accordingly, SMT system may therefore produce the misleading translation in (2).
(2) They love their children.
The system would not produce translations with the opposite meanings if the noisy entries like &#8220;ihre kinder nicht &#8594; their children&#8221; were excluded from the translation candidates. Eliminating the noise should help to improve the system&#8217;s performance, for both efficiency and translation quality.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The phrases in a translation model are extracted heuristically from a word alignment between the parallel texts in two languages using machine learning techniques.</text>
              <doc_id>21</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The translation model feature values are stored in the form of a so-called phrase-table,</text>
              <doc_id>22</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 128&#8211;136, Boulder, Colorado, June 2009. c&#9675;2009 Association for Computational Linguistics</text>
              <doc_id>23</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>while the distortion model is in the reordering-table.</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As we have said models built in this way tend to contain a contains a considerable amount of noise.</text>
              <doc_id>25</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The phrase-table entries are far less reliable than the lexicons and grammar rules handcrafted for rule-based systems.</text>
              <doc_id>26</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The main source of noise in the phrase table is errors from the word alignment process.</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For example, many function words occur so frequently that they are incorrectly mapped to translations of many function words in the other language to which they are, in fact, unrelated.</text>
              <doc_id>28</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>On the other hand, many words remain unaligned on account of their very low frequency.</text>
              <doc_id>29</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Another source noise comes from the phrase extraction algorithm itself.</text>
              <doc_id>30</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The unaligned words are usually attached to aligned sequences In order to achieve longer phrase pairs.</text>
              <doc_id>31</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The final selection of entries from the phrase table is based not only on the values assigned to them there, but also to values coming from the language and reordering models, so that entries that receive an initially high value may end up not being preferred.</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(1) Sie lieben ihre Kinder nicht.</text>
              <doc_id>33</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>they love their children not</text>
              <doc_id>34</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>They don&#8217;t love their children.</text>
              <doc_id>35</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The frequently occurring German negative &#8220;nicht&#8221; in (1).</text>
              <doc_id>36</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>is sometimes difficult for SMT systems to translate into English because it may appear in many positions of a sentence.</text>
              <doc_id>37</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For instance, it occurs at the end of the sentence in (1).</text>
              <doc_id>38</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The phrase pairs &#8220;ihre kinder nicht &#8594; their children are not&#8221; and &#8220;ihre kinder nicht &#8594; their children&#8221; are both likely also to appear in the phrase table and the former has greater estimated probability.</text>
              <doc_id>39</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>However, the language model would preferred the latter in this example because the sentence &#8220;They love their children are not.&#8221; is unlikely to be attested.</text>
              <doc_id>40</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Accordingly, SMT system may therefore produce the misleading translation in (2).</text>
              <doc_id>41</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(2) They love their children.</text>
              <doc_id>42</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The system would not produce translations with the opposite meanings if the noisy entries like &#8220;ihre kinder nicht &#8594; their children&#8221; were excluded from the translation candidates.</text>
              <doc_id>43</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Eliminating the noise should help to improve the system&#8217;s performance, for both efficiency and translation quality.</text>
              <doc_id>44</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Triangulated filtering</title>
        <text>While direct translation and pivot translation through a bridge language presumably introduce noise, in substantially similar amounts, there is no reason to expect the noise in the two systems to correlate strongly. In fact, the noise from such different sources, tends to be quite distinct, whereas the more useful information is often retained. This encourages us to hope that information gathered from various sources will be more reliable overall.
Our plan is to ameliorate the noise problem by constructing a smaller phrase-table by taking the intersection of a number of sources. We reason that a target phrase is will appear as a candidate translation of a given source phrase, only if it also appears as a candidate translation for some word or phrase in the bridge language mapping to the source phrase. We refer to this triangulation approach as triangulated phrase-table filtering.
Source&#8722;Bridge Model
Target&#8722;Bridge Model
Source Text
Monolingual Corpus Parallel Corpus
Translation Model
Filtered Model
Counting Smoothing
SMT Decoder
Language Model
Alignment, Phrase Extraction
Filtering
Target Text
Figure 1 illustrates our triangulation approach. Two bridge models are first constructed: one from the source language to the bridge language, and another from the target language to the bridge language. Then, we use these two models to filter the original source-target model. For each phrase pair in the original table, we try to find a common link in these bridge models to connect both phrases. If such links do not exist, we remove the entry from the table. The probability values in the table remain
unchanged. The reduced table can be used in place of the original one in the SMT system.
There are various forms of links that can be used as our evidence for the filtering process. One obvious form is complete phrases in the bridge language, which means, for each phrase pair in the model to be filtered, we should look for a third phrase in the bridge language that can relate the two phrases in the pair. This approach to filtering examines each phrase pair presented in the phrase-table one by one. For each phrase pair, we collect the corresponding translations using the models for translation into a third language. If both phrases can be mapped to some phrases in the bridge language, but to different ones, we should remove it from the model. It is also possible that neither of the phrases appear in corresponding bridge models. In this case, we consider the bridge models insufficient for making the filtering decision and prefer to keep the pair in the table.
The way a decoder constructs translation hypotheses is directly related to the weights for different model features in a SMT system, which are usually optimized for a given set of models with minimum error rate training (MERT) (Och, 2003) to achieve better translation performance. In other words, the weights obtained for a model do not necessarily apply to another model. Since the triangulated filtering method removes a part of the model, it is important to readjust the feature weights for the reduced phrase-table.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>While direct translation and pivot translation through a bridge language presumably introduce noise, in substantially similar amounts, there is no reason to expect the noise in the two systems to correlate strongly.</text>
              <doc_id>45</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In fact, the noise from such different sources, tends to be quite distinct, whereas the more useful information is often retained.</text>
              <doc_id>46</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This encourages us to hope that information gathered from various sources will be more reliable overall.</text>
              <doc_id>47</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Our plan is to ameliorate the noise problem by constructing a smaller phrase-table by taking the intersection of a number of sources.</text>
              <doc_id>48</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We reason that a target phrase is will appear as a candidate translation of a given source phrase, only if it also appears as a candidate translation for some word or phrase in the bridge language mapping to the source phrase.</text>
              <doc_id>49</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We refer to this triangulation approach as triangulated phrase-table filtering.</text>
              <doc_id>50</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Source&#8722;Bridge Model</text>
              <doc_id>51</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Target&#8722;Bridge Model</text>
              <doc_id>52</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Source Text</text>
              <doc_id>53</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Monolingual Corpus Parallel Corpus</text>
              <doc_id>54</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Translation Model</text>
              <doc_id>55</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Filtered Model</text>
              <doc_id>56</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Counting Smoothing</text>
              <doc_id>57</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>SMT Decoder</text>
              <doc_id>58</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Language Model</text>
              <doc_id>59</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Alignment, Phrase Extraction</text>
              <doc_id>60</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Filtering</text>
              <doc_id>61</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Target Text</text>
              <doc_id>62</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Figure 1 illustrates our triangulation approach.</text>
              <doc_id>63</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Two bridge models are first constructed: one from the source language to the bridge language, and another from the target language to the bridge language.</text>
              <doc_id>64</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Then, we use these two models to filter the original source-target model.</text>
              <doc_id>65</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>For each phrase pair in the original table, we try to find a common link in these bridge models to connect both phrases.</text>
              <doc_id>66</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>If such links do not exist, we remove the entry from the table.</text>
              <doc_id>67</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The probability values in the table remain</text>
              <doc_id>68</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>unchanged.</text>
              <doc_id>69</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The reduced table can be used in place of the original one in the SMT system.</text>
              <doc_id>70</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>There are various forms of links that can be used as our evidence for the filtering process.</text>
              <doc_id>71</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>One obvious form is complete phrases in the bridge language, which means, for each phrase pair in the model to be filtered, we should look for a third phrase in the bridge language that can relate the two phrases in the pair.</text>
              <doc_id>72</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This approach to filtering examines each phrase pair presented in the phrase-table one by one.</text>
              <doc_id>73</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>For each phrase pair, we collect the corresponding translations using the models for translation into a third language.</text>
              <doc_id>74</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>If both phrases can be mapped to some phrases in the bridge language, but to different ones, we should remove it from the model.</text>
              <doc_id>75</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>It is also possible that neither of the phrases appear in corresponding bridge models.</text>
              <doc_id>76</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In this case, we consider the bridge models insufficient for making the filtering decision and prefer to keep the pair in the table.</text>
              <doc_id>77</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The way a decoder constructs translation hypotheses is directly related to the weights for different model features in a SMT system, which are usually optimized for a given set of models with minimum error rate training (MERT) (Och, 2003) to achieve better translation performance.</text>
              <doc_id>78</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In other words, the weights obtained for a model do not necessarily apply to another model.</text>
              <doc_id>79</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Since the triangulated filtering method removes a part of the model, it is important to readjust the feature weights for the reduced phrase-table.</text>
              <doc_id>80</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Experimental design</title>
        <text>All the text data used in our experiments are from Release v3 of &#8220;European Parliament Proceedings Parallel Corpus 1996-2006&#8221; (Europarl) corpus (Koehn, 2005). We mainly investigated translations from Spanish to English. There are enough structural differences in these two language to introduce some noise in the phrase table. French, Portuguese, Danish, German and Finnish were used as bridge languages. Portuguese is very similar to Spanish and French somewhat less so. Finnish is unrelated and fairly different typologically with Danish and German occupying the middle ground. In addition, we also present briefly the results on German- English translations with Dutch, Spanish and Danish as bridges.
For the Spanish-English pair, three translation models were constructed over the same parallel corpora. We acquired comparable data sets by drawing several subsets from the same corpus according to various maximal sentence lengths. The subsets
we used in the experiments are presented by &#8220;EP- 20&#8221;, &#8220;EP-40&#8221; and &#8220;EP-50&#8221;, in which the numbers indicate the maximal sentence length in respective Europarl subsets. Table 1 lists the characteristics of the Spanish-English subsets. Although the maximal sentence length in these sets is far less than that of the whole corpus (880 tokens), EP-50 already includes nearly 85% of Spanish-English sentence pairs from Europarl. The translations models, both the models to be filtered and the bridge models, were generated from compatible Europarl subsets using the Moses toolkit (Koehn et al., 2007) with the most basic configurations. The feature weights for the Spanish- English translation models were optimized over a development set of 500 sentences using MERT to maximize BLEU (Papineni et al., 2001). The triangulated filtering algorithm was applied to each combination of a translation model and a third language. The reordering models were also filtered according to the phrase-table. Only those phrase pairs that appeared in the phrase-table remained in the reordering table. We rerun the MERT process solely based on the remaining entries in the filtered tables. Each table is used to translate a set of 2,000 sentences of test data (from the shared task of the third Workshop on Statistical Machine Translation, 2008 1 ). Both the test set and the development data set have been excluded from the training data. We evaluated the proposed phrase-table filtering
1 For details, see
http://www.statmt.org/wmt08/shared-task.html
method mainly from two points of view: the efficiency of systems with filtered tables and the quality of output translations produced by the systems.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>All the text data used in our experiments are from Release v3 of &#8220;European Parliament Proceedings Parallel Corpus 1996-2006&#8221; (Europarl) corpus (Koehn, 2005).</text>
              <doc_id>81</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We mainly investigated translations from Spanish to English.</text>
              <doc_id>82</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>There are enough structural differences in these two language to introduce some noise in the phrase table.</text>
              <doc_id>83</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>French, Portuguese, Danish, German and Finnish were used as bridge languages.</text>
              <doc_id>84</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Portuguese is very similar to Spanish and French somewhat less so.</text>
              <doc_id>85</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Finnish is unrelated and fairly different typologically with Danish and German occupying the middle ground.</text>
              <doc_id>86</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In addition, we also present briefly the results on German- English translations with Dutch, Spanish and Danish as bridges.</text>
              <doc_id>87</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For the Spanish-English pair, three translation models were constructed over the same parallel corpora.</text>
              <doc_id>88</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We acquired comparable data sets by drawing several subsets from the same corpus according to various maximal sentence lengths.</text>
              <doc_id>89</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The subsets</text>
              <doc_id>90</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>we used in the experiments are presented by &#8220;EP- 20&#8221;, &#8220;EP-40&#8221; and &#8220;EP-50&#8221;, in which the numbers indicate the maximal sentence length in respective Europarl subsets.</text>
              <doc_id>91</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Table 1 lists the characteristics of the Spanish-English subsets.</text>
              <doc_id>92</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Although the maximal sentence length in these sets is far less than that of the whole corpus (880 tokens), EP-50 already includes nearly 85% of Spanish-English sentence pairs from Europarl.</text>
              <doc_id>93</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The translations models, both the models to be filtered and the bridge models, were generated from compatible Europarl subsets using the Moses toolkit (Koehn et al., 2007) with the most basic configurations.</text>
              <doc_id>94</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The feature weights for the Spanish- English translation models were optimized over a development set of 500 sentences using MERT to maximize BLEU (Papineni et al., 2001).</text>
              <doc_id>95</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The triangulated filtering algorithm was applied to each combination of a translation model and a third language.</text>
              <doc_id>96</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The reordering models were also filtered according to the phrase-table.</text>
              <doc_id>97</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Only those phrase pairs that appeared in the phrase-table remained in the reordering table.</text>
              <doc_id>98</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>We rerun the MERT process solely based on the remaining entries in the filtered tables.</text>
              <doc_id>99</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Each table is used to translate a set of 2,000 sentences of test data (from the shared task of the third Workshop on Statistical Machine Translation, 2008 1 ).</text>
              <doc_id>100</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Both the test set and the development data set have been excluded from the training data.</text>
              <doc_id>101</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>We evaluated the proposed phrase-table filtering</text>
              <doc_id>102</doc_id>
              <sec_id>11</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 For details, see</text>
              <doc_id>103</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>http://www.statmt.org/wmt08/shared-task.html</text>
              <doc_id>104</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>method mainly from two points of view: the efficiency of systems with filtered tables and the quality of output translations produced by the systems.</text>
              <doc_id>105</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>5 Results</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>106</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 System efficiency</title>
            <text>Often the question of machine translation is not only how to produce a good translation, but also how to produce it quickly. To evaluate the system efficiency, we measured both storage space and time consumption. For recording the computation time, we run an identical of installation of the decoder with different models and then measure the average execution time for the given translation task.
In Table 2, we give the number of entries in each phrase table (N), and the physical file size of the phrase table (S P T ) and the reordering table (S RT ) (without any compression or binarization), T l , the time for the program to load phrase tables and T t the time to translate the complete test set. We also highlighted the largest and the smallest reduction from each group.
All filtered models showed significant reductions in size. The greatest reduction of model sizes, taking both phrase-table and reordering table into account, is nearly 11 gigabytes for filtering the largest model (EP-50) with a Finnish bridge, which leads to the maximal time saving of 939 seconds, or almost 16 minutes, for translating two thousand sentences. The reduction rates from two larger models are very close to each other whereas the filtered table scaled down the most significantly on the smallest model (EP-20), which was in fact constructed over a much smaller subset of Europarl corpus, consisting of less than half of the sentences pairs in the other two Europarl subsets. Compared to the larger Europarl subsets, the small data set is expected to produce more errors through training as there is much less relevant data for the machine learning algorithm to correctly extract useful information from. Consequently, there are more noisy entries in this small model, and therefore more entries to be removed. In addition, the filtering is done by exact matching of complete phrases, which presumably happens much less frequently even for correctly paired phrase pairs in the very limited data supplied by the smallest training set. For the same reason, the distinction between different bridge languages was less clear for this small model.
Due to hardware limitation, we are not able to fit the unfiltered phrase tables completely into the memory. Every table was filtered based on the given input so only a small portion of each table was loaded into memory. This may diminish the difference between the original and the filtered table to a certain degree. The relative time consumptionnevertheless agrees with the reduction in size: phrase tables from the smallest model showed the most reduction for both loading the models and processing the translations.
For loading time, we count the time it takes to start and to load the bilingual phrase-tables plus reordering tables and the monolingual language model into the memory. The majority of the loading time for the smallest model, even before filtering, has been used for loading language models and other start-up processes, could not be reduced as much as the reduction on table size.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Often the question of machine translation is not only how to produce a good translation, but also how to produce it quickly.</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To evaluate the system efficiency, we measured both storage space and time consumption.</text>
                  <doc_id>108</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For recording the computation time, we run an identical of installation of the decoder with different models and then measure the average execution time for the given translation task.</text>
                  <doc_id>109</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In Table 2, we give the number of entries in each phrase table (N), and the physical file size of the phrase table (S P T ) and the reordering table (S RT ) (without any compression or binarization), T l , the time for the program to load phrase tables and T t the time to translate the complete test set.</text>
                  <doc_id>110</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We also highlighted the largest and the smallest reduction from each group.</text>
                  <doc_id>111</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>All filtered models showed significant reductions in size.</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The greatest reduction of model sizes, taking both phrase-table and reordering table into account, is nearly 11 gigabytes for filtering the largest model (EP-50) with a Finnish bridge, which leads to the maximal time saving of 939 seconds, or almost 16 minutes, for translating two thousand sentences.</text>
                  <doc_id>113</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The reduction rates from two larger models are very close to each other whereas the filtered table scaled down the most significantly on the smallest model (EP-20), which was in fact constructed over a much smaller subset of Europarl corpus, consisting of less than half of the sentences pairs in the other two Europarl subsets.</text>
                  <doc_id>114</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Compared to the larger Europarl subsets, the small data set is expected to produce more errors through training as there is much less relevant data for the machine learning algorithm to correctly extract useful information from.</text>
                  <doc_id>115</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Consequently, there are more noisy entries in this small model, and therefore more entries to be removed.</text>
                  <doc_id>116</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, the filtering is done by exact matching of complete phrases, which presumably happens much less frequently even for correctly paired phrase pairs in the very limited data supplied by the smallest training set.</text>
                  <doc_id>117</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>For the same reason, the distinction between different bridge languages was less clear for this small model.</text>
                  <doc_id>118</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Due to hardware limitation, we are not able to fit the unfiltered phrase tables completely into the memory.</text>
                  <doc_id>119</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Every table was filtered based on the given input so only a small portion of each table was loaded into memory.</text>
                  <doc_id>120</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This may diminish the difference between the original and the filtered table to a certain degree.</text>
                  <doc_id>121</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The relative time consumptionnevertheless agrees with the reduction in size: phrase tables from the smallest model showed the most reduction for both loading the models and processing the translations.</text>
                  <doc_id>122</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For loading time, we count the time it takes to start and to load the bilingual phrase-tables plus reordering tables and the monolingual language model into the memory.</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The majority of the loading time for the smallest model, even before filtering, has been used for loading language models and other start-up processes, could not be reduced as much as the reduction on table size.</text>
                  <doc_id>124</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Translation quality</title>
            <text>Bridge EP-20 EP-40 EP-50
Efficiency aside, a translation system should be able to produce useful translation. It is important to verify that the filtering approach does not affect the translation quality of the system. Table 3 show the BLEU scores of each translation acquired in the experiments.
Between translation models of different sizes, there are obvious performance gaps. Different bridge languages can cause different effects on performance. However, the translation qualities from a single model are fairly close to each other. We therefore take it that the effect of the triangulation approach is rather robust across translation models of different sizes.
Time Table Size Model+Bridge T l (s) T t (s) N S P T (byte) S RT (byte)
It is obvious that the best systems are usually NOT from the filtered tables that preserved the most entries from the original. All the filtered models showed some improvement in quality with updated model weights. Mostly around 1.5 BLEU points, the increases ranged from 0.36 to 2.25. Table 4 gives a set of translations from the experiments. The unfiltered baseline system inserted the negative by mistake while all the filtered systems are able to avoid this. It indicates that there are indeed noisy entries affecting translation quality in the original table. We were able to achieve better translations by eliminating noisy entries.
The filtering methods indeed tend to remove entries composed of long phrases. Table 5 lists the average length of phrases in several models. Both source phrases and target phrases are taken into account. The best models have shortest phrases on average. Discarding such entries seems to be necessary. This is consistent with the findings in (Koehn, 2003) that phrases longer than three words improve performance little for training corpora of up to 20 million words.
Quality gains appeared to converge in the results across different bridge languages while the original models became larger. Translations generated using large models filtered with different bridge lan-
Bridge EP-20 EP-40 EP-50
guages are less diverse. Meanwhile, the degradation is less for a larger model. It is reasonable to expect improvements for extremely large models with arbitrary bridge languages. For relatively small models, the selection of bridge languages would be critical for the effect of our approach.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Bridge EP-20 EP-40 EP-50</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Efficiency aside, a translation system should be able to produce useful translation.</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It is important to verify that the filtering approach does not affect the translation quality of the system.</text>
                  <doc_id>127</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Table 3 show the BLEU scores of each translation acquired in the experiments.</text>
                  <doc_id>128</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Between translation models of different sizes, there are obvious performance gaps.</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Different bridge languages can cause different effects on performance.</text>
                  <doc_id>130</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, the translation qualities from a single model are fairly close to each other.</text>
                  <doc_id>131</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We therefore take it that the effect of the triangulation approach is rather robust across translation models of different sizes.</text>
                  <doc_id>132</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Time Table Size Model+Bridge T l (s) T t (s) N S P T (byte) S RT (byte)</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It is obvious that the best systems are usually NOT from the filtered tables that preserved the most entries from the original.</text>
                  <doc_id>134</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>All the filtered models showed some improvement in quality with updated model weights.</text>
                  <doc_id>135</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Mostly around 1.5 BLEU points, the increases ranged from 0.36 to 2.25.</text>
                  <doc_id>136</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Table 4 gives a set of translations from the experiments.</text>
                  <doc_id>137</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The unfiltered baseline system inserted the negative by mistake while all the filtered systems are able to avoid this.</text>
                  <doc_id>138</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>It indicates that there are indeed noisy entries affecting translation quality in the original table.</text>
                  <doc_id>139</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>We were able to achieve better translations by eliminating noisy entries.</text>
                  <doc_id>140</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The filtering methods indeed tend to remove entries composed of long phrases.</text>
                  <doc_id>141</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Table 5 lists the average length of phrases in several models.</text>
                  <doc_id>142</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Both source phrases and target phrases are taken into account.</text>
                  <doc_id>143</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The best models have shortest phrases on average.</text>
                  <doc_id>144</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Discarding such entries seems to be necessary.</text>
                  <doc_id>145</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This is consistent with the findings in (Koehn, 2003) that phrases longer than three words improve performance little for training corpora of up to 20 million words.</text>
                  <doc_id>146</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Quality gains appeared to converge in the results across different bridge languages while the original models became larger.</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Translations generated using large models filtered with different bridge lan-</text>
                  <doc_id>148</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Bridge EP-20 EP-40 EP-50</text>
                  <doc_id>149</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>guages are less diverse.</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Meanwhile, the degradation is less for a larger model.</text>
                  <doc_id>151</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It is reasonable to expect improvements for extremely large models with arbitrary bridge languages.</text>
                  <doc_id>152</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For relatively small models, the selection of bridge languages would be critical for the effect of our approach.</text>
                  <doc_id>153</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>5.3 Language clustering</title>
            <text>To further understand how the triangulated filtering approach worked and why it could work as it did, we examined a randomly selected phrase table fragment through the experiments. The segment included 10 potential English translations of the same Spanish word &#8220;fabricantes&#8221;, the plural form of the word &#8220;fabricante&#8221; (manufacturer).
Table 6 shows the filtering results on a randomly selected segment from the original &#8220;EP-40&#8221; model, including 10 English translations of the same source
source
ref
baseline
pt
fr
da
de
fi
As&#237;, se van modificando poco a poco los principios habituales del Estado de derecho por influencia de una concepcin extremista de la lucha con tra las discriminaciones.. thus , the usual principles of the rule of law are being gradually altered under the influence of an extremist approach to combating discrimination. we are not changing the usual principles of the rule of law from the influence of an extremist approach in the fight against discrimination. so , are gradually changing normal principles of the rule of law by influence of an extremist conception of the fight against discrimination. so , we are gradually changing the usual principles of the rule of law by influence of an extremist conception of the fight against discrimination. so , are gradually changing the usual principles of the rule of law by influence of an extremist conception of the fight against discrimination. thus , we are gradually altering the usual principles of the rule of law by influence of an extremist conception of the fight against discrimination. so , are gradually changing normal principles of the rule of law by influence of an extremist conception of the fight against discrimination.
fabricantes pt fr da de fi
word &#8220;fabricantes&#8221;. &#780;&#10003; indicates that the corresponding English phrase remained in the table after triangulated filtering with the corresponding bridge language. We also counted the number of tables that included each phrase pair.
Regardless of the bridge language, the triangulated filtering approach had removed those entries that are clearly noise. Meanwhile, entries which are surely correct were always preserved in the filtered tables. The results of using different bridge languages turned out to be consistent on these extreme cases. The 5 filtering processes agreed on six out of ten pairs.
As for the other 4 pairs, the decisions were different using different bridge languages. The remaining entries were always different when the bridge was changed. None of the languages led to the identical eliminations. None of the cases excludes all errors. Apparently, the selection of bridge languages had immediate effects on the filtering results.
BLEU (%)
32.8
32.6
32.4
32.2
31.8
31.6
31.4
31.2
31 4 6 8 10 12 14 16 18 20 Phrase-table Entries (Mil.)
Portugese French Danish German Finnish Baseline
We compared two factors of these filtered tables: their sizes and the corresponding BLEU scores. Figure 2 shows interesting signs of language similarity/dissimilarity. There are apparently two groups of languages having extremely close performance, which happen to fall in two language groups: Germanic (German and Danish) and Romance (French and Portuguese). The Romance group was associated with larger filtered tables that produced slightly better translations. The filtered tables created with Germanic bridge languages contained ap-
proximately 2 million entries less than Romance groups. The translation quality difference between these two groups was within 1 point of BLEU.
Observed from this figure, it seems that the translation quality was connected to the similarity between the bridge language and the source language. The closer the bridge is to the source language, the better translations it may produce. For instance, Portuguese led to a filtered table that produced the best translations. On the other hand, the more different the bridge languages compared to the source, the larger portion of the phrase-table the filtering algorithm will remove. The table filtered with German was the smallest in the four cases.
Finnish, a language that is unrelated to others, was associated with distinctive results. The size of the table filtered with Finnish is only 23% of the original, almost half of the table generated with Portuguese. Finnish has extremely rich morphology, hence a great many word-forms, which would make exact matching in bridge models less likely to happen. Many more phrase pairs in the original table were removed for this reason even though some of these entries were beneficial for translations. Even though the improvement on translation quality due to the Finnish bridge was less significant than the others, it is clear that triangulated filtering retained the useful information from the original model.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>To further understand how the triangulated filtering approach worked and why it could work as it did, we examined a randomly selected phrase table fragment through the experiments.</text>
                  <doc_id>154</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The segment included 10 potential English translations of the same Spanish word &#8220;fabricantes&#8221;, the plural form of the word &#8220;fabricante&#8221; (manufacturer).</text>
                  <doc_id>155</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 6 shows the filtering results on a randomly selected segment from the original &#8220;EP-40&#8221; model, including 10 English translations of the same source</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>source</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>ref</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>baseline</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>pt</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>fr</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>da</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>de</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>fi</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As&#237;, se van modificando poco a poco los principios habituales del Estado de derecho por influencia de una concepcin extremista de la lucha con tra las discriminaciones.. thus , the usual principles of the rule of law are being gradually altered under the influence of an extremist approach to combating discrimination.</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>we are not changing the usual principles of the rule of law from the influence of an extremist approach in the fight against discrimination.</text>
                  <doc_id>166</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>so , are gradually changing normal principles of the rule of law by influence of an extremist conception of the fight against discrimination.</text>
                  <doc_id>167</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>so , we are gradually changing the usual principles of the rule of law by influence of an extremist conception of the fight against discrimination.</text>
                  <doc_id>168</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>so , are gradually changing the usual principles of the rule of law by influence of an extremist conception of the fight against discrimination.</text>
                  <doc_id>169</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>thus , we are gradually altering the usual principles of the rule of law by influence of an extremist conception of the fight against discrimination.</text>
                  <doc_id>170</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>so , are gradually changing normal principles of the rule of law by influence of an extremist conception of the fight against discrimination.</text>
                  <doc_id>171</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>fabricantes pt fr da de fi</text>
                  <doc_id>172</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>word &#8220;fabricantes&#8221;.</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>&#780;&#10003; indicates that the corresponding English phrase remained in the table after triangulated filtering with the corresponding bridge language.</text>
                  <doc_id>174</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We also counted the number of tables that included each phrase pair.</text>
                  <doc_id>175</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Regardless of the bridge language, the triangulated filtering approach had removed those entries that are clearly noise.</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Meanwhile, entries which are surely correct were always preserved in the filtered tables.</text>
                  <doc_id>177</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The results of using different bridge languages turned out to be consistent on these extreme cases.</text>
                  <doc_id>178</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The 5 filtering processes agreed on six out of ten pairs.</text>
                  <doc_id>179</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As for the other 4 pairs, the decisions were different using different bridge languages.</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The remaining entries were always different when the bridge was changed.</text>
                  <doc_id>181</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>None of the languages led to the identical eliminations.</text>
                  <doc_id>182</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>None of the cases excludes all errors.</text>
                  <doc_id>183</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Apparently, the selection of bridge languages had immediate effects on the filtering results.</text>
                  <doc_id>184</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BLEU (%)</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>32.8</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>32.6</text>
                  <doc_id>187</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>32.4</text>
                  <doc_id>188</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>32.2</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>31.8</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>31.6</text>
                  <doc_id>191</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>31.4</text>
                  <doc_id>192</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>31.2</text>
                  <doc_id>193</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>31 4 6 8 10 12 14 16 18 20 Phrase-table Entries (Mil.</text>
                  <doc_id>194</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>)</text>
                  <doc_id>195</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Portugese French Danish German Finnish Baseline</text>
                  <doc_id>196</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We compared two factors of these filtered tables: their sizes and the corresponding BLEU scores.</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 2 shows interesting signs of language similarity/dissimilarity.</text>
                  <doc_id>198</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>There are apparently two groups of languages having extremely close performance, which happen to fall in two language groups: Germanic (German and Danish) and Romance (French and Portuguese).</text>
                  <doc_id>199</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The Romance group was associated with larger filtered tables that produced slightly better translations.</text>
                  <doc_id>200</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The filtered tables created with Germanic bridge languages contained ap-</text>
                  <doc_id>201</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>proximately 2 million entries less than Romance groups.</text>
                  <doc_id>202</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The translation quality difference between these two groups was within 1 point of BLEU.</text>
                  <doc_id>203</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Observed from this figure, it seems that the translation quality was connected to the similarity between the bridge language and the source language.</text>
                  <doc_id>204</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The closer the bridge is to the source language, the better translations it may produce.</text>
                  <doc_id>205</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For instance, Portuguese led to a filtered table that produced the best translations.</text>
                  <doc_id>206</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>On the other hand, the more different the bridge languages compared to the source, the larger portion of the phrase-table the filtering algorithm will remove.</text>
                  <doc_id>207</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The table filtered with German was the smallest in the four cases.</text>
                  <doc_id>208</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Finnish, a language that is unrelated to others, was associated with distinctive results.</text>
                  <doc_id>209</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The size of the table filtered with Finnish is only 23% of the original, almost half of the table generated with Portuguese.</text>
                  <doc_id>210</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Finnish has extremely rich morphology, hence a great many word-forms, which would make exact matching in bridge models less likely to happen.</text>
                  <doc_id>211</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Many more phrase pairs in the original table were removed for this reason even though some of these entries were beneficial for translations.</text>
                  <doc_id>212</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Even though the improvement on translation quality due to the Finnish bridge was less significant than the others, it is clear that triangulated filtering retained the useful information from the original model.</text>
                  <doc_id>213</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>5.4 Further filtering</title>
            <text>The filtering decision with a bridge language on a particular phrase pair is fixed: either to keep the entry or to discard it. It is difficult to adjust the system to work differently. However, as the triangulated filtering procedure does not consider probability distributions in the models, it is possible to further filter the tables according to the probabilities.
The phrase pairs are associated with values computed from the given set of feature weights and sorted, so that we can remove any portions of the remain entries based on the values. Each generated table is used to translate the test set again. Figure 3 shows BLEU scores of the translation outputs produced with tables derived from the &#8220;EP-50&#8221; model with respect to their sizes. We also included the curve of probability-based filtering alone as the baseline.
The difference between filtered tables at the same
BLEU (%) 32
0 10 20 30 40 50 Phrase-table Entries (Mil.)
Baseline Portugese
French Danish German Finnish
size can be over 6 BLEU points, which is a remarkable advantage for the triangulated filtering approach always producing better translations. The curves of the triangulated filtered models are clearly much steeper than that of the naive pruned ones. Data in these filtered models are more compact than the original model before any filtering. The triangulated filtered phrase-tables contain more useful information than a normal phrase-table of the same size. The curves representing the triangulated filtering performance are always on the left of the original curves.
We are able to use less than 6% of the original phrase table (40% of the table filtered with Finnish) to obtain translations with the same quality as the original. The extreme case, using only 1.4% of the original table, leads to a reasonable BLEU score, indicating that most of the output sentences should still be understandable. In this case, the overall size of the phrase table and the reordering table was less than 100 megabytes, potentially feasible for mobile devices, whereas the original models took nearly 12.5 gigabytes of disk space.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The filtering decision with a bridge language on a particular phrase pair is fixed: either to keep the entry or to discard it.</text>
                  <doc_id>214</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It is difficult to adjust the system to work differently.</text>
                  <doc_id>215</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, as the triangulated filtering procedure does not consider probability distributions in the models, it is possible to further filter the tables according to the probabilities.</text>
                  <doc_id>216</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The phrase pairs are associated with values computed from the given set of feature weights and sorted, so that we can remove any portions of the remain entries based on the values.</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Each generated table is used to translate the test set again.</text>
                  <doc_id>218</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 3 shows BLEU scores of the translation outputs produced with tables derived from the &#8220;EP-50&#8221; model with respect to their sizes.</text>
                  <doc_id>219</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We also included the curve of probability-based filtering alone as the baseline.</text>
                  <doc_id>220</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The difference between filtered tables at the same</text>
                  <doc_id>221</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BLEU (%) 32</text>
                  <doc_id>222</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0 10 20 30 40 50 Phrase-table Entries (Mil.</text>
                  <doc_id>223</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>)</text>
                  <doc_id>224</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Baseline Portugese</text>
                  <doc_id>225</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>French Danish German Finnish</text>
                  <doc_id>226</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>size can be over 6 BLEU points, which is a remarkable advantage for the triangulated filtering approach always producing better translations.</text>
                  <doc_id>227</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The curves of the triangulated filtered models are clearly much steeper than that of the naive pruned ones.</text>
                  <doc_id>228</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Data in these filtered models are more compact than the original model before any filtering.</text>
                  <doc_id>229</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The triangulated filtered phrase-tables contain more useful information than a normal phrase-table of the same size.</text>
                  <doc_id>230</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The curves representing the triangulated filtering performance are always on the left of the original curves.</text>
                  <doc_id>231</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We are able to use less than 6% of the original phrase table (40% of the table filtered with Finnish) to obtain translations with the same quality as the original.</text>
                  <doc_id>232</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The extreme case, using only 1.4% of the original table, leads to a reasonable BLEU score, indicating that most of the output sentences should still be understandable.</text>
                  <doc_id>233</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In this case, the overall size of the phrase table and the reordering table was less than 100 megabytes, potentially feasible for mobile devices, whereas the original models took nearly 12.5 gigabytes of disk space.</text>
                  <doc_id>234</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>4</index>
            <title>5.5 Different source language Table 7:</title>
            <text>BLEU)
Bridge EP-40 EP-50
Filtered German-English systems (Size and
In addition to Spanish-English translation, we also conducted experiments on German-English translation. The results, shown in Table 7, appear consistent with the results of Spanish-English translation. Translations in most cases have performance close to the original unfiltered models, whereas the reduction in phrase-table size ranged from 40% to 85%. Meanwhile, translation speed has been increased up to 17%.
Due to German&#8217;s rich morphology, the unfiltered German-English models contain many more entries than the Spanish-English ones constructed from similar data sets. Unlike the Spanish-English models, the difference between &#8220;EP-40&#8221; and &#8220;EP- 50&#8221; was not significant. Neither was the difference between the impacts of the filtering in terms of translation quality. In addition, German and English are so dissimilar that none of the three bridge languages we chose turned out to be significantly superior.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>BLEU)</text>
                  <doc_id>235</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Bridge EP-40 EP-50</text>
                  <doc_id>236</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Filtered German-English systems (Size and</text>
                  <doc_id>237</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In addition to Spanish-English translation, we also conducted experiments on German-English translation.</text>
                  <doc_id>238</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The results, shown in Table 7, appear consistent with the results of Spanish-English translation.</text>
                  <doc_id>239</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Translations in most cases have performance close to the original unfiltered models, whereas the reduction in phrase-table size ranged from 40% to 85%.</text>
                  <doc_id>240</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Meanwhile, translation speed has been increased up to 17%.</text>
                  <doc_id>241</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Due to German&#8217;s rich morphology, the unfiltered German-English models contain many more entries than the Spanish-English ones constructed from similar data sets.</text>
                  <doc_id>242</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Unlike the Spanish-English models, the difference between &#8220;EP-40&#8221; and &#8220;EP- 50&#8221; was not significant.</text>
                  <doc_id>243</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Neither was the difference between the impacts of the filtering in terms of translation quality.</text>
                  <doc_id>244</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, German and English are so dissimilar that none of the three bridge languages we chose turned out to be significantly superior.</text>
                  <doc_id>245</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>6</index>
        <title>6 Conclusions</title>
        <text>We highlighted one problem of the state-of-the-art SMT systems that was generally neglected: the noise in the translation models. Accordingly, we proposed triangulated filtering methods to deal with this problem. We used data in a third language as evidence to locate the less probable items in the translation models so as to obtain the intersection of information extracted from multilingual data. Only the occurrences of complete phrases were taken into account. The probability distributions of the phrases have not been considered so far. Although the approach was fairly naive, our experiments showed it to be effective. The approaches were applied to SMT systems built with the Moses toolkit. The translation quality was improved at least 1 BLEU for all 15 cases (filtering 3 different models with 5 bridge languages). The improvement can be as much as 2.25 BLEU. It is also clear that the best translations were not linked to the largest translation models. We also sketched a simple extension to the triangulated filtering approach to further reduce the model size, which allows us to generate reasonable results with only 1.4% of the entries from the original table. The results varied for different bridge languages as well as different models. For translation from Spanish to English, Finnish, the most distinctive bridge language, appeared to be a more effective intermediate language which could remove more phrase pair entries while still improving the translation quality. Portuguese, the most close to the source language, always leads to a filtered model that produces the best translations. The selection of bridge languages has more obvious impact on the performance of our approach when the size of the model to filter was larger. The work gave one instance of the general approach described in Section 3. There are several potential directions for continuing this work. The most straightforward one is to use our approaches with more different languages, such as Chinese and Arabic, and incompatible corpora, for example, different segments of Europarl. The main focus of such experiments should be verifying the conclusions we had in this paper.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We highlighted one problem of the state-of-the-art SMT systems that was generally neglected: the noise in the translation models.</text>
              <doc_id>246</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Accordingly, we proposed triangulated filtering methods to deal with this problem.</text>
              <doc_id>247</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We used data in a third language as evidence to locate the less probable items in the translation models so as to obtain the intersection of information extracted from multilingual data.</text>
              <doc_id>248</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Only the occurrences of complete phrases were taken into account.</text>
              <doc_id>249</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The probability distributions of the phrases have not been considered so far.</text>
              <doc_id>250</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Although the approach was fairly naive, our experiments showed it to be effective.</text>
              <doc_id>251</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The approaches were applied to SMT systems built with the Moses toolkit.</text>
              <doc_id>252</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The translation quality was improved at least 1 BLEU for all 15 cases (filtering 3 different models with 5 bridge languages).</text>
              <doc_id>253</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>The improvement can be as much as 2.25 BLEU.</text>
              <doc_id>254</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>It is also clear that the best translations were not linked to the largest translation models.</text>
              <doc_id>255</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>We also sketched a simple extension to the triangulated filtering approach to further reduce the model size, which allows us to generate reasonable results with only 1.4% of the entries from the original table.</text>
              <doc_id>256</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>The results varied for different bridge languages as well as different models.</text>
              <doc_id>257</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>For translation from Spanish to English, Finnish, the most distinctive bridge language, appeared to be a more effective intermediate language which could remove more phrase pair entries while still improving the translation quality.</text>
              <doc_id>258</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>Portuguese, the most close to the source language, always leads to a filtered model that produces the best translations.</text>
              <doc_id>259</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>The selection of bridge languages has more obvious impact on the performance of our approach when the size of the model to filter was larger.</text>
              <doc_id>260</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>The work gave one instance of the general approach described in Section 3.</text>
              <doc_id>261</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>There are several potential directions for continuing this work.</text>
              <doc_id>262</doc_id>
              <sec_id>16</sec_id>
            </sentence>
            <sentence>
              <text>The most straightforward one is to use our approaches with more different languages, such as Chinese and Arabic, and incompatible corpora, for example, different segments of Europarl.</text>
              <doc_id>263</doc_id>
              <sec_id>17</sec_id>
            </sentence>
            <sentence>
              <text>The main focus of such experiments should be verifying the conclusions we had in this paper.</text>
              <doc_id>264</doc_id>
              <sec_id>18</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>7</index>
        <title>Acknowledgments</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>265</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Europarl subsets for building the Spanish- English SMT system</caption>
        <reference_text>In PAGE 3: ...Table 1: Europarl subsets for building the Spanish- English SMT system we used in the experiments are presented by  EP- 20 ,  EP-40  and  EP-50 , in which the numbers indicate the maximal sentence length in respective Europarl subsets.  Table1  lists the characteristics of the Spanish-English subsets. Although the max- imal sentence length in these sets is far less than that of the whole corpus (880 tokens), EP-50 al- ready includes nearly 85% of Spanish-English sen- tence pairs from Europarl....</reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>to various maximal sentence lengths.</cell>
              <cell>to various maximal sentence lengths.</cell>
              <cell>to various maximal sentence lengths.</cell>
              <cell>to various maximal sentence lengths. The subsets</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>Tokens</cell>
              <cell>Tokens</cell>
            </row>
            <row>
              <cell>Model</cell>
              <cell>Sentences</cell>
              <cell>Spanish</cell>
              <cell>English</cell>
            </row>
            <row>
              <cell>EP-20</cell>
              <cell>410,487</cell>
              <cell>5,220,142</cell>
              <cell>5,181,452</cell>
            </row>
            <row>
              <cell>EP-40</cell>
              <cell>964,687</cell>
              <cell>20,820,067</cell>
              <cell>20,229,833</cell>
            </row>
            <row>
              <cell>EP-50</cell>
              <cell>1,100,813</cell>
              <cell>26,731,269</cell>
              <cell>25,867,370</cell>
            </row>
            <row>
              <cell>Europarl</cell>
              <cell>1,304,116</cell>
              <cell>37,870,751</cell>
              <cell>36,429,274</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: System efficiency: time consumption and phrase-table size</caption>
        <reference_text></reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>EP-50+</cell>
              <cell>&#8212;</cell>
              <cell>140</cell>
              <cell>4130</cell>
              <cell>54,382,715</cell>
              <cell>7.1G</cell>
              <cell>5.4G</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>EP-50+</cell>
              <cell>pt</cell>
              <cell>78</cell>
              <cell>3410</cell>
              <cell>13,225,654 (24.32%)</cell>
              <cell>1.6G</cell>
              <cell>1.3G</cell>
            </row>
            <row>
              <cell>EP-50+</cell>
              <cell>fr</cell>
              <cell>97</cell>
              <cell>3616</cell>
              <cell>24,057,849 (44.24%)</cell>
              <cell>3.0G</cell>
              <cell>2.3G</cell>
            </row>
            <row>
              <cell>EP-50+</cell>
              <cell>da</cell>
              <cell>81</cell>
              <cell>3418</cell>
              <cell>12,547,839 (23.07%)</cell>
              <cell>1.5G</cell>
              <cell>1.2G</cell>
            </row>
            <row>
              <cell>EP-50+</cell>
              <cell>de</cell>
              <cell>95</cell>
              <cell>3488</cell>
              <cell>15,938,151 (29.31%)</cell>
              <cell>1.9G</cell>
              <cell>1.5G</cell>
            </row>
            <row>
              <cell>EP-50+</cell>
              <cell>fi</cell>
              <cell>71</cell>
              <cell>3191</cell>
              <cell>7,691,904 (17.75%)</cell>
              <cell>895M</cell>
              <cell>677M</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TET</source>
        <caption>Table 3: BLEU scores of translations using filtered phrase tables</caption>
        <reference_text></reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>&#8212;</cell>
              <cell>26.62</cell>
              <cell>31.43</cell>
              <cell>31.68</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>pt</cell>
              <cell>28.40</cell>
              <cell>32.90</cell>
              <cell>33.93</cell>
            </row>
            <row>
              <cell>fr</cell>
              <cell>28.28</cell>
              <cell>32.69</cell>
              <cell>33.47</cell>
            </row>
            <row>
              <cell>da</cell>
              <cell>28.48</cell>
              <cell>32.47</cell>
              <cell>33.88</cell>
            </row>
            <row>
              <cell>de</cell>
              <cell>28.05</cell>
              <cell>32.65</cell>
              <cell>33.13</cell>
            </row>
            <row>
              <cell>fi</cell>
              <cell>28.02</cell>
              <cell>31.91</cell>
              <cell>33.04</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: Examples</caption>
        <reference_text>In PAGE 5: ...ncreases ranged from 0.36 to 2.25.  Table4  gives a set of translations from the experiments. The unfil- tered baseline system inserted the negative by mis- take while all the filtered systems are able to avoid this....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>fabricantes  a manufacturer  battalions  car manufacturers have</cell>
              <cell>pt   check   check</cell>
              <cell>fr   check   check</cell>
              <cell>da   check   check</cell>
              <cell>de</cell>
              <cell>fi   check</cell>
              <cell>4    3    0</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>car manufacturers</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>5</cell>
            </row>
            <row>
              <cell>makers</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>check</cell>
              <cell>3</cell>
            </row>
            <row>
              <cell>manufacturer</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>5</cell>
            </row>
            <row>
              <cell>manufacturers</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>5</cell>
            </row>
            <row>
              <cell>producers are</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>None</cell>
              <cell>3</cell>
            </row>
            <row>
              <cell>producers need</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>0</cell>
            </row>
            <row>
              <cell>producers</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>check</cell>
              <cell>5</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TableSeer</source>
        <caption>Table 5: Average phrase length</caption>
        <reference_text>In PAGE 5: ... The filtering methods indeed tend to remove en- tries composed of long phrases.  Table5  lists the average length of phrases in several models. Both source phrases and target phrases are taken into ac- count....</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>Bridge</cell>
              <cell>EP-20</cell>
              <cell>EP-40</cell>
              <cell>EP-50</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>?</cell>
              <cell>3.776</cell>
              <cell>4.242</cell>
              <cell>4.335</cell>
            </row>
            <row>
              <cell>pt</cell>
              <cell>3.195</cell>
              <cell>3.943</cell>
              <cell>3.740</cell>
            </row>
            <row>
              <cell>fr</cell>
              <cell>3.003</cell>
              <cell>3.809</cell>
              <cell>3.947</cell>
            </row>
            <row>
              <cell>da</cell>
              <cell>3.005</cell>
              <cell>3.74</cell>
              <cell>3.453</cell>
            </row>
            <row>
              <cell>de</cell>
              <cell>2.535</cell>
              <cell>3.501</cell>
              <cell>3.617</cell>
            </row>
            <row>
              <cell>fi</cell>
              <cell>2.893</cell>
              <cell>3.521</cell>
              <cell>3.262</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>6</id>
        <source>TET</source>
        <caption>Table 6: Phrase-table entries before and after filtering a model with different bridges</caption>
        <reference_text></reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>a manufacturer</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell></cell>
              <cell>&#780;&#10003;</cell>
              <cell>4</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>battalions</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell></cell>
              <cell></cell>
              <cell>3</cell>
            </row>
            <row>
              <cell>car manufacturers have</cell>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell>0</cell>
            </row>
            <row>
              <cell>car manufacturers</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>5</cell>
            </row>
            <row>
              <cell>makers</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell></cell>
              <cell></cell>
              <cell>&#780;&#10003;</cell>
              <cell>3</cell>
            </row>
            <row>
              <cell>manufacturer</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>5</cell>
            </row>
            <row>
              <cell>manufacturers</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>5</cell>
            </row>
            <row>
              <cell>producers are</cell>
              <cell></cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell></cell>
              <cell>3</cell>
            </row>
            <row>
              <cell>producers need</cell>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell>0</cell>
            </row>
            <row>
              <cell>producers</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>&#780;&#10003;</cell>
              <cell>5</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>7</id>
        <source>TableSeer</source>
        <caption>Table 7: Filtered German-English systems (Size and BLEU)</caption>
        <reference_text>In PAGE 8: ...also conducted experiments on German-English translation. The results, shown in  Table7 , appear consistent with the results of Spanish-English trans- lation. Translations in most cases have performance close to the original unfiltered models, whereas the reduction in phrase-table size ranged from 40% to 85%....</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>12.5 gigabytes of disk space.  Different source language</cell>
              <cell>12.5 gigabytes of disk space.   Different source language</cell>
              <cell>12.5 gigabytes of disk space.   Different source language</cell>
              <cell>Different source language</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Bridge</cell>
              <cell>EP-40</cell>
              <cell>EP-40</cell>
              <cell>EP-50</cell>
              <cell>EP-50</cell>
            </row>
            <row>
              <cell>?</cell>
              <cell>5.1G</cell>
              <cell>26.92</cell>
              <cell>6.5G</cell>
              <cell>27.23</cell>
            </row>
            <row>
              <cell>Dutch</cell>
              <cell>562M</cell>
              <cell>27.11</cell>
              <cell>1.3G</cell>
              <cell>28.14</cell>
            </row>
            <row>
              <cell>Spanish</cell>
              <cell>3.0G</cell>
              <cell>27.28</cell>
              <cell>3.6G</cell>
              <cell>28.09</cell>
            </row>
            <row>
              <cell>Danish</cell>
              <cell>505M</cell>
              <cell>28.04</cell>
              <cell>780M</cell>
              <cell>28.21</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Yu Chen</author>
          <author>Andreas Eisele</author>
          <author>Martin Kay</author>
        </authors>
        <title>Improving Statistical Machine Translation Efficiency by Triangulation.</title>
        <publication>In the 6th International Conference on Language Resources and Evaluation (LREC &#8217;08),</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Trevor Cohn</author>
          <author>Mirella Lapata</author>
        </authors>
        <title>Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora.</title>
        <publication>In the 45th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Karim Filali</author>
          <author>Jeff Bilmes</author>
        </authors>
        <title>Leveraging Multiple Languages to Improve Statistical MT Word Alignments.</title>
        <publication>In IEEE Automatic Speech Recognition and Understanding (ASRU),</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>J Howard Johnson</author>
          <author>Joel Martin</author>
          <author>George Foster</author>
          <author>Roland Kuhn</author>
        </authors>
        <title>Improving Translation Quality by Discarding Most of the Phrasetable.</title>
        <publication>In the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural 135 Learning (EMNLP-CoNLL),</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Martin Kay</author>
        </authors>
        <title>The proper place of men and machines in language translation.</title>
        <publication>None</publication>
        <pages>12--1</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Hieu Hoang</author>
          <author>Alexandra Birch</author>
          <author>Chris Callison-Burch</author>
          <author>Marcello Federico</author>
          <author>Nicola Bertoldi</author>
          <author>Brooke Cowan</author>
          <author>Wade Shen</author>
          <author>Christine Moran</author>
          <author>Richard Zens</author>
        </authors>
        <title>Chris Dyer, Ondrej Bojar,</title>
        <publication>In the 45th Annual Meeting of the Association for Computational Linguistics (ACL),</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Philipp Koehn</author>
        </authors>
        <title>Noun Phrase Translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Philipp Koehn</author>
        </authors>
        <title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
        <publication>In MT Summit</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Shankar Kumar</author>
          <author>Franz Josef Och</author>
          <author>Wolfgang Macherey</author>
        </authors>
        <title>Improving word alignment with bridge languages.</title>
        <publication>In the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</publication>
        <pages>42--50</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Statistical multi-source translation.</title>
        <publication>In MT Summit VIII, Santiago de Compostela,</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>In ACL &#8217;03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</publication>
        <pages>160--167</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>BLEU: a method for automatic evaluation of machine translation.</title>
        <publication>In the 40th Annual Meeting on Association for Computational Linguistics,</publication>
        <pages>311--318</pages>
        <date>2001</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Chen et al., 2008</string>
        <sentence_id>22229</sentence_id>
        <char_offset>66</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Cohn and Lapata, 2007</string>
        <sentence_id>22232</sentence_id>
        <char_offset>67</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Filali and Bilmes, 2005</string>
        <sentence_id>22232</sentence_id>
        <char_offset>90</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>3</reference_id>
        <string>Johnson et al., 2007</string>
        <sentence_id>22229</sentence_id>
        <char_offset>39</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>4</reference_id>
        <string>Kay, 1997</string>
        <sentence_id>22231</sentence_id>
        <char_offset>53</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>5</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>22309</sentence_id>
        <char_offset>152</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>6</reference_id>
        <string>Koehn, 2003</string>
        <sentence_id>22360</sentence_id>
        <char_offset>41</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>7</reference_id>
        <string>Koehn, 2005</string>
        <sentence_id>22296</sentence_id>
        <char_offset>144</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>8</reference_id>
        <string>Kumar et al., 2007</string>
        <sentence_id>22232</sentence_id>
        <char_offset>47</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>9</reference_id>
        <string>Och and Ney, 2001</string>
        <sentence_id>22232</sentence_id>
        <char_offset>129</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>10</reference_id>
        <string>Och, 2003</string>
        <sentence_id>22293</sentence_id>
        <char_offset>228</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>11</reference_id>
        <string>Papineni et al., 2001</string>
        <sentence_id>22310</sentence_id>
        <char_offset>148</char_offset>
      </citation>
    </citations>
  </content>
</document>
