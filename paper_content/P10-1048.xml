<document>
  <filename>P10-1048</filename>
  <authors/>
  <title>Hindi-to-Urdu Machine Translation Through Transliteration</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>We present a novel approach to integrate transliteration into Hindi-to-Urdu statistical machine translation. We propose two probabilistic models, based on conditional and joint probability formulations, that are novel solutions to the problem. Our models consider both transliteration and translation when translating a particular Hindi word given the context whereas in previous work transliteration is only used for translating OOV (out-of-vocabulary) words. We use transliteration as a tool for disambiguation of Hindi homonyms which can be both translated or transliterated or transliterated differently based on different contexts. We obtain final BLEU scores of 19.35 (conditional probability model) and 19.00 (joint probability model) as compared to 14.30 for a baseline phrase-based system and 16.25 for a system which transliterates OOV words in the baseline system. This indicates that transliteration is useful for more than only translating OOV words for language pairs like Hindi-Urdu.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We present a novel approach to integrate transliteration into Hindi-to-Urdu statistical machine translation.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We propose two probabilistic models, based on conditional and joint probability formulations, that are novel solutions to the problem.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Our models consider both transliteration and translation when translating a particular Hindi word given the context whereas in previous work transliteration is only used for translating OOV (out-of-vocabulary) words.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We use transliteration as a tool for disambiguation of Hindi homonyms which can be both translated or transliterated or transliterated differently based on different contexts.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We obtain final BLEU scores of 19.35 (conditional probability model) and 19.00 (joint probability model) as compared to 14.30 for a baseline phrase-based system and 16.25 for a system which transliterates OOV words in the baseline system.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This indicates that transliteration is useful for more than only translating OOV words for language pairs like Hindi-Urdu.</text>
              <doc_id>5</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Hindi is an official language of India and is written in Devanagari script. Urdu is the national language of Pakistan, and also one of the state languages in India, and is written in Perso-Arabic script. Hindi inherits its vocabulary from Sanskrit while Urdu descends from several languages including Arabic, Farsi (Persian), Turkish and Sanskrit. Hindi and Urdu share grammatical structure and a large proportion of vocabulary that they both inherited from Sanskrit. Most of the verbs and closed-class words (pronouns, auxiliaries, casemarkers, etc) are the same. Because both languages have lived together for centuries, some Urdu words which originally came from Arabic and Farsi have also mixed into Hindi and are now part of the Hindi vocabulary. The spoken form of the two languages is very similar.
The extent of overlap between Hindi and Urdu vocabulary depends upon the domain of the text. Text coming from the literary domain like novels or history tend to have more Sanskrit (for Hindi) and Persian/Arabic (for Urdu) vocabulary. However, news wire that contains text related to media, sports and politics, etc., is more likely to have common vocabulary.
In an initial study on a small news corpus of 5000 words, randomly selected from BBC 1 News, we found that approximately 62% of the Hindi types are also part of Urdu vocabulary and thus can be transliterated while only 38% have to be translated. This provides a strong motivation to implement an end-to-end translation system which strongly relies on high quality transliteration from Hindi to Urdu.
Hindi and Urdu have similar sound systems but transliteration from Hindi to Urdu is still very hard because some phonemes in Hindi have several orthographic equivalents in Urdu. For example the &#8220;z&#8221; sound 2 can only be written as whenever it occurs in a Hindi word but can be written as , , and in an Urdu word. Transliteration becomes non-trivial in cases where the multiple orthographic equivalents for a Hindi word are all valid Urdu words. Context is required to resolve ambiguity in such cases. Our transliterator (described in sections 3.1.2 and 4.1.3) gives an accuracy of 81.6% and a 25-best accuracy of 92.3%.
Transliteration has been previously used only as a back-off measure to translate NEs (Name Entities) and OOV words in a pre- or post-processing step. The problem we are solving is more difficult than techniques aimed at handling OOV words,
1 http://www.bbc.co.uk/hindi/index.shtml 2 All sounds are represented using SAMPA notation.
Hindi Urdu SAMPA Gloss
/ Am Mango/Ordinary
/ d ZAli Fake/Net
/ Ser Lion/Verse
Hindi Urdu SAMPA Gloss
/ simA Border/Seema
/ Amb@r Sky/Ambar
/ vId Ze Victory/Vijay
which focus primarily on name transliteration, because we need different transliterations in different contexts; in their case context is irrelevant. For example: consider the problem of transliterating the English word &#8220;read&#8221; to a phoneme representation in the context &#8220;I will read&#8221; versus the context &#8220;I have read&#8221;. An example of this for Hindi to Urdu transliteration: the two Urdu words (face/condition) and (chapter of the Koran) are both written as (sur@t d) in Hindi. The two are pronounced identically in Urdu but written differently. In such cases we hope to choose the correct transliteration by using context. Some other examples are shown in Table 1.
Sometimes there is also an ambiguity of whether to translate or transliterate a particular word. The Hindi word , for example, will be translated to (peace, s@kun) when it is a common noun but transliterated to (Shanti, SAnt di) when it is a proper name. We try to model whether to translate or transliterate in a given situation. Some other examples are shown in Table 2.
The remainder of this paper is organized as follows. Section 2 provides a review of previous work. Section 3 introduces two probabilistic models for integrating translations and transliterations into a translation model which are based on conditional and joint probability distributions. Section 4 discusses the training data, parameter optimization and the initial set of experiments that compare our two models with a baseline Hindi-Urdu phrasebased system and with two transliteration-aided phrase-based systems in terms of BLEU scores
(Papineni et al., 2001). Section 5 performs an error analysis showing interesting weaknesses in the initial formulations. We remedy the problems by adding some heuristics and modifications to our models which show improvements in the results as discussed in section 6. Section 7 gives two examples illustrating how our model decides whether to translate or transliterate and how it is able to choose among different valid transliterations given the context. Section 8 concludes the paper.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Hindi is an official language of India and is written in Devanagari script.</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Urdu is the national language of Pakistan, and also one of the state languages in India, and is written in Perso-Arabic script.</text>
              <doc_id>7</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Hindi inherits its vocabulary from Sanskrit while Urdu descends from several languages including Arabic, Farsi (Persian), Turkish and Sanskrit.</text>
              <doc_id>8</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Hindi and Urdu share grammatical structure and a large proportion of vocabulary that they both inherited from Sanskrit.</text>
              <doc_id>9</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Most of the verbs and closed-class words (pronouns, auxiliaries, casemarkers, etc) are the same.</text>
              <doc_id>10</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Because both languages have lived together for centuries, some Urdu words which originally came from Arabic and Farsi have also mixed into Hindi and are now part of the Hindi vocabulary.</text>
              <doc_id>11</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The spoken form of the two languages is very similar.</text>
              <doc_id>12</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The extent of overlap between Hindi and Urdu vocabulary depends upon the domain of the text.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Text coming from the literary domain like novels or history tend to have more Sanskrit (for Hindi) and Persian/Arabic (for Urdu) vocabulary.</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, news wire that contains text related to media, sports and politics, etc., is more likely to have common vocabulary.</text>
              <doc_id>15</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In an initial study on a small news corpus of 5000 words, randomly selected from BBC 1 News, we found that approximately 62% of the Hindi types are also part of Urdu vocabulary and thus can be transliterated while only 38% have to be translated.</text>
              <doc_id>16</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This provides a strong motivation to implement an end-to-end translation system which strongly relies on high quality transliteration from Hindi to Urdu.</text>
              <doc_id>17</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Hindi and Urdu have similar sound systems but transliteration from Hindi to Urdu is still very hard because some phonemes in Hindi have several orthographic equivalents in Urdu.</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For example the &#8220;z&#8221; sound 2 can only be written as whenever it occurs in a Hindi word but can be written as , , and in an Urdu word.</text>
              <doc_id>19</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Transliteration becomes non-trivial in cases where the multiple orthographic equivalents for a Hindi word are all valid Urdu words.</text>
              <doc_id>20</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Context is required to resolve ambiguity in such cases.</text>
              <doc_id>21</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Our transliterator (described in sections 3.1.2 and 4.1.3) gives an accuracy of 81.6% and a 25-best accuracy of 92.3%.</text>
              <doc_id>22</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Transliteration has been previously used only as a back-off measure to translate NEs (Name Entities) and OOV words in a pre- or post-processing step.</text>
              <doc_id>23</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The problem we are solving is more difficult than techniques aimed at handling OOV words,</text>
              <doc_id>24</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 http://www.bbc.co.uk/hindi/index.shtml 2 All sounds are represented using SAMPA notation.</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Hindi Urdu SAMPA Gloss</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>/ Am Mango/Ordinary</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>/ d ZAli Fake/Net</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>/ Ser Lion/Verse</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Hindi Urdu SAMPA Gloss</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>/ simA Border/Seema</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>/ Amb@r Sky/Ambar</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>/ vId Ze Victory/Vijay</text>
              <doc_id>33</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>which focus primarily on name transliteration, because we need different transliterations in different contexts; in their case context is irrelevant.</text>
              <doc_id>34</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For example: consider the problem of transliterating the English word &#8220;read&#8221; to a phoneme representation in the context &#8220;I will read&#8221; versus the context &#8220;I have read&#8221;.</text>
              <doc_id>35</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>An example of this for Hindi to Urdu transliteration: the two Urdu words (face/condition) and (chapter of the Koran) are both written as (sur@t d) in Hindi.</text>
              <doc_id>36</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The two are pronounced identically in Urdu but written differently.</text>
              <doc_id>37</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In such cases we hope to choose the correct transliteration by using context.</text>
              <doc_id>38</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Some other examples are shown in Table 1.</text>
              <doc_id>39</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Sometimes there is also an ambiguity of whether to translate or transliterate a particular word.</text>
              <doc_id>40</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The Hindi word , for example, will be translated to (peace, s@kun) when it is a common noun but transliterated to (Shanti, SAnt di) when it is a proper name.</text>
              <doc_id>41</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We try to model whether to translate or transliterate in a given situation.</text>
              <doc_id>42</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Some other examples are shown in Table 2.</text>
              <doc_id>43</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The remainder of this paper is organized as follows.</text>
              <doc_id>44</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Section 2 provides a review of previous work.</text>
              <doc_id>45</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Section 3 introduces two probabilistic models for integrating translations and transliterations into a translation model which are based on conditional and joint probability distributions.</text>
              <doc_id>46</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Section 4 discusses the training data, parameter optimization and the initial set of experiments that compare our two models with a baseline Hindi-Urdu phrasebased system and with two transliteration-aided phrase-based systems in terms of BLEU scores</text>
              <doc_id>47</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(Papineni et al., 2001).</text>
              <doc_id>48</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Section 5 performs an error analysis showing interesting weaknesses in the initial formulations.</text>
              <doc_id>49</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We remedy the problems by adding some heuristics and modifications to our models which show improvements in the results as discussed in section 6.</text>
              <doc_id>50</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Section 7 gives two examples illustrating how our model decides whether to translate or transliterate and how it is able to choose among different valid transliterations given the context.</text>
              <doc_id>51</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Section 8 concludes the paper.</text>
              <doc_id>52</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Previous Work</title>
        <text>There has been a significant amount of work on transliteration. We can break down previous work into three groups. The first group is generic transliteration work, which is evaluated outside of the context of translation. This work uses either grapheme or phoneme based models to transliterate words lists (Knight and Graehl, 1998; Li et al., 2004; Ekbal et al., 2006; Malik et al., 2008). The work by Malik et al. addresses Hindi to Urdu transliteration using hand-crafted rules and a phonemic representation; it ignores translation context. A second group deals with out-of-vocabulary words for SMT systems built on large parallel corpora, and therefore focuses on name transliteration, which is largely independent of context. Al- Onaizan and Knight (2002) transliterate Arabic NEs into English and score them against their respective translations using a modified IBM Model 1. The options are further re-ranked based on different measures such as web counts and using coreference to resolve ambiguity. These re-ranking methodologies can not be performed in SMT at the decoding time. An efficient way to compute and re-rank the transliterations of NEs and integrate them on the fly might be possible. However, this is not practical in our case as our model considers transliterations of all input words and not just NEs. A log-linear block transliteration model is applied to OOV NEs in Arabic to English SMT by Zhao et al. (2007). This work is also transliterating only NEs and not doing any disambiguation. The best method proposed by Kashani et al. (2007) integrates translations provided by external sources such as transliteration or rule-base translation of numbers and dates, for an arbitrary number of entries within the input text. Our work is different from Kashani et al. (2007) in that our model compares transliterations with translations
on the fly whereas transliterations in Kashani et al. do not compete with internal phrase tables. They only compete amongst themselves during a second pass of decoding. Hermjakob et al. (2008) use a tagger to identify good candidates for transliteration (which are mostly NEs) in input text and add transliterations to the SMT phrase table dynamically such that they can directly compete with translations during decoding. This is closer to our approach except that we use transliteration as an alternative to translation for all Hindi words. Our focus is disambiguation of Hindi homonyms whereas they are concentrating only on transliterating NE&#8217;s. Moreover, they are working with a large bitext so they can rely on their translation model and only need to transliterate NEs and OOVs. Our translation model is based on data which is both sparse and noisy. Therefore we pit transliterations against translations for every input word. Sinha (2009) presents a rule-based MT system that uses Hindi as a pivot to translate from English to Urdu. This work also uses transliteration only for the translation of unknown words. Their work can not be used for direct translation from Hindi to Urdu (independently of English) &#8220;due to various ambiguous mappings that have to be resolved&#8221;.
The third group uses transliteration models inside of a cross-lingual IR system (AbdulJaleel and Larkey, 2003; Virga and Khudanpur, 2003; Pirkola et al., 2003). Picking a single best transliteration or translation in context is not important in an IR system. Instead, all the options are used by giving them weights and context is typically not taken into account.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>There has been a significant amount of work on transliteration.</text>
              <doc_id>53</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We can break down previous work into three groups.</text>
              <doc_id>54</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The first group is generic transliteration work, which is evaluated outside of the context of translation.</text>
              <doc_id>55</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This work uses either grapheme or phoneme based models to transliterate words lists (Knight and Graehl, 1998; Li et al., 2004; Ekbal et al., 2006; Malik et al., 2008).</text>
              <doc_id>56</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The work by Malik et al. addresses Hindi to Urdu transliteration using hand-crafted rules and a phonemic representation; it ignores translation context.</text>
              <doc_id>57</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>A second group deals with out-of-vocabulary words for SMT systems built on large parallel corpora, and therefore focuses on name transliteration, which is largely independent of context.</text>
              <doc_id>58</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Al- Onaizan and Knight (2002) transliterate Arabic NEs into English and score them against their respective translations using a modified IBM Model 1.</text>
              <doc_id>59</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The options are further re-ranked based on different measures such as web counts and using coreference to resolve ambiguity.</text>
              <doc_id>60</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>These re-ranking methodologies can not be performed in SMT at the decoding time.</text>
              <doc_id>61</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>An efficient way to compute and re-rank the transliterations of NEs and integrate them on the fly might be possible.</text>
              <doc_id>62</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>However, this is not practical in our case as our model considers transliterations of all input words and not just NEs.</text>
              <doc_id>63</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>A log-linear block transliteration model is applied to OOV NEs in Arabic to English SMT by Zhao et al. (2007).</text>
              <doc_id>64</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>This work is also transliterating only NEs and not doing any disambiguation.</text>
              <doc_id>65</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>The best method proposed by Kashani et al. (2007) integrates translations provided by external sources such as transliteration or rule-base translation of numbers and dates, for an arbitrary number of entries within the input text.</text>
              <doc_id>66</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>Our work is different from Kashani et al. (2007) in that our model compares transliterations with translations</text>
              <doc_id>67</doc_id>
              <sec_id>14</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>on the fly whereas transliterations in Kashani et al. do not compete with internal phrase tables.</text>
              <doc_id>68</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>They only compete amongst themselves during a second pass of decoding.</text>
              <doc_id>69</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Hermjakob et al. (2008) use a tagger to identify good candidates for transliteration (which are mostly NEs) in input text and add transliterations to the SMT phrase table dynamically such that they can directly compete with translations during decoding.</text>
              <doc_id>70</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This is closer to our approach except that we use transliteration as an alternative to translation for all Hindi words.</text>
              <doc_id>71</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Our focus is disambiguation of Hindi homonyms whereas they are concentrating only on transliterating NE&#8217;s.</text>
              <doc_id>72</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, they are working with a large bitext so they can rely on their translation model and only need to transliterate NEs and OOVs.</text>
              <doc_id>73</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Our translation model is based on data which is both sparse and noisy.</text>
              <doc_id>74</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Therefore we pit transliterations against translations for every input word.</text>
              <doc_id>75</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Sinha (2009) presents a rule-based MT system that uses Hindi as a pivot to translate from English to Urdu.</text>
              <doc_id>76</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>This work also uses transliteration only for the translation of unknown words.</text>
              <doc_id>77</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Their work can not be used for direct translation from Hindi to Urdu (independently of English) &#8220;due to various ambiguous mappings that have to be resolved&#8221;.</text>
              <doc_id>78</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The third group uses transliteration models inside of a cross-lingual IR system (AbdulJaleel and Larkey, 2003; Virga and Khudanpur, 2003; Pirkola et al., 2003).</text>
              <doc_id>79</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Picking a single best transliteration or translation in context is not important in an IR system.</text>
              <doc_id>80</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Instead, all the options are used by giving them weights and context is typically not taken into account.</text>
              <doc_id>81</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Our Approach</title>
        <text>Both of our models combine a character-based transliteration model with a word-based translation model. Our models look for the most probable Urdu token sequence u n 1 for a given Hindi token sequence h n 1 . We assume that each Hindi token is mapped to exactly one Urdu token and that there is no reordering. The assumption of no reordering is reasonable given the fact that Hindi and Urdu have identical grammar structure and the same word order. An Urdu token might consist of more than one Urdu word 3 . The following sections give a math-
3 This occurs frequently in case markers with nouns,
derivational affixes and compounds etc. These are written as single words in Hindi as opposed to Urdu where they are
ematical formulation of our two models, Model-1 and Model-2.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Both of our models combine a character-based transliteration model with a word-based translation model.</text>
              <doc_id>82</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our models look for the most probable Urdu token sequence u n 1 for a given Hindi token sequence h n 1 .</text>
              <doc_id>83</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We assume that each Hindi token is mapped to exactly one Urdu token and that there is no reordering.</text>
              <doc_id>84</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The assumption of no reordering is reasonable given the fact that Hindi and Urdu have identical grammar structure and the same word order.</text>
              <doc_id>85</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>An Urdu token might consist of more than one Urdu word 3 .</text>
              <doc_id>86</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The following sections give a math-</text>
              <doc_id>87</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3 This occurs frequently in case markers with nouns,</text>
              <doc_id>88</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>derivational affixes and compounds etc.</text>
              <doc_id>89</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>These are written as single words in Hindi as opposed to Urdu where they are</text>
              <doc_id>90</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>ematical formulation of our two models, Model-1 and Model-2.</text>
              <doc_id>91</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Model-1 : Conditional Probability Model</title>
            <text>Applying a noisy channel model to compute the most probable translation &#251; n 1 , we get:
arg max
u n 1
p(u n 1 |h n 1 ) = arg max p(u n
u n 1 )p(h n 1 |u n 1 )
(1)
3.1.1 Language Model The language model (LM) p(u n 1 ) is implemented as an n-gram model using the SRILM-Toolkit (Stolcke, 2002) with Kneser-Ney smoothing. The parameters of the language model are learned from a monolingual Urdu corpus. The language model is defined as:
p(u n 1 ) = n&#8719;
i=1
p LM (u i |u i&#8722;1 i&#8722;k ) (2)
where k is a parameter indicating the amount of context used (e.g., k = 4 means 5-gram model). u i can be a single or a multi-word token. A multi-word token consists of two or more Urdu words. For a multi-word u i we do multiple language model look-ups, one for each u ix in u i = u i1 , . . . , u im and take their product to obtain the value p LM (u i |u i&#8722;1 i&#8722;k ).
Language Model for Unknown Words: Our model generates transliterations that can be known or unknown to the language model and the translation model. We refer to the words known to the language model and to the translation model as LM-known and TM-known words respectively and to words that are unknown as LM-unknown and TM-unknown respectively.
We assign a special value &#968; to the LM-unknown words. If one or more u ix in a multi-word u i are LM-unknown we assign a language model score p LM (u i |u i&#8722;1 i&#8722;k ) = &#968; for the entire u i, meaning that we consider partially known transliterations to be as bad as fully unknown transliterations. The parameter &#968; controls the trade-off between LMknown and LM-unknown transliterations. It does not influence translation options because they are always LM-known in our case. This is because our monolingual corpus also contains the Urdu part of translation corpus. The optimization of &#968; is described in section 4.2.1.
written as two words. For example (beautiful ; xubsur@t d) and (your&#8217;s ; ApkA) are written as
and respectively in Urdu.
3.1.2 Translation Model The translation model (TM) p(h n 1 |un 1 ) is approximated with a context-independent model:
p(h n 1 |u n 1 ) = n&#8719;
p(h i |u i ) (3)
i=1
where h i and u i are Hindi and Urdu tokens respectively. Our model estimates the conditional probability p(h i |u i ) by interpolating a wordbased model and a character-based (transliteration) model.
p(h i |u i ) = &#955;p w (h i |u i ) + (1 &#8722; &#955;)p c (h i |u i ) (4)
The parameters of the word-based translation model p w (h|u) are estimated from the word alignments of a small parallel corpus. We only retain 1-1/1-N (1 Hindi word, 1 or more Urdu words) alignments and throw away N-1 and M-N alignments for our models. This is further discussed in section 4.1.1.
The character-based transliteration model p c (h|u) is computed in terms of p c (h, u), a joint character model, which is also used for Chinese- English back-transliteration (Li et al., 2004) and Bengali-English name transliteration (Ekbal et al., 2006). The character-based transliteration probability is defined as follows: &#8721;
p c (h, u) = p(a n 1 )
a n 1 &#8712;align(h,u)
= &#8721; n&#8719;
a n 1 &#8712;align(h,u) i=1
p(a i |a i&#8722;1 i&#8722;k ) (5)
where a i is a pair consisting of the i-th Hindi character h i and the sequence of 0 or more Urdu characters that it is aligned with. A sample alignment is shown in Table 3(b) in section 4.1.3. Our best results are obtained with a 5-gram model. The parameters p(a i |a i&#8722;1 i&#8722;k ) are estimated from a small transliteration corpus which we automatically extracted from the translation corpus. The extraction details are also discussed in section 4.1.3. Because our overall model is a conditional probability model, joint-probabilities are marginalized using character-based prior probabilities:
p c (h|u) = p c(h, u) p c (u) (6)
The prior probability p c (u) of the character sequence u = c m 1 is defined with a character-based language model:
p c (u) = m&#8719;
i=1
p(c i |c i&#8722;1 i&#8722;k ) (7)
The parameters p(c i |c i&#8722;1 i&#8722;k ) are estimated from the Urdu part of the character-aligned transliteration corpus. Replacing (6) in (4) we get:
p(h i |u i ) = &#955;p w (h i |u i ) + (1 &#8722; &#955;) p c(h i , u i ) p c (u i ) (8)
Having all the components of our model defined we insert (8) and (2) in (1) to obtain the final equation:
&#251; n 1 = arg max
u n 1
n&#8719;
i=1
p LM (u i |u i&#8722;1 i&#8722;k )[&#955;p w(h i |u i )
+ (1 &#8722; &#955;) p c(h i , u i ) p c (u i ) ] (9)
The optimization of the interpolating factor &#955; is discussed in section 4.2.1.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Applying a noisy channel model to compute the most probable translation &#251; n 1 , we get:</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>arg max</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u n 1</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(u n 1 |h n 1 ) = arg max p(u n</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u n 1 )p(h n 1 |u n 1 )</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(1)</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3.1.1 Language Model The language model (LM) p(u n 1 ) is implemented as an n-gram model using the SRILM-Toolkit (Stolcke, 2002) with Kneser-Ney smoothing.</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The parameters of the language model are learned from a monolingual Urdu corpus.</text>
                  <doc_id>99</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The language model is defined as:</text>
                  <doc_id>100</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(u n 1 ) = n&#8719;</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p LM (u i |u i&#8722;1 i&#8722;k ) (2)</text>
                  <doc_id>103</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where k is a parameter indicating the amount of context used (e.g., k = 4 means 5-gram model).</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>u i can be a single or a multi-word token.</text>
                  <doc_id>105</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>A multi-word token consists of two or more Urdu words.</text>
                  <doc_id>106</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For a multi-word u i we do multiple language model look-ups, one for each u ix in u i = u i1 , .</text>
                  <doc_id>107</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>108</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>109</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>, u im and take their product to obtain the value p LM (u i |u i&#8722;1 i&#8722;k ).</text>
                  <doc_id>110</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Language Model for Unknown Words: Our model generates transliterations that can be known or unknown to the language model and the translation model.</text>
                  <doc_id>111</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We refer to the words known to the language model and to the translation model as LM-known and TM-known words respectively and to words that are unknown as LM-unknown and TM-unknown respectively.</text>
                  <doc_id>112</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We assign a special value &#968; to the LM-unknown words.</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If one or more u ix in a multi-word u i are LM-unknown we assign a language model score p LM (u i |u i&#8722;1 i&#8722;k ) = &#968; for the entire u i, meaning that we consider partially known transliterations to be as bad as fully unknown transliterations.</text>
                  <doc_id>114</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The parameter &#968; controls the trade-off between LMknown and LM-unknown transliterations.</text>
                  <doc_id>115</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>It does not influence translation options because they are always LM-known in our case.</text>
                  <doc_id>116</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This is because our monolingual corpus also contains the Urdu part of translation corpus.</text>
                  <doc_id>117</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The optimization of &#968; is described in section 4.2.1.</text>
                  <doc_id>118</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>written as two words.</text>
                  <doc_id>119</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For example (beautiful ; xubsur@t d) and (your&#8217;s ; ApkA) are written as</text>
                  <doc_id>120</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and respectively in Urdu.</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3.1.2 Translation Model The translation model (TM) p(h n 1 |un 1 ) is approximated with a context-independent model:</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(h n 1 |u n 1 ) = n&#8719;</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(h i |u i ) (3)</text>
                  <doc_id>124</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where h i and u i are Hindi and Urdu tokens respectively.</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Our model estimates the conditional probability p(h i |u i ) by interpolating a wordbased model and a character-based (transliteration) model.</text>
                  <doc_id>127</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(h i |u i ) = &#955;p w (h i |u i ) + (1 &#8722; &#955;)p c (h i |u i ) (4)</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The parameters of the word-based translation model p w (h|u) are estimated from the word alignments of a small parallel corpus.</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We only retain 1-1/1-N (1 Hindi word, 1 or more Urdu words) alignments and throw away N-1 and M-N alignments for our models.</text>
                  <doc_id>130</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This is further discussed in section 4.1.1.</text>
                  <doc_id>131</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The character-based transliteration model p c (h|u) is computed in terms of p c (h, u), a joint character model, which is also used for Chinese- English back-transliteration (Li et al., 2004) and Bengali-English name transliteration (Ekbal et al., 2006).</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The character-based transliteration probability is defined as follows: &#8721;</text>
                  <doc_id>133</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p c (h, u) = p(a n 1 )</text>
                  <doc_id>134</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>a n 1 &#8712;align(h,u)</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>= &#8721; n&#8719;</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>a n 1 &#8712;align(h,u) i=1</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(a i |a i&#8722;1 i&#8722;k ) (5)</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where a i is a pair consisting of the i-th Hindi character h i and the sequence of 0 or more Urdu characters that it is aligned with.</text>
                  <doc_id>139</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A sample alignment is shown in Table 3(b) in section 4.1.3.</text>
                  <doc_id>140</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Our best results are obtained with a 5-gram model.</text>
                  <doc_id>141</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The parameters p(a i |a i&#8722;1 i&#8722;k ) are estimated from a small transliteration corpus which we automatically extracted from the translation corpus.</text>
                  <doc_id>142</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The extraction details are also discussed in section 4.1.3.</text>
                  <doc_id>143</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Because our overall model is a conditional probability model, joint-probabilities are marginalized using character-based prior probabilities:</text>
                  <doc_id>144</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p c (h|u) = p c(h, u) p c (u) (6)</text>
                  <doc_id>145</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The prior probability p c (u) of the character sequence u = c m 1 is defined with a character-based language model:</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p c (u) = m&#8719;</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>148</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(c i |c i&#8722;1 i&#8722;k ) (7)</text>
                  <doc_id>149</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The parameters p(c i |c i&#8722;1 i&#8722;k ) are estimated from the Urdu part of the character-aligned transliteration corpus.</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Replacing (6) in (4) we get:</text>
                  <doc_id>151</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(h i |u i ) = &#955;p w (h i |u i ) + (1 &#8722; &#955;) p c(h i , u i ) p c (u i ) (8)</text>
                  <doc_id>152</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Having all the components of our model defined we insert (8) and (2) in (1) to obtain the final equation:</text>
                  <doc_id>153</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#251; n 1 = arg max</text>
                  <doc_id>154</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u n 1</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n&#8719;</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p LM (u i |u i&#8722;1 i&#8722;k )[&#955;p w(h i |u i )</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>+ (1 &#8722; &#955;) p c(h i , u i ) p c (u i ) ] (9)</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The optimization of the interpolating factor &#955; is discussed in section 4.2.1.</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Model-2 : Joint Probability Model</title>
            <text>This section briefly defines a variant of our model where we interpolate joint probabilities instead of conditional probabilities. Again, the translation model p(h n 1 |un 1 ) is approximated with a contextindependent model:
p(h n 1 |u n 1 ) = n&#8719;
p(h i |u i ) =
i=1
n&#8719;
i=1
p(h i , u i ) p(u i ) (10)
The joint probability p(h i , u i ) of a Hindi and an Urdu word is estimated by interpolating a wordbased model and a character-based model.
p(h i , u i ) = &#955;p w (h i , u i ) + (1 &#8722; &#955;)p c (h i , u i ) (11)
and the prior probability p(u i ) is estimated as:
p(u i ) = &#955;p w (u i ) + (1 &#8722; &#955;)p c (u i ) (12)
The parameters of the translation model p w (h i , u i ) and the word-based prior probabilities p w (u i ) are estimated from the 1-1/1-N word-aligned corpus (the one that we also used to estimate translation probabilities p w (h i |u i ) previously). The character-based transliteration probability p c (h i , u i ) and the character-based prior probability p c (u i ) are defined by (5) and (7) respectively in
the previous section. Putting (11) and (12) in (10) we get
n&#8719; p(h n 1 |u n &#955;p w (h i , u i ) + (1 &#8722; &#955;)p c (h i , u i )
1 ) =
&#955;p
i=1 w (u i ) + (1 &#8722; &#955;)p c (u i )
(13) The idea is to interpolate joint probabilities and divide them by the interpolated marginals. The final equation for Model-2 is given as:
&#251; n 1 = arg max
u n 1</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>This section briefly defines a variant of our model where we interpolate joint probabilities instead of conditional probabilities.</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Again, the translation model p(h n 1 |un 1 ) is approximated with a contextindependent model:</text>
                  <doc_id>162</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(h n 1 |u n 1 ) = n&#8719;</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(h i |u i ) =</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n&#8719;</text>
                  <doc_id>166</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(h i , u i ) p(u i ) (10)</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The joint probability p(h i , u i ) of a Hindi and an Urdu word is estimated by interpolating a wordbased model and a character-based model.</text>
                  <doc_id>169</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(h i , u i ) = &#955;p w (h i , u i ) + (1 &#8722; &#955;)p c (h i , u i ) (11)</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and the prior probability p(u i ) is estimated as:</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(u i ) = &#955;p w (u i ) + (1 &#8722; &#955;)p c (u i ) (12)</text>
                  <doc_id>172</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The parameters of the translation model p w (h i , u i ) and the word-based prior probabilities p w (u i ) are estimated from the 1-1/1-N word-aligned corpus (the one that we also used to estimate translation probabilities p w (h i |u i ) previously).</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The character-based transliteration probability p c (h i , u i ) and the character-based prior probability p c (u i ) are defined by (5) and (7) respectively in</text>
                  <doc_id>174</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the previous section.</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Putting (11) and (12) in (10) we get</text>
                  <doc_id>176</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n&#8719; p(h n 1 |u n &#955;p w (h i , u i ) + (1 &#8722; &#955;)p c (h i , u i )</text>
                  <doc_id>177</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 ) =</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#955;p</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1 w (u i ) + (1 &#8722; &#955;)p c (u i )</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(13) The idea is to interpolate joint probabilities and divide them by the interpolated marginals.</text>
                  <doc_id>181</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The final equation for Model-2 is given as:</text>
                  <doc_id>182</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#251; n 1 = arg max</text>
                  <doc_id>183</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>u n 1</text>
                  <doc_id>184</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Search</title>
            <text>n&#8719;
i=1
p LM (u i |u i&#8722;1 i&#8722;k )&#215;
&#955;p w (h i , u i ) + (1 &#8722; &#955;)p c (h i , u i ) &#955;p w (u i ) + (1 &#8722; &#955;)p c (u i ) (14)
The decoder performs a stack-based search using a beam-search algorithm similar to the one used in Pharoah (Koehn, 2004a). It searches for an Urdu string that maximizes the product of translation probability and the language model probability (equation 1) by translating one Hindi word at a time. It is implemented as a two-level process. At the lower level, it computes n-best transliterations for each Hindi word h i according to p c (h, u). The joint probabilities given by p c (h, u) are marginalized for each Urdu transliteration to give p c (h|u). At the higher level, transliteration probabilities are interpolated with p w (h|u) and then multiplied with language model probabilities to give the probability of a hypothesis. We use 20-best translations and 25-best transliterations for p w (h|u) and p c (h|u) respectively and a 5-gram language model.
To keep the search space manageable and time complexity polynomial we apply pruning and recombination. Since our model uses monotonic decoding we only need to recombine hypotheses that have the same context (last n-1 words). Next we do histogram-based pruning, maintaining the 100- best hypotheses for each stack.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>n&#8719;</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p LM (u i |u i&#8722;1 i&#8722;k )&#215;</text>
                  <doc_id>187</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#955;p w (h i , u i ) + (1 &#8722; &#955;)p c (h i , u i ) &#955;p w (u i ) + (1 &#8722; &#955;)p c (u i ) (14)</text>
                  <doc_id>188</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The decoder performs a stack-based search using a beam-search algorithm similar to the one used in Pharoah (Koehn, 2004a).</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It searches for an Urdu string that maximizes the product of translation probability and the language model probability (equation 1) by translating one Hindi word at a time.</text>
                  <doc_id>190</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It is implemented as a two-level process.</text>
                  <doc_id>191</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>At the lower level, it computes n-best transliterations for each Hindi word h i according to p c (h, u).</text>
                  <doc_id>192</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The joint probabilities given by p c (h, u) are marginalized for each Urdu transliteration to give p c (h|u).</text>
                  <doc_id>193</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>At the higher level, transliteration probabilities are interpolated with p w (h|u) and then multiplied with language model probabilities to give the probability of a hypothesis.</text>
                  <doc_id>194</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>We use 20-best translations and 25-best transliterations for p w (h|u) and p c (h|u) respectively and a 5-gram language model.</text>
                  <doc_id>195</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To keep the search space manageable and time complexity polynomial we apply pruning and recombination.</text>
                  <doc_id>196</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Since our model uses monotonic decoding we only need to recombine hypotheses that have the same context (last n-1 words).</text>
                  <doc_id>197</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Next we do histogram-based pruning, maintaining the 100- best hypotheses for each stack.</text>
                  <doc_id>198</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Evaluation</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>199</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Training</title>
            <text>This section discusses the training of the different model components.
4.1.1 Translation Corpus We used the freely available EMILLE Corpus as our bilingual resource which contains roughly 13,000 Urdu and 12,300 Hindi sentences. From these we were able to sentence-align 7000 sentence pairs using the sentence alignment algorithm given by Moore (2002).
The word alignments for this task were extracted by using GIZA++ (Och and Ney, 2003) in both directions. We extracted a total of 107323 alignment pairs (5743 N-1 alignments, 8404 M- N alignments and 93176 1-1/1-N alignments). Of these alignments M-N and N-1 alignment pairs were ignored. We manually inspected a sample of 1000 instances of M-N/N-1 alignments and found that more than 70% of these were (totally or partially) wrong. Of the 30% correct alignments, roughly one-third constitute N-1 alignments. Most of these are cases where the Urdu part of the alignment actually consists of two (or three) words but was written without space because of lack of standard writing convention in Urdu. For example (can go ; d ZA s@kt de) is alternatively written as (can go ; d ZAs@kt de) i.e. without space. We learned that these N-1 translations could be safely dropped because we can generate a separate Urdu word for each Hindi word. For valid M-N alignments we observed that these could be broken into 1-1/1-N alignments in most of the cases. We also observed that we usually have coverage of the resulting 1-1 and 1-N alignments in our translation corpus. Looking at the noise in the incorrect alignments we decided to drop N-1 and M-N cases. We do not model deletions and insertions so we ignored null alignments. Also 1-N alignments with gaps were ignored. Only the alignments with contiguous words were kept.
4.1.2 Monolingual Corpus
Our monolingual Urdu corpus consists of roughly 114K sentences. This comprises 108K sentences from the data made available by the University of Leipzig 4 + 5600 sentences from the training data of each fold during cross validation.
4.1.3 Transliteration Corpus
The training corpus for transliteration is extracted from the 1-1/1-N word-alignments of the EMILLE corpus discussed in section 4.1.1. We use an edit distance algorithm to align this training corpus at the character level and we eliminate translation pairs with high edit distance which are unlikely to be transliterations.
4 http://corpora.informatik.uni-leipzig.de/
We used our knowledge of the Hindi and Urdu scripts to define the initial character mapping. The mapping was further extended by looking into available Hindi-Urdu transliteration systems [5,6] and other resources (Gupta, 2004; Malik et al., 2008; Jawaid and Ahmed, 2009). Each pair in the character map is assigned a cost. A Hindi character that always map to only one Urdu character is assigned a cost of 0 whereas the Hindi characters that map to different Urdu characters are assigned a cost of 0.2. The edit distance metric allows insert, delete and replace operations. The handcrafted pairs define the cost of replace operations. We set a cost of 0.6 for deletions and insertions. These costs were optimized on held out data. The details of optimization are not mentioned due to limited space. Using this metric we filter out the word pairs with high edit-distance to extract our transliteration corpus. We were able to extract roughly 2100 unique pairs along with their alignments. The resulting alignments are modified by merging unaligned &#8709; &#8594; 1 (no character on source side, 1 character on target side) or &#8709; &#8594; N alignments with the preceding alignment pair. If there is no preceding alignment pair then it is merged with the following pair. Table 3 gives an example showing initial alignment (a) and the final alignment (b) after applying the merge operation. Our model retains 1 &#8594; &#8709; and N &#8594; &#8709; alignments as deletion operations.
a) Hindi &#8709; b c &#8709; e f
The parameters p c (h, u) and p c (u) are trained on the aligned corpus using the SRILM toolkit. We use Add-1 smoothing for unigrams and Kneser-Ney smoothing for higher n-grams.
4.1.4 Diacritic Removal and Normalization
In Urdu, short vowels are represented with diacritics but these are rarely written in practice. In order to keep the data consistent, all diacritics are removed. This loss of information is not harmful when transliterating/translating from Hindi to Urdu because undiacritized text is equally read-
5 CRULP: http://www.crulp.org/software/langproc.htm 6 Malerkotla.org: http://translate.malerkotla.co.in
able to native speakers as its diacritized counter part. However leaving occasional diacritics in the corpus can worsen the problem of data sparsity by creating spurious ambiguity 7 .
There are a few Urdu characters that have multiple equivalent Unicodes. All such forms are normalized to have only one representation 8 .</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>This section discusses the training of the different model components.</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.1.1 Translation Corpus We used the freely available EMILLE Corpus as our bilingual resource which contains roughly 13,000 Urdu and 12,300 Hindi sentences.</text>
                  <doc_id>201</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>From these we were able to sentence-align 7000 sentence pairs using the sentence alignment algorithm given by Moore (2002).</text>
                  <doc_id>202</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The word alignments for this task were extracted by using GIZA++ (Och and Ney, 2003) in both directions.</text>
                  <doc_id>203</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We extracted a total of 107323 alignment pairs (5743 N-1 alignments, 8404 M- N alignments and 93176 1-1/1-N alignments).</text>
                  <doc_id>204</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Of these alignments M-N and N-1 alignment pairs were ignored.</text>
                  <doc_id>205</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We manually inspected a sample of 1000 instances of M-N/N-1 alignments and found that more than 70% of these were (totally or partially) wrong.</text>
                  <doc_id>206</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Of the 30% correct alignments, roughly one-third constitute N-1 alignments.</text>
                  <doc_id>207</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Most of these are cases where the Urdu part of the alignment actually consists of two (or three) words but was written without space because of lack of standard writing convention in Urdu.</text>
                  <doc_id>208</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>For example (can go ; d ZA s@kt de) is alternatively written as (can go ; d ZAs@kt de) i.e. without space.</text>
                  <doc_id>209</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>We learned that these N-1 translations could be safely dropped because we can generate a separate Urdu word for each Hindi word.</text>
                  <doc_id>210</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>For valid M-N alignments we observed that these could be broken into 1-1/1-N alignments in most of the cases.</text>
                  <doc_id>211</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>We also observed that we usually have coverage of the resulting 1-1 and 1-N alignments in our translation corpus.</text>
                  <doc_id>212</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>Looking at the noise in the incorrect alignments we decided to drop N-1 and M-N cases.</text>
                  <doc_id>213</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>We do not model deletions and insertions so we ignored null alignments.</text>
                  <doc_id>214</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
                <sentence>
                  <text>Also 1-N alignments with gaps were ignored.</text>
                  <doc_id>215</doc_id>
                  <sec_id>12</sec_id>
                </sentence>
                <sentence>
                  <text>Only the alignments with contiguous words were kept.</text>
                  <doc_id>216</doc_id>
                  <sec_id>13</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.1.2 Monolingual Corpus</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Our monolingual Urdu corpus consists of roughly 114K sentences.</text>
                  <doc_id>218</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This comprises 108K sentences from the data made available by the University of Leipzig 4 + 5600 sentences from the training data of each fold during cross validation.</text>
                  <doc_id>219</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.1.3 Transliteration Corpus</text>
                  <doc_id>220</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The training corpus for transliteration is extracted from the 1-1/1-N word-alignments of the EMILLE corpus discussed in section 4.1.1.</text>
                  <doc_id>221</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We use an edit distance algorithm to align this training corpus at the character level and we eliminate translation pairs with high edit distance which are unlikely to be transliterations.</text>
                  <doc_id>222</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4 http://corpora.informatik.uni-leipzig.de/</text>
                  <doc_id>223</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We used our knowledge of the Hindi and Urdu scripts to define the initial character mapping.</text>
                  <doc_id>224</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The mapping was further extended by looking into available Hindi-Urdu transliteration systems [5,6] and other resources (Gupta, 2004; Malik et al., 2008; Jawaid and Ahmed, 2009).</text>
                  <doc_id>225</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Each pair in the character map is assigned a cost.</text>
                  <doc_id>226</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>A Hindi character that always map to only one Urdu character is assigned a cost of 0 whereas the Hindi characters that map to different Urdu characters are assigned a cost of 0.2.</text>
                  <doc_id>227</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The edit distance metric allows insert, delete and replace operations.</text>
                  <doc_id>228</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The handcrafted pairs define the cost of replace operations.</text>
                  <doc_id>229</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>We set a cost of 0.6 for deletions and insertions.</text>
                  <doc_id>230</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>These costs were optimized on held out data.</text>
                  <doc_id>231</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>The details of optimization are not mentioned due to limited space.</text>
                  <doc_id>232</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>Using this metric we filter out the word pairs with high edit-distance to extract our transliteration corpus.</text>
                  <doc_id>233</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>We were able to extract roughly 2100 unique pairs along with their alignments.</text>
                  <doc_id>234</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>The resulting alignments are modified by merging unaligned &#8709; &#8594; 1 (no character on source side, 1 character on target side) or &#8709; &#8594; N alignments with the preceding alignment pair.</text>
                  <doc_id>235</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
                <sentence>
                  <text>If there is no preceding alignment pair then it is merged with the following pair.</text>
                  <doc_id>236</doc_id>
                  <sec_id>12</sec_id>
                </sentence>
                <sentence>
                  <text>Table 3 gives an example showing initial alignment (a) and the final alignment (b) after applying the merge operation.</text>
                  <doc_id>237</doc_id>
                  <sec_id>13</sec_id>
                </sentence>
                <sentence>
                  <text>Our model retains 1 &#8594; &#8709; and N &#8594; &#8709; alignments as deletion operations.</text>
                  <doc_id>238</doc_id>
                  <sec_id>14</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>a) Hindi &#8709; b c &#8709; e f</text>
                  <doc_id>239</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The parameters p c (h, u) and p c (u) are trained on the aligned corpus using the SRILM toolkit.</text>
                  <doc_id>240</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We use Add-1 smoothing for unigrams and Kneser-Ney smoothing for higher n-grams.</text>
                  <doc_id>241</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.1.4 Diacritic Removal and Normalization</text>
                  <doc_id>242</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In Urdu, short vowels are represented with diacritics but these are rarely written in practice.</text>
                  <doc_id>243</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In order to keep the data consistent, all diacritics are removed.</text>
                  <doc_id>244</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This loss of information is not harmful when transliterating/translating from Hindi to Urdu because undiacritized text is equally read-</text>
                  <doc_id>245</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5 CRULP: http://www.crulp.org/software/langproc.htm 6 Malerkotla.org: http://translate.malerkotla.co.in</text>
                  <doc_id>246</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>able to native speakers as its diacritized counter part.</text>
                  <doc_id>247</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However leaving occasional diacritics in the corpus can worsen the problem of data sparsity by creating spurious ambiguity 7 .</text>
                  <doc_id>248</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>There are a few Urdu characters that have multiple equivalent Unicodes.</text>
                  <doc_id>249</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>All such forms are normalized to have only one representation 8 .</text>
                  <doc_id>250</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Experimental Setup</title>
            <text>We perform a 5-fold cross validation taking 4/5 of the data as training and 1/5 as test data. Each fold comprises roughly 1400 test sentences and 5600 training sentences.
4.2.1 Parameter Optimization
Our model contains two parameters &#955; (the interpolating factor between translation and transliteration modules) and &#968; (the factor that controls the trade-off between LM-known and LM-unknown transliterations). The interpolating factor &#955; is initialized, inspired by Written-Bell smoothing, with
N
a value of
N+B 9 . We chose a very low value
1e &#8722;40 for the factor &#968; initially, favoring LMknown transliterations very strongly. Both of these parameters are optimized as described below.
Because our training data is very sparse we do not use held-out data for parameter optimization. Instead we optimize these parameters by performing a 2-fold optimization for each of the 5 folds. Each fold is divided into two halves. The parameters &#955; and &#968; are optimized on the first half and the other half is used for testing, then optimization is done on the second half and the first half is used for testing. The optimal value for parameter &#955; occurs between 0.7-0.84 and for the parameter &#968; between 1e &#8722;5 and 1e &#8722;10 .
4.2.2 Results
Baseline P b 0 : We ran Moses (Koehn et al., 2007) using Koehn&#8217;s training scripts 10 , doing a 5-fold cross validation with no reordering 11 . For the other parameters we use the default values i.e. 5-gram language model and maximum phraselength= 6. Again, the language model is imple-
7 It should be noted though that diacritics play a very important role when transliterating in the reverse direction because these are virtually always written in Hindi as dependent vowels. 8 www.crulp.org/software/langproc/urdunormalization.htm 9 N is the number of aligned word pairs (tokens) and B is
the number of different aligned word pairs (types). 10 http://statmt.org/wmt08/baseline.html 11 Results are worse with reordering enabled.
mented as an n-gram model using the SRILM- Toolkit with Kneser-Ney smoothing. Each fold comprises roughly 1400 test sentences, 5000 in training and 600 in dev 12 . We also used two methods to incorporate transliterations in the phrasebased system:
Post-process P b 1 : All the OOV words in the phrase-based output are replaced with their topcandidate transliteration as given by our transliteration system.
Pre-process P b 2 : Instead of adding transliterations as a post process we do a second pass by adding the unknown words with their topcandidate transliteration to the training corpus and rerun Koehn&#8217;s training script with the new training corpus. Table 4 shows results (taking arithmetic average over 5 folds) from Model-1 and Model- 2 in comparison with three baselines discussed above.
Both our systems (Model-1 and Model-2) beat the baseline phrase-based system with a BLEU point difference of 4.30 and 2.75 respectively. The transliteration aided phrase-based systems P b 1 and P b 2 are closer to our Model-2 results but are way below Model-1 results. The difference of 2.35 BLEU points between M 1 and P b 1 indicates that transliteration is useful for more than only translating OOV words for language pairs like Hindi-Urdu. Our models choose between translations and transliterations based on context unlike the phrase-based systems P b 1 and P b 2 which use transliteration only as a tool to translate OOV words.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We perform a 5-fold cross validation taking 4/5 of the data as training and 1/5 as test data.</text>
                  <doc_id>251</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Each fold comprises roughly 1400 test sentences and 5600 training sentences.</text>
                  <doc_id>252</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.2.1 Parameter Optimization</text>
                  <doc_id>253</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Our model contains two parameters &#955; (the interpolating factor between translation and transliteration modules) and &#968; (the factor that controls the trade-off between LM-known and LM-unknown transliterations).</text>
                  <doc_id>254</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The interpolating factor &#955; is initialized, inspired by Written-Bell smoothing, with</text>
                  <doc_id>255</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>N</text>
                  <doc_id>256</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>a value of</text>
                  <doc_id>257</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>N+B 9 .</text>
                  <doc_id>258</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We chose a very low value</text>
                  <doc_id>259</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1e &#8722;40 for the factor &#968; initially, favoring LMknown transliterations very strongly.</text>
                  <doc_id>260</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Both of these parameters are optimized as described below.</text>
                  <doc_id>261</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Because our training data is very sparse we do not use held-out data for parameter optimization.</text>
                  <doc_id>262</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Instead we optimize these parameters by performing a 2-fold optimization for each of the 5 folds.</text>
                  <doc_id>263</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Each fold is divided into two halves.</text>
                  <doc_id>264</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The parameters &#955; and &#968; are optimized on the first half and the other half is used for testing, then optimization is done on the second half and the first half is used for testing.</text>
                  <doc_id>265</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The optimal value for parameter &#955; occurs between 0.7-0.84 and for the parameter &#968; between 1e &#8722;5 and 1e &#8722;10 .</text>
                  <doc_id>266</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.2.2 Results</text>
                  <doc_id>267</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Baseline P b 0 : We ran Moses (Koehn et al., 2007) using Koehn&#8217;s training scripts 10 , doing a 5-fold cross validation with no reordering 11 .</text>
                  <doc_id>268</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For the other parameters we use the default values i.e. 5-gram language model and maximum phraselength= 6.</text>
                  <doc_id>269</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Again, the language model is imple-</text>
                  <doc_id>270</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>7 It should be noted though that diacritics play a very important role when transliterating in the reverse direction because these are virtually always written in Hindi as dependent vowels.</text>
                  <doc_id>271</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>8 www.crulp.org/software/langproc/urdunormalization.htm 9 N is the number of aligned word pairs (tokens) and B is</text>
                  <doc_id>272</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the number of different aligned word pairs (types).</text>
                  <doc_id>273</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>10 http://statmt.org/wmt08/baseline.html 11 Results are worse with reordering enabled.</text>
                  <doc_id>274</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>mented as an n-gram model using the SRILM- Toolkit with Kneser-Ney smoothing.</text>
                  <doc_id>275</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Each fold comprises roughly 1400 test sentences, 5000 in training and 600 in dev 12 .</text>
                  <doc_id>276</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We also used two methods to incorporate transliterations in the phrasebased system:</text>
                  <doc_id>277</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Post-process P b 1 : All the OOV words in the phrase-based output are replaced with their topcandidate transliteration as given by our transliteration system.</text>
                  <doc_id>278</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Pre-process P b 2 : Instead of adding transliterations as a post process we do a second pass by adding the unknown words with their topcandidate transliteration to the training corpus and rerun Koehn&#8217;s training script with the new training corpus.</text>
                  <doc_id>279</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Table 4 shows results (taking arithmetic average over 5 folds) from Model-1 and Model- 2 in comparison with three baselines discussed above.</text>
                  <doc_id>280</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Both our systems (Model-1 and Model-2) beat the baseline phrase-based system with a BLEU point difference of 4.30 and 2.75 respectively.</text>
                  <doc_id>281</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The transliteration aided phrase-based systems P b 1 and P b 2 are closer to our Model-2 results but are way below Model-1 results.</text>
                  <doc_id>282</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The difference of 2.35 BLEU points between M 1 and P b 1 indicates that transliteration is useful for more than only translating OOV words for language pairs like Hindi-Urdu.</text>
                  <doc_id>283</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Our models choose between translations and transliterations based on context unlike the phrase-based systems P b 1 and P b 2 which use transliteration only as a tool to translate OOV words.</text>
                  <doc_id>284</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Error Analysis</title>
        <text>Based on preliminary experiments we found three major flaws in our initial formulations. This section discusses each one of them and provides some heuristics and modifications that we employ to try to correct deficiencies we found in the two models described in section 3.1 and 3.2.
12 After having the MERT parameters, we add the 600 dev
sentences back into the training corpus, retrain GIZA, and then estimate a new phrase table on all 5600 sentences. We then use the MERT parameters obtained before together with the newer (larger) phrase-table set.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Based on preliminary experiments we found three major flaws in our initial formulations.</text>
              <doc_id>285</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This section discusses each one of them and provides some heuristics and modifications that we employ to try to correct deficiencies we found in the two models described in section 3.1 and 3.2.</text>
              <doc_id>286</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>12 After having the MERT parameters, we add the 600 dev</text>
              <doc_id>287</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>sentences back into the training corpus, retrain GIZA, and then estimate a new phrase table on all 5600 sentences.</text>
              <doc_id>288</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We then use the MERT parameters obtained before together with the newer (larger) phrase-table set.</text>
              <doc_id>289</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Heuristic-1</title>
            <text>A lot of errors occur because our translation model is built on very sparse and noisy data. The motivation for this heuristic is to counter wrong alignments at least in the case of verbs and functional words (which are often transliterations). This heuristic favors translations that also appear in the n-best transliteration list over only-translation and only-transliteration options. We modify the translation model for both the conditional and the joint model by adding another factor which strongly weighs translation+transliteration options by taking the square-root of the product of the translation and transliteration probabilities. Thus modifying equations (8) and (11) in Model-1 and Model-2 we obtain equations (15) and (16) respectively:
p(h i |u i ) = &#955; 1 p w (h i |u i ) + &#955; 2 p c (h i , u i ) p c (u i )
+ &#955; 3 &#8730;
p w (h i |u i ) p c(h i , u i ) p c (u i )
p(h i , u i ) = &#955; 1 p w (h i , u i ) + &#955; 2 p c (h i , u i )
(15)
+ &#955; 3 &#8730;
pw (h i , u i )p c (h i , u i ) (16)
For the optimization of lambda parameters we hold the value of the translation coefficient &#955; 1
and the transliteration coefficient &#955; 2 constant (using the optimized values as discussed in section 4.2.1) and optimize &#955; 3 again using 2-fold optimization on all the folds as described above 14 .</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>A lot of errors occur because our translation model is built on very sparse and noisy data.</text>
                  <doc_id>290</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The motivation for this heuristic is to counter wrong alignments at least in the case of verbs and functional words (which are often transliterations).</text>
                  <doc_id>291</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This heuristic favors translations that also appear in the n-best transliteration list over only-translation and only-transliteration options.</text>
                  <doc_id>292</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We modify the translation model for both the conditional and the joint model by adding another factor which strongly weighs translation+transliteration options by taking the square-root of the product of the translation and transliteration probabilities.</text>
                  <doc_id>293</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Thus modifying equations (8) and (11) in Model-1 and Model-2 we obtain equations (15) and (16) respectively:</text>
                  <doc_id>294</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(h i |u i ) = &#955; 1 p w (h i |u i ) + &#955; 2 p c (h i , u i ) p c (u i )</text>
                  <doc_id>295</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>+ &#955; 3 &#8730;</text>
                  <doc_id>296</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p w (h i |u i ) p c(h i , u i ) p c (u i )</text>
                  <doc_id>297</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(h i , u i ) = &#955; 1 p w (h i , u i ) + &#955; 2 p c (h i , u i )</text>
                  <doc_id>298</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(15)</text>
                  <doc_id>299</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>+ &#955; 3 &#8730;</text>
                  <doc_id>300</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>pw (h i , u i )p c (h i , u i ) (16)</text>
                  <doc_id>301</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For the optimization of lambda parameters we hold the value of the translation coefficient &#955; 1</text>
                  <doc_id>302</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and the transliteration coefficient &#955; 2 constant (using the optimized values as discussed in section 4.2.1) and optimize &#955; 3 again using 2-fold optimization on all the folds as described above 14 .</text>
                  <doc_id>303</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Heuristic-2</title>
            <text>When an unknown Hindi word occurs for which all transliteration options are LM-unknown then the best transliteration should be selected. The problem in our original models is that a fixed LM probability &#968; is used for LM-unknown transliterations. Hence our model selects the transliteration that has the best pc(h i,u i )
p c(u i )
score i.e. we maximize p c (h i |u i ) instead of p c (u i |h i ) (or equivalently p c (h i , u i )). The reason is an inconsistency in our models. The language model probability of unknown words is uniform (and equal to &#968;) whereas the translation model uses the nonuniform prior probability p c (u i ) for these words. There is another reason why we can not use the
13 The translation coefficient &#955; 1 is same as &#955; used in previous models and the transliteration coefficient &#955; 2 = 1 &#8722; &#955; 14 After optimization we normalize the lambdas to make
their sum equal to 1.
value &#968; in this case. Our transliterator model also produces space inserted words. The value of &#968; is very small because of which transliterations that are actually LM-unknown, but are mistakenly broken into constituents that are LM-known, will always be preferred over their counter parts. An example of this is (America) for which two possible transliterations as given by our model are
(AmerIkA, without space) and (AmerI kA, with space). The latter version is LM-known as its constituents are LM-known. Our models always favor the latter version. Space insertion is an important feature of our transliteration model. We want our transliterator to tackle compound words, derivational affixes, case-markers with nouns that are written as one word in Hindi but as two or more words in Urdu. Examples were already shown in section 3&#8217;s footnote.
We eliminate the inconsistency by using p c (u i ) as the 0-gram back-off probability distribution in the language model. For an LM-unknown transliterations we now get in Model-1:
p(u i |u i&#8722;1 i&#8722;k )[&#955;p w(h i |u i ) + (1 &#8722; &#955;) p c(h i , u i ) p c (u i ) ]
= p(u i |u i&#8722;1 i&#8722;k )[(1 &#8722; &#955;)p c(h i , u i ) p c (u i ) ]
=
=
k&#8719;
j=0
k&#8719;
j=0
&#945;(u i&#8722;1 i&#8722;j )p c(u i )[(1 &#8722; &#955;) p c(h i , u i ) p c (u i ) ]
&#945;(u i&#8722;1 i&#8722;j )[(1 &#8722; &#955;)p c(h i , u i )] where &#8719; k
j=0 &#945;(ui&#8722;1 i&#8722;j
) is just the constant that SRILM returns for unknown words. The last line of the calculation shows that we simply drop p &#8719; c (u i ) if u i is LM-unknown and use the constant
k j=0 &#945;(ui&#8722;1 i&#8722;j
) instead of &#968;. A similar calculation for Model-2 gives &#8719; k
j=0 &#945;(ui&#8722;1 i&#8722;j )p c(h i , u i ).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>When an unknown Hindi word occurs for which all transliteration options are LM-unknown then the best transliteration should be selected.</text>
                  <doc_id>304</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The problem in our original models is that a fixed LM probability &#968; is used for LM-unknown transliterations.</text>
                  <doc_id>305</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Hence our model selects the transliteration that has the best pc(h i,u i )</text>
                  <doc_id>306</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p c(u i )</text>
                  <doc_id>307</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>score i.e. we maximize p c (h i |u i ) instead of p c (u i |h i ) (or equivalently p c (h i , u i )).</text>
                  <doc_id>308</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The reason is an inconsistency in our models.</text>
                  <doc_id>309</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The language model probability of unknown words is uniform (and equal to &#968;) whereas the translation model uses the nonuniform prior probability p c (u i ) for these words.</text>
                  <doc_id>310</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>There is another reason why we can not use the</text>
                  <doc_id>311</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>13 The translation coefficient &#955; 1 is same as &#955; used in previous models and the transliteration coefficient &#955; 2 = 1 &#8722; &#955; 14 After optimization we normalize the lambdas to make</text>
                  <doc_id>312</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>their sum equal to 1.</text>
                  <doc_id>313</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>value &#968; in this case.</text>
                  <doc_id>314</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Our transliterator model also produces space inserted words.</text>
                  <doc_id>315</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The value of &#968; is very small because of which transliterations that are actually LM-unknown, but are mistakenly broken into constituents that are LM-known, will always be preferred over their counter parts.</text>
                  <doc_id>316</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>An example of this is (America) for which two possible transliterations as given by our model are</text>
                  <doc_id>317</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(AmerIkA, without space) and (AmerI kA, with space).</text>
                  <doc_id>318</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The latter version is LM-known as its constituents are LM-known.</text>
                  <doc_id>319</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Our models always favor the latter version.</text>
                  <doc_id>320</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Space insertion is an important feature of our transliteration model.</text>
                  <doc_id>321</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We want our transliterator to tackle compound words, derivational affixes, case-markers with nouns that are written as one word in Hindi but as two or more words in Urdu.</text>
                  <doc_id>322</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Examples were already shown in section 3&#8217;s footnote.</text>
                  <doc_id>323</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We eliminate the inconsistency by using p c (u i ) as the 0-gram back-off probability distribution in the language model.</text>
                  <doc_id>324</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For an LM-unknown transliterations we now get in Model-1:</text>
                  <doc_id>325</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(u i |u i&#8722;1 i&#8722;k )[&#955;p w(h i |u i ) + (1 &#8722; &#955;) p c(h i , u i ) p c (u i ) ]</text>
                  <doc_id>326</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>= p(u i |u i&#8722;1 i&#8722;k )[(1 &#8722; &#955;)p c(h i , u i ) p c (u i ) ]</text>
                  <doc_id>327</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>=</text>
                  <doc_id>328</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>=</text>
                  <doc_id>329</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k&#8719;</text>
                  <doc_id>330</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=0</text>
                  <doc_id>331</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k&#8719;</text>
                  <doc_id>332</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=0</text>
                  <doc_id>333</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#945;(u i&#8722;1 i&#8722;j )p c(u i )[(1 &#8722; &#955;) p c(h i , u i ) p c (u i ) ]</text>
                  <doc_id>334</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#945;(u i&#8722;1 i&#8722;j )[(1 &#8722; &#955;)p c(h i , u i )] where &#8719; k</text>
                  <doc_id>335</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=0 &#945;(ui&#8722;1 i&#8722;j</text>
                  <doc_id>336</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>) is just the constant that SRILM returns for unknown words.</text>
                  <doc_id>337</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The last line of the calculation shows that we simply drop p &#8719; c (u i ) if u i is LM-unknown and use the constant</text>
                  <doc_id>338</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k j=0 &#945;(ui&#8722;1 i&#8722;j</text>
                  <doc_id>339</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>) instead of &#968;.</text>
                  <doc_id>340</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A similar calculation for Model-2 gives &#8719; k</text>
                  <doc_id>341</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=0 &#945;(ui&#8722;1 i&#8722;j )p c(h i , u i ).</text>
                  <doc_id>342</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>5.3 Heuristic-3</title>
            <text>This heuristic discusses a flaw in Model-2. For transliteration options that are TM-unknown, the p w (h, u) and p w (u) factors becomes zero and the translation model probability as given by equation (13) becomes:
(1 &#8722; &#955;)p c (h i , u i ) (1 &#8722; &#955;)p c (u i )
= p c(h i , u i )
p c (u i )
In such cases the &#955; factor cancels out and no weighting of word translation vs. transliteration
occurs anymore. As a result of this, transliterations are sometimes incorrectly favored over their translation alternatives.
In order to remedy this problem we assign a minimal probability &#946; to the word-based prior p w (u i ) in case of TM-unknown transliterations, which prevents it from ever being zero. Because of this addition the translation model probability for LM-unknown words becomes:
(1 &#8722; &#955;)p c (h i , u i ) &#955;&#946; + (1 &#8722; &#955;)p c (u i ) where &#946; = 1 Urdu Types in TM</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>This heuristic discusses a flaw in Model-2.</text>
                  <doc_id>343</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For transliteration options that are TM-unknown, the p w (h, u) and p w (u) factors becomes zero and the translation model probability as given by equation (13) becomes:</text>
                  <doc_id>344</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(1 &#8722; &#955;)p c (h i , u i ) (1 &#8722; &#955;)p c (u i )</text>
                  <doc_id>345</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>= p c(h i , u i )</text>
                  <doc_id>346</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p c (u i )</text>
                  <doc_id>347</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In such cases the &#955; factor cancels out and no weighting of word translation vs. transliteration</text>
                  <doc_id>348</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>occurs anymore.</text>
                  <doc_id>349</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As a result of this, transliterations are sometimes incorrectly favored over their translation alternatives.</text>
                  <doc_id>350</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In order to remedy this problem we assign a minimal probability &#946; to the word-based prior p w (u i ) in case of TM-unknown transliterations, which prevents it from ever being zero.</text>
                  <doc_id>351</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Because of this addition the translation model probability for LM-unknown words becomes:</text>
                  <doc_id>352</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(1 &#8722; &#955;)p c (h i , u i ) &#955;&#946; + (1 &#8722; &#955;)p c (u i ) where &#946; = 1 Urdu Types in TM</text>
                  <doc_id>353</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>6</index>
        <title>6 Final Results</title>
        <text>This section shows the improvement in BLEU score by applying heuristics and combinations of heuristics in both the models. Tables 5 and 6 show the improvements achieved by using the different heuristics and modifications discussed in section 5. We refer to the results as M x H y where x denotes the model number, 1 for the conditional probability model and 2 for the joint probability model and y denotes a heuristic or a combination of heuristics applied to that model 15 .
Both heuristics (H 1 and H 2 ) show improvements over their base models M 1 and M 2 . Heuristic-1 shows notable improvement for both models in parts of test data which has high number of common vocabulary words. Using heuristic 2 we were able to properly score LM-unknown transliterations against each other. Using these heuristics together we obtain a gain of 0.75 over M-1 and a gain of 1.29 over M-2.
Heuristic-3 remedies the flaw in M 2 by assigning a special value to the word-based prior p w (u i ) for TM-unknown words which prevents the cancelation of interpolating parameter &#955;. M 2 combined with heuristic 3 (M 2 H 3 ) results in a 1.47
15 For example M 1H 1 refers to the results when heuristic-
1 is applied to model-1 whereas M 2H 12 refers to the results when heuristics 1 and 2 are together applied to model 2.
BLEU point improvement and combined with all the heuristics (M 2 H 123 ) gives an overall gain of 1.95 BLEU points and is close to our best results (M 1 H 12 ). We also performed significance test by concatenating all the fold results. Both our best systems M 1 H 12 and M 2 H 123 are statistically significant (p &lt; 0.05) 16 over all the baselines discussed in section 4.2.2. One important issue that has not been investigated yet is that BLEU has not yet been shown to have good performance in morphologically rich target languages like Urdu, but there is no metric known to work better. We observed that sometimes on data where the translators preferred to translate rather than doing transliteration our system is penalized by BLEU even though our output string is a valid translation. For other parts of the data where the translators have heavily used transliteration, the system may receive a higher BLEU score. We feel that this is an interesting area of research for automatic metric developers, and that a large scale task of translation to Urdu which would involve a human evaluation campaign would be very interesting.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This section shows the improvement in BLEU score by applying heuristics and combinations of heuristics in both the models.</text>
              <doc_id>354</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Tables 5 and 6 show the improvements achieved by using the different heuristics and modifications discussed in section 5.</text>
              <doc_id>355</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We refer to the results as M x H y where x denotes the model number, 1 for the conditional probability model and 2 for the joint probability model and y denotes a heuristic or a combination of heuristics applied to that model 15 .</text>
              <doc_id>356</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Both heuristics (H 1 and H 2 ) show improvements over their base models M 1 and M 2 .</text>
              <doc_id>357</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Heuristic-1 shows notable improvement for both models in parts of test data which has high number of common vocabulary words.</text>
              <doc_id>358</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Using heuristic 2 we were able to properly score LM-unknown transliterations against each other.</text>
              <doc_id>359</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Using these heuristics together we obtain a gain of 0.75 over M-1 and a gain of 1.29 over M-2.</text>
              <doc_id>360</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Heuristic-3 remedies the flaw in M 2 by assigning a special value to the word-based prior p w (u i ) for TM-unknown words which prevents the cancelation of interpolating parameter &#955;. M 2 combined with heuristic 3 (M 2 H 3 ) results in a 1.47</text>
              <doc_id>361</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>15 For example M 1H 1 refers to the results when heuristic-</text>
              <doc_id>362</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 is applied to model-1 whereas M 2H 12 refers to the results when heuristics 1 and 2 are together applied to model 2.</text>
              <doc_id>363</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>BLEU point improvement and combined with all the heuristics (M 2 H 123 ) gives an overall gain of 1.95 BLEU points and is close to our best results (M 1 H 12 ).</text>
              <doc_id>364</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We also performed significance test by concatenating all the fold results.</text>
              <doc_id>365</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Both our best systems M 1 H 12 and M 2 H 123 are statistically significant (p &lt; 0.05) 16 over all the baselines discussed in section 4.2.2.</text>
              <doc_id>366</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>One important issue that has not been investigated yet is that BLEU has not yet been shown to have good performance in morphologically rich target languages like Urdu, but there is no metric known to work better.</text>
              <doc_id>367</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We observed that sometimes on data where the translators preferred to translate rather than doing transliteration our system is penalized by BLEU even though our output string is a valid translation.</text>
              <doc_id>368</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>For other parts of the data where the translators have heavily used transliteration, the system may receive a higher BLEU score.</text>
              <doc_id>369</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We feel that this is an interesting area of research for automatic metric developers, and that a large scale task of translation to Urdu which would involve a human evaluation campaign would be very interesting.</text>
              <doc_id>370</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>7</index>
        <title>7 Sample Output</title>
        <text>This section gives two examples showing how our model (M 1H2) performs disambiguation. Given below are some test sentences that have Hindi homonyms (underlined in the examples) along with Urdu output given by our system. In the first
example (given in Figure 1) Hindi word can be transliterated to ( Lion) or (Verse) depending upon the context. Our model correctly identifies which transliteration to choose given the context.
In the second example (shown in Figure 2) Hindi word can be translated to (peace, s@kun) when it is a common noun but transliterated to (Shanti, SAnt di) when it is a proper name. Our model successfully decides whether to translate or transliterate given the context.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This section gives two examples showing how our model (M 1H2) performs disambiguation.</text>
              <doc_id>371</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Given below are some test sentences that have Hindi homonyms (underlined in the examples) along with Urdu output given by our system.</text>
              <doc_id>372</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In the first</text>
              <doc_id>373</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>example (given in Figure 1) Hindi word can be transliterated to ( Lion) or (Verse) depending upon the context.</text>
              <doc_id>374</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our model correctly identifies which transliteration to choose given the context.</text>
              <doc_id>375</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the second example (shown in Figure 2) Hindi word can be translated to (peace, s@kun) when it is a common noun but transliterated to (Shanti, SAnt di) when it is a proper name.</text>
              <doc_id>376</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our model successfully decides whether to translate or transliterate given the context.</text>
              <doc_id>377</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>8</index>
        <title>8 Conclusion</title>
        <text>We have presented a novel way to integrate transliterations into machine translation. In closely related language pairs such as Hindi-Urdu with a significant amount of vocabulary overlap, 16 We used Kevin Gimpel&#8217;s tester (http://www.ark.cs.cmu.edu/MT/) which uses bootstrap resampling (Koehn, 2004b), with 1000 samples. Ser d Z@ngl kA rAd ZA he &#8220;Lion is the king of jungle&#8221; AIqbAl kA Aek xub sur@t d Ser he &#8220;There is a beautiful verse from Iqbal&#8221;
words. We have addressed two problems. First, transliteration helps overcome the problem of data sparsity and noisy alignments. We are able to generate word translations that are unseen in the translation corpus but known to the language model. Additionally, we can generate novel transliterations (that are LM-Unknown). Second, generating multiple transliterations for homograph Hindi words and using language model context helps us solve the problem of disambiguation. We found that the joint probability model performs almost as well as the conditional probability model but that it was more complex to make it work well.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have presented a novel way to integrate transliterations into machine translation.</text>
              <doc_id>378</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In closely related language pairs such as Hindi-Urdu with a significant amount of vocabulary overlap, 16 We used Kevin Gimpel&#8217;s tester (http://www.ark.cs.cmu.edu/MT/) which uses bootstrap resampling (Koehn, 2004b), with 1000 samples.</text>
              <doc_id>379</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Ser d Z@ngl kA rAd ZA he &#8220;Lion is the king of jungle&#8221; AIqbAl kA Aek xub sur@t d Ser he &#8220;There is a beautiful verse from Iqbal&#8221;</text>
              <doc_id>380</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>words.</text>
              <doc_id>381</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We have addressed two problems.</text>
              <doc_id>382</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>First, transliteration helps overcome the problem of data sparsity and noisy alignments.</text>
              <doc_id>383</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We are able to generate word translations that are unseen in the translation corpus but known to the language model.</text>
              <doc_id>384</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Additionally, we can generate novel transliterations (that are LM-Unknown).</text>
              <doc_id>385</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Second, generating multiple transliterations for homograph Hindi words and using language model context helps us solve the problem of disambiguation.</text>
              <doc_id>386</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We found that the joint probability model performs almost as well as the conditional probability model but that it was more complex to make it work well.</text>
              <doc_id>387</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>9</index>
        <title>Acknowledgments</title>
        <text>The first two authors were funded by the Higher Education Commission (HEC) of Pakistan. The third author was funded by Deutsche Forschungsgemeinschaft grants SFB 732 and MorphoSynt. The fourth author was funded by Deutsche Forschungsgemeinschaft grant SFB 732.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The first two authors were funded by the Higher Education Commission (HEC) of Pakistan.</text>
              <doc_id>388</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The third author was funded by Deutsche Forschungsgemeinschaft grants SFB 732 and MorphoSynt.</text>
              <doc_id>389</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The fourth author was funded by Deutsche Forschungsgemeinschaft grant SFB 732.</text>
              <doc_id>390</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Hindi Words That Can Be Transliterated Differently in Different Contexts</caption>
        <reference_text>None</reference_text>
        <page_num>2</page_num>
        <head>
          <rows>
            <row>
              <cell>Hindi</cell>
              <cell>Urdu</cell>
              <cell>SAMPA</cell>
              <cell>Gloss</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>/</cell>
              <cell>simA</cell>
              <cell>Border/Seema</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>Amb@r</cell>
              <cell>Sky/Ambar</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>/</cell>
              <cell>vId Ze</cell>
              <cell>Victory/Vijay</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Alignment (a) Before (b) After Merge</caption>
        <reference_text>In PAGE 4: ... The character-based transliteration proba- bility is defined as follows: pc(h, u) = summationdisplay an 1?align(h,u) p(an 1) = summationdisplay an 1?align(h,u) n productdisplay i=1 p(ai|ai?1 i?k) (5) where ai is a pair consisting of the i-th Hindi char- acter hi and the sequence of 0 or more Urdu char- acters that it is aligned with. A sample alignment is shown in  Table3 (b) in section 4....  In PAGE 6: ... If there is no preceding alignment pair then it is merged with the following pair.  Table3  gives an example showing initial alignment (a) and the final align- ment (b) after applying the merge operation. Our model retains 1 ? ? and N ? ? alignments as...</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>deletion operations.</cell>
              <cell>deletion operations.</cell>
              <cell>deletion operations.</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>a)</cell>
              <cell>Hindi</cell>
              <cell>?</cell>
              <cell>b</cell>
              <cell>c</cell>
              <cell>?</cell>
              <cell>e</cell>
              <cell>f</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Urdu</cell>
              <cell>A</cell>
              <cell>XY</cell>
              <cell>C</cell>
              <cell>D</cell>
              <cell>?</cell>
              <cell>F</cell>
            </row>
            <row>
              <cell>b)</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>e</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Urdu</cell>
              <cell>None</cell>
              <cell>AXY</cell>
              <cell>CD</cell>
              <cell>None</cell>
              <cell>?</cell>
              <cell>F</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TET</source>
        <caption>Table 4: Comparing Model-1 and Model-2 with Phrase-based Systems</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>M</cell>
              <cell>Pb 0</cell>
              <cell>Pb 1</cell>
              <cell>Pb 2</cell>
              <cell>M 1</cell>
              <cell>M 2</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>BLEU</cell>
              <cell>14.3</cell>
              <cell>16.25</cell>
              <cell>16.13</cell>
              <cell>18.6</cell>
              <cell>17.05</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TET</source>
        <caption>Table 5: Applying Heuristics 1 and 2 and their Combinations to Model-1 and Model-2</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell>H 1</cell>
              <cell>H 2</cell>
              <cell>H 12</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>M 1</cell>
              <cell>18.86</cell>
              <cell>18.97</cell>
              <cell>19.35</cell>
            </row>
            <row>
              <cell>M 2</cell>
              <cell>17.56</cell>
              <cell>17.85</cell>
              <cell>18.34</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>6</id>
        <source>TableSeer</source>
        <caption>Table 5: Applying Heuristics 1 and 2 and their Combinations to Model-1 and Model-2#@#@Table 6: Applying Heuristic 3 and its Combinations with other Heuristics to Model-2</caption>
        <reference_text>None</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell>H3#@#@H 3</cell>
              <cell>H13#@#@H 13</cell>
              <cell>H23#@#@H 23</cell>
              <cell>H123#@#@H 123</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>M2#@#@M 2</cell>
              <cell>18.52</cell>
              <cell>18.93</cell>
              <cell>18.55</cell>
              <cell>19.00</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Nasreen AbdulJaleel</author>
          <author>Leah S Larkey</author>
        </authors>
        <title>Statistical transliteration for English-Arabic cross language information retrieval.</title>
        <publication>In CIKM 03: Proceedings of the twelfth international conference on Information and knowledge management,</publication>
        <pages>139--146</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Yaser Al-Onaizan</author>
          <author>Kevin Knight</author>
        </authors>
        <title>Translating named entities using monolingual and bilingual resources.</title>
        <publication>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>400--408</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Asif Ekbal</author>
          <author>Sudip Kumar Naskar</author>
          <author>Sivaji Bandyopadhyay</author>
        </authors>
        <title>A modified joint source-channel model for transliteration.</title>
        <publication>In Proceedings of the COLING/ACL poster sessions,</publication>
        <pages>191--198</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Swati Gupta</author>
        </authors>
        <title>Aligning Hindi and Urdu bilingual corpora for robust projection.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Ulf Hermjakob</author>
          <author>Kevin Knight</author>
          <author>Hal Daum&#233;</author>
        </authors>
        <title>Name translation in statistical machine translation - learning when to transliterate.</title>
        <publication>In Proceedings of ACL-08: HLT,</publication>
        <pages>389--397</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Bushra Jawaid</author>
          <author>Tafseer Ahmed</author>
        </authors>
        <title>Hindi to Urdu conversion: beyond simple transliteration.</title>
        <publication>In Conference on Language and Technology</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Mehdi M Kashani</author>
          <author>Eric Joanis</author>
          <author>Roland Kuhn</author>
          <author>George Foster</author>
          <author>Fred Popowich</author>
        </authors>
        <title>Integration of an Arabic transliteration module into a statistical machine translation system.</title>
        <publication>In Proceedings of the Second Workshop on Statistical Machine Translation,</publication>
        <pages>17--24</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Kevin Knight</author>
          <author>Jonathan Graehl</author>
        </authors>
        <title>None</title>
        <publication>Machine transliteration. Computational Linguistics,</publication>
        <pages>24--4</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Hieu Hoang</author>
          <author>Alexandra Birch</author>
          <author>Chris Callison-Burch</author>
          <author>Marcello Federico</author>
          <author>Nicola Bertoldi</author>
          <author>Brooke Cowan</author>
          <author>Wade Shen</author>
          <author>Christine Moran</author>
          <author>Richard Zens</author>
        </authors>
        <title>Chris Dyer, Ondrej Bojar,</title>
        <publication>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Demonstration Program,</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Philipp Koehn</author>
        </authors>
        <title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
        <publication>In AMTA,</publication>
        <pages>115--124</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Philipp Koehn</author>
        </authors>
        <title>Statistical significance tests for machine translation evaluation.</title>
        <publication>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</publication>
        <pages>388--395</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Haizhou Li</author>
          <author>Zhang Min</author>
          <author>Su Jian</author>
        </authors>
        <title>A joint source-channel model for machine transliteration.</title>
        <publication>In ACL &#8217;04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</publication>
        <pages>159--166</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>M G Abbas Malik</author>
          <author>Christian Boitet</author>
          <author>Pushpak Bhattacharyya</author>
        </authors>
        <title>Hindi Urdu machine transliteration using finite-state transducers.</title>
        <publication>In Proceedings of the 22nd International Conference on Computational Linguistics,</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Robert C Moore</author>
        </authors>
        <title>Fast and accurate sentence alignment of bilingual corpora.</title>
        <publication>In Conference of the Association for Machine Translation in the Americas (AMTA).</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Franz J Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A systematic comparison of various statistical alignment models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Kishore A Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>Wei-Jing Zhu</author>
        </authors>
        <title>BLEU: a method for automatic evaluation of machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Ari Pirkola</author>
          <author>Jarmo Toivonen</author>
          <author>Heikki Keskustalo</author>
          <author>Kari Visala</author>
          <author>Kalervo J&#228;rvelin</author>
        </authors>
        <title>Fuzzy translation of cross-lingual spelling variants.</title>
        <publication>In SIGIR &#8217;03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,</publication>
        <pages>345--352</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>R Mahesh K Sinha</author>
        </authors>
        <title>Developing English-Urdu machine translation via Hindi. In</title>
        <publication>Third Workshop on Computational Approaches to Arabic Scriptbased Languages (CAASL3), MT Summit XII,</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Andreas Stolcke</author>
        </authors>
        <title>SRILM - an extensible language modeling toolkit.</title>
        <publication>In Intl. Conf. Spoken Language Processing,</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Paola Virga</author>
          <author>Sanjeev Khudanpur</author>
        </authors>
        <title>Transliteration of proper names in cross-lingual information retrieval.</title>
        <publication>In Proceedings of the ACL 2003 workshop on Multilingual and mixed-language named entity recognition,</publication>
        <pages>57--64</pages>
        <date>2003</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>AbdulJaleel and Larkey, 2003</string>
        <sentence_id>29631</sentence_id>
        <char_offset>81</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>2</reference_id>
        <string>Ekbal et al., 2006</string>
        <sentence_id>29608</sentence_id>
        <char_offset>127</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Ekbal et al., 2006</string>
        <sentence_id>29674</sentence_id>
        <char_offset>234</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>3</reference_id>
        <string>Gupta, 2004</string>
        <sentence_id>29776</sentence_id>
        <char_offset>121</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>4</reference_id>
        <string>Hermjakob et al. (2008)</string>
        <sentence_id>29622</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>5</reference_id>
        <string>Jawaid and Ahmed, 2009</string>
        <sentence_id>29776</sentence_id>
        <char_offset>154</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>6</reference_id>
        <string>Kashani et al. (2007)</string>
        <sentence_id>29618</sentence_id>
        <char_offset>28</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>6</reference_id>
        <string>Kashani et al. (2007)</string>
        <sentence_id>29619</sentence_id>
        <char_offset>27</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>7</reference_id>
        <string>Knight and Graehl, 1998</string>
        <sentence_id>29608</sentence_id>
        <char_offset>85</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>8</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>29819</sentence_id>
        <char_offset>31</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>9</reference_id>
        <string>Koehn, 2004</string>
        <sentence_id>29731</sentence_id>
        <char_offset>108</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>9</reference_id>
        <string>Koehn, 2004</string>
        <sentence_id>29931</sentence_id>
        <char_offset>200</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>10</reference_id>
        <string>Koehn, 2004</string>
        <sentence_id>29731</sentence_id>
        <char_offset>108</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>10</reference_id>
        <string>Koehn, 2004</string>
        <sentence_id>29931</sentence_id>
        <char_offset>200</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>11</reference_id>
        <string>Li et al., 2004</string>
        <sentence_id>29608</sentence_id>
        <char_offset>110</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>11</reference_id>
        <string>Li et al., 2004</string>
        <sentence_id>29674</sentence_id>
        <char_offset>175</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>12</reference_id>
        <string>Malik et al., 2008</string>
        <sentence_id>29608</sentence_id>
        <char_offset>147</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>12</reference_id>
        <string>Malik et al., 2008</string>
        <sentence_id>29776</sentence_id>
        <char_offset>134</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>13</reference_id>
        <string>Moore (2002)</string>
        <sentence_id>29753</sentence_id>
        <char_offset>110</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>14</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>29754</sentence_id>
        <char_offset>66</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>15</reference_id>
        <string>Papineni et al., 2001</string>
        <sentence_id>29600</sentence_id>
        <char_offset>1</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>16</reference_id>
        <string>Pirkola et al., 2003</string>
        <sentence_id>29631</sentence_id>
        <char_offset>138</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>17</reference_id>
        <string>Sinha (2009)</string>
        <sentence_id>29628</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>18</reference_id>
        <string>Stolcke, 2002</string>
        <sentence_id>29640</sentence_id>
        <char_offset>114</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>19</reference_id>
        <string>Virga and Khudanpur, 2003</string>
        <sentence_id>29631</sentence_id>
        <char_offset>111</char_offset>
      </citation>
    </citations>
  </content>
</document>
