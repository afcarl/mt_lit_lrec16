<PAPER>
  <FILENO/>
  <TITLE>Exploring Normalization Techniques for Human Judgments of Machine Translation Adequacy Collected Using Amazon Mechanical Turk</TITLE>
  <AUTHORS>
    <AUTHOR>Michael Denkowski</AUTHOR>
  </AUTHORS>
  <ABSTRACT>
    <A-S ID="S-42521">This paper discusses a machine translation evaluation task conducted using Amazon Mechanical Turk.</A-S>
    <A-S ID="S-42522">We present a translation adequacy assessment task for untrained Arabicspeaking annotators and discuss several techniques for normalizing the resulting data.</A-S>
    <A-S ID="S-42523">We present a novel 2-stage normalization technique shown to have the best performance on this task and further discuss the results of all techniques and the usability of the resulting adequacy scores.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-42524">Human judgments of translation quality play a vital role in the development of effective machine translation (MT) systems.</S>
        <S ID="S-42525">Such judgments can be used to measure system quality in evaluations (Callison- Burch et al., 2009) and to tune automatic metrics such as METEOR (<REF ID="R-00" RPTR="0">Banerjee and Lavie, 2005</REF>) which act as stand-ins for human evaluators.</S>
        <S ID="S-42526">However, collecting reliable human judgments often requires significant time commitments from expert annotators, leading to a general scarcity of judgments and a significant time lag when seeking judgments for new tasks or languages.</S>
        <S ID="S-42527">Amazon&#8217;s Mechanical Turk (MTurk) service facilitates inexpensive collection of large amounts of data from users around the world.</S>
        <S ID="S-42528">However, Turkers are not trained to provide reliable annotations for natural language processing (NLP) tasks, and some Turkers attempt to game the system by submitting random answers.</S>
        <S ID="S-42529">For these reasons, NLP tasks must be designed to be accessible to untrained users and data normalization techniques must be employed to ensure that the data collected is usable.</S>
        <S ID="S-42530">This paper describes a MT evaluation task for translations of English into Arabic conducted using MTurk and compares several data normalization techniques.</S>
        <S ID="S-42531">A novel 2-stage normalization technique is demonstrated to produce the highest agreement between Turkers and experts while retaining enough judgments to provide a robust tuning set for automatic evaluation metrics.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Data Set</HEADER>
      <P>
        <S ID="S-42552">Our data set consists of human adequacy judgments for automatic translations of 1314 English sentences into Arabic.</S>
        <S ID="S-42553">The English source sentences and Arabic reference translations are taken from the Arabic- English sections of the NIST Open Machine Translation Evaluation (<REF ID="R-03" RPTR="4">Garofolo, 2001</REF>) data sets for 2002 through 2005.</S>
        <S ID="S-42554">Selected sentences are between 10 and 20 words in length on the Arabic side.</S>
        <S ID="S-42555">Arabic machine translation (MT) hypotheses are obtained by passing the English sentences through Google&#8217;s free online translation service.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>2.1 Data Collection</HEADER>
        <P>
          <S ID="S-42532">Human judgments of translation adequacy are collected for each of the 1314 Arabic MT output hypotheses.</S>
          <S ID="S-42533">Given a translation hypothesis and the corresponding reference translation, annotators are asked to assign an adequacy score according to the following scale:</S>
        </P>
        <P>
          <S ID="S-42534">4 &#8211; Hypothesis is completely meaning equivalent with the reference translation.</S>
        </P>
        <P>
          <S ID="S-42535">3 &#8211; Hypothesis captures more than half of meaning of the reference translation.</S>
          <S ID="S-42536">&#8226; &#8710;: For judgments (J = j 1 ...j n ) and gold standard (G = g 1 ...g n ), we define average distance:</S>
        </P>
        <P>
          <S ID="S-42537">2 &#8211; Hypothesis captures less than half of meaning of the reference translation.</S>
          <S ID="S-42538">&#8710;(J, G) =</S>
        </P>
        <P>
          <S ID="S-42539">&#8721; ni=1 |g i &#8722; j i | n</S>
        </P>
        <P>
          <S ID="S-42540">1 &#8211; Hypothesis captures no meaning of the reference translation.</S>
        </P>
        <P>
          <S ID="S-42541">Adequacy judgments are collected from untrained Arabic-speaking annotators using Amazon&#8217;s Mechanical Turk (MTurk) service.</S>
          <S ID="S-42542">We create a human intelligence task (HIT) type that presents Turkers with a MT hypothesis/reference pair and asks for an adequacy judgment.</S>
          <S ID="S-42543">To make this task accessible to non-experts, the traditional definitions of adequacy scores are replaced with the following: (4) excellent, (3) good, (2) bad, (1) very bad.</S>
          <S ID="S-42544">Each rating is accompanied by an example from the data set which fits the corresponding criteria from the traditional scale.</S>
          <S ID="S-42545">To make this task accessible to the Arabic speakers we would like to complete the HITs, the instructions are provided in Arabic as well as English.</S>
          <S ID="S-42546">To allow experimentation with various data normalization techniques, we collect judgments from 10 unique Turkers for each of the translations.</S>
          <S ID="S-42547">We also ask an expert to provide &#8220;gold standard&#8221; judgments for 101 translations drawn uniformly from the data.</S>
          <S ID="S-42548">These 101 translations are recombined with the data and repeated such that every 6th translation has a gold standard judgment, resulting in a total of 1455 HITs.</S>
          <S ID="S-42549">We pay Turkers $0.01 per HIT and Amazon fees of $0.005 per HIT, leading to a total cost of $218.25 for data collection and an effective cost of $0.015 per judgment.</S>
          <S ID="S-42550">Despite requiring Arabic speakers, our HITs are completed at a rate of 1000- 3000 per day.</S>
          <S ID="S-42551">It should be noted that the vast majority of Turkers working on our HITs are located in India, with fewer in Arabic-speaking countries such as Egypt and Syria.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Normalization Techniques</HEADER>
      <P>
        <S ID="S-42579">We apply multiple normalization techniques to the data set and evaluate their relative performance.</S>
        <S ID="S-42580">Several techniques use the following measures:</S>
      </P>
      <P>
        <S ID="S-42581">&#8226; K: For two annotators, Cohen&#8217;s kappa coefficient (Smeeton, 1985) is defined:</S>
      </P>
      <P>
        <S ID="S-42582">K =</S>
      </P>
      <P>
        <S ID="S-42583">P (A) &#8722; P (E) 1 &#8722; P (E)</S>
      </P>
      <P>
        <S ID="S-42584">where P (A) is the proportion of times that annotators agree and P (E) is the proportion of times that agreement is expected by chance.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>3.1 Straight Average</HEADER>
        <P>
          <S ID="S-42556">The baseline approach consists of keeping all judgments and taking the straight average on a pertranslation basis without additional normalization.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.2 Removing Low-Agreement Judges</HEADER>
        <P>
          <S ID="S-42557">Following <REF ID="R-01" RPTR="2">Callison-Burch et al. (2009)</REF>, we calculate pairwise inter-annotator agreement (P (A)) of each annotator with all others and remove judgments from annotators with P (A) below some threshold.</S>
          <S ID="S-42558">We set this threshold such that the highest overall agreement can be achieved while retaining at least one judgment for each translation.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.3 Removing Outlying Judgments</HEADER>
        <P>
          <S ID="S-42559">For a given translation and human judgments (j 1 ...j n ), we calculate the distance (&#948;) of each judgment from the mean (&#175;j):</S>
        </P>
        <P>
          <S ID="S-42560">&#948;(j i ) = |j i &#8722; &#175;j|</S>
        </P>
        <P>
          <S ID="S-42561">We then remove outlying judgments with &#948;(j i ) exceeding some threshold.</S>
          <S ID="S-42562">This threshold is also set such that the highest agreement is achieved while retaining at least one judgment per translation.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.4 Weighted Voting</HEADER>
        <P>
          <S ID="S-42563">Following <REF ID="R-02" RPTR="3">Callison-Burch (2009)</REF>, we treat evaluation as a weighted voting problem where each annotator&#8217;s contribution is weighted by agreement with either a gold standard or with other annotators.</S>
          <S ID="S-42564">For this evaluation, we weigh contribution by P (A) with the 101 gold standard judgments.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.5 Scaling Judgments</HEADER>
        <P>
          <S ID="S-42565">To account for the notion that some annotators judge translations more harshly than others, we apply perannotator scaling to the adequacy judgments based on annotators&#8217; signed distance from gold standard judgments.</S>
          <S ID="S-42566">For judgments (J = j 1 ...j n ) and gold standard (G = g 1 ...g n ), an additive scaling factor is calculated: &#8721; ni=1</S>
        </P>
        <P>
          <S ID="S-42567">g i &#8722; j i &#955; + (J, G) = n Adding this scaling factor to each judgment has the effect of shifting the judgments&#8217; center of mass to match that of the gold standard.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.6 2-Stage Technique</HEADER>
        <P>
          <S ID="S-42568">We combine judgment scaling with weighted voting to produce a 2-stage normalization technique addressing two types of divergence in Turker judgments from the gold standard.</S>
          <S ID="S-42569">Divergence can be either consistent, where Turkers regularly assign higher or lower scores than experts, or random, where Turkers guess blindly or do not understand the task.</S>
        </P>
        <P>
          <S ID="S-42570">Stage 1: Given a gold standard (G = g 1 ...g n ), consistent divergences are corrected by calculating &#955; + (J, G) for each annotator&#8217;s judgments (J = j i ...j n ) and applying &#955; + (J, G) to each j i to produce adjusted judgment set J &#8242; .</S>
          <S ID="S-42571">If &#8710;(J &#8242; , G) &lt; &#8710;(J, G), where &#8710;(J, G) is defined in Section 3, the annotator is considered consistently divergent and J &#8242; is used in place of J. Inconsistently divergent annotators&#8217; judgments are unaffected by this stage.</S>
        </P>
        <P>
          <S ID="S-42572">Stage 2: All annotators are considered in a weighted voting scenario.</S>
          <S ID="S-42573">In this case, annotator contribution is determined by a distance measure similar to the kappa coefficient.</S>
          <S ID="S-42574">For judgments (J = j 1 ...j n ) and gold standard (G = g 1 ...g n ), we define:</S>
        </P>
        <P>
          <S ID="S-42575">K &#8710; (J, G) = (max &#8710; &#8722; &#8710;(J, G)) &#8722; E(&#8710;) max &#8710; &#8722; E(&#8710;)</S>
        </P>
        <P>
          <S ID="S-42576">where max &#8710; is the average maximum distance between judgments and E(&#8710;) is the expected distance between judgments.</S>
          <S ID="S-42577">Perfect agreement with the gold standard produces K &#8710; = 1 while chance agreement produces K &#8710; = 0.</S>
          <S ID="S-42578">Annotators with K &#8710; &#8804; 0 are removed from the voting pool and final scores are calculated as the weighted averages of judgments from all remaining annotators.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 Results</HEADER>
      <P>
        <S ID="S-42592">Table 1 outlines the performance of all normalization techniques.</S>
        <S ID="S-42593">To calculate P (A) and K with the gold standard, final adequacy scores are rounded to the nearest whole number.</S>
        <S ID="S-42594">As shown in the table, removing low-agreement annotators or outlying judgments greatly improves Turker agreement and, in the case of removing judgments, decreases distance from the gold standard.</S>
        <S ID="S-42595">However, these approaches remove a large portion of the judgments, leaving a skewed data set.</S>
        <S ID="S-42596">When removing judgments, 1172 of the 1314 translations receive a score of 3, making tasks such as tuning automatic metrics infeasible.</S>
      </P>
      <P>
        <S ID="S-42597">Weighing votes by agreement with the gold standard retains most judgments, though neither Turker agreement nor agreement with the gold standard improves.</S>
        <S ID="S-42598">The scaling approach retains all judgments and slightly improves correlation and &#8710;, though K decreases.</S>
        <S ID="S-42599">As scaled judgments are not whole numbers, Turker P (A) and K are not applicable.</S>
      </P>
      <P>
        <S ID="S-42600">The 2-stage approach outperforms all other techniques when compared against the gold standard, being the only technique to significantly raise correlation.</S>
        <S ID="S-42601">Over 90% of the judgments are used, as shown in Figure 1.</S>
        <S ID="S-42602">Further, the distribution of final adequacy scores (shown in Figure 2) resembles a normal distribution, allowing this data to be used for tuning automatic evaluation metrics.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>4.1 Resistance to Randomness</HEADER>
        <P>
          <S ID="S-42585">To verify that our 2-stage technique handles problematic data properly, we simulate user data from 5 unreliable Turkers.</S>
          <S ID="S-42586">Turkers &#8220;Uniform-a&#8221; and &#8220;Uniform-b&#8221; draw answers randomly from a uniform distribution.</S>
          <S ID="S-42587">&#8220;Gaussian&#8221; Turkers draw answers randomly from Gaussian distributions with &#963; = 1 and &#181; according to name.</S>
          <S ID="S-42588">Each &#8220;Turker&#8221; contributes one judgment for each translation.</S>
          <S ID="S-42589">As shown in Ta-</S>
        </P>
        <P>
          <S ID="S-42590">ble 2, only Gaussian-2.5 receives substantial weight while the others receive low or zero weight.</S>
          <S ID="S-42591">This follows from the fact that the actual data follows a similar distribution, and thus the random Turkers have negligible impact on the final distribution of scores.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Conclusions and Future Work</HEADER>
      <P>
        <S ID="S-42603">We have presented an Arabic MT evaluation task conducted using Amazon MTurk and discussed several possibilities for normalizing the collected data.</S>
        <S ID="S-42604">Our 2-stage normalization technique has been shown to provide the highest agreement between Turkers and experts while retaining enough judgments to avoid problems of data sparsity and appropriately down-weighting random data.</S>
        <S ID="S-42605">As we currently have a single set of expert judgments, our future work involves collecting additional judgments from multiple experts against which to further test our techniques.</S>
        <S ID="S-42606">We then plan to use normalized</S>
      </P>
      <P>
        <S ID="S-42607">normalization Turker adequacy judgments to tune an Arabic version of the METEOR (<REF ID="R-00" RPTR="1">Banerjee and Lavie, 2005</REF>) MT evaluation metric.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS/>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>Satanjeev Banerjee</RAUTHOR>
      <REFTITLE>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>Chris Callison-Burch</RAUTHOR>
      <REFTITLE>Findings of WMT09. In</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>Chris Callison-Burch</RAUTHOR>
      <REFTITLE>Fast, cheap, and creative: Evaluating translation quality using Amazon&#8217;s Mechanical Turk. In</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>John Garofolo</RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2001</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
