<document>
  <filename>P09-1105</filename>
  <authors/>
  <title>Confidence Measure for Word Alignment</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>In this paper we present a confidence measure for word alignment based on the posterior probability of alignment links. We introduce sentence alignment confidence measure and alignment link confidence measure. Based on these measures, we improve the alignment quality by selecting high confidence sentence alignments and alignment links from multiple word alignments of the same sentence pair. Additionally, we remove low confidence alignment links from the word alignment of a bilingual training corpus, which increases the alignment F-score, improves Chinese-English and Arabic-English translation quality and significantly reduces the phrase translation table size.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper we present a confidence measure for word alignment based on the posterior probability of alignment links.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We introduce sentence alignment confidence measure and alignment link confidence measure.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Based on these measures, we improve the alignment quality by selecting high confidence sentence alignments and alignment links from multiple word alignments of the same sentence pair.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Additionally, we remove low confidence alignment links from the word alignment of a bilingual training corpus, which increases the alignment F-score, improves Chinese-English and Arabic-English translation quality and significantly reduces the phrase translation table size.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Data-driven approaches have been quite active in recent machine translation (MT) research. Many MT systems, such as statistical phrase-based and syntax-based systems, learn phrase translation pairs or translation rules from large amount of bilingual data with word alignment. The quality of the parallel data and the word alignment have significant impacts on the learned translation models and ultimately the quality of translation output. Due to the high cost of commissioned translation, many parallel sentences are automatically extracted from comparable corpora, which inevitably introduce many &#8221;noises&#8221;, i.e., inaccurate or non-literal translations. Given the huge amount of bilingual training data, word alignments are automatically generated using various algorithms ((Brown et al., 1994), (Vogel et al., 1996) Figure 1: An example of inaccurate translation and word alignment.
and (Ittycheriah and Roukos, 2005)), which also introduce many word alignment errors.
The example in Figure 1 shows the word alignment of the given Chinese and English sentence pair, where the English words following each Chinese word is its literal translation. We find untranslated Chinese and English words (marked with underlines). These spurious words cause significant word alignment errors (as shown with dash lines), which in turn directly affect the quality of phrase translation tables or translation rules that are learned based on word alignment.
In this paper we introduce a confidence measure for word alignment, which is robust to extra or missing words in the bilingual sentence pairs, as well as word alignment errors. We propose a sentence alignment confidence measure based on the alignment&#8217;s posterior probability, and extend it to the alignment link confidence measure. We illustrate the correlation between the alignment confidence measure and the alignment quality on the sentence level, and present several approaches to improve alignment accuracy based on the proposed confidence measure: sentence alignment selection, alignment link combination and alignment link filtering. Finally we demonstrate
the improved alignments also lead to better MT quality. The paper is organized as follows: In section 2 we introduce the sentence and alignment link confidence measures. In section 3 we demonstrate two approaches to improve alignment accuracy through alignment combination. In section 4 we show how to improve a MaxEnt word alignment quality by removing low confidence alignment links, which also leads to improved translation quality as shown in section 5.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Data-driven approaches have been quite active in recent machine translation (MT) research.</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Many MT systems, such as statistical phrase-based and syntax-based systems, learn phrase translation pairs or translation rules from large amount of bilingual data with word alignment.</text>
              <doc_id>5</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The quality of the parallel data and the word alignment have significant impacts on the learned translation models and ultimately the quality of translation output.</text>
              <doc_id>6</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Due to the high cost of commissioned translation, many parallel sentences are automatically extracted from comparable corpora, which inevitably introduce many &#8221;noises&#8221;, i.e., inaccurate or non-literal translations.</text>
              <doc_id>7</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Given the huge amount of bilingual training data, word alignments are automatically generated using various algorithms ((Brown et al., 1994), (Vogel et al., 1996) Figure 1: An example of inaccurate translation and word alignment.</text>
              <doc_id>8</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and (Ittycheriah and Roukos, 2005)), which also introduce many word alignment errors.</text>
              <doc_id>9</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The example in Figure 1 shows the word alignment of the given Chinese and English sentence pair, where the English words following each Chinese word is its literal translation.</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We find untranslated Chinese and English words (marked with underlines).</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>These spurious words cause significant word alignment errors (as shown with dash lines), which in turn directly affect the quality of phrase translation tables or translation rules that are learned based on word alignment.</text>
              <doc_id>12</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this paper we introduce a confidence measure for word alignment, which is robust to extra or missing words in the bilingual sentence pairs, as well as word alignment errors.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We propose a sentence alignment confidence measure based on the alignment&#8217;s posterior probability, and extend it to the alignment link confidence measure.</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We illustrate the correlation between the alignment confidence measure and the alignment quality on the sentence level, and present several approaches to improve alignment accuracy based on the proposed confidence measure: sentence alignment selection, alignment link combination and alignment link filtering.</text>
              <doc_id>15</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Finally we demonstrate</text>
              <doc_id>16</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the improved alignments also lead to better MT quality.</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The paper is organized as follows: In section 2 we introduce the sentence and alignment link confidence measures.</text>
              <doc_id>18</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In section 3 we demonstrate two approaches to improve alignment accuracy through alignment combination.</text>
              <doc_id>19</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In section 4 we show how to improve a MaxEnt word alignment quality by removing low confidence alignment links, which also leads to improved translation quality as shown in section 5.</text>
              <doc_id>20</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Sentence Alignment Confidence Measure</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>21</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Definition</title>
            <text>Given a bilingual sentence pair (S,T ) where S={s 1 ,. . . , s I } is the source sentence and T ={t 1 , . . . ,t J } is the target sentence. Let A = {a ij } be the alignment between S and T . The alignment confidence measure C(A|S, T ) is defined as the geometric mean of the alignment posterior probabilities calculated in both directions:
C(A|S, T ) = &#8730; P s2t (A|S, T )P t2s (A|T, S), (1)
where
P s2t (A|S, T ) = P (A, T |S)
&#8721;A &#8242; P (A&#8242; , T |S) . (2)
When computing the source-to-target alignment posterior probability, the numerator is the sentence translation probability calculated according to the given alignment A:
P (A, T |S) = J&#8719;
p(t j |s i , a ij &#8712; A). (3)
j=1
It is the product of lexical translation probabilities for the aligned word pairs. For unaligned target word t j , consider s i = NULL. The source-totarget lexical translation model p(t|s) and targetto-source model p(s|t) can be obtained through IBM Model-1 or HMM training. The denominator is the sentence translation probability summing over all possible alignments, which can be calculated similar to IBM Model 1 in (Brown et al., 1994):
&#8721;
A &#8242; P (A &#8242; , T |S) =
J&#8719;
j=1 i=1
I&#8721; p(t j |s i ). (4) Aligner F-score Cor. Coeff.
Note that here only the word-based lexicon model is used to compute the confidence measure. More complex models such as alignment models, fertility models and distortion models as described in (Brown et al., 1994) could estimate the probability of a given alignment more accurately. However the summation over all possible alignments is very complicated, even intractable, with the richer models. For the efficient computation of the denominator, we use the lexical translation model.
Similarly,
and
P t2s (A|T, S) =
P (A, S|T ) =
&#8721; P (A, S|T )
&#8721;A &#8242; P (A&#8242; , S|T ) , (5)
I&#8719; p(s i |t j , a ij &#8712; A). (6)
i=1
A &#8242; P (A &#8242; , S|T ) =
I&#8719;
i=1 j=1
J&#8721; p(s i |t j ). (7)
We randomly selected 512 Chinese-English (C- E) sentence pairs and generated word alignment using the MaxEnt aligner (Ittycheriah and Roukos, 2005). We evaluate per sentence alignment F- scores by comparing the system output with a reference alignment. For each sentence pair, we also calculate the sentence alignment confidence score &#8722; log C(A|S, T ). We compute the correlation coefficients between the alignment confidence measure and the alignment F-scores. The results in Figure 2 shows strong correlation between the confidence measure and the alignment F-score, with the correlation coefficients equals to -0.69. Such strong correlation is also observed on an HMM alignment (Ge, 2004) and a Block Model (BM) alignment (Zhao et al., 2005) with varying alignment accuracies, as seen in Table1.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Given a bilingual sentence pair (S,T ) where S={s 1 ,.</text>
                  <doc_id>22</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>23</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>24</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>, s I } is the source sentence and T ={t 1 , .</text>
                  <doc_id>25</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>26</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>27</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>,t J } is the target sentence.</text>
                  <doc_id>28</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Let A = {a ij } be the alignment between S and T .</text>
                  <doc_id>29</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>The alignment confidence measure C(A|S, T ) is defined as the geometric mean of the alignment posterior probabilities calculated in both directions:</text>
                  <doc_id>30</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>C(A|S, T ) = &#8730; P s2t (A|S, T )P t2s (A|T, S), (1)</text>
                  <doc_id>31</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where</text>
                  <doc_id>32</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P s2t (A|S, T ) = P (A, T |S)</text>
                  <doc_id>33</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;A &#8242; P (A&#8242; , T |S) .</text>
                  <doc_id>34</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(2)</text>
                  <doc_id>35</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>When computing the source-to-target alignment posterior probability, the numerator is the sentence translation probability calculated according to the given alignment A:</text>
                  <doc_id>36</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (A, T |S) = J&#8719;</text>
                  <doc_id>37</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(t j |s i , a ij &#8712; A).</text>
                  <doc_id>38</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(3)</text>
                  <doc_id>39</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=1</text>
                  <doc_id>40</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It is the product of lexical translation probabilities for the aligned word pairs.</text>
                  <doc_id>41</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For unaligned target word t j , consider s i = NULL.</text>
                  <doc_id>42</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The source-totarget lexical translation model p(t|s) and targetto-source model p(s|t) can be obtained through IBM Model-1 or HMM training.</text>
                  <doc_id>43</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The denominator is the sentence translation probability summing over all possible alignments, which can be calculated similar to IBM Model 1 in (Brown et al., 1994):</text>
                  <doc_id>44</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;</text>
                  <doc_id>45</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A &#8242; P (A &#8242; , T |S) =</text>
                  <doc_id>46</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>J&#8719;</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=1 i=1</text>
                  <doc_id>48</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>I&#8721; p(t j |s i ).</text>
                  <doc_id>49</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(4) Aligner F-score Cor.</text>
                  <doc_id>50</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Coeff.</text>
                  <doc_id>51</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Note that here only the word-based lexicon model is used to compute the confidence measure.</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>More complex models such as alignment models, fertility models and distortion models as described in (Brown et al., 1994) could estimate the probability of a given alignment more accurately.</text>
                  <doc_id>53</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However the summation over all possible alignments is very complicated, even intractable, with the richer models.</text>
                  <doc_id>54</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For the efficient computation of the denominator, we use the lexical translation model.</text>
                  <doc_id>55</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Similarly,</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and</text>
                  <doc_id>57</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P t2s (A|T, S) =</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (A, S|T ) =</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; P (A, S|T )</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;A &#8242; P (A&#8242; , S|T ) , (5)</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>I&#8719; p(s i |t j , a ij &#8712; A).</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(6)</text>
                  <doc_id>63</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A &#8242; P (A &#8242; , S|T ) =</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>I&#8719;</text>
                  <doc_id>66</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1 j=1</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>J&#8721; p(s i |t j ).</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(7)</text>
                  <doc_id>69</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We randomly selected 512 Chinese-English (C- E) sentence pairs and generated word alignment using the MaxEnt aligner (Ittycheriah and Roukos, 2005).</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We evaluate per sentence alignment F- scores by comparing the system output with a reference alignment.</text>
                  <doc_id>71</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For each sentence pair, we also calculate the sentence alignment confidence score &#8722; log C(A|S, T ).</text>
                  <doc_id>72</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We compute the correlation coefficients between the alignment confidence measure and the alignment F-scores.</text>
                  <doc_id>73</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The results in Figure 2 shows strong correlation between the confidence measure and the alignment F-score, with the correlation coefficients equals to -0.69.</text>
                  <doc_id>74</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Such strong correlation is also observed on an HMM alignment (Ge, 2004) and a Block Model (BM) alignment (Zhao et al., 2005) with varying alignment accuracies, as seen in Table1.</text>
                  <doc_id>75</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Sentence Alignment Selection Based on Confidence Measure</title>
            <text>The strong correlation between the sentence alignment confidence measure and the alignment F-
the higher confidence the link has. Similarly, the target-to-source link posterior probability is defined as:
q t2s (a ij |T, S) =
p(s i |t j ) &#8721; I
i &#8242; =1 p(s i &#8242;|t j) . (10)
measure suggests the possibility of selecting the alignment with the highest confidence score to obtain better alignments. For each sentence pair in the C-E test set, we calculate the confidence scores of the HMM alignment, the Block Model alignment and the MaxEnt alignment, then select the alignment with the highest confidence score. As a result, 82% of selected alignments have higher F- scores, and the F-measure of the combined alignments is increased over the best aligner (the Max- Ent aligner) by 0.8. This relatively small improvement is mainly due to the selection of the whole sentence alignment: for many sentences the best alignment still contains alignment errors, some of which could be fixed by other aligners. Therefore, it is desirable to combine alignment links from different alignments.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The strong correlation between the sentence alignment confidence measure and the alignment F-</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the higher confidence the link has.</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Similarly, the target-to-source link posterior probability is defined as:</text>
                  <doc_id>78</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>q t2s (a ij |T, S) =</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(s i |t j ) &#8721; I</text>
                  <doc_id>80</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i &#8242; =1 p(s i &#8242;|t j) .</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(10)</text>
                  <doc_id>82</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>measure suggests the possibility of selecting the alignment with the highest confidence score to obtain better alignments.</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For each sentence pair in the C-E test set, we calculate the confidence scores of the HMM alignment, the Block Model alignment and the MaxEnt alignment, then select the alignment with the highest confidence score.</text>
                  <doc_id>84</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As a result, 82% of selected alignments have higher F- scores, and the F-measure of the combined alignments is increased over the best aligner (the Max- Ent aligner) by 0.8.</text>
                  <doc_id>85</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This relatively small improvement is mainly due to the selection of the whole sentence alignment: for many sentences the best alignment still contains alignment errors, some of which could be fixed by other aligners.</text>
                  <doc_id>86</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, it is desirable to combine alignment links from different alignments.</text>
                  <doc_id>87</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Alignment Link Confidence Measure</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>88</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Definition</title>
            <text>Similar to the sentence alignment confidence measure, the confidence of an alignment link a ij in the sentence pair (S, T ) is defined as &#8730;
c(a ij |S, T ) = q s2t (a ij |S, T )q t2s (a ij |T, S) (8) where the source-to-target link posterior probability
q s2t (a ij |S, T ) =
p(t j |s i ) &#8721; J
j &#8242; =1 p(t j &#8242;|s i) , (9)
which is defined as the word translation probability of the aligned word pair divided by the sum of the translation probabilities over all the target words in the sentence. The higher p(t j |s i ) is,
Intuitively, the above link confidence definition compares the lexical translation probability of the aligned word pair with the translation probabilities of all the target words given the source word. If a word t occurs N times in the target sentence, for any i &#8712; {1, ..., I},
J&#8721; p(t j &#8242;|s i ) &#8805; Np(t|s i ),
j &#8242; =1
thus for any t j = t,
q s2t (a ij ) &#8804; 1 N .
This indicates that the confidence score of any link connecting t j to any source word is at most 1/N. On the one hand this is expected because multiple occurrences of the same word does increase the confusion for word alignment and reduce the link confidence. On the other hand, additional information (such as the distance of the word pair, the alignment of neighbor words) could indicate higher likelihood for the alignment link. We will introduce a context-dependent link confidence measure in section 4.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Similar to the sentence alignment confidence measure, the confidence of an alignment link a ij in the sentence pair (S, T ) is defined as &#8730;</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c(a ij |S, T ) = q s2t (a ij |S, T )q t2s (a ij |T, S) (8) where the source-to-target link posterior probability</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>q s2t (a ij |S, T ) =</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(t j |s i ) &#8721; J</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j &#8242; =1 p(t j &#8242;|s i) , (9)</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>which is defined as the word translation probability of the aligned word pair divided by the sum of the translation probabilities over all the target words in the sentence.</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The higher p(t j |s i ) is,</text>
                  <doc_id>95</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Intuitively, the above link confidence definition compares the lexical translation probability of the aligned word pair with the translation probabilities of all the target words given the source word.</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If a word t occurs N times in the target sentence, for any i &#8712; {1, ..., I},</text>
                  <doc_id>97</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>J&#8721; p(t j &#8242;|s i ) &#8805; Np(t|s i ),</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j &#8242; =1</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>thus for any t j = t,</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>q s2t (a ij ) &#8804; 1 N .</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>This indicates that the confidence score of any link connecting t j to any source word is at most 1/N.</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>On the one hand this is expected because multiple occurrences of the same word does increase the confusion for word alignment and reduce the link confidence.</text>
                  <doc_id>103</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>On the other hand, additional information (such as the distance of the word pair, the alignment of neighbor words) could indicate higher likelihood for the alignment link.</text>
                  <doc_id>104</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We will introduce a context-dependent link confidence measure in section 4.</text>
                  <doc_id>105</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Alignment Link Selection</title>
            <text>From multiple alignments of the same sentence pair, we select high confidence links from different alignments based on their link confidence scores and alignment agreement ratio.
Typically, links appearing in multiple alignments are more likely correct alignments. The alignment agreement ratio measures the popularity of a link. Suppose the sentence pair (S, T ) have alignments A 1 ,. . . , A D , the agreement ratio of a link a ij is defined as
r(a ij |S, T ) = &#8721;
d C(A d|S, T : a ij &#8712; A d ) &#8721;d &#8242; C(A d &#8242;|S, T ) , (11)
where C(A) is the confidence score of the alignment A as defined in formula 1. This formula computes the sum of the alignment confidence scores for the alignments containing a ij , which is
normalized by the sum of all alignments&#8217; confidence scores.
We collect all the links from all the alignments. For each link we calculate the link confidence score c(a ij ) and the alignment agreement ratio r(a ij ). We link the word pair (s i , t j ) if either c(a ij ) &gt; h 1 or r(a ij ) &gt; r 1 , where h 1 and r 1 are empirically chosen thresholds.
We combine the HMM alignment, the BM alignment and the MaxEnt alignment (ME) using the above link selection algorithm. Figure 3 shows such an example, where alignment errors in the MaxEnt alignment are shown with dotted lines. As some of the links are correctly aligned in the HMM and BM alignments (shown with solid lines), the combined alignment corrects some alignment errors while still contains common incorrect alignment links.
Table 2 shows the precision, recall and F-score of individual alignments and the combined alignment. F-content and F-function are the F-scores for content words and function words, respectively. The link selection algorithm improves the recall over the best aligner (the ME alignment) by 7 points (from 65.4 to 72.5) while decreasing the precision by 4.4 points (from 73.6 to 69.2). Overall it improves the F-score by 1.5 points (from 69.3 to 70.8), 1.8 point improvement for content words and 1.0 point for function words. It also significantly outperforms the traditionally used heuristics, &#8221;intersection-union-refine&#8221; (Och and Ney, 2003) by 6 points.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>From multiple alignments of the same sentence pair, we select high confidence links from different alignments based on their link confidence scores and alignment agreement ratio.</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Typically, links appearing in multiple alignments are more likely correct alignments.</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The alignment agreement ratio measures the popularity of a link.</text>
                  <doc_id>108</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Suppose the sentence pair (S, T ) have alignments A 1 ,.</text>
                  <doc_id>109</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>110</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>111</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>, A D , the agreement ratio of a link a ij is defined as</text>
                  <doc_id>112</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r(a ij |S, T ) = &#8721;</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>d C(A d|S, T : a ij &#8712; A d ) &#8721;d &#8242; C(A d &#8242;|S, T ) , (11)</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where C(A) is the confidence score of the alignment A as defined in formula 1.</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This formula computes the sum of the alignment confidence scores for the alignments containing a ij , which is</text>
                  <doc_id>116</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>normalized by the sum of all alignments&#8217; confidence scores.</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We collect all the links from all the alignments.</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For each link we calculate the link confidence score c(a ij ) and the alignment agreement ratio r(a ij ).</text>
                  <doc_id>119</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We link the word pair (s i , t j ) if either c(a ij ) &gt; h 1 or r(a ij ) &gt; r 1 , where h 1 and r 1 are empirically chosen thresholds.</text>
                  <doc_id>120</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We combine the HMM alignment, the BM alignment and the MaxEnt alignment (ME) using the above link selection algorithm.</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 3 shows such an example, where alignment errors in the MaxEnt alignment are shown with dotted lines.</text>
                  <doc_id>122</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As some of the links are correctly aligned in the HMM and BM alignments (shown with solid lines), the combined alignment corrects some alignment errors while still contains common incorrect alignment links.</text>
                  <doc_id>123</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 2 shows the precision, recall and F-score of individual alignments and the combined alignment.</text>
                  <doc_id>124</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>F-content and F-function are the F-scores for content words and function words, respectively.</text>
                  <doc_id>125</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The link selection algorithm improves the recall over the best aligner (the ME alignment) by 7 points (from 65.4 to 72.5) while decreasing the precision by 4.4 points (from 73.6 to 69.2).</text>
                  <doc_id>126</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Overall it improves the F-score by 1.5 points (from 69.3 to 70.8), 1.8 point improvement for content words and 1.0 point for function words.</text>
                  <doc_id>127</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>It also significantly outperforms the traditionally used heuristics, &#8221;intersection-union-refine&#8221; (Och and Ney, 2003) by 6 points.</text>
                  <doc_id>128</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Improved MaxEnt Aligner with Confidence-based Link Filtering</title>
        <text>In addition to the alignment combination, we also improve the performance of the MaxEnt aligner through confidence-based alignment link filtering. Here we select the MaxEnt aligner because it has
the highest F-measure among the three aligners, although the algorithm described below can be applied to any aligner.
It is often observed that words within a constituent (such as NP, PP) are typically translated together, and their alignments are close. As a result the confidence measure of an alignment link a ij can be boosted given the alignment of its context words. From the initial sentence alignment we first identify an anchor link a mn , the high confidence alignment link closest to a ij . The anchor link is considered as the most reliable connection between the source and target context. The context is then defined as a window centering at a mn with window width proportional to the distance between a ij and a mn . When computing the context-dependent link confidence, we only consider words within the context window. The context-dependent alignment link confidence is calculated in the following steps:
1. Calculate the context-independent link confidence measure c(a ij ) according to formula (8).
2. Sort all links based on their link confidence measures in decreasing order.
3. Select links whose confidence scores are higher than an empirically chosen threshold H as anchor links 1 .
4. Walking along the remaining sorted links. For each link {a ij : c(a ij ) &lt; H},
(a) Find the closest anchor link a mn 2 , (b) Define the context window width w =
|m &#8722; i| + |n &#8722; j|.
1 H is selected to maximize the F-score on an alignment
devset. 2 When two equally close alignment links have the same
confidence score), we randomly select one of the tied links as the anchor link.
(c) Compute the link posterior probabilities within the context window:
q s2t (a ij |a mn ) =
q t2s (a ij |a mn ) =
p(t j |s i ) &#8721; j+w
j &#8242; =j&#8722;w p(t j &#8242;|s i) ,
p(s i |t j ) &#8721; i+w
i &#8242; =i&#8722;w p(s i &#8242;|t j) .
(d) Compute the context-dependent link confidence score c(a ij |a mn ) =
&#8730; q s2t (a ij |a mn )q t2s (a ij |a mn ).
If c(a ij |a mn ) &gt; H, add a ij into the set of anchor links.
5. Only keep anchor links and remove all the remaining links with low confidence scores.
The above link filtering algorithm is designed to remove incorrect links. Furthermore, it is possible to create new links by relinking unaligned source and target word pairs within the context window if their context-dependent link posterior probability is high. Figure 4 shows context-independent link confidence scores for the given sentence alignment. The subscript following each word indicates the word&#8217;s position. Incorrect alignment links are shown with dashed lines, which have low confidence scores (a 5,7 , a 7,3 , a 8,2 , a 11,9 ) and will be removed through filtering. When the anchor link a 4,11 is selected, the context-dependent link confidence of a 6,12 is increased from 0.12 to 0.51. Also note that a new link a 7,12 (shown as a dotted line) is created because within the context window, the link confidence score is as high as 0.96. This example shows that the context-dependent link filtering not only removes incorrect links, but also create new links based on updated confidence scores.
We applied the confidence-based link filtering on Chinese-English and Arabic-English word alignment. The C-E alignment test set is the same
512 sentence pairs, and the A-E alignment test set is the 200 Arabic-English sentence pairs from NIST MT03 test set.
Tables 3 and 4 show the improvement of C-E and A-E alignment F-measures with the confidence-based alignment link filtering (ALF). For C-E alignment, removing low confidence alignment links increased alignment precision by 5.5 point, while decreased recall by 1.8 point, and the overall alignment F-measure is increased by 1.3 point. When looking into the alignment links which are removed during the alignment link filtering process, we found that 80% of the removed links (1320 out of 1661 links) are incorrect alignments, For A-E alignment, it increased the precision by 3 points while reducing recall by 0.5 points, and the alignment F-measure is increased by about 1.5 points absolute, a 10% relative alignment error rate reduction. Similarly, 90% of the removed links are incorrect alignments.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In addition to the alignment combination, we also improve the performance of the MaxEnt aligner through confidence-based alignment link filtering.</text>
              <doc_id>129</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Here we select the MaxEnt aligner because it has</text>
              <doc_id>130</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the highest F-measure among the three aligners, although the algorithm described below can be applied to any aligner.</text>
              <doc_id>131</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>It is often observed that words within a constituent (such as NP, PP) are typically translated together, and their alignments are close.</text>
              <doc_id>132</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As a result the confidence measure of an alignment link a ij can be boosted given the alignment of its context words.</text>
              <doc_id>133</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>From the initial sentence alignment we first identify an anchor link a mn , the high confidence alignment link closest to a ij .</text>
              <doc_id>134</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The anchor link is considered as the most reliable connection between the source and target context.</text>
              <doc_id>135</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The context is then defined as a window centering at a mn with window width proportional to the distance between a ij and a mn .</text>
              <doc_id>136</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>When computing the context-dependent link confidence, we only consider words within the context window.</text>
              <doc_id>137</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The context-dependent alignment link confidence is calculated in the following steps:</text>
              <doc_id>138</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1.</text>
              <doc_id>139</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Calculate the context-independent link confidence measure c(a ij ) according to formula (8).</text>
              <doc_id>140</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.</text>
              <doc_id>141</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Sort all links based on their link confidence measures in decreasing order.</text>
              <doc_id>142</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3.</text>
              <doc_id>143</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Select links whose confidence scores are higher than an empirically chosen threshold H as anchor links 1 .</text>
              <doc_id>144</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>4.</text>
              <doc_id>145</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Walking along the remaining sorted links.</text>
              <doc_id>146</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For each link {a ij : c(a ij ) &lt; H},</text>
              <doc_id>147</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(a) Find the closest anchor link a mn 2 , (b) Define the context window width w =</text>
              <doc_id>148</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>|m &#8722; i| + |n &#8722; j|.</text>
              <doc_id>149</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 H is selected to maximize the F-score on an alignment</text>
              <doc_id>150</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>devset.</text>
              <doc_id>151</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2 When two equally close alignment links have the same</text>
              <doc_id>152</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>confidence score), we randomly select one of the tied links as the anchor link.</text>
              <doc_id>153</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(c) Compute the link posterior probabilities within the context window:</text>
              <doc_id>154</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>q s2t (a ij |a mn ) =</text>
              <doc_id>155</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>q t2s (a ij |a mn ) =</text>
              <doc_id>156</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>p(t j |s i ) &#8721; j+w</text>
              <doc_id>157</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>j &#8242; =j&#8722;w p(t j &#8242;|s i) ,</text>
              <doc_id>158</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>p(s i |t j ) &#8721; i+w</text>
              <doc_id>159</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i &#8242; =i&#8722;w p(s i &#8242;|t j) .</text>
              <doc_id>160</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(d) Compute the context-dependent link confidence score c(a ij |a mn ) =</text>
              <doc_id>161</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8730; q s2t (a ij |a mn )q t2s (a ij |a mn ).</text>
              <doc_id>162</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>If c(a ij |a mn ) &gt; H, add a ij into the set of anchor links.</text>
              <doc_id>163</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>5.</text>
              <doc_id>164</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Only keep anchor links and remove all the remaining links with low confidence scores.</text>
              <doc_id>165</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The above link filtering algorithm is designed to remove incorrect links.</text>
              <doc_id>166</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Furthermore, it is possible to create new links by relinking unaligned source and target word pairs within the context window if their context-dependent link posterior probability is high.</text>
              <doc_id>167</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Figure 4 shows context-independent link confidence scores for the given sentence alignment.</text>
              <doc_id>168</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The subscript following each word indicates the word&#8217;s position.</text>
              <doc_id>169</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Incorrect alignment links are shown with dashed lines, which have low confidence scores (a 5,7 , a 7,3 , a 8,2 , a 11,9 ) and will be removed through filtering.</text>
              <doc_id>170</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>When the anchor link a 4,11 is selected, the context-dependent link confidence of a 6,12 is increased from 0.12 to 0.51.</text>
              <doc_id>171</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Also note that a new link a 7,12 (shown as a dotted line) is created because within the context window, the link confidence score is as high as 0.96.</text>
              <doc_id>172</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>This example shows that the context-dependent link filtering not only removes incorrect links, but also create new links based on updated confidence scores.</text>
              <doc_id>173</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We applied the confidence-based link filtering on Chinese-English and Arabic-English word alignment.</text>
              <doc_id>174</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The C-E alignment test set is the same</text>
              <doc_id>175</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>512 sentence pairs, and the A-E alignment test set is the 200 Arabic-English sentence pairs from NIST MT03 test set.</text>
              <doc_id>176</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Tables 3 and 4 show the improvement of C-E and A-E alignment F-measures with the confidence-based alignment link filtering (ALF).</text>
              <doc_id>177</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For C-E alignment, removing low confidence alignment links increased alignment precision by 5.5 point, while decreased recall by 1.8 point, and the overall alignment F-measure is increased by 1.3 point.</text>
              <doc_id>178</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>When looking into the alignment links which are removed during the alignment link filtering process, we found that 80% of the removed links (1320 out of 1661 links) are incorrect alignments, For A-E alignment, it increased the precision by 3 points while reducing recall by 0.5 points, and the alignment F-measure is increased by about 1.5 points absolute, a 10% relative alignment error rate reduction.</text>
              <doc_id>179</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Similarly, 90% of the removed links are incorrect alignments.</text>
              <doc_id>180</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>5 Translation</title>
        <text>We evaluate the improved alignment on several Chinese-English and Arabic-English machine translation tasks. The documents to be translated are from difference genres: newswire (NW) and web-blog (WB). The MT system is a phrasebased SMT system as described in (Al-Onaizan and Papineni, 2006). The training data are bilingual sentence pairs with word alignment, from which we obtained phrase translation pairs. We extract phrase translation tables from the baseline MaxEnt word alignment as well as the alignment with confidence-based link filtering, then translate the test set with each phrase translation table. We measure the translation quality with automatic metrics including BLEU (Papineni et al., 2001) and TER (Snover et al., 2006). The higher the BLEU score is, or the lower the TER score is, the better the translation quality is. We combine the two metrics into (TER-BLEU)/2 and try to minimize it. In addition to the whole test set&#8217;s scores, we also measure the scores of the &#8221;tail&#8221; documents, whose (TER-BLEU)/2 scores are at the bottom 10 percentile (for A-E translation) and 20 percentile (for C-E translation) and are considered the most difficult documents to translate.
In the Chinese-English MT experiment, we selected 40 NW documents, 41 WB documents as the test set, which includes 623 sentences with 16667 words. The training data includes 333 thousand C-E sentence pairs subsampled from 10 million sentence pairs according to the test data. Tables 5 and 6 show the newswire and web-blog translation scores as well as the number of phrase translation pairs obtained from each alignment. Because the alignment link filtering removes many incorrect alignment links, the number of phrase translation pairs is reduced by 15%. For newswire, the translation quality is improved by 0.44 on the whole test set and 1.1 on the tail documents, as measured by (TER-BLEU)/2. For web-blog, we observed 0.2 improvement on the whole test set and 0.5 on the tail documents. The tail documents typically have lower phrase coverage, thus incorrect phrase translation pairs derived from incorrect
# phrase pairs Average Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2
# phrase pairs Average Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2
alignment links are more likely to be selected. The removal of incorrect alignment links and cleaner phrase translation pairs brought more gains on the tail documents.
In the Arabic-English MT, we selected 80 NW documents and 55 WB documents. The NW training data includes 319 thousand A-E sentence pairs subsampled from 7.2 million sentence pairs with word alignments. The WB training data includes 240 thousand subsampled sentence pairs. Tables 7 and 8 show the corresponding translation results. Similarly, the phrase table size is significantly reduced by 35%, while the gains on the tail documents range from 0.6 to 1.4. On the whole test set the difference is smaller, 0.07 for the newswire translation and 0.58 for the web-blog translation.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We evaluate the improved alignment on several Chinese-English and Arabic-English machine translation tasks.</text>
              <doc_id>181</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The documents to be translated are from difference genres: newswire (NW) and web-blog (WB).</text>
              <doc_id>182</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The MT system is a phrasebased SMT system as described in (Al-Onaizan and Papineni, 2006).</text>
              <doc_id>183</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The training data are bilingual sentence pairs with word alignment, from which we obtained phrase translation pairs.</text>
              <doc_id>184</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We extract phrase translation tables from the baseline MaxEnt word alignment as well as the alignment with confidence-based link filtering, then translate the test set with each phrase translation table.</text>
              <doc_id>185</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We measure the translation quality with automatic metrics including BLEU (Papineni et al., 2001) and TER (Snover et al., 2006).</text>
              <doc_id>186</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The higher the BLEU score is, or the lower the TER score is, the better the translation quality is.</text>
              <doc_id>187</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>We combine the two metrics into (TER-BLEU)/2 and try to minimize it.</text>
              <doc_id>188</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>In addition to the whole test set&#8217;s scores, we also measure the scores of the &#8221;tail&#8221; documents, whose (TER-BLEU)/2 scores are at the bottom 10 percentile (for A-E translation) and 20 percentile (for C-E translation) and are considered the most difficult documents to translate.</text>
              <doc_id>189</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the Chinese-English MT experiment, we selected 40 NW documents, 41 WB documents as the test set, which includes 623 sentences with 16667 words.</text>
              <doc_id>190</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The training data includes 333 thousand C-E sentence pairs subsampled from 10 million sentence pairs according to the test data.</text>
              <doc_id>191</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Tables 5 and 6 show the newswire and web-blog translation scores as well as the number of phrase translation pairs obtained from each alignment.</text>
              <doc_id>192</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Because the alignment link filtering removes many incorrect alignment links, the number of phrase translation pairs is reduced by 15%.</text>
              <doc_id>193</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>For newswire, the translation quality is improved by 0.44 on the whole test set and 1.1 on the tail documents, as measured by (TER-BLEU)/2.</text>
              <doc_id>194</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>For web-blog, we observed 0.2 improvement on the whole test set and 0.5 on the tail documents.</text>
              <doc_id>195</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The tail documents typically have lower phrase coverage, thus incorrect phrase translation pairs derived from incorrect</text>
              <doc_id>196</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text># phrase pairs Average Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2</text>
              <doc_id>197</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text># phrase pairs Average Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2</text>
              <doc_id>198</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>alignment links are more likely to be selected.</text>
              <doc_id>199</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The removal of incorrect alignment links and cleaner phrase translation pairs brought more gains on the tail documents.</text>
              <doc_id>200</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the Arabic-English MT, we selected 80 NW documents and 55 WB documents.</text>
              <doc_id>201</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The NW training data includes 319 thousand A-E sentence pairs subsampled from 7.2 million sentence pairs with word alignments.</text>
              <doc_id>202</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The WB training data includes 240 thousand subsampled sentence pairs.</text>
              <doc_id>203</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Tables 7 and 8 show the corresponding translation results.</text>
              <doc_id>204</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Similarly, the phrase table size is significantly reduced by 35%, while the gains on the tail documents range from 0.6 to 1.4.</text>
              <doc_id>205</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>On the whole test set the difference is smaller, 0.07 for the newswire translation and 0.58 for the web-blog translation.</text>
              <doc_id>206</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>6 Related Work</title>
        <text>In the machine translation area, most research on confidence measure focus on the confidence of MT output: how accurate a translated sentence is. (Gandrabur and Foster, 2003) used neural-net to improve the confidence estimate for text predictions in a machine-assisted translation tool. (Ueffing et al., 2003) presented several word-level confidence measures for machine translation based on word posterior probabilities. (Blatz et al., 2004) conducted extensive study incorporating various sentence-level and word-level features thru multilayer perceptron and naive Bayes algorithms for sentence and word confidence estimation. (Quirk, 2004) trained a sentence level confidence measure using a human annotated corpus. (Bach et al., 2008) used the sentence-pair confidence scores estimated with source and target language models to weight phrase translation pairs. However, there has been little research focusing on confidence measure for word alignment. This work is the first attempt to address the alignment confidence problem.
Regarding word alignment combination, in addition to the commonly used &#8221;intersection-unionrefine&#8221; approach (Och and Ney, 2003), (Ayan and Dorr, 2006b) and (Ayan et al., 2005) combined alignment links from multiple word alignment based on a set of linguistic and alignment features within the MaxEnt framework or a neural net model. While in this paper, the alignment links are combined based on their confidence scores and alignment agreement ratios.
(Fraser and Marcu, 2007) discussed the impact of word alignment&#8217;s precision and recall on MT quality. Here removing low confidence links results in higher precision and slightly lower recall for the alignment. In our phrase extraction, we allow extracting phrase translation pairs with unaligned functional words at the boundary. This is similar to the &#8221;loose phrases&#8221; described in (Ayan and Dorr, 2006a), which increased the number of correct phrase translations and improved the translation quality. On the other hand, removing incorrect content word links produced cleaner phrase translation tables. When translating documents with lower phrase coverage (typically the &#8220;tail&#8221; documents), high quality phrase translations are particularly important because a bad phrase translation can be picked up more easily due to limited phrase translation pairs available.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In the machine translation area, most research on confidence measure focus on the confidence of MT output: how accurate a translated sentence is.</text>
              <doc_id>207</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>(Gandrabur and Foster, 2003) used neural-net to improve the confidence estimate for text predictions in a machine-assisted translation tool.</text>
              <doc_id>208</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>(Ueffing et al., 2003) presented several word-level confidence measures for machine translation based on word posterior probabilities.</text>
              <doc_id>209</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>(Blatz et al., 2004) conducted extensive study incorporating various sentence-level and word-level features thru multilayer perceptron and naive Bayes algorithms for sentence and word confidence estimation.</text>
              <doc_id>210</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>(Quirk, 2004) trained a sentence level confidence measure using a human annotated corpus.</text>
              <doc_id>211</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>(Bach et al., 2008) used the sentence-pair confidence scores estimated with source and target language models to weight phrase translation pairs.</text>
              <doc_id>212</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>However, there has been little research focusing on confidence measure for word alignment.</text>
              <doc_id>213</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>This work is the first attempt to address the alignment confidence problem.</text>
              <doc_id>214</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Regarding word alignment combination, in addition to the commonly used &#8221;intersection-unionrefine&#8221; approach (Och and Ney, 2003), (Ayan and Dorr, 2006b) and (Ayan et al., 2005) combined alignment links from multiple word alignment based on a set of linguistic and alignment features within the MaxEnt framework or a neural net model.</text>
              <doc_id>215</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>While in this paper, the alignment links are combined based on their confidence scores and alignment agreement ratios.</text>
              <doc_id>216</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(Fraser and Marcu, 2007) discussed the impact of word alignment&#8217;s precision and recall on MT quality.</text>
              <doc_id>217</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Here removing low confidence links results in higher precision and slightly lower recall for the alignment.</text>
              <doc_id>218</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In our phrase extraction, we allow extracting phrase translation pairs with unaligned functional words at the boundary.</text>
              <doc_id>219</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This is similar to the &#8221;loose phrases&#8221; described in (Ayan and Dorr, 2006a), which increased the number of correct phrase translations and improved the translation quality.</text>
              <doc_id>220</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>On the other hand, removing incorrect content word links produced cleaner phrase translation tables.</text>
              <doc_id>221</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>When translating documents with lower phrase coverage (typically the &#8220;tail&#8221; documents), high quality phrase translations are particularly important because a bad phrase translation can be picked up more easily due to limited phrase translation pairs available.</text>
              <doc_id>222</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>7</index>
        <title>7 Conclusion</title>
        <text>In this paper we presented two alignment confidence measures for word alignment. The first is the sentence alignment confidence measure, based on which the best whole sentence alignment is se-
lected among multiple alignments and it obtained 0.8 F-measure improvement over the single best Chinese-English aligner. The second is the alignment link confidence measure, which selects the most reliable links from multiple alignments and obtained 1.5 F-measure improvement. When we removed low confidence links from the MaxEnt aligner, we reduced the Chinese-English alignment error by 5% and the Arabic-English alignment error by 10%. The cleaned alignment significantly reduced the size of phrase translation tables by 15-35%. It furthermore led to better translation scores for Chinese and Arabic documents with different genres. In particular, it improved the translation scores of the tail documents by 0.5-1.4 points measured by the combined metric of (TER- BLEU)/2. For future work we would like to explore richer models to estimate alignment posterior probability. In most cases, exact calculation by summing over all possible alignments is impossible, and approximation using N-best alignments is needed.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper we presented two alignment confidence measures for word alignment.</text>
              <doc_id>223</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The first is the sentence alignment confidence measure, based on which the best whole sentence alignment is se-</text>
              <doc_id>224</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>lected among multiple alignments and it obtained 0.8 F-measure improvement over the single best Chinese-English aligner.</text>
              <doc_id>225</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The second is the alignment link confidence measure, which selects the most reliable links from multiple alignments and obtained 1.5 F-measure improvement.</text>
              <doc_id>226</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>When we removed low confidence links from the MaxEnt aligner, we reduced the Chinese-English alignment error by 5% and the Arabic-English alignment error by 10%.</text>
              <doc_id>227</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The cleaned alignment significantly reduced the size of phrase translation tables by 15-35%.</text>
              <doc_id>228</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>It furthermore led to better translation scores for Chinese and Arabic documents with different genres.</text>
              <doc_id>229</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In particular, it improved the translation scores of the tail documents by 0.5-1.4 points measured by the combined metric of (TER- BLEU)/2.</text>
              <doc_id>230</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>For future work we would like to explore richer models to estimate alignment posterior probability.</text>
              <doc_id>231</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In most cases, exact calculation by summing over all possible alignments is impossible, and approximation using N-best alignments is needed.</text>
              <doc_id>232</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>8</index>
        <title>Acknowledgments</title>
        <text>We are grateful to Abraham Ittycheriah, Yaser Al- Onaizan, Niyu Ge and Salim Roukos and anonymous reviewers for their constructive comments. This work was supported in part by the DARPA GALE project, contract No. HR0011-08-C-0110.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We are grateful to Abraham Ittycheriah, Yaser Al- Onaizan, Niyu Ge and Salim Roukos and anonymous reviewers for their constructive comments.</text>
              <doc_id>233</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This work was supported in part by the DARPA GALE project, contract No.</text>
              <doc_id>234</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>HR0011-08-C-0110.</text>
              <doc_id>235</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Correlation coefficients of multiple alignments.</caption>
        <reference_text></reference_text>
        <page_num>1</page_num>
        <head>
          <rows>
            <row>
              <cell>HMM</cell>
              <cell>54.72</cell>
              <cell>-0.710</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>BM</cell>
              <cell>62.53</cell>
              <cell>-0.699</cell>
            </row>
            <row>
              <cell>MaxEnt</cell>
              <cell>69.26</cell>
              <cell>-0.699</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Link Selection and Combination Results</caption>
        <reference_text></reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell>Precision</cell>
              <cell>Recall</cell>
              <cell>F-score</cell>
              <cell>F-content</cell>
              <cell>F-function</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>HMM</cell>
              <cell>62.65</cell>
              <cell>48.57</cell>
              <cell>54.72</cell>
              <cell>62.10</cell>
              <cell>34.39</cell>
            </row>
            <row>
              <cell>BM</cell>
              <cell>72.76</cell>
              <cell>54.82</cell>
              <cell>62.53</cell>
              <cell>68.64</cell>
              <cell>43.93</cell>
            </row>
            <row>
              <cell>ME</cell>
              <cell>72.66</cell>
              <cell>66.17</cell>
              <cell>69.26</cell>
              <cell>72.52</cell>
              <cell>61.41</cell>
            </row>
            <row>
              <cell>Link-Select</cell>
              <cell>69.19</cell>
              <cell>72.49</cell>
              <cell>70.81</cell>
              <cell>74.31</cell>
              <cell>60.26</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TET</source>
        <caption>Table 3: Confidence-based Alignment Link Filtering on C-E Alignment</caption>
        <reference_text></reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell>Precision</cell>
              <cell>Recall</cell>
              <cell>F-score</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Baseline</cell>
              <cell>72.66</cell>
              <cell>66.17</cell>
              <cell>69.26</cell>
            </row>
            <row>
              <cell>+ALF</cell>
              <cell>78.14</cell>
              <cell>64.36</cell>
              <cell>70.59</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 3: Confidence-based Alignment Link Filter- ing on C-E Alignment#@#@Table 4: Confidence-based Alignment Link Filtering on A-E Alignment</caption>
        <reference_text>None</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell>Precision</cell>
              <cell>Recall</cell>
              <cell>F-score</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Baseline</cell>
              <cell>84.43</cell>
              <cell>83.64</cell>
              <cell>84.04</cell>
            </row>
            <row>
              <cell>+ALF</cell>
              <cell>88.29</cell>
              <cell>83.14</cell>
              <cell>85.64</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TET</source>
        <caption>Table 5: Improved Chinese-English Newswire Translation with Alignment Link Filtering</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Baseline</cell>
              <cell>934206</cell>
              <cell>60.74</cell>
              <cell>28.05</cell>
              <cell>16.35</cell>
              <cell>69.02</cell>
              <cell>17.83</cell>
              <cell>25.60</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>ALF</cell>
              <cell>797685</cell>
              <cell>60.33</cell>
              <cell>28.52</cell>
              <cell>15.91</cell>
              <cell>68.31</cell>
              <cell>19.27</cell>
              <cell>24.52</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>6</id>
        <source>TableSeer</source>
        <caption>Table 5: Improved Chinese-English Newswire Translation with Alignment Link Filtering#@#@Table 6: Improved Chinese-English Web-Blog Translation with Alignment Link Filtering</caption>
        <reference_text>None</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>None</cell>
              <cell># phrase pairs</cell>
              <cell>TER</cell>
              <cell>Average   BLEU</cell>
              <cell>(TER-BLEU)/2</cell>
              <cell>TER</cell>
              <cell>BLEU</cell>
              <cell>Tail    (TER-BLEU)/2</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Baseline</cell>
              <cell>934206</cell>
              <cell>62.87</cell>
              <cell>25.08</cell>
              <cell>18.89</cell>
              <cell>66.55</cell>
              <cell>18.80</cell>
              <cell>23.88</cell>
            </row>
            <row>
              <cell>ALF</cell>
              <cell>797685</cell>
              <cell>62.30</cell>
              <cell>24.89</cell>
              <cell>18.70</cell>
              <cell>65.97</cell>
              <cell>19.25</cell>
              <cell>23.36</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>7</id>
        <source>TET</source>
        <caption>Table 7: Improved Arabic-English Newswire Translation with Alignment Link Filtering</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>Baseline</cell>
              <cell>939911</cell>
              <cell>43.53</cell>
              <cell>50.51</cell>
              <cell>-3.49</cell>
              <cell>53.14</cell>
              <cell>40.60</cell>
              <cell>6.27</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>ALF</cell>
              <cell>618179</cell>
              <cell>43.11</cell>
              <cell>50.24</cell>
              <cell>-3.56</cell>
              <cell>51.75</cell>
              <cell>42.05</cell>
              <cell>4.85</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>8</id>
        <source>TableSeer</source>
        <caption>Table 7: Improved Arabic-English Newswire Translation with Alignment Link Filtering#@#@Table 8: Improved Arabic-English Web-Blog Translation with Alignment Link Filtering</caption>
        <reference_text>None</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>None</cell>
              <cell># phrase pairs</cell>
              <cell>TER</cell>
              <cell>Average   BLEU</cell>
              <cell>(TER-BLEU)/2</cell>
              <cell>TER</cell>
              <cell>BLEU</cell>
              <cell>Tail    (TER-BLEU)/2</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Baseline</cell>
              <cell>598721</cell>
              <cell>49.91</cell>
              <cell>39.90</cell>
              <cell>5.00</cell>
              <cell>57.30</cell>
              <cell>30.98</cell>
              <cell>13.16</cell>
            </row>
            <row>
              <cell>ALF</cell>
              <cell>383561</cell>
              <cell>48.94</cell>
              <cell>40.00</cell>
              <cell>4.42</cell>
              <cell>55.99</cell>
              <cell>31.92</cell>
              <cell>12.04</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Yaser Al-Onaizan</author>
          <author>Kishore Papineni</author>
        </authors>
        <title>Distortion Models for Statistical Machine Translation.</title>
        <publication>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>529--536</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Necip Fazil Ayan</author>
          <author>Bonnie J Dorr</author>
        </authors>
        <title>Going beyond aer: An extensive analysis of word alignments and their impact on mt.</title>
        <publication>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>9--16</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Necip Fazil Ayan</author>
          <author>Bonnie J Dorr</author>
        </authors>
        <title>A maximum entropy approach to combining word alignments.</title>
        <publication>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</publication>
        <pages>96--103</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Necip Fazil Ayan</author>
          <author>Bonnie J Dorr</author>
          <author>Christof Monz</author>
        </authors>
        <title>Neuralign: Combining word alignments using neural networks.</title>
        <publication>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>65--72</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Nguyen Bach</author>
          <author>Qin Gao</author>
          <author>Stephan Vogel</author>
        </authors>
        <title>Improving word alignment with language model based confidence scores.</title>
        <publication>In Proceedings of the Third Workshop on Statistical Machine Translation,</publication>
        <pages>151--154</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>John Blatz</author>
          <author>Erin Fitzgerald</author>
          <author>George Foster</author>
          <author>Simona Gandrabur</author>
          <author>Cyril Goutte</author>
          <author>Alex Kulesza</author>
          <author>Alberto Sanchis</author>
          <author>Nicola Ueffing</author>
        </authors>
        <title>Confidence estimation for machine translation.</title>
        <publication>In COLING &#8217;04: Proceedings of the 20th international conference on Computational Linguistics,</publication>
        <pages>315</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Peter F Brown</author>
          <author>Stephen Della Pietra</author>
          <author>Vincent J Della Pietra</author>
          <author>Robert L Mercer</author>
        </authors>
        <title>The Mathematic of Statistical Machine Translation: Parameter Estimation.</title>
        <publication>None</publication>
        <pages>311</pages>
        <date>1994</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Fraser</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Measuring word alignment quality for statistical machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Simona Gandrabur</author>
          <author>George Foster</author>
        </authors>
        <title>Confidence estimation for translation prediction.</title>
        <publication>In Proceedings of the seventh conference on Natural language learning at HLT-NAACL</publication>
        <pages>95--102</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Niyu Ge</author>
        </authors>
        <title>Max-posterior hmm alignment for machine translation.</title>
        <publication>In Presentation given at DARPA/TIDES NIST MT Evaluation workshop.</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Abraham Ittycheriah</author>
          <author>Salim Roukos</author>
        </authors>
        <title>A maximum entropy word aligner for arabic-english machine translation.</title>
        <publication>In HLT &#8217;05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</publication>
        <pages>89--96</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Franz J Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A systematic comparison of various statistical alignment models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
        <publication>In ACL &#8217;02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</publication>
        <pages>311--318</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Chris Quirk</author>
        </authors>
        <title>Training a sentence-level machine translation confidence measure. In</title>
        <publication>In Proc. LREC 2004,</publication>
        <pages>825--828</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Matthew Snover</author>
          <author>Bonnie Dorr</author>
          <author>Richard Schwartz</author>
          <author>Linnea Micciulla</author>
          <author>John Makhoul</author>
        </authors>
        <title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
        <publication>In Proceedings of Association for Machine Translation in the Americas.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Nicola Ueffing</author>
          <author>Klaus Macherey</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Confidence measures for statistical machine translation. In</title>
        <publication>In Proc. MT Summit IX,</publication>
        <pages>394--401</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Stephan Vogel</author>
          <author>Hermann Ney</author>
          <author>Christoph Tillmann</author>
        </authors>
        <title>Hmm-based word alignment in statistical translation.</title>
        <publication>In Proceedings of the 16th conference on Computational linguistics,</publication>
        <pages>836--841</pages>
        <date>1996</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Al-Onaizan and Papineni, 2006</string>
        <sentence_id>28125</sentence_id>
        <char_offset>59</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Ayan and Dorr, 2006</string>
        <sentence_id>28157</sentence_id>
        <char_offset>129</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>Ayan and Dorr, 2006</string>
        <sentence_id>28162</sentence_id>
        <char_offset>53</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>2</reference_id>
        <string>Ayan and Dorr, 2006</string>
        <sentence_id>28157</sentence_id>
        <char_offset>129</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>2</reference_id>
        <string>Ayan and Dorr, 2006</string>
        <sentence_id>28162</sentence_id>
        <char_offset>53</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>3</reference_id>
        <string>Ayan et al., 2005</string>
        <sentence_id>28157</sentence_id>
        <char_offset>156</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>4</reference_id>
        <string>Bach et al., 2008</string>
        <sentence_id>28154</sentence_id>
        <char_offset>1</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>5</reference_id>
        <string>Blatz et al., 2004</string>
        <sentence_id>28152</sentence_id>
        <char_offset>1</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>6</reference_id>
        <string>Brown et al., 1994</string>
        <sentence_id>27950</sentence_id>
        <char_offset>121</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>6</reference_id>
        <string>Brown et al., 1994</string>
        <sentence_id>27985</sentence_id>
        <char_offset>145</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>6</reference_id>
        <string>Brown et al., 1994</string>
        <sentence_id>27994</sentence_id>
        <char_offset>102</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>7</reference_id>
        <string>Fraser and Marcu, 2007</string>
        <sentence_id>28159</sentence_id>
        <char_offset>1</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>8</reference_id>
        <string>Gandrabur and Foster, 2003</string>
        <sentence_id>28150</sentence_id>
        <char_offset>1</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>9</reference_id>
        <string>Ge, 2004</string>
        <sentence_id>28016</sentence_id>
        <char_offset>62</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>10</reference_id>
        <string>Ittycheriah and Roukos, 2005</string>
        <sentence_id>27951</sentence_id>
        <char_offset>5</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>10</reference_id>
        <string>Ittycheriah and Roukos, 2005</string>
        <sentence_id>28011</sentence_id>
        <char_offset>118</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>11</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>28069</sentence_id>
        <char_offset>98</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>11</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>28157</sentence_id>
        <char_offset>108</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>12</reference_id>
        <string>Papineni et al., 2001</string>
        <sentence_id>28128</sentence_id>
        <char_offset>74</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>13</reference_id>
        <string>Quirk, 2004</string>
        <sentence_id>28153</sentence_id>
        <char_offset>1</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>14</reference_id>
        <string>Snover et al., 2006</string>
        <sentence_id>28128</sentence_id>
        <char_offset>106</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>15</reference_id>
        <string>Ueffing et al., 2003</string>
        <sentence_id>28151</sentence_id>
        <char_offset>1</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>16</reference_id>
        <string>Vogel et al., 1996</string>
        <sentence_id>27950</sentence_id>
        <char_offset>143</char_offset>
      </citation>
    </citations>
  </content>
</document>
