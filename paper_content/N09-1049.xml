<document>
  <filename>N09-1049</filename>
  <authors/>
  <title>Hierarchical Phrase-Based Translation with Weighted Finite State Transducers</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>This paper describes a lattice-based decoder for hierarchical phrase-based translation. The decoder is implemented with standard WFST operations as an alternative to the well-known cube pruning procedure. We find that the use of WFSTs rather than k-best lists requires less pruning in translation search, resulting in fewer search errors, direct generation of translation lattices in the target language, better parameter optimization, and improved translation performance when rescoring with long-span language models and MBR decoding. We report translation experiments for the Arabic-to-English and Chinese-to-English NIST translation tasks and contrast the WFSTbased hierarchical decoder with hierarchical translation under cube pruning.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This paper describes a lattice-based decoder for hierarchical phrase-based translation.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The decoder is implemented with standard WFST operations as an alternative to the well-known cube pruning procedure.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We find that the use of WFSTs rather than k-best lists requires less pruning in translation search, resulting in fewer search errors, direct generation of translation lattices in the target language, better parameter optimization, and improved translation performance when rescoring with long-span language models and MBR decoding.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We report translation experiments for the Arabic-to-English and Chinese-to-English NIST translation tasks and contrast the WFSTbased hierarchical decoder with hierarchical translation under cube pruning.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Hierarchical phrase-based translation generates translation hypotheses via the application of hierarchical rules in CYK parsing (Chiang, 2005). Cube pruning is used to apply language models at each cell of the CYK grid as part of the search for a k-best list of translation candidates (Chiang, 2005; Chiang, 2007). While this approach is very effective and has been shown to produce very good quality translation, the reliance on k-best lists is a limitation. We take an alternative approach and describe a lattice-based hierarchical decoder implemented with Weighted Finite State Transducers (WFSTs). In every CYK cell we build a single, minimal word lattice containing all possible translations of the source sentence span covered by that cell. When derivations contain non-terminals, we use pointers to lowerlevel lattices for memory efficiency. The pointers are only expanded to the actual translations if pruning is required during search; expansion is otherwise only carried out at the upper-most cell, after the full CYK grid has been traversed.
We describe how this decoder can be easily implemented with WFSTs. For this we employ the OpenFST libraries (Allauzen et al., 2007). Using standard FST operations such as composition, epsilon removal, determinization, minimization and shortest-path, we find this search procedure to be simpler to implement than cube pruning. The main modeling advantages are a significant reduction in search errors, a simpler implementation, direct generation of target language word lattices, and better integration with other statistical MT procedures. We report translation results in Arabic-to-English and Chinese-to-English translation and contrast the performance of lattice-based and cube pruning hierarchical decoding.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Hierarchical phrase-based translation generates translation hypotheses via the application of hierarchical rules in CYK parsing (Chiang, 2005).</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Cube pruning is used to apply language models at each cell of the CYK grid as part of the search for a k-best list of translation candidates (Chiang, 2005; Chiang, 2007).</text>
              <doc_id>5</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>While this approach is very effective and has been shown to produce very good quality translation, the reliance on k-best lists is a limitation.</text>
              <doc_id>6</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We take an alternative approach and describe a lattice-based hierarchical decoder implemented with Weighted Finite State Transducers (WFSTs).</text>
              <doc_id>7</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In every CYK cell we build a single, minimal word lattice containing all possible translations of the source sentence span covered by that cell.</text>
              <doc_id>8</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>When derivations contain non-terminals, we use pointers to lowerlevel lattices for memory efficiency.</text>
              <doc_id>9</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The pointers are only expanded to the actual translations if pruning is required during search; expansion is otherwise only carried out at the upper-most cell, after the full CYK grid has been traversed.</text>
              <doc_id>10</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We describe how this decoder can be easily implemented with WFSTs.</text>
              <doc_id>11</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For this we employ the OpenFST libraries (Allauzen et al., 2007).</text>
              <doc_id>12</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Using standard FST operations such as composition, epsilon removal, determinization, minimization and shortest-path, we find this search procedure to be simpler to implement than cube pruning.</text>
              <doc_id>13</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The main modeling advantages are a significant reduction in search errors, a simpler implementation, direct generation of target language word lattices, and better integration with other statistical MT procedures.</text>
              <doc_id>14</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We report translation results in Arabic-to-English and Chinese-to-English translation and contrast the performance of lattice-based and cube pruning hierarchical decoding.</text>
              <doc_id>15</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>1.1 Related Work</title>
            <text>Hierarchical phrase-based translation has emerged as one of the dominant current approaches to statistical machine translation. Hiero translation systems incorporate many of the strengths of phrase-based translation systems, such as feature-based translation and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned parallel text. We summarize some extensions to the basic approach to put our work in context.
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 433&#8211;441, Boulder, Colorado, June 2009. c&#9675;2009 Association for Computational Linguistics
Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999).
Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008).
Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007).
WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine translation (Bangalore and Riccardi, 2001; Casacuberta, 2001; Kumar and Byrne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008).
To our knowledge, this paper presents the first description of hierarchical phrase-based translation in terms of lattices rather than k-best lists. The next section describes hierarchical phrase-based translation with WFSTs, including the lattice construction over the CYK grid and pruning strategies. Section 3 reports translation experiments for Arabic-to- English and Chinese-to-English, and Section 4 concludes.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Hierarchical phrase-based translation has emerged as one of the dominant current approaches to statistical machine translation.</text>
                  <doc_id>16</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Hiero translation systems incorporate many of the strengths of phrase-based translation systems, such as feature-based translation and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned parallel text.</text>
                  <doc_id>17</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We summarize some extensions to the basic approach to put our work in context.</text>
                  <doc_id>18</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 433&#8211;441, Boulder, Colorado, June 2009. c&#9675;2009 Association for Computational Linguistics</text>
                  <doc_id>19</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed.</text>
                  <doc_id>20</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007).</text>
                  <doc_id>21</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests.</text>
                  <doc_id>22</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999).</text>
                  <doc_id>23</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008).</text>
                  <doc_id>24</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English.</text>
                  <doc_id>25</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems.</text>
                  <doc_id>26</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007).</text>
                  <doc_id>27</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine translation (Bangalore and Riccardi, 2001; Casacuberta, 2001; Kumar and Byrne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008).</text>
                  <doc_id>28</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To our knowledge, this paper presents the first description of hierarchical phrase-based translation in terms of lattices rather than k-best lists.</text>
                  <doc_id>29</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The next section describes hierarchical phrase-based translation with WFSTs, including the lattice construction over the CYK grid and pruning strategies.</text>
                  <doc_id>30</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Section 3 reports translation experiments for Arabic-to- English and Chinese-to-English, and Section 4 concludes.</text>
                  <doc_id>31</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>2</index>
        <title>2 Hierarchical Translation with WFSTs</title>
        <text>The translation system is based on a variant of the CYK algorithm closely related to CYK+ (Chappelier and Rajman, 1998). Parsing follows the description of Chiang (2005; 2007), maintaining backpointers and employing hypothesis recombination without pruning. The underlying model is a synchronous context-free grammar consisting of a set R = {R r } of rules R r : N &#8594; &#12296;&#947; r ,&#945; r &#12297; / p r , with &#8216;glue&#8217; rules, S &#8594; &#12296;X,X&#12297; and S &#8594; &#12296;S X,S X&#12297;. If a rule has probability p r , it is transformed to a cost c r ; here we use the tropical semiring, so c r = &#8722; log p r . N denotes a non-terminal; in this paper, N can be either S, X, or V (see section 3.2). T denotes the terminals (words), and the grammar builds parses based on strings &#947;, &#945; &#8712; {{S, X, V } &#8746; T} + . Each cell in the CYK grid is specified by a non-terminal symbol and position in the CYK grid: (N, x, y), which spans sx x+y&#8722;1 on the source sentence.
In effect, the source language sentence is parsed using a context-free grammar with rules N &#8594; &#947;. The generation of translations is a second step that follows parsing. For this second step, we describe a method to construct word lattices with all possible translations that can be produced by the hierarchical rules. Construction proceeds by traversing the CYK grid along the backpointers established in parsing. In each cell (N, x, y) in the CYK grid, we build a target language word lattice L(N, x, y). This lattice contains every translation of sx x+y&#8722;1 from every derivation headed by N. These lattices also contain the translation scores on their arc weights.
The ultimate objective is the word lattice L(S, 1, J) which corresponds to all the analyses that cover the source sentence s J 1 . Once this is built, we can apply a target language model to L(S, 1, J) to obtain the final target language translation lattice (Allauzen et al., 2003).
We use the approach of Mohri (2002) in applying WFSTs to statistical NLP. This fits well with the use of the OpenFST toolkit (Allauzen et al., 2007) to implement our decoder.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The translation system is based on a variant of the CYK algorithm closely related to CYK+ (Chappelier and Rajman, 1998).</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Parsing follows the description of Chiang (2005; 2007), maintaining backpointers and employing hypothesis recombination without pruning.</text>
              <doc_id>33</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The underlying model is a synchronous context-free grammar consisting of a set R = {R r } of rules R r : N &#8594; &#12296;&#947; r ,&#945; r &#12297; / p r , with &#8216;glue&#8217; rules, S &#8594; &#12296;X,X&#12297; and S &#8594; &#12296;S X,S X&#12297;.</text>
              <doc_id>34</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>If a rule has probability p r , it is transformed to a cost c r ; here we use the tropical semiring, so c r = &#8722; log p r .</text>
              <doc_id>35</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>N denotes a non-terminal; in this paper, N can be either S, X, or V (see section 3.2).</text>
              <doc_id>36</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>T denotes the terminals (words), and the grammar builds parses based on strings &#947;, &#945; &#8712; {{S, X, V } &#8746; T} + .</text>
              <doc_id>37</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Each cell in the CYK grid is specified by a non-terminal symbol and position in the CYK grid: (N, x, y), which spans sx x+y&#8722;1 on the source sentence.</text>
              <doc_id>38</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In effect, the source language sentence is parsed using a context-free grammar with rules N &#8594; &#947;.</text>
              <doc_id>39</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The generation of translations is a second step that follows parsing.</text>
              <doc_id>40</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For this second step, we describe a method to construct word lattices with all possible translations that can be produced by the hierarchical rules.</text>
              <doc_id>41</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Construction proceeds by traversing the CYK grid along the backpointers established in parsing.</text>
              <doc_id>42</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In each cell (N, x, y) in the CYK grid, we build a target language word lattice L(N, x, y).</text>
              <doc_id>43</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This lattice contains every translation of sx x+y&#8722;1 from every derivation headed by N.</text>
              <doc_id>44</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>These lattices also contain the translation scores on their arc weights.</text>
              <doc_id>45</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The ultimate objective is the word lattice L(S, 1, J) which corresponds to all the analyses that cover the source sentence s J 1 .</text>
              <doc_id>46</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Once this is built, we can apply a target language model to L(S, 1, J) to obtain the final target language translation lattice (Allauzen et al., 2003).</text>
              <doc_id>47</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We use the approach of Mohri (2002) in applying WFSTs to statistical NLP.</text>
              <doc_id>48</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This fits well with the use of the OpenFST toolkit (Allauzen et al., 2007) to implement our decoder.</text>
              <doc_id>49</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Lattice Construction Over the CYK Grid</title>
            <text>In each cell (N, x, y), the set of rule indices used by the parser is denoted R(N, x, y), i.e. for r &#8712; R(N, x, y), N &#8594; &#12296;&#947; r ,&#945; r &#12297; was used in at least one derivation involving that cell.
For each rule R r , r &#8712; R(N, x, y), we build a lattice L(N, x, y, r). This lattice is derived from the target side of the rule &#945; r by concatenating lattices
R 1 : X &#8594; &#12296;s 1 s 2 s 3,t 1 t 2&#12297; R 2 : X &#8594; &#12296;s 1 s 2,t 7 t 8&#12297; R 3 : X &#8594; &#12296;s 3,t 9&#12297; R 4 : S &#8594; &#12296;X,X&#12297; R 5 : S &#8594; &#12296;S X,S X&#12297;
L(S, 1, 3) = L(S, 1, 3, 4) &#8853; L(S, 1, 3, 5) L(S, 1, 3, 4) = L(X, 1, 3) = L(X, 1, 3, 1) = = A(t 1) &#8855; A(t 2)
L(S, 1, 3, 5) = L(S, 1, 2) &#8855; L(X, 3, 1) L(S, 1, 2) = L(S, 1, 2, 4) = L(X, 1, 2) = = L(X, 1, 2, 2) = A(t 7) &#8855; A(t 8)
L(X, 3, 1) = L(X, 3, 1, 3) = A(t 9) L(S, 1, 3, 5) = A(t 7) &#8855; A(t 8) &#8855; A(t 9)
L(S, 1, 3) = (A(t 1) &#8855; A(t 2)) &#8853; (A(t 7) &#8855; A(t 8) &#8855; A(t 9))
corresponding to the elements of &#945; r = &#945; r 1 ...&#945;r |&#945; r | . If an &#945; r i is a terminal, creating its lattice is straightforward. If &#945; r i is a non-terminal, it refers to a cell (N &#8242; , x &#8242; , y &#8242; ) lower in the grid identified by the backpointer BP (N, x, y, r, i); in this case, the lattice used is L(N &#8242; , x &#8242; , y &#8242; ). Taken together,
L(N, x, y, r) = &#8855;
i=1..|&#945; r |
L(N, x, y, r, i) (1)
{ A(&#945;
L(N, x, y, r, i) = i ) if &#945; i &#8712; T L(N &#8242; , x &#8242; , y &#8242; ) else (2) where A(t), t &#8712; T returns a single-arc acceptor which accepts only the symbol t. The lattice L(N, x, y) is then built as the union of lattices corresponding to the rules in R(N, x, y):
L(N, x, y) = &#8853;
r&#8712;R(N,x,y)
L(N, x, y, r) (3)
Lattice union and concatenation are performed using the &#8853; and &#8855; WFST operations respectively, as described by Allauzen et al.(2007). If a rule R r has a cost c r , it is applied to the exit state of the lattice L(N, x, y, r) prior to the operation of Equation 3.
2.1.1 An Example of Phrase-based Translation
Figure 1 illustrates this process for a three word source sentence s 1 s 2 s 3 under monotone phrasebased translation. The left-hand side shows the state of the CYK grid after parsing using the rules R 1 to R 5 . These include 3 rules with only terminals (R 1 , R 2 , R 3 ) and the glue rules (R 4 , R 5 ). Arrows represent backpointers to lower-level cells. We are interested in the upper-most S cell (S, 1, 3), as it represents the search space of translation hypotheses covering the whole source sentence. Two rules (R 4 , R 5 ) are in this cell, so the lattice L(S, 1, 3) will be obtained by the union of the two lattices found by the backpointers of these two rules. This process is explicitly derived in the right-hand side of Figure 1.
2.1.2 An Example of Hierarchical Translation
Figure 2 shows a hierarchical scenario for the same sentence. Three rules, R 6 , R 7 , R 8 , are added to the example of Figure 1, thus providing two additional derivations. This makes use of sublattices already produced in the creation of L(S, 1, 3, 5) and L(X, 1, 3, 1) in Figure 1; these are within {}.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In each cell (N, x, y), the set of rule indices used by the parser is denoted R(N, x, y), i.e. for r &#8712; R(N, x, y), N &#8594; &#12296;&#947; r ,&#945; r &#12297; was used in at least one derivation involving that cell.</text>
                  <doc_id>50</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For each rule R r , r &#8712; R(N, x, y), we build a lattice L(N, x, y, r).</text>
                  <doc_id>51</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This lattice is derived from the target side of the rule &#945; r by concatenating lattices</text>
                  <doc_id>52</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R 1 : X &#8594; &#12296;s 1 s 2 s 3,t 1 t 2&#12297; R 2 : X &#8594; &#12296;s 1 s 2,t 7 t 8&#12297; R 3 : X &#8594; &#12296;s 3,t 9&#12297; R 4 : S &#8594; &#12296;X,X&#12297; R 5 : S &#8594; &#12296;S X,S X&#12297;</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(S, 1, 3) = L(S, 1, 3, 4) &#8853; L(S, 1, 3, 5) L(S, 1, 3, 4) = L(X, 1, 3) = L(X, 1, 3, 1) = = A(t 1) &#8855; A(t 2)</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(S, 1, 3, 5) = L(S, 1, 2) &#8855; L(X, 3, 1) L(S, 1, 2) = L(S, 1, 2, 4) = L(X, 1, 2) = = L(X, 1, 2, 2) = A(t 7) &#8855; A(t 8)</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(X, 3, 1) = L(X, 3, 1, 3) = A(t 9) L(S, 1, 3, 5) = A(t 7) &#8855; A(t 8) &#8855; A(t 9)</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(S, 1, 3) = (A(t 1) &#8855; A(t 2)) &#8853; (A(t 7) &#8855; A(t 8) &#8855; A(t 9))</text>
                  <doc_id>57</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>corresponding to the elements of &#945; r = &#945; r 1 ...&#945;r |&#945; r | .</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If an &#945; r i is a terminal, creating its lattice is straightforward.</text>
                  <doc_id>59</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>If &#945; r i is a non-terminal, it refers to a cell (N &#8242; , x &#8242; , y &#8242; ) lower in the grid identified by the backpointer BP (N, x, y, r, i); in this case, the lattice used is L(N &#8242; , x &#8242; , y &#8242; ).</text>
                  <doc_id>60</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Taken together,</text>
                  <doc_id>61</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(N, x, y, r) = &#8855;</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1..|&#945; r |</text>
                  <doc_id>63</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(N, x, y, r, i) (1)</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>{ A(&#945;</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(N, x, y, r, i) = i ) if &#945; i &#8712; T L(N &#8242; , x &#8242; , y &#8242; ) else (2) where A(t), t &#8712; T returns a single-arc acceptor which accepts only the symbol t.</text>
                  <doc_id>66</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The lattice L(N, x, y) is then built as the union of lattices corresponding to the rules in R(N, x, y):</text>
                  <doc_id>67</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(N, x, y) = &#8853;</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>r&#8712;R(N,x,y)</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(N, x, y, r) (3)</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Lattice union and concatenation are performed using the &#8853; and &#8855; WFST operations respectively, as described by Allauzen et al.(2007).</text>
                  <doc_id>71</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If a rule R r has a cost c r , it is applied to the exit state of the lattice L(N, x, y, r) prior to the operation of Equation 3.</text>
                  <doc_id>72</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.1.1 An Example of Phrase-based Translation</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 1 illustrates this process for a three word source sentence s 1 s 2 s 3 under monotone phrasebased translation.</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The left-hand side shows the state of the CYK grid after parsing using the rules R 1 to R 5 .</text>
                  <doc_id>75</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>These include 3 rules with only terminals (R 1 , R 2 , R 3 ) and the glue rules (R 4 , R 5 ).</text>
                  <doc_id>76</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Arrows represent backpointers to lower-level cells.</text>
                  <doc_id>77</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We are interested in the upper-most S cell (S, 1, 3), as it represents the search space of translation hypotheses covering the whole source sentence.</text>
                  <doc_id>78</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Two rules (R 4 , R 5 ) are in this cell, so the lattice L(S, 1, 3) will be obtained by the union of the two lattices found by the backpointers of these two rules.</text>
                  <doc_id>79</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>This process is explicitly derived in the right-hand side of Figure 1.</text>
                  <doc_id>80</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.1.2 An Example of Hierarchical Translation</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 2 shows a hierarchical scenario for the same sentence.</text>
                  <doc_id>82</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Three rules, R 6 , R 7 , R 8 , are added to the example of Figure 1, thus providing two additional derivations.</text>
                  <doc_id>83</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This makes use of sublattices already produced in the creation of L(S, 1, 3, 5) and L(X, 1, 3, 1) in Figure 1; these are within {}.</text>
                  <doc_id>84</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 A Procedure for Lattice Construction</title>
            <text>Figure 3 presents an algorithm to build the lattice for every cell. The algorithm uses memoization: if a lattice for a requested cell already exists, it is returned (line 2); otherwise it is constructed via equations 1,2,3. For every rule, each element of the target side (lines 3,4) is checked as terminal or nonterminal (equation 2). If it is a terminal element (line 5), a simple acceptor is built. If it is a nonterminal (line 6), the lattice associated to its backpointer is returned (lines 7 and 8). The complete lattice L(N, x, y, r) for each rule is built by equation 1 (line 9). The lattice L(N, x, y) for this cell is then found by union of all the component rules (line 10, equation 3); this lattice is then reduced by
R 6 : X &#8594; &#12296;s 1,t 20&#12297; R 7 : X &#8594; &#12296;X 1 s 2 X 2,X 1 t 10 X 2&#12297; R 8 : X &#8594; &#12296;X 1 s 2 X 2,X 2 t 10 X 1&#12297; L(S, 1, 3) = L(S, 1, 3, 4) &#8853;{L(S, 1, 3, 5)}
L(S, 1, 3, 4) = L(X, 1, 3) = ={L(X, 1, 3, 1)} &#8853;L(X, 1, 3, 7) &#8853; L(X, 1, 3, 8)
L(X, 1, 3, 7) = L(X, 1, 1, 6) &#8855; A(t 10) &#8855; L(X, 3, 1, 3) = = A(t 20) &#8855; A(t 10) &#8855; A(t 9)
L(X, 1, 3, 8) = A(t 9) &#8855; A(t 10) &#8855; A(t 20)
L(S, 1, 3) = {(A(t 1) &#8855; A(t 2))} &#8853; &#8853;(A(t 20) &#8855; A(t 10) &#8855; A(t 9)) &#8853; (A(t 9) &#8855; A(t 10) &#8855; A(t 20))&#8853; &#8853;{(A(t 7) &#8855; A(t 8) &#8855; A(t 9))}
standard WFST operations (lines 11,12,13). It is important at this point to remove any epsilon arcs which may have been introduced by the various WFST union, concatenation, and replacement operations (Allauzen et al., 2007).
1 function buildFst(N,x,y) 2 if &#8707; L(N, x, y) return L(N, x, y) 3 for r &#8712; R(N, x, y), R r : N &#8594; &#12296;&#947;,&#945;&#12297; 4 for i = 1...|&#945;| 5 if &#945; i &#8712; T, L(N, x, y, r, i) = A(&#945; i ) 6 else 7 (N &#8242; , x &#8242; , y &#8242; ) = BP (&#945; i ) 8 L(N, x, y, r, i) = buildFst(N &#8242; , x &#8242; , y &#8242; ) 9 L(N, x, y, r)= &#8855; i=1..|&#945;| L(N, x, y, r, i) 10 L(N, x, y) = &#8853; r&#8712;R(N,x,y) L(N, x, y, r) 11 fstRmEpsilon L(N, x, y) 12 fstDeterminize L(N, x, y) 13 fstMinimize L(N, x, y) 14 return L(N, x, y)</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Figure 3 presents an algorithm to build the lattice for every cell.</text>
                  <doc_id>85</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The algorithm uses memoization: if a lattice for a requested cell already exists, it is returned (line 2); otherwise it is constructed via equations 1,2,3.</text>
                  <doc_id>86</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For every rule, each element of the target side (lines 3,4) is checked as terminal or nonterminal (equation 2).</text>
                  <doc_id>87</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>If it is a terminal element (line 5), a simple acceptor is built.</text>
                  <doc_id>88</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>If it is a nonterminal (line 6), the lattice associated to its backpointer is returned (lines 7 and 8).</text>
                  <doc_id>89</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The complete lattice L(N, x, y, r) for each rule is built by equation 1 (line 9).</text>
                  <doc_id>90</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The lattice L(N, x, y) for this cell is then found by union of all the component rules (line 10, equation 3); this lattice is then reduced by</text>
                  <doc_id>91</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R 6 : X &#8594; &#12296;s 1,t 20&#12297; R 7 : X &#8594; &#12296;X 1 s 2 X 2,X 1 t 10 X 2&#12297; R 8 : X &#8594; &#12296;X 1 s 2 X 2,X 2 t 10 X 1&#12297; L(S, 1, 3) = L(S, 1, 3, 4) &#8853;{L(S, 1, 3, 5)}</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(S, 1, 3, 4) = L(X, 1, 3) = ={L(X, 1, 3, 1)} &#8853;L(X, 1, 3, 7) &#8853; L(X, 1, 3, 8)</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(X, 1, 3, 7) = L(X, 1, 1, 6) &#8855; A(t 10) &#8855; L(X, 3, 1, 3) = = A(t 20) &#8855; A(t 10) &#8855; A(t 9)</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(X, 1, 3, 8) = A(t 9) &#8855; A(t 10) &#8855; A(t 20)</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(S, 1, 3) = {(A(t 1) &#8855; A(t 2))} &#8853; &#8853;(A(t 20) &#8855; A(t 10) &#8855; A(t 9)) &#8853; (A(t 9) &#8855; A(t 10) &#8855; A(t 20))&#8853; &#8853;{(A(t 7) &#8855; A(t 8) &#8855; A(t 9))}</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>standard WFST operations (lines 11,12,13).</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It is important at this point to remove any epsilon arcs which may have been introduced by the various WFST union, concatenation, and replacement operations (Allauzen et al., 2007).</text>
                  <doc_id>98</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 function buildFst(N,x,y) 2 if &#8707; L(N, x, y) return L(N, x, y) 3 for r &#8712; R(N, x, y), R r : N &#8594; &#12296;&#947;,&#945;&#12297; 4 for i = 1...|&#945;| 5 if &#945; i &#8712; T, L(N, x, y, r, i) = A(&#945; i ) 6 else 7 (N &#8242; , x &#8242; , y &#8242; ) = BP (&#945; i ) 8 L(N, x, y, r, i) = buildFst(N &#8242; , x &#8242; , y &#8242; ) 9 L(N, x, y, r)= &#8855; i=1..|&#945;| L(N, x, y, r, i) 10 L(N, x, y) = &#8853; r&#8712;R(N,x,y) L(N, x, y, r) 11 fstRmEpsilon L(N, x, y) 12 fstDeterminize L(N, x, y) 13 fstMinimize L(N, x, y) 14 return L(N, x, y)</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>2.3 Delayed Translation</title>
            <text>Equation 2 leads to the recursive construction of lattices in upper-levels of the grid through the union and concatenation of lattices from lower levels. If equations 1 and 3 are actually carried out over fully expanded word lattices, the memory required by the upper lattices will increase exponentially.
To avoid this, we use special arcs that serve as pointers to the low-level lattices. This effectively builds a skeleton of the desired lattice and delays the creation of the final word lattice until a single replacement operation is carried out in the top cell (S, 1, J). To make this exact, we define a function g(N, x, y) which returns a unique tag for each lattice in each cell, and use it to redefine equation 2. With the backpointer (N &#8242; , x &#8242; , y &#8242; ) = BP (N, x, y, r, i), these special arcs are introduced as:
{ A(&#945; L(N, x, y, r, i) = i ) if &#945; i &#8712; T A(g(N &#8242; , x &#8242; , y &#8242; )) else (4)
The resulting lattices L(N, x, y) are a mix of target language words and lattice pointers (Figure 4, top). However each still represents the entire search space of all translation hypotheses covering the span. Importantly, operations on these lattices &#8211; such as lossless size reduction via determinization and minimization &#8211; can still be performed. Owing to the existence of multiple hierarchical rules which share the same low-level dependencies, these operations can greatly reduce the size of the skeleton lattice; Figure 4 shows the effect on the translation example. This process is carried out for the lattice at every cell, even at the lowest level where there are only sequences of word terminals. As stated, size reductions can be significant. However not all redudancy is removed, since duplicate paths may arise through the concatenation and union of sublattices with different spans.
At the upper-most cell, the lattice L(S, 1, J) contains pointers to lower-level lattices. A single FST replace operation (Allauzen et al., 2007) recursively substitutes all pointers by their lower-level lattices until no pointers are left, thus producing the complete target word lattice for the whole source sentence. The use of the lattice pointer arc was inspired by the &#8216;lazy evaluation&#8217; techniques developed by Mohri et al (2000). Its implementation uses the infrastructure provided by the OpenFST libraries for
0
0
t1
g(X,1,2)
g(X,1,1)
g(X,3,1)
g(X,1,1)
t1
g(X,3,1)
g(X,1,2)
t10
t10
t10
t10
t2
g(X,3,1)
t2
g(X,3,1)
g(X,1,1)
g(X,3,1)
g(X,1,1)
delayed composition, etc.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Equation 2 leads to the recursive construction of lattices in upper-levels of the grid through the union and concatenation of lattices from lower levels.</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If equations 1 and 3 are actually carried out over fully expanded word lattices, the memory required by the upper lattices will increase exponentially.</text>
                  <doc_id>101</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To avoid this, we use special arcs that serve as pointers to the low-level lattices.</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This effectively builds a skeleton of the desired lattice and delays the creation of the final word lattice until a single replacement operation is carried out in the top cell (S, 1, J).</text>
                  <doc_id>103</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>To make this exact, we define a function g(N, x, y) which returns a unique tag for each lattice in each cell, and use it to redefine equation 2.</text>
                  <doc_id>104</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>With the backpointer (N &#8242; , x &#8242; , y &#8242; ) = BP (N, x, y, r, i), these special arcs are introduced as:</text>
                  <doc_id>105</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>{ A(&#945; L(N, x, y, r, i) = i ) if &#945; i &#8712; T A(g(N &#8242; , x &#8242; , y &#8242; )) else (4)</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The resulting lattices L(N, x, y) are a mix of target language words and lattice pointers (Figure 4, top).</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However each still represents the entire search space of all translation hypotheses covering the span.</text>
                  <doc_id>108</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Importantly, operations on these lattices &#8211; such as lossless size reduction via determinization and minimization &#8211; can still be performed.</text>
                  <doc_id>109</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Owing to the existence of multiple hierarchical rules which share the same low-level dependencies, these operations can greatly reduce the size of the skeleton lattice; Figure 4 shows the effect on the translation example.</text>
                  <doc_id>110</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This process is carried out for the lattice at every cell, even at the lowest level where there are only sequences of word terminals.</text>
                  <doc_id>111</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>As stated, size reductions can be significant.</text>
                  <doc_id>112</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>However not all redudancy is removed, since duplicate paths may arise through the concatenation and union of sublattices with different spans.</text>
                  <doc_id>113</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>At the upper-most cell, the lattice L(S, 1, J) contains pointers to lower-level lattices.</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A single FST replace operation (Allauzen et al., 2007) recursively substitutes all pointers by their lower-level lattices until no pointers are left, thus producing the complete target word lattice for the whole source sentence.</text>
                  <doc_id>115</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The use of the lattice pointer arc was inspired by the &#8216;lazy evaluation&#8217; techniques developed by Mohri et al (2000).</text>
                  <doc_id>116</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Its implementation uses the infrastructure provided by the OpenFST libraries for</text>
                  <doc_id>117</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0</text>
                  <doc_id>119</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>t1</text>
                  <doc_id>120</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>g(X,1,2)</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>g(X,1,1)</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>g(X,3,1)</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>g(X,1,1)</text>
                  <doc_id>124</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>t1</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>g(X,3,1)</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>g(X,1,2)</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>t10</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>t10</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>t10</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>t10</text>
                  <doc_id>131</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>t2</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>g(X,3,1)</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>t2</text>
                  <doc_id>134</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>g(X,3,1)</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>g(X,1,1)</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>g(X,3,1)</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>g(X,1,1)</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>delayed composition, etc.</text>
                  <doc_id>139</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>2.4 Pruning in Lattice Construction</title>
            <text>The final translation lattice L(S, 1, J) can grow very large after the pointer arcs are expanded. We therefore apply a word-based language model, via WFST composition, and perform likelihood-based pruning (Allauzen et al., 2007) based on the combined translation and language model scores.
Pruning can also be performed on sublattices during search. One simple strategy is to monitor the number of states in the determinized lattices L(N, x, y). If this number is above a threshold, we expand any pointer arcs and apply a word-based language model via composition. The resulting lattice is then reduced by likelihood-based pruning, after which the LM scores are removed. This search pruning can be very selective. For example, the pruning threshold can depend on the height of the cell in the grid. In this way the risk of search errors can be controlled.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The final translation lattice L(S, 1, J) can grow very large after the pointer arcs are expanded.</text>
                  <doc_id>140</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We therefore apply a word-based language model, via WFST composition, and perform likelihood-based pruning (Allauzen et al., 2007) based on the combined translation and language model scores.</text>
                  <doc_id>141</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Pruning can also be performed on sublattices during search.</text>
                  <doc_id>142</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>One simple strategy is to monitor the number of states in the determinized lattices L(N, x, y).</text>
                  <doc_id>143</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>If this number is above a threshold, we expand any pointer arcs and apply a word-based language model via composition.</text>
                  <doc_id>144</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The resulting lattice is then reduced by likelihood-based pruning, after which the LM scores are removed.</text>
                  <doc_id>145</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This search pruning can be very selective.</text>
                  <doc_id>146</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For example, the pruning threshold can depend on the height of the cell in the grid.</text>
                  <doc_id>147</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>In this way the risk of search errors can be controlled.</text>
                  <doc_id>148</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Translation Experiments</title>
        <text>We report experiments on the NIST MT08 Arabicto-English and Chinese-to-English translation tasks. We contrast two hierarchical phrase-based decoders. The first decoder, Hiero Cube Pruning (HCP), is a k-
best decoder using cube pruning implemented as described by Chiang (2007). In our implementation, k- best lists contain unique hypotheses. The second decoder, Hiero FST (HiFST), is a lattice-based decoder implemented with Weighted Finite State Transducers as described in the previous section. Hypotheses are generated after determinization under the tropical semiring so that scores assigned to hypotheses arise from single minimum cost / maximum likelihood derivations. We also use a variant of the k-best decoder which works in alignment mode: given an input k-best list, it outputs the feature scores of each hypothesis in the list without applying any pruning. This is used for Minimum Error Training (MET) with the HiFST system.
These two language pairs pose very different translation challenges. For example, Chineseto-English translation requires much greater word movement than Arabic-to-English. In the framework of hierarchical translation systems, we have found that shallow decoding (see section 3.2) is as good as full hierarchical decoding in Arabicto-English (Iglesias et al., 2009). In Chinese-to- English, we have not found this to be the case. Therefore, we contrast the performance of HiFST and HCP under shallow hierarchical decoding for Arabic-to-English, while for Chinese-to-English we perform full hierarchical decoding.
Both hierarchical translation systems share a common architecture. For both language pairs, alignments are generated over the parallel data. The following features are extracted and used in translation: target language model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule count features inspired by Bender et al. (2007). The initial English language model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition. Details of the parallel corpus and development sets used for each language pair are given in their respective section. Standard MET (Och, 2003) iterative parameter estimation under IBM BLEU (Papineni et al., 2001) is performed on the corresponding development set. For the HCP system, MET is done following Chiang (2007). For the HiFST system, we obtain a k-
best list from the translation lattice and extract each feature score with the aligner variant of the k-best decoder. After translation with optimized feature weights, we carry out the two following rescoring steps.
&#8226; Large-LM rescoring. We build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using &#8764;4.7B words of English newswire text, and apply them to rescore either 10000-best lists generated by HCP or word lattices generated by HiFST. Lattices provide a vast search space relative to k-best lists, with translation lattice sizes of 10 81 hypotheses reported in the literature (Tromble et al., 2008).
&#8226; Minimum Bayes Risk (MBR). We rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function (Kumar and Byrne, 2004).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We report experiments on the NIST MT08 Arabicto-English and Chinese-to-English translation tasks.</text>
              <doc_id>149</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We contrast two hierarchical phrase-based decoders.</text>
              <doc_id>150</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The first decoder, Hiero Cube Pruning (HCP), is a k-</text>
              <doc_id>151</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>best decoder using cube pruning implemented as described by Chiang (2007).</text>
              <doc_id>152</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In our implementation, k- best lists contain unique hypotheses.</text>
              <doc_id>153</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The second decoder, Hiero FST (HiFST), is a lattice-based decoder implemented with Weighted Finite State Transducers as described in the previous section.</text>
              <doc_id>154</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Hypotheses are generated after determinization under the tropical semiring so that scores assigned to hypotheses arise from single minimum cost / maximum likelihood derivations.</text>
              <doc_id>155</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We also use a variant of the k-best decoder which works in alignment mode: given an input k-best list, it outputs the feature scores of each hypothesis in the list without applying any pruning.</text>
              <doc_id>156</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This is used for Minimum Error Training (MET) with the HiFST system.</text>
              <doc_id>157</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>These two language pairs pose very different translation challenges.</text>
              <doc_id>158</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For example, Chineseto-English translation requires much greater word movement than Arabic-to-English.</text>
              <doc_id>159</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In the framework of hierarchical translation systems, we have found that shallow decoding (see section 3.2) is as good as full hierarchical decoding in Arabicto-English (Iglesias et al., 2009).</text>
              <doc_id>160</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Chinese-to- English, we have not found this to be the case.</text>
              <doc_id>161</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, we contrast the performance of HiFST and HCP under shallow hierarchical decoding for Arabic-to-English, while for Chinese-to-English we perform full hierarchical decoding.</text>
              <doc_id>162</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Both hierarchical translation systems share a common architecture.</text>
              <doc_id>163</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For both language pairs, alignments are generated over the parallel data.</text>
              <doc_id>164</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The following features are extracted and used in translation: target language model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule count features inspired by Bender et al. (2007).</text>
              <doc_id>165</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The initial English language model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition.</text>
              <doc_id>166</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Details of the parallel corpus and development sets used for each language pair are given in their respective section.</text>
              <doc_id>167</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Standard MET (Och, 2003) iterative parameter estimation under IBM BLEU (Papineni et al., 2001) is performed on the corresponding development set.</text>
              <doc_id>168</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>For the HCP system, MET is done following Chiang (2007).</text>
              <doc_id>169</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>For the HiFST system, we obtain a k-</text>
              <doc_id>170</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>best list from the translation lattice and extract each feature score with the aligner variant of the k-best decoder.</text>
              <doc_id>171</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>After translation with optimized feature weights, we carry out the two following rescoring steps.</text>
              <doc_id>172</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; Large-LM rescoring.</text>
              <doc_id>173</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using &#8764;4.7B words of English newswire text, and apply them to rescore either 10000-best lists generated by HCP or word lattices generated by HiFST.</text>
              <doc_id>174</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Lattices provide a vast search space relative to k-best lists, with translation lattice sizes of 10 81 hypotheses reported in the literature (Tromble et al., 2008).</text>
              <doc_id>175</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; Minimum Bayes Risk (MBR).</text>
              <doc_id>176</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function (Kumar and Byrne, 2004).</text>
              <doc_id>177</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Building the Rule Sets</title>
            <text>We extract hierarchical phrases from word alignments, applying the same restrictions as introduced by Chiang (2005). Additionally, following Iglesias et al. (2009) we carry out two rule filtering strategies:
&#8226; we exclude rules with two non-terminals with the same order on the source and target side
&#8226; we consider only the 20 most frequent translations for each rule
For each development set, this produces approximately 4.3M rules in Arabic-to-English and 2.0M rules in Chinese-to-English.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We extract hierarchical phrases from word alignments, applying the same restrictions as introduced by Chiang (2005).</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Additionally, following Iglesias et al. (2009) we carry out two rule filtering strategies:</text>
                  <doc_id>179</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; we exclude rules with two non-terminals with the same order on the source and target side</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; we consider only the 20 most frequent translations for each rule</text>
                  <doc_id>181</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For each development set, this produces approximately 4.3M rules in Arabic-to-English and 2.0M rules in Chinese-to-English.</text>
                  <doc_id>182</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Arabic-to-English Translation</title>
            <text>We translate Arabic-to-English with shallow hierarchical decoding, i.e. only phrases are allowed to be substituted into non-terminals. The rules used in this case are, in addition to the glue rules:
X &#8594; &#12296;&#947; s ,&#945; s &#12297; X &#8594; &#12296;V ,V &#12297; V &#8594; &#12296;s,t&#12297; s, t &#8712; T + ; &#947; s , &#945; s &#8712; ({V } &#8746; T) +
For translation model training, we use all allowed parallel corpora in the NIST MT08 Arabic track (&#8764;150M words per language). In addition to the MT08 set itself, we use a development set mt02-05- tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form the validation set mt02- 05-test. The mt02-05-tune set has 2,075 sentences.
The cube pruning decoder, HCP, employs k-best lists of depth k=10000 (unique). Using deeper lists results in excessive memory and time requirements. In contrast, the WFST-based decoder, HiFST, requires no local pruning during lattice construction for this task and the language model is not applied until the lattice is fully built at the upper-most cell of the CYK grid. Table 1 shows results for mt02-05-tune, mt02- 05-test and mt08, as measured by lowercased IBM BLEU and TER (Snover et al., 2006). MET parameters are optimized for the HCP decoder. As shown in rows &#8216;a&#8217; and &#8216;b&#8217;, results after MET are comparable.
Search Errors Since both decoders use exactly the same features, we can measure their search errors on a sentence-by-sentence basis. A search error is assigned to one of the decoders if the other has found a hypothesis with lower cost. For mt02-05-tune, we find that in 18.5% of the sentences HiFST finds a hypothesis with lower cost than HCP. In contrast, HCP never finds any hypothesis with lower cost for any sentence. This is as expected: the HiFST decoder requires no pruning prior to applying the language model, so search is exact. Lattice/k-best Quality Rescoring results are different for cube pruning and WFST-based decoders. Whereas HCP improves by 0.9 BLEU, HiFST improves over 1.5 BLEU. Clearly, search errors in HCP not only affect the 1-best output but also the quality of the resulting k-best lists. For HCP, this limits the possible gain from subsequent rescoring steps such as large LMs and MBR.
Translation Speed HCP requires an average of 1.1 seconds per input word. HiFST cuts this time by half, producing output at a rate of 0.5 seconds per word. It proves much more efficient to process compact lattices contaning many hypotheses rather than to independently processing each one of them in k- best form.
The mixed case NIST BLEU-4 for the HiFST system on mt08 is 42.9. This is directly comparable to the official MT08 Constrained Training Track evaluation results 1 .</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We translate Arabic-to-English with shallow hierarchical decoding, i.e. only phrases are allowed to be substituted into non-terminals.</text>
                  <doc_id>183</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The rules used in this case are, in addition to the glue rules:</text>
                  <doc_id>184</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>X &#8594; &#12296;&#947; s ,&#945; s &#12297; X &#8594; &#12296;V ,V &#12297; V &#8594; &#12296;s,t&#12297; s, t &#8712; T + ; &#947; s , &#945; s &#8712; ({V } &#8746; T) +</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For translation model training, we use all allowed parallel corpora in the NIST MT08 Arabic track (&#8764;150M words per language).</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In addition to the MT08 set itself, we use a development set mt02-05- tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form the validation set mt02- 05-test.</text>
                  <doc_id>187</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The mt02-05-tune set has 2,075 sentences.</text>
                  <doc_id>188</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The cube pruning decoder, HCP, employs k-best lists of depth k=10000 (unique).</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Using deeper lists results in excessive memory and time requirements.</text>
                  <doc_id>190</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast, the WFST-based decoder, HiFST, requires no local pruning during lattice construction for this task and the language model is not applied until the lattice is fully built at the upper-most cell of the CYK grid.</text>
                  <doc_id>191</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Table 1 shows results for mt02-05-tune, mt02- 05-test and mt08, as measured by lowercased IBM BLEU and TER (Snover et al., 2006).</text>
                  <doc_id>192</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>MET parameters are optimized for the HCP decoder.</text>
                  <doc_id>193</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>As shown in rows &#8216;a&#8217; and &#8216;b&#8217;, results after MET are comparable.</text>
                  <doc_id>194</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Search Errors Since both decoders use exactly the same features, we can measure their search errors on a sentence-by-sentence basis.</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A search error is assigned to one of the decoders if the other has found a hypothesis with lower cost.</text>
                  <doc_id>196</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For mt02-05-tune, we find that in 18.5% of the sentences HiFST finds a hypothesis with lower cost than HCP.</text>
                  <doc_id>197</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast, HCP never finds any hypothesis with lower cost for any sentence.</text>
                  <doc_id>198</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This is as expected: the HiFST decoder requires no pruning prior to applying the language model, so search is exact.</text>
                  <doc_id>199</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Lattice/k-best Quality Rescoring results are different for cube pruning and WFST-based decoders.</text>
                  <doc_id>200</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Whereas HCP improves by 0.9 BLEU, HiFST improves over 1.5 BLEU.</text>
                  <doc_id>201</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Clearly, search errors in HCP not only affect the 1-best output but also the quality of the resulting k-best lists.</text>
                  <doc_id>202</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>For HCP, this limits the possible gain from subsequent rescoring steps such as large LMs and MBR.</text>
                  <doc_id>203</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Translation Speed HCP requires an average of 1.1 seconds per input word.</text>
                  <doc_id>204</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>HiFST cuts this time by half, producing output at a rate of 0.5 seconds per word.</text>
                  <doc_id>205</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It proves much more efficient to process compact lattices contaning many hypotheses rather than to independently processing each one of them in k- best form.</text>
                  <doc_id>206</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The mixed case NIST BLEU-4 for the HiFST system on mt08 is 42.9.</text>
                  <doc_id>207</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This is directly comparable to the official MT08 Constrained Training Track evaluation results 1 .</text>
                  <doc_id>208</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Chinese-to-English Translation</title>
            <text>We translate Chinese-to-English with full hierarchical decoding, i.e. hierarchical rules are allowed to be substituted into non-terminals. We consider a maximum span of 10 words for the application of hierarchical rules and only glue rules are allowed at upper levels of the CYK grid. For translation model training, we use all available data for the GALE 2008 evaluation 2 , approx. 250M words per language. In addition to the MT08 set itself, we use a development set tune-nw and a validation set test-nw. These contain a mix of the newswire portions of MT02 through MT05 and additional developments sets created by translation within the GALE program. The tune-nw set has 1,755 sentences. Again, the HCP decoder employs k-best lists of depth k=10000. The HiFST decoder applies pruning in search as described in Section 2.4, so that any lattice in the CYK grid is pruned if it covers at least 3 source words and contains more than 10k states. The likelihood pruning threshold relative to the best path in the lattice is 9. This is a very broad threshold so that very few paths are discarded.
1 Full MT08 results are available at http://www.nist.gov/
speech/tests/mt/2008/doc/mt08 official results v0.html. It is worth noting that many of the top entries make use of system combination; the results reported here are for single system translation. 2 See http://projects.ldc.upenn.edu/gale/data/catalog.html.
Improved Optimization Table 2 shows results for tune-nw, test-nw and mt08, as measured by lowercased IBM BLEU and TER. The first two rows show results for HCP when using MET parameters optimized over k-best lists produced by HCP (row &#8216;a&#8217;) and by HiFST (row &#8216;b&#8217;). We find that using the k- best list obtained by the HiFST decoder yields better parameters during optimization. Tuning on the HiFST k-best lists improves the HCP BLEU score, as well. We find consistent improvements in BLEU; TER also improves overall, although less consistently.
Search Errors Measured over the tune-nw development set, HiFST finds a hypothesis with lower cost in 48.4% of the sentences. In contrast, HCP never finds any hypothesis with a lower cost for any sentence, indicating that the described pruning strategy for HiFST is much broader than that of HCP. Note that HCP search errors are more frequent for this language pair. This is due to the larger search space required in fully hierarchical translation; the larger the search space, the more search errors will be produced by the cube pruning k-best implementation.
Lattice/k-best Quality The lattices produced by HiFST yield greater gains in LM rescoring than the k-best lists produced by HCP. Including the subsequent MBR rescoring, translation improves as much as 1.2 BLEU, compared to 0.7 BLEU with HCP. The mixed case NIST BLEU-4 for the HiFST system on mt08 is 27.8, comparable to official results in the UnConstrained Training Track of the NIST 2008 evaluation.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We translate Chinese-to-English with full hierarchical decoding, i.e. hierarchical rules are allowed to be substituted into non-terminals.</text>
                  <doc_id>209</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We consider a maximum span of 10 words for the application of hierarchical rules and only glue rules are allowed at upper levels of the CYK grid.</text>
                  <doc_id>210</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For translation model training, we use all available data for the GALE 2008 evaluation 2 , approx.</text>
                  <doc_id>211</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>250M words per language.</text>
                  <doc_id>212</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In addition to the MT08 set itself, we use a development set tune-nw and a validation set test-nw.</text>
                  <doc_id>213</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>These contain a mix of the newswire portions of MT02 through MT05 and additional developments sets created by translation within the GALE program.</text>
                  <doc_id>214</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The tune-nw set has 1,755 sentences.</text>
                  <doc_id>215</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Again, the HCP decoder employs k-best lists of depth k=10000.</text>
                  <doc_id>216</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>The HiFST decoder applies pruning in search as described in Section 2.4, so that any lattice in the CYK grid is pruned if it covers at least 3 source words and contains more than 10k states.</text>
                  <doc_id>217</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>The likelihood pruning threshold relative to the best path in the lattice is 9.</text>
                  <doc_id>218</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>This is a very broad threshold so that very few paths are discarded.</text>
                  <doc_id>219</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 Full MT08 results are available at http://www.nist.gov/</text>
                  <doc_id>220</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>speech/tests/mt/2008/doc/mt08 official results v0.html.</text>
                  <doc_id>221</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It is worth noting that many of the top entries make use of system combination; the results reported here are for single system translation.</text>
                  <doc_id>222</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>2 See http://projects.ldc.upenn.edu/gale/data/catalog.html.</text>
                  <doc_id>223</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Improved Optimization Table 2 shows results for tune-nw, test-nw and mt08, as measured by lowercased IBM BLEU and TER.</text>
                  <doc_id>224</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The first two rows show results for HCP when using MET parameters optimized over k-best lists produced by HCP (row &#8216;a&#8217;) and by HiFST (row &#8216;b&#8217;).</text>
                  <doc_id>225</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We find that using the k- best list obtained by the HiFST decoder yields better parameters during optimization.</text>
                  <doc_id>226</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Tuning on the HiFST k-best lists improves the HCP BLEU score, as well.</text>
                  <doc_id>227</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We find consistent improvements in BLEU; TER also improves overall, although less consistently.</text>
                  <doc_id>228</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Search Errors Measured over the tune-nw development set, HiFST finds a hypothesis with lower cost in 48.4% of the sentences.</text>
                  <doc_id>229</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast, HCP never finds any hypothesis with a lower cost for any sentence, indicating that the described pruning strategy for HiFST is much broader than that of HCP.</text>
                  <doc_id>230</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Note that HCP search errors are more frequent for this language pair.</text>
                  <doc_id>231</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This is due to the larger search space required in fully hierarchical translation; the larger the search space, the more search errors will be produced by the cube pruning k-best implementation.</text>
                  <doc_id>232</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Lattice/k-best Quality The lattices produced by HiFST yield greater gains in LM rescoring than the k-best lists produced by HCP.</text>
                  <doc_id>233</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Including the subsequent MBR rescoring, translation improves as much as 1.2 BLEU, compared to 0.7 BLEU with HCP.</text>
                  <doc_id>234</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The mixed case NIST BLEU-4 for the HiFST system on mt08 is 27.8, comparable to official results in the UnConstrained Training Track of the NIST 2008 evaluation.</text>
                  <doc_id>235</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Conclusions</title>
        <text>The lattice-based decoder for hierarchical phrasebased translation described in this paper can be easily implemented using Weighted Finite State Transducers. We find many benefits in this approach to translation. From a practical perspective, the computational operations required can be easily carried out using standard operations already implemented in general purpose libraries. From a modeling perspective, the compact representation of multiple translation hypotheses in lattice form requires less pruning in hierarchical search. The result is fewer search errors and reduced overall memory use relative to cube pruning over k-best lists. We also find improved performance of subsequent rescoring procedures which rely on the translation scores. In direct comparison to k-best lists generated under cube pruning, we find that MET parameter optimization, rescoring with large language models, and MBR decoding, are all improved when applied to translations generated by the lattice-based hierarchical decoder.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The lattice-based decoder for hierarchical phrasebased translation described in this paper can be easily implemented using Weighted Finite State Transducers.</text>
              <doc_id>236</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We find many benefits in this approach to translation.</text>
              <doc_id>237</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>From a practical perspective, the computational operations required can be easily carried out using standard operations already implemented in general purpose libraries.</text>
              <doc_id>238</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>From a modeling perspective, the compact representation of multiple translation hypotheses in lattice form requires less pruning in hierarchical search.</text>
              <doc_id>239</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The result is fewer search errors and reduced overall memory use relative to cube pruning over k-best lists.</text>
              <doc_id>240</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We also find improved performance of subsequent rescoring procedures which rely on the translation scores.</text>
              <doc_id>241</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In direct comparison to k-best lists generated under cube pruning, we find that MET parameter optimization, rescoring with large language models, and MBR decoding, are all improved when applied to translations generated by the lattice-based hierarchical decoder.</text>
              <doc_id>242</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>Acknowledgments</title>
        <text>This work was supported in part by the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011- 06-C-0022. G. Iglesias supported by Spanish Government research grant BES-2007-15956 (project TEC2006-13694- C03-03).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This work was supported in part by the GALE program of the Defense Advanced Research Projects Agency, Contract No.</text>
              <doc_id>243</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>HR0011- 06-C-0022.</text>
              <doc_id>244</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>G. Iglesias supported by Spanish Government research grant BES-2007-15956 (project TEC2006-13694- C03-03).</text>
              <doc_id>245</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Constrative Arabic-to-English translation results (lower-cased IBM BLEU | TER) after MET and subsequent rescoring steps. Decoding time reported for mt02-05-tune.</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>decoder</cell>
              <cell>mt02-05-tune</cell>
              <cell>mt02-05-test</cell>
              <cell>mt08</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell></cell>
              <cell></cell>
              <cell>BLEU</cell>
              <cell>TER</cell>
              <cell>BLEU</cell>
              <cell>TER</cell>
              <cell>BLEU</cell>
              <cell>TER</cell>
            </row>
            <row>
              <cell>a</cell>
              <cell>HCP</cell>
              <cell>52.2</cell>
              <cell>41.6</cell>
              <cell>51.5</cell>
              <cell>42.2</cell>
              <cell>42.5</cell>
              <cell>48.6</cell>
            </row>
            <row>
              <cell></cell>
              <cell>+5gram</cell>
              <cell>53.1</cell>
              <cell>41.0</cell>
              <cell>52.5</cell>
              <cell>41.5</cell>
              <cell>43.3</cell>
              <cell>48.3</cell>
            </row>
            <row>
              <cell></cell>
              <cell>+MBR</cell>
              <cell>53.2</cell>
              <cell>40.8</cell>
              <cell>52.6</cell>
              <cell>41.4</cell>
              <cell>43.4</cell>
              <cell>48.1</cell>
            </row>
            <row>
              <cell>b</cell>
              <cell>HiFST</cell>
              <cell>52.2</cell>
              <cell>41.5</cell>
              <cell>51.6</cell>
              <cell>42.1</cell>
              <cell>42.4</cell>
              <cell>48.7</cell>
            </row>
            <row>
              <cell></cell>
              <cell>+5gram</cell>
              <cell>53.3</cell>
              <cell>40.6</cell>
              <cell>52.7</cell>
              <cell>41.3</cell>
              <cell>43.7</cell>
              <cell>48.1</cell>
            </row>
            <row>
              <cell></cell>
              <cell>+MBR</cell>
              <cell>53.7</cell>
              <cell>40.4</cell>
              <cell>53.3</cell>
              <cell>40.9</cell>
              <cell>44.0</cell>
              <cell>48.0</cell>
            </row>
            <row>
              <cell>Decoding time in secs/word: 1.1 for HCP; 0.5 for HiFST.</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Contrastive Chinese-to-English translation results (lower-cased IBM BLEU|TER) after MET and subsequent rescoring steps. The MET k-best column indicates which decoder generated the k-best lists used in MET optimization.</caption>
        <reference_text>In PAGE 7: ...See http://projects.ldc.upenn.edu/gale/data/catalog.html. Improved Optimization  Table2  shows results for tune-nw, test-nw and mt08, as measured by lower- cased IBM BLEU and TER. The first two rows show results for HCP when using MET parameters opti- mized over k-best lists produced by HCP (row ?a?) and by HiFST (row ?b?)....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>decoder</cell>
              <cell>MET k-best</cell>
              <cell>tune-nw</cell>
              <cell>test-nw</cell>
              <cell>mt08</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell>BLEU</cell>
              <cell>TER</cell>
              <cell>BLEU</cell>
              <cell>TER</cell>
              <cell>BLEU</cell>
              <cell>TER</cell>
            </row>
            <row>
              <cell>a</cell>
              <cell>HCP</cell>
              <cell>HCP</cell>
              <cell>31.6</cell>
              <cell>59.7</cell>
              <cell>31.9</cell>
              <cell>59.7</cell>
              <cell>&#8211;</cell>
              <cell>&#8211;</cell>
            </row>
            <row>
              <cell>b</cell>
              <cell>HCP</cell>
              <cell></cell>
              <cell>31.7</cell>
              <cell>60.0</cell>
              <cell>32.2</cell>
              <cell>59.9</cell>
              <cell>27.2</cell>
              <cell>60.2</cell>
            </row>
            <row>
              <cell></cell>
              <cell>+5gram</cell>
              <cell>HiFST</cell>
              <cell>32.2</cell>
              <cell>59.3</cell>
              <cell>32.6</cell>
              <cell>59.4</cell>
              <cell>27.8</cell>
              <cell>59.3</cell>
            </row>
            <row>
              <cell></cell>
              <cell>+MBR</cell>
              <cell></cell>
              <cell>32.4</cell>
              <cell>59.2</cell>
              <cell>32.7</cell>
              <cell>59.4</cell>
              <cell>28.1</cell>
              <cell>59.3</cell>
            </row>
            <row>
              <cell>c</cell>
              <cell>HiFST</cell>
              <cell></cell>
              <cell>32.0</cell>
              <cell>60.1</cell>
              <cell>32.2</cell>
              <cell>60.0</cell>
              <cell>27.1</cell>
              <cell>60.5</cell>
            </row>
            <row>
              <cell></cell>
              <cell>+5gram</cell>
              <cell>HiFST</cell>
              <cell>32.7</cell>
              <cell>58.3</cell>
              <cell>33.1</cell>
              <cell>58.4</cell>
              <cell>28.1</cell>
              <cell>59.1</cell>
            </row>
            <row>
              <cell></cell>
              <cell>+MBR</cell>
              <cell></cell>
              <cell>32.9</cell>
              <cell>58.4</cell>
              <cell>33.4</cell>
              <cell>58.5</cell>
              <cell>28.9</cell>
              <cell>58.9</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Cyril Allauzen</author>
          <author>Mehryar Mohri</author>
          <author>Brian Roark</author>
        </authors>
        <title>Generalized algorithms for constructing statistical language models.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>557--564</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Cyril Allauzen</author>
          <author>Michael Riley</author>
          <author>Johan Schalkwyk</author>
          <author>Wojciech Skut</author>
          <author>Mehryar Mohri</author>
        </authors>
        <title>OpenFst: A general and efficient weighted finite-state transducer library.</title>
        <publication>In Proceedings of CIAA,</publication>
        <pages>11--23</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Srinivas Bangalore</author>
          <author>Giuseppe Riccardi</author>
        </authors>
        <title>A finite-state approach to machine translation.</title>
        <publication>In Proceedings of NAACL.</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Oliver Bender</author>
          <author>Evgeny Matusov</author>
          <author>Stefan Hahn</author>
          <author>Sasa Hasan</author>
          <author>Shahram Khadivi</author>
          <author>Hermann Ney</author>
        </authors>
        <title>The RWTH Arabic-to-English spoken language translation system.</title>
        <publication>In Proceedings of ASRU,</publication>
        <pages>396--401</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Phil Blunsom</author>
          <author>Trevor Cohn</author>
          <author>Miles Osborne</author>
        </authors>
        <title>A discriminative latent variable model for statistical machine translation.</title>
        <publication>In Proceedings of ACL-HLT,</publication>
        <pages>200--208</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Thorsten Brants</author>
          <author>Ashok C Popat</author>
          <author>Peng Xu</author>
          <author>Franz J Och</author>
          <author>Jeffrey Dean</author>
        </authors>
        <title>Large language models in machine translation.</title>
        <publication>In Proceedings of EMNLP-ACL,</publication>
        <pages>858--867</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Francisco Casacuberta</author>
        </authors>
        <title>Finite-state transducers for speech-input translation.</title>
        <publication>In Proceedings of ASRU.</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Jean-C&#233;dric Chappelier</author>
          <author>Martin Rajman</author>
        </authors>
        <title>A generalized CYK algorithm for parsing stochastic CFG.</title>
        <publication>In Proceedings of TAPD,</publication>
        <pages>133--137</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Jean-C&#233;dric Chappelier</author>
          <author>Martin Rajman</author>
          <author>Ram&#243;n Arag&#252;&#233;s</author>
          <author>Antoine Rozenknop</author>
        </authors>
        <title>Lattice parsing for speech recognition.</title>
        <publication>In Proceedings of TALN,</publication>
        <pages>95--104</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>A hierarchical phrase-based model for statistical machine translation.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>263--270</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Chiang</author>
        </authors>
        <title>Hierarchical phrase-based translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Christopher Dyer</author>
          <author>Smaranda Muresan</author>
          <author>Philip Resnik</author>
        </authors>
        <title>Generalizing word lattice translation.</title>
        <publication>In Proceedings of ACL-HLT,</publication>
        <pages>1012--1020</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Jonathan Graehl</author>
          <author>Kevin Knight</author>
          <author>Jonathan May</author>
        </authors>
        <title>Training tree transducers.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Liang Huang</author>
          <author>David Chiang</author>
        </authors>
        <title>Forest rescoring: Faster decoding with integrated language models.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>144--151</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Gonzalo Iglesias</author>
          <author>Adri&#224; de Gispert</author>
          <author>Eduardo R Banga</author>
          <author>William Byrne</author>
        </authors>
        <title>Rule filtering by pattern for efficient hierarchical translation.</title>
        <publication>In Proceedings of EACL.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Shankar Kumar</author>
          <author>William Byrne</author>
        </authors>
        <title>Minimum Bayes-risk decoding for statistical machine translation.</title>
        <publication>In Proceedings of HLT-NAACL,</publication>
        <pages>169--176</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Shankar Kumar</author>
          <author>William Byrne</author>
        </authors>
        <title>Local phrase reordering models for statistical machine translation.</title>
        <publication>In Proceedings of HLT-EMNLP,</publication>
        <pages>161--168</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Zhifei Li</author>
          <author>Sanjeev Khudanpur</author>
        </authors>
        <title>A scalable decoder for parsing-based machine translation with equivalent language model state maintenance.</title>
        <publication>In Proceedings of the ACL-HLT Second Workshop on Syntax and Structure in Statistical Translation,</publication>
        <pages>10--18</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Adam Lopez</author>
        </authors>
        <title>Tera-scale translation models via pattern matching.</title>
        <publication>In Proceedings of COLING,</publication>
        <pages>505--512</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Yuval Marton</author>
          <author>Philip Resnik</author>
        </authors>
        <title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
        <publication>In Proceedings of ACL-HLT,</publication>
        <pages>1003--1011</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Lambert Mathias</author>
          <author>William Byrne</author>
        </authors>
        <title>Statistical phrase-based speech translation.</title>
        <publication>In Proceedings of ICASSP.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Mehryar Mohri</author>
          <author>Fernando Pereira</author>
          <author>Michael Riley</author>
        </authors>
        <title>The design principles of a weighted finitestate transducer library.</title>
        <publication>None</publication>
        <pages>231--17</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Mehryar Mohri</author>
          <author>Fernando Pereira</author>
          <author>Michael Riley</author>
        </authors>
        <title>Weighted finite-state transducers in speech recognition.</title>
        <publication>In Computer Speech and Language,</publication>
        <pages>69--88</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Franz J Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>160--167</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>BLEU: a method for automatic evaluation of machine translation.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>311--318</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Antti-Veikko Rosti</author>
          <author>Necip Fazil Ayan</author>
          <author>Bing Xiang</author>
          <author>Spyros Matsoukas</author>
          <author>Richard Schwartz</author>
          <author>Bonnie Dorr</author>
        </authors>
        <title>Combining outputs from multiple machine translation systems.</title>
        <publication>In Proceedings of HLT-NAACL,</publication>
        <pages>228--235</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>Libin Shen</author>
          <author>Jinxi Xu</author>
          <author>Ralph Weischedel</author>
        </authors>
        <title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
        <publication>In Proceedings of ACL-HLT,</publication>
        <pages>577--585</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>27</id>
        <authors>
          <author>Khe Chai Sim</author>
          <author>William Byrne</author>
          <author>Mark Gales</author>
          <author>Hichem Sahbi</author>
          <author>Phil Woodland</author>
        </authors>
        <title>Consensus network decoding for statistical machine translation system combination.</title>
        <publication>In Proceedings of ICASSP,</publication>
        <pages>105--108</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>28</id>
        <authors>
          <author>Matthew Snover</author>
          <author>Bonnie J Dorr</author>
          <author>Richard Schwartz</author>
          <author>Linnea Micciulla</author>
          <author>John Makhoul</author>
        </authors>
        <title>A study of translation edit rate with targeted human annotation.</title>
        <publication>In Proceedings of AMTA,</publication>
        <pages>223--231</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>29</id>
        <authors>
          <author>Roy Tromble</author>
          <author>Shankar Kumar</author>
          <author>Franz J Och</author>
          <author>Wolfgang Macherey</author>
        </authors>
        <title>Lattice Minimum Bayes-Risk decoding for statistical machine translation.</title>
        <publication>In Proceedings of EMNLP,</publication>
        <pages>620--629</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>30</id>
        <authors>
          <author>Ashish Venugopal</author>
          <author>Andreas Zollmann</author>
          <author>Vogel Stephan</author>
        </authors>
        <title>An efficient two-pass approach to synchronous-CFG driven statistical MT.</title>
        <publication>In Proceedings of HLT-NAACL,</publication>
        <pages>500--507</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>31</id>
        <authors>
          <author>Hao Zhang</author>
          <author>Daniel Gildea</author>
        </authors>
        <title>Synchronous binarization for machine translation.</title>
        <publication>In Proceedings of HLT-NAACL,</publication>
        <pages>256--263</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>32</id>
        <authors>
          <author>Andreas Zollmann</author>
          <author>Ashish Venugopal</author>
        </authors>
        <title>Syntax augmented machine translation via chart parsing.</title>
        <publication>In Proceedings of NAACL Workshop on Statistical Machine Translation,</publication>
        <pages>138--141</pages>
        <date>2006</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Allauzen et al., 2003</string>
        <sentence_id>3655</sentence_id>
        <char_offset>128</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Allauzen et al., 2007</string>
        <sentence_id>3537</sentence_id>
        <char_offset>42</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>Allauzen et al., 2007</string>
        <sentence_id>3657</sentence_id>
        <char_offset>52</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>1</reference_id>
        <string>Allauzen et al., 2007</string>
        <sentence_id>3589</sentence_id>
        <char_offset>158</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>1</reference_id>
        <string>Allauzen et al., 2007</string>
        <sentence_id>3606</sentence_id>
        <char_offset>32</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>1</reference_id>
        <string>Allauzen et al., 2007</string>
        <sentence_id>3632</sentence_id>
        <char_offset>108</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>2</reference_id>
        <string>Bangalore and Riccardi, 2001</string>
        <sentence_id>3525</sentence_id>
        <char_offset>113</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>3</reference_id>
        <string>Bender et al. (2007)</string>
        <sentence_id>3732</sentence_id>
        <char_offset>306</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>4</reference_id>
        <string>Blunsom et al., 2008</string>
        <sentence_id>3521</sentence_id>
        <char_offset>266</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>5</reference_id>
        <string>Brants et al., 2007</string>
        <sentence_id>3741</sentence_id>
        <char_offset>54</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>6</reference_id>
        <string>Casacuberta, 2001</string>
        <sentence_id>3525</sentence_id>
        <char_offset>143</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>7</reference_id>
        <string>Chappelier and Rajman, 1998</string>
        <sentence_id>3640</sentence_id>
        <char_offset>91</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>8</reference_id>
        <string>Chappelier et al. (1999)</string>
        <sentence_id>3520</sentence_id>
        <char_offset>105</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>9</reference_id>
        <string>Chiang (2005</string>
        <sentence_id>3641</sentence_id>
        <char_offset>35</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>9</reference_id>
        <string>Chiang (2005</string>
        <sentence_id>3658</sentence_id>
        <char_offset>102</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>9</reference_id>
        <string>Chiang (2005)</string>
        <sentence_id>3658</sentence_id>
        <char_offset>102</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>9</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>3529</sentence_id>
        <char_offset>129</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>9</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>3530</sentence_id>
        <char_offset>142</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>10</reference_id>
        <string>Chiang (2007)</string>
        <sentence_id>3517</sentence_id>
        <char_offset>35</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>10</reference_id>
        <string>Chiang (2007)</string>
        <sentence_id>3518</sentence_id>
        <char_offset>195</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>10</reference_id>
        <string>Chiang (2007)</string>
        <sentence_id>3719</sentence_id>
        <char_offset>60</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>10</reference_id>
        <string>Chiang (2007)</string>
        <sentence_id>3736</sentence_id>
        <char_offset>42</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>10</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>3530</sentence_id>
        <char_offset>156</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>11</reference_id>
        <string>Dyer et al. (2008)</string>
        <sentence_id>3520</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>12</reference_id>
        <string>Graehl et al., 2008</string>
        <sentence_id>3525</sentence_id>
        <char_offset>210</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>13</reference_id>
        <string>Huang and Chiang (2007)</string>
        <sentence_id>3517</sentence_id>
        <char_offset>25</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>15</reference_id>
        <string>Kumar and Byrne, 2004</string>
        <sentence_id>3744</sentence_id>
        <char_offset>120</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>16</reference_id>
        <string>Kumar and Byrne, 2005</string>
        <sentence_id>3525</sentence_id>
        <char_offset>162</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>17</reference_id>
        <string>Li and Khudanpur (2008)</string>
        <sentence_id>3519</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>18</reference_id>
        <string>Lopez (2008)</string>
        <sentence_id>3523</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>19</reference_id>
        <string>Marton and Resnik, 2008</string>
        <sentence_id>3521</sentence_id>
        <char_offset>188</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>20</reference_id>
        <string>Mathias and Byrne, 2006</string>
        <sentence_id>3525</sentence_id>
        <char_offset>185</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>21</reference_id>
        <string>Mohri et al (2000)</string>
        <sentence_id>3607</sentence_id>
        <char_offset>97</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>23</reference_id>
        <string>Och, 2003</string>
        <sentence_id>3735</sentence_id>
        <char_offset>14</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>24</reference_id>
        <string>Papineni et al., 2001</string>
        <sentence_id>3735</sentence_id>
        <char_offset>72</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>25</reference_id>
        <string>Rosti et al., 2007</string>
        <sentence_id>3524</sentence_id>
        <char_offset>137</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>26</reference_id>
        <string>Shen et al., 2008</string>
        <sentence_id>3521</sentence_id>
        <char_offset>169</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>27</reference_id>
        <string>Sim et al., 2007</string>
        <sentence_id>3524</sentence_id>
        <char_offset>119</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>28</reference_id>
        <string>Snover et al., 2006</string>
        <sentence_id>3672</sentence_id>
        <char_offset>108</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>29</reference_id>
        <string>Tromble et al., 2008</string>
        <sentence_id>3742</sentence_id>
        <char_offset>142</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>30</reference_id>
        <string>Venugopal et al. (2007)</string>
        <sentence_id>3518</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>31</reference_id>
        <string>Zhang and Gildea, 2006</string>
        <sentence_id>3521</sentence_id>
        <char_offset>145</char_offset>
      </citation>
      <citation>
        <id>42</id>
        <reference_id>32</reference_id>
        <string>Zollmann and Venugopal, 2006</string>
        <sentence_id>3521</sentence_id>
        <char_offset>115</char_offset>
      </citation>
    </citations>
  </content>
</document>
