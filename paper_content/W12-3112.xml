<document>
  <filename>W12-3112</filename>
  <authors>
    <author>Christian Hardmeier</author>
    <author>Joakim Nivre</author>
  </authors>
  <title>Tree Kernels for Machine Translation Quality Estimation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>This paper describes Uppsala University&#8217;s submissions to the Quality Estimation (QE) shared task at WMT 2012. We present a QE system based on Support Vector Machine regression, using a number of explicitly defined features extracted from the Machine Translation input, output and models in combination with tree kernels over constituency and dependency parse trees for the input and output sentences. We confirm earlier results suggesting that tree kernels can be a useful tool for QE system construction especially in the early stages of system design.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This paper describes Uppsala University&#8217;s submissions to the Quality Estimation (QE) shared task at WMT 2012.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We present a QE system based on Support Vector Machine regression, using a number of explicitly defined features extracted from the Machine Translation input, output and models in combination with tree kernels over constituency and dependency parse trees for the input and output sentences.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We confirm earlier results suggesting that tree kernels can be a useful tool for QE system construction especially in the early stages of system design.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>The goal of the WMT 2012 Quality Estimation (QE) shared task (Callison-Burch et al., 2012) was to create automatic systems to judge the quality of the translations produced by a Statistical Machine Translation (SMT) system given the input text, the proposed translations and information about the models used by the SMT system. The shared task organisers provided a training set of 1832 sentences drawn from earlier WMT Machine Translation test sets, translated from English to Spanish with a phrase-based SMT system, along with the models used and diagnostic output produced by the SMT system as well as manual translation quality annotations on a 1&#8211;5 scale for each sentence. Additionally, a set of 17 baseline features was made available to the participants. Systems were evaluated on a test set of 422 sentences annotated in the same way.
Uppsala University submitted two systems to this shared task. Our systems were fairly successful and achieved results that were outperformed by only one competing group. They improve over the baseline performance in two ways, building on and extending earlier work by Hardmeier (2011), on which the system description in the following sections is partly based: On the one hand, we enhance the set of 17 baseline features provided by the organisers with another 82 explicitly defined features. On the other hand, we use syntactic tree kernels to extract implicit features from constituency and dependency parse trees over the input sentences and the Machine Translation (MT) output. The experimental results confirm the findings of our earlier work, showing tree kernels to be a valuable tool for rapid prototyping of QE systems.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The goal of the WMT 2012 Quality Estimation (QE) shared task (Callison-Burch et al., 2012) was to create automatic systems to judge the quality of the translations produced by a Statistical Machine Translation (SMT) system given the input text, the proposed translations and information about the models used by the SMT system.</text>
              <doc_id>3</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The shared task organisers provided a training set of 1832 sentences drawn from earlier WMT Machine Translation test sets, translated from English to Spanish with a phrase-based SMT system, along with the models used and diagnostic output produced by the SMT system as well as manual translation quality annotations on a 1&#8211;5 scale for each sentence.</text>
              <doc_id>4</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Additionally, a set of 17 baseline features was made available to the participants.</text>
              <doc_id>5</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Systems were evaluated on a test set of 422 sentences annotated in the same way.</text>
              <doc_id>6</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Uppsala University submitted two systems to this shared task.</text>
              <doc_id>7</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our systems were fairly successful and achieved results that were outperformed by only one competing group.</text>
              <doc_id>8</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>They improve over the baseline performance in two ways, building on and extending earlier work by Hardmeier (2011), on which the system description in the following sections is partly based: On the one hand, we enhance the set of 17 baseline features provided by the organisers with another 82 explicitly defined features.</text>
              <doc_id>9</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>On the other hand, we use syntactic tree kernels to extract implicit features from constituency and dependency parse trees over the input sentences and the Machine Translation (MT) output.</text>
              <doc_id>10</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The experimental results confirm the findings of our earlier work, showing tree kernels to be a valuable tool for rapid prototyping of QE systems.</text>
              <doc_id>11</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Features</title>
        <text>Our QE systems used two types of features: On the one hand, we used a set of explicit features that were extracted from the data before running the Machine Learning (ML) component. On the other hand, syntactic parse trees of the MT input and output sentences provided implicit features that were computed directly by the ML component using tree kernels.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Our QE systems used two types of features: On the one hand, we used a set of explicit features that were extracted from the data before running the Machine Learning (ML) component.</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>On the other hand, syntactic parse trees of the MT input and output sentences provided implicit features that were computed directly by the ML component using tree kernels.</text>
              <doc_id>13</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Explicit features</title>
            <text>Both of the QE systems we submitted to the shared task used the complete set of 17 baseline features provided by the workshop organisers. Additionally, the UU best system also contained all the features presented by Hardmeier (2011) with the exception
of a few features specific to the film subtitle genre and inapplicable to the text type of the shared task, as well as a small number of features not included in that work. Many of these features were modelled on QE features described by Specia et al. (2009). In particular, the following features were included in addition to the baseline feature set:
&#8226; number of words, length ratio (4 features)
&#8226; source and target type-token ratios (2 features)
&#8226; number of tokens matching particular patterns (3 features each):
&#8211; numbers &#8211; opening and closing parentheses &#8211; strong punctuation signs &#8211; weak punctuation signs &#8211; ellipsis signs &#8211; hyphens &#8211; single and double quotes &#8211; apostrophe-s tokens &#8211; short alphabetic tokens (&#8804; 3 letters) &#8211; long alphabetic tokens (&#8805; 4 letters)
&#8226; source and target language model (LM) and log-LM scores (4 features)
&#8226; LM and log-LM scores normalised by sentence length (4 features)
&#8226; number and percentage of out-of-vocabulary words (2 features)
&#8226; percentage of source 1-, 2-, 3- and 4-grams occurring in the source part of the training corpus (4 features)
&#8226; percentage of source 1-, 2-, 3- and 4-grams in each frequency quartile of the training corpus (16 features)
&#8226; a binary feature indicating that the output contains more than three times as many alphabetic tokens as the input (1 feature)
&#8226; percentage of unaligned words and words with 1 : 1, 1 : n, n : 1 and m : n alignments (10 features)
&#8226; average number of translations per word, unweighted and weighted by word frequency and reciprocal word frequency (3 features)
&#8226; translation model entropy for the input words, cumulatively per sentence and averaged per word, computed based on the SMT lexical weight model (2 features).
Whenever applicable, features were computed for both the source and the target language, and additional features were added to represent the squared difference of the source and target language feature values. All feature values were scaled so that their values ranged between 0 and 1 over the training set.
The total number of features of the UU best system amounted to 99. It should be noted, however, that there is considerable redundancy in the feature set and that the 82 features of Hardmeier (2011) overlap with the 17 baseline features to some extent. We did not make any attempt to reduce feature overlap and relied on the learning algorithm for feature selection.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Both of the QE systems we submitted to the shared task used the complete set of 17 baseline features provided by the workshop organisers.</text>
                  <doc_id>14</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Additionally, the UU best system also contained all the features presented by Hardmeier (2011) with the exception</text>
                  <doc_id>15</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>of a few features specific to the film subtitle genre and inapplicable to the text type of the shared task, as well as a small number of features not included in that work.</text>
                  <doc_id>16</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Many of these features were modelled on QE features described by Specia et al. (2009).</text>
                  <doc_id>17</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In particular, the following features were included in addition to the baseline feature set:</text>
                  <doc_id>18</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; number of words, length ratio (4 features)</text>
                  <doc_id>19</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; source and target type-token ratios (2 features)</text>
                  <doc_id>20</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; number of tokens matching particular patterns (3 features each):</text>
                  <doc_id>21</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8211; numbers &#8211; opening and closing parentheses &#8211; strong punctuation signs &#8211; weak punctuation signs &#8211; ellipsis signs &#8211; hyphens &#8211; single and double quotes &#8211; apostrophe-s tokens &#8211; short alphabetic tokens (&#8804; 3 letters) &#8211; long alphabetic tokens (&#8805; 4 letters)</text>
                  <doc_id>22</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; source and target language model (LM) and log-LM scores (4 features)</text>
                  <doc_id>23</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; LM and log-LM scores normalised by sentence length (4 features)</text>
                  <doc_id>24</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; number and percentage of out-of-vocabulary words (2 features)</text>
                  <doc_id>25</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; percentage of source 1-, 2-, 3- and 4-grams occurring in the source part of the training corpus (4 features)</text>
                  <doc_id>26</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; percentage of source 1-, 2-, 3- and 4-grams in each frequency quartile of the training corpus (16 features)</text>
                  <doc_id>27</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; a binary feature indicating that the output contains more than three times as many alphabetic tokens as the input (1 feature)</text>
                  <doc_id>28</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; percentage of unaligned words and words with 1 : 1, 1 : n, n : 1 and m : n alignments (10 features)</text>
                  <doc_id>29</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; average number of translations per word, unweighted and weighted by word frequency and reciprocal word frequency (3 features)</text>
                  <doc_id>30</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; translation model entropy for the input words, cumulatively per sentence and averaged per word, computed based on the SMT lexical weight model (2 features).</text>
                  <doc_id>31</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Whenever applicable, features were computed for both the source and the target language, and additional features were added to represent the squared difference of the source and target language feature values.</text>
                  <doc_id>32</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>All feature values were scaled so that their values ranged between 0 and 1 over the training set.</text>
                  <doc_id>33</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The total number of features of the UU best system amounted to 99.</text>
                  <doc_id>34</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It should be noted, however, that there is considerable redundancy in the feature set and that the 82 features of Hardmeier (2011) overlap with the 17 baseline features to some extent.</text>
                  <doc_id>35</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We did not make any attempt to reduce feature overlap and relied on the learning algorithm for feature selection.</text>
                  <doc_id>36</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Parse trees</title>
            <text>Both the English input text and the Spanish Machine Translations were annotated with syntactic parse trees from which to derive implicit features. In English, we were able to produce both constituency and dependency parses. In Spanish, we were limited to dependency parses because of the better availability of parsing models. English constituency parses were produced with the Stanford parser (Klein and Manning, 2003) using the model bundled with the parser. For dependency parsing, we used MaltParser (Nivre et al., 2006). POS tagging was done with HunPOS (Hal&#225;csy et al., 2007) for English and SVMTool (Gim&#233;nez and M&#225;rquez, 2004) for Spanish, with the models provided by the OPUS project (Tiedemann, 2009). As in previous work (Hardmeier, 2011), we treated the parser as a black box and made no attempt to handle the fact that parsing accuracy may be decreased over malformed SMT output. To be used with tree kernels, the output of the dependency parser had to be transformed into a single tree structure with a unique label per node and unlabelled edges, similar to a constituency parse tree. We followed Johansson and Moschitti (2010) in using a tree representation which encodes partof-speech tags, dependency relations and words as sequences of child nodes (see fig. 1).
2006b). Predicted scores less than 1 were set to 1 and predicted scores greater than 5 were set to 5 as this was known to be the range of valid scores. Our learning algorithm had some free hyperparameters. Three of them were optimised by joint grid search with 5-fold cross-validation over the training set: the SVM training error/margin trade-off (C parameter), one free parameter of the explicit feature kernel and the ratio between explicit feature and tree kernels (see below). All other parameters were left at their default values. Before running it over the test set, the system was retrained on the complete training set using the parameters found with crossvalidation.
we initially followed the advice given by Hsu et al. (2010), using a Gaussian RBF kernel and optimising the SVM C parameter and the &#947; parameter of the RBF Fig. with2. grid Atreewithsomeofitssubsettrees search. While this gave reasonable results, (SSTs). it turned out that slightly better prediction could be achieved by using a polynomial kernel, so we chose to use this kernel is for our final submission and used grid search What to tune the offer degree of the polynomial instead. The improvement an plan over the Gaussian kernel was, however, marginal. direct stock purchase A tree and some of its Partial Tree Fragments 3.3 Tree kernels To exploit parse tree information in our Machine
Learning (ML) component, we used tree kernel (PTs). Kernel and by the Partial Tree Kernel. Illustrations by functions. Tree kernels (Collins and Duffy, 2001) Moschitti (2006a). are kernel functions defined over pairs of tree structures. They measure the similarity between two trees constraint 3 Machine over Learning the SSTs, component we obtain a morebygeneral countingform the number of substructures of common substructures. that we call partial trees (PTs). These can be generated Implicitly, they bydefine the application an infinite-dimensional of partial feature space whose dimensions correspond to all pos-
production The QE sharedrules task asked of the bothgrammar, for an estimate consequently of sible tree fragments.
valida 1&#8211;5PTs. qualityFigure score for3each shows segment that in the test number set cover of PTs different derived kinds offrom abstract the node same configurations tree as before and ais ranking still higher of the sentences (i.e. 30 according PTs). toThese qual- intuitive We decidedquantification to treat score estimation of the as primary different mensions information are effectively levels selected among by the the SVMtree- train- different that can occur substructure in a tree. Thenumbers important feature provide di- anity. and address the task as a regression problem. For ing algorithm through the selection and weighting based representations. the ranking task, we simply submitted the ranking of the support vectors. The intuition behind our induced by the regression output, breaking ties randomlytify constructions that are difficult to translate in the use of tree kernels is that they may help us iden-</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Both the English input text and the Spanish Machine Translations were annotated with syntactic parse trees from which to derive implicit features.</text>
                  <doc_id>37</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In English, we were able to produce both constituency and dependency parses.</text>
                  <doc_id>38</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In Spanish, we were limited to dependency parses because of the better availability of parsing models.</text>
                  <doc_id>39</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>English constituency parses were produced with the Stanford parser (Klein and Manning, 2003) using the model bundled with the parser.</text>
                  <doc_id>40</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For dependency parsing, we used MaltParser (Nivre et al., 2006).</text>
                  <doc_id>41</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>POS tagging was done with HunPOS (Hal&#225;csy et al., 2007) for English and SVMTool (Gim&#233;nez and M&#225;rquez, 2004) for Spanish, with the models provided by the OPUS project (Tiedemann, 2009).</text>
                  <doc_id>42</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>As in previous work (Hardmeier, 2011), we treated the parser as a black box and made no attempt to handle the fact that parsing accuracy may be decreased over malformed SMT output.</text>
                  <doc_id>43</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>To be used with tree kernels, the output of the dependency parser had to be transformed into a single tree structure with a unique label per node and unlabelled edges, similar to a constituency parse tree.</text>
                  <doc_id>44</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>We followed Johansson and Moschitti (2010) in using a tree representation which encodes partof-speech tags, dependency relations and words as sequences of child nodes (see fig.</text>
                  <doc_id>45</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>1).</text>
                  <doc_id>46</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2006b).</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Predicted scores less than 1 were set to 1 and predicted scores greater than 5 were set to 5 as this was known to be the range of valid scores.</text>
                  <doc_id>48</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Our learning algorithm had some free hyperparameters.</text>
                  <doc_id>49</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Three of them were optimised by joint grid search with 5-fold cross-validation over the training set: the SVM training error/margin trade-off (C parameter), one free parameter of the explicit feature kernel and the ratio between explicit feature and tree kernels (see below).</text>
                  <doc_id>50</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>All other parameters were left at their default values.</text>
                  <doc_id>51</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Before running it over the test set, the system was retrained on the complete training set using the parameters found with crossvalidation.</text>
                  <doc_id>52</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>we initially followed the advice given by Hsu et al. (2010), using a Gaussian RBF kernel and optimising the SVM C parameter and the &#947; parameter of the RBF Fig.</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>with2.</text>
                  <doc_id>54</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>grid Atreewithsomeofitssubsettrees search.</text>
                  <doc_id>55</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>While this gave reasonable results, (SSTs).</text>
                  <doc_id>56</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>it turned out that slightly better prediction could be achieved by using a polynomial kernel, so we chose to use this kernel is for our final submission and used grid search What to tune the offer degree of the polynomial instead.</text>
                  <doc_id>57</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The improvement an plan over the Gaussian kernel was, however, marginal.</text>
                  <doc_id>58</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>direct stock purchase A tree and some of its Partial Tree Fragments 3.3 Tree kernels To exploit parse tree information in our Machine</text>
                  <doc_id>59</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Learning (ML) component, we used tree kernel (PTs).</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Kernel and by the Partial Tree Kernel.</text>
                  <doc_id>61</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Illustrations by functions.</text>
                  <doc_id>62</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Tree kernels (Collins and Duffy, 2001) Moschitti (2006a).</text>
                  <doc_id>63</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>are kernel functions defined over pairs of tree structures.</text>
                  <doc_id>64</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>They measure the similarity between two trees constraint 3 Machine over Learning the SSTs, component we obtain a morebygeneral countingform the number of substructures of common substructures.</text>
                  <doc_id>65</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>that we call partial trees (PTs).</text>
                  <doc_id>66</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>These can be generated Implicitly, they bydefine the application an infinite-dimensional of partial feature space whose dimensions correspond to all pos-</text>
                  <doc_id>67</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>production The QE sharedrules task asked of the bothgrammar, for an estimate consequently of sible tree fragments.</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>valida 1&#8211;5PTs.</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>qualityFigure score for3each shows segment that in the test number set cover of PTs different derived kinds offrom abstract the node same configurations tree as before and ais ranking still higher of the sentences (i.e. 30 according PTs).</text>
                  <doc_id>70</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>toThese qual- intuitive We decidedquantification to treat score estimation of the as primary different mensions information are effectively levels selected among by the the SVMtree- train- different that can occur substructure in a tree.</text>
                  <doc_id>71</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Thenumbers important feature provide di- anity.</text>
                  <doc_id>72</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>and address the task as a regression problem.</text>
                  <doc_id>73</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For ing algorithm through the selection and weighting based representations.</text>
                  <doc_id>74</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>the ranking task, we simply submitted the ranking of the support vectors.</text>
                  <doc_id>75</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>The intuition behind our induced by the regression output, breaking ties randomlytify constructions that are difficult to translate in the use of tree kernels is that they may help us iden-</text>
                  <doc_id>76</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Our Fast systemTree was based Kernel on SVMFunctions</title>
        <text>implemented by the SVMlight software (Joachims, the output language. Note that we do not currently 1999) with tree kernel extensions (Moschitti, compare parse trees across languages; tree kernels
The main idea of tree kernels is to compute the number of common substructures between two trees T 1 and T 2 without explicitly considering the whole fragment space. We have designed a general function
to compute the ST, SST and PT kernels. Our fast evaluation of the PT kernel is inspired by the efficient evaluation of non-continuous subsequences (described in [13]). To increase the computation speed of the above tree kernels, we also apply the pre-selection of node pairs
are applied to trees of the same type in the same language only.
We used two different types of tree kernels for the different types of parse trees (see fig. 2). The Subset Tree Kernel (Collins and Duffy, 2001) considers tree fragments consisting of more than one node with the restriction that if one child of a node is included, then all its siblings must be included as well so that the underlying production rule is completely represented. This kind of kernel is well suited for constituency parse trees and was used for the source language constituency parses. For the dependency trees, we used the Partial Tree Kernel (Moschitti, 2006a) instead. It extends the Subset Tree Kernel by permitting also the extraction of tree fragments comprising only part of the children of any given node. Lifting this restriction makes sense for dependency trees since a node and its children do not correspond to a grammatical production in a dependency tree in the same way as they do in a constituency tree (Moschitti, 2006a). It was used for the dependency trees in the source and in the target language.
The explicit feature kernel and the three tree kernels were combined additively, with a single weight parameter to balance the sum of the tree kernels against the explicit feature kernel. This coefficient was optimised together with the other two hyperparameters mentioned above. It turned out that best results could be obtained with a fairly low weight for the tree kernels, but in the cross-validation experiments adding tree kernels did give an improvement over not having them at all.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>implemented by the SVMlight software (Joachims, the output language.</text>
              <doc_id>77</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Note that we do not currently 1999) with tree kernel extensions (Moschitti, compare parse trees across languages; tree kernels</text>
              <doc_id>78</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The main idea of tree kernels is to compute the number of common substructures between two trees T 1 and T 2 without explicitly considering the whole fragment space.</text>
              <doc_id>79</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We have designed a general function</text>
              <doc_id>80</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>to compute the ST, SST and PT kernels.</text>
              <doc_id>81</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our fast evaluation of the PT kernel is inspired by the efficient evaluation of non-continuous subsequences (described in [13]).</text>
              <doc_id>82</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>To increase the computation speed of the above tree kernels, we also apply the pre-selection of node pairs</text>
              <doc_id>83</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>are applied to trees of the same type in the same language only.</text>
              <doc_id>84</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We used two different types of tree kernels for the different types of parse trees (see fig.</text>
              <doc_id>85</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2).</text>
              <doc_id>86</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The Subset Tree Kernel (Collins and Duffy, 2001) considers tree fragments consisting of more than one node with the restriction that if one child of a node is included, then all its siblings must be included as well so that the underlying production rule is completely represented.</text>
              <doc_id>87</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This kind of kernel is well suited for constituency parse trees and was used for the source language constituency parses.</text>
              <doc_id>88</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>For the dependency trees, we used the Partial Tree Kernel (Moschitti, 2006a) instead.</text>
              <doc_id>89</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>It extends the Subset Tree Kernel by permitting also the extraction of tree fragments comprising only part of the children of any given node.</text>
              <doc_id>90</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Lifting this restriction makes sense for dependency trees since a node and its children do not correspond to a grammatical production in a dependency tree in the same way as they do in a constituency tree (Moschitti, 2006a).</text>
              <doc_id>91</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>It was used for the dependency trees in the source and in the target language.</text>
              <doc_id>92</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The explicit feature kernel and the three tree kernels were combined additively, with a single weight parameter to balance the sum of the tree kernels against the explicit feature kernel.</text>
              <doc_id>93</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This coefficient was optimised together with the other two hyperparameters mentioned above.</text>
              <doc_id>94</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>It turned out that best results could be obtained with a fairly low weight for the tree kernels, but in the cross-validation experiments adding tree kernels did give an improvement over not having them at all.</text>
              <doc_id>95</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Experimental Results</title>
        <text>Results for some of our experiments are shown in table 1. The two systems we submitted to the shared task are marked with their system identifiers. A few other systems are included for comparison and are numbered (a) to (e) for easier reference. Our system using only the baseline features (d) performs a bit worse than the reference system of the shared task organisers. We use the same learning algorithm, so this seems to indicate that the kernel and the hyperparameters they selected worked slightly better than our choices. Using only tree kernels with no explicit features at all (e) creates a system that works considerably worse under crossvalidation, however we note that its performance on the test set is very close to that of system (d). Adding the 82 additional features of Hardmeier (2011) to the system without tree kernels slightly improves the performance both under cross-validation and on the test set (c). Adding tree kernels has a similar effect, which is a bit less pronounced for the cross-validation setting, but quite comparable on the test set (UU bltk, b). Finally, combining the full feature set with tree kernels results in an additional gain under cross-validation, but unfortunately the improvement does not carry over to the test set (UU best, a).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Results for some of our experiments are shown in table 1.</text>
              <doc_id>96</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The two systems we submitted to the shared task are marked with their system identifiers.</text>
              <doc_id>97</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A few other systems are included for comparison and are numbered (a) to (e) for easier reference.</text>
              <doc_id>98</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Our system using only the baseline features (d) performs a bit worse than the reference system of the shared task organisers.</text>
              <doc_id>99</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We use the same learning algorithm, so this seems to indicate that the kernel and the hyperparameters they selected worked slightly better than our choices.</text>
              <doc_id>100</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Using only tree kernels with no explicit features at all (e) creates a system that works considerably worse under crossvalidation, however we note that its performance on the test set is very close to that of system (d).</text>
              <doc_id>101</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Adding the 82 additional features of Hardmeier (2011) to the system without tree kernels slightly improves the performance both under cross-validation and on the test set (c).</text>
              <doc_id>102</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Adding tree kernels has a similar effect, which is a bit less pronounced for the cross-validation setting, but quite comparable on the test set (UU bltk, b).</text>
              <doc_id>103</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Finally, combining the full feature set with tree kernels results in an additional gain under cross-validation, but unfortunately the improvement does not carry over to the test set (UU best, a).</text>
              <doc_id>104</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>5 Conclusions</title>
        <text>In sum, the results confirm the findings made in our earlier work (Hardmeier, 2011). They show that tree kernels can be a valuable tool to boost the initial 112 performance of a Quality Estimation system without spending much effort on feature engineering. Unfortunately, it seems that the gains achieved by tree kernels over simple parse trees and by the additional explicit features used in our systems do not necessarily add up. Nevertheless, comparison with other participating systems shows that either of them is sufficient for state-of-the-art performance.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In sum, the results confirm the findings made in our earlier work (Hardmeier, 2011).</text>
              <doc_id>105</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>They show that tree kernels can be a valuable tool to boost the initial 112 performance of a Quality Estimation system without spending much effort on feature engineering.</text>
              <doc_id>106</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Unfortunately, it seems that the gains achieved by tree kernels over simple parse trees and by the additional explicit features used in our systems do not necessarily add up.</text>
              <doc_id>107</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Nevertheless, comparison with other participating systems shows that either of them is sufficient for state-of-the-art performance.</text>
              <doc_id>108</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Experimental results</caption>
        <reference_text></reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>T : Tree kernel weight</cell>
              <cell>C: Training error/margin trade-off</cell>
              <cell>d: Degree of polynomial kernel</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>&#8710;: DeltaAvg score</cell>
              <cell>&#961;: Spearman rank correlation</cell>
              <cell>MAE: Mean Average Error</cell>
            </row>
            <row>
              <cell>RMS: Root Mean Square Error</cell>
              <cell>TK: Tree kernels</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Chris Callison-Burch</author>
          <author>Philipp Koehn</author>
          <author>Christof Monz</author>
          <author>Matt Post</author>
          <author>Radu Soricut</author>
          <author>Lucia Specia</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>1</id>
        <authors/>
        <title>Findings of the 2012 Workshop on Statistical Machine Translation.</title>
        <publication>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</publication>
        <pages>None</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Michael Collins</author>
          <author>Nigel Duffy</author>
        </authors>
        <title>Convolution kernels for natural language.</title>
        <publication>In Proceedings of NIPS</publication>
        <pages>625--632</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Jes&#250;s Gim&#233;nez</author>
          <author>Llu&#237;s M&#225;rquez</author>
        </authors>
        <title>SVMTool: A general POS tagger generator based on Support Vector Machines.</title>
        <publication>In Proceedings of the 4th Conference on International Language Resources and Evaluation (LREC-2004),</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>P&#233;ter Hal&#225;csy</author>
          <author>Andr&#225;s Kornai</author>
          <author>Csaba Oravecz</author>
        </authors>
        <title>HunPos &#8211; an open source trigram tagger.</title>
        <publication>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics. Companion Volume: Proceedings of the Demo and Poster Sessions,</publication>
        <pages>209--212</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Christian Hardmeier</author>
        </authors>
        <title>Improving machine translation quality prediction with syntactic tree kernels.</title>
        <publication>Proceedings of the 15th conference of the European Association for Machine Translation (EAMT 2011),</publication>
        <pages>233--240</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Chih-Wei Hsu</author>
          <author>Chih-Chung Chang</author>
          <author>Chih-Jen Lin</author>
        </authors>
        <title>A practical guide to support vector classification.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Thorsten Joachims</author>
        </authors>
        <title>Making large-scale SVM learning practical.</title>
        <publication>Advances in Kernel Methods &#8211; Support Vector Learning.</publication>
        <pages>None</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Richard Johansson</author>
          <author>Alessandro Moschitti</author>
        </authors>
        <title>Syntactic and semantic structure for opinion expression detection.</title>
        <publication>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</publication>
        <pages>67--76</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Dan Klein</author>
          <author>Christopher D Manning</author>
        </authors>
        <title>Accurate unlexicalized parsing.</title>
        <publication>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>423--430</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Alessandro Moschitti</author>
        </authors>
        <title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
        <publication>In Proceedings of the 17th European Conference on Machine Learning,</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Alessandro Moschitti</author>
        </authors>
        <title>Making tree kernels practical for natural language learning.</title>
        <publication>In Proceedings of the Eleventh International Conference of the European Association for Computational Linguistics,</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Joakim Nivre</author>
          <author>Johan Hall</author>
          <author>Jens Nilsson</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>MaltParser</author>
        </authors>
        <title>A language-independent system for datadriven dependency parsing.</title>
        <publication>In Proceedings of the 5th Conference on International Language Resources and Evaluation (LREC-2006),</publication>
        <pages>2216--2219</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Lucia Specia</author>
          <author>Craig Saunders</author>
          <author>Marco Turchi</author>
          <author>Zhuoran Wang</author>
          <author>John Shawe-Taylor</author>
        </authors>
        <title>Improving the confidence of Machine Translation quality estimates.</title>
        <publication>In Proceedings of MT Summit XII,</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Callison-Burch et al., 2012</string>
        <sentence_id>49512</sentence_id>
        <char_offset>62</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>2</reference_id>
        <string>Collins and Duffy, 2001</string>
        <sentence_id>49570</sentence_id>
        <char_offset>14</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Collins and Duffy, 2001</string>
        <sentence_id>49596</sentence_id>
        <char_offset>24</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>3</reference_id>
        <string>Gim&#233;nez and M&#225;rquez, 2004</string>
        <sentence_id>49549</sentence_id>
        <char_offset>81</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>4</reference_id>
        <string>Hal&#225;csy et al., 2007</string>
        <sentence_id>49549</sentence_id>
        <char_offset>34</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>5</reference_id>
        <string>Hardmeier (2011)</string>
        <sentence_id>49518</sentence_id>
        <char_offset>98</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>5</reference_id>
        <string>Hardmeier (2011)</string>
        <sentence_id>49522</sentence_id>
        <char_offset>78</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>5</reference_id>
        <string>Hardmeier (2011)</string>
        <sentence_id>49542</sentence_id>
        <char_offset>114</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>5</reference_id>
        <string>Hardmeier (2011)</string>
        <sentence_id>49611</sentence_id>
        <char_offset>37</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>6</reference_id>
        <string>Hsu et al. (2010)</string>
        <sentence_id>49560</sentence_id>
        <char_offset>42</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>8</reference_id>
        <string>Johansson and Moschitti (2010)</string>
        <sentence_id>49552</sentence_id>
        <char_offset>12</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>9</reference_id>
        <string>Klein and Manning, 2003</string>
        <sentence_id>49547</sentence_id>
        <char_offset>68</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>10</reference_id>
        <string>Moschitti, 2006</string>
        <sentence_id>49598</sentence_id>
        <char_offset>59</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>10</reference_id>
        <string>Moschitti, 2006</string>
        <sentence_id>49600</sentence_id>
        <char_offset>206</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>10</reference_id>
        <string>Moschitti (2006</string>
        <sentence_id>49570</sentence_id>
        <char_offset>39</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>11</reference_id>
        <string>Moschitti, 2006</string>
        <sentence_id>49598</sentence_id>
        <char_offset>59</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>11</reference_id>
        <string>Moschitti, 2006</string>
        <sentence_id>49600</sentence_id>
        <char_offset>206</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>11</reference_id>
        <string>Moschitti (2006</string>
        <sentence_id>49570</sentence_id>
        <char_offset>39</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>12</reference_id>
        <string>Nivre et al., 2006</string>
        <sentence_id>49548</sentence_id>
        <char_offset>44</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>14</reference_id>
        <string>Specia et al. (2009)</string>
        <sentence_id>49524</sentence_id>
        <char_offset>65</char_offset>
      </citation>
    </citations>
  </content>
</document>
