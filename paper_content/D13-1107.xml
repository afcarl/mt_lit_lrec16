<document>
  <filename>D13-1107</filename>
  <authors/>
  <title>Multi-domain Adaptation for SMT Using Multi-task Learning &#8727;</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>Domain adaptation for SMT usually adapts models to an individual specific domain. However, it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality. In this paper, we propose a novel multi-domain adaptation approach for SMT using Multi-Task Learning (MTL), with in-domain models tailored for each specific domain and a general-domain model shared by different domains. The parameters of these models are tuned jointly via MTL so that they can learn general knowledge more accurately and exploit domain knowledge better. Our experiments on a largescale English-to-Chinese translation task validate that the MTL-based adaptation approach significantly and consistently improves the translation quality compared to a non-adapted baseline. Furthermore, it also outperforms the individual adaptation of each specific domain.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Domain adaptation for SMT usually adapts models to an individual specific domain.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, we propose a novel multi-domain adaptation approach for SMT using Multi-Task Learning (MTL), with in-domain models tailored for each specific domain and a general-domain model shared by different domains.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The parameters of these models are tuned jointly via MTL so that they can learn general knowledge more accurately and exploit domain knowledge better.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Our experiments on a largescale English-to-Chinese translation task validate that the MTL-based adaptation approach significantly and consistently improves the translation quality compared to a non-adapted baseline.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Furthermore, it also outperforms the individual adaptation of each specific domain.</text>
              <doc_id>5</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Domain adaptation is an active topic in statistical machine learning and aims to alleviate the domain mismatch between training and testing data. Like many machine learning tasks, Statistical Machine Translation (SMT) assumes that the data distributions of training and testing domains are similar. However, this assumption does not hold for real world SMT systems since training data for SMT models may come from a variety of domains. The translation quality is often unsatisfactory when
&#8727;
This work was done while the first and second authors were visiting Microsoft Research Asia.
translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora. Therefore, domain adaptation is crucial for SMT systems to achieve better performance.
Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L&#252; et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc. Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance. It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved. Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains. To leverage the common knowledge, we need to devise a multi-domain adaptation approach that jointly adapts the SMT models. Multi-domain adaptation has been proved quite effective in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011), where the commonalities and differences across multiple domains are explicitly addressed by Multitask Learning (MTL). MTL is an approach that learns one target problem with other related problems at the same time, using a shared feature representation. The key advantage of MTL is to enable implicit data sharing and regularization. Therefore, it often leads to a better model for each task. Analogously, we expect that the overall translation quality can be further improved by using an MTL-based
Entire Training Data T
multi-domain adaptation approach.
In this paper, we use MTL to jointly adapt SMT models to multiple domains. Specifically, we develop multiple SMT systems based on mixture models, where each system is tailored for one specific domain with an in-domain Translation Model (TM) and an in-domain Language Model (LM). Meanwhile, all the systems share a same general-domain TM and LM. These SMT systems are considered as several related tasks with a shared feature representation, which fits well into a unified MTL framework. With the MTL-based joint tuning, general knowledge can be better learned by the generaldomain models, while domain knowledge can be better exploited by the in-domain models as well. By using a distributed stochastic learning approach (Simianer et al., 2012), we can estimate the feature weights of multiple SMT systems at the same time. Furthermore, we modify the algorithm to treat in-domain and general-domain features separately, which brings regularization to multiple SMT systems in an efficient way. Experimental results have shown that our method can significantly improve the translation quality on multiple domains over a nonadapted baseline. Moreover, the MTL-based adaptation also outperforms the conventional individual adaptation approach towards each domain.
The rest of the paper is organized as follows: The proposed approach is explained in Section 2. Experimental results are presented in Section 3. Section 4 introduces some related work. Section 5 concludes the paper and suggests future research directions.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Domain adaptation is an active topic in statistical machine learning and aims to alleviate the domain mismatch between training and testing data.</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Like many machine learning tasks, Statistical Machine Translation (SMT) assumes that the data distributions of training and testing domains are similar.</text>
              <doc_id>7</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, this assumption does not hold for real world SMT systems since training data for SMT models may come from a variety of domains.</text>
              <doc_id>8</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The translation quality is often unsatisfactory when</text>
              <doc_id>9</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8727;</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>This work was done while the first and second authors were visiting Microsoft Research Asia.</text>
              <doc_id>11</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora.</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, domain adaptation is crucial for SMT systems to achieve better performance.</text>
              <doc_id>13</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Previous research on domain adaptation for SMT includes data selection and weighting (Eck et al., 2004; L&#252; et al., 2007; Foster et al., 2010; Moore and Lewis, 2010; Axelrod et al., 2011), mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Sennrich, 2012; Razmara et al., 2012), and semi-supervised transductive learning (Ueffing et al., 2007), etc.</text>
              <doc_id>14</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance.</text>
              <doc_id>15</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>It is natural that real world SMT systems should adapt the models to multiple domains because the input may be heterogeneous, so that the overall translation quality can be improved.</text>
              <doc_id>16</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains.</text>
              <doc_id>17</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>To leverage the common knowledge, we need to devise a multi-domain adaptation approach that jointly adapts the SMT models.</text>
              <doc_id>18</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Multi-domain adaptation has been proved quite effective in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011), where the commonalities and differences across multiple domains are explicitly addressed by Multitask Learning (MTL).</text>
              <doc_id>19</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>MTL is an approach that learns one target problem with other related problems at the same time, using a shared feature representation.</text>
              <doc_id>20</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The key advantage of MTL is to enable implicit data sharing and regularization.</text>
              <doc_id>21</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, it often leads to a better model for each task.</text>
              <doc_id>22</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Analogously, we expect that the overall translation quality can be further improved by using an MTL-based</text>
              <doc_id>23</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Entire Training Data T</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>multi-domain adaptation approach.</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this paper, we use MTL to jointly adapt SMT models to multiple domains.</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Specifically, we develop multiple SMT systems based on mixture models, where each system is tailored for one specific domain with an in-domain Translation Model (TM) and an in-domain Language Model (LM).</text>
              <doc_id>27</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Meanwhile, all the systems share a same general-domain TM and LM.</text>
              <doc_id>28</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>These SMT systems are considered as several related tasks with a shared feature representation, which fits well into a unified MTL framework.</text>
              <doc_id>29</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>With the MTL-based joint tuning, general knowledge can be better learned by the generaldomain models, while domain knowledge can be better exploited by the in-domain models as well.</text>
              <doc_id>30</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>By using a distributed stochastic learning approach (Simianer et al., 2012), we can estimate the feature weights of multiple SMT systems at the same time.</text>
              <doc_id>31</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Furthermore, we modify the algorithm to treat in-domain and general-domain features separately, which brings regularization to multiple SMT systems in an efficient way.</text>
              <doc_id>32</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Experimental results have shown that our method can significantly improve the translation quality on multiple domains over a nonadapted baseline.</text>
              <doc_id>33</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, the MTL-based adaptation also outperforms the conventional individual adaptation approach towards each domain.</text>
              <doc_id>34</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The rest of the paper is organized as follows: The proposed approach is explained in Section 2.</text>
              <doc_id>35</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Experimental results are presented in Section 3.</text>
              <doc_id>36</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Section 4 introduces some related work.</text>
              <doc_id>37</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Section 5 concludes the paper and suggests future research directions.</text>
              <doc_id>38</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 The Proposed Approach</title>
        <text>Figure 1 gives an example with N pre-defined domains to illustrate the main idea. There are three steps in the training phase. First, in-domain training data is selected according to the pre-defined domains (Section 2.1). Second, in-domain models and general-domain models are trained to develop the domain-specific SMT systems (Section 2.2). Third, multiple domain-specific SMT systems are tuned jointly by using an MTL-based approach (Section 2.3).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Figure 1 gives an example with N pre-defined domains to illustrate the main idea.</text>
              <doc_id>39</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>There are three steps in the training phase.</text>
              <doc_id>40</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>First, in-domain training data is selected according to the pre-defined domains (Section 2.1).</text>
              <doc_id>41</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Second, in-domain models and general-domain models are trained to develop the domain-specific SMT systems (Section 2.2).</text>
              <doc_id>42</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Third, multiple domain-specific SMT systems are tuned jointly by using an MTL-based approach (Section 2.3).</text>
              <doc_id>43</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 In-domain Data Selection</title>
            <text>In the first step, in-domain bilingual data is selected from all the bilingual data to train in-domain TMs. We use the bilingual cross-entropy based approach (Axelrod et al., 2011) to obtain the in-domain data:
[H I&#8722;src (s)&#8722;H G&#8722;src (s)]+[H I&#8722;tgt (t)&#8722;H G&#8722;tgt (t)] (1)
where {s,t} is a bilingual sentence pair in the entire bilingual corpus. H I&#8722;xxx (&#183;) and H G&#8722;xxx (&#183;) represent the cross-entropy of a string according to an indomain LM and a general-domain LM, respectively. &#8221;xxx&#8221; denotes either the source language (src) or the target language (tgt). H I&#8722;src (s) &#8722; H G&#8722;src (s) is the cross-entropy difference of string s between the indomain and general-domain source-side LMs, and H I&#8722;tgt (t) &#8722; H G&#8722;tgt (t) is the cross-entropy difference of string t between the in-domain and generaldomain target-side LMs. This criterion biases towards sentence pairs that are like the in-domain corpus but unlike the general-domain corpus. Therefore, the sentence pairs with lower scores (larger differences) are presumed to be better.
Now, the question is how to find sufficient monolingual data to train in-domain LMs. A straightforward solution is to collect the data from the internet. There are a large number of monolingual webpages with domain information from web portal sites 1 , which can be collected to train in-domain LMs. In large-scale real world SMT systems, practical domain adaptation techniques should target more domains rather than just one due to heterogeneous input. Therefore, we use a web crawler to collect monolingual webpages of N domains from web portal sites, for both the source language and the target language. The statistics of web-crawled data is given in Section 3.1. We use the web-crawled monolingual documents to train N in-domain source-side LMs and N in-domain target-side LMs. Additionally, we also train the source-side and target-side general-domain LMs with all the web-crawled documents from different domains. Finally, these indomain and general-domain LMs are used to select in-domain bilingual data for different domains according to Formula (1).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In the first step, in-domain bilingual data is selected from all the bilingual data to train in-domain TMs.</text>
                  <doc_id>44</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We use the bilingual cross-entropy based approach (Axelrod et al., 2011) to obtain the in-domain data:</text>
                  <doc_id>45</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>[H I&#8722;src (s)&#8722;H G&#8722;src (s)]+[H I&#8722;tgt (t)&#8722;H G&#8722;tgt (t)] (1)</text>
                  <doc_id>46</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where {s,t} is a bilingual sentence pair in the entire bilingual corpus.</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>H I&#8722;xxx (&#183;) and H G&#8722;xxx (&#183;) represent the cross-entropy of a string according to an indomain LM and a general-domain LM, respectively.</text>
                  <doc_id>48</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>&#8221;xxx&#8221; denotes either the source language (src) or the target language (tgt).</text>
                  <doc_id>49</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>H I&#8722;src (s) &#8722; H G&#8722;src (s) is the cross-entropy difference of string s between the indomain and general-domain source-side LMs, and H I&#8722;tgt (t) &#8722; H G&#8722;tgt (t) is the cross-entropy difference of string t between the in-domain and generaldomain target-side LMs.</text>
                  <doc_id>50</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This criterion biases towards sentence pairs that are like the in-domain corpus but unlike the general-domain corpus.</text>
                  <doc_id>51</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, the sentence pairs with lower scores (larger differences) are presumed to be better.</text>
                  <doc_id>52</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Now, the question is how to find sufficient monolingual data to train in-domain LMs.</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A straightforward solution is to collect the data from the internet.</text>
                  <doc_id>54</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>There are a large number of monolingual webpages with domain information from web portal sites 1 , which can be collected to train in-domain LMs.</text>
                  <doc_id>55</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In large-scale real world SMT systems, practical domain adaptation techniques should target more domains rather than just one due to heterogeneous input.</text>
                  <doc_id>56</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, we use a web crawler to collect monolingual webpages of N domains from web portal sites, for both the source language and the target language.</text>
                  <doc_id>57</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The statistics of web-crawled data is given in Section 3.1.</text>
                  <doc_id>58</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>We use the web-crawled monolingual documents to train N in-domain source-side LMs and N in-domain target-side LMs.</text>
                  <doc_id>59</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Additionally, we also train the source-side and target-side general-domain LMs with all the web-crawled documents from different domains.</text>
                  <doc_id>60</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, these indomain and general-domain LMs are used to select in-domain bilingual data for different domains according to Formula (1).</text>
                  <doc_id>61</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 SMT Systems with Mixture Models</title>
            <text>In the second step, with the selected in-domain training data, we develop SMT systems based on mixture models. In particular, we use the mixture model based approach proposed by Koehn and Schroeder
1 Many web portal sites contain domain information
for webpages, such as &#8221;www.yahoo.com&#8221; in English and &#8221;www.sina.com.cn&#8221; in Chinese and etc. The webpages are often categorized by human editors into different domains, such as politics, sports, business, etc.
(2007). Specifically, we have developed N SMT systems for N domains respectively, where each system is a typical log-linear model. For each system, the best translation candidate &#710;f is given by:
&#710;f = arg max {P (f|e)} (2)
f
where the translation probability P (f |e) is given by:
P (f|e) &#8733; &#8721; i w i &#183; log &#966; i (f, e)
= &#8721; w j &#183; log &#966; j (f, e) + &#8721; w k &#183; log &#966; k (f, e)
j&#8712;I k&#8712;G
where &#966; j (f, e) is the in-domain feature function and w j is the corresponding feature weight. &#966; k (f, e) is the general-domain feature function and w k is the feature weight. The detailed feature description is as follows:
In-domain features
&#8226; An in-domain TM, including phrase translation probabilities and lexical weights for both directions (4 features)
&#8226; An in-domain target-side LM (1 feature)
&#8226; word count (1 feature)
&#8226; phrase count (1 feature)
&#8226; NULL penalty (1 feature)
&#8226; Number of hierarchical rules used (1 feature)
General-domain features
&#8226; A general-domain TM, including phrase translation probabilities and lexical weights for both directions (4 features)
&#8226; A general-domain target-side LM (1 feature)
The feature description indicates that each SMT system contains two TMs and two LMs. The indomain TMs are trained using the selected bilingual training data according to Formula (1), and the general-domain TM is trained using the entire bilingual training data. For the LMs, we re-use the targetside in-domain LMs and general-domain LM trained
for data selection (Section 2.1). Compared with a normal single-model system, the system with mixture models can balance the contributions from the general-domain and in-domain knowledge. Hence it potentially benefits from both.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In the second step, with the selected in-domain training data, we develop SMT systems based on mixture models.</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In particular, we use the mixture model based approach proposed by Koehn and Schroeder</text>
                  <doc_id>63</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 Many web portal sites contain domain information</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>for webpages, such as &#8221;www.yahoo.com&#8221; in English and &#8221;www.sina.com.cn&#8221; in Chinese and etc.</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The webpages are often categorized by human editors into different domains, such as politics, sports, business, etc.</text>
                  <doc_id>66</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(2007).</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Specifically, we have developed N SMT systems for N domains respectively, where each system is a typical log-linear model.</text>
                  <doc_id>68</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For each system, the best translation candidate &#710;f is given by:</text>
                  <doc_id>69</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#710;f = arg max {P (f|e)} (2)</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f</text>
                  <doc_id>71</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where the translation probability P (f |e) is given by:</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (f|e) &#8733; &#8721; i w i &#183; log &#966; i (f, e)</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>= &#8721; w j &#183; log &#966; j (f, e) + &#8721; w k &#183; log &#966; k (f, e)</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j&#8712;I k&#8712;G</text>
                  <doc_id>75</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where &#966; j (f, e) is the in-domain feature function and w j is the corresponding feature weight.</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>&#966; k (f, e) is the general-domain feature function and w k is the feature weight.</text>
                  <doc_id>77</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The detailed feature description is as follows:</text>
                  <doc_id>78</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In-domain features</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; An in-domain TM, including phrase translation probabilities and lexical weights for both directions (4 features)</text>
                  <doc_id>80</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; An in-domain target-side LM (1 feature)</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; word count (1 feature)</text>
                  <doc_id>82</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; phrase count (1 feature)</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; NULL penalty (1 feature)</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Number of hierarchical rules used (1 feature)</text>
                  <doc_id>85</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>General-domain features</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; A general-domain TM, including phrase translation probabilities and lexical weights for both directions (4 features)</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; A general-domain target-side LM (1 feature)</text>
                  <doc_id>88</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The feature description indicates that each SMT system contains two TMs and two LMs.</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The indomain TMs are trained using the selected bilingual training data according to Formula (1), and the general-domain TM is trained using the entire bilingual training data.</text>
                  <doc_id>90</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For the LMs, we re-use the targetside in-domain LMs and general-domain LM trained</text>
                  <doc_id>91</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>for data selection (Section 2.1).</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Compared with a normal single-model system, the system with mixture models can balance the contributions from the general-domain and in-domain knowledge.</text>
                  <doc_id>93</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Hence it potentially benefits from both.</text>
                  <doc_id>94</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>2.3 MTL-based Tuning</title>
            <text>In the third step, the feature weights in multiple domain-specific SMT systems are estimated. Instead of tuning each domain-specific system separately, we treat different systems as related tasks and tune them jointly in an MTL framework. There are two main reasons for MTL-based tuning:
1. Domain-specific translation tasks share the same general-domain LM and TM. MTL often leads to better performance by leveraging commonalities among different tasks.
2. By enforcing that the general-domain LM and TM perform equally across different domains, MTL provides a kind of regularization to prevent over-fitting.
Formally, the objective function of the proposed MTL-based approach is described as follows:
min
W
{ &#8721; N }
Loss(E i , &#234;(F i , w i ))
i=1
(4)
where N is the number of pre-defined domains. {F i ,E i } is the in-domain development dataset for the i-th domain. F i denotes the source sentences and E i denotes the reference translations. w i is a D-length feature weight column vector for the i-th domain, where D is the dimension of the feature space. W is a N-by-D matrix, representing [w 1 |w 2 | . . . |w N ] T . &#234;(F i , w i ) are the best translations obtained for F i with parameters w i . Loss(&#183;, &#183;) denotes the loss between the system&#8217;s output and the reference translations. The basic idea of the objective function is to minimize the sum of loss functions for all the domains, rather than one domain at a time. Therefore, by adjusting the in-domain and general-domain feature weights, the translation quality is expected to be good across different domains.
To effectively tune SMT systems jointly, we modify the asynchronous Stochastic Gradient Descend (SGD) Algorithm (Simianer et al., 2012) to optimize objective function (4). We follow the pairwise ranking approach with the perceptron algorithm (Shen and Joshi, 2005) to update feature weights. Let a translation candidate be denoted by its feature vector v &#8712; R D , the pairwise preference for training is constructed by ranking two candidates according to the smoothed sentence-level BLEU (Liang et al., 2006). For a preference pair v [j] =(v (1) , v (2) ) where v (1) is preferred, a hinge loss is used:
L(w i ) = (&#8722;&#12296;w i , v (1) &#8722; v (2) &#12297;) + (5)
where (x) + = max(0, x) and &#12296;&#183;, &#183;&#12297; denotes the inner product of two vectors. With the perceptron algorithm (Shen and Joshi, 2005), the gradient of the hinge loss is:
&#8711;L(w i ) =
{ v (2) &#8722; v (1) if&#12296;w i , v (1) &#8722; v (2) &#12297; &#8804; 0 0 otherwise (6)
The training instances for the discriminative learning in pairwise ranking are made by comparing the N-best list of the translation candidates scored by the smoothed sentence-level BLEU (Liang et al., 2006). Following Simianer et al. (2012), the N-best list is divided into three bins: the top 10% (High), the middle 80% (Middle), and the last 10% (Low). These bins are used for pairwise ranking where the translation preference pairs are built between the candidates in High-Middle, Middle-Low, and High- Low, but not the candidates within the same bin, which is shown in Figure 2. The idea is to guarantee that the ranker is more discriminative to prefer the good translations to the bad ones.
N-best list
High: 10%
Middle: 80%
Low: 10%
Our modified algorithm is illustrated in Algorithm 1. Each column vector w i is further split into two parts w I i and w G i , representing the In-domain and General-domain feature weights respectively. In Algorithm 1, we first distribute the domain-specific SMT decoders to different machines and initialize the feature weights (line 1-2). Typically, the SGD algorithm runs in several iterations (In this study, we set the number of epochs T to 20) (line 3). Multiple SMT decoders run in parallel and each decoder updates its feature weights individually using its indomain development data (line 4-15). For each domain, the domain-specific decoder translates each in-domain development sentence and determines the N-best translations (line 4-8). The preference pairs are built and used to update the parameters by gradient descent with &#951; = 0.0001 (line 9-13). Each domain-specific decoder translates its in-domain development data multiple times. After each iteration, feature weights from all decoders are collected (line 16-19). In contrast to the original algorithm (Simianer et al., 2012), we only average the generaldomain feature weights w G 1 , . . . , wG N , but do not average the in-domain feature weights (line 20-25). The reason is we hope to leverage the commonalities among these systems. Meanwhile, general knowledge is enforced to be conveyed equally across different domains. Finally, the algorithm returns all the domain-specific feature weights w 1 , w 2 , . . . , w N that are used for testing (line 27).
After the joint MTL-based tuning, the feature weights tailored for domain-specific SMT systems are used to translate the testing data. We collect indomain testing data for each domain to evaluate the domain-specific systems. Although this is not always the case in real applications where the testing domain is known, this study mainly focuses on the effectiveness of the MTL-based tuning approach.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In the third step, the feature weights in multiple domain-specific SMT systems are estimated.</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Instead of tuning each domain-specific system separately, we treat different systems as related tasks and tune them jointly in an MTL framework.</text>
                  <doc_id>96</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>There are two main reasons for MTL-based tuning:</text>
                  <doc_id>97</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1.</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Domain-specific translation tasks share the same general-domain LM and TM.</text>
                  <doc_id>99</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>MTL often leads to better performance by leveraging commonalities among different tasks.</text>
                  <doc_id>100</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>By enforcing that the general-domain LM and TM perform equally across different domains, MTL provides a kind of regularization to prevent over-fitting.</text>
                  <doc_id>102</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Formally, the objective function of the proposed MTL-based approach is described as follows:</text>
                  <doc_id>103</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>min</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>W</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>{ &#8721; N }</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Loss(E i , &#234;(F i , w i ))</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>108</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(4)</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where N is the number of pre-defined domains.</text>
                  <doc_id>110</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>{F i ,E i } is the in-domain development dataset for the i-th domain.</text>
                  <doc_id>111</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>F i denotes the source sentences and E i denotes the reference translations.</text>
                  <doc_id>112</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>w i is a D-length feature weight column vector for the i-th domain, where D is the dimension of the feature space.</text>
                  <doc_id>113</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>W is a N-by-D matrix, representing [w 1 |w 2 | .</text>
                  <doc_id>114</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>115</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>116</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>|w N ] T .</text>
                  <doc_id>117</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>&#234;(F i , w i ) are the best translations obtained for F i with parameters w i .</text>
                  <doc_id>118</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>Loss(&#183;, &#183;) denotes the loss between the system&#8217;s output and the reference translations.</text>
                  <doc_id>119</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>The basic idea of the objective function is to minimize the sum of loss functions for all the domains, rather than one domain at a time.</text>
                  <doc_id>120</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, by adjusting the in-domain and general-domain feature weights, the translation quality is expected to be good across different domains.</text>
                  <doc_id>121</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To effectively tune SMT systems jointly, we modify the asynchronous Stochastic Gradient Descend (SGD) Algorithm (Simianer et al., 2012) to optimize objective function (4).</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We follow the pairwise ranking approach with the perceptron algorithm (Shen and Joshi, 2005) to update feature weights.</text>
                  <doc_id>123</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Let a translation candidate be denoted by its feature vector v &#8712; R D , the pairwise preference for training is constructed by ranking two candidates according to the smoothed sentence-level BLEU (Liang et al., 2006).</text>
                  <doc_id>124</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For a preference pair v [j] =(v (1) , v (2) ) where v (1) is preferred, a hinge loss is used:</text>
                  <doc_id>125</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(w i ) = (&#8722;&#12296;w i , v (1) &#8722; v (2) &#12297;) + (5)</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where (x) + = max(0, x) and &#12296;&#183;, &#183;&#12297; denotes the inner product of two vectors.</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>With the perceptron algorithm (Shen and Joshi, 2005), the gradient of the hinge loss is:</text>
                  <doc_id>128</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8711;L(w i ) =</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>{ v (2) &#8722; v (1) if&#12296;w i , v (1) &#8722; v (2) &#12297; &#8804; 0 0 otherwise (6)</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The training instances for the discriminative learning in pairwise ranking are made by comparing the N-best list of the translation candidates scored by the smoothed sentence-level BLEU (Liang et al., 2006).</text>
                  <doc_id>131</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Following Simianer et al. (2012), the N-best list is divided into three bins: the top 10% (High), the middle 80% (Middle), and the last 10% (Low).</text>
                  <doc_id>132</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>These bins are used for pairwise ranking where the translation preference pairs are built between the candidates in High-Middle, Middle-Low, and High- Low, but not the candidates within the same bin, which is shown in Figure 2.</text>
                  <doc_id>133</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The idea is to guarantee that the ranker is more discriminative to prefer the good translations to the bad ones.</text>
                  <doc_id>134</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>N-best list</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>High: 10%</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Middle: 80%</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Low: 10%</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Our modified algorithm is illustrated in Algorithm 1.</text>
                  <doc_id>139</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Each column vector w i is further split into two parts w I i and w G i , representing the In-domain and General-domain feature weights respectively.</text>
                  <doc_id>140</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In Algorithm 1, we first distribute the domain-specific SMT decoders to different machines and initialize the feature weights (line 1-2).</text>
                  <doc_id>141</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Typically, the SGD algorithm runs in several iterations (In this study, we set the number of epochs T to 20) (line 3).</text>
                  <doc_id>142</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Multiple SMT decoders run in parallel and each decoder updates its feature weights individually using its indomain development data (line 4-15).</text>
                  <doc_id>143</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For each domain, the domain-specific decoder translates each in-domain development sentence and determines the N-best translations (line 4-8).</text>
                  <doc_id>144</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The preference pairs are built and used to update the parameters by gradient descent with &#951; = 0.0001 (line 9-13).</text>
                  <doc_id>145</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Each domain-specific decoder translates its in-domain development data multiple times.</text>
                  <doc_id>146</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>After each iteration, feature weights from all decoders are collected (line 16-19).</text>
                  <doc_id>147</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast to the original algorithm (Simianer et al., 2012), we only average the generaldomain feature weights w G 1 , .</text>
                  <doc_id>148</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>149</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>150</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
                <sentence>
                  <text>, wG N , but do not average the in-domain feature weights (line 20-25).</text>
                  <doc_id>151</doc_id>
                  <sec_id>12</sec_id>
                </sentence>
                <sentence>
                  <text>The reason is we hope to leverage the commonalities among these systems.</text>
                  <doc_id>152</doc_id>
                  <sec_id>13</sec_id>
                </sentence>
                <sentence>
                  <text>Meanwhile, general knowledge is enforced to be conveyed equally across different domains.</text>
                  <doc_id>153</doc_id>
                  <sec_id>14</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, the algorithm returns all the domain-specific feature weights w 1 , w 2 , .</text>
                  <doc_id>154</doc_id>
                  <sec_id>15</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>155</doc_id>
                  <sec_id>16</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>156</doc_id>
                  <sec_id>17</sec_id>
                </sentence>
                <sentence>
                  <text>, w N that are used for testing (line 27).</text>
                  <doc_id>157</doc_id>
                  <sec_id>18</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>After the joint MTL-based tuning, the feature weights tailored for domain-specific SMT systems are used to translate the testing data.</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We collect indomain testing data for each domain to evaluate the domain-specific systems.</text>
                  <doc_id>159</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Although this is not always the case in real applications where the testing domain is known, this study mainly focuses on the effectiveness of the MTL-based tuning approach.</text>
                  <doc_id>160</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Experiments</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>161</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Data</title>
            <text>We evaluated our MTL-based domain adaptation approach on a large-scale English-to-Chinese machine translation task. The training data consisted of two parts: monolingual data and bilingual data. The monolingual data was used to train the sourceside and target-side LMs, both of which were used for data selection in Section 2.1. In addition, the target-side LMs were re-used in the SMT systems as features. As mentioned in Section 2.1, we built a web crawler to collect a large number of webpages from web portal sites in English and Chinese respectively. In the experiments, we mainly focused on six popular domains, namely Business, Entertainment, Health, Science &amp; Technology, Sports, and Politics. For both English and Chinese webpages, the HTML tags were removed and the main content was extracted. The data statistics are shown in Table 1. The bilingual data we used was mainly mined from the web using the method proposed by Jiang et al. (2009), with a post-processing step using our bilingual data cleaning method (Cui et al., 2013). Therefore, the data quality is pretty good. In addition, we also used the English-Chinese parallel corpus released by LDC 2 . In total, the bilingual data
2 LDC2003E07, LDC2003E14, LDC2004E12,
LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26,
contained around 30 million sentence pairs, with 404M words in English and 329M words in Chinese. For each domain, we used the cross-entropy based method in Section 2.1 to rank the entire bilingual data, and the top 10% sentence pairs from the ranked bilingual data were selected as the in-domain data to train the in-domain TM. Moreover, we prepared 2,000 in-domain sentences for development and 1,000 in-domain sentences for testing in each domain. The details are shown in Table 2.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We evaluated our MTL-based domain adaptation approach on a large-scale English-to-Chinese machine translation task.</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The training data consisted of two parts: monolingual data and bilingual data.</text>
                  <doc_id>163</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The monolingual data was used to train the sourceside and target-side LMs, both of which were used for data selection in Section 2.1.</text>
                  <doc_id>164</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, the target-side LMs were re-used in the SMT systems as features.</text>
                  <doc_id>165</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>As mentioned in Section 2.1, we built a web crawler to collect a large number of webpages from web portal sites in English and Chinese respectively.</text>
                  <doc_id>166</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In the experiments, we mainly focused on six popular domains, namely Business, Entertainment, Health, Science &amp; Technology, Sports, and Politics.</text>
                  <doc_id>167</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>For both English and Chinese webpages, the HTML tags were removed and the main content was extracted.</text>
                  <doc_id>168</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>The data statistics are shown in Table 1.</text>
                  <doc_id>169</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>The bilingual data we used was mainly mined from the web using the method proposed by Jiang et al. (2009), with a post-processing step using our bilingual data cleaning method (Cui et al., 2013).</text>
                  <doc_id>170</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, the data quality is pretty good.</text>
                  <doc_id>171</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, we also used the English-Chinese parallel corpus released by LDC 2 .</text>
                  <doc_id>172</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>In total, the bilingual data</text>
                  <doc_id>173</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 LDC2003E07, LDC2003E14, LDC2004E12,</text>
                  <doc_id>174</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26,</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>contained around 30 million sentence pairs, with 404M words in English and 329M words in Chinese.</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For each domain, we used the cross-entropy based method in Section 2.1 to rank the entire bilingual data, and the top 10% sentence pairs from the ranked bilingual data were selected as the in-domain data to train the in-domain TM.</text>
                  <doc_id>177</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Moreover, we prepared 2,000 in-domain sentences for development and 1,000 in-domain sentences for testing in each domain.</text>
                  <doc_id>178</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The details are shown in Table 2.</text>
                  <doc_id>179</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Setup</title>
            <text>An in-house hierarchical phrase-based SMT decoder was implemented for our experiments. The CKY decoding algorithm was used and cube pruning was performed with the same default parameter settings as in Chiang (2007). We used a 100-best list from the decoder for the pairwise ranking algorithm. Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to
LDC2006E34, LDC2006E85, LDC2006E92.
refine the symmetric word alignment. The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency. An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data. The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002). A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>An in-house hierarchical phrase-based SMT decoder was implemented for our experiments.</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The CKY decoding algorithm was used and cube pruning was performed with the same default parameter settings as in Chiang (2007).</text>
                  <doc_id>181</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We used a 100-best list from the decoder for the pairwise ranking algorithm.</text>
                  <doc_id>182</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Translation models were trained over the bilingual data that was automatically word-aligned using GIZA++ (Och and Ney, 2003) in both directions, and the diag-grow-final heuristic was used to</text>
                  <doc_id>183</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>LDC2006E34, LDC2006E85, LDC2006E92.</text>
                  <doc_id>184</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>refine the symmetric word alignment.</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The phrase tables were filtered to retain top-20 translation candidates for each source phrase for efficiency.</text>
                  <doc_id>186</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>An in-house language modeling toolkit was used to train the 4-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995) over the web-crawled data.</text>
                  <doc_id>187</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The evaluation metric for the overall translation quality was case-insensitive BLEU4 (Papineni et al., 2002).</text>
                  <doc_id>188</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>A statistical significance test was performed using the bootstrap resampling method (Koehn, 2004).</text>
                  <doc_id>189</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Baseline</title>
            <text>We have two baselines. The first baseline is a nonadapted Hiero using our implementation. It contained the general-domain TM and LM, as well as other standard features. In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used. The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets. The second baseline is Google Online Translation Service 3 . We obtained the English-to- Chinese translations of the testing data from Google Translation to have a more solid comparison.
Moreover, we also compared our method with the adapted systems towards each domain individually (Koehn and Schroeder, 2007). This is to demonstrate the superiority of our MTL-based tuning approach across different domains.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We have two baselines.</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The first baseline is a nonadapted Hiero using our implementation.</text>
                  <doc_id>191</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It contained the general-domain TM and LM, as well as other standard features.</text>
                  <doc_id>192</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, the fix-discount method (Foster et al., 2006) for phrase table smoothing was also used.</text>
                  <doc_id>193</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The system was general-domain oriented and it was tuned by using MERT (Och, 2003) with a combination of six in-domain development datasets.</text>
                  <doc_id>194</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The second baseline is Google Online Translation Service 3 .</text>
                  <doc_id>195</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>We obtained the English-to- Chinese translations of the testing data from Google Translation to have a more solid comparison.</text>
                  <doc_id>196</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Moreover, we also compared our method with the adapted systems towards each domain individually (Koehn and Schroeder, 2007).</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This is to demonstrate the superiority of our MTL-based tuning approach across different domains.</text>
                  <doc_id>198</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>3.4 Results</title>
            <text>The end-to-end translation performance is shown in Table 3. We found that the baseline has a similar performance to Google Translation, with certain domains performed even better (Business, Sci&amp;Tech, Sports, Politics). This demonstrates that the translation quality of our baseline is state-of-the-art. Moreover, we can answer three questions according to the experimental results as follow: First, is domain mismatch a significant problem for a real world SMT system? We used the same system only with general-domain TM and LM, but tuned towards each domain individually using in-domain dev data. Table 3 shows that the setting &#8221;[A] G-TM + G-LM&#8221; performs much better than
3 http://translate.google.com
the non-adapted baseline across all domains with at least 1.2 BLEU points. In addition, the setting &#8221;[A] G-TM + G-LM&#8221; also outperforms Google Translation on all domains. Analogous to previous research, this confirms that the domain mismatch indeed exists and the parameter estimation using in-domain dev data is quite useful.
Second, does the mixture models based adaptation work for a variety of domains? We experimented with different settings with multiple TMs or LMs, or both. It is interesting to note that for largescale SMT systems, using in-domain models alone is inferior to using the general models alone. The setting &#8221;[A] G-TM + G-LM&#8221; is better than the setting &#8221;[A] I-TM + I-LM&#8221; across different domains. The reason is the data for general models has already included the in-domain data and the data coverage is much larger, thus the probability estimation is more reliable and the translation quality is much better.
For the LM, the in-domain LM performs better than the general-domain LM because our monolingual data (Table 1) for each domain is already sufficient for training an in-domain LM with good performance. From Table 3, we observed that the setting &#8221;[A] (G+I)-TM + I-LM&#8221; outperforms &#8221;[A] (G+I)-TM + G-LM&#8221;, with the &#8221;Sports&#8221; domain being the most significant. For the TM, the performance of the in-domain TM is inferior to the general-domain TM. The results show that the setting &#8221;[A] (G+I)-LM + G-TM&#8221; is significantly better than &#8221;[A] (G+I)-LM + I-TM&#8221;. The main reason is the data coverage for in-domain TM is much smaller than the general model. When each system uses two TMs and two LMs, it consistently results in better performance, indicating that mixture models are crucial for domain adaptation in SMT.
Third, can MTL further improve the translation quality? We used the MTL-based approach to jointly tune multiple domain-specific systems, leveraging the commonalities among different but related tasks. From Table 3, the MTL-based approach significantly improve the translation quality over the non-adapted baseline, and also outperforms conventional mixture models based methods. In particular, the &#8221;Sports&#8221; domain benefits the most from the indomain knowledge, which confirms that domain discrepancy should be addressed and may bring large improvements on certain domains.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The end-to-end translation performance is shown in Table 3.</text>
                  <doc_id>199</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We found that the baseline has a similar performance to Google Translation, with certain domains performed even better (Business, Sci&amp;Tech, Sports, Politics).</text>
                  <doc_id>200</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This demonstrates that the translation quality of our baseline is state-of-the-art.</text>
                  <doc_id>201</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Moreover, we can answer three questions according to the experimental results as follow: First, is domain mismatch a significant problem for a real world SMT system?</text>
                  <doc_id>202</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We used the same system only with general-domain TM and LM, but tuned towards each domain individually using in-domain dev data.</text>
                  <doc_id>203</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Table 3 shows that the setting &#8221;[A] G-TM + G-LM&#8221; performs much better than</text>
                  <doc_id>204</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3 http://translate.google.com</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the non-adapted baseline across all domains with at least 1.2 BLEU points.</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, the setting &#8221;[A] G-TM + G-LM&#8221; also outperforms Google Translation on all domains.</text>
                  <doc_id>207</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Analogous to previous research, this confirms that the domain mismatch indeed exists and the parameter estimation using in-domain dev data is quite useful.</text>
                  <doc_id>208</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Second, does the mixture models based adaptation work for a variety of domains?</text>
                  <doc_id>209</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We experimented with different settings with multiple TMs or LMs, or both.</text>
                  <doc_id>210</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It is interesting to note that for largescale SMT systems, using in-domain models alone is inferior to using the general models alone.</text>
                  <doc_id>211</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The setting &#8221;[A] G-TM + G-LM&#8221; is better than the setting &#8221;[A] I-TM + I-LM&#8221; across different domains.</text>
                  <doc_id>212</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The reason is the data for general models has already included the in-domain data and the data coverage is much larger, thus the probability estimation is more reliable and the translation quality is much better.</text>
                  <doc_id>213</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For the LM, the in-domain LM performs better than the general-domain LM because our monolingual data (Table 1) for each domain is already sufficient for training an in-domain LM with good performance.</text>
                  <doc_id>214</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>From Table 3, we observed that the setting &#8221;[A] (G+I)-TM + I-LM&#8221; outperforms &#8221;[A] (G+I)-TM + G-LM&#8221;, with the &#8221;Sports&#8221; domain being the most significant.</text>
                  <doc_id>215</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For the TM, the performance of the in-domain TM is inferior to the general-domain TM.</text>
                  <doc_id>216</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The results show that the setting &#8221;[A] (G+I)-LM + G-TM&#8221; is significantly better than &#8221;[A] (G+I)-LM + I-TM&#8221;.</text>
                  <doc_id>217</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The main reason is the data coverage for in-domain TM is much smaller than the general model.</text>
                  <doc_id>218</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>When each system uses two TMs and two LMs, it consistently results in better performance, indicating that mixture models are crucial for domain adaptation in SMT.</text>
                  <doc_id>219</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Third, can MTL further improve the translation quality?</text>
                  <doc_id>220</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We used the MTL-based approach to jointly tune multiple domain-specific systems, leveraging the commonalities among different but related tasks.</text>
                  <doc_id>221</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>From Table 3, the MTL-based approach significantly improve the translation quality over the non-adapted baseline, and also outperforms conventional mixture models based methods.</text>
                  <doc_id>222</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In particular, the &#8221;Sports&#8221; domain benefits the most from the indomain knowledge, which confirms that domain discrepancy should be addressed and may bring large improvements on certain domains.</text>
                  <doc_id>223</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>4</index>
            <title>3.5 Discussion</title>
            <text>According to our experiments, only averaging over the out-of-domain feature weights returned robust and converged results. We do not have theoretically grounded guarantee. However, we observed that the BLEU score of our method on DEV data was slightly lower than that in the baseline system, which indicates the out-of-domain features are less over-fitting on the domain-specific DEV data since
SOURSE
REF
A point begins with a player serving the ball. This means one player hits the ball towards the other player. (The serve must be played from behind the
baseline and must &#10047;&#10047;&#10047;&#10047; land in the service box . Players get two attempts to make a good serve.) &#24471; &#20998; &#30001; &#19968; &#20010; &#29699; &#21592; &#21457; &#29699; &#24320; &#22987; , &#36825; &#26159; &#25351; &#19968; &#20010; &#29699; &#21592; &#21521; &#21478; &#19968; &#20010; &#29699; &#21592; &#20987;
&#29699; &#12290;( &#21457; &#29699; &#26102; &#36873; &#25163; &#24517; &#39035; &#31449; &#22312; &#24213; &#32447; &#20043; &#22806; , &#29699; &#24517; &#39035; &#35201; &#33853; &#22312; &#23545; &#26041; &#30340; &#21457; &#29699; &#21306; &#20869; ,
&#10047;&#10047;&#10047;&#10047;
&#27599; &#27425; &#21457; &#29699; &#20801; &#35768; &#26377; &#19968; &#27425; &#22833; &#35823; &#12290;) [N] Baseline (G-TM + G-LM) &#33310; &#20250; &#22987; &#20110; &#29609; &#23478; &#26381; &#21153; &#30340; &#19968; &#20010; &#28857; &#12290; &#36825; &#24847; &#21619; &#30528; &#29609; &#23478; &#23545; &#20854; &#20182; &#29609; &#23478; &#30340; &#20987; &#29699; &#12290;
[A] (G+I)-TM + (G+I)-LM
[A,MTL](G+I)-TM + (G+I)-LM ( &#35813; &#26381; &#21153; &#24517; &#39035; &#20174; &#32972; &#21518; &#25171; &#30340; &#22522; &#32447; &#21644; &#24517; &#39035; &#38477; &#33853; &#22312; &#26381; &#21153; &#26694; &#12290; &#29699; &#21592; &#20004; &#27425; &#35797; &#22270; &#25104;
&#10047;&#10047;&#10047;&#10047;&#10047;&#10047;&#10047;
&#20026; &#19968; &#20010; &#22909; &#30340; &#26381; &#21153; &#12290;) &#19968; &#24320; &#22987; &#29699; &#30340; &#29699; &#21592; , &#36825; &#24847; &#21619; &#30528; &#19968; &#21517; &#29699; &#21592; &#29699; &#25171; &#21521; &#20854; &#20182; &#29699; &#21592; &#12290;( &#24517; &#39035; &#20174;
&#24213; &#32447; &#21457; &#29699; , &#24517; &#39035; &#22312; &#21457; &#29699; &#21306; &#30340; &#21306; &#22495; &#12290; &#29699; &#21592; &#21482; &#26377; &#20004; &#27425; &#23581; &#35797; &#21435; &#20570; &#19968; &#20010; &#22909;
&#10047;&#10047;&#10047;&#10047;&#10047;
&#30340; &#21457; &#29699; &#12290;) &#31532; &#19968; &#29699; &#30340; &#29699; &#21592; , &#36825; &#24847; &#21619; &#30528; &#19968; &#21517; &#29699; &#21592; &#23545; &#21478; &#19968; &#20010; &#29699; &#21592; &#20987; &#29699; &#12290;( &#24517; &#39035; &#22312; &#24213;
&#32447; &#21518; &#38754; &#21457; &#29699; , &#24182; &#19988; &#24517; &#39035; &#38477; &#33853; &#22312; &#21457; &#29699; &#21306; &#12290; &#29699; &#21592; &#20004; &#27425; &#35797; &#22270; &#25104; &#20026; &#19968; &#20010; &#22909;
&#10047;&#10047;&#10047;&#10047;&#10047;&#10047;&#10047;
&#30340; &#21457; &#29699; &#12290;)
we enforced them to play the same role across different domains. It seems that averaging the out-ofdomain feature weights can be considered as a kind of regularization.
An example sentence from the Sports domain with translations from different methods is shown in Table 4. In this sentence, the baseline always translates &#8221;player&#8221; to &#8221; &#29609; &#23478; &#8221; (game player), which should be &#8221; &#29699; &#21592; &#8221; (ball player). And, the baseline translates &#8221;serve&#8221; to &#8221; &#26381; &#21153; &#8221; (work for), which should be &#8221; &#21457; &#29699; &#8221; (put the ball into play). The phrase &#8221;service box&#8221; here means &#8221; &#21457; &#29699; &#21306; &#8221;, which denotes the zone where the ball is to be served. However, the baseline incorrectly splits them into two words, then translates &#8221;service&#8221; to &#8221; &#26381; &#21153; &#8221; and &#8221;box&#8221; to &#8221; &#26694; &#8221;. In contrast, the approaches with adapted models are able to translate these words very well. Both our MTL-based approach and the conventional adaptation methods leverage the mixture models. A natural question is why our MTL-based approach performs better than the individual adaptation. To answer this question, we looked into the details of the tuning and decoding procedures in the MTL-based approach. We observed that the BLEU score on the development data for each system was lower than the score when conducting individual adaptation. Considering that the algorithm enforcing the general features play the same role across different domains, we suspect that MTL-based approach introduces a kind of regularization for each domain-specific system. The regularization prevents the general features from biasing towards certain domains to the extreme. This property is quite important for real world SMT systems. Usually, a sentence is composed of some domain-specific words and some general words, so it is often improper to translate every word in the sentence using the indomain knowledge. For the example in Table 4, the individual adaptation method &#8221;[A] (G+I)-TM + (G+I)-LM&#8221; translates &#8221;land&#8221; to &#8221; &#21306; &#22495; &#8221; (zone) improperly, because &#8221; &#21306; &#22495; &#8221; appears more often in the Sports text than the general-domain text. This shows that the individual adaptation methods tend to overfit the in-domain development data. In contrast, the MTL-based approach &#8221;[A,MTL](G+I)-TM + (G+I)- LM&#8221; just translates &#8221;land&#8221; to &#8221; &#38477; &#33853; &#22312; &#8221; (fall on), which is more appropriate.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>According to our experiments, only averaging over the out-of-domain feature weights returned robust and converged results.</text>
                  <doc_id>224</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We do not have theoretically grounded guarantee.</text>
                  <doc_id>225</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, we observed that the BLEU score of our method on DEV data was slightly lower than that in the baseline system, which indicates the out-of-domain features are less over-fitting on the domain-specific DEV data since</text>
                  <doc_id>226</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>SOURSE</text>
                  <doc_id>227</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>REF</text>
                  <doc_id>228</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A point begins with a player serving the ball.</text>
                  <doc_id>229</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This means one player hits the ball towards the other player.</text>
                  <doc_id>230</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>(The serve must be played from behind the</text>
                  <doc_id>231</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>baseline and must &#10047;&#10047;&#10047;&#10047; land in the service box .</text>
                  <doc_id>232</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Players get two attempts to make a good serve.</text>
                  <doc_id>233</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>) &#24471; &#20998; &#30001; &#19968; &#20010; &#29699; &#21592; &#21457; &#29699; &#24320; &#22987; , &#36825; &#26159; &#25351; &#19968; &#20010; &#29699; &#21592; &#21521; &#21478; &#19968; &#20010; &#29699; &#21592; &#20987;</text>
                  <doc_id>234</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#29699; &#12290;( &#21457; &#29699; &#26102; &#36873; &#25163; &#24517; &#39035; &#31449; &#22312; &#24213; &#32447; &#20043; &#22806; , &#29699; &#24517; &#39035; &#35201; &#33853; &#22312; &#23545; &#26041; &#30340; &#21457; &#29699; &#21306; &#20869; ,</text>
                  <doc_id>235</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#10047;&#10047;&#10047;&#10047;</text>
                  <doc_id>236</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#27599; &#27425; &#21457; &#29699; &#20801; &#35768; &#26377; &#19968; &#27425; &#22833; &#35823; &#12290;) [N] Baseline (G-TM + G-LM) &#33310; &#20250; &#22987; &#20110; &#29609; &#23478; &#26381; &#21153; &#30340; &#19968; &#20010; &#28857; &#12290; &#36825; &#24847; &#21619; &#30528; &#29609; &#23478; &#23545; &#20854; &#20182; &#29609; &#23478; &#30340; &#20987; &#29699; &#12290;</text>
                  <doc_id>237</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>[A] (G+I)-TM + (G+I)-LM</text>
                  <doc_id>238</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>[A,MTL](G+I)-TM + (G+I)-LM ( &#35813; &#26381; &#21153; &#24517; &#39035; &#20174; &#32972; &#21518; &#25171; &#30340; &#22522; &#32447; &#21644; &#24517; &#39035; &#38477; &#33853; &#22312; &#26381; &#21153; &#26694; &#12290; &#29699; &#21592; &#20004; &#27425; &#35797; &#22270; &#25104;</text>
                  <doc_id>239</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#10047;&#10047;&#10047;&#10047;&#10047;&#10047;&#10047;</text>
                  <doc_id>240</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#20026; &#19968; &#20010; &#22909; &#30340; &#26381; &#21153; &#12290;) &#19968; &#24320; &#22987; &#29699; &#30340; &#29699; &#21592; , &#36825; &#24847; &#21619; &#30528; &#19968; &#21517; &#29699; &#21592; &#29699; &#25171; &#21521; &#20854; &#20182; &#29699; &#21592; &#12290;( &#24517; &#39035; &#20174;</text>
                  <doc_id>241</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#24213; &#32447; &#21457; &#29699; , &#24517; &#39035; &#22312; &#21457; &#29699; &#21306; &#30340; &#21306; &#22495; &#12290; &#29699; &#21592; &#21482; &#26377; &#20004; &#27425; &#23581; &#35797; &#21435; &#20570; &#19968; &#20010; &#22909;</text>
                  <doc_id>242</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#10047;&#10047;&#10047;&#10047;&#10047;</text>
                  <doc_id>243</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#30340; &#21457; &#29699; &#12290;) &#31532; &#19968; &#29699; &#30340; &#29699; &#21592; , &#36825; &#24847; &#21619; &#30528; &#19968; &#21517; &#29699; &#21592; &#23545; &#21478; &#19968; &#20010; &#29699; &#21592; &#20987; &#29699; &#12290;( &#24517; &#39035; &#22312; &#24213;</text>
                  <doc_id>244</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#32447; &#21518; &#38754; &#21457; &#29699; , &#24182; &#19988; &#24517; &#39035; &#38477; &#33853; &#22312; &#21457; &#29699; &#21306; &#12290; &#29699; &#21592; &#20004; &#27425; &#35797; &#22270; &#25104; &#20026; &#19968; &#20010; &#22909;</text>
                  <doc_id>245</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#10047;&#10047;&#10047;&#10047;&#10047;&#10047;&#10047;</text>
                  <doc_id>246</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#30340; &#21457; &#29699; &#12290;)</text>
                  <doc_id>247</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>we enforced them to play the same role across different domains.</text>
                  <doc_id>248</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It seems that averaging the out-ofdomain feature weights can be considered as a kind of regularization.</text>
                  <doc_id>249</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>An example sentence from the Sports domain with translations from different methods is shown in Table 4.</text>
                  <doc_id>250</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this sentence, the baseline always translates &#8221;player&#8221; to &#8221; &#29609; &#23478; &#8221; (game player), which should be &#8221; &#29699; &#21592; &#8221; (ball player).</text>
                  <doc_id>251</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>And, the baseline translates &#8221;serve&#8221; to &#8221; &#26381; &#21153; &#8221; (work for), which should be &#8221; &#21457; &#29699; &#8221; (put the ball into play).</text>
                  <doc_id>252</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The phrase &#8221;service box&#8221; here means &#8221; &#21457; &#29699; &#21306; &#8221;, which denotes the zone where the ball is to be served.</text>
                  <doc_id>253</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>However, the baseline incorrectly splits them into two words, then translates &#8221;service&#8221; to &#8221; &#26381; &#21153; &#8221; and &#8221;box&#8221; to &#8221; &#26694; &#8221;.</text>
                  <doc_id>254</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast, the approaches with adapted models are able to translate these words very well.</text>
                  <doc_id>255</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Both our MTL-based approach and the conventional adaptation methods leverage the mixture models.</text>
                  <doc_id>256</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>A natural question is why our MTL-based approach performs better than the individual adaptation.</text>
                  <doc_id>257</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>To answer this question, we looked into the details of the tuning and decoding procedures in the MTL-based approach.</text>
                  <doc_id>258</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>We observed that the BLEU score on the development data for each system was lower than the score when conducting individual adaptation.</text>
                  <doc_id>259</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>Considering that the algorithm enforcing the general features play the same role across different domains, we suspect that MTL-based approach introduces a kind of regularization for each domain-specific system.</text>
                  <doc_id>260</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>The regularization prevents the general features from biasing towards certain domains to the extreme.</text>
                  <doc_id>261</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
                <sentence>
                  <text>This property is quite important for real world SMT systems.</text>
                  <doc_id>262</doc_id>
                  <sec_id>12</sec_id>
                </sentence>
                <sentence>
                  <text>Usually, a sentence is composed of some domain-specific words and some general words, so it is often improper to translate every word in the sentence using the indomain knowledge.</text>
                  <doc_id>263</doc_id>
                  <sec_id>13</sec_id>
                </sentence>
                <sentence>
                  <text>For the example in Table 4, the individual adaptation method &#8221;[A] (G+I)-TM + (G+I)-LM&#8221; translates &#8221;land&#8221; to &#8221; &#21306; &#22495; &#8221; (zone) improperly, because &#8221; &#21306; &#22495; &#8221; appears more often in the Sports text than the general-domain text.</text>
                  <doc_id>264</doc_id>
                  <sec_id>14</sec_id>
                </sentence>
                <sentence>
                  <text>This shows that the individual adaptation methods tend to overfit the in-domain development data.</text>
                  <doc_id>265</doc_id>
                  <sec_id>15</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast, the MTL-based approach &#8221;[A,MTL](G+I)-TM + (G+I)- LM&#8221; just translates &#8221;land&#8221; to &#8221; &#38477; &#33853; &#22312; &#8221; (fall on), which is more appropriate.</text>
                  <doc_id>266</doc_id>
                  <sec_id>16</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Related Work</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>267</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Domain Adaptation</title>
            <text>One direction of domain adaptation explored the data selection and weighting approach to improve the performance of SMT on specific domains. Eck
et al. (2004) first decoded the testing data with a general TM, and then used the translation results to train an adapted LM, which was in turn used to re-decode the testing data. L&#252; et al. (2007) tried to weight the training data according to the similarity with test data using information retrieval models, while Foster et al. (2010) trained a discriminative model to estimate a weight for each sentence in the training corpus. Other methods conducted data selection based on cross-entropy (Moore and Lewis, 2010), and Axelrod et al. (2011) further extended their cross-entropy based method to the selection of bilingual corpus in the hope that more relevant corpus to the target domain could yield smaller models with better performance. Other methods included using semi-supervised transductive learning techniques to exploit the monolingual in-domain data (Ueffing et al., 2007).
Adaptation methods also involved the utilization of mixture models. Foster and Kuhn (2007) explored a number of variants of utilizing multiple TMs and LMs by interpolation. Koehn and Schroeder (2007) used MERT to simultaneously tune two TMs or LMs. Sennrich (2012) investigated the TM perplexity minimization as a method to set model weights in mixture modeling. In addition, inspired by system combination approaches, Razmara et al. (2012) used the ensemble decoding method to mix multiple translation models, which outperformed a variety of strong baselines. Generally, most previous methods merely conducted domain adaption for a single domain, rather than multiple domains at the same time. One could also simply build multiple SMT systems that were adapted to multiple domains, but they were often separated and not tuned together. So far, there has been little research into the multi-domain adaptation problem over mixture models for SMT systems, as proposed in this paper.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>One direction of domain adaptation explored the data selection and weighting approach to improve the performance of SMT on specific domains.</text>
                  <doc_id>268</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Eck</text>
                  <doc_id>269</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>et al. (2004) first decoded the testing data with a general TM, and then used the translation results to train an adapted LM, which was in turn used to re-decode the testing data.</text>
                  <doc_id>270</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>L&#252; et al. (2007) tried to weight the training data according to the similarity with test data using information retrieval models, while Foster et al. (2010) trained a discriminative model to estimate a weight for each sentence in the training corpus.</text>
                  <doc_id>271</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Other methods conducted data selection based on cross-entropy (Moore and Lewis, 2010), and Axelrod et al. (2011) further extended their cross-entropy based method to the selection of bilingual corpus in the hope that more relevant corpus to the target domain could yield smaller models with better performance.</text>
                  <doc_id>272</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Other methods included using semi-supervised transductive learning techniques to exploit the monolingual in-domain data (Ueffing et al., 2007).</text>
                  <doc_id>273</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Adaptation methods also involved the utilization of mixture models.</text>
                  <doc_id>274</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Foster and Kuhn (2007) explored a number of variants of utilizing multiple TMs and LMs by interpolation.</text>
                  <doc_id>275</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Koehn and Schroeder (2007) used MERT to simultaneously tune two TMs or LMs.</text>
                  <doc_id>276</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Sennrich (2012) investigated the TM perplexity minimization as a method to set model weights in mixture modeling.</text>
                  <doc_id>277</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, inspired by system combination approaches, Razmara et al. (2012) used the ensemble decoding method to mix multiple translation models, which outperformed a variety of strong baselines.</text>
                  <doc_id>278</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Generally, most previous methods merely conducted domain adaption for a single domain, rather than multiple domains at the same time.</text>
                  <doc_id>279</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>One could also simply build multiple SMT systems that were adapted to multiple domains, but they were often separated and not tuned together.</text>
                  <doc_id>280</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>So far, there has been little research into the multi-domain adaptation problem over mixture models for SMT systems, as proposed in this paper.</text>
                  <doc_id>281</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Multi-task Learning</title>
            <text>In machine learning, MTL is an approach to learn one target problem with other related problems at the same time. This often leads to a better model for the main task because it allows the learner to use the commonality among the tasks. MTL is performed by learning tasks in parallel while using a shared representation. Therefore, what is learned for each task can help other tasks be learned better.
MTL was successfully applied in some Natural Language Processing (NLP) tasks. For example, Blitzer et al. (2006) extended the MTL approach (Ando and Zhang, 2005) to domain adaptation tasks in part-of-speech tagging. Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling. They reported that jointly learning these tasks led to superior performance. MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation. In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking on sparse feature sets, where each N-best list corresponded to a distinct task. Simianer et al. (2012) proposed distributed stochastic learning with feature selection inspired by MTL. The distributed learning approach outperformed several other training methods including MIRA and SGD.
Inspired by these methods, we used MTL to tune multiple SMT systems at the same time, where each system was composed of in-domain and generaldomain models. Through a shared feature representation, the commonalities among the SMT systems were better learned by the general models. In addition, domain-specific translation knowledge was also better characterized by the in-domain models.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In machine learning, MTL is an approach to learn one target problem with other related problems at the same time.</text>
                  <doc_id>282</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This often leads to a better model for the main task because it allows the learner to use the commonality among the tasks.</text>
                  <doc_id>283</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>MTL is performed by learning tasks in parallel while using a shared representation.</text>
                  <doc_id>284</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, what is learned for each task can help other tasks be learned better.</text>
                  <doc_id>285</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>MTL was successfully applied in some Natural Language Processing (NLP) tasks.</text>
                  <doc_id>286</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For example, Blitzer et al. (2006) extended the MTL approach (Ando and Zhang, 2005) to domain adaptation tasks in part-of-speech tagging.</text>
                  <doc_id>287</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Collobert and Weston (2008) proposed using deep neural networks to train a set of tasks, including part-of-speech tagging, chunking, named entity recognition, and semantic roles labeling.</text>
                  <doc_id>288</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>They reported that jointly learning these tasks led to superior performance.</text>
                  <doc_id>289</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>MTL was also applied in sentiment analysis (Dredze and Crammer, 2008) and web ranking (Chapelle et al., 2011) to address the multi-domain learning and adaptation.</text>
                  <doc_id>290</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In SMT, Duh et al. (2010) proposed using MTL for N-best re-ranking on sparse feature sets, where each N-best list corresponded to a distinct task.</text>
                  <doc_id>291</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Simianer et al. (2012) proposed distributed stochastic learning with feature selection inspired by MTL.</text>
                  <doc_id>292</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>The distributed learning approach outperformed several other training methods including MIRA and SGD.</text>
                  <doc_id>293</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Inspired by these methods, we used MTL to tune multiple SMT systems at the same time, where each system was composed of in-domain and generaldomain models.</text>
                  <doc_id>294</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Through a shared feature representation, the commonalities among the SMT systems were better learned by the general models.</text>
                  <doc_id>295</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, domain-specific translation knowledge was also better characterized by the in-domain models.</text>
                  <doc_id>296</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Conclusion and Future Work</title>
        <text>In this paper, we propose an MTL-based approach to address multi-domain adaptation for SMT. We first use the cross-entropy based data selection method to obtain in-domain bilingual data. After that, indomain TMs and LMs are trained for each domainspecific SMT system. In addition, the generaldomain TM and LM are also trained and shared across different systems. Finally, MTL is leveraged to tune multiple systems jointly. Experimental results have shown that our approach is quite promising for the multi-domain adaptation problem, and it brings significant improvement over both the non-adapted baselines and the conventional domain adaptation methods with mixture models. We assume the domain information for testing 1063 data is known beforehand in this study. However, this is not always the case for real world SMT systems. Therefore, to apply our approach in real applications, the domain information needs to be identified automatically. In the future, we will pre-define more popular domains and develop automatic domain classifiers. For those domains that are identified with high confidence, we use the domainspecific system to translate the texts. For other texts, we use the general system to translate them. Furthermore, since our approach is a general training method, we may also combine this approach with other domain adaptation methods to get more performance improvement.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper, we propose an MTL-based approach to address multi-domain adaptation for SMT.</text>
              <doc_id>297</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We first use the cross-entropy based data selection method to obtain in-domain bilingual data.</text>
              <doc_id>298</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>After that, indomain TMs and LMs are trained for each domainspecific SMT system.</text>
              <doc_id>299</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In addition, the generaldomain TM and LM are also trained and shared across different systems.</text>
              <doc_id>300</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Finally, MTL is leveraged to tune multiple systems jointly.</text>
              <doc_id>301</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Experimental results have shown that our approach is quite promising for the multi-domain adaptation problem, and it brings significant improvement over both the non-adapted baselines and the conventional domain adaptation methods with mixture models.</text>
              <doc_id>302</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We assume the domain information for testing 1063 data is known beforehand in this study.</text>
              <doc_id>303</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>However, this is not always the case for real world SMT systems.</text>
              <doc_id>304</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, to apply our approach in real applications, the domain information needs to be identified automatically.</text>
              <doc_id>305</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>In the future, we will pre-define more popular domains and develop automatic domain classifiers.</text>
              <doc_id>306</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>For those domains that are identified with high confidence, we use the domainspecific system to translate the texts.</text>
              <doc_id>307</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>For other texts, we use the general system to translate them.</text>
              <doc_id>308</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Furthermore, since our approach is a general training method, we may also combine this approach with other domain adaptation methods to get more performance improvement.</text>
              <doc_id>309</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>Acknowledgments</title>
        <text>We are especially grateful to Nan Yang, Yajuan Duan, Hong Sun and Danran Chen for the helpful discussions. We also thank the anonymous reviewers for their insightful comments.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We are especially grateful to Nan Yang, Yajuan Duan, Hong Sun and Danran Chen for the helpful discussions.</text>
              <doc_id>310</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We also thank the anonymous reviewers for their insightful comments.</text>
              <doc_id>311</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Statistics of web-crawled monolingual data, in numbers of documents and words (main content). &#8221;M&#8221; refers to million and &#8221;B&#8221; refers to billion.</caption>
        <reference_text>In PAGE 7: ... The reason is the data for general models has already included the in-domain data and the data coverage is much larger, thus the probability estimation is more reliable and the translation quality is much better. For the LM, the in-domain LM performs better than the general-domain LM because our mono- lingual data ( Table1 ) for each domain is already sufficient for training an in-domain LM with good performance. From Table 3, we observed that the setting  [A] (G+I)-TM + I-LM  outperforms  [A] (G+I)-TM + G-LM , with the  Sports  domain be- ing the most significant....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Domain</cell>
              <cell>English</cell>
              <cell>English</cell>
              <cell>Chinese</cell>
              <cell>Chinese</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>Docs</cell>
              <cell>Words</cell>
              <cell>Docs</cell>
              <cell>Words</cell>
            </row>
            <row>
              <cell>Business</cell>
              <cell>21M</cell>
              <cell>10.4B</cell>
              <cell>7.91M</cell>
              <cell>2.73B</cell>
            </row>
            <row>
              <cell>Ent.</cell>
              <cell>18.3M</cell>
              <cell>8.29B</cell>
              <cell>4.16M</cell>
              <cell>1.31B</cell>
            </row>
            <row>
              <cell>Health</cell>
              <cell>8.7M</cell>
              <cell>4.73B</cell>
              <cell>0.9M</cell>
              <cell>0.42B</cell>
            </row>
            <row>
              <cell>Sci amp</cell>
              <cell>Tech</cell>
              <cell>10.9M</cell>
              <cell>5.33B</cell>
              <cell>5.28M</cell>
              <cell>1.6B</cell>
            </row>
            <row>
              <cell>Sports</cell>
              <cell>18.9M</cell>
              <cell>9.58B</cell>
              <cell>2.49M</cell>
              <cell>0.59B</cell>
            </row>
            <row>
              <cell>Politics</cell>
              <cell>10.3M</cell>
              <cell>5.56B</cell>
              <cell>1.67M</cell>
              <cell>0.39B</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Statistics of in-domain training, development and testing data, in number of words.</caption>
        <reference_text>None</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Domain</cell>
              <cell>Train</cell>
              <cell>Train</cell>
              <cell>Dev</cell>
              <cell>Dev</cell>
              <cell>Test</cell>
              <cell>Test</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>En</cell>
              <cell>Ch</cell>
              <cell>En</cell>
              <cell>Ch</cell>
              <cell>En</cell>
              <cell>Ch</cell>
            </row>
            <row>
              <cell>Business</cell>
              <cell>30M</cell>
              <cell>28M</cell>
              <cell>36K</cell>
              <cell>35K</cell>
              <cell>19K</cell>
              <cell>19K</cell>
            </row>
            <row>
              <cell>Ent.</cell>
              <cell>25M</cell>
              <cell>22M</cell>
              <cell>21K</cell>
              <cell>18K</cell>
              <cell>13K</cell>
              <cell>12K</cell>
            </row>
            <row>
              <cell>Health</cell>
              <cell>23M</cell>
              <cell>20M</cell>
              <cell>33K</cell>
              <cell>33K</cell>
              <cell>21K</cell>
              <cell>22K</cell>
            </row>
            <row>
              <cell>Sci amp</cell>
              <cell>Tech</cell>
              <cell>28M</cell>
              <cell>26M</cell>
              <cell>46K</cell>
              <cell>45K</cell>
              <cell>27K</cell>
              <cell>27K</cell>
            </row>
            <row>
              <cell>Sports</cell>
              <cell>19M</cell>
              <cell>16M</cell>
              <cell>18K</cell>
              <cell>14K</cell>
              <cell>10K</cell>
              <cell>9K</cell>
            </row>
            <row>
              <cell>Politics</cell>
              <cell>28M</cell>
              <cell>24M</cell>
              <cell>19K</cell>
              <cell>17K</cell>
              <cell>13K</cell>
              <cell>12K</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: End-to-end experimental results (BLEU4%) with large-scale training data (p &lt; 0.05). &#8221;[N]&#8221; means the system is non-adapted and tuned using MERT on general-domain dev data. &#8221;[A]&#8221; denotes that the system is adapted towards each domain individually using MERT on in-domain dev data. &#8221;[A,MTL]&#8221; indicates that the system was tuned using our MTL-based approach on in-domain dev data. &#8221;I-TM&#8221; and &#8221;G-TM&#8221; denote the in-domain and general-domain translation model. &#8221;I-LM&#8221; and &#8221;G-LM&#8221; denote the in-domain and general-domain language model. We also obtained translations of the testing data using Google Translation for comparison.</caption>
        <reference_text>In PAGE 6: ... More- over, we can answer three questions according to the experimental results as follow: First, is domain mismatch a significant prob- lem for a real world SMT system? We used the same system only with general-domain TM and LM, but tuned towards each domain individually using in-domain dev data.  Table3  shows that the setting  [A] G-TM + G-LM  performs much better than 3http://translate.google....  In PAGE 7: ... For the LM, the in-domain LM performs better than the general-domain LM because our mono- lingual data (Table 1) for each domain is already sufficient for training an in-domain LM with good performance. From  Table3 , we observed that the setting  [A] (G+I)-TM + I-LM  outperforms  [A] (G+I)-TM + G-LM , with the  Sports  domain be- ing the most significant. For the TM, the per- formance of the in-domain TM is inferior to the general-domain TM....  In PAGE 7: ... Third, can MTL further improve the transla- tion quality? We used the MTL-based approach to jointly tune multiple domain-specific systems, lever- aging the commonalities among different but related tasks. From  Table3 , the MTL-based approach sig- nificantly improve the translation quality over the non-adapted baseline, and also outperforms conven- tional mixture models based methods. In particular, the  Sports  domain benefits the most from the in- domain knowledge, which confirms that domain dis- crepancy should be addressed and may bring large improvements on certain domains....</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell>Business</cell>
              <cell>Ent.</cell>
              <cell>Health</cell>
              <cell>Sci amp#@#@Sci&amp;Tech</cell>
              <cell>Tech#@#@Sports</cell>
              <cell>Sports  Politics#@#@Politics</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>[N] Baseline (G-TM + G-LM)</cell>
              <cell>27.19</cell>
              <cell>17.87</cell>
              <cell>25.79</cell>
              <cell>25.34</cell>
              <cell>25.53</cell>
              <cell>23.01</cell>
            </row>
            <row>
              <cell>Google Translation</cell>
              <cell>26.01</cell>
              <cell>18.44</cell>
              <cell>27.71</cell>
              <cell>25.07</cell>
              <cell>24.08</cell>
              <cell>22.97</cell>
            </row>
            <row>
              <cell>[A] G-TM + G-LM</cell>
              <cell>29.58</cell>
              <cell>19.08</cell>
              <cell>28.80</cell>
              <cell>26.84</cell>
              <cell>30.28</cell>
              <cell>25.64</cell>
            </row>
            <row>
              <cell>[A] I-TM + I-LM</cell>
              <cell>28.20</cell>
              <cell>17.25</cell>
              <cell>27.20</cell>
              <cell>25.41</cell>
              <cell>30.12</cell>
              <cell>22.97</cell>
            </row>
            <row>
              <cell>[A] (G+I)-TM + G-LM</cell>
              <cell>29.45</cell>
              <cell>19.22</cell>
              <cell>28.93</cell>
              <cell>27.01</cell>
              <cell>31.01</cell>
              <cell>25.40</cell>
            </row>
            <row>
              <cell>[A] (G+I)-TM + I-LM</cell>
              <cell>29.60</cell>
              <cell>19.43</cell>
              <cell>28.94</cell>
              <cell>27.05</cell>
              <cell>34.36</cell>
              <cell>25.98</cell>
            </row>
            <row>
              <cell>[A] (G+I)-LM + G-TM</cell>
              <cell>29.66</cell>
              <cell>19.50</cell>
              <cell>29.00</cell>
              <cell>27.10</cell>
              <cell>33.60</cell>
              <cell>26.03</cell>
            </row>
            <row>
              <cell>[A] (G+I)-LM + I-TM</cell>
              <cell>28.50</cell>
              <cell>17.66</cell>
              <cell>27.58</cell>
              <cell>25.99</cell>
              <cell>30.44</cell>
              <cell>23.30</cell>
            </row>
            <row>
              <cell>[A] (G+I)-TM + (G+I)-LM</cell>
              <cell>29.82</cell>
              <cell>19.53</cell>
              <cell>29.03</cell>
              <cell>26.94</cell>
              <cell>33.77</cell>
              <cell>26.09</cell>
            </row>
            <row>
              <cell>[A,MTL] (G+I)-TM + (G+I)-LM</cell>
              <cell>30.26</cell>
              <cell>19.94</cell>
              <cell>29.08</cell>
              <cell>27.17</cell>
              <cell>34.11</cell>
              <cell>26.50</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Rie Kubota Ando</author>
          <author>Tong Zhang</author>
        </authors>
        <title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
        <publication>None</publication>
        <pages>6--1817</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Amittai Axelrod</author>
          <author>Xiaodong He</author>
          <author>Jianfeng Gao</author>
        </authors>
        <title>Domain adaptation via pseudo in-domain data selection.</title>
        <publication>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>355--362</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>John Blitzer</author>
          <author>Ryan McDonald</author>
          <author>Fernando Pereira</author>
        </authors>
        <title>Domain adaptation with structural correspondence learning.</title>
        <publication>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>120--128</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Olivier Chapelle</author>
          <author>Pannagadatta Shivaswamy</author>
          <author>Srinivas Vadrevu</author>
          <author>Kilian Weinberger</author>
          <author>Ya Zhang</author>
          <author>Belle Tseng</author>
        </authors>
        <title>None</title>
        <publication>Boosted multi-task learning. Machine learning,</publication>
        <pages>85--1</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>Hierarchical phrase-based translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Ronan Collobert</author>
          <author>Jason Weston</author>
        </authors>
        <title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
        <publication>In Proceedings of the 25th international conference on Machine learning,</publication>
        <pages>160--167</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Lei Cui</author>
          <author>Dongdong Zhang</author>
          <author>Shujie Liu</author>
          <author>Mu Li</author>
          <author>Ming Zhou</author>
        </authors>
        <title>Bilingual data cleaning for smt using graph-based random walk.</title>
        <publication>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</publication>
        <pages>340--345</pages>
        <date>2013</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Mark Dredze</author>
          <author>Koby Crammer</author>
        </authors>
        <title>Online methods for multi-domain learning and adaptation.</title>
        <publication>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>689--697</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Kevin Duh</author>
          <author>Katsuhito Sudoh</author>
          <author>Hajime Tsukada</author>
          <author>Hideki Isozaki</author>
          <author>Masaaki Nagata</author>
        </authors>
        <title>N-best reranking by multitask learning.</title>
        <publication>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</publication>
        <pages>375--383</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Matthias Eck</author>
          <author>Stephan Vogel</author>
          <author>Alex Waibel</author>
        </authors>
        <title>Language model adaptation for statistical machine translation based on information retrieval.</title>
        <publication>In In Proc. of LREC.</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>George Foster</author>
          <author>Roland Kuhn</author>
        </authors>
        <title>Mixture-model adaptation for SMT.</title>
        <publication>In Proceedings of the Second Workshop on Statistical Machine Translation,</publication>
        <pages>128--135</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>George Foster</author>
          <author>Roland Kuhn</author>
          <author>Howard Johnson</author>
        </authors>
        <title>Phrasetable smoothing for statistical machine translation.</title>
        <publication>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>53--61</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>George Foster</author>
          <author>Cyril Goutte</author>
          <author>Roland Kuhn</author>
        </authors>
        <title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
        <publication>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>451--459</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Long Jiang</author>
          <author>Shiquan Yang</author>
          <author>Ming Zhou</author>
          <author>Xiaohua Liu</author>
          <author>Qingsheng Zhu</author>
        </authors>
        <title>Mining bilingual data from the web with adaptively learnt patterns.</title>
        <publication>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</publication>
        <pages>870--878</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Reinhard Kneser</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Improved backing-off for m-gram language modeling.</title>
        <publication>In Acoustics, Speech, and Signal Processing,</publication>
        <pages>181--184</pages>
        <date>1995</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Philipp Koehn</author>
        </authors>
        <title>Statistical significance tests for machine translation evaluation.</title>
        <publication>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</publication>
        <pages>388--395</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Percy Liang</author>
          <author>Alexandre Bouchard-C&#244;t&#233;</author>
          <author>Dan Klein</author>
          <author>Ben Taskar</author>
        </authors>
        <title>An end-to-end discriminative approach to machine translation.</title>
        <publication>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>761--768</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Yajuan L&#252;</author>
          <author>Jin Huang</author>
          <author>Qun Liu</author>
        </authors>
        <title>Improving statistical machine translation performance by training data selection and optimization.</title>
        <publication>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</publication>
        <pages>343--350</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Robert C Moore</author>
          <author>William Lewis</author>
        </authors>
        <title>Intelligent selection of language model training data.</title>
        <publication>In Proceedings of the ACL 2010 Conference Short Papers,</publication>
        <pages>220--224</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A systematic comparison of various statistical alignment models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>160--167</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>Bleu: a method for automatic evaluation of machine translation.</title>
        <publication>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>311--318</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Majid Razmara</author>
          <author>George Foster</author>
          <author>Baskaran Sankaran</author>
          <author>Anoop Sarkar</author>
        </authors>
        <title>Mixing multiple translation models in statistical machine translation.</title>
        <publication>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</publication>
        <pages>940--949</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Rico Sennrich</author>
        </authors>
        <title>Perplexity minimization for translation model domain adaptation in statistical machine translation.</title>
        <publication>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</publication>
        <pages>539--549</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>Libin Shen</author>
          <author>Aravind K Joshi</author>
        </authors>
        <title>Ranking and reranking with perceptron.</title>
        <publication>Machine Learning,</publication>
        <pages>60--1</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Patrick Simianer</author>
          <author>Stefan Riezler</author>
          <author>Chris Dyer</author>
        </authors>
        <title>Joint feature selection in distributed stochastic learning for large-scale discriminative training in smt.</title>
        <publication>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</publication>
        <pages>11--21</pages>
        <date>2012</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Ando and Zhang, 2005</string>
        <sentence_id>15678</sentence_id>
        <char_offset>62</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Axelrod et al., 2011</string>
        <sentence_id>15406</sentence_id>
        <char_offset>165</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>Axelrod et al., 2011</string>
        <sentence_id>15432</sentence_id>
        <char_offset>51</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>1</reference_id>
        <string>Axelrod et al. (2011)</string>
        <sentence_id>15663</sentence_id>
        <char_offset>91</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>2</reference_id>
        <string>Blitzer et al. (2006)</string>
        <sentence_id>15678</sentence_id>
        <char_offset>13</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>3</reference_id>
        <string>Chapelle et al., 2011</string>
        <sentence_id>15411</sentence_id>
        <char_offset>122</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>3</reference_id>
        <string>Chapelle et al., 2011</string>
        <sentence_id>15681</sentence_id>
        <char_offset>87</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>4</reference_id>
        <string>Chiang (2007)</string>
        <sentence_id>15572</sentence_id>
        <char_offset>114</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>5</reference_id>
        <string>Collobert and Weston (2008)</string>
        <sentence_id>15679</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>6</reference_id>
        <string>Cui et al., 2013</string>
        <sentence_id>15561</sentence_id>
        <char_offset>177</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>7</reference_id>
        <string>Dredze and Crammer, 2008</string>
        <sentence_id>15411</sentence_id>
        <char_offset>79</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>7</reference_id>
        <string>Dredze and Crammer, 2008</string>
        <sentence_id>15681</sentence_id>
        <char_offset>44</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>8</reference_id>
        <string>Duh et al. (2010)</string>
        <sentence_id>15682</sentence_id>
        <char_offset>8</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>9</reference_id>
        <string>Eck et al., 2004</string>
        <sentence_id>15406</sentence_id>
        <char_offset>86</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>10</reference_id>
        <string>Foster and Kuhn, 2007</string>
        <sentence_id>15406</sentence_id>
        <char_offset>204</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>10</reference_id>
        <string>Foster and Kuhn (2007)</string>
        <sentence_id>15666</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>11</reference_id>
        <string>Foster et al., 2006</string>
        <sentence_id>15584</sentence_id>
        <char_offset>38</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>12</reference_id>
        <string>Foster et al. (2010)</string>
        <sentence_id>15662</sentence_id>
        <char_offset>136</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>12</reference_id>
        <string>Foster et al., 2010</string>
        <sentence_id>15406</sentence_id>
        <char_offset>121</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>13</reference_id>
        <string>Jiang et al. (2009)</string>
        <sentence_id>15561</sentence_id>
        <char_offset>86</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>14</reference_id>
        <string>Kneser and Ney, 1995</string>
        <sentence_id>15578</sentence_id>
        <char_offset>119</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>15</reference_id>
        <string>Koehn, 2004</string>
        <sentence_id>15580</sentence_id>
        <char_offset>85</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>16</reference_id>
        <string>Liang et al., 2006</string>
        <sentence_id>15511</sentence_id>
        <char_offset>196</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>16</reference_id>
        <string>Liang et al., 2006</string>
        <sentence_id>15518</sentence_id>
        <char_offset>187</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>17</reference_id>
        <string>L&#252; et al., 2007</string>
        <sentence_id>15406</sentence_id>
        <char_offset>104</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>17</reference_id>
        <string>L&#252; et al. (2007)</string>
        <sentence_id>15662</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>18</reference_id>
        <string>Moore and Lewis, 2010</string>
        <sentence_id>15406</sentence_id>
        <char_offset>142</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>18</reference_id>
        <string>Moore and Lewis, 2010</string>
        <sentence_id>15663</sentence_id>
        <char_offset>63</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>19</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>15574</sentence_id>
        <char_offset>106</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>20</reference_id>
        <string>Och, 2003</string>
        <sentence_id>15585</sentence_id>
        <char_offset>71</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>21</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>15579</sentence_id>
        <char_offset>86</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>22</reference_id>
        <string>Razmara et al. (2012)</string>
        <sentence_id>15669</sentence_id>
        <char_offset>56</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>22</reference_id>
        <string>Razmara et al., 2012</string>
        <sentence_id>15406</sentence_id>
        <char_offset>270</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>23</reference_id>
        <string>Sennrich (2012)</string>
        <sentence_id>15668</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>23</reference_id>
        <string>Sennrich, 2012</string>
        <sentence_id>15406</sentence_id>
        <char_offset>254</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>24</reference_id>
        <string>Shen and Joshi, 2005</string>
        <sentence_id>15510</sentence_id>
        <char_offset>71</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>24</reference_id>
        <string>Shen and Joshi, 2005</string>
        <sentence_id>15515</sentence_id>
        <char_offset>31</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>25</reference_id>
        <string>Simianer et al. (2012)</string>
        <sentence_id>15519</sentence_id>
        <char_offset>10</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>25</reference_id>
        <string>Simianer et al. (2012)</string>
        <sentence_id>15683</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>25</reference_id>
        <string>Simianer et al., 2012</string>
        <sentence_id>15423</sentence_id>
        <char_offset>53</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>25</reference_id>
        <string>Simianer et al., 2012</string>
        <sentence_id>15509</sentence_id>
        <char_offset>113</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>25</reference_id>
        <string>Simianer et al., 2012</string>
        <sentence_id>15535</sentence_id>
        <char_offset>39</char_offset>
      </citation>
    </citations>
  </content>
</document>
