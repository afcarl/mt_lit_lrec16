<document>
  <filename>D09-1073</filename>
  <authors/>
  <title>Tree Kernel-based SVM with Structured Syntactic Knowledge for BTG-based Phrase Reordering</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>Structured syntactic knowledge is important for phrase reordering. This paper proposes using convolution tree kernel over source parse tree to model structured syntactic knowledge for BTG-based phrase reordering in the context of statistical machine translation. Our study reveals that the structured syntactic features over the source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel. We further combine the structured features and other commonly-used linear features into a composite kernel. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Structured syntactic knowledge is important for phrase reordering.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This paper proposes using convolution tree kernel over source parse tree to model structured syntactic knowledge for BTG-based phrase reordering in the context of statistical machine translation.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Our study reveals that the structured syntactic features over the source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We further combine the structured features and other commonly-used linear features into a composite kernel.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT). As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods. Many research efforts have been made to address this issue, which can be summarized into two ideas. One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases. In this paper, we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phrase reordering in a phrase-based SMT system with BTG (Bracketing Transduction Grammar) constraints (Wu, 1997).
Word and phrase reordering is a crucial component in a SMT system. In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (zhang et al., 2007a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007). In phrasebased method, local word reordering 1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model. Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008). However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited. This makes the phrasebased method particularly weak in handling global phrase reordering. From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applica-
1 This paper follows the term convention of global reordering and local reordering of Li et al. (2007), between which the distinction is solely defined by reordering distance (whether beyond four source words) (Li et al., 2007).
tions. For example, one cannot enumerate efficiently all the sub-tree features for a full parse tree. This would be the reason why structured features are not fully utilized in previous statistical feature-based phrase reordering model.
Thanks to the nice property of kernel-based machine learning method that can implicitly explore (structured) features in a high dimensional feature space (Vapnik, 1995), in this paper we propose using convolution tree kernel (Haussler, 1999; Collins and Duffy, 2001) to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the model&#8217;s predictive ability. Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001), semantic role labeling (Moschitti, 2004; Zhang et al., 2007b), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006) and question classification (Zhang and Lee, 2003). However, to our knowledge, such technique still remains unexplored for phrase reordering.
In this paper, we look into the phrase reordering problem in two aspects: 1) how to model and optimize structured features, and 2) how to combine the structured features with other linear features and further integrate them into the loglinear model-based translation framework. Our study shows that: 1) the structured syntactic features are very useful and 2) our kernel-based model can well explore diverse knowledge, including previously-used linear features and the structured syntactic features, for phrase reordering. Our model displays one advantage over the previous work that it is able to utilize the structured syntactic features without the need for extensive feature engineering in decoding a parse tree into a set of linear syntactic features.
To have a more insightful evaluation, we design three experiments with three different evaluation metrics. Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our method statistically significantly outperforms the baseline methods in term of the three different evaluation metrics.
The rest of the paper is organized as follows. Section 2 introduces the baseline method of BTG-based phrase translation method while section 3 discusses the proposed method in detail. The experimental results are reported and discussed in section 4. Finally, we conclude the paper in section 5.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Phrase-based method (Koehn et al., 2003; Och and Ney, 2004; Koehn et al., 2007) and syntaxbased method (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Chiang, 2005; Cowan et al., 2006; Marcu et al., 2006; Liu et al., 2007; Zhang et al., 2007c, 2008a, 2008b; Shen et al., 2008; Mi and Huang, 2008) represent the state-of-the-art technologies in statistical machine translation (SMT).</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods.</text>
              <doc_id>6</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Many research efforts have been made to address this issue, which can be summarized into two ideas.</text>
              <doc_id>7</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases.</text>
              <doc_id>8</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phrase reordering in a phrase-based SMT system with BTG (Bracketing Transduction Grammar) constraints (Wu, 1997).</text>
              <doc_id>9</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Word and phrase reordering is a crucial component in a SMT system.</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (zhang et al., 2007a) and the impact of syntax on reordering is difficult to single out (Li et al., 2007).</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In phrasebased method, local word reordering 1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model.</text>
              <doc_id>12</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (Koehn et al., 2003; Och and Ney, 2004), flat reordering model (Wu, 1997; Zens et al., 2004), lexicalized reordering model (Tillmann, 2004; Kumar and Byrne, 2005), to hierarchical phrase-based model (Chiang, 2005; Setiawan et al., 2007) and classifier-based reordering model with linear features (Zens and Ney, 2006; Xiong et al., 2006; Zhang et al., 2007a; Xiong et al., 2008).</text>
              <doc_id>13</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (Li et al., 2007; Elming, 2008), has not been well exploited.</text>
              <doc_id>14</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This makes the phrasebased method particularly weak in handling global phrase reordering.</text>
              <doc_id>15</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>From machine learning viewpoint (Vapnik, 1995), it is computationally infeasible to explicitly generate features involving structured information in many NLP applica-</text>
              <doc_id>16</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 This paper follows the term convention of global reordering and local reordering of Li et al. (2007), between which the distinction is solely defined by reordering distance (whether beyond four source words) (Li et al., 2007).</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tions.</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For example, one cannot enumerate efficiently all the sub-tree features for a full parse tree.</text>
              <doc_id>19</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This would be the reason why structured features are not fully utilized in previous statistical feature-based phrase reordering model.</text>
              <doc_id>20</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Thanks to the nice property of kernel-based machine learning method that can implicitly explore (structured) features in a high dimensional feature space (Vapnik, 1995), in this paper we propose using convolution tree kernel (Haussler, 1999; Collins and Duffy, 2001) to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the model&#8217;s predictive ability.</text>
              <doc_id>21</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001), semantic role labeling (Moschitti, 2004; Zhang et al., 2007b), relation extraction (Zhang et al., 2006), pronoun resolution (Yang et al., 2006) and question classification (Zhang and Lee, 2003).</text>
              <doc_id>22</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, to our knowledge, such technique still remains unexplored for phrase reordering.</text>
              <doc_id>23</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this paper, we look into the phrase reordering problem in two aspects: 1) how to model and optimize structured features, and 2) how to combine the structured features with other linear features and further integrate them into the loglinear model-based translation framework.</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our study shows that: 1) the structured syntactic features are very useful and 2) our kernel-based model can well explore diverse knowledge, including previously-used linear features and the structured syntactic features, for phrase reordering.</text>
              <doc_id>25</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Our model displays one advantage over the previous work that it is able to utilize the structured syntactic features without the need for extensive feature engineering in decoding a parse tree into a set of linear syntactic features.</text>
              <doc_id>26</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To have a more insightful evaluation, we design three experiments with three different evaluation metrics.</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our method statistically significantly outperforms the baseline methods in term of the three different evaluation metrics.</text>
              <doc_id>28</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The rest of the paper is organized as follows.</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Section 2 introduces the baseline method of BTG-based phrase translation method while section 3 discusses the proposed method in detail.</text>
              <doc_id>30</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The experimental results are reported and discussed in section 4.</text>
              <doc_id>31</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we conclude the paper in section 5.</text>
              <doc_id>32</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Baseline System and Method</title>
        <text>We use the MaxEnt-based BTG translation system (Xiong et al., 2006) as our baseline. It is a phrase-based SMT system with BTG reordering constraint. The system uses the BTG lexical translation rules ( &#65533;&#65533;&#65533;/&#65533;) to translate the source phrase &#65533; into target phrase &#65533; , and the BTG merging rules ( &#65533;&#65533;&#65533;&#65533;, &#65533;&#65533;| &#65533;&#65533;,&#65533;&#65533;) to combine two neighboring phrases with a straight or inverted order. In the translation model, the BTG lexical rules are weighted with several features, such as phrase translation, word penalty and language models, in a log-linear form. With the BTG constraint, the reordering model &#937; is defined on the two neighboring phrases &#65533; &#65533; and &#65533; &#65533; and their order &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;, &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; as follows: &#937;&#65533;f(&#65533;, &#65533; &#65533; ,&#65533; &#65533; ) (1)
In the baseline system, a MaxEnt-based classifier with boundary words of the two neighboring phrases as features is used to model the merging/reordering order. The baseline MaxEntbased reordering model is formulized as follows:
&#937;&#65533;&#65533; &#65533; (&#65533;|&#65533; &#65533; ,&#65533; &#65533; ) &#65533; &#65533;&#65533;&#65533;(&#8721; &#65533; &#65533; &#65533;&#65533; &#65533; (&#65533;,&#65533; &#65533; ,&#65533; &#65533; )) (2)
&#8721;&#65533; &#65533;&#65533;&#65533;(&#8721;&#65533; &#65533; &#65533; &#65533; &#65533; (&#65533;,&#65533; &#65533; ,&#65533; &#65533; ))
where the functions &#65533; &#65533; (&#65533;, &#65533; &#65533; ,&#65533; &#65533; ) &#65533; &#65533;0,1&#65533; are model feature functions using the boundary words of the two neighboring phrases as features, and &#65533; &#65533; are feature weights that are trained based on the MaxEnt-based criteria.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We use the MaxEnt-based BTG translation system (Xiong et al., 2006) as our baseline.</text>
              <doc_id>33</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It is a phrase-based SMT system with BTG reordering constraint.</text>
              <doc_id>34</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The system uses the BTG lexical translation rules ( &#65533;&#65533;&#65533;/&#65533;) to translate the source phrase &#65533; into target phrase &#65533; , and the BTG merging rules ( &#65533;&#65533;&#65533;&#65533;, &#65533;&#65533;| &#65533;&#65533;,&#65533;&#65533;) to combine two neighboring phrases with a straight or inverted order.</text>
              <doc_id>35</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In the translation model, the BTG lexical rules are weighted with several features, such as phrase translation, word penalty and language models, in a log-linear form.</text>
              <doc_id>36</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>With the BTG constraint, the reordering model &#937; is defined on the two neighboring phrases &#65533; &#65533; and &#65533; &#65533; and their order &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;, &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; as follows: &#937;&#65533;f(&#65533;, &#65533; &#65533; ,&#65533; &#65533; ) (1)</text>
              <doc_id>37</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the baseline system, a MaxEnt-based classifier with boundary words of the two neighboring phrases as features is used to model the merging/reordering order.</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The baseline MaxEntbased reordering model is formulized as follows:</text>
              <doc_id>39</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#937;&#65533;&#65533; &#65533; (&#65533;|&#65533; &#65533; ,&#65533; &#65533; ) &#65533; &#65533;&#65533;&#65533;(&#8721; &#65533; &#65533; &#65533;&#65533; &#65533; (&#65533;,&#65533; &#65533; ,&#65533; &#65533; )) (2)</text>
              <doc_id>40</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8721;&#65533; &#65533;&#65533;&#65533;(&#8721;&#65533; &#65533; &#65533; &#65533; &#65533; (&#65533;,&#65533; &#65533; ,&#65533; &#65533; ))</text>
              <doc_id>41</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where the functions &#65533; &#65533; (&#65533;, &#65533; &#65533; ,&#65533; &#65533; ) &#65533; &#65533;0,1&#65533; are model feature functions using the boundary words of the two neighboring phrases as features, and &#65533; &#65533; are feature weights that are trained based on the MaxEnt-based criteria.</text>
              <doc_id>42</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Tree Kernel-based Phrase Reordering Model</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>43</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Kernel-based Classifier Solution to</title>
            <text>Phrase Reordering
In this paper, phrase reordering is recast as a classification issue as done in previous work (Xiong et al., 2006 &amp; 2008; Zhang et al., 2007a). In training, we use a machine learning algorithm training on the annotated phrase reordering instances that are automatically extracted from word-aligned, source sentence parsed training corpus, to learn a classifier. In testing (decoding), the learned classifier is applied to two adjacent source phrases to decide whether they should be merged (straight) or reordered (inverted) and what their probabilities are, and then these probabilities are used as one feature in the log-linear model in a phrase-based decoder.
In addition to the previously-used linear features, we are more interested in the value of structured syntax in phrase reordering and how to capture it using kernel methods. However, not
all classifiers are able to work with kernel methods. Only those dot-product-based classifiers can work with kernels by replacing the dot product with a kernel function, where the kernel function is able to directly calculate the similarity between two (structured) objects without enumerating them into linear feature vectors. In this paper, we select SVM as our classifier. In this section, we first define the structured syntactic features and introduce the commonly used linear features, and then discuss how to utilize these features by kernel methods together SVM for phrase reordering</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Phrase Reordering</text>
                  <doc_id>44</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In this paper, phrase reordering is recast as a classification issue as done in previous work (Xiong et al., 2006 &amp; 2008; Zhang et al., 2007a).</text>
                  <doc_id>45</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In training, we use a machine learning algorithm training on the annotated phrase reordering instances that are automatically extracted from word-aligned, source sentence parsed training corpus, to learn a classifier.</text>
                  <doc_id>46</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In testing (decoding), the learned classifier is applied to two adjacent source phrases to decide whether they should be merged (straight) or reordered (inverted) and what their probabilities are, and then these probabilities are used as one feature in the log-linear model in a phrase-based decoder.</text>
                  <doc_id>47</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In addition to the previously-used linear features, we are more interested in the value of structured syntax in phrase reordering and how to capture it using kernel methods.</text>
                  <doc_id>48</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, not</text>
                  <doc_id>49</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>all classifiers are able to work with kernel methods.</text>
                  <doc_id>50</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Only those dot-product-based classifiers can work with kernels by replacing the dot product with a kernel function, where the kernel function is able to directly calculate the similarity between two (structured) objects without enumerating them into linear feature vectors.</text>
                  <doc_id>51</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In this paper, we select SVM as our classifier.</text>
                  <doc_id>52</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In this section, we first define the structured syntactic features and introduce the commonly used linear features, and then discuss how to utilize these features by kernel methods together SVM for phrase reordering</text>
                  <doc_id>53</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Structured Syntactic Features</title>
            <text>A reordering instance &#65533;&#65533;&#65533;&#65533; &#65533; ,&#65533; &#65533; &#65533; (see Eq.1) in this paper refers to two adjacent source phrases &#65533; &#65533; and &#65533; &#65533; to be translated. The structured syntactic feature spaces of a reordering instance are defined as the portion of a parse tree of the source sentence that at least covers the span of the reordering instance (i.e. the two neighboring phrases). The syntactic features are defined as all
T 1 ) Minimum Sub-Tree (MST)
T 2 ) Minimum Sub-Structure (MSS) T 4 ) Chunking Tree (CT)
T 3 ) Context-sensitive Minimum Sub-Structure (CMSS)
Figure 1. Different representations of structured syntactic features of a reordering instance in the example sentence excerpted from our training corpus &#8220;&#8230; &#24314; &#31435; /build &#35268; &#27169; /scale &#23439; &#22823; /mighty &#30340; /of &#21508; &#31867; /various types &#20154; &#25165; /qualified personnel &#38431; &#20237; /contingent &#39318; &#20808; /above all &#36843; &#20999; /urgently &#38656; &#35201; /necessary &#20013; &#22830; /central authorities &#32479; &#31609; /overall &#35268; &#21010; /planning&#8230;(To build a mighty contingent of qualified personnel of various types, it is necessary, above all, for the central authorities to make overall planning.) &#8221;, where &#8220; &#21508; &#31867; /various types &#20154; &#25165; /qualified personnel &#38431; &#20237; /contingent (contingent of qualified personnel of various types)&#8221; is the 1 st /left phrase and &#8220; &#39318; &#20808; /above all &#36843; &#20999; /urgent &#38656; &#35201; /necessary (it is necessary, above all, &#8230;)&#8221; is the 2 nd /right phrase. Note that different function tags are attached to the grammar tag of each internal node.
the possible subtrees in the structured feature spaces. We can see that the structured feature spaces and their features are encapsulated by a full parse tree of source sentences. Thus, it is critical to understand which portion of a parse tree (i.e. structured feature space) is the most effective to represent a reordering instance. Motivated by the work of (Zhang et al., 2006), we here examine four cases that contain different sub-structures as shown in Fig. 1.
(1) Minimum Sub-Tree (MST): the sub-tree rooted by the nearest common ancestor of the two phrases. This feature records the minimum sub-structure covering the two phrases and its left and right contexts as shown in Fig 1.T 1 . (2) Minimum Sub-Structure (MSS): the smallest common sub-structure covering the two phrases. It is enclosed by the shortest path linking the two phrases. Thus, its leaf nodes exactly consist of all the phrasal words. (3) Context-sensitive Minimum Sub-Structure (CMSS): the MSS extending with the 1 st left sibling node of the left phrase and the 1 st right sibling node of the right phrase and their descendants. If sibling is unavailable, then we move to the parent of current node and repeat the same process until the sibling is available or the root of the MST is reached. (4) Chunking Tree (CT): the base phrase list extracted from the MSS. We prune out all the internal structures of the MSS and only keep the root node and the base phrase list for generating the chunking tree.
Fig. 1 illustrates the different representations of an example reordering instance. T 1 is the MST for the example instance, where the sub-structure circled by a dotted line is the MSS, which is also shown in T 2 for clarity. We can see that the MSS is a subset of the MST. By T 2 we would like to evaluate whether the structured information is effective for phrase reordering while by comparing the system performance when using T 1 and T 2, we would like to evaluate whether the structured context information embedded in the MST is useful to phrase reordering. T 3 is the CMSS, where the two sub-structures circled by dotted lines are included as the context to T 2 and make T 3 limited context-sensitive. This is to evaluate whether the limited context information in the CMSS is helpful. By comparing the performance of T 1 and T 3, we would like to see whether the larger context in T 1 is a noisy feature. T 4 is the CT, where only the basic structured information is kept. By comparing the performance of T 2 and T 4, we would like to study whether the high-level structured syntactic features in T 2 are useful to phrase reordering.
After defining the four structured feature spaces, we further partition each feature space into five parts according to their functionalities. Because it only makes sense to evaluate two partitions of the same functionality between two reordering instances, the feature space partition leads to a more precise similarity calculation. As shown in Fig 1, all the internal nodes in each partition are labeled with a unique function tag in the following way:
&#8226; Left Context (-lc): nodes in this partition do not cover any phrase word and they are all in the left of the left phrase.
&#8226; Right Context (-rc): nodes in this partition do not cover any phrase word and they are all in the right of the right phrase.
&#8226; Left Phrase (-lp): nodes in this partition only cover the first phrase and/or its left context.
&#8226; Right Phrase (-rp): nodes in this partition only cover the second phrase and/or its right context.
&#8226; Shared Part (-sp): nodes in this partition at least cover both of the two phrases partially.
No lexical word is tagged since it is not a part of the structured features, and therefore not participating in the tree kernel computing.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>A reordering instance &#65533;&#65533;&#65533;&#65533; &#65533; ,&#65533; &#65533; &#65533; (see Eq.1) in this paper refers to two adjacent source phrases &#65533; &#65533; and &#65533; &#65533; to be translated.</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The structured syntactic feature spaces of a reordering instance are defined as the portion of a parse tree of the source sentence that at least covers the span of the reordering instance (i.e. the two neighboring phrases).</text>
                  <doc_id>55</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The syntactic features are defined as all</text>
                  <doc_id>56</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>T 1 ) Minimum Sub-Tree (MST)</text>
                  <doc_id>57</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>T 2 ) Minimum Sub-Structure (MSS) T 4 ) Chunking Tree (CT)</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>T 3 ) Context-sensitive Minimum Sub-Structure (CMSS)</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 1.</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Different representations of structured syntactic features of a reordering instance in the example sentence excerpted from our training corpus &#8220;&#8230; &#24314; &#31435; /build &#35268; &#27169; /scale &#23439; &#22823; /mighty &#30340; /of &#21508; &#31867; /various types &#20154; &#25165; /qualified personnel &#38431; &#20237; /contingent &#39318; &#20808; /above all &#36843; &#20999; /urgently &#38656; &#35201; /necessary &#20013; &#22830; /central authorities &#32479; &#31609; /overall &#35268; &#21010; /planning&#8230;(To build a mighty contingent of qualified personnel of various types, it is necessary, above all, for the central authorities to make overall planning.</text>
                  <doc_id>61</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>) &#8221;, where &#8220; &#21508; &#31867; /various types &#20154; &#25165; /qualified personnel &#38431; &#20237; /contingent (contingent of qualified personnel of various types)&#8221; is the 1 st /left phrase and &#8220; &#39318; &#20808; /above all &#36843; &#20999; /urgent &#38656; &#35201; /necessary (it is necessary, above all, &#8230;)&#8221; is the 2 nd /right phrase.</text>
                  <doc_id>62</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Note that different function tags are attached to the grammar tag of each internal node.</text>
                  <doc_id>63</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the possible subtrees in the structured feature spaces.</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We can see that the structured feature spaces and their features are encapsulated by a full parse tree of source sentences.</text>
                  <doc_id>65</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Thus, it is critical to understand which portion of a parse tree (i.e. structured feature space) is the most effective to represent a reordering instance.</text>
                  <doc_id>66</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Motivated by the work of (Zhang et al., 2006), we here examine four cases that contain different sub-structures as shown in Fig.</text>
                  <doc_id>67</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>1.</text>
                  <doc_id>68</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(1) Minimum Sub-Tree (MST): the sub-tree rooted by the nearest common ancestor of the two phrases.</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This feature records the minimum sub-structure covering the two phrases and its left and right contexts as shown in Fig 1.T 1 .</text>
                  <doc_id>70</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>(2) Minimum Sub-Structure (MSS): the smallest common sub-structure covering the two phrases.</text>
                  <doc_id>71</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>It is enclosed by the shortest path linking the two phrases.</text>
                  <doc_id>72</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Thus, its leaf nodes exactly consist of all the phrasal words.</text>
                  <doc_id>73</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>(3) Context-sensitive Minimum Sub-Structure (CMSS): the MSS extending with the 1 st left sibling node of the left phrase and the 1 st right sibling node of the right phrase and their descendants.</text>
                  <doc_id>74</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>If sibling is unavailable, then we move to the parent of current node and repeat the same process until the sibling is available or the root of the MST is reached.</text>
                  <doc_id>75</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>(4) Chunking Tree (CT): the base phrase list extracted from the MSS.</text>
                  <doc_id>76</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>We prune out all the internal structures of the MSS and only keep the root node and the base phrase list for generating the chunking tree.</text>
                  <doc_id>77</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Fig.</text>
                  <doc_id>78</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>1 illustrates the different representations of an example reordering instance.</text>
                  <doc_id>79</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>T 1 is the MST for the example instance, where the sub-structure circled by a dotted line is the MSS, which is also shown in T 2 for clarity.</text>
                  <doc_id>80</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We can see that the MSS is a subset of the MST.</text>
                  <doc_id>81</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>By T 2 we would like to evaluate whether the structured information is effective for phrase reordering while by comparing the system performance when using T 1 and T 2, we would like to evaluate whether the structured context information embedded in the MST is useful to phrase reordering.</text>
                  <doc_id>82</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>T 3 is the CMSS, where the two sub-structures circled by dotted lines are included as the context to T 2 and make T 3 limited context-sensitive.</text>
                  <doc_id>83</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>This is to evaluate whether the limited context information in the CMSS is helpful.</text>
                  <doc_id>84</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>By comparing the performance of T 1 and T 3, we would like to see whether the larger context in T 1 is a noisy feature.</text>
                  <doc_id>85</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>T 4 is the CT, where only the basic structured information is kept.</text>
                  <doc_id>86</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>By comparing the performance of T 2 and T 4, we would like to study whether the high-level structured syntactic features in T 2 are useful to phrase reordering.</text>
                  <doc_id>87</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>After defining the four structured feature spaces, we further partition each feature space into five parts according to their functionalities.</text>
                  <doc_id>88</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Because it only makes sense to evaluate two partitions of the same functionality between two reordering instances, the feature space partition leads to a more precise similarity calculation.</text>
                  <doc_id>89</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As shown in Fig 1, all the internal nodes in each partition are labeled with a unique function tag in the following way:</text>
                  <doc_id>90</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Left Context (-lc): nodes in this partition do not cover any phrase word and they are all in the left of the left phrase.</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Right Context (-rc): nodes in this partition do not cover any phrase word and they are all in the right of the right phrase.</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Left Phrase (-lp): nodes in this partition only cover the first phrase and/or its left context.</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Right Phrase (-rp): nodes in this partition only cover the second phrase and/or its right context.</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Shared Part (-sp): nodes in this partition at least cover both of the two phrases partially.</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>No lexical word is tagged since it is not a part of the structured features, and therefore not participating in the tree kernel computing.</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Linear Features</title>
            <text>In our study, we define the following lexicalized linear features which are easily to be extracted and integrated to our composite kernel:
&#8226; Leftmost and rightmost boundary words of the left and right source phrases
&#8226; Leftmost and rightmost boundary words of the left and right target phrases
&#8226; Internal words of the four phrases (excluding boundary words)
&#8226; Target language model (LM) score difference (monotone-inverted)
In total, we arrive at 13 features, including 8 boundary word features, 4 (kinds of) internal word features and 1 LM feature. The first 12 features have been proven useful (Xiong et al., 2006; Zhang et al., 2007a) to phrase reordering. LM score is certainly a strong evidence for modeling word orders and lexical selection. Although it is already used as a standalone feature in the log-linear model, we also would like to explicitly re-optimize it together with other reordering features in our reordering model.
3.4 Tree Kernel, Composite Kernel and Integrating into our Reordering Model
As discussed before, we use convolution tree kernel to capture the structured syntactic feature implicitly by directly computing similarity between the parse-tree representations of two reordering instances with explicitly enumerating all the features one by one. In convolution tree kernel (Collins and Duffy, 2001), a parse tree T is implicitly represented by a vector of integer counts of each sub-tree type (regardless of its ancestors):
&#966; ( T ) = (# subtree 1 (T), &#8230;, # subtree n (T))
where # subtree i (T) is the occurrence number of the i th sub-tree type (subtree i ) in T. Since the number of different sub-trees is exponential with the parse tree size, it is computationally infeasible to directly use the feature vector &#966; ( T ) . To solve this computational issue, Collins and Duffy (2001) proposed the following parse tree kernel to calculate the dot product between the above high dimensional vectors implicitly. KT (
1, T2) =&lt; &#966;( T1), &#966;( T2) &gt;
= I ( n ) I ( n )
&#8721; ((
) (
))
i &#8721; subtree
&#8901;
n1&#8712;N1 i
&#8721;n subtree
2&#8712;N2 i
&#8721; &#8721; 1 2
= &#916;( n , n )
n1&#8712;N1 n2&#8712;N2
where N 1 and N 2 are the sets of nodes in trees T 1 and T 2 , respectively, and I
subtree i
( n ) is a function
that is 1 iff the subtree i occurs with root at node n and zero otherwise, and &#916; ( n1, n2) is the number of the common subtrees rooted at n 1 and n 2 , i.e.,
&#916; ( n1, n2) = &#8721; Isubtree ( n1) &#8901; I (
2) i i subtree
n
i
&#916;( n1, n2) can be further computed efficiently by the following recursive rules: Rule 1: if the productions (CFG rules) at n 1 and
n
are different, &#916; ( n1, n2) = 0; Rule 2: else if both n 1 and n
are pre-terminals
(POS tags), &#916; ( n1, n2) = 1&#215; &#955; ; Rule 3: else,
( 1 )
&#916; ( n 1, n 2) = &#955;&#8719; nc n (1 + &#916;( ch ( n 1 1, j ), ch ( n 2, j ))), j = where nc( n1 ) is the child number of n
, ch(n,j) is
the j th child of node n and &#955; (0&lt; &#955; &lt;1) is the decay factor in order to make the kernel value less variable with respect to the subtree sizes. In addition, the recursive Rule 3 holds because given two nodes with the same children, one can construct common sub-trees using these children and common sub-trees of further offspring. The time
complexity for computing this kernel is O(| N1| &#8901;| N2 |) and in practice in near to linear computational time without the need of enumerating all subtree features.
In our study, the linear feature-based similarity is simply calculated using dot-product. We then define the following composite kernel to combine the structured features-based and the linear features-based similarities:
&#65533; &#65533; (&#65533; &#65533; ,&#65533; &#65533; ) &#65533;&#65533;&#183;&#65533; &#65533; (&#65533; &#65533; ,&#65533; &#65533; ) &#65533; (1 &#65533;&#65533;) &#183;&#65533; &#65533; (&#65533; &#65533; ,&#65533; &#65533; ) (3)
where K t is the tree kernel over the structured features and K l is the linear kernel (dot-product) over the linear features. The composite kernel K c is a linear combination of the two individual kernels, where the coefficient &#945; is set to its default value 0.3 as that in Moschitti (2004)&#8217;s implementation. The kernels return the similarities between two reordering instances based on their features used. Our basic assumption is, the more similar the two reordering instances of x 1 and x 2 are, the more chance they share the same order.
Now let us see how to integrate the kernel functions into SVM. The linear classifier learned by SVM is formulized as:
f ( x) = sgn( &#8721; ya
i ix&#8226; xi + b) (4) i
where ai is the weight of a support vector x
i
(i.e.,
a support reordering instance &#65533; &#65533; &#65533;&#65533;&#65533; &#65533; ,&#65533; &#65533; &#65533;in our study), y
i
is its class label (1: &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; or -
1: &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; in our study) and b is the intercept of the hyperplane. An input reordering instance x is classified as positive (negative) if f ( x ) &gt;0 ( f ( x ) &lt;0). Based on the linear classifier, a kernelized SVM can be easily implemented by simply replacing the dot product x &#8727; xi in Eq (4) with a
kernel function Kxx ( , i ) . Thus, the kernelized SVM classifier is formulated as:
f ( x) = sgn( &#8721; yaK
i i
( x, xi) + b) (5)
i
where Kxx ( , i ) is either Kc( x, x
i) , Kt( x, xi) or
K ( x, x ) in our study. Following Eq (1), our
l i
reordering model (implemented by the kernelized SVM) can be formulized as follows:
&#937;&#65533;f(&#65533;, &#65533; &#65533; ,&#65533; &#65533; ) &#65533;&#65533; &#65533;&#65533;&#65533; (&#65533;|&#65533; &#65533;&#65533;&#65533; &#65533; ,&#65533; &#65533; &#65533;) &#65533;&#65533;&#65533;&#65533;(&#8721; &#65533; (&#65533; &#65533; &#65533; &#65533; &#65533;(&#65533;, &#65533; &#65533; ) &#65533;&#65533;) ) (6)
A reordering instance x is classified as straight (or inverted) if &#65533; &#65533;&#65533;&#65533; (&#65533;|&#65533;) &#65533;0 (or &#65533; &#65533;&#65533;&#65533; (&#65533;|&#65533;) &#65533; 0). Eq (6) and Eq (2) show the difference between our kernalized SVM-based reordering
model and the MaxEnt-based reordering model. The main difference between them lies in that our model is able to utilize structured syntactic features by kernalized SVM while the previous work can only use lexicalized word features by MaxEnt-based classifier.
Finally, because the return value of &#65533; &#65533;&#65533;&#65533; (&#65533;|&#65533;) is a distance function rather than a probability, we use a sigmoid function to convert &#65533; &#65533;&#65533;&#65533; (&#65533;|&#65533;) to a posterior probability as shown using the following to functions and apply it as one feature to the log-linear model in the decoding. 1
P( straight | x) = and +
psvm ( o| x) 1 e &#8722;
1 Pinverted ( | x) =
psvm ( o| x) 1 + e
where straight represents a positive instance and inverted represents a negative instance.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In our study, we define the following lexicalized linear features which are easily to be extracted and integrated to our composite kernel:</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Leftmost and rightmost boundary words of the left and right source phrases</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Leftmost and rightmost boundary words of the left and right target phrases</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Internal words of the four phrases (excluding boundary words)</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Target language model (LM) score difference (monotone-inverted)</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In total, we arrive at 13 features, including 8 boundary word features, 4 (kinds of) internal word features and 1 LM feature.</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The first 12 features have been proven useful (Xiong et al., 2006; Zhang et al., 2007a) to phrase reordering.</text>
                  <doc_id>103</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>LM score is certainly a strong evidence for modeling word orders and lexical selection.</text>
                  <doc_id>104</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Although it is already used as a standalone feature in the log-linear model, we also would like to explicitly re-optimize it together with other reordering features in our reordering model.</text>
                  <doc_id>105</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3.4 Tree Kernel, Composite Kernel and Integrating into our Reordering Model</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As discussed before, we use convolution tree kernel to capture the structured syntactic feature implicitly by directly computing similarity between the parse-tree representations of two reordering instances with explicitly enumerating all the features one by one.</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In convolution tree kernel (Collins and Duffy, 2001), a parse tree T is implicitly represented by a vector of integer counts of each sub-tree type (regardless of its ancestors):</text>
                  <doc_id>108</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#966; ( T ) = (# subtree 1 (T), &#8230;, # subtree n (T))</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where # subtree i (T) is the occurrence number of the i th sub-tree type (subtree i ) in T.</text>
                  <doc_id>110</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Since the number of different sub-trees is exponential with the parse tree size, it is computationally infeasible to directly use the feature vector &#966; ( T ) .</text>
                  <doc_id>111</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>To solve this computational issue, Collins and Duffy (2001) proposed the following parse tree kernel to calculate the dot product between the above high dimensional vectors implicitly.</text>
                  <doc_id>112</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>KT (</text>
                  <doc_id>113</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1, T2) =&lt; &#966;( T1), &#966;( T2) &gt;</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>= I ( n ) I ( n )</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; ((</text>
                  <doc_id>116</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>) (</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>))</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i &#8721; subtree</text>
                  <doc_id>119</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8901;</text>
                  <doc_id>120</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n1&#8712;N1 i</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;n subtree</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2&#8712;N2 i</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; &#8721; 1 2</text>
                  <doc_id>124</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>= &#916;( n , n )</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n1&#8712;N1 n2&#8712;N2</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where N 1 and N 2 are the sets of nodes in trees T 1 and T 2 , respectively, and I</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>subtree i</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>( n ) is a function</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>that is 1 iff the subtree i occurs with root at node n and zero otherwise, and &#916; ( n1, n2) is the number of the common subtrees rooted at n 1 and n 2 , i.e.,</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#916; ( n1, n2) = &#8721; Isubtree ( n1) &#8901; I (</text>
                  <doc_id>131</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2) i i subtree</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i</text>
                  <doc_id>134</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#916;( n1, n2) can be further computed efficiently by the following recursive rules: Rule 1: if the productions (CFG rules) at n 1 and</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>are different, &#916; ( n1, n2) = 0; Rule 2: else if both n 1 and n</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>are pre-terminals</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(POS tags), &#916; ( n1, n2) = 1&#215; &#955; ; Rule 3: else,</text>
                  <doc_id>139</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>( 1 )</text>
                  <doc_id>140</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#916; ( n 1, n 2) = &#955;&#8719; nc n (1 + &#916;( ch ( n 1 1, j ), ch ( n 2, j ))), j = where nc( n1 ) is the child number of n</text>
                  <doc_id>141</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>, ch(n,j) is</text>
                  <doc_id>142</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the j th child of node n and &#955; (0&lt; &#955; &lt;1) is the decay factor in order to make the kernel value less variable with respect to the subtree sizes.</text>
                  <doc_id>143</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, the recursive Rule 3 holds because given two nodes with the same children, one can construct common sub-trees using these children and common sub-trees of further offspring.</text>
                  <doc_id>144</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The time</text>
                  <doc_id>145</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>complexity for computing this kernel is O(| N1| &#8901;| N2 |) and in practice in near to linear computational time without the need of enumerating all subtree features.</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In our study, the linear feature-based similarity is simply calculated using dot-product.</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We then define the following composite kernel to combine the structured features-based and the linear features-based similarities:</text>
                  <doc_id>148</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533; &#65533; (&#65533; &#65533; ,&#65533; &#65533; ) &#65533;&#65533;&#183;&#65533; &#65533; (&#65533; &#65533; ,&#65533; &#65533; ) &#65533; (1 &#65533;&#65533;) &#183;&#65533; &#65533; (&#65533; &#65533; ,&#65533; &#65533; ) (3)</text>
                  <doc_id>149</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where K t is the tree kernel over the structured features and K l is the linear kernel (dot-product) over the linear features.</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The composite kernel K c is a linear combination of the two individual kernels, where the coefficient &#945; is set to its default value 0.3 as that in Moschitti (2004)&#8217;s implementation.</text>
                  <doc_id>151</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The kernels return the similarities between two reordering instances based on their features used.</text>
                  <doc_id>152</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Our basic assumption is, the more similar the two reordering instances of x 1 and x 2 are, the more chance they share the same order.</text>
                  <doc_id>153</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Now let us see how to integrate the kernel functions into SVM.</text>
                  <doc_id>154</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The linear classifier learned by SVM is formulized as:</text>
                  <doc_id>155</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f ( x) = sgn( &#8721; ya</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i ix&#8226; xi + b) (4) i</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where ai is the weight of a support vector x</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(i.e.,</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>a support reordering instance &#65533; &#65533; &#65533;&#65533;&#65533; &#65533; ,&#65533; &#65533; &#65533;in our study), y</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>is its class label (1: &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; or -</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1: &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; in our study) and b is the intercept of the hyperplane.</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>An input reordering instance x is classified as positive (negative) if f ( x ) &gt;0 ( f ( x ) &lt;0).</text>
                  <doc_id>165</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Based on the linear classifier, a kernelized SVM can be easily implemented by simply replacing the dot product x &#8727; xi in Eq (4) with a</text>
                  <doc_id>166</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>kernel function Kxx ( , i ) .</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Thus, the kernelized SVM classifier is formulated as:</text>
                  <doc_id>168</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f ( x) = sgn( &#8721; yaK</text>
                  <doc_id>169</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i i</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>( x, xi) + b) (5)</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i</text>
                  <doc_id>172</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where Kxx ( , i ) is either Kc( x, x</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i) , Kt( x, xi) or</text>
                  <doc_id>174</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>K ( x, x ) in our study.</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Following Eq (1), our</text>
                  <doc_id>176</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>l i</text>
                  <doc_id>177</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>reordering model (implemented by the kernelized SVM) can be formulized as follows:</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#937;&#65533;f(&#65533;, &#65533; &#65533; ,&#65533; &#65533; ) &#65533;&#65533; &#65533;&#65533;&#65533; (&#65533;|&#65533; &#65533;&#65533;&#65533; &#65533; ,&#65533; &#65533; &#65533;) &#65533;&#65533;&#65533;&#65533;(&#8721; &#65533; (&#65533; &#65533; &#65533; &#65533; &#65533;(&#65533;, &#65533; &#65533; ) &#65533;&#65533;) ) (6)</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A reordering instance x is classified as straight (or inverted) if &#65533; &#65533;&#65533;&#65533; (&#65533;|&#65533;) &#65533;0 (or &#65533; &#65533;&#65533;&#65533; (&#65533;|&#65533;) &#65533; 0).</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Eq (6) and Eq (2) show the difference between our kernalized SVM-based reordering</text>
                  <doc_id>181</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>model and the MaxEnt-based reordering model.</text>
                  <doc_id>182</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The main difference between them lies in that our model is able to utilize structured syntactic features by kernalized SVM while the previous work can only use lexicalized word features by MaxEnt-based classifier.</text>
                  <doc_id>183</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Finally, because the return value of &#65533; &#65533;&#65533;&#65533; (&#65533;|&#65533;) is a distance function rather than a probability, we use a sigmoid function to convert &#65533; &#65533;&#65533;&#65533; (&#65533;|&#65533;) to a posterior probability as shown using the following to functions and apply it as one feature to the log-linear model in the decoding.</text>
                  <doc_id>184</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>1</text>
                  <doc_id>185</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P( straight | x) = and +</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>psvm ( o| x) 1 e &#8722;</text>
                  <doc_id>187</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 Pinverted ( | x) =</text>
                  <doc_id>188</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>psvm ( o| x) 1 + e</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where straight represents a positive instance and inverted represents a negative instance.</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Experiments and Discussion</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>191</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Experimental Settings</title>
            <text>Basic Settings: we evaluate our method on Chinese-English translation task. We use the FBIS corpus as training set, the NIST MT-2002 test set as development (dev) set and the NIST MT-2005 test set as test set. The Stanford parser (Klein and Manning, 2003) is used to parse Chinese sentences on the training, dev and test sets. GIZA++ (Och and Ney, 2004) and the heuristics &#8220;growdiag-final-and&#8221; are used to generate m-to-n word alignments. The translation model is trained on the FBIS corpus and a 4-gram language model is trained on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kenser and Ney, 1995). For the MER training (Och, 2003), we modify Koehn&#8217;s MER trainer (Koehn, 2004) to train our system. For significance test, we use Zhang et al&#8217;s implementation (Zhang et al, 2004).
Baseline Systems: we set three baseline systems: B1) Moses (Koehn et al., 2007) that uses lexicalized unigram reordering model to predict three orientations: monotone, swap and discontinuous; B2) MaxEnt-based reordering model with lexical boundary word features only (Xiong et al., 2006); B3) Linguistically annotated reordering model for BTG-based (LABTG) SMT (Xiong et al., 2008). For Moses, we used the default settings. We build a CKY-style decoder and integrate the corresponding reordering modelling methods into the decoder to implement the 2 nd and the 3 rd baseline systems and our system. Except reordering models, all the four systems use the same features in translation model, language model and distortion model as Moses in the loglinear framework. We tune the four systems using the strategies as discussed previously in this section.
Reordering Model Training: we extract all reordering instances from the m-to-n wordaligned training corpus. The reordering instances include the two source phrases, two target phrases, order label and its corresponding parse tree. We generate the boundary word features from the extracted reordering instances in the same way as discussed in Xiong et al. (2006) and use Zhang&#8217;s MaxEnt Tools 2 to train a reordering model for the 2 nd baseline system. Similarly, we use the algorithm 1 in Xiong et al. (2008) to extract features and use the same MaxEnt Tools to train a reordering model for the 3 rd baseline system. Based on the extracted reordering instances, we generate the four structured features and the linear features, and then use the Tree Kernel Tools (Moschitti, 2004) to train our kernel-based reordering model (linear, tree and composite).
Experimental Design and Evaluation Metrics: we design three experiments and evaluate them using three metrics.
Classification-based: in the first experiment, we extract all reordering instances and their features from the dev and test sets, and then use the reordering models trained on the training set to classify (label) those instances extracted from the dev and test sets. In this way, we can isolate the reordering problem from the influence of others, such as translation model, pruning and decoding strategies, to better examine the reordering models&#8217; ability and to give analytical insights into the features. Classification Accuracy (CAcc), the percentage of the correctly labeled instances over all trials, is used as the evaluation metric.
Forced decoding 3 -based and normal decodingbased: the two experiments evaluate the reordering models through a real SMT system. The reordering model and the language model are the same in the two experiments. However, in forced decoding, we train two translation models, one using training data only while another using both
2 http://homepages.inf.ed.ac.uk/s0450736/maxent.html 3 A normal SMT decoder filters a translation model according to the source sentences, whereas in forced decoding, a translation model is filtered based on both source sentence and target references. In other words, in forced decoding, the decoder is forced to use those phrases whose translations are already in the references.
training, dev and test data. By forced decoding, we aim to isolate the reordering problem from those of OOV and lexical selections resulting from imperfect translation model in the context of a real SMT task. Besides the the case-sensitive BLEU-4 (Papineni et al., 2002) used in the two experiments, we design another evaluation metrics Reordering Accuracy (RAcc) for forced decoding evaluation. RAcc is the percentage of the adjacent word pairs with correct word order 4 over all words in one-best translation results. Similar to BLEU score, we also use the similar Brevity Penalty BP (Papineni et al., 2002) to penalize the short translations in computing RAcc. Finally, please note for the three evaluation metrics, the higher values represent better performance.
Feature Spaces CAcc (%) Dev Test Minimum Sub-Tree (MST) 89.87 89.92</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Basic Settings: we evaluate our method on Chinese-English translation task.</text>
                  <doc_id>192</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We use the FBIS corpus as training set, the NIST MT-2002 test set as development (dev) set and the NIST MT-2005 test set as test set.</text>
                  <doc_id>193</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The Stanford parser (Klein and Manning, 2003) is used to parse Chinese sentences on the training, dev and test sets.</text>
                  <doc_id>194</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>GIZA++ (Och and Ney, 2004) and the heuristics &#8220;growdiag-final-and&#8221; are used to generate m-to-n word alignments.</text>
                  <doc_id>195</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The translation model is trained on the FBIS corpus and a 4-gram language model is trained on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kenser and Ney, 1995).</text>
                  <doc_id>196</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For the MER training (Och, 2003), we modify Koehn&#8217;s MER trainer (Koehn, 2004) to train our system.</text>
                  <doc_id>197</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>For significance test, we use Zhang et al&#8217;s implementation (Zhang et al, 2004).</text>
                  <doc_id>198</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Baseline Systems: we set three baseline systems: B1) Moses (Koehn et al., 2007) that uses lexicalized unigram reordering model to predict three orientations: monotone, swap and discontinuous; B2) MaxEnt-based reordering model with lexical boundary word features only (Xiong et al., 2006); B3) Linguistically annotated reordering model for BTG-based (LABTG) SMT (Xiong et al., 2008).</text>
                  <doc_id>199</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For Moses, we used the default settings.</text>
                  <doc_id>200</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We build a CKY-style decoder and integrate the corresponding reordering modelling methods into the decoder to implement the 2 nd and the 3 rd baseline systems and our system.</text>
                  <doc_id>201</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Except reordering models, all the four systems use the same features in translation model, language model and distortion model as Moses in the loglinear framework.</text>
                  <doc_id>202</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We tune the four systems using the strategies as discussed previously in this section.</text>
                  <doc_id>203</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Reordering Model Training: we extract all reordering instances from the m-to-n wordaligned training corpus.</text>
                  <doc_id>204</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The reordering instances include the two source phrases, two target phrases, order label and its corresponding parse tree.</text>
                  <doc_id>205</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We generate the boundary word features from the extracted reordering instances in the same way as discussed in Xiong et al. (2006) and use Zhang&#8217;s MaxEnt Tools 2 to train a reordering model for the 2 nd baseline system.</text>
                  <doc_id>206</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Similarly, we use the algorithm 1 in Xiong et al. (2008) to extract features and use the same MaxEnt Tools to train a reordering model for the 3 rd baseline system.</text>
                  <doc_id>207</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Based on the extracted reordering instances, we generate the four structured features and the linear features, and then use the Tree Kernel Tools (Moschitti, 2004) to train our kernel-based reordering model (linear, tree and composite).</text>
                  <doc_id>208</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Experimental Design and Evaluation Metrics: we design three experiments and evaluate them using three metrics.</text>
                  <doc_id>209</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Classification-based: in the first experiment, we extract all reordering instances and their features from the dev and test sets, and then use the reordering models trained on the training set to classify (label) those instances extracted from the dev and test sets.</text>
                  <doc_id>210</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this way, we can isolate the reordering problem from the influence of others, such as translation model, pruning and decoding strategies, to better examine the reordering models&#8217; ability and to give analytical insights into the features.</text>
                  <doc_id>211</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Classification Accuracy (CAcc), the percentage of the correctly labeled instances over all trials, is used as the evaluation metric.</text>
                  <doc_id>212</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Forced decoding 3 -based and normal decodingbased: the two experiments evaluate the reordering models through a real SMT system.</text>
                  <doc_id>213</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The reordering model and the language model are the same in the two experiments.</text>
                  <doc_id>214</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, in forced decoding, we train two translation models, one using training data only while another using both</text>
                  <doc_id>215</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 http://homepages.inf.ed.ac.uk/s0450736/maxent.html 3 A normal SMT decoder filters a translation model according to the source sentences, whereas in forced decoding, a translation model is filtered based on both source sentence and target references.</text>
                  <doc_id>216</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In other words, in forced decoding, the decoder is forced to use those phrases whose translations are already in the references.</text>
                  <doc_id>217</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>training, dev and test data.</text>
                  <doc_id>218</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>By forced decoding, we aim to isolate the reordering problem from those of OOV and lexical selections resulting from imperfect translation model in the context of a real SMT task.</text>
                  <doc_id>219</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Besides the the case-sensitive BLEU-4 (Papineni et al., 2002) used in the two experiments, we design another evaluation metrics Reordering Accuracy (RAcc) for forced decoding evaluation.</text>
                  <doc_id>220</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>RAcc is the percentage of the adjacent word pairs with correct word order 4 over all words in one-best translation results.</text>
                  <doc_id>221</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Similar to BLEU score, we also use the similar Brevity Penalty BP (Papineni et al., 2002) to penalize the short translations in computing RAcc.</text>
                  <doc_id>222</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, please note for the three evaluation metrics, the higher values represent better performance.</text>
                  <doc_id>223</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Feature Spaces CAcc (%) Dev Test Minimum Sub-Tree (MST) 89.87 89.92</text>
                  <doc_id>224</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Experimental Results</title>
            <text>Classification of Instances: Table 1 reports the performance of our defined four structured features, linear feature and the composite kernel. The results are summarized as follows.
The last row reports the performance without using any reordering features. We just suppose that all the translations are monotonic, no reordering happens. The CAccs of 78.92% and 78.67% serve as the bottom line in our study. Compared with the bottom line, the tree kernels over the 4 structured features are very effective for phrase
4 An adjacent word pair w i w i+1 in a translation have correct
word order if and only if w i appears before w i+1 in translation references. Note than the two words may not be adjacent in the references even if they have correct word order.
reordering since only structured information is used in the tree kernel 5 .
The CTs performs the worst among the 4 structured features. This suggests that the middle and high-level structures beyond base phrases are very useful for phrase reordering. The MSSs show lower performance than the CMSSs and the MSTs achieve the best performance. This clearly indicates that the structured context information is useful for phrase reordering. For this reason, the subsequent discussions are focused on the MSTs, unless otherwise specified. The MSSs without using the 5 function tags perform much worse than the original ones. This suggests that the partitions of the structured feature spaces are very helpful, which can effectively avoid the undesired matching between partitions of different functionalities. Comparison of K l and K l-LM shows the LM plays an important role in phrase reordering. The composite kernel (K c ) performs much better than the two individual kernels. This suggests that the structured and linear features are complementary and the composite kernel can well integrate them for phrase reordering.
Methods CAcc (%) Dev Test Minimum Sub-Tree (MST) 89.87 89.92 Linear Features (K l ) 90.79 90.46 Composite Kernel (K c : MST+K l ) 92.98 92.67 MaxEnt+boundary word (B2) 88.33 86.97 MaxEnt+linguistic features (B3_1) 84.83 83.92 MaxEnt+LABTG (B3: B2+ B3_1) 88.82 88.18
Table 2 compares the performance of the baseline methods with ours. Comparison between B3_1 and MST clearly demonstrates that the structured syntactic features are much more effective than the linear syntactic features that are manually extracted via heuristics. It also suggests that the tree kernel can well capture the structured features implicitly. K l outperforms B2. This is mainly due to the contribution of LM features. B2 (MaxEnt-based) significantly outperforms K l-
LM in Table 1 (SVM-based). This suggests that
phrase reordering may not be a good linearly binary-separable task if only boundary word features are used. Our composite kernel (K c ) significantly outperforms LABTG (B3). This mainly
5 The tree kernel algorithm only compares internal structures. It does not concern any lexical leaf nodes.
attributes to the contributions of structured syntactic features, LM and the tree kernel.
Forced Decoding: Table 3 compares the performance of our composite kernel with that of the LABTG (Baseline 3) in forced decoding. As discussed before, here we try two translation models.
The composite kernel outperforms the LABTG in all test cases. This further validates the effectiveness of the kernel methods in phrase reordering. There are still around 30% words reordered incorrectly even if we use the translation model trained on both training, dev and test sets. This reveals the limitations of current SMT modeling methods and suggests interesting future work in this area. The source language OOV 6 rate in forced decoding (13.6%) is much higher that in normal decoding (6.22%, see table 4). This is mainly due to the fact that the phrase table in forced decoding is filtered out based on both source and target languages while in normal decoding it is based on source language only. As a result, more phrases are filtered out in the forced decoding. There is 1.4% OOV even if the translation model is trained on the test set. This is due to the incorrect word alignment, large-span word alignment and different English tokenization strategies used in BLEU-scoring tool and ours.
MaxEnt+LABTG (B3) +translation model on training, dev and test 48.96 71.45 13.6 1.41 37.32 62.14
Methods Test Set BLEU(%) OOV(%) Composite Kernel (K c ) 27.65 6.26 Moses (B1) 25.71 6.17
MaxEnt+boundary word(B2) 25.99 6.22
MaxEnt+LABTG (B3) 26.63 6.22
6 OOV means a source words has no any English translation
according to the translation model. OOV rate is the percentage of the number of OOV words over all the source words.
Normal Decoding/Translation: Table 4 reports the translation performance of our system and the three baseline systems.
Moses (B1) and the MaxEnt-based boundary word model (B2) achieve comparable performance. This means the lexicalized orientationbased reordering model in Moses performs similarly to the boundary word-based reordering model since the two models are both lexical word-based. However, theoretically, the Max- Ent-based model may suffer less from data sparseness issue since it does not depends on internal phrasal words and uses MaxEnt to optimize feature weights while the orientationbased model uses relative frequency of the entire phrases to compute the posterior probabilities. s. The MaxEnt-based LABTG model significantly outperforms (p&lt;0.05) the MaxEnt-based boundary word model and the lexicalized orientationbased reordering model. This indicates that the linearly linguistically syntactic information is a useful feature to phrase reordering.
Our composite kernel-based model significantly outperforms (p&lt;0.01) the three baseline methods. This again proves that the structured syntactic features are much more effective than the linear syntactic features for phrase reordering and the tree kernel method can well capture the informative structured features. The four methods show very slight difference in OOV rates. This is mainly due to the difference in implementation detail, such as different OOV penalties and other pruning thresholds.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Classification of Instances: Table 1 reports the performance of our defined four structured features, linear feature and the composite kernel.</text>
                  <doc_id>225</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The results are summarized as follows.</text>
                  <doc_id>226</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The last row reports the performance without using any reordering features.</text>
                  <doc_id>227</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We just suppose that all the translations are monotonic, no reordering happens.</text>
                  <doc_id>228</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The CAccs of 78.92% and 78.67% serve as the bottom line in our study.</text>
                  <doc_id>229</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Compared with the bottom line, the tree kernels over the 4 structured features are very effective for phrase</text>
                  <doc_id>230</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4 An adjacent word pair w i w i+1 in a translation have correct</text>
                  <doc_id>231</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>word order if and only if w i appears before w i+1 in translation references.</text>
                  <doc_id>232</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Note than the two words may not be adjacent in the references even if they have correct word order.</text>
                  <doc_id>233</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>reordering since only structured information is used in the tree kernel 5 .</text>
                  <doc_id>234</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The CTs performs the worst among the 4 structured features.</text>
                  <doc_id>235</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This suggests that the middle and high-level structures beyond base phrases are very useful for phrase reordering.</text>
                  <doc_id>236</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The MSSs show lower performance than the CMSSs and the MSTs achieve the best performance.</text>
                  <doc_id>237</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This clearly indicates that the structured context information is useful for phrase reordering.</text>
                  <doc_id>238</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For this reason, the subsequent discussions are focused on the MSTs, unless otherwise specified.</text>
                  <doc_id>239</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The MSSs without using the 5 function tags perform much worse than the original ones.</text>
                  <doc_id>240</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>This suggests that the partitions of the structured feature spaces are very helpful, which can effectively avoid the undesired matching between partitions of different functionalities.</text>
                  <doc_id>241</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Comparison of K l and K l-LM shows the LM plays an important role in phrase reordering.</text>
                  <doc_id>242</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>The composite kernel (K c ) performs much better than the two individual kernels.</text>
                  <doc_id>243</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>This suggests that the structured and linear features are complementary and the composite kernel can well integrate them for phrase reordering.</text>
                  <doc_id>244</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Methods CAcc (%) Dev Test Minimum Sub-Tree (MST) 89.87 89.92 Linear Features (K l ) 90.79 90.46 Composite Kernel (K c : MST+K l ) 92.98 92.67 MaxEnt+boundary word (B2) 88.33 86.97 MaxEnt+linguistic features (B3_1) 84.83 83.92 MaxEnt+LABTG (B3: B2+ B3_1) 88.82 88.18</text>
                  <doc_id>245</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 2 compares the performance of the baseline methods with ours.</text>
                  <doc_id>246</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Comparison between B3_1 and MST clearly demonstrates that the structured syntactic features are much more effective than the linear syntactic features that are manually extracted via heuristics.</text>
                  <doc_id>247</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It also suggests that the tree kernel can well capture the structured features implicitly.</text>
                  <doc_id>248</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>K l outperforms B2.</text>
                  <doc_id>249</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This is mainly due to the contribution of LM features.</text>
                  <doc_id>250</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>B2 (MaxEnt-based) significantly outperforms K l-</text>
                  <doc_id>251</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>LM in Table 1 (SVM-based).</text>
                  <doc_id>252</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This suggests that</text>
                  <doc_id>253</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>phrase reordering may not be a good linearly binary-separable task if only boundary word features are used.</text>
                  <doc_id>254</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Our composite kernel (K c ) significantly outperforms LABTG (B3).</text>
                  <doc_id>255</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This mainly</text>
                  <doc_id>256</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5 The tree kernel algorithm only compares internal structures.</text>
                  <doc_id>257</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It does not concern any lexical leaf nodes.</text>
                  <doc_id>258</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>attributes to the contributions of structured syntactic features, LM and the tree kernel.</text>
                  <doc_id>259</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Forced Decoding: Table 3 compares the performance of our composite kernel with that of the LABTG (Baseline 3) in forced decoding.</text>
                  <doc_id>260</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As discussed before, here we try two translation models.</text>
                  <doc_id>261</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The composite kernel outperforms the LABTG in all test cases.</text>
                  <doc_id>262</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This further validates the effectiveness of the kernel methods in phrase reordering.</text>
                  <doc_id>263</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>There are still around 30% words reordered incorrectly even if we use the translation model trained on both training, dev and test sets.</text>
                  <doc_id>264</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This reveals the limitations of current SMT modeling methods and suggests interesting future work in this area.</text>
                  <doc_id>265</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The source language OOV 6 rate in forced decoding (13.6%) is much higher that in normal decoding (6.22%, see table 4).</text>
                  <doc_id>266</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This is mainly due to the fact that the phrase table in forced decoding is filtered out based on both source and target languages while in normal decoding it is based on source language only.</text>
                  <doc_id>267</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>As a result, more phrases are filtered out in the forced decoding.</text>
                  <doc_id>268</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>There is 1.4% OOV even if the translation model is trained on the test set.</text>
                  <doc_id>269</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>This is due to the incorrect word alignment, large-span word alignment and different English tokenization strategies used in BLEU-scoring tool and ours.</text>
                  <doc_id>270</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>MaxEnt+LABTG (B3) +translation model on training, dev and test 48.96 71.45 13.6 1.41 37.32 62.14</text>
                  <doc_id>271</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Methods Test Set BLEU(%) OOV(%) Composite Kernel (K c ) 27.65 6.26 Moses (B1) 25.71 6.17</text>
                  <doc_id>272</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>MaxEnt+boundary word(B2) 25.99 6.22</text>
                  <doc_id>273</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>MaxEnt+LABTG (B3) 26.63 6.22</text>
                  <doc_id>274</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>6 OOV means a source words has no any English translation</text>
                  <doc_id>275</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>according to the translation model.</text>
                  <doc_id>276</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>OOV rate is the percentage of the number of OOV words over all the source words.</text>
                  <doc_id>277</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Normal Decoding/Translation: Table 4 reports the translation performance of our system and the three baseline systems.</text>
                  <doc_id>278</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Moses (B1) and the MaxEnt-based boundary word model (B2) achieve comparable performance.</text>
                  <doc_id>279</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This means the lexicalized orientationbased reordering model in Moses performs similarly to the boundary word-based reordering model since the two models are both lexical word-based.</text>
                  <doc_id>280</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, theoretically, the Max- Ent-based model may suffer less from data sparseness issue since it does not depends on internal phrasal words and uses MaxEnt to optimize feature weights while the orientationbased model uses relative frequency of the entire phrases to compute the posterior probabilities.</text>
                  <doc_id>281</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>s.</text>
                  <doc_id>282</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The MaxEnt-based LABTG model significantly outperforms (p&lt;0.05) the MaxEnt-based boundary word model and the lexicalized orientationbased reordering model.</text>
                  <doc_id>283</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This indicates that the linearly linguistically syntactic information is a useful feature to phrase reordering.</text>
                  <doc_id>284</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Our composite kernel-based model significantly outperforms (p&lt;0.01) the three baseline methods.</text>
                  <doc_id>285</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This again proves that the structured syntactic features are much more effective than the linear syntactic features for phrase reordering and the tree kernel method can well capture the informative structured features.</text>
                  <doc_id>286</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The four methods show very slight difference in OOV rates.</text>
                  <doc_id>287</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This is mainly due to the difference in implementation detail, such as different OOV penalties and other pruning thresholds.</text>
                  <doc_id>288</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Conclusion and Future Work</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>289</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>6 OOV means a source words has no any English translation</title>
        <text>according to the translation model. OOV rate is the percentage of the number of OOV words over all the source words. Normal Decoding/Translation: Table 4 reports the translation performance of our system and the three baseline systems. Moses (B1) and the MaxEnt-based boundary word model (B2) achieve comparable performance. This means the lexicalized orientationbased reordering model in Moses performs similarly to the boundary word-based reordering model since the two models are both lexical word-based. However, theoretically, the Max- Ent-based model may suffer less from data sparseness issue since it does not depends on internal phrasal words and uses MaxEnt to optimize feature weights while the orientationbased model uses relative frequency of the entire phrases to compute the posterior probabilities. s. The MaxEnt-based LABTG model significantly outperforms (p&amp;lt;0.05) the MaxEnt-based boundary word model and the lexicalized orientationbased reordering model. This indicates that the linearly linguistically syntactic information is a useful feature to phrase reordering. Our composite kernel-based model significantly outperforms (p&amp;lt;0.01) the three baseline methods. This again proves that the structured syntactic features are much more effective than the linear syntactic features for phrase reordering and the tree kernel method can well capture the informative structured features. The four methods show very slight difference in OOV rates. This is mainly due to the difference in implementation detail, such as different OOV penalties and other pruning thresholds.
Structured syntactic knowledge is very useful to phrase reordering. This paper provides insights into how the structured feature can be used for phrase reordering. In previous work, the structured features are selected manually by heuristics and represented by a linear feature vector. This may largely compromise the contribution of the structured features to phrase reordering. Thanks to the nice properties of kernel-based learning method and SVM classifier, we propose leveraging on the kernelized SVM learning algorithm to address the problem. Specifically, we propose using convolution tree kernel to capture the structured features and design a composite kernel to combine the structured features and other linear features for phrase reordering. The tree kernel is able to directly take the structured reordering instances as inputs and compute their similarities without enumerating them into a set of liner 705 features. In addition, we also study how to find the optimal structured feature space and how to partition the structured feature spaces according to their functionalities. Finally, we evaluate our method on the NIST MT-2005 Chinese-English translation tasks. To provide insights into the model, we design three kinds of experiments together with three different evaluation metrics. Experimental results show that the structured features are very effective and our composite kernel can well capture both the structured and the linear features without the need for extensive feature engineering. It also shows that our method significantly outperforms the baseline methods. The tree kernel-based phrase reordering method is not only applicable to adjacent phrases. It is able to work with any long phrase pairs with gap of any length in-between. We will study this case in the near future. We would also like to use one individual tree kernel for one partition in a structured feature space. In doing so, we are able to give different weights to different partitions according to their functionalities and contributions. Note that these weights can be automatically tuned and optimized easily against a dev set.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>according to the translation model.</text>
              <doc_id>290</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>OOV rate is the percentage of the number of OOV words over all the source words.</text>
              <doc_id>291</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Normal Decoding/Translation: Table 4 reports the translation performance of our system and the three baseline systems.</text>
              <doc_id>292</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Moses (B1) and the MaxEnt-based boundary word model (B2) achieve comparable performance.</text>
              <doc_id>293</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This means the lexicalized orientationbased reordering model in Moses performs similarly to the boundary word-based reordering model since the two models are both lexical word-based.</text>
              <doc_id>294</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>However, theoretically, the Max- Ent-based model may suffer less from data sparseness issue since it does not depends on internal phrasal words and uses MaxEnt to optimize feature weights while the orientationbased model uses relative frequency of the entire phrases to compute the posterior probabilities.</text>
              <doc_id>295</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>s.</text>
              <doc_id>296</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The MaxEnt-based LABTG model significantly outperforms (p&amp;lt;0.05) the MaxEnt-based boundary word model and the lexicalized orientationbased reordering model.</text>
              <doc_id>297</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>This indicates that the linearly linguistically syntactic information is a useful feature to phrase reordering.</text>
              <doc_id>298</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Our composite kernel-based model significantly outperforms (p&amp;lt;0.01) the three baseline methods.</text>
              <doc_id>299</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>This again proves that the structured syntactic features are much more effective than the linear syntactic features for phrase reordering and the tree kernel method can well capture the informative structured features.</text>
              <doc_id>300</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>The four methods show very slight difference in OOV rates.</text>
              <doc_id>301</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>This is mainly due to the difference in implementation detail, such as different OOV penalties and other pruning thresholds.</text>
              <doc_id>302</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Structured syntactic knowledge is very useful to phrase reordering.</text>
              <doc_id>303</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This paper provides insights into how the structured feature can be used for phrase reordering.</text>
              <doc_id>304</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In previous work, the structured features are selected manually by heuristics and represented by a linear feature vector.</text>
              <doc_id>305</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This may largely compromise the contribution of the structured features to phrase reordering.</text>
              <doc_id>306</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Thanks to the nice properties of kernel-based learning method and SVM classifier, we propose leveraging on the kernelized SVM learning algorithm to address the problem.</text>
              <doc_id>307</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Specifically, we propose using convolution tree kernel to capture the structured features and design a composite kernel to combine the structured features and other linear features for phrase reordering.</text>
              <doc_id>308</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The tree kernel is able to directly take the structured reordering instances as inputs and compute their similarities without enumerating them into a set of liner 705 features.</text>
              <doc_id>309</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In addition, we also study how to find the optimal structured feature space and how to partition the structured feature spaces according to their functionalities.</text>
              <doc_id>310</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we evaluate our method on the NIST MT-2005 Chinese-English translation tasks.</text>
              <doc_id>311</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>To provide insights into the model, we design three kinds of experiments together with three different evaluation metrics.</text>
              <doc_id>312</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Experimental results show that the structured features are very effective and our composite kernel can well capture both the structured and the linear features without the need for extensive feature engineering.</text>
              <doc_id>313</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>It also shows that our method significantly outperforms the baseline methods.</text>
              <doc_id>314</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>The tree kernel-based phrase reordering method is not only applicable to adjacent phrases.</text>
              <doc_id>315</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>It is able to work with any long phrase pairs with gap of any length in-between.</text>
              <doc_id>316</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>We will study this case in the near future.</text>
              <doc_id>317</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>We would also like to use one individual tree kernel for one partition in a structured feature space.</text>
              <doc_id>318</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>In doing so, we are able to give different weights to different partitions according to their functionalities and contributions.</text>
              <doc_id>319</doc_id>
              <sec_id>16</sec_id>
            </sentence>
            <sentence>
              <text>Note that these weights can be automatically tuned and optimized easily against a dev set.</text>
              <doc_id>320</doc_id>
              <sec_id>17</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Performance of our methods on the dev and test sets with different feature combinations</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Minimum Sub-Structure (MSS)</cell>
              <cell>87.95 87.88</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Context-Sensitive MSS (CMSS)</cell>
              <cell>89.11 89.01</cell>
            </row>
            <row>
              <cell>Chunking Tree (CT)</cell>
              <cell>86.17 86.21</cell>
            </row>
            <row>
              <cell>Linear Features (K l )</cell>
              <cell>90.79 90.46</cell>
            </row>
            <row>
              <cell>K l w/o using LM feature (K l-LM )</cell>
              <cell>84.24 84.06</cell>
            </row>
            <row>
              <cell>Composite Kernel (K c : MST+K l ) 92.98 92.67</cell>
            </row>
            <row>
              <cell>MST w/o the 5 function tags</cell>
              <cell>86.94 87.03</cell>
            </row>
            <row>
              <cell>All are straight (monotonic)</cell>
              <cell>78.92 78.67</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TET</source>
        <caption>Table 3: Performance comparison of forced decoding</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>Methods</cell>
              <cell>Test Set (%)</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>RAcc OOV</cell>
              <cell>BLEU</cell>
            </row>
            <row>
              <cell>Composite Kernel (K c )
+translation model on
Training, dev and test</cell>
              <cell>51.03
72.67</cell>
              <cell>13.6
1.41</cell>
              <cell>38.56
62.87</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>A hierarchical phrase-based model for SMT.</title>
        <publication>ACL-05.</publication>
        <pages>263--270</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Michael Collins</author>
          <author>N Duffy</author>
        </authors>
        <title>Convolution Kernels for Natural Language.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>M R Costa-juss&#224;</author>
          <author>J A R Fonollosa</author>
        </authors>
        <title>None</title>
        <publication>Statistical Machine Reordering. EMNLP-06.</publication>
        <pages>70--76</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Brooke Cowan</author>
        </authors>
        <title>Ivona Kucerova and Michael Collins.</title>
        <publication>None</publication>
        <pages>06--232</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Jason Eisner</author>
        </authors>
        <title>Learning non-isomorphic tree mappings for MT.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Jakob Elming</author>
        </authors>
        <title>None</title>
        <publication>Syntactic Reordering Integrated with Phrase-Based SMT. COLING-08.</publication>
        <pages>209--216</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Michel Galley</author>
          <author>Christopher D Manning</author>
        </authors>
        <title>A Simple and Effective Hierarchical Phrase Reordering Model.</title>
        <publication>None</publication>
        <pages>848--856</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>David Haussler</author>
        </authors>
        <title>Convolution Kernels on Discrete Structures.</title>
        <publication>None</publication>
        <pages>99--10</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>T Joachims</author>
        </authors>
        <title>Text Categorization with SVM: learning with many relevant features.</title>
        <publication>None</publication>
        <pages>98</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Dan Klein</author>
          <author>Christopher D Manning</author>
        </authors>
        <title>None</title>
        <publication>Accurate Unlexicalized Parsing. ACL-03.</publication>
        <pages>423--430</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Reinhard Kenser</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Improved backing-off for M-gram language modeling.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1995</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>F Och</author>
          <author>D Marcu</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>H Hoang</author>
          <author>A Birch</author>
          <author>C C-Burch</author>
          <author>M Federico</author>
          <author>N Bertoldi</author>
          <author>B Cowan</author>
          <author>W Shen</author>
          <author>C Moran</author>
          <author>R Zens</author>
          <author>C Dyer</author>
          <author>O Bojar</author>
          <author>A Constantin</author>
          <author>E Herbst</author>
        </authors>
        <title>Moses: Open Source Toolkit for SMT.</title>
        <publication>None</publication>
        <pages>07--77</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Shankar Kumar</author>
          <author>William Byrne</author>
        </authors>
        <title>Local Phrase Reordering Models for Statistical Machine Translation.</title>
        <publication>None</publication>
        <pages>161--168</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Chi-Ho Li</author>
          <author>Dongdong Zhang</author>
          <author>Mu Li</author>
          <author>Ming Zhou</author>
          <author>Minghui Li</author>
          <author>Yi Guan</author>
        </authors>
        <title>A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation.</title>
        <publication>None</publication>
        <pages>720--727</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Yang Liu</author>
          <author>Yun Huang</author>
          <author>Qun Liu</author>
          <author>Shouxun Lin</author>
        </authors>
        <title>Forest-to-String Statistical Translation Rules.</title>
        <publication>None</publication>
        <pages>704--711</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Daniel Marcu</author>
          <author>W Wang</author>
          <author>A Echihabi</author>
          <author>K Knight</author>
        </authors>
        <title>SPMT: SMT with Syntactified Target Language Phrases.</title>
        <publication>None</publication>
        <pages>44--52</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Haitao Mi</author>
          <author>Liang Huang</author>
        </authors>
        <title>Forest-based Translation Rule Extraction.</title>
        <publication>None</publication>
        <pages>206--214</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Alessandro Moschitti</author>
        </authors>
        <title>A Study on Convolution Kernels for Shallow Semantic Parsing.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Masaaki Nagata</author>
          <author>Kuniko Saito</author>
          <author>Kazuhide Yamamoto</author>
          <author>Kazuteru Ohashi</author>
        </authors>
        <title>A Clustered Global Phrase Reordering Model for Statistical Machine Translation.</title>
        <publication>None</publication>
        <pages>713--720</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Franz J Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Discriminative training and maximum entropy models for statistical machine translation.</title>
        <publication>None</publication>
        <pages>02--295</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Franz J Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>None</publication>
        <pages>03--160</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Franz J Och</author>
          <author>H Ney</author>
        </authors>
        <title>A Systematic Comparison of Various Statistical Alignment Methods.</title>
        <publication>None</publication>
        <pages>29--1</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Franz J Och</author>
          <author>H Ney</author>
        </authors>
        <title>The alignment template approach to statistical machine translation.</title>
        <publication>None</publication>
        <pages>30--4</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>S Roukos</author>
          <author>T</author>
          <author>W Zhu</author>
        </authors>
        <title>BLEU: a method for automatic evaluation of machine translation.</title>
        <publication>None</publication>
        <pages>02--311</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Hendra Setiawan</author>
          <author>Min-Yen Kan</author>
          <author>Haizhou Li</author>
        </authors>
        <title>Ordering Phrases with Function Words.</title>
        <publication>None</publication>
        <pages>712--719</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>Libin Shen</author>
          <author>Jinxi Xu</author>
          <author>Ralph Weischedel</author>
        </authors>
        <title>A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model.</title>
        <publication>None</publication>
        <pages>577--585</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>27</id>
        <authors>
          <author>Andreas Stolcke</author>
        </authors>
        <title>SRILM - an extensible language modeling toolkit.</title>
        <publication>None</publication>
        <pages>02--901</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>28</id>
        <authors>
          <author>Christoph Tillmann</author>
        </authors>
        <title>A Unigram Orientation Model for Statistical Machine Translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>29</id>
        <authors>
          <author>Vladimir N Vapnik</author>
        </authors>
        <title>The Nature of Statistical Learning Theory.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1995</date>
      </reference>
      <reference>
        <id>30</id>
        <authors>
          <author>M Collins Wang</author>
          <author>P Koehn</author>
        </authors>
        <title>Chinese Syntactic Reordering for Statistical Machine Translation.</title>
        <publication>None</publication>
        <pages>734--745</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>31</id>
        <authors>
          <author>Dekai Wu</author>
        </authors>
        <title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
        <publication>None</publication>
        <pages>23--3</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>32</id>
        <authors>
          <author>Fei Xia</author>
          <author>Michael McCord</author>
        </authors>
        <title>None</title>
        <publication>Improving a Statistical MT System with Automatically Learned Rewrite Patterns. COLING-04.</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>33</id>
        <authors>
          <author>Deyi Xiong</author>
          <author>Qun Liu</author>
          <author>Shouxun Lin</author>
        </authors>
        <title>Maximum Entropy Based Phrase Reordering Model for SMT.</title>
        <publication>None</publication>
        <pages>06--521</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>34</id>
        <authors>
          <author>Deyi Xiong</author>
          <author>Min Zhang</author>
          <author>Aiti Aw</author>
          <author>Haizhou Li</author>
        </authors>
        <title>A Linguistically Annotated Reordering Model for BTG-based Statistical Machine Translation. ACL-HLT-08 (short paper).</title>
        <publication>None</publication>
        <pages>149--152</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>35</id>
        <authors>
          <author>Kenji Yamada</author>
          <author>K Knight</author>
        </authors>
        <title>A syntax-based statistical translation model.</title>
        <publication>None</publication>
        <pages>01--523</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>36</id>
        <authors>
          <author>Xiaofeng Yang</author>
          <author>Jian Su</author>
          <author>Chew Lim Tan</author>
        </authors>
        <title>Kernel-Based Pronoun Resolution with Structured Syntactic Knowledge.</title>
        <publication>None</publication>
        <pages>41--48</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>37</id>
        <authors>
          <author>Richard Zens</author>
          <author>H Ney</author>
          <author>T Watanabe</author>
          <author>E Sumita</author>
        </authors>
        <title>Reordering Constraints for Phrase-Based Statistical Machine Translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>38</id>
        <authors>
          <author>Richard Zens</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Discriminative Reordering Models for Statistical Machine Translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>39</id>
        <authors>
          <author>Dell Zhang</author>
          <author>W Lee</author>
        </authors>
        <title>Question classification using support vector machines.</title>
        <publication>None</publication>
        <pages>03</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>40</id>
        <authors>
          <author>Min Zhang</author>
          <author>Jie Zhang</author>
          <author>Jian Su</author>
          <author>GuoDong Zhou</author>
        </authors>
        <title>A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features.</title>
        <publication>None</publication>
        <pages>825--832</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>41</id>
        <authors>
          <author>Dongdong Zhang</author>
          <author>M Li</author>
          <author>C H Li</author>
          <author>M Zhou</author>
        </authors>
        <title>Phrase Reordering Model Integrating Syntactic Knowledge for SMT.</title>
        <publication>None</publication>
        <pages>07--533</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>42</id>
        <authors>
          <author>Min Zhang</author>
          <author>W Che</author>
          <author>A Aw</author>
          <author>C Tan</author>
          <author>G Zhou</author>
          <author>T Liu</author>
          <author>S Li</author>
        </authors>
        <title>A Grammar-driven Convolution Tree Kernel for Semantic Role Classification.</title>
        <publication>None</publication>
        <pages>200--207</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>43</id>
        <authors>
          <author>Min Zhang</author>
          <author>Hongfei Jiang</author>
          <author>Ai Ti Aw</author>
          <author>Jun Sun</author>
          <author>Sheng Li</author>
          <author>Chew Lim Tan</author>
        </authors>
        <title>A Tree-to-Tree Alignment-based Model for Statistical Machine Translation.MT-Summit-07.</title>
        <publication>None</publication>
        <pages>535--542</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>44</id>
        <authors>
          <author>Min Zhang</author>
          <author>Hongfei Jiang</author>
          <author>Haizhou Li</author>
          <author>Aiti Aw</author>
          <author>Sheng Li</author>
        </authors>
        <title>None</title>
        <publication>Grammar Comparison Study for Translational Equivalence Modeling and Statistical Machine Translation. COLING-08.</publication>
        <pages>1097--1104</pages>
        <date>2008</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>5529</sentence_id>
        <char_offset>153</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>0</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>5537</sentence_id>
        <char_offset>312</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>Collins and Duffy, 2001</string>
        <sentence_id>5545</sentence_id>
        <char_offset>242</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>1</reference_id>
        <string>Collins and Duffy, 2001</string>
        <sentence_id>5546</sentence_id>
        <char_offset>120</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>1</reference_id>
        <string>Collins and Duffy, 2001</string>
        <sentence_id>5631</sentence_id>
        <char_offset>28</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>4</reference_id>
        <string>Eisner, 2003</string>
        <sentence_id>5529</sentence_id>
        <char_offset>139</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>5</reference_id>
        <string>Elming, 2008</string>
        <sentence_id>5538</sentence_id>
        <char_offset>153</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>7</reference_id>
        <string>Haussler, 1999</string>
        <sentence_id>5545</sentence_id>
        <char_offset>226</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>9</reference_id>
        <string>Klein and Manning, 2003</string>
        <sentence_id>5717</sentence_id>
        <char_offset>21</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>10</reference_id>
        <string>Kenser and Ney, 1995</string>
        <sentence_id>5719</sentence_id>
        <char_offset>221</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>11</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>5529</sentence_id>
        <char_offset>21</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>11</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>5537</sentence_id>
        <char_offset>113</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>12</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>5529</sentence_id>
        <char_offset>60</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>12</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>5722</sentence_id>
        <char_offset>60</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>13</reference_id>
        <string>Kumar and Byrne, 2005</string>
        <sentence_id>5537</sentence_id>
        <char_offset>252</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>14</reference_id>
        <string>Li et al., 2007</string>
        <sentence_id>5535</sentence_id>
        <char_offset>243</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>14</reference_id>
        <string>Li et al., 2007</string>
        <sentence_id>5538</sentence_id>
        <char_offset>136</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>14</reference_id>
        <string>Li et al., 2007</string>
        <sentence_id>5541</sentence_id>
        <char_offset>211</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>14</reference_id>
        <string>Li et al. (2007)</string>
        <sentence_id>5541</sentence_id>
        <char_offset>86</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>15</reference_id>
        <string>Liu et al., 2007</string>
        <sentence_id>5529</sentence_id>
        <char_offset>207</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>16</reference_id>
        <string>Marcu et al., 2006</string>
        <sentence_id>5529</sentence_id>
        <char_offset>187</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>17</reference_id>
        <string>Mi and Huang, 2008</string>
        <sentence_id>5529</sentence_id>
        <char_offset>279</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>18</reference_id>
        <string>Moschitti, 2004</string>
        <sentence_id>5546</sentence_id>
        <char_offset>170</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>18</reference_id>
        <string>Moschitti, 2004</string>
        <sentence_id>5731</sentence_id>
        <char_offset>147</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>18</reference_id>
        <string>Moschitti (2004)</string>
        <sentence_id>5674</sentence_id>
        <char_offset>147</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>21</reference_id>
        <string>Och, 2003</string>
        <sentence_id>5720</sentence_id>
        <char_offset>22</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>23</reference_id>
        <string>Och and Ney, 2004</string>
        <sentence_id>5529</sentence_id>
        <char_offset>41</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>23</reference_id>
        <string>Och and Ney, 2004</string>
        <sentence_id>5537</sentence_id>
        <char_offset>133</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>23</reference_id>
        <string>Och and Ney, 2004</string>
        <sentence_id>5718</sentence_id>
        <char_offset>8</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>24</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>5743</sentence_id>
        <char_offset>39</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>24</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>5745</sentence_id>
        <char_offset>67</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>25</reference_id>
        <string>Setiawan et al., 2007</string>
        <sentence_id>5537</sentence_id>
        <char_offset>326</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>26</reference_id>
        <string>Shen et al., 2008</string>
        <sentence_id>5529</sentence_id>
        <char_offset>260</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>27</reference_id>
        <string>Stolcke, 2002</string>
        <sentence_id>5719</sentence_id>
        <char_offset>170</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>28</reference_id>
        <string>Tillmann, 2004</string>
        <sentence_id>5537</sentence_id>
        <char_offset>236</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>29</reference_id>
        <string>Vapnik, 1995</string>
        <sentence_id>5540</sentence_id>
        <char_offset>33</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>29</reference_id>
        <string>Vapnik, 1995</string>
        <sentence_id>5545</sentence_id>
        <char_offset>155</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>31</reference_id>
        <string>Wu, 1997</string>
        <sentence_id>5529</sentence_id>
        <char_offset>104</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>31</reference_id>
        <string>Wu, 1997</string>
        <sentence_id>5533</sentence_id>
        <char_offset>224</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>31</reference_id>
        <string>Wu, 1997</string>
        <sentence_id>5537</sentence_id>
        <char_offset>176</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>33</reference_id>
        <string>Xiong et al., 2006</string>
        <sentence_id>5537</sentence_id>
        <char_offset>429</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>33</reference_id>
        <string>Xiong et al., 2006</string>
        <sentence_id>5557</sentence_id>
        <char_offset>48</char_offset>
      </citation>
      <citation>
        <id>42</id>
        <reference_id>33</reference_id>
        <string>Xiong et al., 2006</string>
        <sentence_id>5568</sentence_id>
        <char_offset>95</char_offset>
      </citation>
      <citation>
        <id>43</id>
        <reference_id>33</reference_id>
        <string>Xiong et al., 2006</string>
        <sentence_id>5626</sentence_id>
        <char_offset>47</char_offset>
      </citation>
      <citation>
        <id>44</id>
        <reference_id>33</reference_id>
        <string>Xiong et al., 2006</string>
        <sentence_id>5722</sentence_id>
        <char_offset>268</char_offset>
      </citation>
      <citation>
        <id>45</id>
        <reference_id>34</reference_id>
        <string>Xiong et al., 2008</string>
        <sentence_id>5537</sentence_id>
        <char_offset>470</char_offset>
      </citation>
      <citation>
        <id>46</id>
        <reference_id>34</reference_id>
        <string>Xiong et al., 2008</string>
        <sentence_id>5722</sentence_id>
        <char_offset>362</char_offset>
      </citation>
      <citation>
        <id>47</id>
        <reference_id>35</reference_id>
        <string>Yamada and Knight, 2001</string>
        <sentence_id>5529</sentence_id>
        <char_offset>114</char_offset>
      </citation>
      <citation>
        <id>48</id>
        <reference_id>36</reference_id>
        <string>Yang et al., 2006</string>
        <sentence_id>5546</sentence_id>
        <char_offset>271</char_offset>
      </citation>
      <citation>
        <id>49</id>
        <reference_id>37</reference_id>
        <string>Zens et al., 2004</string>
        <sentence_id>5537</sentence_id>
        <char_offset>186</char_offset>
      </citation>
      <citation>
        <id>50</id>
        <reference_id>38</reference_id>
        <string>Zens and Ney, 2006</string>
        <sentence_id>5537</sentence_id>
        <char_offset>409</char_offset>
      </citation>
      <citation>
        <id>51</id>
        <reference_id>39</reference_id>
        <string>Zhang and Lee, 2003</string>
        <sentence_id>5546</sentence_id>
        <char_offset>319</char_offset>
      </citation>
      <citation>
        <id>52</id>
        <reference_id>40</reference_id>
        <string>Zhang et al., 2006</string>
        <sentence_id>5546</sentence_id>
        <char_offset>230</char_offset>
      </citation>
      <citation>
        <id>53</id>
        <reference_id>40</reference_id>
        <string>Zhang et al., 2006</string>
        <sentence_id>5590</sentence_id>
        <char_offset>26</char_offset>
      </citation>
      <citation>
        <id>54</id>
        <reference_id>41</reference_id>
        <string>Zhang et al., 2007</string>
        <sentence_id>5529</sentence_id>
        <char_offset>225</char_offset>
      </citation>
      <citation>
        <id>55</id>
        <reference_id>41</reference_id>
        <string>Zhang et al., 2007</string>
        <sentence_id>5537</sentence_id>
        <char_offset>449</char_offset>
      </citation>
      <citation>
        <id>56</id>
        <reference_id>41</reference_id>
        <string>Zhang et al., 2007</string>
        <sentence_id>5546</sentence_id>
        <char_offset>187</char_offset>
      </citation>
      <citation>
        <id>57</id>
        <reference_id>41</reference_id>
        <string>Zhang et al., 2007</string>
        <sentence_id>5568</sentence_id>
        <char_offset>122</char_offset>
      </citation>
      <citation>
        <id>58</id>
        <reference_id>41</reference_id>
        <string>Zhang et al., 2007</string>
        <sentence_id>5626</sentence_id>
        <char_offset>67</char_offset>
      </citation>
      <citation>
        <id>59</id>
        <reference_id>42</reference_id>
        <string>Zhang et al., 2007</string>
        <sentence_id>5529</sentence_id>
        <char_offset>225</char_offset>
      </citation>
      <citation>
        <id>60</id>
        <reference_id>42</reference_id>
        <string>Zhang et al., 2007</string>
        <sentence_id>5537</sentence_id>
        <char_offset>449</char_offset>
      </citation>
      <citation>
        <id>61</id>
        <reference_id>42</reference_id>
        <string>Zhang et al., 2007</string>
        <sentence_id>5546</sentence_id>
        <char_offset>187</char_offset>
      </citation>
      <citation>
        <id>62</id>
        <reference_id>42</reference_id>
        <string>Zhang et al., 2007</string>
        <sentence_id>5568</sentence_id>
        <char_offset>122</char_offset>
      </citation>
      <citation>
        <id>63</id>
        <reference_id>42</reference_id>
        <string>Zhang et al., 2007</string>
        <sentence_id>5626</sentence_id>
        <char_offset>67</char_offset>
      </citation>
      <citation>
        <id>64</id>
        <reference_id>43</reference_id>
        <string>Zhang et al., 2007</string>
        <sentence_id>5529</sentence_id>
        <char_offset>225</char_offset>
      </citation>
      <citation>
        <id>65</id>
        <reference_id>43</reference_id>
        <string>Zhang et al., 2007</string>
        <sentence_id>5537</sentence_id>
        <char_offset>449</char_offset>
      </citation>
      <citation>
        <id>66</id>
        <reference_id>43</reference_id>
        <string>Zhang et al., 2007</string>
        <sentence_id>5546</sentence_id>
        <char_offset>187</char_offset>
      </citation>
      <citation>
        <id>67</id>
        <reference_id>43</reference_id>
        <string>Zhang et al., 2007</string>
        <sentence_id>5568</sentence_id>
        <char_offset>122</char_offset>
      </citation>
      <citation>
        <id>68</id>
        <reference_id>43</reference_id>
        <string>Zhang et al., 2007</string>
        <sentence_id>5626</sentence_id>
        <char_offset>67</char_offset>
      </citation>
    </citations>
  </content>
</document>
