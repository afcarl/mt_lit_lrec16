<document>
  <filename>W12-2101</filename>
  <authors/>
  <title>Analyzing Urdu Social Media for Sentiments using Transfer Learning with Controlled Translations Author 2 Smruthi Mukund</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>The main aim of this work is to perform sentiment analysis on Urdu blog data. We use the method of structural correspondence learning (SCL) to transfer sentiment analysis learning from Urdu newswire data to Urdu blog data. The pivots needed to transfer learning from newswire domain to blog domain is not trivial as Urdu blog data, unlike newswire data is written in Latin script and exhibits codemixing and code-switching behavior. We consider two oracles to generate the pivots. 1. Transliteration oracle, to accommodate script variation and spelling variation and 2. Translation oracle, to accommodate code-switching and code-mixing behavior. In order to identify strong candidates for translation, we propose a novel part-of-speech tagging method that helps select words based on POS categories that strongly reflect code-mixing behavior. We validate our approach against a supervised learning method and show that the performance of our proposed approach is comparable.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The main aim of this work is to perform sentiment analysis on Urdu blog data.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We use the method of structural correspondence learning (SCL) to transfer sentiment analysis learning from Urdu newswire data to Urdu blog data.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The pivots needed to transfer learning from newswire domain to blog domain is not trivial as Urdu blog data, unlike newswire data is written in Latin script and exhibits codemixing and code-switching behavior.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We consider two oracles to generate the pivots.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>1.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Transliteration oracle, to accommodate script variation and spelling variation and 2.</text>
              <doc_id>5</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Translation oracle, to accommodate code-switching and code-mixing behavior.</text>
              <doc_id>6</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In order to identify strong candidates for translation, we propose a novel part-of-speech tagging method that helps select words based on POS categories that strongly reflect code-mixing behavior.</text>
              <doc_id>7</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>We validate our approach against a supervised learning method and show that the performance of our proposed approach is comparable.</text>
              <doc_id>8</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>The ability to break language barriers and understand people's feelings and emotions towards societal issues can assist in bridging the gulf that exists today. Often emotions are captured in blogs or discussion forums where writers are common people empathizing with the situations they describe. As an example, the incident where a cricket team visiting Pakistan was attacked caused widespread anguish among the youth in that country who thought that they will no longer be able to host international tournaments. The angry emotion was towards the failure of the government to provide adequate protection for citizens and visitors. Discussion forums and blogs on cricket, mainly written by Pakistani cricket fans, around the time, verbalized this emotion. Clearly analyzing blog data helps to estimate emotion responses to domestic situations that are common to many societies.
Traditional approaches to sentiment analysis require access to annotated data. But facilitating such data is laborious, time consuming and most importantly fail to scale to new domains and capture peculiarities that blog data exhibits; 1. spelling variations and 2. code mixing and code switching. 3. script difference (Nastaliq vs Latin script). In this work, we present a new approach to polarity classification of code-mixed data that builds on a theory called structural correspondence learning (SCL) for domain adaptation. This approach uses labeled polarity data from the base language (in this case, Urdu newswire data - source) along with two simple oracles that provide one-one mapping between the source and the target data set (Urdu blog data).
Subsequent sections are organized as follows. Section 2 describes the issues seen in Urdu blog data followed by section 3 that explains the concept of structural correspondence learning. Section 4 details the code mixing and code switching behavior seen in blog data. Section 5 describes the statistical part of speech (POS) tagger developed for blog data required to identify mixing patterns followed by the sentiment analysis model in section 6. We conclude with section 7 and briefly outline analysis and future work in section 8.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The ability to break language barriers and understand people's feelings and emotions towards societal issues can assist in bridging the gulf that exists today.</text>
              <doc_id>9</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Often emotions are captured in blogs or discussion forums where writers are common people empathizing with the situations they describe.</text>
              <doc_id>10</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>As an example, the incident where a cricket team visiting Pakistan was attacked caused widespread anguish among the youth in that country who thought that they will no longer be able to host international tournaments.</text>
              <doc_id>11</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The angry emotion was towards the failure of the government to provide adequate protection for citizens and visitors.</text>
              <doc_id>12</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Discussion forums and blogs on cricket, mainly written by Pakistani cricket fans, around the time, verbalized this emotion.</text>
              <doc_id>13</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Clearly analyzing blog data helps to estimate emotion responses to domestic situations that are common to many societies.</text>
              <doc_id>14</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Traditional approaches to sentiment analysis require access to annotated data.</text>
              <doc_id>15</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>But facilitating such data is laborious, time consuming and most importantly fail to scale to new domains and capture peculiarities that blog data exhibits; 1. spelling variations and 2. code mixing and code switching.</text>
              <doc_id>16</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>3. script difference (Nastaliq vs Latin script).</text>
              <doc_id>17</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In this work, we present a new approach to polarity classification of code-mixed data that builds on a theory called structural correspondence learning (SCL) for domain adaptation.</text>
              <doc_id>18</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This approach uses labeled polarity data from the base language (in this case, Urdu newswire data - source) along with two simple oracles that provide one-one mapping between the source and the target data set (Urdu blog data).</text>
              <doc_id>19</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Subsequent sections are organized as follows.</text>
              <doc_id>20</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Section 2 describes the issues seen in Urdu blog data followed by section 3 that explains the concept of structural correspondence learning.</text>
              <doc_id>21</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Section 4 details the code mixing and code switching behavior seen in blog data.</text>
              <doc_id>22</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Section 5 describes the statistical part of speech (POS) tagger developed for blog data required to identify mixing patterns followed by the sentiment analysis model in section 6.</text>
              <doc_id>23</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We conclude with section 7 and briefly outline analysis and future work in section 8.</text>
              <doc_id>24</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Urdu Blog Data</title>
        <text>Though non-topical text analysis like emotion detection and sentiment analysis, have been explored mostly in the English language, they have also gained some exposure in non-English languages like Urdu (Mukund and Srihari, 2010), Arabic (Mageed et al., 2011) and Hindi (Joshi and Bhattacharya, 2012). Urdu newswire data is written using Nastaliq script and follows a relatively strict grammatical guideline. Many of the techniques proposed either depend heavily on NLP features or annotated data. But, data in blogs and discussion forums especially written in a language like Urdu cannot be analyzed by using modules developed for Nastaliq script for the following reasons; (1) the tone of the text in blogs and discussion forums is informal and hence differs in the grammatical structure (2) the text is written using Latin script (3) the text exhibits code mixing and code switching behavior (with English) (4) there exists spelling errors which occur mostly due to the lack of predefined standards to represent Urdu data in Latin script.
Urdish (Urdu blog data) is the term used for Urdu, which is (1) written either in Nastaliq or Latin script, and (2) contains several English words/phrases/sentences. In other words, Urdish is a name given to a language that has Urdu as the base language and English as the seasoning language. With the wide spread use of English keyboards these days, using Latin script to encode Urdu is very common. Data in Urdish is never in pure Urdu. English words and phrases are commonly used in the flow integrating tightly with the base language. Table 1 shows examples of different flavors in which Urdu appears in the internet.
Different Forms of Data 1. Urdu written in Nastaliq
2. Urdu written in ASCII
Main Issues
1. Lack of tools for basic operations such as segmentation and diacritic restoration 2. Lack of sufficient annotated data for POS and NE tagging 3. Lack of annotated data for more advanced NLP 1. Several variations in spellings that need to be normalized
Example Sentence
&#1601;&#1608;&#1580;&#1740; &#1580;&#1608;&#1575;&#1606;&#1608;&#1722; &#1705;&#1608; &#1705;&#1574;&#1740; &#1604;&#1608;&#1711;&#1608;&#1722; &#1587;&#1746; &#1594;&#1589;&#1729; &#1570;&#1711;&#1740;&#1575; [ The soldiers were angry with a lot of people]
Wo Mulk Jisko Hum nay 1000000 logoon sey zayada Loogoon (English)
3. Urdish written in Nastaliq
4. Urdish written in ASCII( English) 2. No normalization standards 3. Preprocessing modules needed if tools for Urdu in Nastaliq are to be used 4. Developing a completely new NLP framework needs annotated data 1. No combined parser that deals with English and Urdu simultaneously 2. English is written in Urdu but with missing diacritics 1. No combined parser that deals with English and Urdu simultaneously 2. Issue of spelling variations that need to be normalized
ki Qurbanian dey ker hasil kia usi mulk main yai kaisa waqt a gay hai ?
[Look at what kind of time the land that had 1000000&#8217;s of people sacrifice their lives is experiencing now]
&#1657;&#1740; &#1608;&#1740; &#1587;&#1657;&#1740;&#1588;&#1606; &#1605;&#1740;&#1722; &#1601;&#1608;&#1606; &#1662;&#1585; &#1601;&#1608;&#1606; &#1570;&#1606;&#1746; &#1604;&#1711;&#1746;
[the phones rang one after the other in the TV station]
Afsoos key baat hai . kal tak jo batain Non Muslim bhi kartay hoay dartay thay abhi this man has brought it out in the open.
[It is sad to see that those words that even a non muslim would fear to utter till yesterday, this man had brought it out in the open] Table 1: Different forms of using Urdu language on the internet
Blog data follows the order shown in example 4 of table 1. Such a code-switching phenomenon is very common in multilingual societies that have significant exposure to English. Other languages exhibiting similar behaviors are Hinglish (Hindi and English), Arabic with English and Spanglish (Spanish with English).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Though non-topical text analysis like emotion detection and sentiment analysis, have been explored mostly in the English language, they have also gained some exposure in non-English languages like Urdu (Mukund and Srihari, 2010), Arabic (Mageed et al., 2011) and Hindi (Joshi and Bhattacharya, 2012).</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Urdu newswire data is written using Nastaliq script and follows a relatively strict grammatical guideline.</text>
              <doc_id>26</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Many of the techniques proposed either depend heavily on NLP features or annotated data.</text>
              <doc_id>27</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>But, data in blogs and discussion forums especially written in a language like Urdu cannot be analyzed by using modules developed for Nastaliq script for the following reasons; (1) the tone of the text in blogs and discussion forums is informal and hence differs in the grammatical structure (2) the text is written using Latin script (3) the text exhibits code mixing and code switching behavior (with English) (4) there exists spelling errors which occur mostly due to the lack of predefined standards to represent Urdu data in Latin script.</text>
              <doc_id>28</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Urdish (Urdu blog data) is the term used for Urdu, which is (1) written either in Nastaliq or Latin script, and (2) contains several English words/phrases/sentences.</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In other words, Urdish is a name given to a language that has Urdu as the base language and English as the seasoning language.</text>
              <doc_id>30</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>With the wide spread use of English keyboards these days, using Latin script to encode Urdu is very common.</text>
              <doc_id>31</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Data in Urdish is never in pure Urdu.</text>
              <doc_id>32</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>English words and phrases are commonly used in the flow integrating tightly with the base language.</text>
              <doc_id>33</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Table 1 shows examples of different flavors in which Urdu appears in the internet.</text>
              <doc_id>34</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Different Forms of Data 1.</text>
              <doc_id>35</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Urdu written in Nastaliq</text>
              <doc_id>36</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.</text>
              <doc_id>37</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Urdu written in ASCII</text>
              <doc_id>38</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Main Issues</text>
              <doc_id>39</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1.</text>
              <doc_id>40</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Lack of tools for basic operations such as segmentation and diacritic restoration 2.</text>
              <doc_id>41</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Lack of sufficient annotated data for POS and NE tagging 3.</text>
              <doc_id>42</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Lack of annotated data for more advanced NLP 1.</text>
              <doc_id>43</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Several variations in spellings that need to be normalized</text>
              <doc_id>44</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Example Sentence</text>
              <doc_id>45</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#1601;&#1608;&#1580;&#1740; &#1580;&#1608;&#1575;&#1606;&#1608;&#1722; &#1705;&#1608; &#1705;&#1574;&#1740; &#1604;&#1608;&#1711;&#1608;&#1722; &#1587;&#1746; &#1594;&#1589;&#1729; &#1570;&#1711;&#1740;&#1575; [ The soldiers were angry with a lot of people]</text>
              <doc_id>46</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Wo Mulk Jisko Hum nay 1000000 logoon sey zayada Loogoon (English)</text>
              <doc_id>47</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3.</text>
              <doc_id>48</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Urdish written in Nastaliq</text>
              <doc_id>49</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>4.</text>
              <doc_id>50</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Urdish written in ASCII( English) 2.</text>
              <doc_id>51</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>No normalization standards 3.</text>
              <doc_id>52</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Preprocessing modules needed if tools for Urdu in Nastaliq are to be used 4.</text>
              <doc_id>53</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Developing a completely new NLP framework needs annotated data 1.</text>
              <doc_id>54</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>No combined parser that deals with English and Urdu simultaneously 2.</text>
              <doc_id>55</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>English is written in Urdu but with missing diacritics 1.</text>
              <doc_id>56</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>No combined parser that deals with English and Urdu simultaneously 2.</text>
              <doc_id>57</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Issue of spelling variations that need to be normalized</text>
              <doc_id>58</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>ki Qurbanian dey ker hasil kia usi mulk main yai kaisa waqt a gay hai ?</text>
              <doc_id>59</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>[Look at what kind of time the land that had 1000000&#8217;s of people sacrifice their lives is experiencing now]</text>
              <doc_id>60</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#1657;&#1740; &#1608;&#1740; &#1587;&#1657;&#1740;&#1588;&#1606; &#1605;&#1740;&#1722; &#1601;&#1608;&#1606; &#1662;&#1585; &#1601;&#1608;&#1606; &#1570;&#1606;&#1746; &#1604;&#1711;&#1746;</text>
              <doc_id>61</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>[the phones rang one after the other in the TV station]</text>
              <doc_id>62</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Afsoos key baat hai .</text>
              <doc_id>63</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>kal tak jo batain Non Muslim bhi kartay hoay dartay thay abhi this man has brought it out in the open.</text>
              <doc_id>64</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>[It is sad to see that those words that even a non muslim would fear to utter till yesterday, this man had brought it out in the open] Table 1: Different forms of using Urdu language on the internet</text>
              <doc_id>65</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Blog data follows the order shown in example 4 of table 1.</text>
              <doc_id>66</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Such a code-switching phenomenon is very common in multilingual societies that have significant exposure to English.</text>
              <doc_id>67</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Other languages exhibiting similar behaviors are Hinglish (Hindi and English), Arabic with English and Spanglish (Spanish with English).</text>
              <doc_id>68</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Structural Correspondence Learning</title>
        <text>For a problem where domain and data changes requires new training and learning, resorting to classical approaches that need annotated data becomes expensive. The need for domain adaptation arises in many NLP tasks &#8211; part of speech tagging, semantic role labeling, dependency parsing, and sentiment analysis and has gained high visibility in the recent years (Daume III and Marcu, 2006; Daume III et al., 2007; Blitzer et al., 2006, Prettenhofer and Stein et al., 2010). There exists two main approaches; supervised and semi-supervised.
In the supervised domain adaptation approach along with labeled source data, there is also access to a small amount of labeled target data. Techniques proposed by Gildea (2001), Roark and Bacchiani (2003), Daume III (2007) are based on the supervised approach. Studies have shown that baseline approaches (based on source only, target only or union of data) for supervised domain adaption work reasonably well and beating this is surprisingly difficult (Daume III, 2007).
In contract, the semi supervised domain adaptation approach has access to labeled data only in the source domain (Blitzer et al., 2006; Dredze et al., 2007; Prettenhofer and Stein et al., 2010). Since there is no access to labeled target data, achieving baseline performance exhibited in the supervised approach requires innovative thinking.
The method of structural correspondence learning (SCL) is related to the structural learning paradigm introduced by Ando and Zhang (2005). The basic idea of structural learning is to constrain the hypothesis space of a learning task by considering multiple different but related tasks on the same input space. SCL was first proposed by Blitzer et al., (2006) for the semi supervised domain adaptation problem and works as follows (Shimizu and Nakagawa, 2007). 1. A set of pivot features are defined on unlabeled data from both the source domain and the target domain 2. These pivot features are used to learn a mapping from the original feature spaces of both domains to a shared, low-dimensional real&#8211; valued feature space. A high inner product in this new space indicates a high degree of correspondence along that feature dimension 3. Both the transformed and the original features
in the source domain are used to train a learning model 4. The effectiveness of the classifier in the source
domain transfers to the target domain based on the mapping learnt
This approach of SCL was applied in the field of cross language sentiment classification scenario by Prettenhofer and Stein (2010) where English was used as the source language and German, French and Japanese as target languages. Their approach induces correspondence among the words from both languages by means of a small number of pivot pairs that are words that process similar semantics in both the source and the target languages. The correlation between the pivots is modeled by a linear classifier and used as a language independent predictor for the two equivalent classes. This approach solves the classification problem directly, instead of resorting to a more general and potentially much harder problem such as machine translation.
The problem of sentiment classification in blog data can be considered as falling in the realm of domain adaptation. In this work, we approach this problem using SCL tailored to accommodate the challenges that code-mixed data exhibits. Similar to the work done by Prettenhofer and Stein (2010), we look at generating pivot pairs that capture codemixing and code-switching behavior and language change.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>For a problem where domain and data changes requires new training and learning, resorting to classical approaches that need annotated data becomes expensive.</text>
              <doc_id>69</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The need for domain adaptation arises in many NLP tasks &#8211; part of speech tagging, semantic role labeling, dependency parsing, and sentiment analysis and has gained high visibility in the recent years (Daume III and Marcu, 2006; Daume III et al., 2007; Blitzer et al., 2006, Prettenhofer and Stein et al., 2010).</text>
              <doc_id>70</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>There exists two main approaches; supervised and semi-supervised.</text>
              <doc_id>71</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the supervised domain adaptation approach along with labeled source data, there is also access to a small amount of labeled target data.</text>
              <doc_id>72</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Techniques proposed by Gildea (2001), Roark and Bacchiani (2003), Daume III (2007) are based on the supervised approach.</text>
              <doc_id>73</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Studies have shown that baseline approaches (based on source only, target only or union of data) for supervised domain adaption work reasonably well and beating this is surprisingly difficult (Daume III, 2007).</text>
              <doc_id>74</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In contract, the semi supervised domain adaptation approach has access to labeled data only in the source domain (Blitzer et al., 2006; Dredze et al., 2007; Prettenhofer and Stein et al., 2010).</text>
              <doc_id>75</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Since there is no access to labeled target data, achieving baseline performance exhibited in the supervised approach requires innovative thinking.</text>
              <doc_id>76</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The method of structural correspondence learning (SCL) is related to the structural learning paradigm introduced by Ando and Zhang (2005).</text>
              <doc_id>77</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The basic idea of structural learning is to constrain the hypothesis space of a learning task by considering multiple different but related tasks on the same input space.</text>
              <doc_id>78</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>SCL was first proposed by Blitzer et al., (2006) for the semi supervised domain adaptation problem and works as follows (Shimizu and Nakagawa, 2007).</text>
              <doc_id>79</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>1.</text>
              <doc_id>80</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>A set of pivot features are defined on unlabeled data from both the source domain and the target domain 2.</text>
              <doc_id>81</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>These pivot features are used to learn a mapping from the original feature spaces of both domains to a shared, low-dimensional real&#8211; valued feature space.</text>
              <doc_id>82</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>A high inner product in this new space indicates a high degree of correspondence along that feature dimension 3.</text>
              <doc_id>83</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Both the transformed and the original features</text>
              <doc_id>84</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>in the source domain are used to train a learning model 4.</text>
              <doc_id>85</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The effectiveness of the classifier in the source</text>
              <doc_id>86</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>domain transfers to the target domain based on the mapping learnt</text>
              <doc_id>87</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>This approach of SCL was applied in the field of cross language sentiment classification scenario by Prettenhofer and Stein (2010) where English was used as the source language and German, French and Japanese as target languages.</text>
              <doc_id>88</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Their approach induces correspondence among the words from both languages by means of a small number of pivot pairs that are words that process similar semantics in both the source and the target languages.</text>
              <doc_id>89</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The correlation between the pivots is modeled by a linear classifier and used as a language independent predictor for the two equivalent classes.</text>
              <doc_id>90</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This approach solves the classification problem directly, instead of resorting to a more general and potentially much harder problem such as machine translation.</text>
              <doc_id>91</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The problem of sentiment classification in blog data can be considered as falling in the realm of domain adaptation.</text>
              <doc_id>92</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this work, we approach this problem using SCL tailored to accommodate the challenges that code-mixed data exhibits.</text>
              <doc_id>93</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Similar to the work done by Prettenhofer and Stein (2010), we look at generating pivot pairs that capture codemixing and code-switching behavior and language change.</text>
              <doc_id>94</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Code Switching and Code Mixing</title>
        <text>Code switching refers to the switch that exists from one language to another and typically involves the use of longer phrases or clauses of another language while conversing in a totally different base language. Code mixing, on the other hand, is a phenomenon of mixing words and other smaller units of one language into the structure of another language. This is mostly inter-sentential.
In a society that is bilingual such as that in Pakistan and India, the use of English in the native language suggests power, social prestige and the status. The younger crowd that is technologically well equipped tends to use the switching phenomenon in their language, be it spoken or written. Several blogs, discussion forums, chat rooms etc. hold information that is expressed is intensely code mixed. Urdu blog data exhibits mix of Urdu language with English.
There are several challenges associated with developing NLP systems for code-switched languages. Work done by Kumar (1986) and Sinha &amp; Thakur, (2005) address issues and challenges associated with Hinglish (Hindi &#8211; English) data. Dussias (2003) and Celia (1997) give an overview of the behavior of code switching occurring in Spanish - Spanglish. This phenomenon can be seen in other languages like Kannada and English, German and English. Rasul (2006) analyzes the linguistic patterns occurring in Urdish (Urdu and English) language. He tries to quantize the extent to which code-mixing occurs in media data, in particular television. Most of his rules are based on
what is proposed by Kachru (1978) for Hinglish and has a pure linguistic approach with manual intervention for both qualitative and quantitative analysis.
Several automated techniques proposed for Hinglish and Spanglish are in the context of machine translation and may not be relevant for a task like information retrieval since converting the data to one standardized form is not required. A more recent work was by Goyal et al., (2003) where they developed a bilingual parser for Hindi and English by treating the code mixed language as a completely different variety. However, the credibility of the system depends on the availability of WordNet 1 .</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Code switching refers to the switch that exists from one language to another and typically involves the use of longer phrases or clauses of another language while conversing in a totally different base language.</text>
              <doc_id>95</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Code mixing, on the other hand, is a phenomenon of mixing words and other smaller units of one language into the structure of another language.</text>
              <doc_id>96</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This is mostly inter-sentential.</text>
              <doc_id>97</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In a society that is bilingual such as that in Pakistan and India, the use of English in the native language suggests power, social prestige and the status.</text>
              <doc_id>98</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The younger crowd that is technologically well equipped tends to use the switching phenomenon in their language, be it spoken or written.</text>
              <doc_id>99</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Several blogs, discussion forums, chat rooms etc. hold information that is expressed is intensely code mixed.</text>
              <doc_id>100</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Urdu blog data exhibits mix of Urdu language with English.</text>
              <doc_id>101</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>There are several challenges associated with developing NLP systems for code-switched languages.</text>
              <doc_id>102</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Work done by Kumar (1986) and Sinha &amp; Thakur, (2005) address issues and challenges associated with Hinglish (Hindi &#8211; English) data.</text>
              <doc_id>103</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Dussias (2003) and Celia (1997) give an overview of the behavior of code switching occurring in Spanish - Spanglish.</text>
              <doc_id>104</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This phenomenon can be seen in other languages like Kannada and English, German and English.</text>
              <doc_id>105</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Rasul (2006) analyzes the linguistic patterns occurring in Urdish (Urdu and English) language.</text>
              <doc_id>106</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>He tries to quantize the extent to which code-mixing occurs in media data, in particular television.</text>
              <doc_id>107</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Most of his rules are based on</text>
              <doc_id>108</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>what is proposed by Kachru (1978) for Hinglish and has a pure linguistic approach with manual intervention for both qualitative and quantitative analysis.</text>
              <doc_id>109</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Several automated techniques proposed for Hinglish and Spanglish are in the context of machine translation and may not be relevant for a task like information retrieval since converting the data to one standardized form is not required.</text>
              <doc_id>110</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A more recent work was by Goyal et al., (2003) where they developed a bilingual parser for Hindi and English by treating the code mixed language as a completely different variety.</text>
              <doc_id>111</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, the credibility of the system depends on the availability of WordNet 1 .</text>
              <doc_id>112</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Understanding Mixing Patterns</title>
            <text>Performing analysis on data that exhibit codeswitching has been attempted by many across various languages. Since the Urdu language is very similar to Hindi, in this section we discuss the code-mixing behavior based on a whole battery of work done by researchers in the Hindi language.
Researchers have studied the behavior of the mixed patterns and generated rules and constraints on code-mixing. The study of code mixing with Hindi as the base language is attempted by Sinha and Thakur (2005) in the context of machine translation. They categorize the phenomenon into two types based on the extent to which mixing happens in text in the context of the main verb. Linguists such as Kachru (1996) and Poplack (1980) have tried to formalize the terminologies used in this kind of behavior. Kumar (1986) says that the motivation for assuming that the switching occurs based on certain set of rules and constraints are based on the fact that users who use this can effectively communicate with each other despite the mixed language. In his paper he proposes a set of rules and constraints for Hindi-English code switching. However, these rules and constraints have been countered by examples proposed in the literature (Agnihotri, 1998). This does not mean that researchers earlier had not considered all the possibilities. It only means that like any other language, the language of code-mixing is evolving over time but at a very fast pace.
One way to address this problem of code-mixing and code switching for our task of sentiment analy-
1 http://www.cfilt.iitb.ac.in/wordnet/webhwn/
sis in blog data is rely on predefined rules to identify mixed words. But this can get laborious and the rules may be insufficient to capture the latest behavior. Our approach is to use a statistical POS model to determine part of speech categories of words that typically undergo such switches.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Performing analysis on data that exhibit codeswitching has been attempted by many across various languages.</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Since the Urdu language is very similar to Hindi, in this section we discuss the code-mixing behavior based on a whole battery of work done by researchers in the Hindi language.</text>
                  <doc_id>114</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Researchers have studied the behavior of the mixed patterns and generated rules and constraints on code-mixing.</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The study of code mixing with Hindi as the base language is attempted by Sinha and Thakur (2005) in the context of machine translation.</text>
                  <doc_id>116</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>They categorize the phenomenon into two types based on the extent to which mixing happens in text in the context of the main verb.</text>
                  <doc_id>117</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Linguists such as Kachru (1996) and Poplack (1980) have tried to formalize the terminologies used in this kind of behavior.</text>
                  <doc_id>118</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Kumar (1986) says that the motivation for assuming that the switching occurs based on certain set of rules and constraints are based on the fact that users who use this can effectively communicate with each other despite the mixed language.</text>
                  <doc_id>119</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In his paper he proposes a set of rules and constraints for Hindi-English code switching.</text>
                  <doc_id>120</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>However, these rules and constraints have been countered by examples proposed in the literature (Agnihotri, 1998).</text>
                  <doc_id>121</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>This does not mean that researchers earlier had not considered all the possibilities.</text>
                  <doc_id>122</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>It only means that like any other language, the language of code-mixing is evolving over time but at a very fast pace.</text>
                  <doc_id>123</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>One way to address this problem of code-mixing and code switching for our task of sentiment analy-</text>
                  <doc_id>124</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 http://www.cfilt.iitb.ac.in/wordnet/webhwn/</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>sis in blog data is rely on predefined rules to identify mixed words.</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>But this can get laborious and the rules may be insufficient to capture the latest behavior.</text>
                  <doc_id>127</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Our approach is to use a statistical POS model to determine part of speech categories of words that typically undergo such switches.</text>
                  <doc_id>128</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Statistical Part of Speech Tagger</title>
        <text>Example 5.1 showcases a typical sentence seen in blog data. Example 5.2 shows the issue with spelling variations sometimes that occur in the same sentence Example 5.1: Otherwise humara bhi wohi haal hoga jo is time Palestine, Iraq, Afghanistan wagera ka hai ~ Otherwise our state will also be like what is in Palestine, Iraq, Afghanistan etc. are experiencing at this time Example 5.2: Shariyat ke aitebaar se bhi ghaur kia jaey tu aap ko ilm ho jaega key joh haraam khata hai uska dil kis tarhan ka hota hey ~ If you look at it from morals point of you too you will understand the heart of people who cheat A statistical POS tagger for blog data has to take into consideration spelling variations, mixing patterns and script change. The goal here is not to generate a perfect POS tagger for blog data (though the idea explained here can be extended for further improvisation) but to be able to identify POS categories that are candidates for switch and mix. The basic idea of our approach is as follows 1. Train Latin script POS tagger (LS tagger) on
pure Urdu Latin script data (Example 2 in table 1 &#8211; using Urdu POS tag set, Muaz et al., 2009) 2. Train English POS tagger on English data
(based on English tag sets, Santorini, 1990) 3. Apply LS tagger and English tagger on Urdish
data and note the confidence measures of the applied tags on each word 4. Use confidence measures, LS tags, phoneme
codes (to accommodate spelling variations) as features to train a new learning model on Urdish data 5. Those words that get tagged with the English
tagset are potential place holders for mixing patterns
The training data needed to develop LS tagger for Urdu is obtained from Hindi. IIIT POS annotated corpus for Hindi contains data in the SSF format (Shakti Standard Format) (Bharati, 2006). This format tries to capture the pronunciation information by assigning unique English characters to Hindi characters. Since this data is already in Latin script with each character capturing a unique pronunciation, changing this data to a form that replicates chat data using heuristic rules is trivial. However, this data is highly sanskritized and hence need to be changed by replacing Sanskrit words with equivalent Urdu words. This replacement is done by using online English to Urdu dictionaries (www.urduword.com and www.hamariweb.com). We have succeeded in replacing 20,000 pure Sanskrit words to Urdu by performing a manual lookup. The advantage with this method is that 1. The whole process of setting up annotation
guidelines and standards is eliminated. 2. The replacement of pure Hindi words with Urdu words in most cases is one-one and the POS assignment is retained without disturbing the entire structure of the sentence. Our training data now consists of Urdu words written in Latin script. We also generate phonemes for each word by running the phonetic model. A POS model is trained using CRF (Lafferty, 2001) learning method with current word, previous word and the phonemes as features. This model called the Latin Script (LS) POS model has an F-score of 83%.
English POS tagger is the Stanford tagger that has a tagging accuracy of about 98.7% 2 .</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Example 5.1 showcases a typical sentence seen in blog data.</text>
              <doc_id>129</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Example 5.2 shows the issue with spelling variations sometimes that occur in the same sentence Example 5.1: Otherwise humara bhi wohi haal hoga jo is time Palestine, Iraq, Afghanistan wagera ka hai ~ Otherwise our state will also be like what is in Palestine, Iraq, Afghanistan etc. are experiencing at this time Example 5.2: Shariyat ke aitebaar se bhi ghaur kia jaey tu aap ko ilm ho jaega key joh haraam khata hai uska dil kis tarhan ka hota hey ~ If you look at it from morals point of you too you will understand the heart of people who cheat A statistical POS tagger for blog data has to take into consideration spelling variations, mixing patterns and script change.</text>
              <doc_id>130</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The goal here is not to generate a perfect POS tagger for blog data (though the idea explained here can be extended for further improvisation) but to be able to identify POS categories that are candidates for switch and mix.</text>
              <doc_id>131</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The basic idea of our approach is as follows 1.</text>
              <doc_id>132</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Train Latin script POS tagger (LS tagger) on</text>
              <doc_id>133</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>pure Urdu Latin script data (Example 2 in table 1 &#8211; using Urdu POS tag set, Muaz et al., 2009) 2.</text>
              <doc_id>134</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Train English POS tagger on English data</text>
              <doc_id>135</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(based on English tag sets, Santorini, 1990) 3.</text>
              <doc_id>136</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Apply LS tagger and English tagger on Urdish</text>
              <doc_id>137</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>data and note the confidence measures of the applied tags on each word 4.</text>
              <doc_id>138</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Use confidence measures, LS tags, phoneme</text>
              <doc_id>139</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>codes (to accommodate spelling variations) as features to train a new learning model on Urdish data 5.</text>
              <doc_id>140</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Those words that get tagged with the English</text>
              <doc_id>141</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tagset are potential place holders for mixing patterns</text>
              <doc_id>142</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The training data needed to develop LS tagger for Urdu is obtained from Hindi.</text>
              <doc_id>143</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>IIIT POS annotated corpus for Hindi contains data in the SSF format (Shakti Standard Format) (Bharati, 2006).</text>
              <doc_id>144</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This format tries to capture the pronunciation information by assigning unique English characters to Hindi characters.</text>
              <doc_id>145</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Since this data is already in Latin script with each character capturing a unique pronunciation, changing this data to a form that replicates chat data using heuristic rules is trivial.</text>
              <doc_id>146</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>However, this data is highly sanskritized and hence need to be changed by replacing Sanskrit words with equivalent Urdu words.</text>
              <doc_id>147</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This replacement is done by using online English to Urdu dictionaries (www.urduword.com and www.hamariweb.com).</text>
              <doc_id>148</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We have succeeded in replacing 20,000 pure Sanskrit words to Urdu by performing a manual lookup.</text>
              <doc_id>149</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The advantage with this method is that 1.</text>
              <doc_id>150</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>The whole process of setting up annotation</text>
              <doc_id>151</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>guidelines and standards is eliminated.</text>
              <doc_id>152</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2.</text>
              <doc_id>153</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The replacement of pure Hindi words with Urdu words in most cases is one-one and the POS assignment is retained without disturbing the entire structure of the sentence.</text>
              <doc_id>154</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Our training data now consists of Urdu words written in Latin script.</text>
              <doc_id>155</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We also generate phonemes for each word by running the phonetic model.</text>
              <doc_id>156</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>A POS model is trained using CRF (Lafferty, 2001) learning method with current word, previous word and the phonemes as features.</text>
              <doc_id>157</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>This model called the Latin Script (LS) POS model has an F-score of 83%.</text>
              <doc_id>158</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>English POS tagger is the Stanford tagger that has a tagging accuracy of about 98.7% 2 .</text>
              <doc_id>159</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Approach</title>
            <text>Urdish blog data consists of Urdu code-mixed with English. Running simple Latin script based Urdu POS tagger results in 81.2% accuracy when POS tags on the entire corpus is considered and 52.3%
2 http://nlp.stanford.edu/software/tagger.shtml
accuracy on only the English words. Running English tagger on the entire corpus improves the POS tagging accuracy of English words to 79.2% accuracy. However, the tagging accuracy on the entire corpus reduces considerably &#8211; 55.4%. This indicates that identifying the language of the words will definitely improve tagging.
Identifying the language of the words can be done simply by a lexicon lookup. Since English words are easily accessible and more enriched, English Wordnet 3 makes a good source to perform this lookup. Running Latin script POS tagger and English tagger on the language specific words resulted in 79.82% accuracy for the entire corpus and 59.2% accuracy for English words. Clearly there is no significant gain in the performance. This is on account of English equivalent Urdu representation of words (e.g. key ~ their, more ~ peacock, bat ~ speak).
Since identifying the language explicitly yields less benefit, we showcase a new approach that is based on the confidence measures of the taggers. We first run the English POS tagger on the entire corpus. This tagger is trained using a CRF model. Scores that indicate the confidence with which this tagger has applied tags to each word in the corpus is also estimated (table 2). Next, the Latin script tagger is applied on the entire corpus and the confidence scores for the selected tags are estimated. So, for each word, there exist two tags, one from the English tagger and the other from the Latin script Urdish tagger along with their confidence scores. This becomes our training corpus.
The CRF learning model trained on the above corpus using features shown in table 3 generates a cross validation accuracy is 90.34%. The accuracy on the test set is 88.2%, clearly indicating the advantages of the statistical approach.
Features used to train Urdish POS tagger Urdish word
POS tag generated by LS tagger POS tag generated by English tagger Confidence measure by LS tagger Confidence measure by English tagger Double metaphone value Previous and next tags for English and Urdu Previous and next words Confidence priorities Table 3. Features used to train the final POS tagger for Urdish data
3 http://wordnet.princeton.edu/
Table 4 illustrates the POS categories used as potential pattern switching place holders
POS Category noun within a noun phrase
Interjection
Adjective
Adverb
Gerund (tagged as a verb by English POS tagger) Verb
Example uski life par itna control acha nahi hai ~ its not good to control his life this much Comon Reema yaar! ~ Hey Man Reema! lol! ~ lol Yeh story bahut hi scary or ugly tha ~ This story was really scary and ugly Babra Shareef ki koi bhi film lagti hai, hum definitely dekhtai ~ I would definitely watch any movie of Babra Shareef Yaha shooting mana hai ~ shooting is prohibited here
Iss movie main I dozed ~ I slept through the movie Verb Afridi.. Cool off! Table 4. POS categories that exhibit pattern switch</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Urdish blog data consists of Urdu code-mixed with English.</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Running simple Latin script based Urdu POS tagger results in 81.2% accuracy when POS tags on the entire corpus is considered and 52.3%</text>
                  <doc_id>161</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 http://nlp.stanford.edu/software/tagger.shtml</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>accuracy on only the English words.</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Running English tagger on the entire corpus improves the POS tagging accuracy of English words to 79.2% accuracy.</text>
                  <doc_id>164</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, the tagging accuracy on the entire corpus reduces considerably &#8211; 55.4%.</text>
                  <doc_id>165</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This indicates that identifying the language of the words will definitely improve tagging.</text>
                  <doc_id>166</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Identifying the language of the words can be done simply by a lexicon lookup.</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Since English words are easily accessible and more enriched, English Wordnet 3 makes a good source to perform this lookup.</text>
                  <doc_id>168</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Running Latin script POS tagger and English tagger on the language specific words resulted in 79.82% accuracy for the entire corpus and 59.2% accuracy for English words.</text>
                  <doc_id>169</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Clearly there is no significant gain in the performance.</text>
                  <doc_id>170</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This is on account of English equivalent Urdu representation of words (e.g. key ~ their, more ~ peacock, bat ~ speak).</text>
                  <doc_id>171</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Since identifying the language explicitly yields less benefit, we showcase a new approach that is based on the confidence measures of the taggers.</text>
                  <doc_id>172</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We first run the English POS tagger on the entire corpus.</text>
                  <doc_id>173</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This tagger is trained using a CRF model.</text>
                  <doc_id>174</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Scores that indicate the confidence with which this tagger has applied tags to each word in the corpus is also estimated (table 2).</text>
                  <doc_id>175</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Next, the Latin script tagger is applied on the entire corpus and the confidence scores for the selected tags are estimated.</text>
                  <doc_id>176</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>So, for each word, there exist two tags, one from the English tagger and the other from the Latin script Urdish tagger along with their confidence scores.</text>
                  <doc_id>177</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>This becomes our training corpus.</text>
                  <doc_id>178</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The CRF learning model trained on the above corpus using features shown in table 3 generates a cross validation accuracy is 90.34%.</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The accuracy on the test set is 88.2%, clearly indicating the advantages of the statistical approach.</text>
                  <doc_id>180</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Features used to train Urdish POS tagger Urdish word</text>
                  <doc_id>181</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>POS tag generated by LS tagger POS tag generated by English tagger Confidence measure by LS tagger Confidence measure by English tagger Double metaphone value Previous and next tags for English and Urdu Previous and next words Confidence priorities Table 3.</text>
                  <doc_id>182</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Features used to train the final POS tagger for Urdish data</text>
                  <doc_id>183</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3 http://wordnet.princeton.edu/</text>
                  <doc_id>184</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 4 illustrates the POS categories used as potential pattern switching place holders</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>POS Category noun within a noun phrase</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Interjection</text>
                  <doc_id>187</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Adjective</text>
                  <doc_id>188</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Adverb</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Gerund (tagged as a verb by English POS tagger) Verb</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Example uski life par itna control acha nahi hai ~ its not good to control his life this much Comon Reema yaar!</text>
                  <doc_id>191</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>~ Hey Man Reema!</text>
                  <doc_id>192</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>lol!</text>
                  <doc_id>193</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>~ lol Yeh story bahut hi scary or ugly tha ~ This story was really scary and ugly Babra Shareef ki koi bhi film lagti hai, hum definitely dekhtai ~ I would definitely watch any movie of Babra Shareef Yaha shooting mana hai ~ shooting is prohibited here</text>
                  <doc_id>194</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Iss movie main I dozed ~ I slept through the movie Verb Afridi.. Cool off!</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Table 4.</text>
                  <doc_id>196</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>POS categories that exhibit pattern switch</text>
                  <doc_id>197</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>6</index>
        <title>6 Sentiment Polarity Detection</title>
        <text>The main goal of this work is to perform sentiment analysis in Urdu blog data. However, this task is not trivial owing to all the peculiarities that blog data exhibits. The work done on Urdu sentiment analysis (Mukund and Srihari, 2010) provided annotated data for sentiments in newswire domain. . Newspaper data make a good corpus to analyze different kinds of emotions and emotional traits of the people. They reflect the collective sentiments and emotions of the people and in turn the society to which they cater. When specific frames are considered (such as semantic verb frames) in the context of the triggering entities &#8211; opinion holders (entities who express these emotions) and opinion targets (entities towards whom the emotion is directed) - performing sentiment analysis becomes more meaningful and newspapers make an excellent source to analyze such phenomena (Mukund et al., 2011). We use SCL to transfer sentiment analysis learning from this newswire data to blog data. Inspired by the work done by (Prettenhofer and Stein, 2010), we rely on oracles to generate pivot pairs. A pivot pair {w S , w T } where w S &#1013; V S (the source language &#8211; Urdu newswire data) and w T &#1013; V T (the target language &#8211; Urdish data) should satisfy two conditions 1. high support and 2. high confidence, making sure that the pairs are predictive of the task.
Prettenhofer and Stein (2010) used a simple translation oracle in their experiments. However there exist several challenges with Urdish data that inhibits the use of a simple translation oracle.
1. Script difference in the source and target languages. Source corpus (Urdu) is written in Nastaleeq and the target corpus (Urdish) is written in ASCII
2. Spelling variations in roman Urdu 3. Frequent use of English words to express
strong emotions We use two oracles to generate pivot pairs. The first oracle accommodates the issue with spelling variations. Each Urdu word is converted to roman Urdu using IPA (1999) guidelines. Using the double metaphone algorithm 4 phoneme code for the Urdu word is determined. This is also applied to Urdish data at the target end. Words that have the same metaphone code across the source and target languages are considered pivot pairs.
The second oracle is a simple translation oracle between Urdu and English. Our first experiment (experiment 1) is using words that belong to the adjective part of speech category as candidates for pivots. We augment this set to include words that belong to other POS categories shown in table 4 that exhibit pattern mixing (experiment 2).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The main goal of this work is to perform sentiment analysis in Urdu blog data.</text>
              <doc_id>198</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, this task is not trivial owing to all the peculiarities that blog data exhibits.</text>
              <doc_id>199</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The work done on Urdu sentiment analysis (Mukund and Srihari, 2010) provided annotated data for sentiments in newswire domain.</text>
              <doc_id>200</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>201</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Newspaper data make a good corpus to analyze different kinds of emotions and emotional traits of the people.</text>
              <doc_id>202</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>They reflect the collective sentiments and emotions of the people and in turn the society to which they cater.</text>
              <doc_id>203</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>When specific frames are considered (such as semantic verb frames) in the context of the triggering entities &#8211; opinion holders (entities who express these emotions) and opinion targets (entities towards whom the emotion is directed) - performing sentiment analysis becomes more meaningful and newspapers make an excellent source to analyze such phenomena (Mukund et al., 2011).</text>
              <doc_id>204</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>We use SCL to transfer sentiment analysis learning from this newswire data to blog data.</text>
              <doc_id>205</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Inspired by the work done by (Prettenhofer and Stein, 2010), we rely on oracles to generate pivot pairs.</text>
              <doc_id>206</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>A pivot pair {w S , w T } where w S &#1013; V S (the source language &#8211; Urdu newswire data) and w T &#1013; V T (the target language &#8211; Urdish data) should satisfy two conditions 1. high support and 2. high confidence, making sure that the pairs are predictive of the task.</text>
              <doc_id>207</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Prettenhofer and Stein (2010) used a simple translation oracle in their experiments.</text>
              <doc_id>208</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However there exist several challenges with Urdish data that inhibits the use of a simple translation oracle.</text>
              <doc_id>209</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1.</text>
              <doc_id>210</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Script difference in the source and target languages.</text>
              <doc_id>211</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Source corpus (Urdu) is written in Nastaleeq and the target corpus (Urdish) is written in ASCII</text>
              <doc_id>212</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.</text>
              <doc_id>213</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Spelling variations in roman Urdu 3.</text>
              <doc_id>214</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Frequent use of English words to express</text>
              <doc_id>215</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>strong emotions We use two oracles to generate pivot pairs.</text>
              <doc_id>216</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The first oracle accommodates the issue with spelling variations.</text>
              <doc_id>217</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Each Urdu word is converted to roman Urdu using IPA (1999) guidelines.</text>
              <doc_id>218</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Using the double metaphone algorithm 4 phoneme code for the Urdu word is determined.</text>
              <doc_id>219</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This is also applied to Urdish data at the target end.</text>
              <doc_id>220</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Words that have the same metaphone code across the source and target languages are considered pivot pairs.</text>
              <doc_id>221</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The second oracle is a simple translation oracle between Urdu and English.</text>
              <doc_id>222</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our first experiment (experiment 1) is using words that belong to the adjective part of speech category as candidates for pivots.</text>
              <doc_id>223</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We augment this set to include words that belong to other POS categories shown in table 4 that exhibit pattern mixing (experiment 2).</text>
              <doc_id>224</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>6.1 Implementation</title>
            <text>The feature used to train the learning algorithm is limited to unigrams. For linear classification, we use libSVM (Chang and Lin, 2011). The computational bottleneck of this method is in the SVD decomposition of the dense parameter matrix W. We set the negative values of W to zero to get a sparse representation of the matrix. For SVD computation the Lanczos algorithm provided by SVDLIBC 5 is employed. Each feature matrix used in libSVM is scaled between -1 and 1 and the final matrix for SVD is standardized to zero mean and unit variance estimated on D S U D u (source subset and target subset).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The feature used to train the learning algorithm is limited to unigrams.</text>
                  <doc_id>225</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For linear classification, we use libSVM (Chang and Lin, 2011).</text>
                  <doc_id>226</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The computational bottleneck of this method is in the SVD decomposition of the dense parameter matrix W.</text>
                  <doc_id>227</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We set the negative values of W to zero to get a sparse representation of the matrix.</text>
                  <doc_id>228</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For SVD computation the Lanczos algorithm provided by SVDLIBC 5 is employed.</text>
                  <doc_id>229</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Each feature matrix used in libSVM is scaled between -1 and 1 and the final matrix for SVD is standardized to zero mean and unit variance estimated on D S U D u (source subset and target subset).</text>
                  <doc_id>230</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>6.2 Results</title>
            <text>The domain of the source data set is limited to cricket and movies in order to ensure domain over-
4 http://en.wikipedia.org/wiki/Double_Metaphone 5 http://tedlab.mit.edu/~dr/SVDLIBC
lap between newswire data that we have and blog data. In order to benchmark the proposed technique, our baseline technique is based on the conventional method of supervised learning approach on annotated data. Urdish data set used for polarity classification contains 705 sentences written in ASCII format (example 6.1). This corpus is manually annotated by one annotator (purely based on intuition and does not follow any predefined annotation guidelines) to get 440 negative sentences and 265 positive sentences. The annotated corpus is purely used for testing and in this work considered as unlabeled data. A suitable linear kernel based support vector machine is modeled on the annotated data and a five-fold cross validation on this set gives an F-Measure of 64.3%. Example 6.1: General zia-ul-haq ke zamane mai qabayli elaqe Russia ke khilaf jang ka merkaz thea aur general Pervez Musharraf ke zamane mai ye qabayli elaqe Pakistan ke khilaf jang ka markaz ban gye . ~ negative
Our first experiment is based on using the second oracle for translations on only adjectives (most obvious choice for emotion words). We use 438 pivot pairs. The average F-measure for the performance is at 55.78% which is still much below the baseline performance of 64.3% if we had access to annotated data. However, the results show the ability of this method.
Our second experiment expands the power of the second oracle to provide translations to other POS categories that exhibit pattern switching. This increased the number of pivot pairs to 640. Increase in pivots improved the precision. Also we see significant improvement in the recall. The newly added pivots brought more sentences under the radar of the transfer model. The average F- Measure increased to 59.71%. The approach can be further enhanced by improving the oracle used to select pivot features. One way is add more pivot pairs based on the correlation in the topic space across language domains (future work).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The domain of the source data set is limited to cricket and movies in order to ensure domain over-</text>
                  <doc_id>231</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4 http://en.wikipedia.org/wiki/Double_Metaphone 5 http://tedlab.mit.edu/~dr/SVDLIBC</text>
                  <doc_id>232</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>lap between newswire data that we have and blog data.</text>
                  <doc_id>233</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In order to benchmark the proposed technique, our baseline technique is based on the conventional method of supervised learning approach on annotated data.</text>
                  <doc_id>234</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Urdish data set used for polarity classification contains 705 sentences written in ASCII format (example 6.1).</text>
                  <doc_id>235</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This corpus is manually annotated by one annotator (purely based on intuition and does not follow any predefined annotation guidelines) to get 440 negative sentences and 265 positive sentences.</text>
                  <doc_id>236</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The annotated corpus is purely used for testing and in this work considered as unlabeled data.</text>
                  <doc_id>237</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>A suitable linear kernel based support vector machine is modeled on the annotated data and a five-fold cross validation on this set gives an F-Measure of 64.3%.</text>
                  <doc_id>238</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Example 6.1: General zia-ul-haq ke zamane mai qabayli elaqe Russia ke khilaf jang ka merkaz thea aur general Pervez Musharraf ke zamane mai ye qabayli elaqe Pakistan ke khilaf jang ka markaz ban gye .</text>
                  <doc_id>239</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>~ negative</text>
                  <doc_id>240</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Our first experiment is based on using the second oracle for translations on only adjectives (most obvious choice for emotion words).</text>
                  <doc_id>241</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We use 438 pivot pairs.</text>
                  <doc_id>242</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The average F-measure for the performance is at 55.78% which is still much below the baseline performance of 64.3% if we had access to annotated data.</text>
                  <doc_id>243</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>However, the results show the ability of this method.</text>
                  <doc_id>244</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Our second experiment expands the power of the second oracle to provide translations to other POS categories that exhibit pattern switching.</text>
                  <doc_id>245</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This increased the number of pivot pairs to 640.</text>
                  <doc_id>246</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Increase in pivots improved the precision.</text>
                  <doc_id>247</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Also we see significant improvement in the recall.</text>
                  <doc_id>248</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The newly added pivots brought more sentences under the radar of the transfer model.</text>
                  <doc_id>249</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The average F- Measure increased to 59.71%.</text>
                  <doc_id>250</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The approach can be further enhanced by improving the oracle used to select pivot features.</text>
                  <doc_id>251</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>One way is add more pivot pairs based on the correlation in the topic space across language domains (future work).</text>
                  <doc_id>252</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>7</index>
        <title>7 Conclusion</title>
        <text>In this work we show a way to perform sentiment analysis in blog data by using the method of structural correspondence learning. This method accommodates the various issues with blog data such as spelling variations, script difference, pattern switching.
We rely on two oracles, one that takes care of spelling variations and the other that provides translations. The words that are selected to be translated by the second oracle are carefully chosen based on POS categories that exhibit emotions and pattern switching. We show that the performance of this approach is comparable to what is achieved by training a supervised learning model. In order to identify the POS categories that exhibit pattern switching, we developed a statistical POS tagger for Urdish blog data using a method that does not require annotated data in the target language. Through these two modules (sentiment analysis and POS tagger for Urdish data) we successfully show that the efforts in performing nontopical analysis in Urdu newswire data can easily be extended to work on Urdish data.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this work we show a way to perform sentiment analysis in blog data by using the method of structural correspondence learning.</text>
              <doc_id>253</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This method accommodates the various issues with blog data such as spelling variations, script difference, pattern switching.</text>
              <doc_id>254</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We rely on two oracles, one that takes care of spelling variations and the other that provides translations.</text>
              <doc_id>255</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The words that are selected to be translated by the second oracle are carefully chosen based on POS categories that exhibit emotions and pattern switching.</text>
              <doc_id>256</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We show that the performance of this approach is comparable to what is achieved by training a supervised learning model.</text>
              <doc_id>257</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In order to identify the POS categories that exhibit pattern switching, we developed a statistical POS tagger for Urdish blog data using a method that does not require annotated data in the target language.</text>
              <doc_id>258</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Through these two modules (sentiment analysis and POS tagger for Urdish data) we successfully show that the efforts in performing nontopical analysis in Urdu newswire data can easily be extended to work on Urdish data.</text>
              <doc_id>259</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>8</index>
        <title>8 Future work</title>
        <text>Analyzing the test data set for missing and false positives, here are some of the examples of where the model did not work Example 7.1: &#8220;tring tring tring tring.. Phone to bar bar bajta hai. Annoying.&#8221; ~ tring tring tring tring tring.. the phone rings repeatedly. Annoying. Example 7.2: &#8220;bookon ko padna tho ab na mumkin hai. Yaha thak mere friends mujhe blindee pukarthey hai&#8221; ~ cannot read books any more. Infact, my friends call me blindee. Example 7.3: &#8220;Ek Tamana Hai Ke Faqt Mujh Pe Mehrban Raho, Tum Kise Or Ko Dekho To Bura Lagta Hai&#8221; ~ I have this one wish that destiny be kind to me If you see someone else I feel bad Our method fails to tag sentences like in example 7.1 where English verbs are used by themselves. Our POS tagger fails to capture such stand-alone 7 verbs as verbs but tags them as nouns. Hence, doesn&#8217;t occur in the pivot set. Our second issue is with Morpho syntactic switching, a behavior seen in example 7.2. Nadhkarni (1975) and Pandaripande (1983) have shown that when two or more languages come into contact, there is mutual feature transfer from one language to another. The languages influence each other considerably and constraints associated with free morphemes fail in most cases. The direction and frequency of influence depends on the social status associated with the languages used in mixing. The language that has a high social status tends to use the morphemes of the lower language. Example 7.4: Bookon &#8211; in books, Fileon &#8211; in files, Companiyaa &#8211; many companies Clearly we can see that English words due to their frequent contact with Urdu grammatical system tend to adopt the morphology associated with the base language and used mostly as native Urdu words. These are some issues, if addressed, will definitely improve the performance of the sentiment analysis model in Urdish data.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Analyzing the test data set for missing and false positives, here are some of the examples of where the model did not work Example 7.1: &#8220;tring tring tring tring.. Phone to bar bar bajta hai.</text>
              <doc_id>260</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Annoying.&#8221; ~ tring tring tring tring tring.. the phone rings repeatedly.</text>
              <doc_id>261</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Annoying.</text>
              <doc_id>262</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Example 7.2: &#8220;bookon ko padna tho ab na mumkin hai.</text>
              <doc_id>263</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Yaha thak mere friends mujhe blindee pukarthey hai&#8221; ~ cannot read books any more.</text>
              <doc_id>264</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Infact, my friends call me blindee.</text>
              <doc_id>265</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Example 7.3: &#8220;Ek Tamana Hai Ke Faqt Mujh Pe Mehrban Raho, Tum Kise Or Ko Dekho To Bura Lagta Hai&#8221; ~ I have this one wish that destiny be kind to me If you see someone else I feel bad Our method fails to tag sentences like in example 7.1 where English verbs are used by themselves.</text>
              <doc_id>266</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Our POS tagger fails to capture such stand-alone 7 verbs as verbs but tags them as nouns.</text>
              <doc_id>267</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Hence, doesn&#8217;t occur in the pivot set.</text>
              <doc_id>268</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Our second issue is with Morpho syntactic switching, a behavior seen in example 7.2.</text>
              <doc_id>269</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Nadhkarni (1975) and Pandaripande (1983) have shown that when two or more languages come into contact, there is mutual feature transfer from one language to another.</text>
              <doc_id>270</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>The languages influence each other considerably and constraints associated with free morphemes fail in most cases.</text>
              <doc_id>271</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>The direction and frequency of influence depends on the social status associated with the languages used in mixing.</text>
              <doc_id>272</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>The language that has a high social status tends to use the morphemes of the lower language.</text>
              <doc_id>273</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>Example 7.4: Bookon &#8211; in books, Fileon &#8211; in files, Companiyaa &#8211; many companies Clearly we can see that English words due to their frequent contact with Urdu grammatical system tend to adopt the morphology associated with the base language and used mostly as native Urdu words.</text>
              <doc_id>274</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>These are some issues, if addressed, will definitely improve the performance of the sentiment analysis model in Urdish data.</text>
              <doc_id>275</doc_id>
              <sec_id>15</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Different forms of using Urdu lan- guage on the internet  Blog data follows the order shown in example  4 of table 1. Such a code-switching phenomenon is  very  common  in  multilingual  societies  that  have  significant  exposure  to  English.  Other  languages  exhibiting  similar  behaviors  are  Hinglish  (Hindi  and  English),  Arabic  with  English  and  Spanglish  (Spanish with English).</caption>
        <reference_text>In PAGE 2: ...  English  words  and  phrases  are  com- monly used in the flow integrating tightly with the  base language.  Table1  shows examples of differ- ent flavors in which Urdu appears in the internet.  Differ- ent  Forms  of Data  Main Issues  Example Sentence  1....</reference_text>
        <page_num>2</page_num>
        <head>
          <rows>
            <row>
              <cell>(Eng-  lish)</cell>
              <cell>2.  No normalization    standards    3.  Preprocessing    modules needed if    tools for Urdu in    Nastaliq are to be    used    4.  Developing a    completely new NLP    framework needs    annotated data</cell>
              <cell>None</cell>
              <cell>ki Qurbanian dey ker    hasil kia usi mulk    main yai kaisa waqt a    gay hai ?      [Look at what kind of    time the land that had    1000000?s of people    sacrifice their lives is    experiencing now]</cell>
              <cell>ki Qurbanian dey ker    hasil kia usi mulk    main yai kaisa waqt a    gay hai ?      [Look at what kind of    time the land that had    1000000?s of people    sacrifice their lives is    experiencing now]</cell>
              <cell>ki Qurbanian dey ker    hasil kia usi mulk    main yai kaisa waqt a    gay hai ?      [Look at what kind of    time the land that had    1000000?s of people    sacrifice their lives is    experiencing now]</cell>
              <cell>ki Qurbanian dey ker    hasil kia usi mulk    main yai kaisa waqt a        [Look at what kind of    time the land that had    1000000?s of people    sacrifice their lives is    experiencing now]</cell>
              <cell>ki Qurbanian dey ker     hasil kia usi mulk     main yai kaisa waqt a           [Look at what kind of     time the land that had     1000000?s of people     sacrifice their lives is     experiencing now]</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>3. Urd-</cell>
              <cell>1.  No combined</cell>
              <cell>None</cell>
              <cell>??</cell>
              <cell>???</cell>
              <cell>None</cell>
              <cell>?????</cell>
              <cell>??</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>???</cell>
              <cell>???</cell>
              <cell>???</cell>
            </row>
            <row>
              <cell>ten in</cell>
              <cell>English and Urdu</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>simultaneously</cell>
              <cell>None</cell>
              <cell>[the phones rang one</cell>
              <cell>[the phones rang one</cell>
              <cell>[the phones rang one</cell>
              <cell>[the phones rang one</cell>
              <cell>[the phones rang one</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>2.  English is written</cell>
              <cell>None</cell>
              <cell>after the other in the</cell>
              <cell>after the other in the</cell>
              <cell>after the other in the</cell>
              <cell>after the other in the</cell>
              <cell>after the other in the</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>in Urdu but with</cell>
              <cell>None</cell>
              <cell>TV station]</cell>
              <cell>TV station]</cell>
              <cell>TV station]</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>missing diacritics</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>1.  No combined</cell>
              <cell>None</cell>
              <cell>Afsoos key baat hai .</cell>
              <cell>Afsoos key baat hai .</cell>
              <cell>Afsoos key baat hai .</cell>
              <cell>Afsoos key baat hai .</cell>
              <cell>Afsoos key baat hai .</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>parser that deals</cell>
              <cell>None</cell>
              <cell>kal tak jo batain</cell>
              <cell>kal tak jo batain</cell>
              <cell>kal tak jo batain</cell>
              <cell>kal tak jo batain</cell>
            </row>
            <row>
              <cell>ten in</cell>
              <cell>with English and</cell>
              <cell>None</cell>
              <cell>Non Muslim bhi</cell>
              <cell>Non Muslim bhi</cell>
              <cell>Non Muslim bhi</cell>
              <cell>Non Muslim bhi</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Urdu simultaneous-</cell>
              <cell>None</cell>
              <cell>kartay hoay dartay</cell>
              <cell>kartay hoay dartay</cell>
              <cell>kartay hoay dartay</cell>
              <cell>kartay hoay dartay</cell>
              <cell>kartay hoay dartay</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>ly</cell>
              <cell>None</cell>
              <cell>thay abhi this man</cell>
              <cell>thay abhi this man</cell>
              <cell>thay abhi this man</cell>
              <cell>thay abhi this man</cell>
              <cell>thay abhi this man</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>2.  Issue of spelling</cell>
              <cell>None</cell>
              <cell>has brought it out in</cell>
              <cell>has brought it out in</cell>
              <cell>has brought it out in</cell>
              <cell>has brought it out in</cell>
              <cell>has brought it out in</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>the open.</cell>
              <cell>the open.</cell>
              <cell>the open.</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>to be normalized</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>[It is sad to see that</cell>
              <cell>[It is sad to see that</cell>
              <cell>[It is sad to see that</cell>
              <cell>[It is sad to see that</cell>
              <cell>[It is sad to see that</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>those words that even</cell>
              <cell>those words that even</cell>
              <cell>those words that even</cell>
              <cell>those words that even</cell>
              <cell>those words that even</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>a non muslim would</cell>
              <cell>a non muslim would</cell>
              <cell>a non muslim would</cell>
              <cell>a non muslim would</cell>
              <cell>a non muslim would</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>fear to utter till yes-</cell>
              <cell>fear to utter till yes-</cell>
              <cell>fear to utter till yes-</cell>
              <cell>fear to utter till yes-</cell>
              <cell>fear to utter till yes-</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>terday, this man had</cell>
              <cell>terday, this man had</cell>
              <cell>terday, this man had</cell>
              <cell>terday, this man had</cell>
              <cell>terday, this man had</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>brought it out in the</cell>
              <cell>brought it out in the</cell>
              <cell>brought it out in the</cell>
              <cell>brought it out in the</cell>
              <cell>brought it out in the</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>open]</cell>
              <cell>open]</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2. POS tagger with confidence measures</caption>
        <reference_text>None</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>Zardari</cell>
              <cell>NNP</cell>
              <cell>NNP</cell>
              <cell>NN</cell>
              <cell>0.69</cell>
              <cell>0.18</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>ko</cell>
              <cell>PSP</cell>
              <cell>NNP</cell>
              <cell>PSP</cell>
              <cell>0.99</cell>
              <cell>0.28</cell>
            </row>
            <row>
              <cell>shoot</cell>
              <cell>VB</cell>
              <cell>NNP</cell>
              <cell>JJ</cell>
              <cell>0.54</cell>
              <cell>0.29</cell>
            </row>
            <row>
              <cell>ker</cell>
              <cell>NN</cell>
              <cell>NNP</cell>
              <cell>NN</cell>
              <cell>0.73</cell>
              <cell>0.29</cell>
            </row>
            <row>
              <cell>dena</cell>
              <cell>VM</cell>
              <cell>NNP</cell>
              <cell>VM</cell>
              <cell>0.83</cell>
              <cell>0.29</cell>
            </row>
            <row>
              <cell>chahiya</cell>
              <cell>VAUX</cell>
              <cell>NNP</cell>
              <cell>VAUX</cell>
              <cell>0.98</cell>
              <cell>0.21</cell>
            </row>
            <row>
              <cell>.</cell>
              <cell>SYM</cell>
              <cell>.</cell>
              <cell>SYM</cell>
              <cell>0.99</cell>
              <cell>0.99</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>M Abdul-Mageed</author>
          <author>M Diab</author>
          <author>M Korayem</author>
        </authors>
        <title>Subjectivity and Sentiment Analysis of Modern Standard Arabic.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Dipti M Sharma</author>
        </authors>
        <title>None</title>
        <publication>In proceedings of the 49th Meeting of ACL.</publication>
        <pages>None</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Shakti Analyser SSF Representation Blitzer</author>
          <author>Ryan McDonald John</author>
          <author>Fernando Pereira</author>
        </authors>
        <title>Domain adaptation with structural correspondence learning.</title>
        <publication>In proceedings of the 2006 Conference on EMNLP,</publication>
        <pages>pp.</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Australia Chang Sydney</author>
          <author>Chih-Jen Lin Chih-Chung</author>
        </authors>
        <title>LIBSVM: a library for support vector machines.</title>
        <publication>None</publication>
        <pages>1--27</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>P E Dussias</author>
        </authors>
        <title>Spanish-English code-mixing at the auxiliary phrase: Evidence from eye-movements. Revista Internacional de Ling&#252;&#237;stica Iberoamerican.</title>
        <publication>None</publication>
        <pages>7--34</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>P Shukla Sharma</author>
          <author>K Vikram</author>
        </authors>
        <title>Saarthaka - A Bilingual Parser for Hindi, English and code-switching structures.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Daniel Marcu</author>
        </authors>
        <title>Domain adaptation for statistical classifiers.</title>
        <publication>In proceedings of the 11th Conference of the ECAL Hal Daume III</publication>
        <pages>101--126</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Istanbul LREC</author>
          <author>Turkey Kachru</author>
          <author>Braj</author>
        </authors>
        <title>Conjunct verbs; verbs or verb phrases?.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1978</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>F</author>
        </authors>
        <title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
        <publication>In proceedings of the XIIth International Congress of Linguistics.</publication>
        <pages>366--70</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Smruthi Mukund</author>
          <author>Rohini K Srihari</author>
        </authors>
        <title>A Vector Space Model for Subjectivity Classification in Urdu aided by CoTraining,</title>
        <publication>In proceedings of the 23rd COLING,</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>R Pandaripande</author>
        </authors>
        <title>Syntax and Semantics of the Passive Construction in selected South Asian Languages.</title>
        <publication>In Proceedings of CONLL Nadkarni, Mangesh. 1975. Bilingualism and Syntactic Change in Konkani Language,</publication>
        <pages>672--683</pages>
        <date>1981</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Peter Prettenhofer</author>
          <author>Benno Stein</author>
        </authors>
        <title>Cross-Lingual Adaptation Using Structural Correspondence Learning.</title>
        <publication>In proceedings of ACL Rasul, Sarwat. 2006. Language Hybridization and Code Mixing in Pakistani Talk Shows. Bahaudin Zakriya University Journal 2nd Issue.</publication>
        <pages>29--41</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Tong Zhang</author>
        </authors>
        <title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
        <publication>In Proceedings of the 2003 Conference of NAACL, HLT -Volume 1 (NAACL &amp;apos;03) Rie-K. Ando</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>13</id>
        <authors/>
        <title>Part-of-speech tagging guidelines for the Penn Treebank Project.</title>
        <publication>University of Pennsylvania, 3rd Revision, 2nd Printing.</publication>
        <pages>1817--1853</pages>
        <date>1990</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Nobuyuki Shimizu</author>
          <author>Hiroshi Nakagawa</author>
        </authors>
        <title>Structural Correspondence Learning for Dependency Parsing.</title>
        <publication>In proceedings of CoNLL Shared Task Session of EMNLPCoNLL.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>2</reference_id>
        <string>Blitzer et al., 2006</string>
        <sentence_id>48952</sentence_id>
        <char_offset>252</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>2</reference_id>
        <string>Blitzer et al., 2006</string>
        <sentence_id>48957</sentence_id>
        <char_offset>114</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Blitzer et al., (2006)</string>
        <sentence_id>48961</sentence_id>
        <char_offset>26</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>4</reference_id>
        <string>Dussias (2003)</string>
        <sentence_id>49002</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>6</reference_id>
        <string>Marcu, 2006</string>
        <sentence_id>48952</sentence_id>
        <char_offset>215</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>9</reference_id>
        <string>Mukund and Srihari, 2010</string>
        <sentence_id>48907</sentence_id>
        <char_offset>203</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>9</reference_id>
        <string>Mukund and Srihari, 2010</string>
        <sentence_id>49110</sentence_id>
        <char_offset>42</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>11</reference_id>
        <string>Prettenhofer and Stein, 2010</string>
        <sentence_id>49116</sentence_id>
        <char_offset>30</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>11</reference_id>
        <string>Prettenhofer and Stein (2010)</string>
        <sentence_id>48970</sentence_id>
        <char_offset>101</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>11</reference_id>
        <string>Prettenhofer and Stein (2010)</string>
        <sentence_id>48976</sentence_id>
        <char_offset>28</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>11</reference_id>
        <string>Prettenhofer and Stein (2010)</string>
        <sentence_id>49118</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>12</reference_id>
        <string>Zhang (2005)</string>
        <sentence_id>48959</sentence_id>
        <char_offset>125</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>14</reference_id>
        <string>Shimizu and Nakagawa, 2007</string>
        <sentence_id>48961</sentence_id>
        <char_offset>121</char_offset>
      </citation>
    </citations>
  </content>
</document>
