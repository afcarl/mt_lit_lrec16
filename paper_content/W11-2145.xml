<document>
  <filename>W11-2145</filename>
  <authors/>
  <title>None</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>1 Introduction</title>
        <text>In this paper we describe our systems for the EMNLP 2011 Sixth Workshop on Statistical Machine Translation. We participated in the Shared Translation Task and submitted translations for English&#8596;German and English&#8596;French. We use a phrase-based decoder that can use lattices as input and developed several models that extend the standard log-linear model combination of phrase-based MT. These include advanced reordering models and corresponding adaptations to the phrase extraction process as well as extension to the translation and language model in form of discriminative word alignment and a bilingual language model to extend source word context. For English-German, language models based on fine-grained part-of-speech tags were used to address the difficult target language generation due to the rich morphology of German.
We also present a filtering method directly addressing the problems of web-crawled corpora, which enabled us to make use of the French-English Giga corpus. Another novelty in our systems this year is the parallel phrase scoring method that reduces the time needed for training which is especially convenient for such big corpora as the Giga corpus.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper we describe our systems for the EMNLP 2011 Sixth Workshop on Statistical Machine Translation.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We participated in the Shared Translation Task and submitted translations for English&#8596;German and English&#8596;French.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We use a phrase-based decoder that can use lattices as input and developed several models that extend the standard log-linear model combination of phrase-based MT.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>These include advanced reordering models and corresponding adaptations to the phrase extraction process as well as extension to the translation and language model in form of discriminative word alignment and a bilingual language model to extend source word context.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>For English-German, language models based on fine-grained part-of-speech tags were used to address the difficult target language generation due to the rich morphology of German.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We also present a filtering method directly addressing the problems of web-crawled corpora, which enabled us to make use of the French-English Giga corpus.</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Another novelty in our systems this year is the parallel phrase scoring method that reduces the time needed for training which is especially convenient for such big corpora as the Giga corpus.</text>
              <doc_id>6</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>2 System Description</title>
        <text>The baseline systems for all languages use a translation model that is trained on EPPS and the News Commentary corpus and the phrase table is based on a GIZA++ word alignment. The language model was trained on the monolingual parts of the same corpora by the SRILM Toolkit (Stolcke, 2002). It is a 4-gram SRI language model using Kneser-Ney smoothing.
The problem of word reordering is addressed using the POS-based reordering model as described in Section 2.4. The part-of-speech tags for the reordering model are obtained using the TreeTagger (Schmid, 1994).
An in-house phrase-based decoder (Vogel, 2003) is used to perform translation and optimization with regard to the BLEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). During decoding only the top 20 translation options for every source phrase were considered.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The baseline systems for all languages use a translation model that is trained on EPPS and the News Commentary corpus and the phrase table is based on a GIZA++ word alignment.</text>
              <doc_id>7</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The language model was trained on the monolingual parts of the same corpora by the SRILM Toolkit (Stolcke, 2002).</text>
              <doc_id>8</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>It is a 4-gram SRI language model using Kneser-Ney smoothing.</text>
              <doc_id>9</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The problem of word reordering is addressed using the POS-based reordering model as described in Section 2.4.</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The part-of-speech tags for the reordering model are obtained using the TreeTagger (Schmid, 1994).</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>An in-house phrase-based decoder (Vogel, 2003) is used to perform translation and optimization with regard to the BLEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005).</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>During decoding only the top 20 translation options for every source phrase were considered.</text>
              <doc_id>13</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Data</title>
            <text>We trained all systems using the parallel EPPS and News Commentary corpora. In addition, the UN corpus and the Giga corpus were used for training
the French-English systems.
Optimization was done for most languages using the news-test2008 data set and news-test2010 was used as test set. The only exception is German- English, where news-test2009 was used for optimization due to system combination arrangements. The language models for the baseline systems were trained on the monolingual versions of the training corpora. Later on, we used the News Shuffle and the Gigaword corpus to train bigger language models. For training a discriminative word alignment model, a small amount of hand-aligned data was used.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We trained all systems using the parallel EPPS and News Commentary corpora.</text>
                  <doc_id>14</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, the UN corpus and the Giga corpus were used for training</text>
                  <doc_id>15</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the French-English systems.</text>
                  <doc_id>16</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Optimization was done for most languages using the news-test2008 data set and news-test2010 was used as test set.</text>
                  <doc_id>17</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The only exception is German- English, where news-test2009 was used for optimization due to system combination arrangements.</text>
                  <doc_id>18</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The language models for the baseline systems were trained on the monolingual versions of the training corpora.</text>
                  <doc_id>19</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Later on, we used the News Shuffle and the Gigaword corpus to train bigger language models.</text>
                  <doc_id>20</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For training a discriminative word alignment model, a small amount of hand-aligned data was used.</text>
                  <doc_id>21</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Preprocessing</title>
            <text>The training data is preprocessed prior to training the system. This includes normalizing special symbols, smart-casing the first words of each sentence and removing long sentences and sentences with length mismatch.
For the German parts of the training corpus we use the hunspell 1 lexicon to map words written according to old German spelling to new German spelling, to obtain a corpus with homogenous spelling. Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The training data is preprocessed prior to training the system.</text>
                  <doc_id>22</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This includes normalizing special symbols, smart-casing the first words of each sentence and removing long sentences and sentences with length mismatch.</text>
                  <doc_id>23</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For the German parts of the training corpus we use the hunspell 1 lexicon to map words written according to old German spelling to new German spelling, to obtain a corpus with homogenous spelling.</text>
                  <doc_id>24</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words.</text>
                  <doc_id>25</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>2.3 Special filtering of the Giga parallel Corpus</title>
            <text>The Giga corpus incorporates non-neglegible amounts of noise even after our usual preprocessing. This noise may be due to different causes. For instance: non-standard HTML characters, meaningless parts composed of only hypertext codes, sentences which are only partial translation of the source, or eventually not a correct translation at all. Such noisy pairs potentially degrade the translation model quality, therefore it seemed more convenient to eliminate them.
Given the size of the corpus, this task could not be performed manually. Consequently, we used an automatic classifier inspired by the work of Munteanu and Marcu (2005) on comparable corpora. This clas-
1 http://hunspell.sourceforge.net/
sifier should be able to filter out the pairs which likely are not beneficial for the translation model.
In order to reliably decide about the classifier to use, we evaluated several techniques. The training and test sets for this evaluation were built respectively from nc-dev2007 and nc-devtest2007. In each set, about 30% randomly selected source sentences switch positions with the immediate following so that they form negative examples. We also used lexical dictionaries in both directions based on EPPS and UN corpora. We relied on seven features in our classifiers: IBM1 score in both directions, number of unaligned source words, the difference in number of words between source and target, the maximum source word fertility, number of unaligned target words, and the maximum target word fertility. It is noteworthy that all the features requiring alignment information (such as the unaligned source words) were computed on the basis of the Viterbi path of the IBM1 alignment. The following classifiers were used:
Regression Choose either class based on a weighted linear combination of the features and a fixed threshold of 0.5.
Logistic regression The probability of the class is expressed as a sigmoid of a linear combination of the different features. Then the class with the highest probability is picked.
Maximum entropy classifier We used the same set of features to train a maximum entropy classifier using the Megam package 2 .
Support vector machines classifier An SVM classifier was trained using the SVM-light package 3 .
Results of these experiments are summarized in Table 1.
The regression weights were estimated so that to minimize the squared error. This gave us a pretty poor F-measure score of 90.42%. Given that the logistic regression is more suited for binary classification in our case than the normal regression, it led to significant increase in the performance. The training
2 http://www.cs.utah.edu/&#732;hal/megam/ 3 http://svmlight.joachims.org/
Approach Precision Recall F-measure
was held by maximizing the likelihood to the data with L 2 regularization (with &#945; = 0.1). This gave an F-measure score of 94.78%.
The maximum entropy classifier performed better than the logistic regression in terms of precision but however it had worse F-measure. Significant improvements could be noticed using the SVM classifier in both precision and recall: 98.20% precision, 96.87% recall, and thus 97.53% F-measure.
As a result, we used the SVM classifier to filter the Giga parallel corpus. The corpus contained originally around 22.52 million pairs. After preprocessing and filtering it was reduced to 16.7 million pairs. Thus throwing around 6 million pairs.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The Giga corpus incorporates non-neglegible amounts of noise even after our usual preprocessing.</text>
                  <doc_id>26</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This noise may be due to different causes.</text>
                  <doc_id>27</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For instance: non-standard HTML characters, meaningless parts composed of only hypertext codes, sentences which are only partial translation of the source, or eventually not a correct translation at all.</text>
                  <doc_id>28</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Such noisy pairs potentially degrade the translation model quality, therefore it seemed more convenient to eliminate them.</text>
                  <doc_id>29</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Given the size of the corpus, this task could not be performed manually.</text>
                  <doc_id>30</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Consequently, we used an automatic classifier inspired by the work of Munteanu and Marcu (2005) on comparable corpora.</text>
                  <doc_id>31</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This clas-</text>
                  <doc_id>32</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 http://hunspell.sourceforge.net/</text>
                  <doc_id>33</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>sifier should be able to filter out the pairs which likely are not beneficial for the translation model.</text>
                  <doc_id>34</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In order to reliably decide about the classifier to use, we evaluated several techniques.</text>
                  <doc_id>35</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The training and test sets for this evaluation were built respectively from nc-dev2007 and nc-devtest2007.</text>
                  <doc_id>36</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In each set, about 30% randomly selected source sentences switch positions with the immediate following so that they form negative examples.</text>
                  <doc_id>37</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We also used lexical dictionaries in both directions based on EPPS and UN corpora.</text>
                  <doc_id>38</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We relied on seven features in our classifiers: IBM1 score in both directions, number of unaligned source words, the difference in number of words between source and target, the maximum source word fertility, number of unaligned target words, and the maximum target word fertility.</text>
                  <doc_id>39</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>It is noteworthy that all the features requiring alignment information (such as the unaligned source words) were computed on the basis of the Viterbi path of the IBM1 alignment.</text>
                  <doc_id>40</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The following classifiers were used:</text>
                  <doc_id>41</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Regression Choose either class based on a weighted linear combination of the features and a fixed threshold of 0.5.</text>
                  <doc_id>42</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Logistic regression The probability of the class is expressed as a sigmoid of a linear combination of the different features.</text>
                  <doc_id>43</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Then the class with the highest probability is picked.</text>
                  <doc_id>44</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Maximum entropy classifier We used the same set of features to train a maximum entropy classifier using the Megam package 2 .</text>
                  <doc_id>45</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Support vector machines classifier An SVM classifier was trained using the SVM-light package 3 .</text>
                  <doc_id>46</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Results of these experiments are summarized in Table 1.</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The regression weights were estimated so that to minimize the squared error.</text>
                  <doc_id>48</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This gave us a pretty poor F-measure score of 90.42%.</text>
                  <doc_id>49</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Given that the logistic regression is more suited for binary classification in our case than the normal regression, it led to significant increase in the performance.</text>
                  <doc_id>50</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The training</text>
                  <doc_id>51</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 http://www.cs.utah.edu/&#732;hal/megam/ 3 http://svmlight.joachims.org/</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Approach Precision Recall F-measure</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>was held by maximizing the likelihood to the data with L 2 regularization (with &#945; = 0.1).</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This gave an F-measure score of 94.78%.</text>
                  <doc_id>55</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The maximum entropy classifier performed better than the logistic regression in terms of precision but however it had worse F-measure.</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Significant improvements could be noticed using the SVM classifier in both precision and recall: 98.20% precision, 96.87% recall, and thus 97.53% F-measure.</text>
                  <doc_id>57</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As a result, we used the SVM classifier to filter the Giga parallel corpus.</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The corpus contained originally around 22.52 million pairs.</text>
                  <doc_id>59</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>After preprocessing and filtering it was reduced to 16.7 million pairs.</text>
                  <doc_id>60</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Thus throwing around 6 million pairs.</text>
                  <doc_id>61</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>2.4 Word Reordering</title>
            <text>In contrast to modeling the reordering by a distancebased reordering model and/or a lexicalized distortion model, we use a different approach that relies on part-of-speech (POS) sequences. By abstracting from surface words to parts-of-speech, we expect to model the reordering more accurately.
2.4.1 POS-based Reordering Model
To model reordering we first learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. Continuous reordering rules are extracted as described in Rottmann and Vogel (2007) to model short-range reorderings. When translating between German and English, we apply a modified reordering model with non-continuous rules to cover also long-range reorderings (Niehues and Kolss, 2009). The reordering rules are applied to the source text and the original order of words and the reordered sentence variants generated by the rules are encoded in a word lattice which is used as input to the decoder.
2.4.2 Lattice Phrase Extraction
For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract the phrase pairs for originally discontinuous phrases and could apply them during translation of reordered test sentences.
Therefore, we build reordering lattices for all training sentences and then extract phrase pairs from the monotone source path as well as from the reordered paths.
To limit the number of extracted phrase pairs, we extract a source phrase only once per sentence even if it is found in different paths.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In contrast to modeling the reordering by a distancebased reordering model and/or a lexicalized distortion model, we use a different approach that relies on part-of-speech (POS) sequences.</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>By abstracting from surface words to parts-of-speech, we expect to model the reordering more accurately.</text>
                  <doc_id>63</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.4.1 POS-based Reordering Model</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To model reordering we first learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information.</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Continuous reordering rules are extracted as described in Rottmann and Vogel (2007) to model short-range reorderings.</text>
                  <doc_id>66</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>When translating between German and English, we apply a modified reordering model with non-continuous rules to cover also long-range reorderings (Niehues and Kolss, 2009).</text>
                  <doc_id>67</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The reordering rules are applied to the source text and the original order of words and the reordered sentence variants generated by the rules are encoded in a word lattice which is used as input to the decoder.</text>
                  <doc_id>68</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.4.2 Lattice Phrase Extraction</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily.</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If we apply this also to the training sentences, we would be able to extract the phrase pairs for originally discontinuous phrases and could apply them during translation of reordered test sentences.</text>
                  <doc_id>71</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Therefore, we build reordering lattices for all training sentences and then extract phrase pairs from the monotone source path as well as from the reordered paths.</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To limit the number of extracted phrase pairs, we extract a source phrase only once per sentence even if it is found in different paths.</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>4</index>
            <title>2.5 Translation and Language Models</title>
            <text>In addition to the models used in the baseline system described above we conducted experiments including additional models that enhance translation quality by introducing alternative or additional information into the translation or language modelling process.
2.5.1 Discriminative Word Alignment
In most of our systems we use the PGIZA++ Toolkit 4 to generate alignments between words in the training corpora. The word alignments are generated in both directions and the grow-diag-final-and heuristic is used to combine them. The phrase extraction is then done based on this word alignment.
In the English-German system we applied the Discriminative Word Alignment approach as described in Niehues and Vogel (2008) instead. This alignment model is trained on a small corpus of hand-aligned data and uses the lexical probability as well as the fertilities generated by the PGIZA++ Toolkit and POS information.
2.5.2 Bilingual Language Model In phrase-based systems the source sentence is segmented by the decoder according to the best combination of phrases that maximize the translation and language model scores. This segmentation into phrases leads to the loss of context information at the phrase boundaries. Although more target side context is available to the language model, source
4 http://www.cs.cmu.edu/&#732;qing/
side context would also be valuable for the decoder when searching for the best translation hypothesis. To make also source language context available we use a bilingual language model, an additional language model in the phrase-based system in which each token consist of a target word and all source words it is aligned to. The bilingual tokens enter the translation process as an additional target factor and the bilingual language model is applied to the additional factor like a normal language model. For more details see (Niehues et al., 2011).
2.5.3 Parallel phrase scoring
The process of phrase scoring is held in two runs. The objective of the first run is to compute the necessary counts and to estimate the scores, all based on the source phrases; while the second run is similarly held based on the target phrases. Thus, the extracted phrases have to be sorted twice: once by source phrase and once by target phrase. These two sorting operations are almost always done on an external storage device and hence consume most of the time spent in this step.
The phrase scoring step was reimplemented in order to exploit the available computation resources more efficiently and therefore reduce the processing time. It uses optimized sorting algorithms for large data volumes which cannot fit into memory (Vitter, 2008). In its core, our implementation relies on STXXL: an extension of the STL library for external memory (Kettner, 2005) and on OpenMP for shared memory parallelization (Chapman et al., 2007).
Table 2 shows a comparison between Moses and our phrase scoring tools. The comparison was held using sixteen-core 64-bit machines with 128 Gb RAM, where the files are accessed through NFS on a RAID disk. The experiments show that the gain grows linearly with the size of input with an average of 40% of speed up.
2.5.4 POS Language Models
In addition to surface word language models, we did experiments with language models based on part-of-speech for English-German. We expect that having additional information in form of probabilities of part-of-speech sequences should help especially in case of the rich morphology of German and #pairs(G) Moses &#8727;10 3 (s) KIT &#8727;10 3 (s)
therefore the more difficult target language generation. The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information. While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RF- Tagger produces 756 different fine-grained tags on the same corpus.
We tried n-gram lengths of 4 and 7. While no improvement in translation quality could be achieved using the POS language models based on the normal POS tags, the 4-gram POS language model based on fine-grained tags could improve the translation system by 0.2 BLEU points as shown in Table 3. Surprisingly, increasing the n-gram length to 7 decreased the translation quality again. To investigate the impact of context length, we performed an analysis on the outputs of two different systems, one without a POS language model and one with the 4-gram fine-grained POS language model. For each of the translations we calculated the average length of the n-grams in the translation when applying one of the two language models using 4- grams of surface words or parts-of-speech. The results are also shown in Table 3. The average n-gram length of surface words on the translation generated by the system without POS language model and the one using the 4-gram POS language model stays practically the same. When measuring the n-gram length using the 4-gram POS language model, the context increases to 3.4. This increase of context is not surprising, since with the more general POS tags longer contexts can be matched. Comparing the POS context length for the two translations, we can see that the context increases from 3.18 to 3.40 due to longer matching POS sequences. This means that the system using
the POS language model actually generates translations with more probable POS sequences so that longer matches are possible. Also the perplexity drops by half since the POS language model helps constructing sentences that have a better structure.
System BLEU avg. ngram length PPL Word POS POS</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In addition to the models used in the baseline system described above we conducted experiments including additional models that enhance translation quality by introducing alternative or additional information into the translation or language modelling process.</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.5.1 Discriminative Word Alignment</text>
                  <doc_id>75</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In most of our systems we use the PGIZA++ Toolkit 4 to generate alignments between words in the training corpora.</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The word alignments are generated in both directions and the grow-diag-final-and heuristic is used to combine them.</text>
                  <doc_id>77</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The phrase extraction is then done based on this word alignment.</text>
                  <doc_id>78</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In the English-German system we applied the Discriminative Word Alignment approach as described in Niehues and Vogel (2008) instead.</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This alignment model is trained on a small corpus of hand-aligned data and uses the lexical probability as well as the fertilities generated by the PGIZA++ Toolkit and POS information.</text>
                  <doc_id>80</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.5.2 Bilingual Language Model In phrase-based systems the source sentence is segmented by the decoder according to the best combination of phrases that maximize the translation and language model scores.</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This segmentation into phrases leads to the loss of context information at the phrase boundaries.</text>
                  <doc_id>82</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Although more target side context is available to the language model, source</text>
                  <doc_id>83</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4 http://www.cs.cmu.edu/&#732;qing/</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>side context would also be valuable for the decoder when searching for the best translation hypothesis.</text>
                  <doc_id>85</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To make also source language context available we use a bilingual language model, an additional language model in the phrase-based system in which each token consist of a target word and all source words it is aligned to.</text>
                  <doc_id>86</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The bilingual tokens enter the translation process as an additional target factor and the bilingual language model is applied to the additional factor like a normal language model.</text>
                  <doc_id>87</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For more details see (Niehues et al., 2011).</text>
                  <doc_id>88</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.5.3 Parallel phrase scoring</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The process of phrase scoring is held in two runs.</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The objective of the first run is to compute the necessary counts and to estimate the scores, all based on the source phrases; while the second run is similarly held based on the target phrases.</text>
                  <doc_id>91</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Thus, the extracted phrases have to be sorted twice: once by source phrase and once by target phrase.</text>
                  <doc_id>92</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>These two sorting operations are almost always done on an external storage device and hence consume most of the time spent in this step.</text>
                  <doc_id>93</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The phrase scoring step was reimplemented in order to exploit the available computation resources more efficiently and therefore reduce the processing time.</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It uses optimized sorting algorithms for large data volumes which cannot fit into memory (Vitter, 2008).</text>
                  <doc_id>95</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In its core, our implementation relies on STXXL: an extension of the STL library for external memory (Kettner, 2005) and on OpenMP for shared memory parallelization (Chapman et al., 2007).</text>
                  <doc_id>96</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 2 shows a comparison between Moses and our phrase scoring tools.</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The comparison was held using sixteen-core 64-bit machines with 128 Gb RAM, where the files are accessed through NFS on a RAID disk.</text>
                  <doc_id>98</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The experiments show that the gain grows linearly with the size of input with an average of 40% of speed up.</text>
                  <doc_id>99</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.5.4 POS Language Models</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In addition to surface word language models, we did experiments with language models based on part-of-speech for English-German.</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We expect that having additional information in form of probabilities of part-of-speech sequences should help especially in case of the rich morphology of German and #pairs(G) Moses &#8727;10 3 (s) KIT &#8727;10 3 (s)</text>
                  <doc_id>102</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>therefore the more difficult target language generation.</text>
                  <doc_id>103</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information.</text>
                  <doc_id>104</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RF- Tagger produces 756 different fine-grained tags on the same corpus.</text>
                  <doc_id>105</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We tried n-gram lengths of 4 and 7.</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>While no improvement in translation quality could be achieved using the POS language models based on the normal POS tags, the 4-gram POS language model based on fine-grained tags could improve the translation system by 0.2 BLEU points as shown in Table 3.</text>
                  <doc_id>107</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Surprisingly, increasing the n-gram length to 7 decreased the translation quality again.</text>
                  <doc_id>108</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>To investigate the impact of context length, we performed an analysis on the outputs of two different systems, one without a POS language model and one with the 4-gram fine-grained POS language model.</text>
                  <doc_id>109</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For each of the translations we calculated the average length of the n-grams in the translation when applying one of the two language models using 4- grams of surface words or parts-of-speech.</text>
                  <doc_id>110</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The results are also shown in Table 3.</text>
                  <doc_id>111</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The average n-gram length of surface words on the translation generated by the system without POS language model and the one using the 4-gram POS language model stays practically the same.</text>
                  <doc_id>112</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>When measuring the n-gram length using the 4-gram POS language model, the context increases to 3.4.</text>
                  <doc_id>113</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>This increase of context is not surprising, since with the more general POS tags longer contexts can be matched.</text>
                  <doc_id>114</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>Comparing the POS context length for the two translations, we can see that the context increases from 3.18 to 3.40 due to longer matching POS sequences.</text>
                  <doc_id>115</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>This means that the system using</text>
                  <doc_id>116</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the POS language model actually generates translations with more probable POS sequences so that longer matches are possible.</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Also the perplexity drops by half since the POS language model helps constructing sentences that have a better structure.</text>
                  <doc_id>118</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>System BLEU avg.</text>
                  <doc_id>119</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>ngram length PPL Word POS POS</text>
                  <doc_id>120</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>2</index>
        <title>3 Results</title>
        <text>Using the models described above we performed several experiments leading finally to the systems used for generating the translations submitted to the workshop. The following sections describe the experiments for the individual language pairs and show the translation results. The results are reported as case-sensitive BLEU scores (Papineni et al., 2002) on one reference translation.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Using the models described above we performed several experiments leading finally to the systems used for generating the translations submitted to the workshop.</text>
              <doc_id>121</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The following sections describe the experiments for the individual language pairs and show the translation results.</text>
              <doc_id>122</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The results are reported as case-sensitive BLEU scores (Papineni et al., 2002) on one reference translation.</text>
              <doc_id>123</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 German-English</title>
            <text>The German-to-English baseline system applies short-range reordering rules and uses a language model trained on the EPPS and News Commentary. By exchanging the baseline language model by one trained on the News Shuffle corpus we improve the translation quality considerably, by more than 3 BLEU points. When we expand the coverage of the reordering rules to enable long-range reordering we can improve even further by 0.4 and adding a second language model trained on the English Gigaword corpus we gain another 0.3 BLEU points. To ensure that the phrase table also includes reordered phrases, we use lattice phrase extraction and can achieve a small improvement. Finally, a bilingual language model is added to extend the context of source language words available for translation, reaching the best score of 23.35 BLEU points. This system was used for generating the translation submitted to the German-English Translation Task.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The German-to-English baseline system applies short-range reordering rules and uses a language model trained on the EPPS and News Commentary.</text>
                  <doc_id>124</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>By exchanging the baseline language model by one trained on the News Shuffle corpus we improve the translation quality considerably, by more than 3 BLEU points.</text>
                  <doc_id>125</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>When we expand the coverage of the reordering rules to enable long-range reordering we can improve even further by 0.4 and adding a second language model trained on the English Gigaword corpus we gain another 0.3 BLEU points.</text>
                  <doc_id>126</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>To ensure that the phrase table also includes reordered phrases, we use lattice phrase extraction and can achieve a small improvement.</text>
                  <doc_id>127</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, a bilingual language model is added to extend the context of source language words available for translation, reaching the best score of 23.35 BLEU points.</text>
                  <doc_id>128</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This system was used for generating the translation submitted to the German-English Translation Task.</text>
                  <doc_id>129</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 English-German</title>
            <text>The English-to-German baseline system also includes short-range reordering and uses translation System Dev Test
and language model based on EPPS and News Commentary. Exchanging the language model by the News Shuffle language model again yields a big improvement by 2.3 BLEU points. Adding long-range reordering improves a lot on the development set while the score on the test set remains practically the same. Replacing the GIZA++ alignments by alignments generated using the Discriminative Word Alignment Model again only leads to a small improvement. By using the bilingual language model to increase context we can gain 0.1 BLEU points and by adding the part-of-speech language model with rich parts-of-speech including case, number and gender information for German we achieve the best score of 16.88. This system was used to generate the translation used for submission.
System Dev Test</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The English-to-German baseline system also includes short-range reordering and uses translation System Dev Test</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and language model based on EPPS and News Commentary.</text>
                  <doc_id>131</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Exchanging the language model by the News Shuffle language model again yields a big improvement by 2.3 BLEU points.</text>
                  <doc_id>132</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Adding long-range reordering improves a lot on the development set while the score on the test set remains practically the same.</text>
                  <doc_id>133</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Replacing the GIZA++ alignments by alignments generated using the Discriminative Word Alignment Model again only leads to a small improvement.</text>
                  <doc_id>134</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>By using the bilingual language model to increase context we can gain 0.1 BLEU points and by adding the part-of-speech language model with rich parts-of-speech including case, number and gender information for German we achieve the best score of 16.88.</text>
                  <doc_id>135</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This system was used to generate the translation used for submission.</text>
                  <doc_id>136</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>System Dev Test</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 English-French</title>
            <text>Table 6 summarizes how our system for English- French evolved. The baseline system for this direction was trained on the EPPS and News Commentary corpora, while the language model was trained on the French part of the EPPS, News Commentary and UN parallel corpora. Some improvement could be already seen by introducing the short-range reorderings trained on the baseline parallel corpus.
Apparently, the UN data brought only slight improvement to the overall performance. On the other hand, adding bigger language models trained on the monolingual French version of EPPS, News Commentary and the News Shuffle together with the French Gigaword corpus introduces an improvement of 3.7 on test. Using a system trained only on the Giga corpus data with the same last configuration shows a significant gain. It showed an improvement of around 1.0. We were able to obtain some further improvements by merging the translation models of the last two systems. i.e. the one system based on EPPS, UN, and News Commentary and the other on the Giga corpus. This merging increased our score by 0.2. Finally, our submitted system for this direction was obtained by using a single language model trained on the union of all the French corpora instead of using multiple models. This resulted in an improvement of 0.1 leading to our best score: 28.28.
System Dev Test</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Table 6 summarizes how our system for English- French evolved.</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The baseline system for this direction was trained on the EPPS and News Commentary corpora, while the language model was trained on the French part of the EPPS, News Commentary and UN parallel corpora.</text>
                  <doc_id>139</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Some improvement could be already seen by introducing the short-range reorderings trained on the baseline parallel corpus.</text>
                  <doc_id>140</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Apparently, the UN data brought only slight improvement to the overall performance.</text>
                  <doc_id>141</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>On the other hand, adding bigger language models trained on the monolingual French version of EPPS, News Commentary and the News Shuffle together with the French Gigaword corpus introduces an improvement of 3.7 on test.</text>
                  <doc_id>142</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Using a system trained only on the Giga corpus data with the same last configuration shows a significant gain.</text>
                  <doc_id>143</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>It showed an improvement of around 1.0.</text>
                  <doc_id>144</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We were able to obtain some further improvements by merging the translation models of the last two systems.</text>
                  <doc_id>145</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>i.e. the one system based on EPPS, UN, and News Commentary and the other on the Giga corpus.</text>
                  <doc_id>146</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>This merging increased our score by 0.2.</text>
                  <doc_id>147</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, our submitted system for this direction was obtained by using a single language model trained on the union of all the French corpora instead of using multiple models.</text>
                  <doc_id>148</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>This resulted in an improvement of 0.1 leading to our best score: 28.28.</text>
                  <doc_id>149</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>System Dev Test</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>3.4 French-English</title>
            <text>The development of our system for the French- English direction is summarized in Table 7. Our system for this direction evolved quite similarly to the opposite direction. The largest improvement accompanied the integration of the bigger language models (trained on the English version of EPPS, News Commentary, News Shuffle and the Gigaword corpus): 3.3 BLEU points, whereas smaller improvements could be gained by applying the short reordering rules and almost no change by including the UN data. Further gains were obtained by training the system on the Giga corpus added to the previous parallel data. This increased our performance by 0.6. The submitted system was obtained by augmenting the last system with a bilingual language model adding around 0.2 to the previous score and thus giving 28.34 as final score.
System Dev Test</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The development of our system for the French- English direction is summarized in Table 7.</text>
                  <doc_id>151</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Our system for this direction evolved quite similarly to the opposite direction.</text>
                  <doc_id>152</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The largest improvement accompanied the integration of the bigger language models (trained on the English version of EPPS, News Commentary, News Shuffle and the Gigaword corpus): 3.3 BLEU points, whereas smaller improvements could be gained by applying the short reordering rules and almost no change by including the UN data.</text>
                  <doc_id>153</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Further gains were obtained by training the system on the Giga corpus added to the previous parallel data.</text>
                  <doc_id>154</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This increased our performance by 0.6.</text>
                  <doc_id>155</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The submitted system was obtained by augmenting the last system with a bilingual language model adding around 0.2 to the previous score and thus giving 28.34 as final score.</text>
                  <doc_id>156</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>System Dev Test</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>4 Conclusions</title>
        <text>We have presented the systems for our participation in the WMT 2011 Evaluation for English&#8596;German and English&#8596;French. For English&#8596;French, a special filtering method for web-crawled data was developed. In addition, a parallel phrase scoring technique was implemented that could speed up the MT training process tremendously. Using these two features, we were able to integrate the huge amounts of data available in the Giga corpus into our systems translating between English and French.
We applied POS-based reordering to improve our translations in all directions, using short-range reordering for English&#8596;French and long-range reordering for English&#8596;German. For German- English, reordering also the training corpus lead to further improvements of the translation quality. A Discriminative Word Alignment Model led to an increase in BLEU for English-German. For this direction we also tried fine-grained POS language models of different n-gram lengths. The best translations could be obtained by using 4-grams. For nearly all experiments, a bilingual language model was applied that expands the context of source words that can be considered during decoding. The improvements range from 0.1 to 0.4 in BLEU score.
Acknowledgments
This work was realized as part of the Quaero Programme, funded by OSEO, French State agency for innovation.
References
Barbara Chapman, Gabriele Jost, and Ruud van der Pas. 2007. Using OpenMP: Portable Shared Memory Parallel Programming (Scientific and Engineering Computation). The MIT Press. Roman Dementiev Lutz Kettner. 2005. Stxxl: Standard template library for xxl data sets. In Proceedings of ESA 2005. Volume 3669 of LNCS, pages 640&#8211;651. Springer.
Philipp Koehn and Kevin Knight. 2003. Empirical Methods for Compound Splitting. In EACL, Budapest, Hungary.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics, 31:477&#8211;504. Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In Fourth Workshop on Statistical Machine Translation (WMT 2009), Athens, Greece. Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling. In Proc. of Third ACL Workshop on Statistical Machine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex Waibel. 2011. Wider Context by Using Bilingual Language Models in Machine Translation. In Sixth Workshop on Statistical Machine Translation (WMT 2011), Edinburgh, UK. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. Technical Report RC22176 (W0109-022), IBM Research Division, T. J. Watson Research Center.
Kay Rottmann and Stephan Vogel. 2007. Word Reordering in Statistical Machine Translation with a POS- Based Distortion Model. In TMI, Sk&#246;vde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation of Conditional Probabilities with Decision Trees and an Application to Fine-Grained POS Tagging. In COL- ING 2008, Manchester, Great Britain.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference on New Methods in Language Processing, Manchester, UK. Andreas Stolcke. 2002. SRILM &#8211; An Extensible Language Modeling Toolkit. In Proc. of ICSLP, Denver, Colorado, USA. Ashish Venugopal, Andreas Zollman, and Alex Waibel. 2005. Training and Evaluation Error Minimization Rules for Statistical Machine Translation. In Workshop on Data-drive Machine Translation and Beyond (WPT-05), Ann Arbor, MI.
Jeffrey Scott Vitter. 2008. Algorithms and Data Structures for External Memory. now Publishers Inc. Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In Int. Conf. on Natural Language Processing and Knowledge Engineering, Beijing, China.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have presented the systems for our participation in the WMT 2011 Evaluation for English&#8596;German and English&#8596;French.</text>
              <doc_id>158</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For English&#8596;French, a special filtering method for web-crawled data was developed.</text>
              <doc_id>159</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In addition, a parallel phrase scoring technique was implemented that could speed up the MT training process tremendously.</text>
              <doc_id>160</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Using these two features, we were able to integrate the huge amounts of data available in the Giga corpus into our systems translating between English and French.</text>
              <doc_id>161</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We applied POS-based reordering to improve our translations in all directions, using short-range reordering for English&#8596;French and long-range reordering for English&#8596;German.</text>
              <doc_id>162</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For German- English, reordering also the training corpus lead to further improvements of the translation quality.</text>
              <doc_id>163</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A Discriminative Word Alignment Model led to an increase in BLEU for English-German.</text>
              <doc_id>164</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>For this direction we also tried fine-grained POS language models of different n-gram lengths.</text>
              <doc_id>165</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The best translations could be obtained by using 4-grams.</text>
              <doc_id>166</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>For nearly all experiments, a bilingual language model was applied that expands the context of source words that can be considered during decoding.</text>
              <doc_id>167</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The improvements range from 0.1 to 0.4 in BLEU score.</text>
              <doc_id>168</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Acknowledgments</text>
              <doc_id>169</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>This work was realized as part of the Quaero Programme, funded by OSEO, French State agency for innovation.</text>
              <doc_id>170</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>References</text>
              <doc_id>171</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Barbara Chapman, Gabriele Jost, and Ruud van der Pas.</text>
              <doc_id>172</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>173</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Using OpenMP: Portable Shared Memory Parallel Programming (Scientific and Engineering Computation).</text>
              <doc_id>174</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The MIT Press.</text>
              <doc_id>175</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Roman Dementiev Lutz Kettner.</text>
              <doc_id>176</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>177</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Stxxl: Standard template library for xxl data sets.</text>
              <doc_id>178</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ESA 2005.</text>
              <doc_id>179</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Volume 3669 of LNCS, pages 640&#8211;651.</text>
              <doc_id>180</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Springer.</text>
              <doc_id>181</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Philipp Koehn and Kevin Knight.</text>
              <doc_id>182</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>183</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Empirical Methods for Compound Splitting.</text>
              <doc_id>184</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In EACL, Budapest, Hungary.</text>
              <doc_id>185</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Dragos Stefan Munteanu and Daniel Marcu.</text>
              <doc_id>186</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>187</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Improving machine translation performance by exploiting non-parallel corpora.</text>
              <doc_id>188</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 31:477&#8211;504.</text>
              <doc_id>189</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Jan Niehues and Muntsin Kolss.</text>
              <doc_id>190</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>191</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>A POS-Based</text>
              <doc_id>192</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Model for Long-Range Reorderings in SMT.</text>
              <doc_id>193</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Fourth Workshop on Statistical Machine Translation (WMT 2009), Athens, Greece.</text>
              <doc_id>194</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Jan Niehues and Stephan Vogel.</text>
              <doc_id>195</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>196</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Discriminative</text>
              <doc_id>197</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Word Alignment via Alignment Matrix Modeling.</text>
              <doc_id>198</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of Third ACL Workshop on Statistical Machine Translation, Columbus, USA.</text>
              <doc_id>199</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex Waibel.</text>
              <doc_id>200</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2011.</text>
              <doc_id>201</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Wider Context by Using Bilingual Language Models in Machine Translation.</text>
              <doc_id>202</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Sixth Workshop on Statistical Machine Translation (WMT 2011), Edinburgh, UK.</text>
              <doc_id>203</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu.</text>
              <doc_id>204</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>205</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Bleu: a Method for Automatic Evaluation of Machine Translation.</text>
              <doc_id>206</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Technical Report RC22176 (W0109-022), IBM Research Division, T.</text>
              <doc_id>207</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>J. Watson Research Center.</text>
              <doc_id>208</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Kay Rottmann and Stephan Vogel.</text>
              <doc_id>209</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>210</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Word Reordering in Statistical Machine Translation with a POS- Based Distortion Model.</text>
              <doc_id>211</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In TMI, Sk&#246;vde, Sweden.</text>
              <doc_id>212</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Helmut Schmid and Florian Laws.</text>
              <doc_id>213</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>214</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Estimation of Conditional Probabilities with Decision Trees and an Application to Fine-Grained POS Tagging.</text>
              <doc_id>215</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In COL- ING 2008, Manchester, Great Britain.</text>
              <doc_id>216</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Helmut Schmid.</text>
              <doc_id>217</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1994.</text>
              <doc_id>218</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Probabilistic Part-of-Speech Tagging Using Decision Trees.</text>
              <doc_id>219</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In International Conference on New Methods in Language Processing, Manchester, UK.</text>
              <doc_id>220</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Andreas Stolcke.</text>
              <doc_id>221</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>222</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>SRILM &#8211; An Extensible Language Modeling Toolkit.</text>
              <doc_id>223</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of ICSLP, Denver, Colorado, USA.</text>
              <doc_id>224</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Ashish Venugopal, Andreas Zollman, and Alex Waibel.</text>
              <doc_id>225</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>226</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Training and Evaluation Error Minimization Rules for Statistical Machine Translation.</text>
              <doc_id>227</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>In Workshop on Data-drive Machine Translation and Beyond (WPT-05), Ann Arbor, MI.</text>
              <doc_id>228</doc_id>
              <sec_id>11</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Jeffrey Scott Vitter.</text>
              <doc_id>229</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>230</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Algorithms and Data Structures for External Memory.</text>
              <doc_id>231</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>now Publishers Inc. Stephan Vogel.</text>
              <doc_id>232</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>233</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>SMT Decoder Dissected: Word</text>
              <doc_id>234</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Reordering.</text>
              <doc_id>235</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Int.</text>
              <doc_id>236</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Conf. on Natural Language Processing and Knowledge Engineering, Beijing, China.</text>
              <doc_id>237</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Results of the filtering experiments</caption>
        <reference_text></reference_text>
        <page_num>2</page_num>
        <head>
          <rows>
            <row>
              <cell>Regression</cell>
              <cell>93.81</cell>
              <cell>87.27</cell>
              <cell>90.42</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>LogReg</cell>
              <cell>93.43</cell>
              <cell>94.84</cell>
              <cell>94.13</cell>
            </row>
            <row>
              <cell>MaxEnt</cell>
              <cell>93.69</cell>
              <cell>94.54</cell>
              <cell>94.11</cell>
            </row>
            <row>
              <cell>SVM</cell>
              <cell>98.20</cell>
              <cell>96.87</cell>
              <cell>97.53</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Comparison of Moses and KIT phrase extraction systems</caption>
        <reference_text></reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>0.203</cell>
              <cell>25.99</cell>
              <cell>17.58</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>1.444</cell>
              <cell>184.19</cell>
              <cell>103.41</cell>
            </row>
            <row>
              <cell>1.693</cell>
              <cell>230.97</cell>
              <cell>132.79</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Analysis of context length</caption>
        <reference_text>None</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>System</cell>
              <cell>BLEU</cell>
              <cell>avg. ngram length</cell>
              <cell>avg. ngram length</cell>
              <cell>PPL</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>Word</cell>
              <cell>POS</cell>
              <cell>POS</cell>
            </row>
            <row>
              <cell>no POS LM</cell>
              <cell>16.64</cell>
              <cell>2.77</cell>
              <cell>3.18</cell>
              <cell>66.78</cell>
            </row>
            <row>
              <cell>POS LM</cell>
              <cell>16.88</cell>
              <cell>2.81</cell>
              <cell>3.40</cell>
              <cell>33.36</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: Translation results for German-English</caption>
        <reference_text>None</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Dev</cell>
              <cell>Test</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Baseline</cell>
              <cell>18.49</cell>
              <cell>19.10</cell>
            </row>
            <row>
              <cell>+ NewsShuffle LM</cell>
              <cell>20.63</cell>
              <cell>22.24</cell>
            </row>
            <row>
              <cell>+ LongRange Reordering</cell>
              <cell>21.00</cell>
              <cell>22.68</cell>
            </row>
            <row>
              <cell>+ Additional Giga LM</cell>
              <cell>21.80</cell>
              <cell>22.92</cell>
            </row>
            <row>
              <cell>+ Lattice Phrase Extraction</cell>
              <cell>21.87</cell>
              <cell>22.96</cell>
            </row>
            <row>
              <cell>+ Bilingual LM</cell>
              <cell>22.05</cell>
              <cell>23.35</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TableSeer</source>
        <caption>Table 5: Translation results for English-German</caption>
        <reference_text>None</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Dev</cell>
              <cell>Test</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Baseline</cell>
              <cell>13.55</cell>
              <cell>14.19</cell>
            </row>
            <row>
              <cell>+ NewsShuffle LM</cell>
              <cell>15.10</cell>
              <cell>16.46</cell>
            </row>
            <row>
              <cell>+ LongRange Reordering</cell>
              <cell>15.79</cell>
              <cell>16.46</cell>
            </row>
            <row>
              <cell>+ DWA</cell>
              <cell>15.81</cell>
              <cell>16.52</cell>
            </row>
            <row>
              <cell>+ Bilingual LM</cell>
              <cell>15.85</cell>
              <cell>16.64</cell>
            </row>
            <row>
              <cell>+ POS LM</cell>
              <cell>15.88</cell>
              <cell>16.88</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>6</id>
        <source>TableSeer</source>
        <caption>Table 6: Translation results for English-French</caption>
        <reference_text>In PAGE 5: ...88 Table 5: Translation results for English-German 3.3 English-French  Table6  summarizes how our system for English- French evolved. The baseline system for this direc- tion was trained on the EPPS and News Commen- tary corpora, while the language model was trained on the French part of the EPPS, News Commen- tary and UN parallel corpora....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Dev</cell>
              <cell>Test</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Baseline</cell>
              <cell>20.62</cell>
              <cell>22.36</cell>
            </row>
            <row>
              <cell>+ Reordering</cell>
              <cell>21.29</cell>
              <cell>23.11</cell>
            </row>
            <row>
              <cell>+ UN</cell>
              <cell>21.27</cell>
              <cell>23.24</cell>
            </row>
            <row>
              <cell>+ Big LMs</cell>
              <cell>23.77</cell>
              <cell>26.90</cell>
            </row>
            <row>
              <cell>Giga data</cell>
              <cell>24.53</cell>
              <cell>27.94</cell>
            </row>
            <row>
              <cell>Merge</cell>
              <cell>24.74</cell>
              <cell>28.14</cell>
            </row>
            <row>
              <cell>+ Merged LMs</cell>
              <cell>25.07</cell>
              <cell>28.28</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>7</id>
        <source>TableSeer</source>
        <caption>Table 7: Translation results for French-English</caption>
        <reference_text>In PAGE 6: ...28 Table 6: Translation results for English-French 3.4 French-English The development of our system for the French- English direction is summarized in  Table7 . Our sys- tem for this direction evolved quite similarly to the opposite direction....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>thus giving 28.34 as final score.</cell>
              <cell>thus giving 28.34 as final score.</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Dev</cell>
              <cell>Test</cell>
            </row>
            <row>
              <cell>Baseline</cell>
              <cell>20.76</cell>
              <cell>23.78</cell>
            </row>
            <row>
              <cell>+ Reordering</cell>
              <cell>21.42</cell>
              <cell>24.28</cell>
            </row>
            <row>
              <cell>+ UN</cell>
              <cell>21.55</cell>
              <cell>24.21</cell>
            </row>
            <row>
              <cell>+ Big LMs</cell>
              <cell>24.16</cell>
              <cell>27.55</cell>
            </row>
            <row>
              <cell>+ Giga data</cell>
              <cell>24.86</cell>
              <cell>28.17</cell>
            </row>
            <row>
              <cell>+ BiLM</cell>
              <cell>25.01</cell>
              <cell>28.34</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references/>
    <citations/>
  </content>
</document>
