<document>
  <filename>W11-1015</filename>
  <authors>
    <author>Greg Hanneman</author>
    <author>Michelle Burroughs</author>
  </authors>
  <title>A General-Purpose Rule Extractor for SCFG-Based Machine Translation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>We present a rule extractor for SCFG-based MT that generalizes many of the contraints present in existing SCFG extraction algorithms. Our method&#8217;s increased rule coverage comes from allowing multiple alignments, virtual nodes, and multiple tree decompositions in the extraction process. At decoding time, we improve automatic metric scores by significantly increasing the number of phrase pairs that match a given test set, while our experiments with hierarchical grammar filtering indicate that more intelligent filtering schemes will also provide a key to future gains.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We present a rule extractor for SCFG-based MT that generalizes many of the contraints present in existing SCFG extraction algorithms.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our method&#8217;s increased rule coverage comes from allowing multiple alignments, virtual nodes, and multiple tree decompositions in the extraction process.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>At decoding time, we improve automatic metric scores by significantly increasing the number of phrase pairs that match a given test set, while our experiments with hierarchical grammar filtering indicate that more intelligent filtering schemes will also provide a key to future gains.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Syntax-based machine translation systems, regardless of the underlying formalism they use, depend on a method for acquiring bilingual rules in that formalism to build the system&#8217;s translation model. In modern syntax-based MT, this formalism is often synchronous context-free grammar (SCFG), and the SCFG rules are obtained automatically from parallel data through a large variety of methods. Some SCFG rule extraction techniques require only Viterbi word alignment links between the source and target sides of the input corpus (Chiang, 2005), while methods based on linguistic constituency structure require the source and/or target side of the input to be parsed. Among such techniques, most retain the dependency on Viterbi word alignments for each sentence (Galley et al., 2004; Zollmann and Venugopal, 2006; Lavie et al., 2008; Chiang, 2010) while others make use of a general, corpus-level statistical lexicon instead of individual alignment links (Zhechev and Way, 2008). Each method may also place constraints on the size, format, or structure of the rules it returns.
This paper describes a new, general-purpose rule extractor intended for cases in which two parse trees and Viterbi word alignment links are provided for each sentence, although compatibility with singleparse-tree extraction methods can be achieved by supplying a flat &#8220;dummy&#8221; parse for the missing tree. Our framework for rule extraction is thus most similar to the Stat-XFER system (Lavie et al., 2008; Ambati et al., 2009) and the tree-to-tree situation considered by Chiang (2010). However, we significantly broaden the scope of allowable rules compared to the Stat-XFER heuristics, and our approach differs from Chiang&#8217;s system in its respect of the linguistic constituency constraints expressed in the input tree structure. In summary, we attempt to extract the greatest possible number of syntactically motivated rules while not allowing them to violate explicit constituent boundaries on either the source or target side. This is achieved by allowing creation of virtual nodes, by allowing multiple decompositions of the same tree pair, and by allowing extraction of SCFG rules beyond the minimial set required to regenerate the tree pair.
After describing our extraction method and comparing it to a number of existing SCFG extraction techniques, we present a series of experiments examining the number of rules that may be produced from an input corpus. We also describe experiments on Chinese-to-English translation that suggest that filtering a very large extracted grammar to a more
moderate-sized translation model is an important consideration for obtaining strong results. Finally, this paper concludes with some suggestions for future work.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Syntax-based machine translation systems, regardless of the underlying formalism they use, depend on a method for acquiring bilingual rules in that formalism to build the system&#8217;s translation model.</text>
              <doc_id>3</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In modern syntax-based MT, this formalism is often synchronous context-free grammar (SCFG), and the SCFG rules are obtained automatically from parallel data through a large variety of methods.</text>
              <doc_id>4</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Some SCFG rule extraction techniques require only Viterbi word alignment links between the source and target sides of the input corpus (Chiang, 2005), while methods based on linguistic constituency structure require the source and/or target side of the input to be parsed.</text>
              <doc_id>5</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Among such techniques, most retain the dependency on Viterbi word alignments for each sentence (Galley et al., 2004; Zollmann and Venugopal, 2006; Lavie et al., 2008; Chiang, 2010) while others make use of a general, corpus-level statistical lexicon instead of individual alignment links (Zhechev and Way, 2008).</text>
              <doc_id>6</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Each method may also place constraints on the size, format, or structure of the rules it returns.</text>
              <doc_id>7</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>This paper describes a new, general-purpose rule extractor intended for cases in which two parse trees and Viterbi word alignment links are provided for each sentence, although compatibility with singleparse-tree extraction methods can be achieved by supplying a flat &#8220;dummy&#8221; parse for the missing tree.</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our framework for rule extraction is thus most similar to the Stat-XFER system (Lavie et al., 2008; Ambati et al., 2009) and the tree-to-tree situation considered by Chiang (2010).</text>
              <doc_id>9</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, we significantly broaden the scope of allowable rules compared to the Stat-XFER heuristics, and our approach differs from Chiang&#8217;s system in its respect of the linguistic constituency constraints expressed in the input tree structure.</text>
              <doc_id>10</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In summary, we attempt to extract the greatest possible number of syntactically motivated rules while not allowing them to violate explicit constituent boundaries on either the source or target side.</text>
              <doc_id>11</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This is achieved by allowing creation of virtual nodes, by allowing multiple decompositions of the same tree pair, and by allowing extraction of SCFG rules beyond the minimial set required to regenerate the tree pair.</text>
              <doc_id>12</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>After describing our extraction method and comparing it to a number of existing SCFG extraction techniques, we present a series of experiments examining the number of rules that may be produced from an input corpus.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We also describe experiments on Chinese-to-English translation that suggest that filtering a very large extracted grammar to a more</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>moderate-sized translation model is an important consideration for obtaining strong results.</text>
              <doc_id>15</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Finally, this paper concludes with some suggestions for future work.</text>
              <doc_id>16</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Rule Extraction Algorithm</title>
        <text>We begin with a parallel sentence consisting of a source-side parse tree S, a target-side parse tree T , and a Viterbi word alignment between the trees&#8217; leaves. A sample sentence of this type is shown in Figure 1. Our goal is to extract a number of SCFG rules that are licensed by this input.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We begin with a parallel sentence consisting of a source-side parse tree S, a target-side parse tree T , and a Viterbi word alignment between the trees&#8217; leaves.</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A sample sentence of this type is shown in Figure 1.</text>
              <doc_id>18</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Our goal is to extract a number of SCFG rules that are licensed by this input.</text>
              <doc_id>19</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Node Alignment</title>
            <text>Our algorithm first computes a node alignment between the parallel trees. A node s in tree S is aligned to a node t in tree T if the following constraints are met. First, all words in the yield of s must either be aligned to words within the yield of t, or they must be unaligned. Second, the reverse must also hold: all words in the yield of t must be aligned to words within the yield of s or again be unaligned. This is analogous to the word-alignment consistency constraint of phrase-based SMT phrase extraction (Koehn et al., 2003). In Figure 1, for example, the NP dominating the French words les voitures bleues is aligned to the equivalent English NP node dominating blue cars.
As in phrase-based SMT, where a phrase in one language may be consistent with multiple possible phrases in the other language, we allow parse nodes in both trees to have multiple node alignments. This is in contrast to one-derivation rule extractors such as that of Lavie et al. (2008), in which each node
in S may only be aligned to a single node in T and vice versa. The French NP node Ma m&#232;re, for example, aligns to both the NNP and NP nodes in English producing Mother.
Besides aligning existing nodes in both parse trees to the extent possible, we also permit the introduction of &#8220;virtual&#8221; nodes into either tree. Virtual nodes are created when two or more contiguous children of an existing node are aligned consistently to a node or a similar set of two or more contiguous children of a node in the opposite parse tree. Virtual nodes may be aligned to &#8220;original&#8221; nodes in the opposite tree or to other virtual nodes.
In Figure 1, the existing English NP node blue cars can be aligned to a new virtual node in French that dominates the N node voitures and the AP node bleues. The virtual node is inserted as the parent of N and AP, and as the child of the NP node directly above. In conjunction with node alignments between existing nodes, this means that the English NP blue cars is now aligned twice: once to the original French NP node and once to the virtual node N+AP. We thus replicate the behavior of &#8220;growing into the gaps&#8221; from phrase-based SMT in the presence of unaligned words. As another example, a virtual node in French covering the V node avait and the ADV node toujours could be created to align consistently with a virtual node in English covering the VBD node had and the ADVP node always.
Since virtual nodes are always created out of children of the same node, they are always consistent with the existing syntactic structure of the tree. Within the constraints of the existing tree structure and word alignments, however, all possible virtual nodes are considered. This is in keeping with our philosophy of allowing multiple alignments without violating constituent boundaries. Near the top of the trees in Figure 1, for example, French virtual nodes NP+VN+NP (aligned to English NP+VP) and VN+NP+PU (aligned to VP+PU) both exist, even though they overlap. In our procedure, we do allow a limit to be placed the number of child nodes that can be combined into a virtual node. Setting this limit to two, for instance, will constrain node alignment to the space of possible synchronous binarizations consistent with the Viterbi word alignments.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Our algorithm first computes a node alignment between the parallel trees.</text>
                  <doc_id>20</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A node s in tree S is aligned to a node t in tree T if the following constraints are met.</text>
                  <doc_id>21</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>First, all words in the yield of s must either be aligned to words within the yield of t, or they must be unaligned.</text>
                  <doc_id>22</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Second, the reverse must also hold: all words in the yield of t must be aligned to words within the yield of s or again be unaligned.</text>
                  <doc_id>23</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This is analogous to the word-alignment consistency constraint of phrase-based SMT phrase extraction (Koehn et al., 2003).</text>
                  <doc_id>24</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In Figure 1, for example, the NP dominating the French words les voitures bleues is aligned to the equivalent English NP node dominating blue cars.</text>
                  <doc_id>25</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As in phrase-based SMT, where a phrase in one language may be consistent with multiple possible phrases in the other language, we allow parse nodes in both trees to have multiple node alignments.</text>
                  <doc_id>26</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This is in contrast to one-derivation rule extractors such as that of Lavie et al. (2008), in which each node</text>
                  <doc_id>27</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>in S may only be aligned to a single node in T and vice versa.</text>
                  <doc_id>28</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The French NP node Ma m&#232;re, for example, aligns to both the NNP and NP nodes in English producing Mother.</text>
                  <doc_id>29</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Besides aligning existing nodes in both parse trees to the extent possible, we also permit the introduction of &#8220;virtual&#8221; nodes into either tree.</text>
                  <doc_id>30</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Virtual nodes are created when two or more contiguous children of an existing node are aligned consistently to a node or a similar set of two or more contiguous children of a node in the opposite parse tree.</text>
                  <doc_id>31</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Virtual nodes may be aligned to &#8220;original&#8221; nodes in the opposite tree or to other virtual nodes.</text>
                  <doc_id>32</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In Figure 1, the existing English NP node blue cars can be aligned to a new virtual node in French that dominates the N node voitures and the AP node bleues.</text>
                  <doc_id>33</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The virtual node is inserted as the parent of N and AP, and as the child of the NP node directly above.</text>
                  <doc_id>34</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In conjunction with node alignments between existing nodes, this means that the English NP blue cars is now aligned twice: once to the original French NP node and once to the virtual node N+AP.</text>
                  <doc_id>35</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We thus replicate the behavior of &#8220;growing into the gaps&#8221; from phrase-based SMT in the presence of unaligned words.</text>
                  <doc_id>36</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>As another example, a virtual node in French covering the V node avait and the ADV node toujours could be created to align consistently with a virtual node in English covering the VBD node had and the ADVP node always.</text>
                  <doc_id>37</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Since virtual nodes are always created out of children of the same node, they are always consistent with the existing syntactic structure of the tree.</text>
                  <doc_id>38</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Within the constraints of the existing tree structure and word alignments, however, all possible virtual nodes are considered.</text>
                  <doc_id>39</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This is in keeping with our philosophy of allowing multiple alignments without violating constituent boundaries.</text>
                  <doc_id>40</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Near the top of the trees in Figure 1, for example, French virtual nodes NP+VN+NP (aligned to English NP+VP) and VN+NP+PU (aligned to VP+PU) both exist, even though they overlap.</text>
                  <doc_id>41</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In our procedure, we do allow a limit to be placed the number of child nodes that can be combined into a virtual node.</text>
                  <doc_id>42</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Setting this limit to two, for instance, will constrain node alignment to the space of possible synchronous binarizations consistent with the Viterbi word alignments.</text>
                  <doc_id>43</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Grammar Extraction</title>
            <text>Given the final set of node alignments between the source tree and the target tree, SCFG rules are obtained via a grammar extraction step. Rule extraction proceeds in a depth-first manner, such that rules are extracted and cached for all descendents of a source node s before rules in which s is the left-hand side are considered. Extracting rules where source node s is the left-hand side consists of two phases: decomposition and combination.
The first phase is decomposition of node s into all distinct sets D = {d 1 , d 2 , . . . , d n } of descendent nodes such that D spans the entire yield of node s, where d i &#8712; D is node-aligned or is an unaligned terminal for all i, and d i has no ancestor a where a is a descendent of s and a is node-aligned. Each D thus represents the right-hand side of a minimal SCFG rule rooted at s. Due to the introduction of overlapping virtual nodes, the decomposition step may involve finding multiple sets of decomposition points when there are multiple nodes with the same span at the same level of the tree.
The second phase involves composition of all rules derived from each element of D subject to certain constraints. Rules are constructed using s, the set of nodes T s = {t | s is aligned to t}, and each decomposed node set D. The set of left-hand sides is {s} &#215; T s , but there may be many right-hand sides for a given t and D. Define rhs(d) as the set of right-hand sides of rules that are derived from d, plus all alignments of d to its aligned set T d . If d is a terminal, word alignments are used in the place of node alignments. To create a set of right-hand sides, we generate the set R = rhs(d 1 ) &#215; . . . &#215; rhs(d n ). For each r &#8712; R, we execute a combine operation such that combine(r) creates a new right-hand side by combining the component right-hand sides and recalculating co-indexes between the source- and target-side nonterminals. Finally, we insert any unaligned terminals on either side.
We work through a small example of grammar extraction using Figure 2, which replicates a fragment of Figure 1 with virtual nodes included. The English node JJ is aligned to the French nodes A and AP, the English node NNS is aligned to the French node N and the virtual node D+N, and the English node NP is aligned to the French node NP and the
&#9127;
&#9130;&#9128; rhs(N+AP) =
&#9130;&#9129;
[N+AP 1 ] :: [NP 1 ], [N 1 AP 2 ] :: [JJ 2 NNS 1 ], [N 1 A 2 ] :: [JJ 2 NNS 1 ], [voitures AP 1 ] :: [JJ 1 cars], [voitures A 1 ] :: [JJ 1 cars], [N 1 bleues] :: [blue NNS 1 ], [voitures bleues] :: [blue cars]
Next we must combine these pieces. For example, from D 1 we derive the full right-hand sides
1. combine([les voitures]::[cars], [bleues]::[blue]) = [les voitures bleues]::[blue cars]
2. combine([les voitures]::[cars], [A 1 ]::[JJ 1 ]) = [les voitures A 1 ]::[JJ 1 cars]
3. combine([les voitures]::[cars], [AP 1 ]::[JJ 1 ]) = [les voitures AP 1 ]::[JJ 1 cars]
4. combine([D+N 1 ]::[NNS 1 ], [bleues]::[blue]) = [D+N 1 bleues]::[blue NNS 1 ]
&#9131;
&#9130;&#9132;
&#9130;&#9133;
virtual node N+AP. To extract rules from the French node NP, we consider two potential decompositions: D 1 = {D+N, AP} and D 2 = {les, N+AP}. Since the French NP is aligned only to the English NP, the set of left-hand sides is {NP::NP}, where we use the symbol &#8220;::&#8221; to separate the source and target sides of joint nonterminal label or a rule.
In the next step, we use cached rules and alignments to generate all potential right-hand-side pieces from these top-level nodes:
rhs(D+N) =
&#9127; &#9128; rhs(AP) = &#9129; { [D+N 1 ] :: [NNS 1 ],
[les voitures] :: [cars]
[AP 1 ] :: [JJ 1 ], [A 1 ] :: [JJ 1 ], [bleues] :: [blue]
rhs(les) = &#8709;
&#9131; &#9132;
&#9133;
}
5. combine([D+N 1 ]::[NNS 1 ], [A 1 ]::[JJ 1 ]) = [D+N 1 A 2 ]::[JJ 2 NNS 1 ]
6. combine([D+N 1 ]::[NNS 1 ], [AP 1 ]::[JJ 1 ]) = [D+N 1 AP 2 ]::[JJ 2 NNS 1 ]
Similarly, we derive seven full right-hand sides from D 2 . Since rhs(les) is empty, rules derived have right-hand sides equivalent to rhs(N+AP) with the unaligned les added on the source side to complete the span of the French NP. For example, combine([N+AP 1 ]::[NP 1 ]) = [les N+AP 1 ]::[NP 1 ].
In the final step, the left-hand side is added to each full right-hand side. Thus,
NP :: NP &#8594; [les voitures A 1 ] :: [JJ 1 cars]
is one example rule extracted from this tree.
The number of rules can grow rapidly: if the parse tree has a branching factor of b and a depth of h, there are potentially O(2 bh ) rules extracted. To control this, we allow certain constraints on the rules extracted that can short-circuit right-hand-side formation. We allow separate restrictions on the number of items that may appear on the right-hand side of phrase pair rules (max p ) and hierarchical grammar rules (max g ). We also optionally allow the exclusion of parallel unary rules &#8212; that is, rules whose right-hand sides consist solely of a pair of aligned nonterminals.
3 Comparison to Other Methods
Table 1 compares the rule extractor described in Section 2 to other SCFG extraction methods described in the literature. We include comparisons of our work against the Hiero system (Chiang, 2005), the Stat-XFER system rule learner most recently described by Ambati et al. (2009), the composed version of GHKM rule extraction (Galley et al., 2006), the so-called Syntax-Augmented MT (SAMT) system (Zollmann and Venugopal, 2006), and a Hiero&#8211; SAMT extension with source- and target-side syntax described by Chiang (2010). Note that some of these methods make use of only target-side parse trees &#8212; or no parse trees at all, in the case of Hiero &#8212; but our primary interest in comparison is the constraints placed on the rule extraction process rather than the final output form of the rules themselves. We highlight four specific dimensions along these lines.
Tree Constraints. As we mentioned in this paper&#8217;s introduction, we do not allow any part of our extracted rules to violate constituent boundaries in the input parse trees. This is in contrast to Hieroderived techniques, which focus on expanding grammar coverage by extracting rules for all spans in the input sentence pair that are consistently wordaligned, regardless of their correspondence to linguistic constituents. Practitioners of both phrasebased and syntax-based SMT have reported severe grammar coverage issues when rules are required to exactly match parse constituents (Koehn et al., 2003; Chiang, 2010). In our work, we attempt to improve the coverage of the grammar by allowing multiple node alignments, virtual nodes, and multiple tree decompositions rather than ignoring structure constraints.
Multiple Alignments. In contrast to all other extraction methods in Table 1, ours allows a node in one parse tree to be aligned with multiple nodes in the other tree, as long as the word-alignment and structure constraints are satisfied. However, we do not allow a node to have multiple simultaneous alignments &#8212; a single node alignment must be chosen for extracting an individual rule. In practice, this prevents extraction of &#8220;triangle&#8221; rules where the same node appears on both the left- and right-hand side of the same rule. 1 Virtual Nodes. In keeping with our philosophy of representing multiple alignments, our use of multiple and overlapping virtual nodes is less restrictive than the single-alignment constraint of Stat-XFER. Another key difference is that Stat-XFER requires all virtual nodes to be aligned to original nodes in the other language, while we permit virtual&#8211;virtual node alignments. In respecting existing tree structure constraints, our virtual node placement is more restrictive than SAMT or Chiang, where extracted nodes may cross existing constituent boundaries.
Multiple Derivations. Galley et al. (2006) argued that breaking a single tree pair into multiple decompositions is important for correct probability modeling. We agree, and we base our rule extractor&#8217;s acquisition of multiple derivations per tree pair on techniques from both GHKM and Hiero. More specifically, we borrow from Hiero the idea of creating hierarchical rules by subtracting and abstracting all possible subsets of smaller phrases (aligned nodes in our case) from larger phrases. Like GHKM,
1 Figure 2 includes a potential triangle rule, D+N :: NNS &#8594;
[les N 1 ] :: [NNS 1 ], where the English NNS node appears on both sides of the rule. It is simultaneously aligned to the French D+N and N nodes.
we do this exhaustively within some limit, although in our case we use a rank limit on a rule&#8217;s right-hand side rather than a limit on the depth of the subnode subtractions. Our constraint achieves the goal of controlling the size of the rule set while remaining flexibile in terms of depth depending on the shape of the parse trees.
4 Experiments
We conducted experiments with our rule extractor on the FBIS corpus, made up of approximately 302,000 Chinese&#8211;English sentence pairs. We parsed the corpus with the Chinese and English grammars of the Berkeley parser (Petrov and Klein, 2007) and word-aligned it with GIZA++ (Och and Ney, 2003). The parsed and word-aligned FBIS corpus served as the input to our rule extractor, which we ran with a number of different settings.
First, we acquired a baseline rule extraction (&#8220;xfer-orig&#8221;) from our corpus using an implementation of the basic Stat-XFER rule learner (Lavie et al., 2008), which decomposes each input tree pair into a single set of minimal SCFG rules 2 using only original nodes in the parse trees. Next, we tested the effect of allowing multiple decompositions by running our own rule learner, but restricting its rules to also only make use of original nodes (&#8220;compatible&#8221;). Finally, we investigated the total number of extractable rules by allowing the creation of virtual nodes from up to four adjacent sibling nodes and placing two different limits on the length of the right-hand side (&#8220;full-short&#8221; and &#8220;full-long&#8221;). These configurations are summarized in Table 2.
Rule Set max p max g Virtual Unary
2 In practice, some Stat-XFER aligned nodes produce two
rules instead of one: a minimal hierarchical SCFG rule is always produced, and a phrase pair rule will also be produced for node yields within the max p cutoff.
4.1 Rules Extracted
As expected, we find that allowing multiple decompositions of each tree pair has a significant effect on the number of extracted rules. Table 3 breaks the extracted rules for each configuration down into phrase pairs (all terminals on the right-hand side) and hierarchical rules (containing at least one nonterminal on the right-hand side). We also count the number of extracted rule instances (tokens) against the number of unique rules (types). The results show that multiple decomposition leads to a four-fold increase in the number of extracted grammar rules, even when the length of the Stat-XFER baseline rules is unbounded. The number of extracted phrase pairs shows a smaller increase, but this is expected: the number of possible phrase pairs is proportional to the square of the sentence length, while the number of possible hierarchical rules is exponential, so there is more room for coverage improvement in the hierarchical grammar.
With virtual nodes included, there is again a large jump in both the number of extracted rule tokens and types, even at relatively short length limits. When both max p and max g are set to 7, our rule extractor produces 1.5 times as many unique phrase pairs and 20.5 times as many unique hierarchical rules as the baseline Stat-XFER system, and nearly twice the number of hierarchical rules as when using length limits of 5. Ambati et al. (2009) showed the usefulness of extending rule extraction from exact original&#8211;original node alignments to cases in which original&#8211;virtual and virtual&#8211;original alignments were also permitted. Our experiments confirm this, as only 60% (full-short) and 54% (fulllong) of our extracted rule types are made up of only original&#8211;original node alignments. Further, we find a contribution from the new virtual&#8211;virtual case: approximately 8% of the rules extracted in the &#8220;fulllong&#8221; configuration from Table 3 are headed by a virtual&#8211;virtual alignment, and a similar number have a virtual&#8211;virtual alignment on their right-hand sides.
All four of the extracted rule sets show Zipfian distributions over rule frequency counts. In the xferorig, full-short, and full-long configurations, between 82% and 86% of the extracted phrase pair rules, and between 88% and 92% of the extracted hierarchical rules, were observed only once. These
Extracted Instances Unique Rules Rule Set Phrase Hierarchical Phrase Hierarchical
percentages are remarkably consistent despite substantial changes in grammar size, meaning that our more exhaustive method of rule extraction does not produce a disproportionate number of singletons. 3 On the other hand, it does weaken the average count of an extracted hierarchical rule type. From Table 3, we can compute that the average phrase pair count remains at 3.5 when we move from xfer-orig to the two full configurations; however, the average hierarchical rule count drops from 2.4 to 1.7 (full-short) and finally 1.4 (full-long). This likely again reflects the exponential increase in the number of extractable hierarchical rules compared to the quadratic increase in the phrase pairs.
4.2 Translation Results
The grammars obtained from our rule extractor can be filtered and formatted for use with a variety of SCFG-based decoders and rule formats. We carried out end-to-end translation experiments with the various extracted rule sets from the FBIS corpus using the open-source decoder Joshua (Li et al., 2009). Given a source-language string, Joshua translates by producing a synchronous parse of it according to a scored SCFG and a target-side language model. A significant engineering challenge in building a real MT system of this type is selecting a more moderatesized subset of all extracted rules to retain in the final translation model. This is an especially important consideration when dealing with expanded rule sets derived from virtual nodes and multiple decompositions in each input tree.
In our experiments, we pass all grammars through</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Given the final set of node alignments between the source tree and the target tree, SCFG rules are obtained via a grammar extraction step.</text>
                  <doc_id>44</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Rule extraction proceeds in a depth-first manner, such that rules are extracted and cached for all descendents of a source node s before rules in which s is the left-hand side are considered.</text>
                  <doc_id>45</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Extracting rules where source node s is the left-hand side consists of two phases: decomposition and combination.</text>
                  <doc_id>46</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The first phase is decomposition of node s into all distinct sets D = {d 1 , d 2 , .</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>48</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>49</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>, d n } of descendent nodes such that D spans the entire yield of node s, where d i &#8712; D is node-aligned or is an unaligned terminal for all i, and d i has no ancestor a where a is a descendent of s and a is node-aligned.</text>
                  <doc_id>50</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Each D thus represents the right-hand side of a minimal SCFG rule rooted at s.</text>
                  <doc_id>51</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Due to the introduction of overlapping virtual nodes, the decomposition step may involve finding multiple sets of decomposition points when there are multiple nodes with the same span at the same level of the tree.</text>
                  <doc_id>52</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The second phase involves composition of all rules derived from each element of D subject to certain constraints.</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Rules are constructed using s, the set of nodes T s = {t | s is aligned to t}, and each decomposed node set D.</text>
                  <doc_id>54</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The set of left-hand sides is {s} &#215; T s , but there may be many right-hand sides for a given t and D.</text>
                  <doc_id>55</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Define rhs(d) as the set of right-hand sides of rules that are derived from d, plus all alignments of d to its aligned set T d .</text>
                  <doc_id>56</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>If d is a terminal, word alignments are used in the place of node alignments.</text>
                  <doc_id>57</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>To create a set of right-hand sides, we generate the set R = rhs(d 1 ) &#215; .</text>
                  <doc_id>58</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>59</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>60</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>&#215; rhs(d n ).</text>
                  <doc_id>61</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>For each r &#8712; R, we execute a combine operation such that combine(r) creates a new right-hand side by combining the component right-hand sides and recalculating co-indexes between the source- and target-side nonterminals.</text>
                  <doc_id>62</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, we insert any unaligned terminals on either side.</text>
                  <doc_id>63</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We work through a small example of grammar extraction using Figure 2, which replicates a fragment of Figure 1 with virtual nodes included.</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The English node JJ is aligned to the French nodes A and AP, the English node NNS is aligned to the French node N and the virtual node D+N, and the English node NP is aligned to the French node NP and the</text>
                  <doc_id>65</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9127;</text>
                  <doc_id>66</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9130;&#9128; rhs(N+AP) =</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9130;&#9129;</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>[N+AP 1 ] :: [NP 1 ], [N 1 AP 2 ] :: [JJ 2 NNS 1 ], [N 1 A 2 ] :: [JJ 2 NNS 1 ], [voitures AP 1 ] :: [JJ 1 cars], [voitures A 1 ] :: [JJ 1 cars], [N 1 bleues] :: [blue NNS 1 ], [voitures bleues] :: [blue cars]</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Next we must combine these pieces.</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For example, from D 1 we derive the full right-hand sides</text>
                  <doc_id>71</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1. combine([les voitures]::[cars], [bleues]::[blue]) = [les voitures bleues]::[blue cars]</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2. combine([les voitures]::[cars], [A 1 ]::[JJ 1 ]) = [les voitures A 1 ]::[JJ 1 cars]</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3. combine([les voitures]::[cars], [AP 1 ]::[JJ 1 ]) = [les voitures AP 1 ]::[JJ 1 cars]</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4. combine([D+N 1 ]::[NNS 1 ], [bleues]::[blue]) = [D+N 1 bleues]::[blue NNS 1 ]</text>
                  <doc_id>75</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9131;</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9130;&#9132;</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9130;&#9133;</text>
                  <doc_id>78</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>virtual node N+AP.</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To extract rules from the French node NP, we consider two potential decompositions: D 1 = {D+N, AP} and D 2 = {les, N+AP}.</text>
                  <doc_id>80</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Since the French NP is aligned only to the English NP, the set of left-hand sides is {NP::NP}, where we use the symbol &#8220;::&#8221; to separate the source and target sides of joint nonterminal label or a rule.</text>
                  <doc_id>81</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In the next step, we use cached rules and alignments to generate all potential right-hand-side pieces from these top-level nodes:</text>
                  <doc_id>82</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>rhs(D+N) =</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9127; &#9128; rhs(AP) = &#9129; { [D+N 1 ] :: [NNS 1 ],</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>[les voitures] :: [cars]</text>
                  <doc_id>85</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>[AP 1 ] :: [JJ 1 ], [A 1 ] :: [JJ 1 ], [bleues] :: [blue]</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>rhs(les) = &#8709;</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9131; &#9132;</text>
                  <doc_id>88</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9133;</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>}</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5. combine([D+N 1 ]::[NNS 1 ], [A 1 ]::[JJ 1 ]) = [D+N 1 A 2 ]::[JJ 2 NNS 1 ]</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>6. combine([D+N 1 ]::[NNS 1 ], [AP 1 ]::[JJ 1 ]) = [D+N 1 AP 2 ]::[JJ 2 NNS 1 ]</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Similarly, we derive seven full right-hand sides from D 2 .</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Since rhs(les) is empty, rules derived have right-hand sides equivalent to rhs(N+AP) with the unaligned les added on the source side to complete the span of the French NP.</text>
                  <doc_id>94</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For example, combine([N+AP 1 ]::[NP 1 ]) = [les N+AP 1 ]::[NP 1 ].</text>
                  <doc_id>95</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In the final step, the left-hand side is added to each full right-hand side.</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Thus,</text>
                  <doc_id>97</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NP :: NP &#8594; [les voitures A 1 ] :: [JJ 1 cars]</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>is one example rule extracted from this tree.</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The number of rules can grow rapidly: if the parse tree has a branching factor of b and a depth of h, there are potentially O(2 bh ) rules extracted.</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To control this, we allow certain constraints on the rules extracted that can short-circuit right-hand-side formation.</text>
                  <doc_id>101</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We allow separate restrictions on the number of items that may appear on the right-hand side of phrase pair rules (max p ) and hierarchical grammar rules (max g ).</text>
                  <doc_id>102</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We also optionally allow the exclusion of parallel unary rules &#8212; that is, rules whose right-hand sides consist solely of a pair of aligned nonterminals.</text>
                  <doc_id>103</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3 Comparison to Other Methods</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 1 compares the rule extractor described in Section 2 to other SCFG extraction methods described in the literature.</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We include comparisons of our work against the Hiero system (Chiang, 2005), the Stat-XFER system rule learner most recently described by Ambati et al. (2009), the composed version of GHKM rule extraction (Galley et al., 2006), the so-called Syntax-Augmented MT (SAMT) system (Zollmann and Venugopal, 2006), and a Hiero&#8211; SAMT extension with source- and target-side syntax described by Chiang (2010).</text>
                  <doc_id>106</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Note that some of these methods make use of only target-side parse trees &#8212; or no parse trees at all, in the case of Hiero &#8212; but our primary interest in comparison is the constraints placed on the rule extraction process rather than the final output form of the rules themselves.</text>
                  <doc_id>107</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We highlight four specific dimensions along these lines.</text>
                  <doc_id>108</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Tree Constraints.</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As we mentioned in this paper&#8217;s introduction, we do not allow any part of our extracted rules to violate constituent boundaries in the input parse trees.</text>
                  <doc_id>110</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This is in contrast to Hieroderived techniques, which focus on expanding grammar coverage by extracting rules for all spans in the input sentence pair that are consistently wordaligned, regardless of their correspondence to linguistic constituents.</text>
                  <doc_id>111</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Practitioners of both phrasebased and syntax-based SMT have reported severe grammar coverage issues when rules are required to exactly match parse constituents (Koehn et al., 2003; Chiang, 2010).</text>
                  <doc_id>112</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In our work, we attempt to improve the coverage of the grammar by allowing multiple node alignments, virtual nodes, and multiple tree decompositions rather than ignoring structure constraints.</text>
                  <doc_id>113</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Multiple Alignments.</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast to all other extraction methods in Table 1, ours allows a node in one parse tree to be aligned with multiple nodes in the other tree, as long as the word-alignment and structure constraints are satisfied.</text>
                  <doc_id>115</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, we do not allow a node to have multiple simultaneous alignments &#8212; a single node alignment must be chosen for extracting an individual rule.</text>
                  <doc_id>116</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In practice, this prevents extraction of &#8220;triangle&#8221; rules where the same node appears on both the left- and right-hand side of the same rule.</text>
                  <doc_id>117</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>1 Virtual Nodes.</text>
                  <doc_id>118</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In keeping with our philosophy of representing multiple alignments, our use of multiple and overlapping virtual nodes is less restrictive than the single-alignment constraint of Stat-XFER.</text>
                  <doc_id>119</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Another key difference is that Stat-XFER requires all virtual nodes to be aligned to original nodes in the other language, while we permit virtual&#8211;virtual node alignments.</text>
                  <doc_id>120</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>In respecting existing tree structure constraints, our virtual node placement is more restrictive than SAMT or Chiang, where extracted nodes may cross existing constituent boundaries.</text>
                  <doc_id>121</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Multiple Derivations.</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Galley et al. (2006) argued that breaking a single tree pair into multiple decompositions is important for correct probability modeling.</text>
                  <doc_id>123</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We agree, and we base our rule extractor&#8217;s acquisition of multiple derivations per tree pair on techniques from both GHKM and Hiero.</text>
                  <doc_id>124</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>More specifically, we borrow from Hiero the idea of creating hierarchical rules by subtracting and abstracting all possible subsets of smaller phrases (aligned nodes in our case) from larger phrases.</text>
                  <doc_id>125</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Like GHKM,</text>
                  <doc_id>126</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 Figure 2 includes a potential triangle rule, D+N :: NNS &#8594;</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>[les N 1 ] :: [NNS 1 ], where the English NNS node appears on both sides of the rule.</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It is simultaneously aligned to the French D+N and N nodes.</text>
                  <doc_id>129</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>we do this exhaustively within some limit, although in our case we use a rank limit on a rule&#8217;s right-hand side rather than a limit on the depth of the subnode subtractions.</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Our constraint achieves the goal of controlling the size of the rule set while remaining flexibile in terms of depth depending on the shape of the parse trees.</text>
                  <doc_id>131</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4 Experiments</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We conducted experiments with our rule extractor on the FBIS corpus, made up of approximately 302,000 Chinese&#8211;English sentence pairs.</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We parsed the corpus with the Chinese and English grammars of the Berkeley parser (Petrov and Klein, 2007) and word-aligned it with GIZA++ (Och and Ney, 2003).</text>
                  <doc_id>134</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The parsed and word-aligned FBIS corpus served as the input to our rule extractor, which we ran with a number of different settings.</text>
                  <doc_id>135</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>First, we acquired a baseline rule extraction (&#8220;xfer-orig&#8221;) from our corpus using an implementation of the basic Stat-XFER rule learner (Lavie et al., 2008), which decomposes each input tree pair into a single set of minimal SCFG rules 2 using only original nodes in the parse trees.</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Next, we tested the effect of allowing multiple decompositions by running our own rule learner, but restricting its rules to also only make use of original nodes (&#8220;compatible&#8221;).</text>
                  <doc_id>137</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, we investigated the total number of extractable rules by allowing the creation of virtual nodes from up to four adjacent sibling nodes and placing two different limits on the length of the right-hand side (&#8220;full-short&#8221; and &#8220;full-long&#8221;).</text>
                  <doc_id>138</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>These configurations are summarized in Table 2.</text>
                  <doc_id>139</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Rule Set max p max g Virtual Unary</text>
                  <doc_id>140</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 In practice, some Stat-XFER aligned nodes produce two</text>
                  <doc_id>141</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>rules instead of one: a minimal hierarchical SCFG rule is always produced, and a phrase pair rule will also be produced for node yields within the max p cutoff.</text>
                  <doc_id>142</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.1 Rules Extracted</text>
                  <doc_id>143</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As expected, we find that allowing multiple decompositions of each tree pair has a significant effect on the number of extracted rules.</text>
                  <doc_id>144</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Table 3 breaks the extracted rules for each configuration down into phrase pairs (all terminals on the right-hand side) and hierarchical rules (containing at least one nonterminal on the right-hand side).</text>
                  <doc_id>145</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We also count the number of extracted rule instances (tokens) against the number of unique rules (types).</text>
                  <doc_id>146</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The results show that multiple decomposition leads to a four-fold increase in the number of extracted grammar rules, even when the length of the Stat-XFER baseline rules is unbounded.</text>
                  <doc_id>147</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The number of extracted phrase pairs shows a smaller increase, but this is expected: the number of possible phrase pairs is proportional to the square of the sentence length, while the number of possible hierarchical rules is exponential, so there is more room for coverage improvement in the hierarchical grammar.</text>
                  <doc_id>148</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>With virtual nodes included, there is again a large jump in both the number of extracted rule tokens and types, even at relatively short length limits.</text>
                  <doc_id>149</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>When both max p and max g are set to 7, our rule extractor produces 1.5 times as many unique phrase pairs and 20.5 times as many unique hierarchical rules as the baseline Stat-XFER system, and nearly twice the number of hierarchical rules as when using length limits of 5.</text>
                  <doc_id>150</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Ambati et al. (2009) showed the usefulness of extending rule extraction from exact original&#8211;original node alignments to cases in which original&#8211;virtual and virtual&#8211;original alignments were also permitted.</text>
                  <doc_id>151</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Our experiments confirm this, as only 60% (full-short) and 54% (fulllong) of our extracted rule types are made up of only original&#8211;original node alignments.</text>
                  <doc_id>152</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Further, we find a contribution from the new virtual&#8211;virtual case: approximately 8% of the rules extracted in the &#8220;fulllong&#8221; configuration from Table 3 are headed by a virtual&#8211;virtual alignment, and a similar number have a virtual&#8211;virtual alignment on their right-hand sides.</text>
                  <doc_id>153</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>All four of the extracted rule sets show Zipfian distributions over rule frequency counts.</text>
                  <doc_id>154</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the xferorig, full-short, and full-long configurations, between 82% and 86% of the extracted phrase pair rules, and between 88% and 92% of the extracted hierarchical rules, were observed only once.</text>
                  <doc_id>155</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>These</text>
                  <doc_id>156</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Extracted Instances Unique Rules Rule Set Phrase Hierarchical Phrase Hierarchical</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>percentages are remarkably consistent despite substantial changes in grammar size, meaning that our more exhaustive method of rule extraction does not produce a disproportionate number of singletons.</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>3 On the other hand, it does weaken the average count of an extracted hierarchical rule type.</text>
                  <doc_id>159</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>From Table 3, we can compute that the average phrase pair count remains at 3.5 when we move from xfer-orig to the two full configurations; however, the average hierarchical rule count drops from 2.4 to 1.7 (full-short) and finally 1.4 (full-long).</text>
                  <doc_id>160</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This likely again reflects the exponential increase in the number of extractable hierarchical rules compared to the quadratic increase in the phrase pairs.</text>
                  <doc_id>161</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.2 Translation Results</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The grammars obtained from our rule extractor can be filtered and formatted for use with a variety of SCFG-based decoders and rule formats.</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We carried out end-to-end translation experiments with the various extracted rule sets from the FBIS corpus using the open-source decoder Joshua (Li et al., 2009).</text>
                  <doc_id>164</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Given a source-language string, Joshua translates by producing a synchronous parse of it according to a scored SCFG and a target-side language model.</text>
                  <doc_id>165</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>A significant engineering challenge in building a real MT system of this type is selecting a more moderatesized subset of all extracted rules to retain in the final translation model.</text>
                  <doc_id>166</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This is an especially important consideration when dealing with expanded rule sets derived from virtual nodes and multiple decompositions in each input tree.</text>
                  <doc_id>167</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In our experiments, we pass all grammars through</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 The compatible configuration is somewhat of an outlier. It</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>169</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Experiments</title>
        <text>We conducted experiments with our rule extractor on the FBIS corpus, made up of approximately 302,000 Chinese&#8211;English sentence pairs. We parsed the corpus with the Chinese and English grammars of the Berkeley parser (Petrov and Klein, 2007) and word-aligned it with GIZA++ (Och and Ney, 2003). The parsed and word-aligned FBIS corpus served as the input to our rule extractor, which we ran with a number of different settings.
First, we acquired a baseline rule extraction (&#8220;xfer-orig&#8221;) from our corpus using an implementation of the basic Stat-XFER rule learner (Lavie et al., 2008), which decomposes each input tree pair into a single set of minimal SCFG rules 2 using only original nodes in the parse trees. Next, we tested the effect of allowing multiple decompositions by running our own rule learner, but restricting its rules to also only make use of original nodes (&#8220;compatible&#8221;). Finally, we investigated the total number of extractable rules by allowing the creation of virtual nodes from up to four adjacent sibling nodes and placing two different limits on the length of the right-hand side (&#8220;full-short&#8221; and &#8220;full-long&#8221;). These configurations are summarized in Table 2.
Rule Set max p max g Virtual Unary
2 In practice, some Stat-XFER aligned nodes produce two
rules instead of one: a minimal hierarchical SCFG rule is always produced, and a phrase pair rule will also be produced for node yields within the max p cutoff.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We conducted experiments with our rule extractor on the FBIS corpus, made up of approximately 302,000 Chinese&#8211;English sentence pairs.</text>
              <doc_id>170</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We parsed the corpus with the Chinese and English grammars of the Berkeley parser (Petrov and Klein, 2007) and word-aligned it with GIZA++ (Och and Ney, 2003).</text>
              <doc_id>171</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The parsed and word-aligned FBIS corpus served as the input to our rule extractor, which we ran with a number of different settings.</text>
              <doc_id>172</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>First, we acquired a baseline rule extraction (&#8220;xfer-orig&#8221;) from our corpus using an implementation of the basic Stat-XFER rule learner (Lavie et al., 2008), which decomposes each input tree pair into a single set of minimal SCFG rules 2 using only original nodes in the parse trees.</text>
              <doc_id>173</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Next, we tested the effect of allowing multiple decompositions by running our own rule learner, but restricting its rules to also only make use of original nodes (&#8220;compatible&#8221;).</text>
              <doc_id>174</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we investigated the total number of extractable rules by allowing the creation of virtual nodes from up to four adjacent sibling nodes and placing two different limits on the length of the right-hand side (&#8220;full-short&#8221; and &#8220;full-long&#8221;).</text>
              <doc_id>175</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>These configurations are summarized in Table 2.</text>
              <doc_id>176</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Rule Set max p max g Virtual Unary</text>
              <doc_id>177</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2 In practice, some Stat-XFER aligned nodes produce two</text>
              <doc_id>178</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>rules instead of one: a minimal hierarchical SCFG rule is always produced, and a phrase pair rule will also be produced for node yields within the max p cutoff.</text>
              <doc_id>179</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Rules Extracted</title>
            <text>As expected, we find that allowing multiple decompositions of each tree pair has a significant effect on the number of extracted rules. Table 3 breaks the extracted rules for each configuration down into phrase pairs (all terminals on the right-hand side) and hierarchical rules (containing at least one nonterminal on the right-hand side). We also count the number of extracted rule instances (tokens) against the number of unique rules (types). The results show that multiple decomposition leads to a four-fold increase in the number of extracted grammar rules, even when the length of the Stat-XFER baseline rules is unbounded. The number of extracted phrase pairs shows a smaller increase, but this is expected: the number of possible phrase pairs is proportional to the square of the sentence length, while the number of possible hierarchical rules is exponential, so there is more room for coverage improvement in the hierarchical grammar.
With virtual nodes included, there is again a large jump in both the number of extracted rule tokens and types, even at relatively short length limits. When both max p and max g are set to 7, our rule extractor produces 1.5 times as many unique phrase pairs and 20.5 times as many unique hierarchical rules as the baseline Stat-XFER system, and nearly twice the number of hierarchical rules as when using length limits of 5. Ambati et al. (2009) showed the usefulness of extending rule extraction from exact original&#8211;original node alignments to cases in which original&#8211;virtual and virtual&#8211;original alignments were also permitted. Our experiments confirm this, as only 60% (full-short) and 54% (fulllong) of our extracted rule types are made up of only original&#8211;original node alignments. Further, we find a contribution from the new virtual&#8211;virtual case: approximately 8% of the rules extracted in the &#8220;fulllong&#8221; configuration from Table 3 are headed by a virtual&#8211;virtual alignment, and a similar number have a virtual&#8211;virtual alignment on their right-hand sides.
All four of the extracted rule sets show Zipfian distributions over rule frequency counts. In the xferorig, full-short, and full-long configurations, between 82% and 86% of the extracted phrase pair rules, and between 88% and 92% of the extracted hierarchical rules, were observed only once. These
Extracted Instances Unique Rules Rule Set Phrase Hierarchical Phrase Hierarchical
percentages are remarkably consistent despite substantial changes in grammar size, meaning that our more exhaustive method of rule extraction does not produce a disproportionate number of singletons. 3 On the other hand, it does weaken the average count of an extracted hierarchical rule type. From Table 3, we can compute that the average phrase pair count remains at 3.5 when we move from xfer-orig to the two full configurations; however, the average hierarchical rule count drops from 2.4 to 1.7 (full-short) and finally 1.4 (full-long). This likely again reflects the exponential increase in the number of extractable hierarchical rules compared to the quadratic increase in the phrase pairs.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>As expected, we find that allowing multiple decompositions of each tree pair has a significant effect on the number of extracted rules.</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Table 3 breaks the extracted rules for each configuration down into phrase pairs (all terminals on the right-hand side) and hierarchical rules (containing at least one nonterminal on the right-hand side).</text>
                  <doc_id>181</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We also count the number of extracted rule instances (tokens) against the number of unique rules (types).</text>
                  <doc_id>182</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The results show that multiple decomposition leads to a four-fold increase in the number of extracted grammar rules, even when the length of the Stat-XFER baseline rules is unbounded.</text>
                  <doc_id>183</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The number of extracted phrase pairs shows a smaller increase, but this is expected: the number of possible phrase pairs is proportional to the square of the sentence length, while the number of possible hierarchical rules is exponential, so there is more room for coverage improvement in the hierarchical grammar.</text>
                  <doc_id>184</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>With virtual nodes included, there is again a large jump in both the number of extracted rule tokens and types, even at relatively short length limits.</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>When both max p and max g are set to 7, our rule extractor produces 1.5 times as many unique phrase pairs and 20.5 times as many unique hierarchical rules as the baseline Stat-XFER system, and nearly twice the number of hierarchical rules as when using length limits of 5.</text>
                  <doc_id>186</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Ambati et al. (2009) showed the usefulness of extending rule extraction from exact original&#8211;original node alignments to cases in which original&#8211;virtual and virtual&#8211;original alignments were also permitted.</text>
                  <doc_id>187</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Our experiments confirm this, as only 60% (full-short) and 54% (fulllong) of our extracted rule types are made up of only original&#8211;original node alignments.</text>
                  <doc_id>188</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Further, we find a contribution from the new virtual&#8211;virtual case: approximately 8% of the rules extracted in the &#8220;fulllong&#8221; configuration from Table 3 are headed by a virtual&#8211;virtual alignment, and a similar number have a virtual&#8211;virtual alignment on their right-hand sides.</text>
                  <doc_id>189</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>All four of the extracted rule sets show Zipfian distributions over rule frequency counts.</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the xferorig, full-short, and full-long configurations, between 82% and 86% of the extracted phrase pair rules, and between 88% and 92% of the extracted hierarchical rules, were observed only once.</text>
                  <doc_id>191</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>These</text>
                  <doc_id>192</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Extracted Instances Unique Rules Rule Set Phrase Hierarchical Phrase Hierarchical</text>
                  <doc_id>193</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>percentages are remarkably consistent despite substantial changes in grammar size, meaning that our more exhaustive method of rule extraction does not produce a disproportionate number of singletons.</text>
                  <doc_id>194</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>3 On the other hand, it does weaken the average count of an extracted hierarchical rule type.</text>
                  <doc_id>195</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>From Table 3, we can compute that the average phrase pair count remains at 3.5 when we move from xfer-orig to the two full configurations; however, the average hierarchical rule count drops from 2.4 to 1.7 (full-short) and finally 1.4 (full-long).</text>
                  <doc_id>196</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This likely again reflects the exponential increase in the number of extractable hierarchical rules compared to the quadratic increase in the phrase pairs.</text>
                  <doc_id>197</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Translation Results</title>
            <text>The grammars obtained from our rule extractor can be filtered and formatted for use with a variety of SCFG-based decoders and rule formats. We carried out end-to-end translation experiments with the various extracted rule sets from the FBIS corpus using the open-source decoder Joshua (Li et al., 2009). Given a source-language string, Joshua translates by producing a synchronous parse of it according to a scored SCFG and a target-side language model. A significant engineering challenge in building a real MT system of this type is selecting a more moderatesized subset of all extracted rules to retain in the final translation model. This is an especially important consideration when dealing with expanded rule sets derived from virtual nodes and multiple decompositions in each input tree.
In our experiments, we pass all grammars through
3 The compatible configuration is somewhat of an outlier. It
has proportionally fewer singleton phrase pairs (80%) than the other variants, likely because it allows multiple alignments and multiple decompositions without allowing virtual nodes.
two preprocessing steps before any translation model scoring. First, we noticed that English cardinal numbers and punctuation marks in many languages tend to receive incorrect nonterminal labels during parsing, despite being closed-class items with clearly defined tags. Therefore, before rule extraction, we globally correct the nodel labels of allnumeral terminals in English and certain punctuation marks in both English and Chinese. Second, we attempt to reduce derivational ambiguity in cases where the same SCFG right-hand side appears in the grammar after extraction with a large number of possible left-hand-side labels. To this end, we sort the possible left-hand sides by frequency for each unique right-hand side, and we remove the least frequent 10 percent of the label distribution.
Our translation model scoring is based on the feature set of Hanneman et al. (2010). This includes the standard bidirectional conditional maximumlikelihood scores at both the word and phrase level on the right-hand side of rules. We also include maximum-likelihood scores for the left-hand-side label given all or part of the right-hand side. Using statistics local to each rule, we set binary indicator features for rules whose frequencies are &#8804; 3, plus five additional indicator features according to the format of the rule&#8217;s right-hand side, such as whether it is fully abstract. Since the system in this paper is not constructed using any non-syntactic rules, we do not include the Hanneman et al. (2010) &#8220;not labelable&#8221; maximum-likelihood features or the indicator features related to non-syntactic labels.
Beyond the above preprocessing and scoring common to all grammars, we experiment with three different solutions to the more difficult problem of selecting a final translation grammar. In any case, we separate phrase pair rules from hierarchical rules
Rule Set Filter BLEU TER MET
and include in the grammar all phrase pair rules matching a given tuning or testing set. Any improvement in phrase pair coverage during the extraction stage is thus directly passed along to decoding. For hierarchical rules, we experiment with retaining the 10,000 or 100,000 most frequently extracted unique rules. We also separate fully abstract hierarchical rules from partially lexicalized hierarchical rules, and in a further selection technique we retain the 5,000 most frequent abstract and 100,000 most frequent partially lexicalized rules.
Given these final rule sets, we tune our MT systems on the NIST MT 2006 data set using the minimum error-rate training package Z-MERT (Zaidan, 2009), and we test on NIST MT 2003. Both sets have four reference translations. Table 4 presents case-insensitive evaluation results on the test set according to the automatic metrics BLEU (Papineni et al., 2002), TER (Snover et al., 2006), and METEOR (Lavie and Denkowski, 2009). 4 The trend in the results is that including a larger grammar is generally better for performance, but filtering techniques also play a substantial role in determining how well a given grammar will perform at run time.
We first compare the results in Table 4 for different rule sets all filtered the same way at decoding time. With only 10,000 hierarchical rules in use (&#8220;10k&#8221;), the improvements in scores indicate that an important contribution is being made by the additional phrase pair coverage provided by each suc-
4 For METEOR scoring we use version 1.0 of the metric,
tuned to HTER with the exact, stemming, and synonymy modules enabled.
cessive rule set. The original Stat-XFER rule extraction provides 244,988 phrase pairs that match the MT 2003 test set. This is already increased to 520,995 in the compatible system using multiple decompositions. With virtual nodes enabled, the full system produces 766,379 matching phrase pairs up to length 5 or 776,707 up to length 7. These systems both score significantly higher than the Stat-XFER baseline according to BLEU and TER, and the ME- TEOR scores are likely statistically equivalent. Across all configurations, we find that changing the grammar filtering technique &#8212; possibly combined with retuned decoder feature weights &#8212; also has a large influence on automatic metric scores. Larger hierarchical grammars tend to score better, in some cases to the point of erasing the score differences between rule sets. From this we conclude that making effective use of the extracted grammar, no matter its size, with intelligent filtering techniques is at least as important as the number and type of rules extracted overall. Though the filtering results in Table 4 are still somewhat inconclusive, the relative success of the &#8220;5k+100k&#8221; setting shows that filtering fully abstract and partially lexicalized rules separately is a reasonable starting approach. While fully abstract rules do tend to be more frequently observed in grammar extraction, and thus more reliably scored in the translation model, they also have the ability to overapply at decoding time because their use is not restricted to any particular lexical context.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The grammars obtained from our rule extractor can be filtered and formatted for use with a variety of SCFG-based decoders and rule formats.</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We carried out end-to-end translation experiments with the various extracted rule sets from the FBIS corpus using the open-source decoder Joshua (Li et al., 2009).</text>
                  <doc_id>199</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Given a source-language string, Joshua translates by producing a synchronous parse of it according to a scored SCFG and a target-side language model.</text>
                  <doc_id>200</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>A significant engineering challenge in building a real MT system of this type is selecting a more moderatesized subset of all extracted rules to retain in the final translation model.</text>
                  <doc_id>201</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This is an especially important consideration when dealing with expanded rule sets derived from virtual nodes and multiple decompositions in each input tree.</text>
                  <doc_id>202</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In our experiments, we pass all grammars through</text>
                  <doc_id>203</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3 The compatible configuration is somewhat of an outlier.</text>
                  <doc_id>204</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It</text>
                  <doc_id>205</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>has proportionally fewer singleton phrase pairs (80%) than the other variants, likely because it allows multiple alignments and multiple decompositions without allowing virtual nodes.</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>two preprocessing steps before any translation model scoring.</text>
                  <doc_id>207</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>First, we noticed that English cardinal numbers and punctuation marks in many languages tend to receive incorrect nonterminal labels during parsing, despite being closed-class items with clearly defined tags.</text>
                  <doc_id>208</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, before rule extraction, we globally correct the nodel labels of allnumeral terminals in English and certain punctuation marks in both English and Chinese.</text>
                  <doc_id>209</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Second, we attempt to reduce derivational ambiguity in cases where the same SCFG right-hand side appears in the grammar after extraction with a large number of possible left-hand-side labels.</text>
                  <doc_id>210</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>To this end, we sort the possible left-hand sides by frequency for each unique right-hand side, and we remove the least frequent 10 percent of the label distribution.</text>
                  <doc_id>211</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Our translation model scoring is based on the feature set of Hanneman et al. (2010).</text>
                  <doc_id>212</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This includes the standard bidirectional conditional maximumlikelihood scores at both the word and phrase level on the right-hand side of rules.</text>
                  <doc_id>213</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We also include maximum-likelihood scores for the left-hand-side label given all or part of the right-hand side.</text>
                  <doc_id>214</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Using statistics local to each rule, we set binary indicator features for rules whose frequencies are &#8804; 3, plus five additional indicator features according to the format of the rule&#8217;s right-hand side, such as whether it is fully abstract.</text>
                  <doc_id>215</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Since the system in this paper is not constructed using any non-syntactic rules, we do not include the Hanneman et al. (2010) &#8220;not labelable&#8221; maximum-likelihood features or the indicator features related to non-syntactic labels.</text>
                  <doc_id>216</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Beyond the above preprocessing and scoring common to all grammars, we experiment with three different solutions to the more difficult problem of selecting a final translation grammar.</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In any case, we separate phrase pair rules from hierarchical rules</text>
                  <doc_id>218</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Rule Set Filter BLEU TER MET</text>
                  <doc_id>219</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and include in the grammar all phrase pair rules matching a given tuning or testing set.</text>
                  <doc_id>220</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Any improvement in phrase pair coverage during the extraction stage is thus directly passed along to decoding.</text>
                  <doc_id>221</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For hierarchical rules, we experiment with retaining the 10,000 or 100,000 most frequently extracted unique rules.</text>
                  <doc_id>222</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We also separate fully abstract hierarchical rules from partially lexicalized hierarchical rules, and in a further selection technique we retain the 5,000 most frequent abstract and 100,000 most frequent partially lexicalized rules.</text>
                  <doc_id>223</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Given these final rule sets, we tune our MT systems on the NIST MT 2006 data set using the minimum error-rate training package Z-MERT (Zaidan, 2009), and we test on NIST MT 2003.</text>
                  <doc_id>224</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Both sets have four reference translations.</text>
                  <doc_id>225</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Table 4 presents case-insensitive evaluation results on the test set according to the automatic metrics BLEU (Papineni et al., 2002), TER (Snover et al., 2006), and METEOR (Lavie and Denkowski, 2009).</text>
                  <doc_id>226</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>4 The trend in the results is that including a larger grammar is generally better for performance, but filtering techniques also play a substantial role in determining how well a given grammar will perform at run time.</text>
                  <doc_id>227</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We first compare the results in Table 4 for different rule sets all filtered the same way at decoding time.</text>
                  <doc_id>228</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>With only 10,000 hierarchical rules in use (&#8220;10k&#8221;), the improvements in scores indicate that an important contribution is being made by the additional phrase pair coverage provided by each suc-</text>
                  <doc_id>229</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4 For METEOR scoring we use version 1.0 of the metric,</text>
                  <doc_id>230</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>tuned to HTER with the exact, stemming, and synonymy modules enabled.</text>
                  <doc_id>231</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>cessive rule set.</text>
                  <doc_id>232</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The original Stat-XFER rule extraction provides 244,988 phrase pairs that match the MT 2003 test set.</text>
                  <doc_id>233</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This is already increased to 520,995 in the compatible system using multiple decompositions.</text>
                  <doc_id>234</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>With virtual nodes enabled, the full system produces 766,379 matching phrase pairs up to length 5 or 776,707 up to length 7.</text>
                  <doc_id>235</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>These systems both score significantly higher than the Stat-XFER baseline according to BLEU and TER, and the ME- TEOR scores are likely statistically equivalent.</text>
                  <doc_id>236</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Across all configurations, we find that changing the grammar filtering technique &#8212; possibly combined with retuned decoder feature weights &#8212; also has a large influence on automatic metric scores.</text>
                  <doc_id>237</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Larger hierarchical grammars tend to score better, in some cases to the point of erasing the score differences between rule sets.</text>
                  <doc_id>238</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>From this we conclude that making effective use of the extracted grammar, no matter its size, with intelligent filtering techniques is at least as important as the number and type of rules extracted overall.</text>
                  <doc_id>239</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>Though the filtering results in Table 4 are still somewhat inconclusive, the relative success of the &#8220;5k+100k&#8221; setting shows that filtering fully abstract and partially lexicalized rules separately is a reasonable starting approach.</text>
                  <doc_id>240</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>While fully abstract rules do tend to be more frequently observed in grammar extraction, and thus more reliably scored in the translation model, they also have the ability to overapply at decoding time because their use is not restricted to any particular lexical context.</text>
                  <doc_id>241</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Conclusions and Future Work</title>
        <text>We demonstrated in Section 4.1 that the general SCFG extraction algorithm described in this paper is capable of producing very large linguistically motivated rule sets. These rule sets can improve automatic metric scores at decoding time. At the same time, we see the results in Section 4.2 as a springboard to more advanced and more intelligent methods of grammar filtering. Our major research question for future work is to determine how to make the best runtime use of the grammars we can extract. As we saw in Section 2, multiple decompositions of a single parse tree allow the same constituent to be built in a variety of ways. This is generally good for coverage, but its downside at run time is that the decoder must manage a larger number of competing 142 derivations that, in the end, produce the same output string. Grammar filtering that explicitly attempts to limit the derivational ambiguity of the retained rules may prevent the translation model probabilities of correct outputs from getting fragmented into redundant derivations. So far we have only approximated this by using fully abstract rules as a proxy for the most derivationally ambiguous rules. Filtering based on the content of virtual nodes may also be a reasonable strategy for selecting useful grammar rules and discarding those whose contributions are less necessary. For example, we find in our current output many applications of rules involving virtual nodes that consist of an openclass category and a mark of punctuation, such as VBD+COMMA and NN+PU. While there is nothing technically wrong with these rules, they may not be as helpful in translation as rules for nouns and adjectives such as JJ+NNP+NN or NNP+NNP in flat noun phrase structures such as former U.S. president Bill Clinton. A final concern in making use of our large extracted grammars is the effect virtual nodes have on the size of the nonterminal set. The Stat-XFER baseline grammar from our &#8220;xfer-orig&#8221; configuration uses a nonterminal set of 1,577 unique labels. In our rule extractor so far, we have adopted the convention of naming virtual nodes with a concatenation of their component sibling labels, separated by &#8220;+&#8221;s. With the large number of virtual node labels that may be created, this gives our &#8220;full-short&#8221; and &#8220;full-long&#8221; extracted grammars nonterminal sets of around 73,000 unique labels. An undesirable consequence of such a large label set is that a particular SCFG right-hand side may acquire a large variety of left-hand-side labels, further contributing to the derivational ambiguity problems discussed above. In future work, the problem could be addressed by reconsidering our naming scheme for virtual nodes, by allowing fuzzy matching of labels at translation time (Chiang, 2010), or by other techniques aimed at reducing the size of the overall nonterminal set.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We demonstrated in Section 4.1 that the general SCFG extraction algorithm described in this paper is capable of producing very large linguistically motivated rule sets.</text>
              <doc_id>242</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>These rule sets can improve automatic metric scores at decoding time.</text>
              <doc_id>243</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>At the same time, we see the results in Section 4.2 as a springboard to more advanced and more intelligent methods of grammar filtering.</text>
              <doc_id>244</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Our major research question for future work is to determine how to make the best runtime use of the grammars we can extract.</text>
              <doc_id>245</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>As we saw in Section 2, multiple decompositions of a single parse tree allow the same constituent to be built in a variety of ways.</text>
              <doc_id>246</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This is generally good for coverage, but its downside at run time is that the decoder must manage a larger number of competing 142 derivations that, in the end, produce the same output string.</text>
              <doc_id>247</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Grammar filtering that explicitly attempts to limit the derivational ambiguity of the retained rules may prevent the translation model probabilities of correct outputs from getting fragmented into redundant derivations.</text>
              <doc_id>248</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>So far we have only approximated this by using fully abstract rules as a proxy for the most derivationally ambiguous rules.</text>
              <doc_id>249</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Filtering based on the content of virtual nodes may also be a reasonable strategy for selecting useful grammar rules and discarding those whose contributions are less necessary.</text>
              <doc_id>250</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>For example, we find in our current output many applications of rules involving virtual nodes that consist of an openclass category and a mark of punctuation, such as VBD+COMMA and NN+PU.</text>
              <doc_id>251</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>While there is nothing technically wrong with these rules, they may not be as helpful in translation as rules for nouns and adjectives such as JJ+NNP+NN or NNP+NNP in flat noun phrase structures such as former U.S. president Bill Clinton.</text>
              <doc_id>252</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>A final concern in making use of our large extracted grammars is the effect virtual nodes have on the size of the nonterminal set.</text>
              <doc_id>253</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>The Stat-XFER baseline grammar from our &#8220;xfer-orig&#8221; configuration uses a nonterminal set of 1,577 unique labels.</text>
              <doc_id>254</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>In our rule extractor so far, we have adopted the convention of naming virtual nodes with a concatenation of their component sibling labels, separated by &#8220;+&#8221;s.</text>
              <doc_id>255</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>With the large number of virtual node labels that may be created, this gives our &#8220;full-short&#8221; and &#8220;full-long&#8221; extracted grammars nonterminal sets of around 73,000 unique labels.</text>
              <doc_id>256</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>An undesirable consequence of such a large label set is that a particular SCFG right-hand side may acquire a large variety of left-hand-side labels, further contributing to the derivational ambiguity problems discussed above.</text>
              <doc_id>257</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>In future work, the problem could be addressed by reconsidering our naming scheme for virtual nodes, by allowing fuzzy matching of labels at translation time (Chiang, 2010), or by other techniques aimed at reducing the size of the overall nonterminal set.</text>
              <doc_id>258</doc_id>
              <sec_id>16</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>Acknowledgments</title>
        <text>This research was supported in part by U.S. National Science Foundation grants IIS-0713402 and IIS- 0915327 and the DARPA GALE program. We thank Vamshi Ambati and Jon Clark for helpful discussions regarding implementation details of the grammar extraction algorithm. Thanks to Chris Dyer for providing the word-aligned and preprocessed FBIS corpus. Finally, we thank Yahoo! for the use of the M45 research computing cluster, where we ran many steps of our experimental pipeline.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This research was supported in part by U.S. National Science Foundation grants IIS-0713402 and IIS- 0915327 and the DARPA GALE program.</text>
              <doc_id>259</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We thank Vamshi Ambati and Jon Clark for helpful discussions regarding implementation details of the grammar extraction algorithm.</text>
              <doc_id>260</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Thanks to Chris Dyer for providing the word-aligned and preprocessed FBIS corpus.</text>
              <doc_id>261</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we thank Yahoo!</text>
              <doc_id>262</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>for the use of the M45 research computing cluster, where we ran many steps of our experimental pipeline.</text>
              <doc_id>263</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Comparisons between the rule extractor described in this paper and other SCFG rule extraction methods.</caption>
        <reference_text></reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>Hiero</cell>
              <cell>No</cell>
              <cell>&#8212;</cell>
              <cell>&#8212;</cell>
              <cell>Yes</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Stat-XFER</cell>
              <cell>Yes</cell>
              <cell>No</cell>
              <cell>Some</cell>
              <cell>No</cell>
            </row>
            <row>
              <cell>GHKM</cell>
              <cell>Yes</cell>
              <cell>No</cell>
              <cell>No</cell>
              <cell>Yes</cell>
            </row>
            <row>
              <cell>SAMT</cell>
              <cell>No</cell>
              <cell>No</cell>
              <cell>Yes</cell>
              <cell>Yes</cell>
            </row>
            <row>
              <cell>Chiang (2010)</cell>
              <cell>No</cell>
              <cell>No</cell>
              <cell>Yes</cell>
              <cell>Yes</cell>
            </row>
            <row>
              <cell>This work</cell>
              <cell>Yes</cell>
              <cell>Yes</cell>
              <cell>Yes</cell>
              <cell>Yes</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 1: Comparisons between the rule extractor described in this paper and other SCFG rule extraction methods.#@#@Table 2: Rule sets considered by a Stat-XFER baseline (&#8220;xfer-orig&#8221;) and our own rule extractor.</caption>
        <reference_text>In PAGE 5: ...Table 1: Comparisons between the rule extractor described in this paper and other SCFG rule extraction methods. 3 Comparison to Other Methods  Table1  compares the rule extractor described in Sec- tion 2 to other SCFG extraction methods described in the literature. We include comparisons of our work against the Hiero system (Chiang, 2005), the Stat-XFER system rule learner most recently de- scribed by Ambati et al....  In PAGE 5: ... Multiple Alignments. In contrast to all other ex- traction methods in  Table1 , ours allows a node in one parse tree to be aligned with multiple nodes in the other tree, as long as the word-alignment and structure constraints are satisfied. However, we do not allow a node to have multiple simultaneous alignments ? a single node alignment must be cho- sen for extracting an individual rule....</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>None</cell>
              <cell>Tree</cell>
              <cell>Multiple</cell>
              <cell>Virtual</cell>
              <cell>Multiple</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System</cell>
              <cell>Constraints</cell>
              <cell>Alignments</cell>
              <cell>Nodes</cell>
              <cell>Derivations</cell>
            </row>
            <row>
              <cell>Hiero</cell>
              <cell>No</cell>
              <cell>?</cell>
              <cell>?</cell>
              <cell>Yes</cell>
            </row>
            <row>
              <cell>Stat-XFER</cell>
              <cell>Yes</cell>
              <cell>No</cell>
              <cell>Some</cell>
              <cell>No</cell>
            </row>
            <row>
              <cell>GHKM</cell>
              <cell>Yes</cell>
              <cell>No</cell>
              <cell>No</cell>
              <cell>Yes</cell>
            </row>
            <row>
              <cell>SAMT</cell>
              <cell>No</cell>
              <cell>No</cell>
              <cell>Yes</cell>
              <cell>Yes</cell>
            </row>
            <row>
              <cell>Chiang (2010)</cell>
              <cell>No</cell>
              <cell>No</cell>
              <cell>Yes</cell>
              <cell>Yes</cell>
            </row>
            <row>
              <cell>This work</cell>
              <cell>Yes</cell>
              <cell>Yes</cell>
              <cell>Yes</cell>
              <cell>Yes</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TET</source>
        <caption>Table 3: The number of extracted rule instances (tokens) and unique rules (types) produced by the Stat-XFER system (&#8220;xfer-orig&#8221;) and three configurations of our rule extractor.</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>xfer-orig</cell>
              <cell>6,646,791</cell>
              <cell>1,876,384</cell>
              <cell>1,929,641</cell>
              <cell>767,573</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>compatible</cell>
              <cell>8,709,589</cell>
              <cell>6,657,590</cell>
              <cell>2,016,227</cell>
              <cell>3,590,184</cell>
            </row>
            <row>
              <cell>full-short</cell>
              <cell>10,190,487</cell>
              <cell>14,190,066</cell>
              <cell>2,877,650</cell>
              <cell>8,313,690</cell>
            </row>
            <row>
              <cell>full-long</cell>
              <cell>10,288,731</cell>
              <cell>22,479,863</cell>
              <cell>2,970,403</cell>
              <cell>15,750,695</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: Automatic metric results using different rule sets, as well as different grammar filtering methods.</caption>
        <reference_text>In PAGE 8: ... Both sets have four reference translations.  Table4  presents case-insensitive evaluation results on the test set ac- cording to the automatic metrics BLEU (Papineni et al., 2002), TER (Snover et al....  In PAGE 8: ...4 The trend in the results is that including a larger grammar is gener- ally better for performance, but filtering techniques also play a substantial role in determining how well a given grammar will perform at run time. We first compare the results in  Table4  for dif- ferent rule sets all filtered the same way at decod- ing time. With only 10,000 hierarchical rules in use ( 10k ), the improvements in scores indicate that an important contribution is being made by the addi- tional phrase pair coverage provided by each suc- 4For METEOR scoring we use version 1....  In PAGE 8: ... From this we conclude that making effective use of the extracted grammar, no matter its size, with intelligent filtering techniques is at least as important as the number and type of rules extracted overall. Though the filtering results in  Table4  are still somewhat inconclusive, the rel- ative success of the  5k+100k  setting shows that filtering fully abstract and partially lexicalized rules separately is a reasonable starting approach. While fully abstract rules do tend to be more frequently ob- served in grammar extraction, and thus more reliably scored in the translation model, they also have the ability to overapply at decoding time because their use is not restricted to any particular lexical context....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>Rule Set</cell>
              <cell>Filter</cell>
              <cell>BLEU</cell>
              <cell>TER</cell>
              <cell>MET</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>xfer-orig</cell>
              <cell>10k</cell>
              <cell>24.39</cell>
              <cell>68.01</cell>
              <cell>54.35</cell>
            </row>
            <row>
              <cell>xfer-orig</cell>
              <cell>5k+100k</cell>
              <cell>25.95</cell>
              <cell>66.27</cell>
              <cell>54.77</cell>
            </row>
            <row>
              <cell>compatible</cell>
              <cell>10k</cell>
              <cell>24.28</cell>
              <cell>65.30</cell>
              <cell>53.58</cell>
            </row>
            <row>
              <cell>full-short</cell>
              <cell>10k</cell>
              <cell>25.16</cell>
              <cell>66.25</cell>
              <cell>54.33</cell>
            </row>
            <row>
              <cell>full-short</cell>
              <cell>100k</cell>
              <cell>25.51</cell>
              <cell>65.56</cell>
              <cell>54.15</cell>
            </row>
            <row>
              <cell>full-short</cell>
              <cell>5k+100k</cell>
              <cell>26.08</cell>
              <cell>64.32</cell>
              <cell>54.58</cell>
            </row>
            <row>
              <cell>full-long</cell>
              <cell>10k</cell>
              <cell>25.74</cell>
              <cell>65.52</cell>
              <cell>54.55</cell>
            </row>
            <row>
              <cell>full-long</cell>
              <cell>100k</cell>
              <cell>25.53</cell>
              <cell>66.24</cell>
              <cell>53.68</cell>
            </row>
            <row>
              <cell>full-long</cell>
              <cell>5k+100k</cell>
              <cell>25.83</cell>
              <cell>64.55</cell>
              <cell>54.35</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Vamshi Ambati</author>
          <author>Alon Lavie</author>
          <author>Jaime Carbonell</author>
        </authors>
        <title>Extraction of syntactic translation models from parallel data using syntax from source and target languages.</title>
        <publication>In Proceedings of the 12th Machine Translation Summit,</publication>
        <pages>190--197</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>A hierarchical phrase-based model for statistical machine translation.</title>
        <publication>In Proceedings of the 43rd Annual Meeting of the ACL,</publication>
        <pages>263--270</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>Learning to translate with source and target syntax.</title>
        <publication>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>1443--1452</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Michel Galley</author>
          <author>Mark Hopkins</author>
          <author>Kevin Knight</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>What&#8217;s in a translation rule?</title>
        <publication>In HLT-NAACL 2004: Main Proceedings,</publication>
        <pages>273--280</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Michel Galley</author>
          <author>Jonathan Graehl</author>
          <author>Kevin Knight</author>
          <author>Daniel Marcu</author>
          <author>Steve DeNeefe</author>
          <author>Wei Wang</author>
          <author>Ignacio Thayer</author>
        </authors>
        <title>Scalable inference and training of context-rich syntactic translation models.</title>
        <publication>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</publication>
        <pages>961--968</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Greg Hanneman</author>
          <author>Jonathan Clark</author>
          <author>Alon Lavie</author>
        </authors>
        <title>Improved features and grammar selection for syntaxbased MT.</title>
        <publication>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</publication>
        <pages>82--87</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz Josef Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical phrase-based translation.</title>
        <publication>In Proceedings of HLT-NAACL 2003,</publication>
        <pages>48--54</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Alon Lavie</author>
          <author>Michael J Denkowski</author>
        </authors>
        <title>The METEOR metric for automatic evaluation of machine translation.</title>
        <publication>None</publication>
        <pages>23--2</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Alon Lavie</author>
          <author>Alok Parlikar</author>
          <author>Vamshi Ambati</author>
        </authors>
        <title>Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora.</title>
        <publication>In Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation,</publication>
        <pages>87--95</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Zaidan</author>
        </authors>
        <title>Joshua: An open source toolkit for parsing-based machine translation.</title>
        <publication>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</publication>
        <pages>135--139</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A systematic comparison of various statistical alignment models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>BLEU: A method for automatic evalution of machine translation.</title>
        <publication>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>311--318</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Slav Petrov</author>
          <author>Dan Klein</author>
        </authors>
        <title>Improved inference for unlexicalized parsing.</title>
        <publication>In Proceedings of NAACL HLT</publication>
        <pages>404--411</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Matthew Snover</author>
          <author>Bonnie Dorr</author>
          <author>Richard Schwartz</author>
          <author>Linnea Micciulla</author>
          <author>John Makhoul</author>
        </authors>
        <title>A study of translation edit rate with targeted human annotation.</title>
        <publication>In Proceedings of the Seventh Conference of the Association for Machine Translation in the Americas,</publication>
        <pages>223--231</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Omar F Zaidan</author>
        </authors>
        <title>Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems.</title>
        <publication>The Prague Bulletin of Mathematical Linguistics,</publication>
        <pages>91--79</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Ventsislav Zhechev</author>
          <author>Andy Way</author>
        </authors>
        <title>Automatic generation of parallel treebanks.</title>
        <publication>In Proceedings of the 22nd International Conference on Computational Linguistics,</publication>
        <pages>1105--1112</pages>
        <date>2008</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Ambati et al. (2009)</string>
        <sentence_id>45578</sentence_id>
        <char_offset>137</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>0</reference_id>
        <string>Ambati et al. (2009)</string>
        <sentence_id>45623</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>0</reference_id>
        <string>Ambati et al. (2009)</string>
        <sentence_id>45652</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>0</reference_id>
        <string>Ambati et al., 2009</string>
        <sentence_id>45484</sentence_id>
        <char_offset>100</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>1</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>45480</sentence_id>
        <char_offset>136</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>1</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>45578</sentence_id>
        <char_offset>61</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>2</reference_id>
        <string>Chiang, 2010</string>
        <sentence_id>45481</sentence_id>
        <char_offset>167</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>2</reference_id>
        <string>Chiang, 2010</string>
        <sentence_id>45584</sentence_id>
        <char_offset>181</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>2</reference_id>
        <string>Chiang, 2010</string>
        <sentence_id>45733</sentence_id>
        <char_offset>159</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>2</reference_id>
        <string>Chiang (2010)</string>
        <sentence_id>45484</sentence_id>
        <char_offset>166</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>2</reference_id>
        <string>Chiang (2010)</string>
        <sentence_id>45578</sentence_id>
        <char_offset>384</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>3</reference_id>
        <string>Galley et al., 2004</string>
        <sentence_id>45481</sentence_id>
        <char_offset>96</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>4</reference_id>
        <string>Galley et al. (2006)</string>
        <sentence_id>45595</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>4</reference_id>
        <string>Galley et al., 2006</string>
        <sentence_id>45578</sentence_id>
        <char_offset>205</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>5</reference_id>
        <string>Hanneman et al. (2010)</string>
        <sentence_id>45677</sentence_id>
        <char_offset>61</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>5</reference_id>
        <string>Hanneman et al. (2010)</string>
        <sentence_id>45681</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>6</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>45496</sentence_id>
        <char_offset>102</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>6</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>45584</sentence_id>
        <char_offset>161</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>7</reference_id>
        <string>Lavie and Denkowski, 2009</string>
        <sentence_id>45691</sentence_id>
        <char_offset>173</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>8</reference_id>
        <string>Lavie et al., 2008</string>
        <sentence_id>45481</sentence_id>
        <char_offset>147</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>8</reference_id>
        <string>Lavie et al., 2008</string>
        <sentence_id>45484</sentence_id>
        <char_offset>80</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>8</reference_id>
        <string>Lavie et al., 2008</string>
        <sentence_id>45608</sentence_id>
        <char_offset>137</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>8</reference_id>
        <string>Lavie et al., 2008</string>
        <sentence_id>45710</sentence_id>
        <char_offset>137</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>8</reference_id>
        <string>Lavie et al. (2008)</string>
        <sentence_id>45499</sentence_id>
        <char_offset>70</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>9</reference_id>
        <string>Zaidan, 2009</string>
        <sentence_id>45689</sentence_id>
        <char_offset>135</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>10</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>45606</sentence_id>
        <char_offset>140</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>10</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>45708</sentence_id>
        <char_offset>140</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>11</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>45691</sentence_id>
        <char_offset>110</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>12</reference_id>
        <string>Petrov and Klein, 2007</string>
        <sentence_id>45606</sentence_id>
        <char_offset>83</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>12</reference_id>
        <string>Petrov and Klein, 2007</string>
        <sentence_id>45708</sentence_id>
        <char_offset>83</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>13</reference_id>
        <string>Snover et al., 2006</string>
        <sentence_id>45691</sentence_id>
        <char_offset>139</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>14</reference_id>
        <string>Zaidan, 2009</string>
        <sentence_id>45689</sentence_id>
        <char_offset>135</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>15</reference_id>
        <string>Zhechev and Way, 2008</string>
        <sentence_id>45481</sentence_id>
        <char_offset>289</char_offset>
      </citation>
    </citations>
  </content>
</document>
