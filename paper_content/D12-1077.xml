<document>
  <filename>D12-1077</filename>
  <authors/>
  <title>None</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>1 Introduction</title>
        <text>Finding the appropriate word ordering in the target language is one of the most difficult problems for statistical machine translation (SMT), particularly for language pairs with widely divergent syntax. As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004).
In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in
The first author is now affiliated with the Nara Institute of Science and Technology.
decoding time. However, these require a good syntactic parser, which is not available for many languages. In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse structure for use in translation as a part of a pre-ordering based translation system.
In this work, we present a method for inducing a parser for SMT by training a discriminative model to maximize reordering accuracy while treating the parse tree as a latent variable. As a learning framework, we use online large-margin methods to train the model to directly minimize two measures of reordering accuracy. We propose a variety of features, and demonstrate that learning can succeed when no linguistic information (POS tags or parse structure) is available in the source language, but also show that this linguistic information can be simply incorporated when it is available. Experiments find that the proposed model improves both reordering and translation accuracy, leading to average gains of 1.2 BLEU points on English-Japanese and Japanese-English translation without linguistic analysis tools, or up to 1.5 BLEU points when these tools are incorporated. In addition, we show that our model is able to effectively maximize various measures of reordering accuracy, and that the reordering measure that we choose has a direct effect on translation results.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Finding the appropriate word ordering in the target language is one of the most difficult problems for statistical machine translation (SMT), particularly for language pairs with widely divergent syntax.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al., 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or preordering (Xia and McCord, 2004).</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In particular, systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in</text>
              <doc_id>2</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The first author is now affiliated with the Nara Institute of Science and Technology.</text>
              <doc_id>3</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>decoding time.</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, these require a good syntactic parser, which is not available for many languages.</text>
              <doc_id>5</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In recent work, DeNero and Uszkoreit (2011) suggest that unsupervised grammar induction can be used to create source-sentence parse structure for use in translation as a part of a pre-ordering based translation system.</text>
              <doc_id>6</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this work, we present a method for inducing a parser for SMT by training a discriminative model to maximize reordering accuracy while treating the parse tree as a latent variable.</text>
              <doc_id>7</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As a learning framework, we use online large-margin methods to train the model to directly minimize two measures of reordering accuracy.</text>
              <doc_id>8</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We propose a variety of features, and demonstrate that learning can succeed when no linguistic information (POS tags or parse structure) is available in the source language, but also show that this linguistic information can be simply incorporated when it is available.</text>
              <doc_id>9</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Experiments find that the proposed model improves both reordering and translation accuracy, leading to average gains of 1.2 BLEU points on English-Japanese and Japanese-English translation without linguistic analysis tools, or up to 1.5 BLEU points when these tools are incorporated.</text>
              <doc_id>10</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In addition, we show that our model is able to effectively maximize various measures of reordering accuracy, and that the reordering measure that we choose has a direct effect on translation results.</text>
              <doc_id>11</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>2 Preordering for SMT</title>
        <text>Machine translation is defined as transformation of source sentence F = f 1 . . . f J to target sentence E = e 1 . . . e I . In this paper, we take
the pre-ordering approach to machine translation (Xia and McCord, 2004), which performs translation as a two step process of reordering and translation (Figure 1). Reordering first deterministically transforms F into F &#8242; , which contains the same words as F but is in the order of E. Translation then transforms F &#8242; into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required.
This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language. Reordering rules are defined over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima&#8217;an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b). However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper. In particular, two methods deserve mention for being similar to our approach. First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, training a monolingual parser to reproduce the induced trees, and training a reordering model that selects a reordering based on this parse structure. In contrast, our method trains the model in a single step, treating the parse structure as a latent variable in a discriminative reordering model. In addition Tromble and Eisner (2009) and Visweswariah et al. (2011) present models that use binary classification to decide whether each pair of words should be placed in forward or reverse order. In contrast, our method uses traditional contextfree-grammar models, which allows for simple parsing and flexible parameterization, including features such as those that utilize the existence of a span in the phrase table. Our work is also unique in that we show that it is possible to directly optimize several measures of reordering accuracy, which proves important for achieving good translations. 1
3 Training a Reordering Model with Latent Derivations
In this section, we provide a basic overview of the proposed method for learning a reordering model with latent derivations using online discriminative learning.
3.1 Space of Reorderings
The model we present here is based on the bracketing transduction grammar (BTG, Wu (1997)) framework. BTGs represent a binary tree derivation D over the source sentence F as shown in Figure 1. Each non-terminal node can either be a straight (str) or inverted (inv) production, and terminals (term) span a nonempty substring f. 2
The ordering of the sentence is determined by the tree structure and the non-terminal labels str and inv, and can be built bottom-up. Each subtree represents a source substring f and its reordered counterpart f &#8242; . For each terminal node, no reordering occurs and f is equal to f &#8242; .
1 The semi-supervised method of Katz-Brown et al.
(2011) also optimizes reordering accuracy, but requires manually annotated parses as seed data. 2 In the original BTG framework used in translation,
terminals produce a bilingual substring pair f/e, but as we are only interested in reordering the source F , we simplify the model by removing the target substring e.
For each non-terminal node spanning f with its left child spanning f 1 and its right child spanning f 2 , if the non-terminal symbol is str, the reordered strings will be concatenated in order as f &#8242; = f &#8242; 1f &#8242; 2, and if the non-terminal symbol is inv, the reordered strings will be concatenated in inverted order as f &#8242; = f &#8242; 2f &#8242; 1.
We define the space of all reorderings that can be produced by the BTG as F &#8242; , and attempt to find the best reordering &#710;F &#8242; within this space. 3
3.2 Reorderings with Latent Derivations
In order to find the best reordering &#710;F &#8242; given only the information in the source side sentence F , we define a scoring function S(F &#8242; |F ), and choose the ordering of maximal score:
&#729; F &#8242; = arg max
F &#8242; S(F &#8242; |F ).
As our model is based on reorderings licensed by BTG derivations, we also assume that there is an underlying derivation D that produced F &#8242; . As we can uniquely determine F &#8242; given F and D, we can define a scoring function S(D|F ) over derivations, find the derivation of maximal score
&#7690; = arg max S(D|F )
D
and use &#7690; to transform F into F &#8242; .
Furthermore, we assume that the score S(D|F ) is the weighted sum of a number of feature functions defined over D and F
S(D|F, w) = &#8721; i w i &#966; i (D, F )
where &#966; i is the ith feature function, and w i is its corresponding weight in weight vector w.
Given this model, we must next consider how to learn the weights w. As the final goal of our model is to produce good reorderings F &#8242; , it is natural to attempt to learn weights that will allow us to produce these high-quality reorderings.
3 BTGs cannot reproduce all possible reorderings, but
can handle most reorderings occurring in natural translated text (Haghighi et al., 2009).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Machine translation is defined as transformation of source sentence F = f 1 .</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>13</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>14</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>f J to target sentence E = e 1 .</text>
              <doc_id>15</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>16</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>17</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>e I .</text>
              <doc_id>18</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, we take</text>
              <doc_id>19</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the pre-ordering approach to machine translation (Xia and McCord, 2004), which performs translation as a two step process of reordering and translation (Figure 1).</text>
              <doc_id>20</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Reordering first deterministically transforms F into F &#8242; , which contains the same words as F but is in the order of E. Translation then transforms F &#8242; into E using a method such as phrase-based SMT (Koehn et al., 2003), which can produce accurate translations when only local reordering is required.</text>
              <doc_id>21</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>This general framework has been widely studied, with the majority of works relying on a syntactic parser being available in the source language.</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Reordering rules are defined over this parse either through machine learning techniques (Xia and McCord, 2004; Zhang et al., 2007; Li et al., 2007; Genzel, 2010; Dyer and Resnik, 2010; Khalilov and Sima&#8217;an, 2011) or linguistically motivated manual rules (Collins et al., 2005; Xu et al., 2009; Carpuat et al., 2010; Isozaki et al., 2010b).</text>
              <doc_id>23</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, as building a parser for each source language is a resourceintensive undertaking, there has also been some interest in developing reordering rules without the use of a parser (Rottmann and Vogel, 2007; Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011), and we will follow this thread of research in this paper.</text>
              <doc_id>24</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In particular, two methods deserve mention for being similar to our approach.</text>
              <doc_id>25</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>First, DeNero and Uszkoreit (2011) learn a reordering model through a three-step process of bilingual grammar induction, training a monolingual parser to reproduce the induced trees, and training a reordering model that selects a reordering based on this parse structure.</text>
              <doc_id>26</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In contrast, our method trains the model in a single step, treating the parse structure as a latent variable in a discriminative reordering model.</text>
              <doc_id>27</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In addition Tromble and Eisner (2009) and Visweswariah et al. (2011) present models that use binary classification to decide whether each pair of words should be placed in forward or reverse order.</text>
              <doc_id>28</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In contrast, our method uses traditional contextfree-grammar models, which allows for simple parsing and flexible parameterization, including features such as those that utilize the existence of a span in the phrase table.</text>
              <doc_id>29</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Our work is also unique in that we show that it is possible to directly optimize several measures of reordering accuracy, which proves important for achieving good translations.</text>
              <doc_id>30</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>1</text>
              <doc_id>31</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3 Training a Reordering Model with Latent Derivations</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this section, we provide a basic overview of the proposed method for learning a reordering model with latent derivations using online discriminative learning.</text>
              <doc_id>33</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3.1 Space of Reorderings</text>
              <doc_id>34</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The model we present here is based on the bracketing transduction grammar (BTG, Wu (1997)) framework.</text>
              <doc_id>35</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>BTGs represent a binary tree derivation D over the source sentence F as shown in Figure 1.</text>
              <doc_id>36</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Each non-terminal node can either be a straight (str) or inverted (inv) production, and terminals (term) span a nonempty substring f. 2</text>
              <doc_id>37</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The ordering of the sentence is determined by the tree structure and the non-terminal labels str and inv, and can be built bottom-up.</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each subtree represents a source substring f and its reordered counterpart f &#8242; .</text>
              <doc_id>39</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For each terminal node, no reordering occurs and f is equal to f &#8242; .</text>
              <doc_id>40</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 The semi-supervised method of Katz-Brown et al.</text>
              <doc_id>41</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(2011) also optimizes reordering accuracy, but requires manually annotated parses as seed data.</text>
              <doc_id>42</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2 In the original BTG framework used in translation,</text>
              <doc_id>43</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>terminals produce a bilingual substring pair f/e, but as we are only interested in reordering the source F , we simplify the model by removing the target substring e.</text>
              <doc_id>44</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For each non-terminal node spanning f with its left child spanning f 1 and its right child spanning f 2 , if the non-terminal symbol is str, the reordered strings will be concatenated in order as f &#8242; = f &#8242; 1f &#8242; 2, and if the non-terminal symbol is inv, the reordered strings will be concatenated in inverted order as f &#8242; = f &#8242; 2f &#8242; 1.</text>
              <doc_id>45</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We define the space of all reorderings that can be produced by the BTG as F &#8242; , and attempt to find the best reordering &#710;F &#8242; within this space.</text>
              <doc_id>46</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>3</text>
              <doc_id>47</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3.2 Reorderings with Latent Derivations</text>
              <doc_id>48</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In order to find the best reordering &#710;F &#8242; given only the information in the source side sentence F , we define a scoring function S(F &#8242; |F ), and choose the ordering of maximal score:</text>
              <doc_id>49</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#729; F &#8242; = arg max</text>
              <doc_id>50</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>F &#8242; S(F &#8242; |F ).</text>
              <doc_id>51</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>As our model is based on reorderings licensed by BTG derivations, we also assume that there is an underlying derivation D that produced F &#8242; .</text>
              <doc_id>52</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As we can uniquely determine F &#8242; given F and D, we can define a scoring function S(D|F ) over derivations, find the derivation of maximal score</text>
              <doc_id>53</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#7690; = arg max S(D|F )</text>
              <doc_id>54</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>D</text>
              <doc_id>55</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and use &#7690; to transform F into F &#8242; .</text>
              <doc_id>56</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Furthermore, we assume that the score S(D|F ) is the weighted sum of a number of feature functions defined over D and F</text>
              <doc_id>57</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S(D|F, w) = &#8721; i w i &#966; i (D, F )</text>
              <doc_id>58</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where &#966; i is the ith feature function, and w i is its corresponding weight in weight vector w.</text>
              <doc_id>59</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Given this model, we must next consider how to learn the weights w.</text>
              <doc_id>60</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As the final goal of our model is to produce good reorderings F &#8242; , it is natural to attempt to learn weights that will allow us to produce these high-quality reorderings.</text>
              <doc_id>61</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3 BTGs cannot reproduce all possible reorderings, but</text>
              <doc_id>62</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>can handle most reorderings occurring in natural translated text (Haghighi et al., 2009).</text>
              <doc_id>63</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>4 Evaluating Reorderings</title>
        <text>Before we explain the learning algorithm, we must know how to distinguish whether the F &#8242; produced by the model is good or bad. This section explains how to calculate oracle reorderings, and assign each F &#8242; a loss and an accuracy according to how well it reproduces the oracle.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Before we explain the learning algorithm, we must know how to distinguish whether the F &#8242; produced by the model is good or bad.</text>
              <doc_id>64</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This section explains how to calculate oracle reorderings, and assign each F &#8242; a loss and an accuracy according to how well it reproduces the oracle.</text>
              <doc_id>65</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Calculating Oracle Orderings</title>
            <text>In order to calculate reordering quality, we first define a ranking function r(f j |F, A), which indicates the relative position of source word f j in the proper target order (Figure 2 (a)). In order to calculate this ranking function, we define A = a 1 , . . . , a J , where each a j is a set of the indices of the words in E to which f j is aligned. 4 Given these alignments, we define an ordering function a j1 &lt; a j2 that indicates that the indices in a j1 come before the indices in a j2 . Formally, we define this function as &#8220;the first index in a j1 is at most the first index in a j2 , similarly for the last index, and either the first or last index in a j1 is less than that of a j2 .&#8221; Given this ordering, we can sort every alignment a j , and use its relative position in the sentence to assign a rank to its word r(f j ). In
4 Null alignments require special treatment. To do so,
we can place unaligned brackets and quotes directly before and after the spans they surround, and attach all other unaligned words to the word directly to the right for head-initial languages (e.g. English), or left for headfinal languages (e.g. Japanese).
the case of ties, where neither a j1 &lt; a j2 nor a j2 &lt; a j1 , both f j1 and f j2 are assigned the same rank. We can now define measures of reordering accuracy for F &#8242; by how well it arranges the words in order of ascending rank. It should be noted that as we allow ties in rank, there are multiple possible F &#8242; where all words are in strictly ascending order, which we will call oracle orderings.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In order to calculate reordering quality, we first define a ranking function r(f j |F, A), which indicates the relative position of source word f j in the proper target order (Figure 2 (a)).</text>
                  <doc_id>66</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In order to calculate this ranking function, we define A = a 1 , .</text>
                  <doc_id>67</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>68</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>69</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>, a J , where each a j is a set of the indices of the words in E to which f j is aligned.</text>
                  <doc_id>70</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>4 Given these alignments, we define an ordering function a j1 &lt; a j2 that indicates that the indices in a j1 come before the indices in a j2 .</text>
                  <doc_id>71</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Formally, we define this function as &#8220;the first index in a j1 is at most the first index in a j2 , similarly for the last index, and either the first or last index in a j1 is less than that of a j2 .&#8221; Given this ordering, we can sort every alignment a j , and use its relative position in the sentence to assign a rank to its word r(f j ).</text>
                  <doc_id>72</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>In</text>
                  <doc_id>73</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4 Null alignments require special treatment.</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To do so,</text>
                  <doc_id>75</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>we can place unaligned brackets and quotes directly before and after the spans they surround, and attach all other unaligned words to the word directly to the right for head-initial languages (e.g. English), or left for headfinal languages (e.g. Japanese).</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the case of ties, where neither a j1 &lt; a j2 nor a j2 &lt; a j1 , both f j1 and f j2 are assigned the same rank.</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We can now define measures of reordering accuracy for F &#8242; by how well it arranges the words in order of ascending rank.</text>
                  <doc_id>78</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It should be noted that as we allow ties in rank, there are multiple possible F &#8242; where all words are in strictly ascending order, which we will call oracle orderings.</text>
                  <doc_id>79</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Kendall&#8217;s &#964;</title>
            <text>The first measure of reordering accuracy that we will consider is Kendall&#8217;s &#964; (Kendall, 1938), a measure of pairwise rank correlation which has been proposed for evaluating translation reordering accuracy (Isozaki et al., 2010a; Birch et al., 2010) and pre-ordering accuracy (Talbot et al., 2011). The fundamental idea behind the measure lies in comparisons between each pair of elements f j &#8242; 1 and f j &#8242; 2 of the reordered sentence, where j 1 &lt; j 2 . Because j 1 &lt; j 2 , f j &#8242; 1 comes before f j &#8242; 2
in the reordered sentence, the ranks should be r(f j &#8242; 1 ) &#8804; r(f j &#8242; 2 ) in order to produce the correct ordering.
Based on this criterion, we first define a loss L t (F &#8242; ) that will be higher for orderings that are further from the oracle. Specifically, we take the sum of all pairwise orderings that do not follow the expected order
L t (F &#8242; ) =
J&#8722;1
&#8721; J&#8721;
j 1 =1 j 2 =j 1 +1
&#948;(r(f &#8242; j 1 ) &gt; r(f &#8242; j 2 ))
where &#948;(&#183;) is an indicator function that is 1 when its condition is true, and 0 otherwise. An example of this is given in Figure 2 (b).
To calculate an accuracy measure for ordering F &#8242; , we first calculate the maximum loss for the sentence, which is equal to the total number of non-equal rank comparisons in the sentence 5
J&#8722;1
&#8721; max L t (F &#8242; ) =
F &#8242;
J&#8721;
j 1 =1 j 2 =j 1 +1
&#948;(r(f &#8242; j 1 ) &#8800; r(f &#8242; j 2 )).
(1)
5 The traditional formulation of Kendall&#8217;s &#964; assumes
no ties in rank, and thus the maximum loss can be calculated as J(J &#8722; 1)/2.
Finally, we use this maximum loss to normalize the actual loss to get an accuracy
A t (F &#8242; ) = 1 &#8722; L t(F &#8242; ) max L t ( &#732;F &#8242; ) ,
&#732;F &#8242;
which will take a value between 0 (when F &#8242; has maximal loss), and 1 (when F &#8242; matches one of the oracle orderings). In Figure 2 (b), L t (F &#8242; ) = 2 and max
&#732;F &#8242; L t ( &#732;F &#8242; ) = 8, so A t (F &#8242; ) = 0.75.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The first measure of reordering accuracy that we will consider is Kendall&#8217;s &#964; (Kendall, 1938), a measure of pairwise rank correlation which has been proposed for evaluating translation reordering accuracy (Isozaki et al., 2010a; Birch et al., 2010) and pre-ordering accuracy (Talbot et al., 2011).</text>
                  <doc_id>80</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The fundamental idea behind the measure lies in comparisons between each pair of elements f j &#8242; 1 and f j &#8242; 2 of the reordered sentence, where j 1 &lt; j 2 .</text>
                  <doc_id>81</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Because j 1 &lt; j 2 , f j &#8242; 1 comes before f j &#8242; 2</text>
                  <doc_id>82</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>in the reordered sentence, the ranks should be r(f j &#8242; 1 ) &#8804; r(f j &#8242; 2 ) in order to produce the correct ordering.</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Based on this criterion, we first define a loss L t (F &#8242; ) that will be higher for orderings that are further from the oracle.</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Specifically, we take the sum of all pairwise orderings that do not follow the expected order</text>
                  <doc_id>85</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L t (F &#8242; ) =</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>J&#8722;1</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; J&#8721;</text>
                  <doc_id>88</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j 1 =1 j 2 =j 1 +1</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#948;(r(f &#8242; j 1 ) &gt; r(f &#8242; j 2 ))</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where &#948;(&#183;) is an indicator function that is 1 when its condition is true, and 0 otherwise.</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>An example of this is given in Figure 2 (b).</text>
                  <doc_id>92</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To calculate an accuracy measure for ordering F &#8242; , we first calculate the maximum loss for the sentence, which is equal to the total number of non-equal rank comparisons in the sentence 5</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>J&#8722;1</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; max L t (F &#8242; ) =</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>F &#8242;</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>J&#8721;</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j 1 =1 j 2 =j 1 +1</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#948;(r(f &#8242; j 1 ) &#8800; r(f &#8242; j 2 )).</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(1)</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5 The traditional formulation of Kendall&#8217;s &#964; assumes</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>no ties in rank, and thus the maximum loss can be calculated as J(J &#8722; 1)/2.</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Finally, we use this maximum loss to normalize the actual loss to get an accuracy</text>
                  <doc_id>103</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A t (F &#8242; ) = 1 &#8722; L t(F &#8242; ) max L t ( &#732;F &#8242; ) ,</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#732;F &#8242;</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>which will take a value between 0 (when F &#8242; has maximal loss), and 1 (when F &#8242; matches one of the oracle orderings).</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In Figure 2 (b), L t (F &#8242; ) = 2 and max</text>
                  <doc_id>107</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#732;F &#8242; L t ( &#732;F &#8242; ) = 8, so A t (F &#8242; ) = 0.75.</text>
                  <doc_id>108</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>4.3 Chunk Fragmentation</title>
            <text>Another measure that has been used in evaluation of translation accuracy (Banerjee and Lavie, 2005) and pre-ordering accuracy (Talbot et al., 2011) is chunk fragmentation. This measure is based on the number of chunks that the sentence needs to be broken into to reproduce the correct ordering, with a motivation that the number of continuous chunks is equal to the number of times the reader will have to jump to a different position in the reordered sentence to read it in the target order. One way to measure the number of continuous chunks is considering whether each word pair f j &#8242; and f j+1 &#8242; is discontinuous (the rank of f j+1 &#8242; is not equal to or one greater than f j &#8242;)
discont(f &#8242; j, f &#8242; j+1) =
&#948;(r(f &#8242; j) &#8800; r(f &#8242; j+1) &#8743; r(f &#8242; j) + 1 &#8800; r(f &#8242; j+1))
and sum over all word pairs in the sentence to create a sentence-based loss
J&#8722;1
&#8721; L c (F &#8242; ) = discont(f j, &#8242; f j+1) &#8242; (2)
j=1
While this is the formulation taken by previous work, we found that this under-penalizes bad reorderings of the first and last words of the sentence, which can contribute to the loss only once, as opposed to other words which can contribute to the loss twice. To account for this, when calculating the chunk fragmentation score, we additionally add two sentence boundary words f 0 and f J+1 with ranks r(f 0 ) = 0 and r(f J+1 ) = 1 + max
f j &#8242; &#8712;F &#8242;r(f j &#8242; ) and redefine the summation in Equation (2) to consider these words (e.g. Figure 2 (c)).
procedure WeightUpdate(F , A, w) D &#8592; parse(F, w) &#8882; Create parse forest &#7690; &#8592; argmax S(D|F, w) + L(D|F, A)
D&#8712;D
&#8882; Find the model parse &#710;D &#8592; argmin L(D|F, A) &#8722; &#945;S(D|F, w)
D&#8712;D
&#8882; Find the oracle parse if L( &#710;D|F, A) &#8800; L(&#7690;|F, A) then
w &#8592; &#946;(w + &#947;(&#966;( &#710;D, F ) &#8722; &#966;(&#7690;, F ))) &#8882; Perform weight update
end if end procedure
Similarly to Kendall&#8217;s &#964;, we can also define an accuracy measure between 0 and 1 using the maximum loss, which will be at most J + 1, which corresponds to the total number of comparisons made in calculating the loss 6
A c (F &#8242; ) = 1 &#8722; L c(F &#8242; ) J + 1 .
In Figure 2 (c), L c (F &#8242; ) = 3 and J + 1 = 6, so A c (F &#8242; ) = 0.5.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Another measure that has been used in evaluation of translation accuracy (Banerjee and Lavie, 2005) and pre-ordering accuracy (Talbot et al., 2011) is chunk fragmentation.</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This measure is based on the number of chunks that the sentence needs to be broken into to reproduce the correct ordering, with a motivation that the number of continuous chunks is equal to the number of times the reader will have to jump to a different position in the reordered sentence to read it in the target order.</text>
                  <doc_id>110</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>One way to measure the number of continuous chunks is considering whether each word pair f j &#8242; and f j+1 &#8242; is discontinuous (the rank of f j+1 &#8242; is not equal to or one greater than f j &#8242;)</text>
                  <doc_id>111</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>discont(f &#8242; j, f &#8242; j+1) =</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#948;(r(f &#8242; j) &#8800; r(f &#8242; j+1) &#8743; r(f &#8242; j) + 1 &#8800; r(f &#8242; j+1))</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and sum over all word pairs in the sentence to create a sentence-based loss</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>J&#8722;1</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; L c (F &#8242; ) = discont(f j, &#8242; f j+1) &#8242; (2)</text>
                  <doc_id>116</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=1</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>While this is the formulation taken by previous work, we found that this under-penalizes bad reorderings of the first and last words of the sentence, which can contribute to the loss only once, as opposed to other words which can contribute to the loss twice.</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To account for this, when calculating the chunk fragmentation score, we additionally add two sentence boundary words f 0 and f J+1 with ranks r(f 0 ) = 0 and r(f J+1 ) = 1 + max</text>
                  <doc_id>119</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f j &#8242; &#8712;F &#8242;r(f j &#8242; ) and redefine the summation in Equation (2) to consider these words (e.g.</text>
                  <doc_id>120</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 2 (c)).</text>
                  <doc_id>121</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>procedure WeightUpdate(F , A, w) D &#8592; parse(F, w) &#8882; Create parse forest &#7690; &#8592; argmax S(D|F, w) + L(D|F, A)</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>D&#8712;D</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8882; Find the model parse &#710;D &#8592; argmin L(D|F, A) &#8722; &#945;S(D|F, w)</text>
                  <doc_id>124</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>D&#8712;D</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8882; Find the oracle parse if L( &#710;D|F, A) &#8800; L(&#7690;|F, A) then</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w &#8592; &#946;(w + &#947;(&#966;( &#710;D, F ) &#8722; &#966;(&#7690;, F ))) &#8882; Perform weight update</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>end if end procedure</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Similarly to Kendall&#8217;s &#964;, we can also define an accuracy measure between 0 and 1 using the maximum loss, which will be at most J + 1, which corresponds to the total number of comparisons made in calculating the loss 6</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A c (F &#8242; ) = 1 &#8722; L c(F &#8242; ) J + 1 .</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In Figure 2 (c), L c (F &#8242; ) = 3 and J + 1 = 6, so A c (F &#8242; ) = 0.5.</text>
                  <doc_id>131</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>5 Learning a BTG Parser for Reordering</title>
        <text>Now that we have a definition of loss over reorderings produced by the model, we have a clear learning objective: we would like to find reorderings F &#8242; with low loss. The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al., 2006), and extended to use large-margin training in an online framework (Watanabe et al., 2007).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Now that we have a definition of loss over reorderings produced by the model, we have a clear learning objective: we would like to find reorderings F &#8242; with low loss.</text>
              <doc_id>132</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al., 2006), and extended to use large-margin training in an online framework (Watanabe et al., 2007).</text>
              <doc_id>133</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Learning Algorithm</title>
            <text>Learning uses the general framework of largemargin online structured prediction (Crammer et al., 2006), which makes several passes through the data, finding a derivation with high model score (the model parse) and a derivation with
6 It should be noted that for sentences of length one or
sentences with tied ranks, the maximum loss may be less than J + 1, but for simplicity we use this approximation.
minimal loss (the oracle parse), and updating w if these two parses diverge (Figure 3).
In order to create both of these parses efficiently, we first create a parse forest encoding a large number of derivations D i according to the model scores. Next, we find the model parse &#7690;i, which is the parse in the forest D i that maximizes the sum of the model score and the loss S(D k |F k , w)+L(D k |F k , A k ). It should be noted that here we are considering not only the model score, but also the derivation&#8217;s loss. This is necessary for loss-driven large-margin training (Crammer et al., 2006), and follows the basic intuition that during training, we would like to make it easier to select negative examples with large loss, causing these examples to be penalized more often and more heavily.
We also find an oracle parse &#710;D i , which is selected solely to minimize the loss L(D k |F k , A k ). One important difference between the model we describe here and traditional parsing models is that the target derivation &#710;D k is a latent variable. Because many D k achieve a particular reordering F &#8242; , many reorderings F &#8242; are able to minimize the loss L(F
k &#8242; |F k, A k ). Thus it is necessary
to choose a single oracle derivation to treat as the target out of many equally good reorderings. DeNero and Uszkoreit (2011) resolve this ambiguity with four features with empirically tuned scores before training a monolingual parser and reordering model. In contrast, we follow previous work on discriminative learning with latent variables (Yu and Joachims, 2009), and break ties within the pool of oracle derivations by selecting the derivation with the largest model score. From an implementation point of view, this can be done by finding the derivation that minimizes L(D k |F k , A k ) &#8722; &#945;S(D k |F k , w), where &#945; is a constant small enough to ensure that the effect of the loss will always be greater than the effect of the score.
Finally, if the model parse &#7690;k has a loss that is greater than that of the oracle parse &#710;D k , we update the weights to increase the score of the oracle parse and decrease the score of the model parse. Any criterion for weight updates may be used, such as the averaged perceptron (Collins, 2002) and MIRA (Crammer et al., 2006), but
we opted to use Pegasos (Shalev-Shwartz et al., 2007) as it allows for the introduction of regularization and relatively stable learning. To perform this full process, given a source sentence F k , alignment A k , and model weights w we need to be able to efficiently calculate scores, calculate losses, and create parse forests for derivations D k , the details of which will be explained in the following sections.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Learning uses the general framework of largemargin online structured prediction (Crammer et al., 2006), which makes several passes through the data, finding a derivation with high model score (the model parse) and a derivation with</text>
                  <doc_id>134</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>6 It should be noted that for sentences of length one or</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>sentences with tied ranks, the maximum loss may be less than J + 1, but for simplicity we use this approximation.</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>minimal loss (the oracle parse), and updating w if these two parses diverge (Figure 3).</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In order to create both of these parses efficiently, we first create a parse forest encoding a large number of derivations D i according to the model scores.</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Next, we find the model parse &#7690;i, which is the parse in the forest D i that maximizes the sum of the model score and the loss S(D k |F k , w)+L(D k |F k , A k ).</text>
                  <doc_id>139</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It should be noted that here we are considering not only the model score, but also the derivation&#8217;s loss.</text>
                  <doc_id>140</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This is necessary for loss-driven large-margin training (Crammer et al., 2006), and follows the basic intuition that during training, we would like to make it easier to select negative examples with large loss, causing these examples to be penalized more often and more heavily.</text>
                  <doc_id>141</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We also find an oracle parse &#710;D i , which is selected solely to minimize the loss L(D k |F k , A k ).</text>
                  <doc_id>142</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>One important difference between the model we describe here and traditional parsing models is that the target derivation &#710;D k is a latent variable.</text>
                  <doc_id>143</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Because many D k achieve a particular reordering F &#8242; , many reorderings F &#8242; are able to minimize the loss L(F</text>
                  <doc_id>144</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k &#8242; |F k, A k ).</text>
                  <doc_id>145</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Thus it is necessary</text>
                  <doc_id>146</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>to choose a single oracle derivation to treat as the target out of many equally good reorderings.</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>DeNero and Uszkoreit (2011) resolve this ambiguity with four features with empirically tuned scores before training a monolingual parser and reordering model.</text>
                  <doc_id>148</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast, we follow previous work on discriminative learning with latent variables (Yu and Joachims, 2009), and break ties within the pool of oracle derivations by selecting the derivation with the largest model score.</text>
                  <doc_id>149</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>From an implementation point of view, this can be done by finding the derivation that minimizes L(D k |F k , A k ) &#8722; &#945;S(D k |F k , w), where &#945; is a constant small enough to ensure that the effect of the loss will always be greater than the effect of the score.</text>
                  <doc_id>150</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Finally, if the model parse &#7690;k has a loss that is greater than that of the oracle parse &#710;D k , we update the weights to increase the score of the oracle parse and decrease the score of the model parse.</text>
                  <doc_id>151</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Any criterion for weight updates may be used, such as the averaged perceptron (Collins, 2002) and MIRA (Crammer et al., 2006), but</text>
                  <doc_id>152</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>we opted to use Pegasos (Shalev-Shwartz et al., 2007) as it allows for the introduction of regularization and relatively stable learning.</text>
                  <doc_id>153</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To perform this full process, given a source sentence F k , alignment A k , and model weights w we need to be able to efficiently calculate scores, calculate losses, and create parse forests for derivations D k , the details of which will be explained in the following sections.</text>
                  <doc_id>154</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Scoring Derivation Trees</title>
            <text>First, we must consider how to efficiently assign scores S(D|F, w) to a derivation or forest during parsing. The most standard and efficient way to do so is to create local features that can be calculated based only on the information included in a single node d in the derivation tree. The score of the whole tree can then be expressed as the sum of the scores from each node:
S(D|F, w) = &#8721; d&#8712;D S(d|F, w)
= &#8721; d&#8712;D &#8721;
w i &#966; i (d, F ).
Based on this restriction, we define a number of features that can be used to score the parse tree. To ease explanation, we represent each node in the derivation as d = &#12296;s, l, c, c + 1, r&#12297;, where s is the node&#8217;s symbol (str, inv, or term), while l and r are the leftmost and rightmost indices of the span that d covers. c and c + 1 are the rightmost index of the left child and leftmost index of the right child for non-terminal nodes. All features are intersected with the node label s, so each feature described below corresponds to three different features (or two for features applicable to only non-terminal nodes).
&#8226; &#966; lex : Identities of words in positions f l , f r , f c , f c+1 , f l&#8722;1 , f r+1 , f l f r , and f c f c+1 .
&#8226; &#966; class : Same as &#966; lex , but with words abstracted to classes. We use the 50 classes automatically generated by Och (1999)&#8217;s method that are calculated during alignment in standard SMT systems.
&#8226; &#966; balance : For non-terminals, features indicating whether the length of the left span
i
(c&#8722;l +1) is lesser than, equal to, or greater than the length of the right span (r &#8722; c).
&#8226; &#966; table : Features, bucketed by length, that indicate whether &#8220;f l . . . f r &#8221; appears as a contiguous phrase in the SMT training data, as well as the log frequency of the number of times the phrase appears total and the number of times it appears as a contiguous phrase (DeNero and Uszkoreit, 2011). Phrase length is limited to 8, and phrases of frequency one are removed.
&#8226; &#966; pos : Same as &#966; lex , but with words abstracted to language-dependent POS tags.
&#8226; &#966; cfg : Features indicating the label of the spans f l . . . f r , f l . . . f c , and f c+1 . . . f r in a supervised parse tree, and the intersection of the three labels. When spans do not correspond to a span in the supervised parse tree, we indicate &#8220;no span&#8221; with the label &#8220;X&#8221; (Zollmann and Venugopal, 2006).
Most of these features can be calculated from only a parallel corpus, but &#966; pos requires a POS tagger and &#966; cfg requires a full syntactic parser in the source language. As it is preferable to have a method that is applicable in languages where these tools are not available, we perform experiments both with and without the features that require linguistic analysis tools.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>First, we must consider how to efficiently assign scores S(D|F, w) to a derivation or forest during parsing.</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The most standard and efficient way to do so is to create local features that can be calculated based only on the information included in a single node d in the derivation tree.</text>
                  <doc_id>156</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The score of the whole tree can then be expressed as the sum of the scores from each node:</text>
                  <doc_id>157</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>S(D|F, w) = &#8721; d&#8712;D S(d|F, w)</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>= &#8721; d&#8712;D &#8721;</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w i &#966; i (d, F ).</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Based on this restriction, we define a number of features that can be used to score the parse tree.</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To ease explanation, we represent each node in the derivation as d = &#12296;s, l, c, c + 1, r&#12297;, where s is the node&#8217;s symbol (str, inv, or term), while l and r are the leftmost and rightmost indices of the span that d covers.</text>
                  <doc_id>162</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>c and c + 1 are the rightmost index of the left child and leftmost index of the right child for non-terminal nodes.</text>
                  <doc_id>163</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>All features are intersected with the node label s, so each feature described below corresponds to three different features (or two for features applicable to only non-terminal nodes).</text>
                  <doc_id>164</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; &#966; lex : Identities of words in positions f l , f r , f c , f c+1 , f l&#8722;1 , f r+1 , f l f r , and f c f c+1 .</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; &#966; class : Same as &#966; lex , but with words abstracted to classes.</text>
                  <doc_id>166</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We use the 50 classes automatically generated by Och (1999)&#8217;s method that are calculated during alignment in standard SMT systems.</text>
                  <doc_id>167</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; &#966; balance : For non-terminals, features indicating whether the length of the left span</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i</text>
                  <doc_id>169</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(c&#8722;l +1) is lesser than, equal to, or greater than the length of the right span (r &#8722; c).</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; &#966; table : Features, bucketed by length, that indicate whether &#8220;f l .</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>172</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>173</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>f r &#8221; appears as a contiguous phrase in the SMT training data, as well as the log frequency of the number of times the phrase appears total and the number of times it appears as a contiguous phrase (DeNero and Uszkoreit, 2011).</text>
                  <doc_id>174</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Phrase length is limited to 8, and phrases of frequency one are removed.</text>
                  <doc_id>175</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; &#966; pos : Same as &#966; lex , but with words abstracted to language-dependent POS tags.</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; &#966; cfg : Features indicating the label of the spans f l .</text>
                  <doc_id>177</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>178</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>179</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>f r , f l .</text>
                  <doc_id>180</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>181</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>182</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>f c , and f c+1 .</text>
                  <doc_id>183</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>184</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>185</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>f r in a supervised parse tree, and the intersection of the three labels.</text>
                  <doc_id>186</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>When spans do not correspond to a span in the supervised parse tree, we indicate &#8220;no span&#8221; with the label &#8220;X&#8221; (Zollmann and Venugopal, 2006).</text>
                  <doc_id>187</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Most of these features can be calculated from only a parallel corpus, but &#966; pos requires a POS tagger and &#966; cfg requires a full syntactic parser in the source language.</text>
                  <doc_id>188</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As it is preferable to have a method that is applicable in languages where these tools are not available, we perform experiments both with and without the features that require linguistic analysis tools.</text>
                  <doc_id>189</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>5.3 Finding Losses for Derivation Trees</title>
            <text>The above features &#966; and their corresponding weights w are all that are needed to calculate scores of derivation trees at test time. However, during training, it is also necessary to find model parses according to the loss-augmented scoring function S(D|F, w)+L(D|F, A) or oracle parses according to the loss L(D|F, A). As noted by Taskar et al. (2003), this is possible if our losses can be factored in the same way as the feature space. In this section, we demonstrate that the loss L(d|F, A) for the evaluation measures we defined in Section 4 can (mostly) be factored over nodes in a fashion similar to features.
5.3.1 Factoring Kendall&#8217;s &#964;
For Kendall&#8217;s &#964;, in the case of terminal nodes, L t (d = &#12296;term, l, r&#12297;|F, A) can be calculated by performing the summation in Equation (1). We can further define this sum recursively and use memoization for improved efficiency
L t (d|F, A) =L t (&#12296;term, l, r &#8722; 1&#12297;|F, A)
&#8721;r&#8722;1 + &#948;(r(f j ) &gt; r(f r )). (3)
j=l
For non-terminal nodes, we first focus on straight non-terminals with parent node d = &#12296;str, l, c, c + 1, r&#12297;, and left and right child nodes d l = &#12296;s l , l, lc, lc+1, c&#12297; and d r = &#12296;s r , c+1, rc, rc+ 1, r&#12297;. First, we note that the loss for the subtree rooted at d can be expressed as
L t (d|F, A) =L t (d l |F, A) + L t (d r |F, A)
c&#8721; r&#8721; + &#948;(r(f j1 ) &gt; r(f j2 )).
j 1 =l j 2 =c+1
In other words, the subtree&#8217;s total loss can be factored into the loss of its left subtree, the loss of its right subtree, and the additional loss contributed by comparisons between the words spanning both subtrees. In the case of inverted terminals, we must simply reverse the comparison in the final sum to be &#948;(r(f j1 ) &lt; r(f j2 )).
5.3.2 Factoring Chunk Fragmentation
Chunk fragmentation loss can be factored in a similar fashion. First, it is clear that the loss for the terminal nodes can be calculated efficiently in a fashion similar to Equation (3). In order to calculate the loss for non-terminals d, we note that the summation in Equation (2) can be divided into the sum over the internal bi-grams in the left and right subtrees, and the bi-gram spanning the reordered trees
L c (d|F, A) =L c (d l |F, A) + L c (d r |F, A)
+ discont(f &#8242; c, f &#8242; c+1).
However, unlike Kendall&#8217;s &#964;, this equation relies not on the ranks of f c and f c+1 in the original sentence, but on the ranks of f &#8242; c and f &#8242; c+1 in the reordered sentence. In order to keep track of these values, it is necessary to augment each node in the tree to be d = &#12296;s, l, c, c + 1, r, tl, tr&#12297; with two additional values tl and tr that indicate the position of the leftmost and rightmost words after reordering. Thus, a straight nonterminal parent d with children d l = &#12296;s l , l, lc, lc+ 1, c, tl, tlr&#12297; and d r = &#12296;s r , c+1, rc, rc+1, r, trl, tr&#12297; will have loss as follows
L c (d|F, A) =L c (d l |F, A) + L c (d r |F, A)
+ discont(f tlr , f trl )
with a similar calculation being possible for inverted non-terminals.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The above features &#966; and their corresponding weights w are all that are needed to calculate scores of derivation trees at test time.</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, during training, it is also necessary to find model parses according to the loss-augmented scoring function S(D|F, w)+L(D|F, A) or oracle parses according to the loss L(D|F, A).</text>
                  <doc_id>191</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As noted by Taskar et al. (2003), this is possible if our losses can be factored in the same way as the feature space.</text>
                  <doc_id>192</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In this section, we demonstrate that the loss L(d|F, A) for the evaluation measures we defined in Section 4 can (mostly) be factored over nodes in a fashion similar to features.</text>
                  <doc_id>193</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5.3.1 Factoring Kendall&#8217;s &#964;</text>
                  <doc_id>194</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For Kendall&#8217;s &#964;, in the case of terminal nodes, L t (d = &#12296;term, l, r&#12297;|F, A) can be calculated by performing the summation in Equation (1).</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We can further define this sum recursively and use memoization for improved efficiency</text>
                  <doc_id>196</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L t (d|F, A) =L t (&#12296;term, l, r &#8722; 1&#12297;|F, A)</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;r&#8722;1 + &#948;(r(f j ) &gt; r(f r )).</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(3)</text>
                  <doc_id>199</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=l</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For non-terminal nodes, we first focus on straight non-terminals with parent node d = &#12296;str, l, c, c + 1, r&#12297;, and left and right child nodes d l = &#12296;s l , l, lc, lc+1, c&#12297; and d r = &#12296;s r , c+1, rc, rc+ 1, r&#12297;.</text>
                  <doc_id>201</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>First, we note that the loss for the subtree rooted at d can be expressed as</text>
                  <doc_id>202</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L t (d|F, A) =L t (d l |F, A) + L t (d r |F, A)</text>
                  <doc_id>203</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c&#8721; r&#8721; + &#948;(r(f j1 ) &gt; r(f j2 )).</text>
                  <doc_id>204</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j 1 =l j 2 =c+1</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In other words, the subtree&#8217;s total loss can be factored into the loss of its left subtree, the loss of its right subtree, and the additional loss contributed by comparisons between the words spanning both subtrees.</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the case of inverted terminals, we must simply reverse the comparison in the final sum to be &#948;(r(f j1 ) &lt; r(f j2 )).</text>
                  <doc_id>207</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5.3.2 Factoring Chunk Fragmentation</text>
                  <doc_id>208</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Chunk fragmentation loss can be factored in a similar fashion.</text>
                  <doc_id>209</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>First, it is clear that the loss for the terminal nodes can be calculated efficiently in a fashion similar to Equation (3).</text>
                  <doc_id>210</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In order to calculate the loss for non-terminals d, we note that the summation in Equation (2) can be divided into the sum over the internal bi-grams in the left and right subtrees, and the bi-gram spanning the reordered trees</text>
                  <doc_id>211</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L c (d|F, A) =L c (d l |F, A) + L c (d r |F, A)</text>
                  <doc_id>212</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>+ discont(f &#8242; c, f &#8242; c+1).</text>
                  <doc_id>213</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>However, unlike Kendall&#8217;s &#964;, this equation relies not on the ranks of f c and f c+1 in the original sentence, but on the ranks of f &#8242; c and f &#8242; c+1 in the reordered sentence.</text>
                  <doc_id>214</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In order to keep track of these values, it is necessary to augment each node in the tree to be d = &#12296;s, l, c, c + 1, r, tl, tr&#12297; with two additional values tl and tr that indicate the position of the leftmost and rightmost words after reordering.</text>
                  <doc_id>215</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Thus, a straight nonterminal parent d with children d l = &#12296;s l , l, lc, lc+ 1, c, tl, tlr&#12297; and d r = &#12296;s r , c+1, rc, rc+1, r, trl, tr&#12297; will have loss as follows</text>
                  <doc_id>216</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L c (d|F, A) =L c (d l |F, A) + L c (d r |F, A)</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>+ discont(f tlr , f trl )</text>
                  <doc_id>218</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>with a similar calculation being possible for inverted non-terminals.</text>
                  <doc_id>219</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>5.4 Parsing Derivation Trees</title>
            <text>Finally, we must be able to create a parse forest from which we select model and oracle parses. As all feature functions factor over single nodes, it is possible to find the parse tree with the highest score in O(J 3 ) time using the CKY algorithm. However, when keeping track of target positions for calculation of chunk fragmentation loss, there are a total of O(J 5 ) nodes, an unreasonable burden in terms of time and memory. To overcome this problem, we note that this setting is nearly identical to translation using synchronous CFGs with an integrated bigram LM, and thus we can employ cube-pruning to reduce our search space (Chiang, 2007).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Finally, we must be able to create a parse forest from which we select model and oracle parses.</text>
                  <doc_id>220</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As all feature functions factor over single nodes, it is possible to find the parse tree with the highest score in O(J 3 ) time using the CKY algorithm.</text>
                  <doc_id>221</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, when keeping track of target positions for calculation of chunk fragmentation loss, there are a total of O(J 5 ) nodes, an unreasonable burden in terms of time and memory.</text>
                  <doc_id>222</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>To overcome this problem, we note that this setting is nearly identical to translation using synchronous CFGs with an integrated bigram LM, and thus we can employ cube-pruning to reduce our search space (Chiang, 2007).</text>
                  <doc_id>223</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>6 Experiments</title>
        <text>Our experiments test the reordering and translation accuracy of translation systems using the proposed method. As reordering metrics, we use Kendall&#8217;s &#964; and chunk fragmentation (Talbot et al., 2011) comparing the system F &#8242; and oracle F &#8242; calculated with manually created alignments. As translation metrics, we use BLEU (Papineni et al., 2002), as well as RIBES (Isozaki et al., 2010a), which is similar to Kendall&#8217;s &#964;, but evaluated on the target sentence E instead of the reordered sentence F &#8242; . All scores are the average of three training runs to control for randomness in training (Clark et al., 2011). For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments. We test three types
of pre-ordering: original order with F &#8242; &#8592; F (orig), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3- step), and the proposed model with latent derivations (lader). 7 Except when stated otherwise, lader was trained to minimize chunk fragmentation loss with a cube pruning stack pop limit of 50, and the regularization constant of 10 &#8722;3 (chosen through cross-validation).
We test our systems on Japanese-English and English-Japanese translation using data from the Kyoto Free Translation Task (Neubig, 2011). We use the training set for training translation and language models, the development set for weight tuning, and the test set for testing (Table 1). We use the designated development and test sets of manually created alignments as training data for the reordering models, removing sentences of more than 60 words. As default features for lader and the monolingual parsing and reordering models in 3-step, we use all the features described in Section 5.2
7 Available open-source: http://phontron.com/lader
except &#966; pos and &#966; cfg . In addition, we test systems with &#966; pos and &#966; cfg added. For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing. For Japanese, we use the KyTea tagger (Neubig et al., 2011) for POS tagging, 8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Our experiments test the reordering and translation accuracy of translation systems using the proposed method.</text>
              <doc_id>224</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As reordering metrics, we use Kendall&#8217;s &#964; and chunk fragmentation (Talbot et al., 2011) comparing the system F &#8242; and oracle F &#8242; calculated with manually created alignments.</text>
              <doc_id>225</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>As translation metrics, we use BLEU (Papineni et al., 2002), as well as RIBES (Isozaki et al., 2010a), which is similar to Kendall&#8217;s &#964;, but evaluated on the target sentence E instead of the reordered sentence F &#8242; .</text>
              <doc_id>226</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>All scores are the average of three training runs to control for randomness in training (Clark et al., 2011).</text>
              <doc_id>227</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>For translation, we use Moses (Koehn et al., 2007) with lexicalized reordering (Koehn et al., 2005) in all experiments.</text>
              <doc_id>228</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We test three types</text>
              <doc_id>229</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>of pre-ordering: original order with F &#8242; &#8592; F (orig), pre-orderings learned using the 3-step process of DeNero and Uszkoreit (2011) (3- step), and the proposed model with latent derivations (lader).</text>
              <doc_id>230</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>7 Except when stated otherwise, lader was trained to minimize chunk fragmentation loss with a cube pruning stack pop limit of 50, and the regularization constant of 10 &#8722;3 (chosen through cross-validation).</text>
              <doc_id>231</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We test our systems on Japanese-English and English-Japanese translation using data from the Kyoto Free Translation Task (Neubig, 2011).</text>
              <doc_id>232</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We use the training set for training translation and language models, the development set for weight tuning, and the test set for testing (Table 1).</text>
              <doc_id>233</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We use the designated development and test sets of manually created alignments as training data for the reordering models, removing sentences of more than 60 words.</text>
              <doc_id>234</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As default features for lader and the monolingual parsing and reordering models in 3-step, we use all the features described in Section 5.2</text>
              <doc_id>235</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>7 Available open-source: http://phontron.com/lader</text>
              <doc_id>236</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>except &#966; pos and &#966; cfg .</text>
              <doc_id>237</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In addition, we test systems with &#966; pos and &#966; cfg added.</text>
              <doc_id>238</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For English, we use the Stanford parser (Klein and Manning, 2003) for both POS tagging and CFG parsing.</text>
              <doc_id>239</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>For Japanese, we use the KyTea tagger (Neubig et al., 2011) for POS tagging, 8 and the EDA word-based dependency parser (Flannery et al., 2011) with simple manual head-rules to convert a dependency parse to a CFG parse.</text>
              <doc_id>240</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>6.1 Effect of Pre-ordering</title>
            <text>Table 2 shows reordering and translation results for orig, 3-step, and lader. It can be seen that the proposed lader outperforms the baselines in both reordering and translation. 9 There are a number of reasons why lader outperforms 3-step. First, the pipeline of 3-step suffers from error propogation, with errors in monolingual parsing and reordering resulting in low overall accuracy. 10 Second, as Section 5.1 describes, lader breaks ties between oracle parses based on model score, allowing easyto-reproduce model parses to be chosen during training. In fact, lader generally found trees that followed from syntactic constituency, while 3-step more often used terminal nodes
8 In addition, following the example of Sudoh et al.
(2011a)&#8217;s reordering rules, we lexicalize all particles. 9 It should be noted that our results for 3-step are
significantly worse than those of DeNero and Uszkoreit (2011). Likely reasons include a 20x difference in training data size, the fact that we are using naturally translated text as opposed to text translated specifically to create word alignments, or differences in implementation. 10 When using oracle parses, chunk accuracy was up to
81%, showing that parsing errors are highly detrimental.
that spanned constituent boundaries (as long as the phrase frequency was high). Finally, as Section 6.2 shows in detail, the ability of lader to maximize reordering accuracy directly allows for improved reordering and translation results.
It can also be seen that incorporating POS tags or parse trees improves accuracy of both lader and 3-step, particularly for English- Japanese, where syntax has proven useful for pre-ordering, and less so for Japanese-English, where syntactic pre-ordering has been less successful (Sudoh et al., 2011b). We also tested Moses&#8217;s implementation of hierarchical phrase-based SMT (Chiang, 2007), which achieved BLEU scores of 23.21 and 19.30 for English-Japanese and Japanese-English respectively, approximately matching lader in accuracy, but with a significant decrease in decoding speed. Further, when pre-ordering with lader and hierarchical phrase-based SMT were combined, BLEU scores rose to 23.29 and 19.69, indicating that the two techniques can be combined for further accuracy improvements.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Table 2 shows reordering and translation results for orig, 3-step, and lader.</text>
                  <doc_id>241</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It can be seen that the proposed lader outperforms the baselines in both reordering and translation.</text>
                  <doc_id>242</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>9 There are a number of reasons why lader outperforms 3-step.</text>
                  <doc_id>243</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>First, the pipeline of 3-step suffers from error propogation, with errors in monolingual parsing and reordering resulting in low overall accuracy.</text>
                  <doc_id>244</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>10 Second, as Section 5.1 describes, lader breaks ties between oracle parses based on model score, allowing easyto-reproduce model parses to be chosen during training.</text>
                  <doc_id>245</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In fact, lader generally found trees that followed from syntactic constituency, while 3-step more often used terminal nodes</text>
                  <doc_id>246</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>8 In addition, following the example of Sudoh et al.</text>
                  <doc_id>247</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(2011a)&#8217;s reordering rules, we lexicalize all particles.</text>
                  <doc_id>248</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>9 It should be noted that our results for 3-step are</text>
                  <doc_id>249</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>significantly worse than those of DeNero and Uszkoreit (2011).</text>
                  <doc_id>250</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Likely reasons include a 20x difference in training data size, the fact that we are using naturally translated text as opposed to text translated specifically to create word alignments, or differences in implementation.</text>
                  <doc_id>251</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>10 When using oracle parses, chunk accuracy was up to</text>
                  <doc_id>252</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>81%, showing that parsing errors are highly detrimental.</text>
                  <doc_id>253</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>that spanned constituent boundaries (as long as the phrase frequency was high).</text>
                  <doc_id>254</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, as Section 6.2 shows in detail, the ability of lader to maximize reordering accuracy directly allows for improved reordering and translation results.</text>
                  <doc_id>255</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It can also be seen that incorporating POS tags or parse trees improves accuracy of both lader and 3-step, particularly for English- Japanese, where syntax has proven useful for pre-ordering, and less so for Japanese-English, where syntactic pre-ordering has been less successful (Sudoh et al., 2011b).</text>
                  <doc_id>256</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We also tested Moses&#8217;s implementation of hierarchical phrase-based SMT (Chiang, 2007), which achieved BLEU scores of 23.21 and 19.30 for English-Japanese and Japanese-English respectively, approximately matching lader in accuracy, but with a significant decrease in decoding speed.</text>
                  <doc_id>257</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Further, when pre-ordering with lader and hierarchical phrase-based SMT were combined, BLEU scores rose to 23.29 and 19.69, indicating that the two techniques can be combined for further accuracy improvements.</text>
                  <doc_id>258</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>6.2 Effect of Training Loss</title>
            <text>Table 3 shows results when one of three losses is optimized during training: chunk fragmentation (L c ), Kendall&#8217;s &#964; (L t ), or the linear interpolation of the two with weights chosen so that both losses contribute equally (L t + L c ). In general, training successfully maximizes the criterion it is trained on, and L t + L c achieves good results on both measures. We also find that L c and L c +L t achieve the best translation results, which is in concert with Talbot et al. (2011), who find chunk fragmentation is better correlated with translation accuracy than Kendall&#8217;s &#964;. This is an important result, as methods such as that of Tromble and Eisner (2009) optimize pairwise
word comparisons equivalent to L t , which may not be optimal for translation.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Table 3 shows results when one of three losses is optimized during training: chunk fragmentation (L c ), Kendall&#8217;s &#964; (L t ), or the linear interpolation of the two with weights chosen so that both losses contribute equally (L t + L c ).</text>
                  <doc_id>259</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In general, training successfully maximizes the criterion it is trained on, and L t + L c achieves good results on both measures.</text>
                  <doc_id>260</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We also find that L c and L c +L t achieve the best translation results, which is in concert with Talbot et al. (2011), who find chunk fragmentation is better correlated with translation accuracy than Kendall&#8217;s &#964;.</text>
                  <doc_id>261</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This is an important result, as methods such as that of Tromble and Eisner (2009) optimize pairwise</text>
                  <doc_id>262</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>word comparisons equivalent to L t , which may not be optimal for translation.</text>
                  <doc_id>263</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>6.3 Effect of Automatic Alignments</title>
            <text>Table 4 shows the difference between using manual and automatic alignments in the training of lader. lader is able to improve over the orig baseline in all cases, but when equal numbers of manual and automatic alignments are used, the reorderer trained on manual alignments is significantly better. However, as the number of automatic alignments is increased, accuracy improves, approaching that of the system trained on a smaller number of manual alignments.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Table 4 shows the difference between using manual and automatic alignments in the training of lader.</text>
                  <doc_id>264</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>lader is able to improve over the orig baseline in all cases, but when equal numbers of manual and automatic alignments are used, the reorderer trained on manual alignments is significantly better.</text>
                  <doc_id>265</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, as the number of automatic alignments is increased, accuracy improves, approaching that of the system trained on a smaller number of manual alignments.</text>
                  <doc_id>266</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>7 Conclusion</title>
        <text>We presented a method for learning a discriminative parser to maximize reordering accuracy for machine translation. Future work includes application to other language pairs, development of more sophisticated features, investigation of probabilistic approaches to inference, and incorporation of the learned trees directly in tree-to-string translation.
Acknowledgments
We thank Isao Goto, Tetsuo Kiso, and anonymous reviewers for their helpful comments, and Daniel Flannery for helping to run his parser.
References
Satanjeev Banerjee and Alon Lavie. 2005. ME- TEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proc. ACL Workshop. Alexandra Birch, Miles Osborne, and Phil Blunsom. 2010. Metrics for MT evaluation: evaluating reordering. Machine Translation, 24(1):15&#8211;26. Marine Carpuat, Yuval Marton, and Nizar Habash. 2010. Improving arabic-to-english statistical machine translation by reordering post-verbal subjects for alignment. In Proc. ACL. David Chiang. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2). Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proc. ACL, pages 176&#8211;181. Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine translation. In Proc. ACL. Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. EMNLP, pages 1&#8211;8. Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551&#8211;585. John DeNero and Jakob Uszkoreit. 2011. Inducing sentence structure from parallel corpora for reordering. In Proc. EMNLP. Chris Dyer and Philip Resnik. 2010. Context-free
reordering, finite-state translation. In Proc. HLT- NAACL. Daniel Flannery, Yusuke Miyao, Graham Neubig, and Shinsuke Mori. 2011. Training dependency parsers from partially annotated corpora. In Proc. IJCNLP, pages 776&#8211;784, Chiang Mai, Thailand, November. Dmitriy Genzel. 2010. Automatically learning
source-side reordering rules for large scale machine translation. In Proc. COLING. Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised ITG models. In Proc. ACL. Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010a. Automatic evaluation of translation quality for distant language pairs. In Proc. EMNLP, pages 944&#8211;952. Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada,
and Kevin Duh. 2010b. Head finalization: A simple reordering rule for sov languages. In Proc. WMT and MetricsMATR.
Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz Och, David Talbot, Hiroshi Ichikawa, Masakazu Seno, and Hideto Kazawa. 2011. Training a parser for machine translation reordering. In Proc. EMNLP, pages 183&#8211;192.
Maurice G. Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1/2):81&#8211;93.
Maxim Khalilov and Khalil Sima&#8217;an. 2011. Contextsensitive syntactic source-reordering by statistical transduction. In Proc. IJCNLP.
Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proc. ACL, pages 423&#8211;430.
Phillip Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. HLT, pages 48&#8211;54.
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proc. IWSLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL, pages 177&#8211;180.
Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. EMNLP.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming Zhou, and Yi Guan. 2007. A probabilistic approach to syntax-based reordering for statistical machine translation. In Proc. ACL.
Percy Liang, Alexandre Bouchard-C&#244;t&#233;, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proc. ACL, pages 761&#8211;768.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori. 2011. Pointwise prediction for robust, adaptable Japanese morphological analysis. In Proc. ACL, pages 529&#8211;533, Portland, USA, June.
Graham Neubig. 2011. The Kyoto free translation task. http://www.phontron.com/kftt.
Franz Josef Och. 1999. An efficient method for determining bilingual word classes. In Proc. EACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. COLING, pages 311&#8211;318. Kay Rottmann and Stephan Vogel. 2007. Word reordering in statistical machine translation with a pos-based distortion model. In Proc. of TMI-2007.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. 2007. Pegasos: Primal estimated subgradient solver for SVM. In Proc. ICML, pages 807&#8211;814.
Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Masaaki Nagata, Xianchao Wu, Takuya Matsuzaki, and Jun&#8217;ichi Tsujii. 2011a. NTT- UT statistical machine translation in NTCIR-9 PatentMT. In Proc. NTCIR.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime Tsukada, and Masaaki Nagata. 2011b. Postordering in statistical machine translation. In Proc. MT Summit.
David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Jason Katz-Brown, Masakazu Seno, and Franz Och. 2011. A lightweight evaluation framework for machine translation reordering. In Proc. WMT. Ben Taskar, Carlos Guestrin, and Daphne Koller.
2003. Max-margin Markov networks. Proc. NIPS, 16. Roy Tromble and Jason Eisner. 2009. Learning linear ordering problems for better translation. In Proc. EMNLP.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur Gandhe, Ananthakrishnan Ramanathan, and Jiri Navratil. 2011. A word reordering model for improved machine translation. In Proc. EMNLP. Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proc. EMNLP, pages 764&#8211;773. Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3). Fei Xia and Michael McCord. 2004. Improving a
statistical MT system with automatically learned rewrite patterns. In Proc. COLING.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proc.
NAACL. Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proc. ACL. Chun-Nam John Yu and Thorsten Joachims. 2009. Learning structural SVMs with latent variables. In Proc. ICML, pages 1169&#8211;1176.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007. Chunk-level reordering of source language sentences with automatically learned rules for statistical machine translation. In Proc. SSST. Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proc. WMT, pages 138&#8211;141.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We presented a method for learning a discriminative parser to maximize reordering accuracy for machine translation.</text>
              <doc_id>267</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Future work includes application to other language pairs, development of more sophisticated features, investigation of probabilistic approaches to inference, and incorporation of the learned trees directly in tree-to-string translation.</text>
              <doc_id>268</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Acknowledgments</text>
              <doc_id>269</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We thank Isao Goto, Tetsuo Kiso, and anonymous reviewers for their helpful comments, and Daniel Flannery for helping to run his parser.</text>
              <doc_id>270</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>References</text>
              <doc_id>271</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Satanjeev Banerjee and Alon Lavie.</text>
              <doc_id>272</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>273</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>ME- TEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</text>
              <doc_id>274</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. ACL Workshop.</text>
              <doc_id>275</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Alexandra Birch, Miles Osborne, and Phil Blunsom.</text>
              <doc_id>276</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>277</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Metrics for MT evaluation: evaluating reordering.</text>
              <doc_id>278</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Machine Translation, 24(1):15&#8211;26.</text>
              <doc_id>279</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Marine Carpuat, Yuval Marton, and Nizar Habash.</text>
              <doc_id>280</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>281</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Improving arabic-to-english statistical machine translation by reordering post-verbal subjects for alignment.</text>
              <doc_id>282</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. ACL.</text>
              <doc_id>283</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>David Chiang.</text>
              <doc_id>284</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>285</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>Hierarchical phrase-based</text>
              <doc_id>286</doc_id>
              <sec_id>14</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>translation.</text>
              <doc_id>287</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 33(2).</text>
              <doc_id>288</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith.</text>
              <doc_id>289</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2011.</text>
              <doc_id>290</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</text>
              <doc_id>291</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. ACL, pages 176&#8211;181.</text>
              <doc_id>292</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Michael Collins, Philipp Koehn, and Ivona Kucerova.</text>
              <doc_id>293</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2005.</text>
              <doc_id>294</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Clause restructuring for statistical machine translation.</text>
              <doc_id>295</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. ACL.</text>
              <doc_id>296</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Michael Collins.</text>
              <doc_id>297</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>298</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</text>
              <doc_id>299</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. EMNLP, pages 1&#8211;8.</text>
              <doc_id>300</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai</text>
              <doc_id>301</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Shalev-Shwartz, and Yoram Singer.</text>
              <doc_id>302</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>303</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Online passive-aggressive algorithms.</text>
              <doc_id>304</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Journal of Machine Learning Research, 7:551&#8211;585.</text>
              <doc_id>305</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>John DeNero and Jakob Uszkoreit.</text>
              <doc_id>306</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2011. Inducing sentence structure from parallel corpora for reordering.</text>
              <doc_id>307</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. EMNLP.</text>
              <doc_id>308</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Chris Dyer and Philip Resnik.</text>
              <doc_id>309</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>310</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Context-free</text>
              <doc_id>311</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>reordering, finite-state translation.</text>
              <doc_id>312</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. HLT- NAACL.</text>
              <doc_id>313</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Daniel Flannery, Yusuke Miyao, Graham Neubig, and Shinsuke Mori.</text>
              <doc_id>314</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2011.</text>
              <doc_id>315</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Training dependency parsers from partially annotated corpora.</text>
              <doc_id>316</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. IJCNLP, pages 776&#8211;784, Chiang Mai, Thailand, November.</text>
              <doc_id>317</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Dmitriy Genzel.</text>
              <doc_id>318</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>319</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Automatically learning</text>
              <doc_id>320</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>source-side reordering rules for large scale machine translation.</text>
              <doc_id>321</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. COLING.</text>
              <doc_id>322</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Aria Haghighi, John Blitzer, John DeNero, and Dan</text>
              <doc_id>323</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Klein.</text>
              <doc_id>324</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>325</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Better word alignments with supervised ITG models.</text>
              <doc_id>326</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. ACL.</text>
              <doc_id>327</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.</text>
              <doc_id>328</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2010a.</text>
              <doc_id>329</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Automatic evaluation of translation quality for distant language pairs.</text>
              <doc_id>330</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. EMNLP, pages 944&#8211;952.</text>
              <doc_id>331</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada,</text>
              <doc_id>332</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and Kevin Duh.</text>
              <doc_id>333</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2010b.</text>
              <doc_id>334</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Head finalization: A simple reordering rule for sov languages.</text>
              <doc_id>335</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. WMT and MetricsMATR.</text>
              <doc_id>336</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz Och, David Talbot, Hiroshi Ichikawa, Masakazu Seno, and Hideto Kazawa.</text>
              <doc_id>337</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2011.</text>
              <doc_id>338</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Training a parser for machine translation reordering.</text>
              <doc_id>339</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. EMNLP, pages 183&#8211;192.</text>
              <doc_id>340</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Maurice G. Kendall.</text>
              <doc_id>341</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1938.</text>
              <doc_id>342</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A new measure of rank correlation.</text>
              <doc_id>343</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Biometrika, 30(1/2):81&#8211;93.</text>
              <doc_id>344</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Maxim Khalilov and Khalil Sima&#8217;an.</text>
              <doc_id>345</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2011.</text>
              <doc_id>346</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Contextsensitive syntactic source-reordering by statistical transduction.</text>
              <doc_id>347</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. IJCNLP.</text>
              <doc_id>348</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Dan Klein and Christopher D. Manning.</text>
              <doc_id>349</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>350</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Accurate unlexicalized parsing.</text>
              <doc_id>351</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. ACL, pages 423&#8211;430.</text>
              <doc_id>352</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Phillip Koehn, Franz Josef Och, and Daniel Marcu.</text>
              <doc_id>353</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>354</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Statistical phrase-based translation.</text>
              <doc_id>355</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. HLT, pages 48&#8211;54.</text>
              <doc_id>356</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot.</text>
              <doc_id>357</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>358</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</text>
              <doc_id>359</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. IWSLT.</text>
              <doc_id>360</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst.</text>
              <doc_id>361</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>362</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Moses: Open source toolkit for statistical machine translation.</text>
              <doc_id>363</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. ACL, pages 177&#8211;180.</text>
              <doc_id>364</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Philipp Koehn.</text>
              <doc_id>365</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2004.</text>
              <doc_id>366</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Statistical significance tests for machine translation evaluation.</text>
              <doc_id>367</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. EMNLP.</text>
              <doc_id>368</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming Zhou, and Yi Guan.</text>
              <doc_id>369</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>370</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A probabilistic approach to syntax-based reordering for statistical machine translation.</text>
              <doc_id>371</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. ACL.</text>
              <doc_id>372</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Percy Liang, Alexandre Bouchard-C&#244;t&#233;, Dan Klein, and Ben Taskar.</text>
              <doc_id>373</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>374</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>An end-to-end discriminative approach to machine translation.</text>
              <doc_id>375</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. ACL, pages 761&#8211;768.</text>
              <doc_id>376</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Graham Neubig, Yosuke Nakata, and Shinsuke Mori.</text>
              <doc_id>377</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2011.</text>
              <doc_id>378</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Pointwise prediction for robust, adaptable Japanese morphological analysis.</text>
              <doc_id>379</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. ACL, pages 529&#8211;533, Portland, USA, June.</text>
              <doc_id>380</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Graham Neubig.</text>
              <doc_id>381</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2011.</text>
              <doc_id>382</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The Kyoto free translation task.</text>
              <doc_id>383</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>http://www.phontron.com/kftt.</text>
              <doc_id>384</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Franz Josef Och.</text>
              <doc_id>385</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1999.</text>
              <doc_id>386</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>An efficient method for determining bilingual word classes.</text>
              <doc_id>387</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. EACL.</text>
              <doc_id>388</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.</text>
              <doc_id>389</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>390</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>BLEU: a method for automatic evaluation of machine translation.</text>
              <doc_id>391</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. COLING, pages 311&#8211;318.</text>
              <doc_id>392</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Kay Rottmann and Stephan Vogel.</text>
              <doc_id>393</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>394</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Word reordering in statistical machine translation with a pos-based distortion model.</text>
              <doc_id>395</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of TMI-2007.</text>
              <doc_id>396</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.</text>
              <doc_id>397</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>398</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Pegasos: Primal estimated subgradient solver for SVM.</text>
              <doc_id>399</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. ICML, pages 807&#8211;814.</text>
              <doc_id>400</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, Masaaki Nagata, Xianchao Wu, Takuya Matsuzaki, and Jun&#8217;ichi Tsujii.</text>
              <doc_id>401</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2011a.</text>
              <doc_id>402</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>NTT- UT statistical machine translation in NTCIR-9 PatentMT.</text>
              <doc_id>403</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. NTCIR.</text>
              <doc_id>404</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime Tsukada, and Masaaki Nagata.</text>
              <doc_id>405</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2011b.</text>
              <doc_id>406</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Postordering in statistical machine translation.</text>
              <doc_id>407</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. MT Summit.</text>
              <doc_id>408</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Jason Katz-Brown, Masakazu Seno, and Franz Och.</text>
              <doc_id>409</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2011.</text>
              <doc_id>410</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A lightweight evaluation framework for machine translation reordering.</text>
              <doc_id>411</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. WMT.</text>
              <doc_id>412</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Ben Taskar, Carlos Guestrin, and Daphne Koller.</text>
              <doc_id>413</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2003.</text>
              <doc_id>414</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Max-margin Markov networks.</text>
              <doc_id>415</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Proc. NIPS, 16.</text>
              <doc_id>416</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Roy Tromble and Jason Eisner.</text>
              <doc_id>417</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>418</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Learning linear ordering problems for better translation.</text>
              <doc_id>419</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. EMNLP.</text>
              <doc_id>420</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur Gandhe, Ananthakrishnan Ramanathan, and Jiri Navratil.</text>
              <doc_id>421</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2011.</text>
              <doc_id>422</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A word reordering model for improved machine translation.</text>
              <doc_id>423</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. EMNLP.</text>
              <doc_id>424</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki.</text>
              <doc_id>425</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>426</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Online large-margin training for statistical machine translation.</text>
              <doc_id>427</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. EMNLP, pages 764&#8211;773.</text>
              <doc_id>428</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Dekai Wu.</text>
              <doc_id>429</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>1997.</text>
              <doc_id>430</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Stochastic inversion transduction</text>
              <doc_id>431</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>grammars and bilingual parsing of parallel corpora.</text>
              <doc_id>432</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 23(3).</text>
              <doc_id>433</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Fei Xia and Michael McCord.</text>
              <doc_id>434</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2004.</text>
              <doc_id>435</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Improving a</text>
              <doc_id>436</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>statistical MT system with automatically learned rewrite patterns.</text>
              <doc_id>437</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. COLING.</text>
              <doc_id>438</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och.</text>
              <doc_id>439</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>440</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Using a dependency parser to improve</text>
              <doc_id>441</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>smt for subject-object-verb languages.</text>
              <doc_id>442</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proc.</text>
              <doc_id>443</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>NAACL.</text>
              <doc_id>444</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Kenji Yamada and Kevin Knight.</text>
              <doc_id>445</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>2001.</text>
              <doc_id>446</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>A syntaxbased statistical translation model.</text>
              <doc_id>447</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. ACL.</text>
              <doc_id>448</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Chun-Nam John Yu and Thorsten Joachims.</text>
              <doc_id>449</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>450</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Learning structural SVMs with latent variables.</text>
              <doc_id>451</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. ICML, pages 1169&#8211;1176.</text>
              <doc_id>452</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Yuqi Zhang, Richard Zens, and Hermann Ney.</text>
              <doc_id>453</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>454</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Chunk-level reordering of source language sentences with automatically learned rules for statistical machine translation.</text>
              <doc_id>455</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. SSST.</text>
              <doc_id>456</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Andreas Zollmann and Ashish Venugopal.</text>
              <doc_id>457</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>458</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Syntax augmented machine translation via chart parsing.</text>
              <doc_id>459</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. WMT, pages 138&#8211;141.</text>
              <doc_id>460</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: The number of sentences and words for training and testing the reordering model (RM), translation model (TM), and language model (LM).</caption>
        <reference_text>None</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell>sent.</cell>
              <cell>word (ja)</cell>
              <cell>word (en)</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>RM-train</cell>
              <cell>602</cell>
              <cell>14.5k</cell>
              <cell>14.3k</cell>
            </row>
            <row>
              <cell>RM-test</cell>
              <cell>555</cell>
              <cell>11.2k</cell>
              <cell>10.4k</cell>
            </row>
            <row>
              <cell>TM/LM</cell>
              <cell>329k</cell>
              <cell>6.08M</cell>
              <cell>5.91M</cell>
            </row>
            <row>
              <cell>Tune</cell>
              <cell>1166</cell>
              <cell>26.8k</cell>
              <cell>24.3k</cell>
            </row>
            <row>
              <cell>Test</cell>
              <cell>1160</cell>
              <cell>28.5k</cell>
              <cell>26.7k</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Reordering (chunk, &#964;) and translation (BLEU, RIBES) results for each system. Bold numbers indicate no significant difference from the best system (bootstrap resampling with p &gt; 0.05) (Koehn, 2004).</caption>
        <reference_text>In PAGE 8: ... 6.1 E?ect of Pre-ordering  Table2  shows reordering and translation results for orig, 3-step, and lader. It can be seen that the proposed lader outperforms the base- lines in both reordering and translation....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>en-ja</cell>
              <cell>ja-en</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell></cell>
              <cell>Chunk</cell>
              <cell>&#964;</cell>
              <cell>BLEU</cell>
              <cell>RIBES</cell>
              <cell>Chunk</cell>
              <cell>&#964;</cell>
              <cell>BLEU</cell>
              <cell>RIBES</cell>
            </row>
            <row>
              <cell>orig</cell>
              <cell>61.22</cell>
              <cell>73.46</cell>
              <cell>21.87</cell>
              <cell>68.25</cell>
              <cell>66.42</cell>
              <cell>72.99</cell>
              <cell>18.34</cell>
              <cell>65.36</cell>
            </row>
            <row>
              <cell>3-step</cell>
              <cell>63.51</cell>
              <cell>72.55</cell>
              <cell>21.45</cell>
              <cell>67.66</cell>
              <cell>67.17</cell>
              <cell>73.01</cell>
              <cell>17.78</cell>
              <cell>64.42</cell>
            </row>
            <row>
              <cell>3-step+&#966; pos</cell>
              <cell>64.28</cell>
              <cell>72.11</cell>
              <cell>21.45</cell>
              <cell>67.44</cell>
              <cell>67.56</cell>
              <cell>74.21</cell>
              <cell>18.18</cell>
              <cell>64.65</cell>
            </row>
            <row>
              <cell>3-step+&#966; cfg</cell>
              <cell>65.76</cell>
              <cell>75.32</cell>
              <cell>21.67</cell>
              <cell>68.47</cell>
              <cell>67.23</cell>
              <cell>74.06</cell>
              <cell>18.18</cell>
              <cell>64.93</cell>
            </row>
            <row>
              <cell>lader</cell>
              <cell>73.19</cell>
              <cell>78.44</cell>
              <cell>23.11</cell>
              <cell>69.86</cell>
              <cell>75.14</cell>
              <cell>79.14</cell>
              <cell>19.54</cell>
              <cell>66.93</cell>
            </row>
            <row>
              <cell>lader+&#966; pos</cell>
              <cell>73.97</cell>
              <cell>79.24</cell>
              <cell>23.32</cell>
              <cell>69.78</cell>
              <cell>75.49</cell>
              <cell>78.79</cell>
              <cell>19.89</cell>
              <cell>67.24</cell>
            </row>
            <row>
              <cell>lader+&#966; cfg</cell>
              <cell>75.06</cell>
              <cell>80.53</cell>
              <cell>23.36</cell>
              <cell>70.89</cell>
              <cell>75.14</cell>
              <cell>77.80</cell>
              <cell>19.35</cell>
              <cell>66.12</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TET</source>
        <caption>Table 3: Results for systems trained to optimize chunk fragmentation (L c ) or Kendall&#8217;s &#964; (L t ).</caption>
        <reference_text></reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>en-ja</cell>
              <cell>ja-en</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell></cell>
              <cell>Chunk</cell>
              <cell>&#964;</cell>
              <cell>BLEU</cell>
              <cell>RIBES</cell>
              <cell>Chunk</cell>
              <cell>&#964;</cell>
              <cell>BLEU</cell>
              <cell>RIBES</cell>
            </row>
            <row>
              <cell>L c</cell>
              <cell>73.19</cell>
              <cell>78.44</cell>
              <cell>23.11</cell>
              <cell>69.86</cell>
              <cell>75.14</cell>
              <cell>79.14</cell>
              <cell>19.54</cell>
              <cell>66.93</cell>
            </row>
            <row>
              <cell>L t</cell>
              <cell>70.37</cell>
              <cell>79.57</cell>
              <cell>22.57</cell>
              <cell>69.47</cell>
              <cell>72.51</cell>
              <cell>78.93</cell>
              <cell>18.52</cell>
              <cell>66.26</cell>
            </row>
            <row>
              <cell>L c + L t</cell>
              <cell>72.55</cell>
              <cell>80.58</cell>
              <cell>22.89</cell>
              <cell>70.34</cell>
              <cell>74.44</cell>
              <cell>79.82</cell>
              <cell>19.21</cell>
              <cell>66.48</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: Results based on data size, and whether manual or automatic alignments are used in training.</caption>
        <reference_text>In PAGE 9: ... 6.3 E?ect of Automatic Alignments  Table4  shows the di?erence between using man- ual and automatic alignments in the training of lader. lader is able to improve over the orig baseline in all cases, but when equal numbers of manual and automatic alignments are used, the reorderer trained on manual alignments is signi?cantly better....</reference_text>
        <page_num>9</page_num>
        <head>
          <rows>
            <row>
              <cell>en-ja</cell>
              <cell>ja-en</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>BLEU/RIBES</cell>
              <cell>BLEU/RIBES</cell>
            </row>
            <row>
              <cell>orig</cell>
              <cell>21.87</cell>
              <cell>68.25</cell>
              <cell>18.34</cell>
              <cell>65.36</cell>
            </row>
            <row>
              <cell>man-602</cell>
              <cell>23.11</cell>
              <cell>69.86</cell>
              <cell>19.54</cell>
              <cell>66.93</cell>
            </row>
            <row>
              <cell>auto-602</cell>
              <cell>22.39</cell>
              <cell>69.19</cell>
              <cell>18.58</cell>
              <cell>66.07</cell>
            </row>
            <row>
              <cell>auto-10k</cell>
              <cell>22.53</cell>
              <cell>69.68</cell>
              <cell>18.79</cell>
              <cell>66.89</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references/>
    <citations/>
  </content>
</document>
