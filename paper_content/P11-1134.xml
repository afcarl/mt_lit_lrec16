<document>
  <filename>P11-1134</filename>
  <authors>
    <author>Povo</author>
    <author>Matteo Negri</author>
    <author>Povo</author>
    <author>Marcello Federico</author>
  </authors>
  <title>Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment Yashar Mehdad FBK irst and Uni. of Trento</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>This paper explores the use of bilingual parallel corpora as a source of lexical knowledge for cross-lingual textual entailment. We claim that, in spite of the inherent difficulties of the task, phrase tables extracted from parallel data allow to capture both lexical relations between single words, and contextual information useful for inference. We experiment with a phrasal matching method in order to: i) build a system portable across languages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This paper explores the use of bilingual parallel corpora as a source of lexical knowledge for cross-lingual textual entailment.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We claim that, in spite of the inherent difficulties of the task, phrase tables extracted from parallel data allow to capture both lexical relations between single words, and contextual information useful for inference.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We experiment with a phrasal matching method in order to: i) build a system portable across languages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Cross-lingual Textual Entailment (CLTE) has been proposed by (Mehdad et al., 2010) as an extension of Textual Entailment (Dagan and Glickman, 2004) that consists in deciding, given two texts T and H in different languages, if the meaning of H can be inferred from the meaning of T. The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level. For instance, the reliance of current monolingual TE systems on lexical resources 1336
(e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic parsers, co-reference resolution tools, temporal expressions recognizers and normalizers) has to confront, at the cross-lingual level, with the limited availability of lexical/semantic resources covering multiple languages, the limited coverage of the existing ones, and the burden of integrating languagespecific components into the same cross-lingual architecture.
As a first step to overcome these problems, (Mehdad et al., 2010) proposes a &#8220;basic solution&#8221;, that brings CLTE back to the monolingual scenario by translating H into the language of T. Despite the advantages in terms of modularity and portability of the architecture, and the promising experimental results, this approach suffers from one main limitation which motivates the investigation on alternative solutions. Decoupling machine translation (MT) and TE, in fact, ties CLTE performance to the availability of MT components, and to the quality of the translations. As a consequence, on one side translation errors propagate to the TE engine hampering the entailment decision process. On the other side such unpredictable errors reduce the possibility to control the behaviour of the engine, and devise adhoc solutions to specific entailment problems.
This paper investigates the idea, still unexplored, of a tighter integration of MT and TE algorithms and techniques. Our aim is to embed cross-lingual processing techniques inside the TE recognition process in order to avoid any dependency on external MT components, and eventually gain full control of the system&#8217;s behaviour. Along this direction, we
start from the acquisition and use of lexical knowledge, which represents the basic building block of any TE system. Using the basic solution proposed by (Mehdad et al., 2010) as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address the following questions: (1) What is the potential of the existing multilingual lexical resources to approach CLTE? To answer this question we experiment with lexical knowledge extracted from bilingual dictionaries, and from a multilingual lexical database. Such experiments show two main limitations of these resources, namely: i) their limited coverage, and ii) the difficulty to capture contextual information when only associations between single words (or at most named entities and multiword expressions) are used to support inference. (2) Does MT provide useful resources or techniques to overcome the limitations of existing resources? We envisage several directions in which inputs from MT research may enable or improve CLTE. As regards the resources, phrase and paraphrase tables extracted from bilingual parallel corpora can be exploited as an effective way to capture both lexical relations between single words, and contextual information useful for inference. As regards the algorithms, statistical models based on cooccurrence observations, similar to those used in MT to estimate translation probabilities, may contribute to estimate entailment probabilities in CLTE. Focusing on the resources direction, the main contribution of this paper is to show that the lexical knowledge extracted from parallel corpora allows to significantly improve the results achieved with other multilingual resources. (3) In the cross-lingual scenario, can we achieve results comparable to those obtained in monolingual TE? Our experiments show that, although CLTE seems intrinsically more difficult, the results obtained using phrase and paraphrase tables are better than those achieved by average systems on monolingual datasets. We argue that this is due to the fact that parallel corpora are a rich source of crosslingual paraphrases with no equivalents in monolingual TE. (4) Can parallel corpora be useful also for monolingual TE? To answer this question, we experiment 1337
on monolingual RTE datasets using paraphrase tables extracted from bilingual parallel corpora. Our results improve those achieved with the most widely used resources in monolingual TE, namely Word- Net, Verbocean, and Wikipedia. The remainder of this paper is structured as follows. Section 2 shortly overviews the role of lexical knowledge in textual entailment, highlighting a gap between TE and CLTE in terms of available knowledge sources. Sections 3 and 4 address the first three questions, giving motivations for the use of bilingual parallel corpora in CLTE, and showing the results of our experiments. Section 5 addresses the last question, reporting on our experiments with paraphrase tables extracted from phrase tables on the monolingual RTE datasets. Section 6 concludes the paper, and outlines the directions of our future research.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Cross-lingual Textual Entailment (CLTE) has been proposed by (Mehdad et al., 2010) as an extension of Textual Entailment (Dagan and Glickman, 2004) that consists in deciding, given two texts T and H in different languages, if the meaning of H can be inferred from the meaning of T.</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level.</text>
              <doc_id>6</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For instance, the reliance of current monolingual TE systems on lexical resources 1336</text>
              <doc_id>7</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic parsers, co-reference resolution tools, temporal expressions recognizers and normalizers) has to confront, at the cross-lingual level, with the limited availability of lexical/semantic resources covering multiple languages, the limited coverage of the existing ones, and the burden of integrating languagespecific components into the same cross-lingual architecture.</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>As a first step to overcome these problems, (Mehdad et al., 2010) proposes a &#8220;basic solution&#8221;, that brings CLTE back to the monolingual scenario by translating H into the language of T.</text>
              <doc_id>9</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Despite the advantages in terms of modularity and portability of the architecture, and the promising experimental results, this approach suffers from one main limitation which motivates the investigation on alternative solutions.</text>
              <doc_id>10</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Decoupling machine translation (MT) and TE, in fact, ties CLTE performance to the availability of MT components, and to the quality of the translations.</text>
              <doc_id>11</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As a consequence, on one side translation errors propagate to the TE engine hampering the entailment decision process.</text>
              <doc_id>12</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>On the other side such unpredictable errors reduce the possibility to control the behaviour of the engine, and devise adhoc solutions to specific entailment problems.</text>
              <doc_id>13</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>This paper investigates the idea, still unexplored, of a tighter integration of MT and TE algorithms and techniques.</text>
              <doc_id>14</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our aim is to embed cross-lingual processing techniques inside the TE recognition process in order to avoid any dependency on external MT components, and eventually gain full control of the system&#8217;s behaviour.</text>
              <doc_id>15</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Along this direction, we</text>
              <doc_id>16</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>start from the acquisition and use of lexical knowledge, which represents the basic building block of any TE system.</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Using the basic solution proposed by (Mehdad et al., 2010) as a term of comparison, we experiment with different sources of multilingual lexical knowledge to address the following questions: (1) What is the potential of the existing multilingual lexical resources to approach CLTE?</text>
              <doc_id>18</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>To answer this question we experiment with lexical knowledge extracted from bilingual dictionaries, and from a multilingual lexical database.</text>
              <doc_id>19</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Such experiments show two main limitations of these resources, namely: i) their limited coverage, and ii) the difficulty to capture contextual information when only associations between single words (or at most named entities and multiword expressions) are used to support inference.</text>
              <doc_id>20</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>(2) Does MT provide useful resources or techniques to overcome the limitations of existing resources?</text>
              <doc_id>21</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We envisage several directions in which inputs from MT research may enable or improve CLTE.</text>
              <doc_id>22</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>As regards the resources, phrase and paraphrase tables extracted from bilingual parallel corpora can be exploited as an effective way to capture both lexical relations between single words, and contextual information useful for inference.</text>
              <doc_id>23</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>As regards the algorithms, statistical models based on cooccurrence observations, similar to those used in MT to estimate translation probabilities, may contribute to estimate entailment probabilities in CLTE.</text>
              <doc_id>24</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Focusing on the resources direction, the main contribution of this paper is to show that the lexical knowledge extracted from parallel corpora allows to significantly improve the results achieved with other multilingual resources.</text>
              <doc_id>25</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>(3) In the cross-lingual scenario, can we achieve results comparable to those obtained in monolingual TE?</text>
              <doc_id>26</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Our experiments show that, although CLTE seems intrinsically more difficult, the results obtained using phrase and paraphrase tables are better than those achieved by average systems on monolingual datasets.</text>
              <doc_id>27</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>We argue that this is due to the fact that parallel corpora are a rich source of crosslingual paraphrases with no equivalents in monolingual TE.</text>
              <doc_id>28</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>(4) Can parallel corpora be useful also for monolingual TE?</text>
              <doc_id>29</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>To answer this question, we experiment 1337</text>
              <doc_id>30</doc_id>
              <sec_id>13</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>on monolingual RTE datasets using paraphrase tables extracted from bilingual parallel corpora.</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our results improve those achieved with the most widely used resources in monolingual TE, namely Word- Net, Verbocean, and Wikipedia.</text>
              <doc_id>32</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The remainder of this paper is structured as follows.</text>
              <doc_id>33</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Section 2 shortly overviews the role of lexical knowledge in textual entailment, highlighting a gap between TE and CLTE in terms of available knowledge sources.</text>
              <doc_id>34</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Sections 3 and 4 address the first three questions, giving motivations for the use of bilingual parallel corpora in CLTE, and showing the results of our experiments.</text>
              <doc_id>35</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Section 5 addresses the last question, reporting on our experiments with paraphrase tables extracted from phrase tables on the monolingual RTE datasets.</text>
              <doc_id>36</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Section 6 concludes the paper, and outlines the directions of our future research.</text>
              <doc_id>37</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Lexical resources for TE and CLTE</title>
        <text>All current approaches to monolingual TE, either syntactically oriented (Rus et al., 2005), or applying logical inference (Tatu and Moldovan, 2005), or adopting transformation-based techniques (Kouleykov and Magnini, 2005; Bar-Haim et al., 2008), incorporate different types of lexical knowledge to support textual inference. Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores. WordNet, the most widely used resource in TE, provides all the three types of information. Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable. Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis. Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009). These include, just to mention the most popular
ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia (Mehdad et al., 2010; Kouylekov et al., 2009). DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses. Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores. Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways. As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others. Some previous works (Bannard and Callison-Burch, 2005; Zhao et al., 2009; Kouylekov et al., 2009) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words.
Addressing CLTE we have to face additional and more problematic issues related to: i) the stronger need of lexical knowledge, and ii) the limited availability of multilingual lexical resources. As regards the first issue, it&#8217;s worth noting that in the monolingual scenario simple &#8220;bag of words&#8221; (or &#8220;bag of n- grams&#8221;) approaches are per se sufficient to achieve results above baseline. In contrast, their application in the cross-lingual setting is not a viable solution due to the impossibility to perform direct lexical matches between texts and hypotheses in different languages. This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap. However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources 1338
are available only for English. Multilingual lexical databases aligned with the English WordNet (e.g. MultiWordNet (Pianta et al., 2002)) have been created for several languages, with different degrees of coverage. As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet&#8217;s synsets, thus making the coverage issue even more problematic than for TE. As regards Wikipedia, the crosslingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE. However, due to their relatively small number (especially for some languages), bilingual lexicons extracted from Wikipedia are still inadequate to provide acceptable coverage. In addition, featuring a bias towards named entities, the information acquired through cross-lingual links can at most complement the lexical knowledge extracted from more generic multilingual resources (e.g bilingual dictionaries).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>All current approaches to monolingual TE, either syntactically oriented (Rus et al., 2005), or applying logical inference (Tatu and Moldovan, 2005), or adopting transformation-based techniques (Kouleykov and Magnini, 2005; Bar-Haim et al., 2008), incorporate different types of lexical knowledge to support textual inference.</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores.</text>
              <doc_id>39</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>WordNet, the most widely used resource in TE, provides all the three types of information.</text>
              <doc_id>40</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each other, thus being interchangeable.</text>
              <doc_id>41</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Hypernymy/hyponymy chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text.</text>
              <doc_id>42</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis.</text>
              <doc_id>43</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009).</text>
              <doc_id>44</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>These include, just to mention the most popular</text>
              <doc_id>45</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia (Mehdad et al., 2010; Kouylekov et al., 2009).</text>
              <doc_id>46</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules.</text>
              <doc_id>47</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates.</text>
              <doc_id>48</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve.</text>
              <doc_id>49</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts and hypotheses.</text>
              <doc_id>50</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores.</text>
              <doc_id>51</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways.</text>
              <doc_id>52</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others.</text>
              <doc_id>53</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Some previous works (Bannard and Callison-Burch, 2005; Zhao et al., 2009; Kouylekov et al., 2009) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words.</text>
              <doc_id>54</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Addressing CLTE we have to face additional and more problematic issues related to: i) the stronger need of lexical knowledge, and ii) the limited availability of multilingual lexical resources.</text>
              <doc_id>55</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As regards the first issue, it&#8217;s worth noting that in the monolingual scenario simple &#8220;bag of words&#8221; (or &#8220;bag of n- grams&#8221;) approaches are per se sufficient to achieve results above baseline.</text>
              <doc_id>56</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In contrast, their application in the cross-lingual setting is not a viable solution due to the impossibility to perform direct lexical matches between texts and hypotheses in different languages.</text>
              <doc_id>57</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This situation makes the availability of multilingual lexical knowledge a necessary condition to bridge the language gap.</text>
              <doc_id>58</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>However, with the only exceptions represented by WordNet and Wikipedia, most of the aforementioned resources 1338</text>
              <doc_id>59</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>are available only for English.</text>
              <doc_id>60</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Multilingual lexical databases aligned with the English WordNet (e.g. MultiWordNet (Pianta et al., 2002)) have been created for several languages, with different degrees of coverage.</text>
              <doc_id>61</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>As an example, the 57,424 synsets of the Spanish section of MultiWordNet aligned to English cover just around 50% of the WordNet&#8217;s synsets, thus making the coverage issue even more problematic than for TE.</text>
              <doc_id>62</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As regards Wikipedia, the crosslingual links between pages in different languages offer a possibility to extract lexical knowledge useful for CLTE.</text>
              <doc_id>63</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>However, due to their relatively small number (especially for some languages), bilingual lexicons extracted from Wikipedia are still inadequate to provide acceptable coverage.</text>
              <doc_id>64</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In addition, featuring a bias towards named entities, the information acquired through cross-lingual links can at most complement the lexical knowledge extracted from more generic multilingual resources (e.g bilingual dictionaries).</text>
              <doc_id>65</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Using Parallel Corpora for CLTE</title>
        <text>Bilingual parallel corpora represent a possible solution to overcome the inadequacy of the existing resources, and to implement a portable approach for CLTE. To this aim, we exploit parallel data to: i) learn alignment criteria between phrasal elements in different languages, ii) use them to automatically extract lexical knowledge in the form of phrase tables, and iii) use the obtained phrase tables to create monolingual paraphrase tables.
Given a cross-lingual T/H pair (with the text in l 1 and the hypothesis in l 2 ), our approach leverages the vast amount of lexical knowledge provided by phrase and paraphrase tables to map H into T. We perform such mapping with two different methods. The first method uses a single phrase table to directly map phrases extracted from the hypothesis to phrases in the text. In order to improve our system&#8217;s generalization capabilities and increase the coverage, the second method combines the phrase table with two monolingual paraphrase tables (one in l 1 , and one in l 2 ). This allows to:
1. use the paraphrase table in l 2 to find paraphrases of phrases extracted from H;
2. map them to entries in the phrase table, and extract their equivalents in l 1 ;
3. use the paraphrase table in l 1 to find paraphrases of the extracted fragments in l 1 ;
4. map such paraphrases to phrases in T.
With the second method, phrasal matches between the text and the hypothesis are indirectly performed through paraphrases of the phrase table entries.
The final entailment decision for a T/H pair is assigned considering a model learned from the similarity scores based on the identified phrasal matches. In particular, &#8220;YES&#8221; and &#8220;NO&#8221; judgements are assigned considering the proportion of words in the hypothesis that are found also in the text. This way to approximate entailment reflects the intuition that, as a directional relation between the text and the hypothesis, the full content of H has to be found in T.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Bilingual parallel corpora represent a possible solution to overcome the inadequacy of the existing resources, and to implement a portable approach for CLTE.</text>
              <doc_id>66</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To this aim, we exploit parallel data to: i) learn alignment criteria between phrasal elements in different languages, ii) use them to automatically extract lexical knowledge in the form of phrase tables, and iii) use the obtained phrase tables to create monolingual paraphrase tables.</text>
              <doc_id>67</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Given a cross-lingual T/H pair (with the text in l 1 and the hypothesis in l 2 ), our approach leverages the vast amount of lexical knowledge provided by phrase and paraphrase tables to map H into T.</text>
              <doc_id>68</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We perform such mapping with two different methods.</text>
              <doc_id>69</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The first method uses a single phrase table to directly map phrases extracted from the hypothesis to phrases in the text.</text>
              <doc_id>70</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In order to improve our system&#8217;s generalization capabilities and increase the coverage, the second method combines the phrase table with two monolingual paraphrase tables (one in l 1 , and one in l 2 ).</text>
              <doc_id>71</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This allows to:</text>
              <doc_id>72</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1. use the paraphrase table in l 2 to find paraphrases of phrases extracted from H;</text>
              <doc_id>73</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2. map them to entries in the phrase table, and extract their equivalents in l 1 ;</text>
              <doc_id>74</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3. use the paraphrase table in l 1 to find paraphrases of the extracted fragments in l 1 ;</text>
              <doc_id>75</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>4. map such paraphrases to phrases in T.</text>
              <doc_id>76</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>With the second method, phrasal matches between the text and the hypothesis are indirectly performed through paraphrases of the phrase table entries.</text>
              <doc_id>77</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The final entailment decision for a T/H pair is assigned considering a model learned from the similarity scores based on the identified phrasal matches.</text>
              <doc_id>78</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In particular, &#8220;YES&#8221; and &#8220;NO&#8221; judgements are assigned considering the proportion of words in the hypothesis that are found also in the text.</text>
              <doc_id>79</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This way to approximate entailment reflects the intuition that, as a directional relation between the text and the hypothesis, the full content of H has to be found in T.</text>
              <doc_id>80</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Extracting Phrase and Paraphrase Tables</title>
            <text>Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities. They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003). There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 . We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.
Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.
In our work we used available 2 paraphrase databases for English and Spanish which have been extracted using the method previously outlined. Moreover, in order to experiment with different paraphrase sets providing different degrees of coverage and precision, we pruned the main paraphrase table based on the probabilities, associated to its entries, of 0.1, 0.2 and 0.3. The number of phrase pairs extracted varies from 6 million to about 80000, with an average of 3.2 words per phrase.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities.</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003).</text>
                  <doc_id>82</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>There are several methods to build phrase tables.</text>
                  <doc_id>83</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus.</text>
                  <doc_id>84</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 1 .</text>
                  <doc_id>85</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level.</text>
                  <doc_id>86</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007).</text>
                  <doc_id>87</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides.</text>
                  <doc_id>88</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5.</text>
                  <doc_id>89</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>The resulting phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase.</text>
                  <doc_id>90</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities.</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009).</text>
                  <doc_id>92</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005).</text>
                  <doc_id>93</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases.</text>
                  <doc_id>94</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases.</text>
                  <doc_id>95</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In our work we used available 2 paraphrase databases for English and Spanish which have been extracted using the method previously outlined.</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Moreover, in order to experiment with different paraphrase sets providing different degrees of coverage and precision, we pruned the main paraphrase table based on the probabilities, associated to its entries, of 0.1, 0.2 and 0.3.</text>
                  <doc_id>97</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The number of phrase pairs extracted varies from 6 million to about 80000, with an average of 3.2 words per phrase.</text>
                  <doc_id>98</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Phrasal Matching Method</title>
            <text>In order to maximize the usage of lexical knowledge, our entailment decision criterion is based on similarity scores calculated with a phrase-to-phrase matching process.
A phrase in our approach is an n-gram composed of up to 5 consecutive words, excluding punctuation. Entailment decisions are estimated by combining phrasal matching scores (Score n ) calculated for each level of n-grams , which is the number of 1-grams, 2-grams,..., 5-grams extracted from H that match with n-grams in T. Phrasal matches are performed either at the level of tokens, lemmas, or stems, can be of two types:
1 http://www.statmt.org/wmt10/
2 http://www.cs.cmu.edu/ alavie/METEOR
1. Exact: in the case that two phrases are identical at one of the three levels (token, lemma, stem);
2. Lexical: in the case that two different phrases can be mapped through entries of the resources used to bridge T and H (i.e. phrase tables, paraphrases tables, dictionaries or any other source of lexical knowledge).
For each phrase in H, we first search for exact matches at the level of token with phrases in T. If no match is found at a token level, the other levels (lemma and stem) are attempted. Then, in case of failure with exact matching, lexical matching is performed at the same three levels. To reduce redundant matches, the lexical matches between pairs of phrases which have already been identified as exact matches are not considered. Once matching for each n-gram level has been concluded, the number of matches (M n ) and the number of phrases in the hypothesis (Nn) are used to estimate the portion of phrases in H that are matched at each level (n). The phrasal matching score for each n-gram level is calculated as follows:
Score n = M n Nn
To combine the phrasal matching scores obtained at each n-gram level, and optimize their relative weights, we trained a Support Vector Machine classifier, SVMlight (Joachims, 1999), using each score as a feature.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In order to maximize the usage of lexical knowledge, our entailment decision criterion is based on similarity scores calculated with a phrase-to-phrase matching process.</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A phrase in our approach is an n-gram composed of up to 5 consecutive words, excluding punctuation.</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Entailment decisions are estimated by combining phrasal matching scores (Score n ) calculated for each level of n-grams , which is the number of 1-grams, 2-grams,..., 5-grams extracted from H that match with n-grams in T.</text>
                  <doc_id>101</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Phrasal matches are performed either at the level of tokens, lemmas, or stems, can be of two types:</text>
                  <doc_id>102</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 http://www.statmt.org/wmt10/</text>
                  <doc_id>103</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 http://www.cs.cmu.edu/ alavie/METEOR</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1.</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Exact: in the case that two phrases are identical at one of the three levels (token, lemma, stem);</text>
                  <doc_id>106</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Lexical: in the case that two different phrases can be mapped through entries of the resources used to bridge T and H (i.e. phrase tables, paraphrases tables, dictionaries or any other source of lexical knowledge).</text>
                  <doc_id>108</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For each phrase in H, we first search for exact matches at the level of token with phrases in T.</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If no match is found at a token level, the other levels (lemma and stem) are attempted.</text>
                  <doc_id>110</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Then, in case of failure with exact matching, lexical matching is performed at the same three levels.</text>
                  <doc_id>111</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>To reduce redundant matches, the lexical matches between pairs of phrases which have already been identified as exact matches are not considered.</text>
                  <doc_id>112</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Once matching for each n-gram level has been concluded, the number of matches (M n ) and the number of phrases in the hypothesis (Nn) are used to estimate the portion of phrases in H that are matched at each level (n).</text>
                  <doc_id>113</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The phrasal matching score for each n-gram level is calculated as follows:</text>
                  <doc_id>114</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Score n = M n Nn</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To combine the phrasal matching scores obtained at each n-gram level, and optimize their relative weights, we trained a Support Vector Machine classifier, SVMlight (Joachims, 1999), using each score as a feature.</text>
                  <doc_id>116</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Experiments on CLTE</title>
        <text>To address the first two questions outlined in Section 1, we experimented with the phrase matching method previously described, contrasting the effectiveness of lexical information extracted from parallel corpora with the knowledge provided by other resources used in the same way.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>To address the first two questions outlined in Section 1, we experimented with the phrase matching method previously described, contrasting the effectiveness of lexical information extracted from parallel corpora with the knowledge provided by other resources used in the same way.</text>
              <doc_id>117</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Dataset</title>
            <text>The dataset used for our experiments is an English- Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish. It consists of 1600 pairs derived from the RTE3 development and test sets (800+800). Translations have been generated by 1340
the CrowdFlower 3 channel to Amazon Mechanical Turk 4 (MTurk), adopting the methodology proposed by (Negri and Mehdad, 2010). The method relies on translation-validation cycles, defined as separate jobs routed to MTurk&#8217;s workforce. Translation jobs return one Spanish version for each hypothesis. Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference. At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job. Although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired CLTE corpus. The validation, carried out by a Spanish native speaker on 100 randomly selected pairs after two translation-validation cycles, showed the good quality of the collected material, with only 3 minor &#8220;errors&#8221; consisting in controversial but substantially acceptable translations reflecting regional Spanish variations. The T-H pairs in the collected English-Spanish entailment corpus were annotated using TreeTagger (Schmid, 1994) and the Snowball stemmer 6 with token, lemma, and stem information.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The dataset used for our experiments is an English- Spanish entailment corpus obtained from the original RTE3 dataset by translating the English hypothesis into Spanish.</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It consists of 1600 pairs derived from the RTE3 development and test sets (800+800).</text>
                  <doc_id>119</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Translations have been generated by 1340</text>
                  <doc_id>120</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the CrowdFlower 3 channel to Amazon Mechanical Turk 4 (MTurk), adopting the methodology proposed by (Negri and Mehdad, 2010).</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The method relies on translation-validation cycles, defined as separate jobs routed to MTurk&#8217;s workforce.</text>
                  <doc_id>122</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Translation jobs return one Spanish version for each hypothesis.</text>
                  <doc_id>123</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference.</text>
                  <doc_id>124</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>At each cycle, the translated hypothesis accepted by the majority of trustful validators 5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job.</text>
                  <doc_id>125</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Although the quality of the results is enhanced by the possibility to automatically weed out untrusted workers using gold units, we performed a manual quality check on a subset of the acquired CLTE corpus.</text>
                  <doc_id>126</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The validation, carried out by a Spanish native speaker on 100 randomly selected pairs after two translation-validation cycles, showed the good quality of the collected material, with only 3 minor &#8220;errors&#8221; consisting in controversial but substantially acceptable translations reflecting regional Spanish variations.</text>
                  <doc_id>127</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>The T-H pairs in the collected English-Spanish entailment corpus were annotated using TreeTagger (Schmid, 1994) and the Snowball stemmer 6 with token, lemma, and stem information.</text>
                  <doc_id>128</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Knowledge sources</title>
            <text>For comparison with the extracted phrase and paraphrase tables, we use a large bilingual dictionary and MultiWordNet as alternative sources of lexical knowledge.
Bilingual dictionaries (DIC) allow for precise mappings between words in H and T. To create a large bilingual English-Spanish dictionary we processed and combined the following dictionaries and bilingual resources: - XDXF Dictionaries 7 : 22,486 entries.
3 http://crowdflower.com/ 4 https://www.mturk.com/mturk/ 5 Workers&#8217; trustworthiness can be automatically determined
by means of hidden gold units randomly inserted into jobs. 6 http://snowball.tartarus.org/
7 http://xdxf.revdanica.com/
- Universal dictionary database 8 : 9,944 entries. - Wiktionary database 9 : 5,866 entries. - Omegawiki database 10 : 8,237 entries. - Wikipedia interlanguage links 11 : 7,425 entries. The resulting dictionary features 53,958 entries, with an average length of 1.2 words.
MultiWordNet (MWN) allows to extract mappings between English and Spanish words connected by entailment-preserving semantic relations. The extraction process is dataset-dependent, as it checks for synonymy and hyponymy relations only between terms found in the dataset. The resulting collection of cross-lingual words associations contains 36,794 pairs of lemmas.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>For comparison with the extracted phrase and paraphrase tables, we use a large bilingual dictionary and MultiWordNet as alternative sources of lexical knowledge.</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Bilingual dictionaries (DIC) allow for precise mappings between words in H and T.</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To create a large bilingual English-Spanish dictionary we processed and combined the following dictionaries and bilingual resources: - XDXF Dictionaries 7 : 22,486 entries.</text>
                  <doc_id>131</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3 http://crowdflower.com/ 4 https://www.mturk.com/mturk/ 5 Workers&#8217; trustworthiness can be automatically determined</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>by means of hidden gold units randomly inserted into jobs.</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>6 http://snowball.tartarus.org/</text>
                  <doc_id>134</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>7 http://xdxf.revdanica.com/</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>- Universal dictionary database 8 : 9,944 entries.</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>- Wiktionary database 9 : 5,866 entries.</text>
                  <doc_id>137</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>- Omegawiki database 10 : 8,237 entries.</text>
                  <doc_id>138</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>- Wikipedia interlanguage links 11 : 7,425 entries.</text>
                  <doc_id>139</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The resulting dictionary features 53,958 entries, with an average length of 1.2 words.</text>
                  <doc_id>140</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>MultiWordNet (MWN) allows to extract mappings between English and Spanish words connected by entailment-preserving semantic relations.</text>
                  <doc_id>141</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The extraction process is dataset-dependent, as it checks for synonymy and hyponymy relations only between terms found in the dataset.</text>
                  <doc_id>142</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The resulting collection of cross-lingual words associations contains 36,794 pairs of lemmas.</text>
                  <doc_id>143</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>4.3 Results and Discussion</title>
            <text>Our results are calculated over 800 test pairs of our CLTE corpus, after training the SVM classifier over 800 development pairs. This section reports the percentage of correct entailment assignments (accuracy), comparing the use of different sources of lexical knowledge.
Initially, in order to find a reasonable trade-off between precision and coverage, we used the 7 phrase tables extracted with different pruning thresholds
8 http://www.dicts.info/ 9 http://en.wiktionary.org/ 10 http://www.omegawiki.org/ 11 http://www.wikipedia.org/
(see Section 3.1). Figure 1 shows that with the pruning threshold set to 0.05, we obtain the highest result of 62.62% on the test set. The curve demonstrates that, although with higher pruning thresholds we retain more reliable phrase pairs, their smaller number provides limited coverage leading to lower results. In contrast, the large coverage obtained with the pruning threshold set to 0.01 leads to a slight performance decrease due to probably less precise phrase pairs.
Once the threshold has been set, in order to prove the effectiveness of information extracted from bilingual corpora, we conducted a series of experiments using the different resources mentioned in Section 4.2.
As it can be observed in Table 1, the highest results are achieved using the phrase table, both alone and in combination with paraphrase tables (62.62% and 62.88% respectively). These results suggest that, with appropriate pruning thresholds, the large number and the longer entries contained in the phrase and paraphrase tables represent an effective way to: i) obtain high coverage, and ii) capture cross-lingual associations between multiple lexical elements. This allows to overcome the bias towards single words featured by dictionaries and lexical databases.
As regards the other resources used for comparison, the results show that dictionaries substantially outperform MWN. This can be explained by the low coverage of MWN, whose entries also represent weaker semantic relations (preserving entailment, but with a lower probability to be applied) than the direct translations between terms contained in the dictionary.
Overall, our results suggest that the lexical knowledge extracted from parallel data can be successfully used to approach the CLTE task.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Our results are calculated over 800 test pairs of our CLTE corpus, after training the SVM classifier over 800 development pairs.</text>
                  <doc_id>144</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This section reports the percentage of correct entailment assignments (accuracy), comparing the use of different sources of lexical knowledge.</text>
                  <doc_id>145</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Initially, in order to find a reasonable trade-off between precision and coverage, we used the 7 phrase tables extracted with different pruning thresholds</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>8 http://www.dicts.info/ 9 http://en.wiktionary.org/ 10 http://www.omegawiki.org/ 11 http://www.wikipedia.org/</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(see Section 3.1).</text>
                  <doc_id>148</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 1 shows that with the pruning threshold set to 0.05, we obtain the highest result of 62.62% on the test set.</text>
                  <doc_id>149</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The curve demonstrates that, although with higher pruning thresholds we retain more reliable phrase pairs, their smaller number provides limited coverage leading to lower results.</text>
                  <doc_id>150</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast, the large coverage obtained with the pruning threshold set to 0.01 leads to a slight performance decrease due to probably less precise phrase pairs.</text>
                  <doc_id>151</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Once the threshold has been set, in order to prove the effectiveness of information extracted from bilingual corpora, we conducted a series of experiments using the different resources mentioned in Section 4.2.</text>
                  <doc_id>152</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As it can be observed in Table 1, the highest results are achieved using the phrase table, both alone and in combination with paraphrase tables (62.62% and 62.88% respectively).</text>
                  <doc_id>153</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These results suggest that, with appropriate pruning thresholds, the large number and the longer entries contained in the phrase and paraphrase tables represent an effective way to: i) obtain high coverage, and ii) capture cross-lingual associations between multiple lexical elements.</text>
                  <doc_id>154</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This allows to overcome the bias towards single words featured by dictionaries and lexical databases.</text>
                  <doc_id>155</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As regards the other resources used for comparison, the results show that dictionaries substantially outperform MWN.</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This can be explained by the low coverage of MWN, whose entries also represent weaker semantic relations (preserving entailment, but with a lower probability to be applied) than the direct translations between terms contained in the dictionary.</text>
                  <doc_id>157</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Overall, our results suggest that the lexical knowledge extracted from parallel data can be successfully used to approach the CLTE task.</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Using parallel corpora for TE</title>
        <text>This section addresses the third and the fourth research questions outlined in Section 1. Building on the positive results achieved on the cross-lingual scenario, we investigate the possibility to exploit bilingual parallel corpora in the traditional monolingual scenario. Using the same approach discussed in Section 4, we compare the results achieved with English paraphrase tables with those obtained with other widely used monolingual knowledge resources over two RTE datasets.
For the sake of completeness, we report in this section also the results obtained adopting the &#8220;basic solution&#8221; proposed by (Mehdad et al., 2010). Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T. The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This section addresses the third and the fourth research questions outlined in Section 1. Building on the positive results achieved on the cross-lingual scenario, we investigate the possibility to exploit bilingual parallel corpora in the traditional monolingual scenario.</text>
              <doc_id>159</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Using the same approach discussed in Section 4, we compare the results achieved with English paraphrase tables with those obtained with other widely used monolingual knowledge resources over two RTE datasets.</text>
              <doc_id>160</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For the sake of completeness, we report in this section also the results obtained adopting the &#8220;basic solution&#8221; proposed by (Mehdad et al., 2010).</text>
              <doc_id>161</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Although it was presented as an approach to CLTE, the proposed method brings the problem back to the monolingual case by translating H into the language of T.</text>
              <doc_id>162</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The comparison with this method aims at verifying the real potential of parallel corpora against the use of a competitive MT system (Google Translate) in the same scenario.</text>
              <doc_id>163</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Dataset</title>
            <text>We experiment with the original RTE3 and RTE5 datasets, annotated with token, lemma, and stem information using the TreeTagger and the Snowball stemmer.
In addition to confront our method with the solution proposed by (Mehdad et al., 2010) we translated the Spanish hypotheses of our CLTE dataset into English using Google Translate. The resulting dataset was annotated in the same way.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We experiment with the original RTE3 and RTE5 datasets, annotated with token, lemma, and stem information using the TreeTagger and the Snowball stemmer.</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In addition to confront our method with the solution proposed by (Mehdad et al., 2010) we translated the Spanish hypotheses of our CLTE dataset into English using Google Translate.</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The resulting dataset was annotated in the same way.</text>
                  <doc_id>166</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Knowledge sources</title>
            <text>We compared the results achieved with paraphrase tables (extracted with different pruning thresholds 12 ) with those obtained using the three most
12 We pruned the paraphrase table (PPHT), with probabilities
set to 0.1 (PPHT 0.1), 0.2 (PPHT 0.2), and 0.3 (PPHT 0.3)
widely used English resources for Textual Entailment (Bentivogli et al., 2010), namely:
WordNet (WN). WordNet 3.0 has been used to extract a set of 5396 pairs of words connected by the hyponymy and synonymy relations.
VerbOcean (VO). VerbOcean has been used to extract 18232 pairs of verbs connected by the &#8220;stronger-than&#8221; relation (e.g. &#8220;kill&#8221; stronger-than &#8220;injure&#8221;).
Wikipedia (WIKI). We performed Latent Semantic Analysis (LSA) over Wikipedia using the jLSI tool (Giuliano, 2007) to measure the relatedness between words in the dataset. Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009). In this way we obtained 13760 word pairs.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We compared the results achieved with paraphrase tables (extracted with different pruning thresholds 12 ) with those obtained using the three most</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>12 We pruned the paraphrase table (PPHT), with probabilities</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>set to 0.1 (PPHT 0.1), 0.2 (PPHT 0.2), and 0.3 (PPHT 0.3)</text>
                  <doc_id>169</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>widely used English resources for Textual Entailment (Bentivogli et al., 2010), namely:</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>WordNet (WN).</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>WordNet 3.0 has been used to extract a set of 5396 pairs of words connected by the hyponymy and synonymy relations.</text>
                  <doc_id>172</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VerbOcean (VO).</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>VerbOcean has been used to extract 18232 pairs of verbs connected by the &#8220;stronger-than&#8221; relation (e.g. &#8220;kill&#8221; stronger-than &#8220;injure&#8221;).</text>
                  <doc_id>174</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Wikipedia (WIKI).</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We performed Latent Semantic Analysis (LSA) over Wikipedia using the jLSI tool (Giuliano, 2007) to measure the relatedness between words in the dataset.</text>
                  <doc_id>176</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Then, we filtered all the pairs with similarity lower than 0.7 as proposed by (Kouylekov et al., 2009).</text>
                  <doc_id>177</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In this way we obtained 13760 word pairs.</text>
                  <doc_id>178</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>5.3 Results and Discussion</title>
            <text>Table 2 shows the accuracy results calculated over the original RTE3 and RTE5 test sets, training our classifier over the corresponding development sets.
The first two rows of the table show that pruned paraphrase tables always outperform the other lexical resources used for comparison, with an accuracy increase up to 3%. In particular, we observe that using 0.2 as a pruning threshold provides a good tradeoff between coverage and precision, leading to our best results on both datasets (63.50% for RTE3, and 62.67% for RTE5). It&#8217;s worth noting that these results, compared with the average scores reported by participants in the two editions of the RTE Challenge (AVG column), represent an accuracy improvement of more than 1%. Overall, these results confirm our claim that increasing the coverage using context sensitive phrase pairs obtained from large parallel corpora, results in better performance not only in CLTE,
but also in the monolingual scenario.
The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations. First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%. This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table. Second, in line with the findings of (Mehdad et al., 2010), the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset (i.e. 63.50%). Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%). In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation. Finally, it&#8217;s worth remarking that applying our phrase matching method on the translated dataset without any additional source of knowledge would result in an overall accuracy of 62.12%, which is lower than the result obtained using only phrase tables on cross-lingual data (62.62%). This demonstrates that phrase tables can successfully replace MT systems in the CLTE task. In light of this, we suggest that extracting lexical knowledge from parallel corpora is a preferable solution to approach CLTE. One of the main reasons is that placing a black-box MT system at the front-end of the entailment process reduces the possibility to cope with wrong translations. Furthermore, the access to MT components is not easy (e.g. Google Translate limits the number and the size of queries, while open source MT tools cover few language pairs). Moreover, the task of developing a full-fledged MT system often requires the availability of parallel corpora, and is much more complex than extracting lexical knowledge from them.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Table 2 shows the accuracy results calculated over the original RTE3 and RTE5 test sets, training our classifier over the corresponding development sets.</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The first two rows of the table show that pruned paraphrase tables always outperform the other lexical resources used for comparison, with an accuracy increase up to 3%.</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In particular, we observe that using 0.2 as a pruning threshold provides a good tradeoff between coverage and precision, leading to our best results on both datasets (63.50% for RTE3, and 62.67% for RTE5).</text>
                  <doc_id>181</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It&#8217;s worth noting that these results, compared with the average scores reported by participants in the two editions of the RTE Challenge (AVG column), represent an accuracy improvement of more than 1%.</text>
                  <doc_id>182</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Overall, these results confirm our claim that increasing the coverage using context sensitive phrase pairs obtained from large parallel corpora, results in better performance not only in CLTE,</text>
                  <doc_id>183</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>but also in the monolingual scenario.</text>
                  <doc_id>184</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The comparison with the results achieved on monolingual data obtained by automatically translating the Spanish hypotheses (RTE3-G row in Table 2) leads to four main observations.</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>First, we notice that dealing with MT-derived inputs, the optimal pruning threshold changes from 0.2 to 0.1, leading to the highest accuracy of 63.50%.</text>
                  <doc_id>186</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This suggests that the noise introduced by incorrect translations can be tackled by increasing the coverage of the paraphrase table.</text>
                  <doc_id>187</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Second, in line with the findings of (Mehdad et al., 2010), the results obtained over the MT-derived corpus are equal to those we achieve over the original RTE3 dataset (i.e. 63.50%).</text>
                  <doc_id>188</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Third, the accuracy obtained over the CLTE corpus using combined phrase and paraphrase tables (62.88%, as reported in Table 1) is comparable to the best result gained over the automatically translated dataset (63.50%).</text>
                  <doc_id>189</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In all the other cases, the use of phrase and paraphrase tables on CLTE data outperforms the results achieved on the same data after translation.</text>
                  <doc_id>190</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, it&#8217;s worth remarking that applying our phrase matching method on the translated dataset without any additional source of knowledge would result in an overall accuracy of 62.12%, which is lower than the result obtained using only phrase tables on cross-lingual data (62.62%).</text>
                  <doc_id>191</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>This demonstrates that phrase tables can successfully replace MT systems in the CLTE task.</text>
                  <doc_id>192</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>In light of this, we suggest that extracting lexical knowledge from parallel corpora is a preferable solution to approach CLTE.</text>
                  <doc_id>193</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>One of the main reasons is that placing a black-box MT system at the front-end of the entailment process reduces the possibility to cope with wrong translations.</text>
                  <doc_id>194</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>Furthermore, the access to MT components is not easy (e.g. Google Translate limits the number and the size of queries, while open source MT tools cover few language pairs).</text>
                  <doc_id>195</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>Moreover, the task of developing a full-fledged MT system often requires the availability of parallel corpora, and is much more complex than extracting lexical knowledge from them.</text>
                  <doc_id>196</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>6</index>
        <title>6 Conclusion and Future Work</title>
        <text>In this paper we approached the cross-lingual Textual Entailment task focusing on the role of lexical knowledge extracted from bilingual parallel corpora. One of the main difficulties in CLTE raises from the lack of adequate knowledge resources to bridge the lexical gap between texts and hypotheses in different languages. Our approach builds on the intuition that the vast amount of knowledge that can be extracted from parallel data (in the form of phrase and paraphrase tables) offers a possible solution to the problem. To check the validity of our assumptions we carried out several experiments on an English-Spanish corpus derived from the RTE3 dataset, using phrasal matches as a criterion to approximate entailment. Our results show that phrase and paraphrase tables allow to: i) outperform the results achieved with the few multilingual lexical resources available, and ii) reach performance levels above the average scores obtained by participants in the monolingual RTE3 challenge. These improvements can be explained by the fact that the lexical knowledge extracted from parallel data provides good coverage both at the level of single words, and at the level of phrases. As a further contribution, we explored the application of paraphrase tables extracted from parallel data in the traditional monolingual scenario. Contrasting results with those obtained with the most widely used resources in TE, we demonstrated the effectiveness of paraphrase tables as a mean to overcome the bias towards single words featured by the existing resources. Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE. On one side, we plan to explore alternative ways to build phrase and paraphrase tables. One possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by (Yamada and Knight, 2001). Another interesting direction is to investigate the potential of paraphrase patterns (i.e. patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009). On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge. As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment. Acknowledgments This work has been partially supported by the ECfunded project CoSyne (FP7-ICT-4-24853).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper we approached the cross-lingual Textual Entailment task focusing on the role of lexical knowledge extracted from bilingual parallel corpora.</text>
              <doc_id>197</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>One of the main difficulties in CLTE raises from the lack of adequate knowledge resources to bridge the lexical gap between texts and hypotheses in different languages.</text>
              <doc_id>198</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Our approach builds on the intuition that the vast amount of knowledge that can be extracted from parallel data (in the form of phrase and paraphrase tables) offers a possible solution to the problem.</text>
              <doc_id>199</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>To check the validity of our assumptions we carried out several experiments on an English-Spanish corpus derived from the RTE3 dataset, using phrasal matches as a criterion to approximate entailment.</text>
              <doc_id>200</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Our results show that phrase and paraphrase tables allow to: i) outperform the results achieved with the few multilingual lexical resources available, and ii) reach performance levels above the average scores obtained by participants in the monolingual RTE3 challenge.</text>
              <doc_id>201</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>These improvements can be explained by the fact that the lexical knowledge extracted from parallel data provides good coverage both at the level of single words, and at the level of phrases.</text>
              <doc_id>202</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>As a further contribution, we explored the application of paraphrase tables extracted from parallel data in the traditional monolingual scenario.</text>
              <doc_id>203</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Contrasting results with those obtained with the most widely used resources in TE, we demonstrated the effectiveness of paraphrase tables as a mean to overcome the bias towards single words featured by the existing resources.</text>
              <doc_id>204</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE.</text>
              <doc_id>205</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>On one side, we plan to explore alternative ways to build phrase and paraphrase tables.</text>
              <doc_id>206</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>One possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by (Yamada and Knight, 2001).</text>
              <doc_id>207</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>Another interesting direction is to investigate the potential of paraphrase patterns (i.e. patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009).</text>
              <doc_id>208</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge.</text>
              <doc_id>209</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment.</text>
              <doc_id>210</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>Acknowledgments This work has been partially supported by the ECfunded project CoSyne (FP7-ICT-4-24853).</text>
              <doc_id>211</doc_id>
              <sec_id>14</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Accuracy results on CLTE using different lexical resources.</caption>
        <reference_text></reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>MWN</cell>
              <cell>DIC</cell>
              <cell>PHT</cell>
              <cell>PPHT</cell>
              <cell>Acc.</cell>
              <cell>&#948;</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>x</cell>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell>55.00</cell>
              <cell>0.00</cell>
            </row>
            <row>
              <cell></cell>
              <cell>x</cell>
              <cell></cell>
              <cell></cell>
              <cell>59.88</cell>
              <cell>+4.88</cell>
            </row>
            <row>
              <cell></cell>
              <cell></cell>
              <cell>x</cell>
              <cell></cell>
              <cell>62.62</cell>
              <cell>+7.62</cell>
            </row>
            <row>
              <cell></cell>
              <cell></cell>
              <cell>x</cell>
              <cell>x</cell>
              <cell>62.88</cell>
              <cell>+7.88</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Accuracy results on monolingual RTE using different lexical resources.</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Dataset</cell>
              <cell>WN</cell>
              <cell>VO</cell>
              <cell>WIKI</cell>
              <cell>PPHT</cell>
              <cell>PPHT 0.1</cell>
              <cell>PPHT 0.2</cell>
              <cell>PPHT 0.3</cell>
              <cell>AVG</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>RTE3</cell>
              <cell>61.88</cell>
              <cell>62.00</cell>
              <cell>61.75</cell>
              <cell>62.88</cell>
              <cell>63.38</cell>
              <cell>63.50</cell>
              <cell>63.00</cell>
              <cell>62.37</cell>
            </row>
            <row>
              <cell>RTE5</cell>
              <cell>62.17</cell>
              <cell>61.67</cell>
              <cell>60.00</cell>
              <cell>61.33</cell>
              <cell>62.50</cell>
              <cell>62.67</cell>
              <cell>62.33</cell>
              <cell>61.41</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Collin F Baker</author>
          <author>Charles J Fillmore</author>
          <author>John B Lowe</author>
        </authors>
        <title>The Berkeley FrameNet project.</title>
        <publication>Proceedings of COLING-ACL.</publication>
        <pages>None</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Colin Bannard</author>
          <author>Chris Callison-Burch</author>
        </authors>
        <title>Paraphrasing with Bilingual Parallel Corpora.</title>
        <publication>Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Iddo Greental</author>
        </authors>
        <title>Efficient semantic deduction and approximate matching over compact parse forests.</title>
        <publication>Proceedings of the TAC 2008 Workshop on Textual Entailment.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Luisa Bentivogli</author>
        </authors>
        <title>Peter Clark, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>4</id>
        <authors/>
        <title>None</title>
        <publication>Proceedings of the the Text Analysis Conference (TAC</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Timothy Chklovski</author>
          <author>Patrick Pantel</author>
        </authors>
        <title>Verbocean: Mining the web for fine-grained semantic verb relations.</title>
        <publication>Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-04).</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Ido Dagan</author>
          <author>Oren Glickman</author>
        </authors>
        <title>Probabilistic textual entailment: Generic applied modeling of language variability.</title>
        <publication>Proceedings of the PASCAL Workshop of Learning Methods for Text Understanding and Mining.</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Ido Dagan</author>
          <author>Bill Dolan</author>
          <author>Bernardo Magnini</author>
          <author>Dan Roth</author>
        </authors>
        <title>Recognizing textual entailment: Rational, evaluation and approaches.</title>
        <publication>None</publication>
        <pages>pp i-xvii.</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Michael Denkowski</author>
          <author>Alon Lavie</author>
        </authors>
        <title>Extending the METEOR Machine Translation Evaluation Metric to the Phrase Level.</title>
        <publication>Proceedings of Human Language Technologies (HLT-NAACL</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Georgiana Dinu</author>
          <author>Rui Wang</author>
        </authors>
        <title>Inference Rules and their Application to Recognizing Textual Entailment.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>10</id>
        <authors/>
        <title>None</title>
        <publication>Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Claudio Giuliano</author>
        </authors>
        <title>jLSI a tool for latent semantic indexing. Software available at http://tcc.itc.it/research/textec/toolsresources/jLSI.html.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Lidija Iordanskaja</author>
          <author>Richard Kittredge</author>
          <author>Alain Polg re</author>
        </authors>
        <title>Lexical selection and paraphrase in a meaning text generation model.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1991</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Thorsten Joachims</author>
        </authors>
        <title>Making large-scale support vector machine learning practical.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz Josef Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical Phrase-Based Translation.</title>
        <publication>Proceedings of HLT/NAACL.</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Hieu Hoang</author>
          <author>Alexandra Birch</author>
          <author>Chris Callison-Burch</author>
          <author>Marcello Federico</author>
          <author>Nicola Bertoldi</author>
          <author>Brooke Cowan</author>
          <author>Wade Shen</author>
        </authors>
        <title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
        <publication>Proceedings of the Conference of the Association for Computational Linguistics (ACL).</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Milen Kouleykov</author>
          <author>Bernardo Magnini</author>
        </authors>
        <title>Tree edit distance for textual entailment.</title>
        <publication>Proceedings of RALNP-2005, International Conference on Recent Advances in Natural Language Processing.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Milen Kouylekov</author>
          <author>Yashar Mehdad</author>
          <author>Matteo Negri</author>
        </authors>
        <title>Mining Wikipedia for Large-Scale Repositories of Context-Sensitive Entailment Rules.</title>
        <publication>Proceedings of the Language Resources and Evaluation Conference (LREC</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Yashar Mehdad</author>
          <author>Alessandro Moschitti</author>
          <author>Fabio Massimo Zanzotto</author>
        </authors>
        <title>Syntactic/semantic structures for textual entailment recognition.</title>
        <publication>Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Dekang Lin</author>
          <author>Patrick Pantel</author>
        </authors>
        <title>DIRT - Discovery of Inference Rules from Text..</title>
        <publication>Proceedings of ACM Conference on Knowledge Discovery and Data Mining (KDD-01).</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Kathleen R McKeown</author>
          <author>Regina Barzilay</author>
          <author>David Evans</author>
          <author>Vasileios Hatzivassiloglou</author>
          <author>Judith L Klavans</author>
          <author>Ani Nenkova</author>
          <author>Carl Sable</author>
          <author>Barry Schiffman</author>
          <author>Sergey Sigelman</author>
        </authors>
        <title>Tracking and summarizing news on a daily basis with Columbias Newsblaster.</title>
        <publication>Proceedings of the Human Language Technology Conference..</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Yashar Mehdad</author>
          <author>Matteo Negri</author>
          <author>Marcello Federico</author>
        </authors>
        <title>Towards Cross-Lingual Textual Entailment.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>22</id>
        <authors/>
        <title>None</title>
        <publication>Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Dan Moldovan</author>
          <author>Adrian Novischi</author>
        </authors>
        <title>Lexical chains for question answering.</title>
        <publication>Proceedings of COLING.</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>Matteo Negri</author>
          <author>Yashar Mehdad</author>
        </authors>
        <title>Creating a Bilingual Entailment Corpus through Translations with Mechanical Turk: $100 for a 10-day Rush.</title>
        <publication>Proceedings of the NAACL 2010 Workshop on Creating Speech and Language Data With Amazons Mechanical Turk .</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A systematic comparison of various statistical alignment models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>Luisa Bentivogli Pianta</author>
          <author>Christian Girardi</author>
        </authors>
        <title>MultiWordNet: Developing and Aligned Multilingual Database.</title>
        <publication>Proceedings of the First International Conference on Global WordNet.</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>27</id>
        <authors>
          <author>Vasile Rus</author>
        </authors>
        <title>Art Graesser, and Kirtan Desai</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>28</id>
        <authors/>
        <title>None</title>
        <publication>Proceedings of RANLP</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>29</id>
        <authors>
          <author>Helmut Schmid</author>
        </authors>
        <title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
        <publication>Proceedings of the International Conference on New Methods in Language Processing.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>30</id>
        <authors>
          <author>Marta Tatu andDan Moldovan</author>
        </authors>
        <title>A semantic approach to recognizing textual entailment.</title>
        <publication>Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>31</id>
        <authors>
          <author>Matthew Snover</author>
          <author>Nitin Madnani</author>
          <author>Bonnie Dorr</author>
          <author>Richard Schwartz</author>
        </authors>
        <title>Fluency, Adequacy, or HTER? Exploring Different Human Judgments with a Tunable MT Metric.</title>
        <publication>Proceedings of WMT09.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>32</id>
        <authors>
          <author>Rui Wang</author>
          <author>Yi Zhang</author>
        </authors>
        <title>Recognizing Textual Relatedness with Predicate-Argument Structures.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>33</id>
        <authors/>
        <title>None</title>
        <publication>Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>34</id>
        <authors>
          <author>Kenji Yamada</author>
          <author>Kevin Knight</author>
        </authors>
        <title>A Syntax-Based Statistical Translation Model.</title>
        <publication>Proceedings of the Conference of the Association for Computational Linguistics (ACL).</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Baker et al., 1998</string>
        <sentence_id>33258</sentence_id>
        <char_offset>85</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Bannard and Callison-Burch, 2005</string>
        <sentence_id>33266</sentence_id>
        <char_offset>21</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>Bannard and Callison-Burch, 2005</string>
        <sentence_id>33290</sentence_id>
        <char_offset>140</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>5</reference_id>
        <string>Chklovski and Pantel, 2004</string>
        <sentence_id>33258</sentence_id>
        <char_offset>46</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>6</reference_id>
        <string>Dagan and Glickman, 2004</string>
        <sentence_id>33217</sentence_id>
        <char_offset>122</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>7</reference_id>
        <string>Dagan et al., 2009</string>
        <sentence_id>33256</sentence_id>
        <char_offset>124</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>8</reference_id>
        <string>Denkowski and Lavie, 2010</string>
        <sentence_id>33289</sentence_id>
        <char_offset>201</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>9</reference_id>
        <string>Dinu and Wang, 2009</string>
        <sentence_id>33289</sentence_id>
        <char_offset>237</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>11</reference_id>
        <string>Giuliano, 2007</string>
        <sentence_id>33383</sentence_id>
        <char_offset>80</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>12</reference_id>
        <string>Iordanskaja et al., 1991</string>
        <sentence_id>33289</sentence_id>
        <char_offset>94</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>13</reference_id>
        <string>Joachims, 1999</string>
        <sentence_id>33313</sentence_id>
        <char_offset>165</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>14</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>33279</sentence_id>
        <char_offset>122</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>15</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>33284</sentence_id>
        <char_offset>104</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>16</reference_id>
        <string>Kouleykov and Magnini, 2005</string>
        <sentence_id>33250</sentence_id>
        <char_offset>194</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>18</reference_id>
        <string>Mehdad et al., 2010</string>
        <sentence_id>33217</sentence_id>
        <char_offset>62</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>18</reference_id>
        <string>Mehdad et al., 2010</string>
        <sentence_id>33221</sentence_id>
        <char_offset>45</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>18</reference_id>
        <string>Mehdad et al., 2010</string>
        <sentence_id>33230</sentence_id>
        <char_offset>38</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>18</reference_id>
        <string>Mehdad et al., 2010</string>
        <sentence_id>33258</sentence_id>
        <char_offset>121</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>18</reference_id>
        <string>Mehdad et al., 2010</string>
        <sentence_id>33406</sentence_id>
        <char_offset>125</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>18</reference_id>
        <string>Mehdad et al., 2010</string>
        <sentence_id>33372</sentence_id>
        <char_offset>66</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>18</reference_id>
        <string>Mehdad et al., 2010</string>
        <sentence_id>33395</sentence_id>
        <char_offset>38</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>19</reference_id>
        <string>Lin and Pantel, 2001</string>
        <sentence_id>33258</sentence_id>
        <char_offset>12</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>20</reference_id>
        <string>McKeown et al., 2002</string>
        <sentence_id>33289</sentence_id>
        <char_offset>150</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>21</reference_id>
        <string>Mehdad et al., 2010</string>
        <sentence_id>33217</sentence_id>
        <char_offset>62</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>21</reference_id>
        <string>Mehdad et al., 2010</string>
        <sentence_id>33221</sentence_id>
        <char_offset>45</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>21</reference_id>
        <string>Mehdad et al., 2010</string>
        <sentence_id>33230</sentence_id>
        <char_offset>38</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>21</reference_id>
        <string>Mehdad et al., 2010</string>
        <sentence_id>33258</sentence_id>
        <char_offset>121</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>21</reference_id>
        <string>Mehdad et al., 2010</string>
        <sentence_id>33406</sentence_id>
        <char_offset>125</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>21</reference_id>
        <string>Mehdad et al., 2010</string>
        <sentence_id>33372</sentence_id>
        <char_offset>66</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>21</reference_id>
        <string>Mehdad et al., 2010</string>
        <sentence_id>33395</sentence_id>
        <char_offset>38</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>24</reference_id>
        <string>Negri and Mehdad, 2010</string>
        <sentence_id>33332</sentence_id>
        <char_offset>101</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>25</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>33283</sentence_id>
        <char_offset>72</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>30</reference_id>
        <string>Moldovan, 2005</string>
        <sentence_id>33250</sentence_id>
        <char_offset>132</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>31</reference_id>
        <string>Snover et al., 2009</string>
        <sentence_id>33292</sentence_id>
        <char_offset>42</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>34</reference_id>
        <string>Yamada and Knight, 2001</string>
        <sentence_id>33419</sentence_id>
        <char_offset>141</char_offset>
      </citation>
    </citations>
  </content>
</document>
