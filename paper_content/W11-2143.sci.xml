<PAPER>
  <FILENO/>
  <TITLE>CMU Syntax-Based Machine Translation at WMT 2011</TITLE>
  <AUTHORS>
    <AUTHOR>Greg Hanneman</AUTHOR>
  </AUTHORS>
  <ABSTRACT>
    <A-S ID="S-47490">We present the Carnegie Mellon University Stat-XFER group submission to the WMT 2011 shared translation task.</A-S>
    <A-S ID="S-47491">We built a hybrid syntactic MT system for French&#8211;English using the Joshua decoder and an automatically acquired SCFG.</A-S>
    <A-S ID="S-47492">New work for this year includes training data selection and grammar filtering.</A-S>
    <A-S ID="S-47493">Expanded training data selection significantly increased translation scores and lowered OOV rates, while results on grammar filtering were mixed.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-47494">During the past year, the statistical transfer machine translation group at Carnegie Mellon University has continued its work on large-scale syntactic MT systems based on automatically acquired synchronous context-free grammars (SCFGs).</S>
        <S ID="S-47495">For the 2011 Workshop on Machine Translation, we built a hybrid MT system, including both syntactic and non-syntactic rules, and submitted it as a constrained entry to the French&#8211;English translation task.</S>
        <S ID="S-47496">This is our fourth yearly submission to the WMT shared translation task.</S>
        <S ID="S-47497">In design and construction, the system is similar to our submission from last year&#8217;s workshop (<REF ID="R-04" RPTR="4">Hanneman et al., 2010</REF>), with changes in the methods we employed for training data selection and SCFG filtering.</S>
        <S ID="S-47498">Continuing WMT&#8217;s general trend, we worked with more data than in previous years, basing our 2011 system on 13.9 million sentences of parallel French&#8211;English training data and an English language model of 1.8 billion words.</S>
        <S ID="S-47499">Decoding was carried out in Joshua (Li et al., 2009), an open-source framework for parsing-based MT.</S>
        <S ID="S-47500">We managed our experiments with LoonyBin (<REF ID="R-02" RPTR="2">Clark and Lavie, 2010</REF>), an open-source tool for defining, modifying, and running complex experimental pipelines.</S>
      </P>
      <P>
        <S ID="S-47501">We describe our system-building process in more detail in Section 2.</S>
        <S ID="S-47502">In Section 3, we evaluate the system&#8217;s performance on WMT development sets and examine the aftermath of training data selection and grammar filtering.</S>
        <S ID="S-47503">Section 4 concludes with possible directions for future work.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 System Construction</HEADER>
      <P>
        <S ID="S-47584"></S>
      </P>
      <DIV DEPTH="1">
        <HEADER>2.1 Training Data Selection</HEADER>
        <P>
          <S ID="S-47504">WMT 2011&#8217;s provided French&#8211;English training data consisted of 36.8 million sentence pairs from the Europarl, news commentary, UN documents, and Giga- FrEn corpora (Table 1).</S>
          <S ID="S-47505">The first three of these are, for the most part, clean data resources that have been successfully employed as MT corpora for a number of years.</S>
          <S ID="S-47506">The Giga-FrEn corpus, though the largest, is also the least precise, as its Web-crawled data sources are less homogeneous and less structured than the other corpora.</S>
          <S ID="S-47507">Nevertheless, Pino et al. (2010) found significant improvements in French&#8211; English MT output quality by including it.</S>
          <S ID="S-47508">Our goal for this year was to strike a middle ground: to avoid computational difficulties in using the entire 36.8 million sentence pairs of training data, but to mine the Giga-FrEn corpus for sentences to increase our system&#8217;s vocabulary coverage.</S>
        </P>
        <P>
          <S ID="S-47509">Our method of training data selection proceeded as follows.</S>
          <S ID="S-47510">We first tokenized all the parallel training</S>
        </P>
        <P>
          <S ID="S-47511">Corpus Released Used</S>
        </P>
        <P>
          <S ID="S-47512">data using the Stanford parser&#8217;s tokenizer (<REF ID="R-05" RPTR="5">Klein and Manning, 2003</REF>) for English and our own in-house script for French.</S>
          <S ID="S-47513">We then passed the Europarl, news commentary, and UN data through a filtering script that removed lines longer than 95 tokens in either language, empty lines, lines with excessively imbalanced length ratios, and lines containing tokens of more than 25 characters in either language.</S>
          <S ID="S-47514">From the filtered data, we computed a list of the sourceside vocabulary words along with their frequency counts.</S>
          <S ID="S-47515">Next, we searched the Giga-FrEn corpus for relatively short lines on the source side (up to 50 tokens long) that contained either a new vocabulary word or a word that had been previously seen fewer than 20 times.</S>
          <S ID="S-47516">Such lines were added to the filtered training data to make up our system&#8217;s final parallel training corpus.</S>
        </P>
        <P>
          <S ID="S-47517">The number of sentences retained from each data source is listed in Table 1; in the end, we trained our system from 13.9 million parallel sentences.</S>
          <S ID="S-47518">With the Giga-FrEn data included, the source side of our parallel corpus had a vocabulary of just over 1.9 million unique words, compared with a coverage of 545,000 words without using Giga-FrEn.</S>
        </P>
        <P>
          <S ID="S-47519">We made the decision to leave the training data in mixed case for our entire system-building process.</S>
          <S ID="S-47520">At the cost of slightly sparser estimates for word alignments and translation probabilities, a mixedcase system avoids the extra step of building a statistical recaser to treat our system&#8217;s output.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.2 Grammar Extraction and Scoring</HEADER>
        <P>
          <S ID="S-47521">Once we had assembled the final training corpus, we annotated it with statistical word alignments and constituent parse trees on both sides.</S>
          <S ID="S-47522">Unidirectional word alignments were provided by MGIZA++ (<REF ID="R-03" RPTR="3">Gao and Vogel, 2008</REF>), then symmetrized with the grow-diag-final-and heuristic (<REF ID="R-07" RPTR="7">Koehn et al., 2005</REF>).</S>
          <S ID="S-47523">For generating parse trees, we used the French and English grammars of the Berkeley statistical parser (<REF ID="R-14" RPTR="14">Petrov and Klein, 2007</REF>).</S>
        </P>
        <P>
          <S ID="S-47524">Except for minor bug fixes, our method for extracting and scoring a translation grammar remains the same as in our WMT 2010 submission.</S>
          <S ID="S-47525">We extracted both syntactic and non-syntactic portions of the translation grammar.</S>
          <S ID="S-47526">The non-syntactic grammar was extracted from the parallel corpus and word alignments following the standard heuristics of phrase-based SMT (<REF ID="R-06" RPTR="6">Koehn et al., 2003</REF>).</S>
          <S ID="S-47527">The syntactic grammar was produced using the method of <REF ID="R-11" RPTR="11">Lavie et al. (2008)</REF>, which decomposes each pair of word-aligned parse trees into a series of minimal SCFG rules.</S>
          <S ID="S-47528">The word alignments are first generalized to node alignments, where nodes s and t are aligned between the source and target parse trees if all word alignments in the yield of s land within the yield of t and vice versa.</S>
          <S ID="S-47529">Minimal SCFG rules are derived from adjacent levels of node alignments: the labels from each pair of aligned nodes forms a rule&#8217;s left-hand side, and the right-hand side is made up of the labels from the frontier of aligned nodes encountered when walking the left-hand side&#8217;s subtrees.</S>
          <S ID="S-47530">Within a phrase length limit, each aligned node pair generate an all-terminal phrase pair rule as well.</S>
        </P>
        <P>
          <S ID="S-47531">Since both grammars are extracted from the same Viterbi word alignments using similar alignment consistency constraints, the phrase pair rules from the syntactic grammar make up a subset of the rules extracted according to phrase-based SMT heuristics.</S>
          <S ID="S-47532">We thus share instance counts between identical phrases extracted in both grammars, then delete the non-syntactic versions.</S>
          <S ID="S-47533">Remaining non-syntactic phrase pairs are converted to SCFG rules, with the phrase pair forming the right-hand side and the dummy label PHR::PHR as the left-hand side.</S>
          <S ID="S-47534">Except for the dummy label, all nonterminals in the final SCFG are made up of a syntactic category label from French joined with a syntactic category label from English, as extracted in the syntactic grammar.</S>
          <S ID="S-47535">A sampling of extracted SCFG rules is shown in Figure 1.</S>
        </P>
        <P>
          <S ID="S-47536">The combined grammar was scored according to the 22 translation model features we used last year.</S>
          <S ID="S-47537">For a generic SCFG rule of the form l s :: l t &#8594;</S>
        </P>
        <P>
          <S ID="S-47538">PHR :: PHR &#8594; [, ainsi qu&#8217;] :: [as well as]</S>
        </P>
        <P>
          <S ID="S-47539">V :: VBN &#8594; [modifi&#233;es] :: [modified]</S>
        </P>
        <P>
          <S ID="S-47540">NP :: NP &#8594; [les conflits arm&#233;s] :: [armed conflict]</S>
        </P>
        <P>
          <S ID="S-47541">AP :: SBAR &#8594; [tel qu&#8217; VPpart 1 ] :: [as VP 1 ]</S>
        </P>
        <P>
          <S ID="S-47542">NP :: NP &#8594; [D 1 N 2 A 3 ] :: [CD 1 JJ 3 NNS 2 ]</S>
        </P>
        <P>
          <S ID="S-47543">[r s ] :: [r t ], we computed 11 maximum-likelihood features as follows:</S>
        </P>
        <P>
          <S ID="S-47544">&#8226; Phrase translation scores P(r s |r t ) and P(r t |r s ) for phrase pair rules, using the larger non-syntactic instance counts for rules that were also extracted syntactically.</S>
        </P>
        <P>
          <S ID="S-47545">&#8226; Hierarchical translation scores P(r s |r t ) and P(r t |r s ) for syntactic rules with nonterminals on the right-hand side.</S>
        </P>
        <P>
          <S ID="S-47546">&#8226; Labeling scores P(l s :: l t |r s ), P(l s :: l t |r t ), and P(l s :: l t |r s ,r t ) for syntactic rules.</S>
        </P>
        <P>
          <S ID="S-47547">&#8226; &#8220;Not syntactically labelable&#8221; scores P(l s :: l t = PHR :: PHR |r s ) and P(l s :: l t = PHR :: PHR |r t ), with additive smoothing (n = 1), for all rules.</S>
        </P>
        <P>
          <S ID="S-47548">&#8226; Bidirectional lexical scores for all rules with lexical items, calculated from a unigram lexicon over Viterbi-aligned word pairs as in the Moses decoder (<REF ID="R-08" RPTR="8">Koehn et al., 2007</REF>).</S>
        </P>
        <P>
          <S ID="S-47549">We also included the following 10 binary indicator features using statistics local to each rule:</S>
        </P>
        <P>
          <S ID="S-47550">&#8226; Three low-count features that equal 1 when the extracted frequency of the rule is exactly equal to 1, 2, or 3.</S>
        </P>
        <P>
          <S ID="S-47551">&#8226; A syntactic feature that equals 1 when the rule&#8217;s label is syntactic, and a corresponding nonsyntactic feature that equals 1 when the rule&#8217;s label is PHR::PHR.</S>
        </P>
        <P>
          <S ID="S-47552">&#8226; Five rule format features that equal 1 when the rule&#8217;s right-hand side has a certain composition.</S>
          <S ID="S-47553">If a s and a t are true when the source and target sides contain only nonterminals, respectively, our rule format features are equal to a s , a t , a s &#8743; &#257; t , &#257; s &#8743; a t , and &#257; s &#8743; &#257; t .</S>
        </P>
        <P>
          <S ID="S-47554">Finally, our model includes a glue rule indicator feature that equals 1 when the rule is a generic glue rule.</S>
          <S ID="S-47555">In the Joshua decoder, glue rules monotonically stitch together adjacent parsed translation fragments at no model cost.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.3 Language Modeling</HEADER>
        <P>
          <S ID="S-47556">This year, our constrained-track system made use of part of the English Gigaword data, along with other provided text, in its target-side language model.</S>
          <S ID="S-47557">From among the data released directly for WMT 2011, we used the English side of the Europarl, news commentary, French&#8211;English UN document, and English monolingual news corpora.</S>
          <S ID="S-47558">From the English Gigaword corpus, we included the entire Xinhua portion and the most recent 13 million sentences of the AP Wire portion.</S>
          <S ID="S-47559">Some of these corpora contain many lines that are repeated a disproportionate number of times &#8212; the monolingual news corpus in particular, when filtered to only one occurrence of each sentence, reaches only 27% of its original line count.</S>
          <S ID="S-47560">As part of preparing our language modeling data, we deduplicated both the English news and the UN documents, the corpora with the highest percentages of repeated sentences.</S>
          <S ID="S-47561">We also removed lines containing more than 750 characters (about 125 average English words) before tokenization.</S>
        </P>
        <P>
          <S ID="S-47562">The final prepared corpus was made up of approximately 1.8 billion words of running text.</S>
          <S ID="S-47563">We built a 5-gram language model from it with the SRI language modeling toolkit (<REF ID="R-17" RPTR="16">Stolcke, 2002</REF>).</S>
          <S ID="S-47564">To match the treatment given to the training data, the language model was also built in mixed case.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.4 Grammar Filtering for Decoding</HEADER>
        <P>
          <S ID="S-47565">As is to be expected from a training corpus of 13.9 million sentence pairs, the grammars we extract according to the procedure of Section 2.2 are quite large: approximately 2.53 billion non-syntactic and 440 million syntactic rule instances, for a combined grammar of 1.26 billion unique rules.</S>
          <S ID="S-47566">In preparation for tuning or decoding, we are faced with the engineering challenge of selecting a subset of the gram-</S>
        </P>
        <P>
          <S ID="S-47567">mar that contains useful rules and fits in a reasonable amount of memory.</S>
          <S ID="S-47568">Before even extracting a syntactic grammar, we passed the automatically generated parse trees on the training corpus through a small tag-correction script as a pre-step.</S>
          <S ID="S-47569">In previous experimentation, we noticed that a surprising proportion of cardinal numbers in English had been tagged with labels other than CD, their correct tag.</S>
          <S ID="S-47570">We also found errors in labeling marks of punctuation in both English and French, when again the canonical labels are unambiguous.</S>
          <S ID="S-47571">To fix these errors, we forcibly overwrote the labels of English tokens made up of only digits with CD, and we overwrote the labels of 25 English and 24 French marks of punctuation or other symbols with the appropriate tag as defined by the relevant treebank tagging guidelines.</S>
        </P>
        <P>
          <S ID="S-47572">After grammar extraction and combination of syntactic and non-syntactic rules, we ran an additional filtering step to reduce derivational ambiguity in the case where the same SCFG right-hand side appeared with more than one left-hand-side label.</S>
          <S ID="S-47573">For each right-hand side, we sorted its possible labels by extracted frequency, then threw out the labels in the bottom 10% of the left-hand-side distribution.</S>
        </P>
        <P>
          <S ID="S-47574">Finally, we ran a main grammar filtering step prior to tuning or decoding, experimenting with two different filtering methods.</S>
          <S ID="S-47575">In both cases, the phrase pair rules in the grammar were split off and filtered so that only those whose source sides completely matched the tuning or test set were retained.</S>
          <S ID="S-47576">The first, more naive grammar filtering method sorted all hierarchical rules by extracted frequency, then retained the most frequent 10,000 rules to join all matching phrase pair rules in the final translation grammar.</S>
          <S ID="S-47577">This is similar to the basic grammar filtering we performed for our WMT 2010 submission.</S>
          <S ID="S-47578">It is based on the rationale that the most frequently extracted rules in the parallel training data are likely to be the most reliably estimated and also frequently used in translating a new data set.</S>
          <S ID="S-47579">However, it also passes through a disproportionate number of fully abstract rules &#8212; that is, rules whose right-hand sides are made up entirely of nonterminals &#8212; which can apply more recklessly on the test set because they are not lexically grounded.</S>
          <S ID="S-47580">Our second, more advanced method of filtering made two improvements over the naive approach.</S>
          <S ID="S-47581">First, it controlled for the imbalance of hierarchical rules by splitting the grammar&#8217;s partially lexicalized rules into a separate group that can be filtered independently.</S>
          <S ID="S-47582">Second, it applied a lexical-match filter such that a partially lexicalized rule was retained only if all its lexicalized source phrases up to bigrams matched the intended tuning or testing set.</S>
          <S ID="S-47583">The final translation grammar in this case was made up of three parts: all phrase pair rules matching the test set (as before), the 100,000 most frequently extracted partially lexicalized rules whose bigrams match the test set, and the 2000 most frequently extracted fully abstract rules.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Experimental Results and Analysis</HEADER>
      <P>
        <S ID="S-47585">We tuned each system variant on the newstest2008 data set, using the Z-MERT package (<REF ID="R-12" RPTR="12">Zaidan, 2009</REF>) for minimum error-rate training to the BLEU metric.</S>
        <S ID="S-47586">We ran development tests on the newstest2009 and newstest2010 data sets; Table 2 reports the results obtained according to various automatic metrics.</S>
        <S ID="S-47587">The evaluation consists of case-insensitive scoring according to METEOR 1.0 (<REF ID="R-10" RPTR="10">Lavie and Denkowski, 2009</REF>) tuned to HTER with the exact, stemming, and synonymy modules enabled, case-insensitive BLEU (<REF ID="R-13" RPTR="13">Papineni et al., 2002</REF>) as implemented by the NIST mteval-v13 script, and case-insensitive TER 0.7.25 (<REF ID="R-16" RPTR="15">Snover et al., 2006</REF>).</S>
        <S ID="S-47588">Table 2 gives comparative results for two major systems: one based on our WMT 2011 data selection as outlined in Section 2.1, and one based on the smaller WMT 2010 training data that we used last year (8.6 million sentence pairs).</S>
        <S ID="S-47589">Each system was run with the two grammar filtering variants described in Section 2.4: the 10,000 most frequently extracted hierarchical rules of any type (&#8220;10k&#8221;), and a combination of the 2000 most frequently extracted abstract rules and the 100,000 most frequently extracted partially lexicalized rules that matched the test set (&#8220;2k+100k&#8221;).</S>
        <S ID="S-47590">Our primary submission to the WMT 2011 shared task was the fourth line of Table 2 (&#8220;WMT 2011 2k+100k&#8221;); we also made a constrastive submission with the system from the second line (&#8220;WMT 2010 2k+100k&#8221;).</S>
      </P>
      <P>
        <S ID="S-47591">Using part of the Giga-FrEn data &#8212; along with the additions to the Europarl, news commentary, and UN document courses released since last year</S>
      </P>
      <P>
        <S ID="S-47592">newstest2009 newstest2010 System METEOR BLEU TER METEOR BLEU TER</S>
      </P>
      <P>
        <S ID="S-47593">Applications 10k 2k+100k</S>
      </P>
      <P>
        <S ID="S-47594">&#8212; is beneficial to translation quality, as there is a clear improvement in metric scores between the 2010 and 2011 systems.</S>
        <S ID="S-47595">Our BLEU score improvements of 1.2 to 1.9 points are statistically significant according to the paired bootstrap resampling method (<REF ID="R-09" RPTR="9">Koehn, 2004</REF>) with n = 1000 and p &lt; 0.01.</S>
        <S ID="S-47596">They are also larger than the 0.7- to 1.1-point gains reported by Pino et al. (2010) when the full Giga-FrEn was added.</S>
        <S ID="S-47597">The 2011 system also shows a significant reduction in the out-of-vocabulary (OOV) rate on both test sets: 38% and 47% fewer OOV types, and 44% and 45% fewer OOV tokens, when compared to the 2010 system.</S>
      </P>
      <P>
        <S ID="S-47598">Differences between grammar filtering techniques, on the other hand, are much less significant according to all three metrics.</S>
        <S ID="S-47599">Under paired bootstrap resampling on the newstest2009 set, the grammar variants in both the 2010 and 2011 systems are statistically equivalent according to BLEU score.</S>
        <S ID="S-47600">On newstest2010, the 2k+100k grammar improves over the 10k version (p &lt; 0.01) in the 2010 system, but the situation is reversed in the 2011 system.</S>
      </P>
      <P>
        <S ID="S-47601">We investigated differences in grammar use with an analysis of rule applications in the two variants of the 2011 system, the results of which are summarized in Table 3.</S>
        <S ID="S-47602">Though the configuration with the 2k+100k grammar does apply syntactic rules 20% more frequently than its 10k counterpart, the 10k system uses overall 53% more unique rules.</S>
        <S ID="S-47603">One contributing factor to this situation could be that the fully abtract rule cutoff is set too low compared to the increase in partially lexicalized rules.</S>
        <S ID="S-47604">The effect of the 2k+100k filtering is to reduce the number of abstract rules from 4000 to 2000 while increasing the number of partially lexicalized rules from 6000 to 100,000.</S>
        <S ID="S-47605">However, we find that the 10k system makes heavy use of some short, meaningful abstract rules that were excluded from the 2k+100k system.</S>
        <S ID="S-47606">The 2k+100k grammar, by contrast, includes a long tail of less frequently used partially lexicalized grammar rules.</S>
        <S ID="S-47607">In practice, there is a balance between the use of syntactic and non-syntactic grammar rules during decoding.</S>
        <S ID="S-47608">We highlight an example of how both types of rules work together in Figure 2, which shows our primary system&#8217;s translation of part of newstest2009 sentence 2271.</S>
        <S ID="S-47609">The French source text is given in italics and segmented into phrases.</S>
        <S ID="S-47610">The SCFG rules used in translation are shown above each phrase, where numerical superscripts on the nonterminal labels indicate those constituents&#8217; relative ordering in the original French sentence.</S>
        <S ID="S-47611">(Monotonic glue rules are not shown.</S>
        <S ID="S-47612">) While nonsyntactic rules can be used for short-distance reordering and fixed phrases, such as t&#233;l&#233;phones mobiles &#8596; mobile phones, the model prefers syntactic translations for more complicated patterns, such as the head&#8211;children reversal in appareils musicaux portables &#8596; portable music devices.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 Conclusions and Future Work</HEADER>
      <P>
        <S ID="S-47613">Compared to last year, the two main differences in our current WMT submission are: (1) a new training data selection strategy aimed at increasing system vocabulary without hugely increasing corpus size, and (2) a new method of grammar filtering that emphasizes partially lexicalized rules over fully ab-</S>
      </P>
      <P>
        <S ID="S-47614">Based on the results presented in Section 3, we feel confident in declaring vocabulary-based filtering of the Giga-FrEn corpus a success.</S>
        <S ID="S-47615">By increasing the size of our parallel corpus by 26%, we more than tripled the number of unique words appearing in the source text.</S>
        <S ID="S-47616">In conjunction with supplements to the Europarl, news commentary, and UN document corpora, this improvement led to 44% fewer OOV tokens at decoding time on two different test sets, as well as a boost in automatic metric scores of 0.6 METEOR, 1.2 BLEU, and 1.5 TER points compared to last year&#8217;s system.</S>
        <S ID="S-47617">We expect to employ similar data selection techniques when building future systems, especially as the amount of parallel data available continues to increase.</S>
        <S ID="S-47618">We did not, however, find significant improvements in translation quality by changing the grammar filtering method.</S>
        <S ID="S-47619">As discussed in Section 3, limiting the grammar to only 2000 fully abstract rules may not have been enough, since additional abstract rules applied fairly frequently in test data if they were available.</S>
        <S ID="S-47620">We plan to experiment with larger filtering cutoffs in future work.</S>
        <S ID="S-47621">A complementary solution could be to increase the number of partially lexicalized rules.</S>
        <S ID="S-47622">Although we found mixed results in their application within our current system, the success of Hiero-derived MT systems (<REF ID="R-00" RPTR="0">Chiang, 2005</REF>; <REF ID="R-01" RPTR="1">Chiang, 2010</REF>) shows that high translation quality can be achieved with rules that are only partially abstract.</S>
        <S ID="S-47623">A major difference between such systems and our current implementation is that ours, at 102,000 rules, has a much smaller grammar.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS>
    <P>
      <S ID="S-47624">This research was supported in part by U.S. National Science Foundation grants IIS-0713402 and IIS-0915327, as well as by the DARPA GALE program.</S>
      <S ID="S-47625">Thanks to Kenneth Heafield for processing the English monolingual data and building the language model file, and to Jonathan Clark for Loony- Bin support and bug fixes.</S>
      <S ID="S-47626">We also thank Yahoo!</S>
      <S ID="S-47627">for the use of the M45 research computing cluster, where we ran many steps of our experimental pipeline.</S>
    </P>
  </ACKNOWLEDGMENTS>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>David Chiang</RAUTHOR>
      <REFTITLE>A hierarchical phrase-based model for statistical machine translation.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>David Chiang</RAUTHOR>
      <REFTITLE>Learning to translate with source and target syntax.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>Clark</RAUTHOR>
      <REFTITLE>LoonyBin: Keeping language technologists sane through automated management of experimental (hyper)workflows.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>Qin Gao</RAUTHOR>
      <REFTITLE>Parallel implementations of word alignment tool.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>Greg Hanneman</RAUTHOR>
      <REFTITLE>Improved features and grammar selection for syntaxbased MT.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>Dan Klein</RAUTHOR>
      <REFTITLE>Fast exact inference with a factored model for natural language parsing.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Statistical phrase-based translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Chris Dyer, Ondrej Bojar,</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Statistical significance tests for machine translation evaluation.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>Alon Lavie</RAUTHOR>
      <REFTITLE>The METEOR metric for automatic evaluation of machine translation.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>Alon Lavie</RAUTHOR>
      <REFTITLE>Syntax-driven learning of sub-sentential translation equivalents and translation rules from parsed parallel corpora.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>Zaidan</RAUTHOR>
      <REFTITLE>Joshua: An open source toolkit for parsing-based machine translation.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>Kishore Papineni</RAUTHOR>
      <REFTITLE>BLEU: A method for automatic evalution of machine translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="14">
      <RAUTHOR>Slav Petrov</RAUTHOR>
      <REFTITLE>Improved inference for unlexicalized parsing.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="15">
      <RAUTHOR>Juan Pino</RAUTHOR>
      <REFTITLE>The CUED HiFST system for the WMT10 translation shared task.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="16">
      <RAUTHOR>Matthew Snover</RAUTHOR>
      <REFTITLE>A study of translation edit rate with targeted human annotation.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="17">
      <RAUTHOR>Andreas Stolcke</RAUTHOR>
      <REFTITLE>SRILM &#8212; an extensible language modeling toolkit.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
