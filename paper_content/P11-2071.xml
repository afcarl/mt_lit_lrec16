<document>
  <filename>P11-2071</filename>
  <authors>
    <author>Hal Daum&#233;</author>
  </authors>
  <title>Domain Adaptation for Machine Translation by Mining Unseen Words</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>We show that unseen words account for a large part of the translation error when moving to new domains. Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al., 2008), we are able to find translations for otherwise OOV terms. We show several approaches to integrating such translations into a phrasebased translation system, yielding consistent improvements in translations quality (between 0.5 and 1.5 Bleu points) on four domains and two language pairs.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We show that unseen words account for a large part of the translation error when moving to new domains.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al., 2008), we are able to find translations for otherwise OOV terms.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We show several approaches to integrating such translations into a phrasebased translation system, yielding consistent improvements in translations quality (between 0.5 and 1.5 Bleu points) on four domains and two language pairs.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Large amounts of data are currently available to train statistical machine translation systems. Unfortunately, these training data are often qualitatively different from the target task of the translation system. In this paper, we consider one specific aspect of domain divergence (Jiang, 2008; Blitzer and Daum&#233; III, 2010): the out-of-vocabulary problem. By considering four different target domains (news, medical, movie subtitles, technical documentation) in two source languages (German, French), we: (1) Ascertain the degree to which domain divergence causes increases in unseen words, and the degree to which this degrades translation performance. (For instance, if all unknown words are names, then copying them verbatim may be sufficient.) (2) Extend known methods for mining dictionaries from comparable corpora to the domain adaptation setting, by &#8220;bootstrapping&#8221; them based on known translations from the source domain. (3) Develop methods for integrating these mined dictionaries into a phrase-based translation system (Koehn et al., 2007).
As we shall see, for most target domains, out of vocabulary terms are the source of approximately half of the additional errors made. The only exception is the news domain, which is sufficiently similar to parliament proceedings (Europarl) that there are essentially no new, frequent words in news. By mining a dictionary and naively incorporating it into a translation system, one can only do slightly better than baseline. However, with a more clever integration, we can close about half of the gap between baseline (unadapted) performance and an oracle experiment. In most cases this amounts to an improvement of about 1.5 Bleu points (Papineni et al., 2002) and 1.5 Meteor points (Banerjee and Lavie, 2005). The specific setting we consider is the one in which we have plentiful parallel (&#8220;labeled&#8221;) data in a source domain (eg., parliament) and plentiful comparable (&#8220;unlabeled&#8221;) data in a target domain (eg., medical). We can use the unlabeled data in the target domain to build a good language model. Finally, we assume access to a very small amount of parallel (&#8220;labeled&#8221;) target data, but only enough to evaluate on, or run weight tuning (Och, 2003). All knowledge about unseen words must come from the comparable data.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Large amounts of data are currently available to train statistical machine translation systems.</text>
              <doc_id>3</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Unfortunately, these training data are often qualitatively different from the target task of the translation system.</text>
              <doc_id>4</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, we consider one specific aspect of domain divergence (Jiang, 2008; Blitzer and Daum&#233; III, 2010): the out-of-vocabulary problem.</text>
              <doc_id>5</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>By considering four different target domains (news, medical, movie subtitles, technical documentation) in two source languages (German, French), we: (1) Ascertain the degree to which domain divergence causes increases in unseen words, and the degree to which this degrades translation performance.</text>
              <doc_id>6</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>(For instance, if all unknown words are names, then copying them verbatim may be sufficient.</text>
              <doc_id>7</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>) (2) Extend known methods for mining dictionaries from comparable corpora to the domain adaptation setting, by &#8220;bootstrapping&#8221; them based on known translations from the source domain.</text>
              <doc_id>8</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>(3) Develop methods for integrating these mined dictionaries into a phrase-based translation system (Koehn et al., 2007).</text>
              <doc_id>9</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>As we shall see, for most target domains, out of vocabulary terms are the source of approximately half of the additional errors made.</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The only exception is the news domain, which is sufficiently similar to parliament proceedings (Europarl) that there are essentially no new, frequent words in news.</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>By mining a dictionary and naively incorporating it into a translation system, one can only do slightly better than baseline.</text>
              <doc_id>12</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>However, with a more clever integration, we can close about half of the gap between baseline (unadapted) performance and an oracle experiment.</text>
              <doc_id>13</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In most cases this amounts to an improvement of about 1.5 Bleu points (Papineni et al., 2002) and 1.5 Meteor points (Banerjee and Lavie, 2005).</text>
              <doc_id>14</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The specific setting we consider is the one in which we have plentiful parallel (&#8220;labeled&#8221;) data in a source domain (eg., parliament) and plentiful comparable (&#8220;unlabeled&#8221;) data in a target domain (eg., medical).</text>
              <doc_id>15</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We can use the unlabeled data in the target domain to build a good language model.</text>
              <doc_id>16</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we assume access to a very small amount of parallel (&#8220;labeled&#8221;) target data, but only enough to evaluate on, or run weight tuning (Och, 2003).</text>
              <doc_id>17</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>All knowledge about unseen words must come from the comparable data.</text>
              <doc_id>18</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Background and Challenges</title>
        <text>Domain adaptation is a well-studied field, both in the NLP community as well as the machine learning and statistics communities. Unlike in machine learning, in the case of translation, it is not enough to simply
adjust the weights of a learned translation model to do well on a new domain. As expected, we shall see that unseen words pose a major challenge for adapting translation systems to distant domains. No machine learning approach to adaptation could hope to attenuate this problem.
There have been a few attempts to measure or perform domain adaptation in machine translation. One of the first approaches essentially performs test-set relativization (choosing training samples that look most like the test data) to improve translation performance, but applies the approach only to very small data sets (Hildebrand et al., 2005). Later approaches are mostly based on a data set made available in the 2007 StatMT workshop (Koehn and Schroeder, 2007), and have attempted to use monolingual (Civera and Juan, 2007; Bertoldi and Federico, 2009) or comparable (Snover et al., 2008) corpus resources. These papers all show small, but significant, gains in performance when moving from Parliament domain to News domain.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Domain adaptation is a well-studied field, both in the NLP community as well as the machine learning and statistics communities.</text>
              <doc_id>19</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Unlike in machine learning, in the case of translation, it is not enough to simply</text>
              <doc_id>20</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>adjust the weights of a learned translation model to do well on a new domain.</text>
              <doc_id>21</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As expected, we shall see that unseen words pose a major challenge for adapting translation systems to distant domains.</text>
              <doc_id>22</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>No machine learning approach to adaptation could hope to attenuate this problem.</text>
              <doc_id>23</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>There have been a few attempts to measure or perform domain adaptation in machine translation.</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>One of the first approaches essentially performs test-set relativization (choosing training samples that look most like the test data) to improve translation performance, but applies the approach only to very small data sets (Hildebrand et al., 2005).</text>
              <doc_id>25</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Later approaches are mostly based on a data set made available in the 2007 StatMT workshop (Koehn and Schroeder, 2007), and have attempted to use monolingual (Civera and Juan, 2007; Bertoldi and Federico, 2009) or comparable (Snover et al., 2008) corpus resources.</text>
              <doc_id>26</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>These papers all show small, but significant, gains in performance when moving from Parliament domain to News domain.</text>
              <doc_id>27</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Data</title>
        <text>Dom Most frequent OOV Words News behavior, favor, neighbors, fueled, neighboring, abe, wwii, favored, nicolas, fa- (17%)
vorable, zhao, ahmedinejad, bernanke, favorite, phelps, ccp, skeptical, neighbor, skeptics, skepticism Emea renal, hepatic, subcutaneous, irbesartan, (49%) ribavirin, olanzapine, serum, patienten,
dl, eine, sie, pharmacokinetics, ritonavir, hydrochlorothiazide, erythropoietin, efavirenz, hypoglycaemia, epoetin, blister, pharmacokinetic Subs (68%)
PHP (44%) gonna, yeah, f...ing, s..., f..., gotta, uh, wanna, mom, lf, ls, em, b....h, daddy, sia, goddamn, sammy, tyler, bye, bigweld php, apache, sql, integer, socket, html, filename, postgresql, unix, mysql, color, constants, syntax, sesam, cookie, cgi, numeric, pdf, ldap, byte
All of these data sets actually come with parallel target domain data. To obtain comparable data, we applied to standard trick of taking the first 50% of the English text as English and the last 50% of the German text as German. While such data is more parallel than, say, Wikipedia, it is far from parallel.
To get a better sense of the differences between these domains, we give some simple statistics about out of vocabulary words and examples in Table 1. Here, for each domain, we show the percentage of words (types) in the target domain that are unseen in the Parliament data. As we can see, it is markedly higher in Emea, Subs and PHP than in News.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Dom Most frequent OOV Words News behavior, favor, neighbors, fueled, neighboring, abe, wwii, favored, nicolas, fa- (17%)</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>vorable, zhao, ahmedinejad, bernanke, favorite, phelps, ccp, skeptical, neighbor, skeptics, skepticism Emea renal, hepatic, subcutaneous, irbesartan, (49%) ribavirin, olanzapine, serum, patienten,</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>dl, eine, sie, pharmacokinetics, ritonavir, hydrochlorothiazide, erythropoietin, efavirenz, hypoglycaemia, epoetin, blister, pharmacokinetic Subs (68%)</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>PHP (44%) gonna, yeah, f...ing, s..., f..., gotta, uh, wanna, mom, lf, ls, em, b....h, daddy, sia, goddamn, sammy, tyler, bye, bigweld php, apache, sql, integer, socket, html, filename, postgresql, unix, mysql, color, constants, syntax, sesam, cookie, cgi, numeric, pdf, ldap, byte</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>All of these data sets actually come with parallel target domain data.</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To obtain comparable data, we applied to standard trick of taking the first 50% of the English text as English and the last 50% of the German text as German.</text>
              <doc_id>33</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>While such data is more parallel than, say, Wikipedia, it is far from parallel.</text>
              <doc_id>34</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To get a better sense of the differences between these domains, we give some simple statistics about out of vocabulary words and examples in Table 1.</text>
              <doc_id>35</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Here, for each domain, we show the percentage of words (types) in the target domain that are unseen in the Parliament data.</text>
              <doc_id>36</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>As we can see, it is markedly higher in Emea, Subs and PHP than in News.</text>
              <doc_id>37</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Dictionary Mining</title>
        <text>Our dictionary mining approach is based on Canonical Correlation Analysis, as used previously by (Haghighi et al., 2008). Briefly, given a multi-view data set, Canonical Correlation Analysis is a technique to find the projection directions in each view so that the objects when projected along these di-
rections are maximally aligned (Hotelling, 1936). Given any new pair of points, the similarity between the them can be computed by first projecting onto the lower dimensions space and computing the cosine similarity between their projections. In general, using all the eigenvectors is sub optimal and thus retaining top eigenvectors leads to an improved generalizability.
Here we describe the use of CCA to find the translations for the OOV German words (Haghighi et al., 2008). From the target domain corpus we extract the most frequent words (approximately 5000) for both the languages. Of these, words that have translation in the bilingual dictionary (learnt from Europarl) are used as training data. We use these words to learn the CCA projections and then mine the translations for the remaining frequent words. The dictionary mining involves multiple stages. In the first stage, we extract feature vectors for all the words. We use context and orthographic features. In the second stage, using the dictionary probabilities of seen words, we identify pairs of words whose feature vectors are used to learn the CCA projection directions. In the final stage, we project all the words into the sub-space identified by CCA and mine translations for the OOV words. We will describe each of these steps in detail in this section.
For each of the frequent words we extract the context vectors using a window of length five. To overcome data sparsity issue, we truncate each context word to its first seven characters. We discard all the context features which co-occur with less than five words. Among the remaining features, we consider only the most frequent 2000 features in each language. We convert the frequency vectors into TFIDF vectors, center the data and then binarize the vectors depending on if the feature value is positive of not. We convert this data into word similarities using linear dot product kernel. We also represent each word using the orthographic features, with n-grams of length 1-3 and convert them into TFIDF form and subsequently turn them into word similarities (again using the linear kernel). Since we convert the data into word similarities, the orthographic features are relevant even though the script of source and target languages differ. Where as using the features directly rending them useless for languages whose script is completely different like Arabic and Enwaste
glish. For each language we linearly combine the kernel matrices obtained using the context vectors and the orthographic features. We use incomlete cholesky decomposition to reduce the dimensionality of the kernel matrices. We do the same preprocessng for all words, the training words and the OOV words. And the resulting feature vectors for each word are used for learning the CCA projections
Since a word can have multiple translations, and that CCA uses only one translation, we form a bipartite graph with the training words in each language as nodes and the edge weight being the translation probability of the word pair. We then run Hungarian algorithm to extract maximum weighted bipartite matching (Jonker and Volgenant, 1987). We then run CCA on the resulting pairs of the bipartite matching to get the projection directions in each language. We retain only the top 35% of the eigenvectors. In other relevant experiments, we have found that this setting of CCA outperforms the baseline approach. We project all the frequent words, including the training words, in both the languages into the lower dimensional spaces and for each of the OOV word return the closest five points from the other language as potential new translations. The dictionary mining, viewed subjectively and intrinsically, performs quite well. In Table 2, we show four randomly selected unseen German words from Emea (that do not occur in the Parliament data), together with the top three translations and associated scores (which are not normalized). Based on a cursory evaluation of 5 randomly selected words in French and German
by native speakers (not the authors!), we found that 8/10 had correct mined translations.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Our dictionary mining approach is based on Canonical Correlation Analysis, as used previously by (Haghighi et al., 2008).</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Briefly, given a multi-view data set, Canonical Correlation Analysis is a technique to find the projection directions in each view so that the objects when projected along these di-</text>
              <doc_id>39</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>rections are maximally aligned (Hotelling, 1936).</text>
              <doc_id>40</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Given any new pair of points, the similarity between the them can be computed by first projecting onto the lower dimensions space and computing the cosine similarity between their projections.</text>
              <doc_id>41</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In general, using all the eigenvectors is sub optimal and thus retaining top eigenvectors leads to an improved generalizability.</text>
              <doc_id>42</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Here we describe the use of CCA to find the translations for the OOV German words (Haghighi et al., 2008).</text>
              <doc_id>43</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>From the target domain corpus we extract the most frequent words (approximately 5000) for both the languages.</text>
              <doc_id>44</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Of these, words that have translation in the bilingual dictionary (learnt from Europarl) are used as training data.</text>
              <doc_id>45</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We use these words to learn the CCA projections and then mine the translations for the remaining frequent words.</text>
              <doc_id>46</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The dictionary mining involves multiple stages.</text>
              <doc_id>47</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In the first stage, we extract feature vectors for all the words.</text>
              <doc_id>48</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We use context and orthographic features.</text>
              <doc_id>49</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In the second stage, using the dictionary probabilities of seen words, we identify pairs of words whose feature vectors are used to learn the CCA projection directions.</text>
              <doc_id>50</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>In the final stage, we project all the words into the sub-space identified by CCA and mine translations for the OOV words.</text>
              <doc_id>51</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>We will describe each of these steps in detail in this section.</text>
              <doc_id>52</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For each of the frequent words we extract the context vectors using a window of length five.</text>
              <doc_id>53</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To overcome data sparsity issue, we truncate each context word to its first seven characters.</text>
              <doc_id>54</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We discard all the context features which co-occur with less than five words.</text>
              <doc_id>55</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Among the remaining features, we consider only the most frequent 2000 features in each language.</text>
              <doc_id>56</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We convert the frequency vectors into TFIDF vectors, center the data and then binarize the vectors depending on if the feature value is positive of not.</text>
              <doc_id>57</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We convert this data into word similarities using linear dot product kernel.</text>
              <doc_id>58</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We also represent each word using the orthographic features, with n-grams of length 1-3 and convert them into TFIDF form and subsequently turn them into word similarities (again using the linear kernel).</text>
              <doc_id>59</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Since we convert the data into word similarities, the orthographic features are relevant even though the script of source and target languages differ.</text>
              <doc_id>60</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Where as using the features directly rending them useless for languages whose script is completely different like Arabic and Enwaste</text>
              <doc_id>61</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>glish.</text>
              <doc_id>62</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For each language we linearly combine the kernel matrices obtained using the context vectors and the orthographic features.</text>
              <doc_id>63</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We use incomlete cholesky decomposition to reduce the dimensionality of the kernel matrices.</text>
              <doc_id>64</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We do the same preprocessng for all words, the training words and the OOV words.</text>
              <doc_id>65</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>And the resulting feature vectors for each word are used for learning the CCA projections</text>
              <doc_id>66</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Since a word can have multiple translations, and that CCA uses only one translation, we form a bipartite graph with the training words in each language as nodes and the edge weight being the translation probability of the word pair.</text>
              <doc_id>67</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We then run Hungarian algorithm to extract maximum weighted bipartite matching (Jonker and Volgenant, 1987).</text>
              <doc_id>68</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We then run CCA on the resulting pairs of the bipartite matching to get the projection directions in each language.</text>
              <doc_id>69</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We retain only the top 35% of the eigenvectors.</text>
              <doc_id>70</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In other relevant experiments, we have found that this setting of CCA outperforms the baseline approach.</text>
              <doc_id>71</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We project all the frequent words, including the training words, in both the languages into the lower dimensional spaces and for each of the OOV word return the closest five points from the other language as potential new translations.</text>
              <doc_id>72</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The dictionary mining, viewed subjectively and intrinsically, performs quite well.</text>
              <doc_id>73</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Table 2, we show four randomly selected unseen German words from Emea (that do not occur in the Parliament data), together with the top three translations and associated scores (which are not normalized).</text>
              <doc_id>74</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Based on a cursory evaluation of 5 randomly selected words in French and German</text>
              <doc_id>75</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>by native speakers (not the authors!</text>
              <doc_id>76</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>), we found that 8/10 had correct mined translations.</text>
              <doc_id>77</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>5 Integration into MT System</title>
        <text>The output of the dicionary mining approach is a list of pairs (f,e) of foreign words and predicted English translations. Each of these comes with an associated score. There are two obvious ways to integrate such a dictionary into a phrase-based translation system: (1) Provide the dictionary entries as (weighted) &#8220;sentence&#8221; pairs in the parallel corpus. These &#8220;sentences&#8221; would each contain exactly one word. The weighting can be derived from the translation probability from the dictionary mining. (2) Append the phrase table of a baseline phrase-based translation model trained only on source domain data with the word pairs. Use the mining probability as the phrase translation probabilities.
It turned out in preliminary experiments (on German/Emea) that neither of these approaches worked particularly well. The first approach did not work at all, even with fairly extensive hand-tuning of the sentence weights. It often hurt translation performance. The second approach did not hurt translation performance, but did not help much either. It led to an average improvement of only about 0.5 Bleu points, on development data. This is likely because weight tuning tuned a single weight to account for the import of the phrase probabilities across both &#8220;true&#8221; phrases as well as these &#8220;mined&#8221; phrases.
We therefore came up with a slightly more complex, but still simple, method for adding the dictionary entries to the phrase table. We add four new features to the model, and set the plain phrasetranslation probabilities for the dictionary entries to zero. These new features are:
1. The dictionary mining translation probability. (Zero for original phrase pairs.)
2. An indicator feature that says whether all German words in this phrase pair were seen in the source data. (This will always be true for source phrases and always be false for dictionary entries.)
3. An indicator that says whether all German words in this phrase pair were seen in target data. (This is not the negation of the previous feature, because there are plenty of words in the target data that had also been seen. This feature might mean something like &#8220;trust this phrase pair a lot.&#8221;)
4. The conjunction of the previous two features.
Interestingly, only adding the first feature was not helpful (performance remained about 0.5 Bleu points above baseline). Adding only the last three features (the indicator features) alone did not help at all (performance was roughly on par with baseline). Only when all four features were included did performance improve significantly. In the results discussed in Section 6.2, we report results on test data using the combination of these four features.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The output of the dicionary mining approach is a list of pairs (f,e) of foreign words and predicted English translations.</text>
              <doc_id>78</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each of these comes with an associated score.</text>
              <doc_id>79</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>There are two obvious ways to integrate such a dictionary into a phrase-based translation system: (1) Provide the dictionary entries as (weighted) &#8220;sentence&#8221; pairs in the parallel corpus.</text>
              <doc_id>80</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>These &#8220;sentences&#8221; would each contain exactly one word.</text>
              <doc_id>81</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The weighting can be derived from the translation probability from the dictionary mining.</text>
              <doc_id>82</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>(2) Append the phrase table of a baseline phrase-based translation model trained only on source domain data with the word pairs.</text>
              <doc_id>83</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Use the mining probability as the phrase translation probabilities.</text>
              <doc_id>84</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>It turned out in preliminary experiments (on German/Emea) that neither of these approaches worked particularly well.</text>
              <doc_id>85</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The first approach did not work at all, even with fairly extensive hand-tuning of the sentence weights.</text>
              <doc_id>86</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>It often hurt translation performance.</text>
              <doc_id>87</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The second approach did not hurt translation performance, but did not help much either.</text>
              <doc_id>88</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>It led to an average improvement of only about 0.5 Bleu points, on development data.</text>
              <doc_id>89</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This is likely because weight tuning tuned a single weight to account for the import of the phrase probabilities across both &#8220;true&#8221; phrases as well as these &#8220;mined&#8221; phrases.</text>
              <doc_id>90</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We therefore came up with a slightly more complex, but still simple, method for adding the dictionary entries to the phrase table.</text>
              <doc_id>91</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We add four new features to the model, and set the plain phrasetranslation probabilities for the dictionary entries to zero.</text>
              <doc_id>92</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>These new features are:</text>
              <doc_id>93</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1.</text>
              <doc_id>94</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The dictionary mining translation probability.</text>
              <doc_id>95</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>(Zero for original phrase pairs.</text>
              <doc_id>96</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>)</text>
              <doc_id>97</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.</text>
              <doc_id>98</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>An indicator feature that says whether all German words in this phrase pair were seen in the source data.</text>
              <doc_id>99</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>(This will always be true for source phrases and always be false for dictionary entries.</text>
              <doc_id>100</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>)</text>
              <doc_id>101</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3.</text>
              <doc_id>102</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>An indicator that says whether all German words in this phrase pair were seen in target data.</text>
              <doc_id>103</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>(This is not the negation of the previous feature, because there are plenty of words in the target data that had also been seen.</text>
              <doc_id>104</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This feature might mean something like &#8220;trust this phrase pair a lot.&#8221;)</text>
              <doc_id>105</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>4.</text>
              <doc_id>106</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The conjunction of the previous two features.</text>
              <doc_id>107</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Interestingly, only adding the first feature was not helpful (performance remained about 0.5 Bleu points above baseline).</text>
              <doc_id>108</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Adding only the last three features (the indicator features) alone did not help at all (performance was roughly on par with baseline).</text>
              <doc_id>109</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Only when all four features were included did performance improve significantly.</text>
              <doc_id>110</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In the results discussed in Section 6.2, we report results on test data using the combination of these four features.</text>
              <doc_id>111</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>6 Experiments</title>
        <text>In all of our experiments, we use two trigram language models. The first is trained on the Gigaword corpus. The second is trained on the English side of the target domain corpus. The two language models are traded-off against each other during weight tuning. In all cases we perform parameter tuning with MERT (Och, 2003), and results are averaged over three runs with different random initializations.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In all of our experiments, we use two trigram language models.</text>
              <doc_id>112</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The first is trained on the Gigaword corpus.</text>
              <doc_id>113</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The second is trained on the English side of the target domain corpus.</text>
              <doc_id>114</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The two language models are traded-off against each other during weight tuning.</text>
              <doc_id>115</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In all cases we perform parameter tuning with MERT (Och, 2003), and results are averaged over three runs with different random initializations.</text>
              <doc_id>116</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>6.1 Baselines and Oracles</title>
            <text>Our first set of experiments is designed to establish baseline performance for the domains. In these experiments, we built a translation model based only on the Parliament proceedings. We then tune it using the small amount of target-domain tuning data and test on the corresponding test data. This is row
BASELINE in Table 3. Next, we build an oracle,
based on using the parallel target domain data. This system, OR in Table 3 is constructed by training a system on a mix of Parliament data and targetdomain data. The last line in this table shows the percent improvement when moving to this oracle system. As we can see, the gains range from tiny (4% relative Bleu points, or1.2 absolute Bleu points for news, which may just be because we have more data) to quite significant (73% for medical texts).
Finally, we consider how much of this gain we could possible hope to realize by our dictionary mining technique. In order to estimate this, we take the OR system, and remove any phrases that contain source-language words that appear in neither
BLEU Meteor News Emea Subs PHP News Emea Subs PHP
BASELINE approach from Table 3.
the Parliament proceedings nor our list of high frequency OOV terms. In other words, if our dictionary mining system found as-good translations for the words in its list as the (cheating) oracle system, this is how well it would do. This is referred to as OR-OOV in Table 3. As we can see, the upper bound on performance based only on mining unseen words is about halfway (absolute) between the baseline and the full Oracle. Except in news, when it is essentially useless (because the vocabulary differences between news and Parliament proceedings are negligible). (Results using Meteor are analogous, but omitted for space.)</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Our first set of experiments is designed to establish baseline performance for the domains.</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In these experiments, we built a translation model based only on the Parliament proceedings.</text>
                  <doc_id>118</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We then tune it using the small amount of target-domain tuning data and test on the corresponding test data.</text>
                  <doc_id>119</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This is row</text>
                  <doc_id>120</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BASELINE in Table 3.</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Next, we build an oracle,</text>
                  <doc_id>122</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>based on using the parallel target domain data.</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This system, OR in Table 3 is constructed by training a system on a mix of Parliament data and targetdomain data.</text>
                  <doc_id>124</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The last line in this table shows the percent improvement when moving to this oracle system.</text>
                  <doc_id>125</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>As we can see, the gains range from tiny (4% relative Bleu points, or1.2 absolute Bleu points for news, which may just be because we have more data) to quite significant (73% for medical texts).</text>
                  <doc_id>126</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Finally, we consider how much of this gain we could possible hope to realize by our dictionary mining technique.</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In order to estimate this, we take the OR system, and remove any phrases that contain source-language words that appear in neither</text>
                  <doc_id>128</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BLEU Meteor News Emea Subs PHP News Emea Subs PHP</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BASELINE approach from Table 3.</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the Parliament proceedings nor our list of high frequency OOV terms.</text>
                  <doc_id>131</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In other words, if our dictionary mining system found as-good translations for the words in its list as the (cheating) oracle system, this is how well it would do.</text>
                  <doc_id>132</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This is referred to as OR-OOV in Table 3.</text>
                  <doc_id>133</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>As we can see, the upper bound on performance based only on mining unseen words is about halfway (absolute) between the baseline and the full Oracle.</text>
                  <doc_id>134</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Except in news, when it is essentially useless (because the vocabulary differences between news and Parliament proceedings are negligible).</text>
                  <doc_id>135</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>(Results using Meteor are analogous, but omitted for space.</text>
                  <doc_id>136</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>)</text>
                  <doc_id>137</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>6.2 Mining Results</title>
            <text>The results of the dictionary mining experiment, in terms of its effect on translation performance, are shown in Table 4. As we can see, there is a modest improvement in Subtitles and PHP, a markedly large improvement in Emea, and a modest improvement in News. Given how tight the ORACLE results were to the BASELINE results in Subs and PHP, it is quite impressive that we were able to improve performance as much as we did. In general, across all the data sets and both languages, we roughly split the difference (in absolute terms) between the
BASELINE and ORACLE-OOV systems.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The results of the dictionary mining experiment, in terms of its effect on translation performance, are shown in Table 4.</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As we can see, there is a modest improvement in Subtitles and PHP, a markedly large improvement in Emea, and a modest improvement in News.</text>
                  <doc_id>139</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Given how tight the ORACLE results were to the BASELINE results in Subs and PHP, it is quite impressive that we were able to improve performance as much as we did.</text>
                  <doc_id>140</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In general, across all the data sets and both languages, we roughly split the difference (in absolute terms) between the</text>
                  <doc_id>141</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BASELINE and ORACLE-OOV systems.</text>
                  <doc_id>142</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>7</index>
        <title>7 Discussion</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>143</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>8</index>
        <title>BASELINE and ORACLE-OOV systems. 7 Discussion</title>
        <text>In this paper we have shown that dictionary mining techniques can be applied to mine unseen words in a domain adaptation task. We have seen positive, consistent results across two languages and four domains. The proposed approach is generic enough to be integrated into a wide variety of translation systems other than simple phrase-based translation. Of course, unseen words are not the only cause of translation divergence between two domains. We have not addressed other issues, such as better estimation of translation probabilities or words that change word sense across domains. The former is precisely the area to which one might apply domain adaptation techniques from the machine learning community. The latter requires significant additional work, since it is quite a bit more difficult to spot foreign language words that are used in new senses, rather that just never seen before. An alternative area of work is to extend these results beyond simply the top-most-frequent words in the target domain.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper we have shown that dictionary mining techniques can be applied to mine unseen words in a domain adaptation task.</text>
              <doc_id>144</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We have seen positive, consistent results across two languages and four domains.</text>
              <doc_id>145</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The proposed approach is generic enough to be integrated into a wide variety of translation systems other than simple phrase-based translation.</text>
              <doc_id>146</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Of course, unseen words are not the only cause of translation divergence between two domains.</text>
              <doc_id>147</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We have not addressed other issues, such as better estimation of translation probabilities or words that change word sense across domains.</text>
              <doc_id>148</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The former is precisely the area to which one might apply domain adaptation techniques from the machine learning community.</text>
              <doc_id>149</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The latter requires significant additional work, since it is quite a bit more difficult to spot foreign language words that are used in new senses, rather that just never seen before.</text>
              <doc_id>150</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>An alternative area of work is to extend these results beyond simply the top-most-frequent words in the target domain.</text>
              <doc_id>151</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: For each domain, the percentage of target domain word tokens that are unseen in the source domain, together with the most frequent English words in the target domains that do not appear in the source domain. (In the actual data the subtitles words do not appear censored.)</caption>
        <reference_text></reference_text>
        <page_num>1</page_num>
        <head>
          <rows>
            <row>
              <cell>Our</cell>
              <cell>source</cell>
              <cell>domain</cell>
              <cell>is</cell>
              <cell>European</cell>
              <cell>Parliament</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>proceedings</cell>
              <cell>(http://www.statmt.org/</cell>
            </row>
            <row>
              <cell>europarl/). We use three target domains: the</cell>
            </row>
            <row>
              <cell>News Commentary corpus (News) used in the MT</cell>
            </row>
            <row>
              <cell>Shared task at ACL 2007, European Medicines</cell>
            </row>
            <row>
              <cell>Agency text (Emea),</cell>
              <cell>the Open Subtitles</cell>
              <cell>data</cell>
            </row>
            <row>
              <cell>(Subs) and the PHP technical document data,</cell>
            </row>
            <row>
              <cell>provided as part of the OPUS corpus http:</cell>
            </row>
            <row>
              <cell>//urd.let.rug.nl/tiedeman/OPUS/).</cell>
            </row>
            <row>
              <cell>We extracted development and test sets from each</cell>
            </row>
            <row>
              <cell>of these corpora, except for news (and the source</cell>
            </row>
            <row>
              <cell>domain) where we preserved the published dev and</cell>
            </row>
            <row>
              <cell>test data. The &#8220;source&#8221; domain of Europarl has 996k</cell>
            </row>
            <row>
              <cell>sentences and 2130k words.) We count the number</cell>
            </row>
            <row>
              <cell>of words and sentences in the English side of the</cell>
            </row>
            <row>
              <cell>parallel data, which is the same for both language</cell>
            </row>
            <row>
              <cell>pairs (i.e. both French-English and German-English</cell>
            </row>
            <row>
              <cell>have the same English). The statistics are:</cell>
            </row>
            <row>
              <cell>Comparable</cell>
              <cell>Tune</cell>
              <cell>Test</cell>
            </row>
            <row>
              <cell></cell>
              <cell>sents</cell>
              <cell>words</cell>
              <cell>sents</cell>
              <cell>sents</cell>
            </row>
            <row>
              <cell>News</cell>
              <cell>35k</cell>
              <cell>753k</cell>
              <cell>1057</cell>
              <cell>2007</cell>
            </row>
            <row>
              <cell>Emea</cell>
              <cell>307k</cell>
              <cell>4220k</cell>
              <cell>1388</cell>
              <cell>4145</cell>
            </row>
            <row>
              <cell>Subs</cell>
              <cell>30k</cell>
              <cell>237k</cell>
              <cell>1545</cell>
              <cell>2493</cell>
            </row>
            <row>
              <cell>PHP</cell>
              <cell>6k</cell>
              <cell>81k</cell>
              <cell>1007</cell>
              <cell>2000</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Random unseen Emea words in German and their mined translations.</caption>
        <reference_text>In PAGE 3: ... The dictionary min- ing, viewed subjectively and intrinsically, performs quite well. In  Table2 , we show four randomly se- lected unseen German words from Emea (that do not occur in the Parliament data), together with the top three translations and associated scores (which are not normalized). Based on a cursory evaluation of 5 randomly selected words in French and German...</reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>waste#@#@</cell>
              <cell>blutdruckabfall</cell>
              <cell>0.274233</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>bleeding</cell>
              <cell>blutdruckabfall</cell>
              <cell>0.206440</cell>
            </row>
            <row>
              <cell>stroke</cell>
              <cell>blutdruckabfall</cell>
              <cell>0.190345</cell>
            </row>
            <row>
              <cell>dysphagia</cell>
              <cell>dysphagie</cell>
              <cell>0.233743</cell>
            </row>
            <row>
              <cell>encephalopathy</cell>
              <cell>dysphagie</cell>
              <cell>0.215684</cell>
            </row>
            <row>
              <cell>lethargy</cell>
              <cell>dysphagie</cell>
              <cell>0.203176</cell>
            </row>
            <row>
              <cell>ribavirin</cell>
              <cell>ribavirin</cell>
              <cell>0.314273</cell>
            </row>
            <row>
              <cell>viraferonpeg</cell>
              <cell>ribavirin</cell>
              <cell>0.206194</cell>
            </row>
            <row>
              <cell>bioavailability</cell>
              <cell>verfgbarkeit</cell>
              <cell>0.409260</cell>
            </row>
            <row>
              <cell>xeristar</cell>
              <cell>xeristar</cell>
              <cell>0.325458</cell>
            </row>
            <row>
              <cell>cymbalta</cell>
              <cell>xeristar</cell>
              <cell>0.284616</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Baseline and oracle scores. The last two rows are the change between the baseline and the two types of oracles, averaged over the two languages.</caption>
        <reference_text>In PAGE 4: ... Next, we build an oracle, based on using the parallel target domain data. This system, OR in  Table3  is constructed by training a system on a mix of Parliament data and target- domain data. The last line in this table shows the percent improvement when moving to this oracle system....  In PAGE 5: ... In other words, if our dictio- nary mining system found as-good translations for the words in its list as the (cheating) oracle system, this is how well it would do. This is referred to as OR-OOV in  Table3 . As we can see, the upper bound on performance based only on mining unseen words is about halfway (absolute) between the base- line and the full Oracle....</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>None</cell>
              <cell>BASELINE</cell>
              <cell>News   23.00</cell>
              <cell>BLEU  Emea   26.62</cell>
              <cell>BLEU   Subs   10.26</cell>
              <cell>PHP   38.67</cell>
              <cell>News   34.58</cell>
              <cell>Meteor  Emea   27.69</cell>
              <cell>Meteor   Subs   15.96</cell>
              <cell>PHP    24.66</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>German</cell>
              <cell>ORACLE-OOV</cell>
              <cell>23.77</cell>
              <cell>33.37</cell>
              <cell>11.20</cell>
              <cell>39.77</cell>
              <cell>34.83</cell>
              <cell>30.99</cell>
              <cell>17.03</cell>
              <cell>25.82</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>ORACLE</cell>
              <cell>24.62</cell>
              <cell>42.77</cell>
              <cell>11.45</cell>
              <cell>41.01</cell>
              <cell>35.46</cell>
              <cell>36.40</cell>
              <cell>17.80</cell>
              <cell>25.85</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>BASELINE</cell>
              <cell>27.30</cell>
              <cell>40.46</cell>
              <cell>16.91</cell>
              <cell>28.12</cell>
              <cell>37.31</cell>
              <cell>35.62</cell>
              <cell>20.61</cell>
              <cell>20.47</cell>
            </row>
            <row>
              <cell>French</cell>
              <cell>ORACLE-OOV</cell>
              <cell>27.92</cell>
              <cell>50.03</cell>
              <cell>19.17</cell>
              <cell>29.48</cell>
              <cell>37.57</cell>
              <cell>39.55</cell>
              <cell>21.79</cell>
              <cell>20.91</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>ORACLE</cell>
              <cell>28.55</cell>
              <cell>59.49</cell>
              <cell>19.81</cell>
              <cell>30.15</cell>
              <cell>38.12</cell>
              <cell>45.55</cell>
              <cell>23.52</cell>
              <cell>21.77</cell>
            </row>
            <row>
              <cell>ORACLE-OOV CHANGE</cell>
              <cell>ORACLE-OOV CHANGE</cell>
              <cell>+2%</cell>
              <cell>+24%</cell>
              <cell>+11%</cell>
              <cell>+5%</cell>
              <cell>+0%</cell>
              <cell>+12%</cell>
              <cell>+6%</cell>
              <cell>+7%</cell>
            </row>
            <row>
              <cell>ORACLE CHANGE</cell>
              <cell>ORACLE CHANGE</cell>
              <cell>+4%</cell>
              <cell>+73%</cell>
              <cell>+15%</cell>
              <cell>+2%</cell>
              <cell>+2%</cell>
              <cell>+29%</cell>
              <cell>+13%</cell>
              <cell>+6%</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: Dictionary-mining system results. The italicized number beneath each score is the improvement over the</caption>
        <reference_text>In PAGE 5: ...) 6.2 Mining Results The results of the dictionary mining experiment, in terms of its effect on translation performance, are shown in  Table4 . As we can see, there is a mod- est improvement in Subtitles and PHP, a markedly large improvement in Emea, and a modest improve- ment in News....</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>German</cell>
              <cell>French</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell></cell>
              <cell>BLEU</cell>
              <cell>Meteor</cell>
              <cell>BLEU</cell>
              <cell>Meteor</cell>
            </row>
            <row>
              <cell>News</cell>
              <cell>23.80</cell>
              <cell>35.53</cell>
              <cell>27.66</cell>
              <cell>37.41</cell>
            </row>
            <row>
              <cell></cell>
              <cell>+0.80</cell>
              <cell>+0.95</cell>
              <cell>+0.36</cell>
              <cell>+0.10</cell>
            </row>
            <row>
              <cell>Emea</cell>
              <cell>28.06</cell>
              <cell>29.18</cell>
              <cell>46.17</cell>
              <cell>37.38</cell>
            </row>
            <row>
              <cell></cell>
              <cell>+1.44</cell>
              <cell>+1.49</cell>
              <cell>+1.51</cell>
              <cell>+1.76</cell>
            </row>
            <row>
              <cell>Subs</cell>
              <cell>10.39</cell>
              <cell>16.27</cell>
              <cell>17.52</cell>
              <cell>21.11</cell>
            </row>
            <row>
              <cell></cell>
              <cell>+0.13</cell>
              <cell>+0.31</cell>
              <cell>+0.61</cell>
              <cell>+0.50</cell>
            </row>
            <row>
              <cell>PHP</cell>
              <cell>38.95</cell>
              <cell>25.53</cell>
              <cell>28.80</cell>
              <cell>20.82</cell>
            </row>
            <row>
              <cell></cell>
              <cell>+0.28</cell>
              <cell>+0.88</cell>
              <cell>+0.68</cell>
              <cell>+0.35</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Satanjeev Banerjee</author>
          <author>Alon Lavie</author>
        </authors>
        <title>Meteor: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
        <publication>In In Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at ACL.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Nicola Bertoldi</author>
          <author>Marcello Federico</author>
        </authors>
        <title>Domain adaptation for statistical machine translation with monolingual resources.</title>
        <publication>In StatMT &#8217;09: Proceedings of the Fourth Workshop on Statistical Machine Translation.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>John Blitzer</author>
          <author>Hal Daum&#233;</author>
        </authors>
        <title>None</title>
        <publication>Domain adaptation. Tutorial at the International Conference on Machine Learning, http:</publication>
        <pages>//adaptationtutorial.blitzer.com/.</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Jorge Civera</author>
          <author>Alfons Juan</author>
        </authors>
        <title>Domain adaptation in statistical machine translation with mixture modelling.</title>
        <publication>In StatMT &#8217;07: Proceedings of the Second Workshop on Statistical Machine Translation.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Aria Haghighi</author>
          <author>Percy Liang</author>
          <author>Taylor Berg-Kirkpatrick</author>
          <author>Dan Klein</author>
        </authors>
        <title>Learning bilingual lexicons from monolingual corpora.</title>
        <publication>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Almut Silja Hildebrand</author>
          <author>Matthias Eck</author>
          <author>Stephan Vogel</author>
          <author>Alex Waibel</author>
        </authors>
        <title>Adaptation of the translation model for statistical machine translation based on information retrieval.</title>
        <publication>In European Association for Machine Translation.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>H Hotelling</author>
        </authors>
        <title>Relation between two sets of variables.</title>
        <publication>None</publication>
        <pages>28--322</pages>
        <date>1936</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>J Jiang</author>
        </authors>
        <title>A literature survey on domain adaptation of statistical classifiers. Available at http://sifaka.cs.uiuc.edu/jiang4/ domain_adaptation/survey.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>R Jonker</author>
          <author>A Volgenant</author>
        </authors>
        <title>A shortest augmenting path algorithm for dense and sparse linear assignment problems.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1987</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Josh Schroeder</author>
        </authors>
        <title>Experiments in domain adaptation for statistical machine translation.</title>
        <publication>In StatMT &#8217;07: Proceedings of the Second Workshop on Statistical Machine Translation.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Hieu Hoang</author>
          <author>Alexandra Birch</author>
          <author>Chris Callison-Burch</author>
          <author>Marcello Federico</author>
          <author>Nicola Bertoldi</author>
          <author>Brooke Cowan</author>
          <author>Wade Shen</author>
        </authors>
        <title>Moses: Open source toolkit for statistical machine translation.</title>
        <publication>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum error rate training for statistical machine translation.</title>
        <publication>In Proceedings of the Conference of the Association for Computational Linguistics (ACL),</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>Bleu: a method for automatic evaluation of machine translation.</title>
        <publication>In Proceedings of the Conference of the Association for Computational Linguistics (ACL),</publication>
        <pages>311--318</pages>
        <date>2002</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Banerjee and Lavie, 2005</string>
        <sentence_id>33564</sentence_id>
        <char_offset>117</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Bertoldi and Federico, 2009</string>
        <sentence_id>33576</sentence_id>
        <char_offset>182</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>3</reference_id>
        <string>Civera and Juan, 2007</string>
        <sentence_id>33576</sentence_id>
        <char_offset>159</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>4</reference_id>
        <string>Haghighi et al., 2008</string>
        <sentence_id>33551</sentence_id>
        <char_offset>88</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>4</reference_id>
        <string>Haghighi et al., 2008</string>
        <sentence_id>33588</sentence_id>
        <char_offset>98</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>4</reference_id>
        <string>Haghighi et al., 2008</string>
        <sentence_id>33593</sentence_id>
        <char_offset>83</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>5</reference_id>
        <string>Hildebrand et al., 2005</string>
        <sentence_id>33575</sentence_id>
        <char_offset>226</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>6</reference_id>
        <string>Hotelling, 1936</string>
        <sentence_id>33590</sentence_id>
        <char_offset>32</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>7</reference_id>
        <string>Jiang, 2008</string>
        <sentence_id>33555</sentence_id>
        <char_offset>69</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>8</reference_id>
        <string>Jonker and Volgenant, 1987</string>
        <sentence_id>33618</sentence_id>
        <char_offset>80</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>9</reference_id>
        <string>Koehn and Schroeder, 2007</string>
        <sentence_id>33576</sentence_id>
        <char_offset>92</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>10</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>33559</sentence_id>
        <char_offset>101</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>11</reference_id>
        <string>Och, 2003</string>
        <sentence_id>33567</sentence_id>
        <char_offset>140</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>11</reference_id>
        <string>Och, 2003</string>
        <sentence_id>33692</sentence_id>
        <char_offset>52</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>12</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>33564</sentence_id>
        <char_offset>71</char_offset>
      </citation>
    </citations>
  </content>
</document>
