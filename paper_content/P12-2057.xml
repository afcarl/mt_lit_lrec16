<document>
  <filename>P12-2057</filename>
  <authors>
    <author>of Computer</author>
    <author>Radio Comms Engineering</author>
    <author>Korea University</author>
  </authors>
  <title>Translation Model Size Reduction for Hierarchical Phrase-based Statistical Machine Translation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>In this paper, we propose a novel method of reducing the size of translation model for hierarchical phrase-based machine translation systems. Previous approaches try to prune infrequent entries or unreliable entries based on statistics, but cause a problem of reducing the translation coverage. On the contrary, the proposed method try to prune only ineffective entries based on the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible. Experimental results on Chinese-to- English machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper, we propose a novel method of reducing the size of translation model for hierarchical phrase-based machine translation systems.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Previous approaches try to prune infrequent entries or unreliable entries based on statistics, but cause a problem of reducing the translation coverage.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>On the contrary, the proposed method try to prune only ineffective entries based on the estimation of the information redundancy encoded in phrase pairs and hierarchical rules, and thus preserve the search space of SMT decoders as much as possible.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Experimental results on Chinese-to- English machine translation tasks show that our method is able to reduce almost the half size of the translation model with very tiny degradation of translation performance.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Statistical Machine Translation (SMT) has gained considerable attention during last decades. From a bilingual corpus, all translation knowledge can be acquired automatically in SMT framework. Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs. This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus. However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition. According to Bloodgood and Callison-Burch (2010) and our own preliminary experiments, the size of phrase table and hierarchical rule table consistently increases linearly with the growth of training size, while the translation performance tends to gain minor improvement after a certain point. Consequently, the model size reduction is necessary and meaningful for SMT systems if it can be performed without significant performance degradation. The smaller the model size is, the faster the SMT decoding speed is, because there are fewer hypotheses to be investigated during decoding. Especially, in a limited environment, such as mobile device, and for a time-urgent task, such as speech-to-speech translation, the compact size of translation rules is required. In this case, the model reduction would be the one of the main techniques we have to consider.
Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010). Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009). These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation. The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules. In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores of entries and scores of their substituents. Suppose that the source phrase s 1 s 2 is always translated into t 1 t 2 with phrase entry &lt;s 1 s 2 &#8594;t 1 t 2 &gt; where s i and t i are correspond-
ing translations. Similarly, source phrases s 1 and s 2 are always translated into t 1 and t 2 , with phrase entries, &lt;s 1 &#8594;t 1 &gt; and &lt;s 2 &#8594;t 2 &gt;, respectively. In this case, it is intuitive that &lt;s 1 s 2 &#8594;t 1 t 2 &gt; could be unnecessary and redundant since its substituent always produces the same result. This paper presents statistical analysis of this redundancy measurement. The redundancy-based reduction can be performed to prune the phrase table, the hierarchical rule table, and both. Since the similar translation knowledge is accumulated at both of tables during the training stage, our reduction method performs effectively and safely. Unlike previous studies solely focus on either phrase table or hierarchical rule table, this work is the first attempt to reduce phrases and hierarchical rules simultaneously.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Statistical Machine Translation (SMT) has gained considerable attention during last decades.</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>From a bilingual corpus, all translation knowledge can be acquired automatically in SMT framework.</text>
              <doc_id>5</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Phrasebased model (Koehn et al., 2003) and hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) show state-of-the-art performance in various language pairs.</text>
              <doc_id>6</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This achievement is mainly benefit from huge size of translational knowledge extracted from sufficient parallel corpus.</text>
              <doc_id>7</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>However, the errors of automatic word alignment and non-parallelized bilingual sentence pairs sometimes have caused the unreliable and unnecessary translation rule acquisition.</text>
              <doc_id>8</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>According to Bloodgood and Callison-Burch (2010) and our own preliminary experiments, the size of phrase table and hierarchical rule table consistently increases linearly with the growth of training size, while the translation performance tends to gain minor improvement after a certain point.</text>
              <doc_id>9</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Consequently, the model size reduction is necessary and meaningful for SMT systems if it can be performed without significant performance degradation.</text>
              <doc_id>10</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The smaller the model size is, the faster the SMT decoding speed is, because there are fewer hypotheses to be investigated during decoding.</text>
              <doc_id>11</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Especially, in a limited environment, such as mobile device, and for a time-urgent task, such as speech-to-speech translation, the compact size of translation rules is required.</text>
              <doc_id>12</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>In this case, the model reduction would be the one of the main techniques we have to consider.</text>
              <doc_id>13</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Previous methods of reducing the size of SMT model try to identify infrequent entries (Zollmann et al., 2008; Huang and Xiang, 2010).</text>
              <doc_id>14</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Several statistical significance testing methods are also examined to detect unreliable noisy entries (Tomeh et al., 2009; Johnson et al., 2007; Yang and Zheng, 2009).</text>
              <doc_id>15</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>These methods could harm the translation performance due to their side effect of algorithms; similar multiple entries can be pruned at the same time deteriorating potential coverage of translation.</text>
              <doc_id>16</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The proposed method, on the other hand, tries to measure the redundancy of phrase pairs and hierarchical rules.</text>
              <doc_id>17</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In this work, redundancy of an entry is defined as its translational ineffectiveness, and estimated by comparing scores of entries and scores of their substituents.</text>
              <doc_id>18</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Suppose that the source phrase s 1 s 2 is always translated into t 1 t 2 with phrase entry &lt;s 1 s 2 &#8594;t 1 t 2 &gt; where s i and t i are correspond-</text>
              <doc_id>19</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>ing translations.</text>
              <doc_id>20</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Similarly, source phrases s 1 and s 2 are always translated into t 1 and t 2 , with phrase entries, &lt;s 1 &#8594;t 1 &gt; and &lt;s 2 &#8594;t 2 &gt;, respectively.</text>
              <doc_id>21</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In this case, it is intuitive that &lt;s 1 s 2 &#8594;t 1 t 2 &gt; could be unnecessary and redundant since its substituent always produces the same result.</text>
              <doc_id>22</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This paper presents statistical analysis of this redundancy measurement.</text>
              <doc_id>23</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The redundancy-based reduction can be performed to prune the phrase table, the hierarchical rule table, and both.</text>
              <doc_id>24</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Since the similar translation knowledge is accumulated at both of tables during the training stage, our reduction method performs effectively and safely.</text>
              <doc_id>25</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Unlike previous studies solely focus on either phrase table or hierarchical rule table, this work is the first attempt to reduce phrases and hierarchical rules simultaneously.</text>
              <doc_id>26</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Proposed Model</title>
        <text>Given an original translation model, TM, our goal is to find the optimally reduced translation model, TM &#8727; , which minimizes the degradation of translation performance. To measure the performance degradation, we introduce a new metric named consistency:
C(TM,TM &#8727; ) =
BLEU(D(s;TM),D(s;TM &#8727; )) (1)
where the function D produces the target sentence of the source sentences, given the translation model T M. Consistency measures the similarity between the two groups of decoded target sentences produced by two different translation models. There are number of similarity metrics such as Dices coefficient (Kondrak et al., 2003), and Jaccard similarity coefficient. Instead, we use BLEU scores (Papineni et al., 2002) since it is one of the primary metrics for machine translation evaluation. Note that our consistency does not require the reference set while the original BLEU does. This means that only (abundant) source-side monolingual corpus is needed to predict performance degradation. Now, our goal can be rewritten with this metric; among all the possible reduced models, we want to find the set which can maximize the consistency:
TM &#8727; = argmax C(TM,TM &#8242; ) (2)
TM &#8242; &#8834;TM
In minimum error rate training (MERT) stages, a development set, which consists of bilingual sentences, is used to find out the best weights of features (Och, 2003). One characteristic of our method is that it isolates feature weights of the translation model from SMT log-linear model, trying to minimize the impact of search path during decoding. The reduction procedure consists of three stages: translation scoring, redundancy estimation, and redundancy-based reduction. Our reduction method starts with measuring the translation scores of the individual phrase and the hierarchical rule. Similar to the decoder, the scoring scheme is based on the log-linear framework:
PS(p) = &#8721; i &#955; i h i (p) (3)
where h is a feature function and &#955; is its weight. As the conventional hierarchical phrase-based SMT model, our features are composed ofP (e|f),P (f|e), P lex (e|f), P lex (f|e), and the number of phrases, where e and f denote a source phrase and a target phrase, respectively. P lex is the lexicalized probability. In a similar manner, the translation scores of hierarchical rules are calculated as follows:
HS(r) = &#8721; i &#955; i h i (r) (4)
The features are as same as those that are used for phrase scoring, except the last feature. Instead of the phrase number penalty, the hierarchical rule number penalty is used. The weight for each feature is shared from the results of MERT. With this scoring scheme, our model is able to measure how important the individual entry is during decoding.
Once translation scores for all entries are estimated, our method retrieves substituent candidates with their combination scores. The combination score is calculated by accumulating translation scores of every member as follows:
CS(p 1...n ) = n&#8721; PS(p i ) (5)
i=1
This scoring scheme follows the same manner what the conventional decoder does, finding the best phrase combination during translation. By comparing the original translation score with combination
scores of its substituents, the redundancy scores are estimated, as follows:
Red(p) = min PS(p)&#8722;CS(p 1...n) (6)
p 1...n &#8712;Sub(p)
where Sub is the function that retrieves all possible substituents (the combinations of sub-phrases, and/or sub-rules that exactly produce the same target phrase, given the source phrase p). If the combination score of the best substituent is same as the translation score of p, the redundancy score becomes zero. In this case, the decoder always produces the same translation results without p. When the redundancy score is negative, the best substituent is more likely to be chosen instead of p. This implies that there is no risk to prune p; the search space is not changed, and the search path is not changed as well.
Our method can be varied according to the designation of Sub function. If both of the phrase table and the hierarchical rule table are allowed, cross reduction can be possible; the phrase table is reduced based on the hierarchical rule table and vice versa. With extensions of combination scoring and redundancy scoring schemes like following equations, our model is able to perform cross reduction.
CS(p 1...n ,h 1...m ) = n&#8721; PS(p i )+
Red(p) =
i=1
min
&lt;p 1...n ,h 1...m &gt;&#8712;Sub(p)
m&#8721; HS(h i ) (7)
i=1
PS(p)&#8722;CS(p 1...n ,h 1...m ) (8)
The proposed method has some restrictions for reduction. First of all, it does not try to prune the phrase that has no substituents, such as unigram phrases; the phrase whose source part is composed of a single word. This restriction guarantees that the translational coverage of the reduced model is as high as those of the original translation model. In addition, our model does not prune the phrases and the hierarchical rules that have reordering within it to prevent information loss of reordering. For instance, if we prune phrase, &lt;s 1 s 2 s 3 &#8594;t 3 t 1 t 2 &gt;, phrases, &lt;s 1 s 2 &#8594;t 1 t 2 &gt; and &lt;s 3 &#8594;t 3 &gt; are not able to produce the same target words without appropriate reordering.
Once the redundancy scores for all entries have been estimated, the next step is to select the best N entries to prune to satisfy a desired model size. We can simply prune the first N from the list of entries sorted by increasing order of redundancy score. However, this method may not result in the optimal reduction, since each redundancy scores are estimated based on the assumption of the existence of all the other entries. In other words, there are dependency relationships among entries. We examine two methods to deal with this problem. The first is to ignore dependency, which is the more efficient manner. The other is to prune independent entries first. After all independent entries are pruned, the dependent entries are started to be pruned. We present the effectiveness of each method in the next section.
Since our goal is to reduce the size of all translation models, the reduction is needed to be performed for both the phrase table and the hierarchical rule table simultaneously, namely joint reduction. Similar to phrase reduction and hierarchical rule reduction, it selects the best N entries of the mixture of phrase and hierarchical rules. This method results in safer pruning; once a phrase is determined to be pruned, the hierarchical rules, which are related to this phrase, are likely to be kept, and vice versa.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Given an original translation model, TM, our goal is to find the optimally reduced translation model, TM &#8727; , which minimizes the degradation of translation performance.</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To measure the performance degradation, we introduce a new metric named consistency:</text>
              <doc_id>28</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>C(TM,TM &#8727; ) =</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>BLEU(D(s;TM),D(s;TM &#8727; )) (1)</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where the function D produces the target sentence of the source sentences, given the translation model T M.</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Consistency measures the similarity between the two groups of decoded target sentences produced by two different translation models.</text>
              <doc_id>32</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>There are number of similarity metrics such as Dices coefficient (Kondrak et al., 2003), and Jaccard similarity coefficient.</text>
              <doc_id>33</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Instead, we use BLEU scores (Papineni et al., 2002) since it is one of the primary metrics for machine translation evaluation.</text>
              <doc_id>34</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Note that our consistency does not require the reference set while the original BLEU does.</text>
              <doc_id>35</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This means that only (abundant) source-side monolingual corpus is needed to predict performance degradation.</text>
              <doc_id>36</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Now, our goal can be rewritten with this metric; among all the possible reduced models, we want to find the set which can maximize the consistency:</text>
              <doc_id>37</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>TM &#8727; = argmax C(TM,TM &#8242; ) (2)</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>TM &#8242; &#8834;TM</text>
              <doc_id>39</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In minimum error rate training (MERT) stages, a development set, which consists of bilingual sentences, is used to find out the best weights of features (Och, 2003).</text>
              <doc_id>40</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>One characteristic of our method is that it isolates feature weights of the translation model from SMT log-linear model, trying to minimize the impact of search path during decoding.</text>
              <doc_id>41</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The reduction procedure consists of three stages: translation scoring, redundancy estimation, and redundancy-based reduction.</text>
              <doc_id>42</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Our reduction method starts with measuring the translation scores of the individual phrase and the hierarchical rule.</text>
              <doc_id>43</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Similar to the decoder, the scoring scheme is based on the log-linear framework:</text>
              <doc_id>44</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>PS(p) = &#8721; i &#955; i h i (p) (3)</text>
              <doc_id>45</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where h is a feature function and &#955; is its weight.</text>
              <doc_id>46</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As the conventional hierarchical phrase-based SMT model, our features are composed ofP (e|f),P (f|e), P lex (e|f), P lex (f|e), and the number of phrases, where e and f denote a source phrase and a target phrase, respectively.</text>
              <doc_id>47</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>P lex is the lexicalized probability.</text>
              <doc_id>48</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In a similar manner, the translation scores of hierarchical rules are calculated as follows:</text>
              <doc_id>49</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>HS(r) = &#8721; i &#955; i h i (r) (4)</text>
              <doc_id>50</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The features are as same as those that are used for phrase scoring, except the last feature.</text>
              <doc_id>51</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Instead of the phrase number penalty, the hierarchical rule number penalty is used.</text>
              <doc_id>52</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The weight for each feature is shared from the results of MERT.</text>
              <doc_id>53</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>With this scoring scheme, our model is able to measure how important the individual entry is during decoding.</text>
              <doc_id>54</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Once translation scores for all entries are estimated, our method retrieves substituent candidates with their combination scores.</text>
              <doc_id>55</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The combination score is calculated by accumulating translation scores of every member as follows:</text>
              <doc_id>56</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>CS(p 1...n ) = n&#8721; PS(p i ) (5)</text>
              <doc_id>57</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1</text>
              <doc_id>58</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>This scoring scheme follows the same manner what the conventional decoder does, finding the best phrase combination during translation.</text>
              <doc_id>59</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>By comparing the original translation score with combination</text>
              <doc_id>60</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>scores of its substituents, the redundancy scores are estimated, as follows:</text>
              <doc_id>61</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Red(p) = min PS(p)&#8722;CS(p 1...n) (6)</text>
              <doc_id>62</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>p 1...n &#8712;Sub(p)</text>
              <doc_id>63</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where Sub is the function that retrieves all possible substituents (the combinations of sub-phrases, and/or sub-rules that exactly produce the same target phrase, given the source phrase p).</text>
              <doc_id>64</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>If the combination score of the best substituent is same as the translation score of p, the redundancy score becomes zero.</text>
              <doc_id>65</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In this case, the decoder always produces the same translation results without p.</text>
              <doc_id>66</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>When the redundancy score is negative, the best substituent is more likely to be chosen instead of p.</text>
              <doc_id>67</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This implies that there is no risk to prune p; the search space is not changed, and the search path is not changed as well.</text>
              <doc_id>68</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Our method can be varied according to the designation of Sub function.</text>
              <doc_id>69</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>If both of the phrase table and the hierarchical rule table are allowed, cross reduction can be possible; the phrase table is reduced based on the hierarchical rule table and vice versa.</text>
              <doc_id>70</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>With extensions of combination scoring and redundancy scoring schemes like following equations, our model is able to perform cross reduction.</text>
              <doc_id>71</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>CS(p 1...n ,h 1...m ) = n&#8721; PS(p i )+</text>
              <doc_id>72</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Red(p) =</text>
              <doc_id>73</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1</text>
              <doc_id>74</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>min</text>
              <doc_id>75</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&lt;p 1...n ,h 1...m &gt;&#8712;Sub(p)</text>
              <doc_id>76</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>m&#8721; HS(h i ) (7)</text>
              <doc_id>77</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1</text>
              <doc_id>78</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>PS(p)&#8722;CS(p 1...n ,h 1...m ) (8)</text>
              <doc_id>79</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The proposed method has some restrictions for reduction.</text>
              <doc_id>80</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>First of all, it does not try to prune the phrase that has no substituents, such as unigram phrases; the phrase whose source part is composed of a single word.</text>
              <doc_id>81</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This restriction guarantees that the translational coverage of the reduced model is as high as those of the original translation model.</text>
              <doc_id>82</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In addition, our model does not prune the phrases and the hierarchical rules that have reordering within it to prevent information loss of reordering.</text>
              <doc_id>83</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>For instance, if we prune phrase, &lt;s 1 s 2 s 3 &#8594;t 3 t 1 t 2 &gt;, phrases, &lt;s 1 s 2 &#8594;t 1 t 2 &gt; and &lt;s 3 &#8594;t 3 &gt; are not able to produce the same target words without appropriate reordering.</text>
              <doc_id>84</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Once the redundancy scores for all entries have been estimated, the next step is to select the best N entries to prune to satisfy a desired model size.</text>
              <doc_id>85</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We can simply prune the first N from the list of entries sorted by increasing order of redundancy score.</text>
              <doc_id>86</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, this method may not result in the optimal reduction, since each redundancy scores are estimated based on the assumption of the existence of all the other entries.</text>
              <doc_id>87</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In other words, there are dependency relationships among entries.</text>
              <doc_id>88</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We examine two methods to deal with this problem.</text>
              <doc_id>89</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The first is to ignore dependency, which is the more efficient manner.</text>
              <doc_id>90</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The other is to prune independent entries first.</text>
              <doc_id>91</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>After all independent entries are pruned, the dependent entries are started to be pruned.</text>
              <doc_id>92</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>We present the effectiveness of each method in the next section.</text>
              <doc_id>93</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Since our goal is to reduce the size of all translation models, the reduction is needed to be performed for both the phrase table and the hierarchical rule table simultaneously, namely joint reduction.</text>
              <doc_id>94</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Similar to phrase reduction and hierarchical rule reduction, it selects the best N entries of the mixture of phrase and hierarchical rules.</text>
              <doc_id>95</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This method results in safer pruning; once a phrase is determined to be pruned, the hierarchical rules, which are related to this phrase, are likely to be kept, and vice versa.</text>
              <doc_id>96</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Experiment</title>
        <text>We investigate the effectiveness of our reduction method by conducting Chinese-to-English translation task. The training data, as same as Cui et al. (2010), consists of about 500K parallel sentence pairs which is a mixture of several datasets published by LDC. NIST 2003 set is used as a development set. NIST 2004, 2005, 2006, and 2008 sets are used for evaluation purpose. For word alignment, we use GIZA++ 1 , an implementation of IBM models (Brown et al., 1993). We have implemented a hierarchical phrase-based SMT model similar to Chiang (2005). The trigram target language model is trained from the Xinhua portion of English Gigaword corpus (Graff and Cieri, 2003). Sampled 10,000 sentences from Chinese Gigaword corpus (Graff, 2007) was used for source-side development dataset to measure consistency. Our main metric for translation performance evaluation is case-
1 http://www.statmt.org/moses/giza/GIZA++.html
1.00
Consistency
0.90
0.80
0.70
0.60
Freq-Cutoff NoDep
Dep CrossNoDep
CrossDep
0.298
BLEU 0.294
0.290
0.286
0% 10% 20% 30% 40% 50% 60% Phrase Reduction Ratio 0% 10% 20% 30% 40% 50% 60%
Hierarchical Rule Reduction Ratio 0% 10% 20% 30% 40% 50% 60% Joint Reduction Ratio
insensitive BLEU-4 scores (Papineni et al., 2002).
As a baseline system, we chose the frequencybased cutoff method, which is one of the most widely used filtering methods. As shown in Figure 1, almost half of the phrases and hierarchical rules are pruned when cutoff=2, while the BLEU score is also deteriorated significantly. We introduced two methods for selecting the N pruning entries considering dependency relationships. The non-dependency method does not consider dependency relationships, while the dependency method prunes independent entries first. Each method can be combined with cross reduction. The performance is measured in three different reduction tasks: phrase reduction, hierarchical rule reduction, and joint reduction. As the reduction ratio becomes higher, the model size, i.e., the number of entries, is reduced while BLEU scores and coverage are decreased. The results show that the translation performance is highly co-related with the consistency. The co-relation scores measured between them on the phrase reduction and the hierarchical rule reduction tasks are 0.99 and 0.95, respectively, which indicates very strong positive relationship.
For the phrase reduction task, the dependency method outperforms the non-dependency method in terms of BLEU score. When the cross reduction technique was used for the phrase reduction task, BLEU score is not deteriorated even when more than half of phrase entries are pruned. This result implies that there is much redundant information stored in the hierarchical rule table. On the other hand, for the hierarchical rule reduction task, the non-dependency method shows the better performance. The dependency method sometimes performs worse than the baseline method. We expect that this is caused by the unreliable estimation of dependency among hierarchical rules since the most of them are automatically generated from the phrases. The excessive dependency of these rules would cause overestimation of hierarchical rule redundancy score.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We investigate the effectiveness of our reduction method by conducting Chinese-to-English translation task.</text>
              <doc_id>97</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The training data, as same as Cui et al. (2010), consists of about 500K parallel sentence pairs which is a mixture of several datasets published by LDC.</text>
              <doc_id>98</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>NIST 2003 set is used as a development set.</text>
              <doc_id>99</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>NIST 2004, 2005, 2006, and 2008 sets are used for evaluation purpose.</text>
              <doc_id>100</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>For word alignment, we use GIZA++ 1 , an implementation of IBM models (Brown et al., 1993).</text>
              <doc_id>101</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We have implemented a hierarchical phrase-based SMT model similar to Chiang (2005).</text>
              <doc_id>102</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The trigram target language model is trained from the Xinhua portion of English Gigaword corpus (Graff and Cieri, 2003).</text>
              <doc_id>103</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Sampled 10,000 sentences from Chinese Gigaword corpus (Graff, 2007) was used for source-side development dataset to measure consistency.</text>
              <doc_id>104</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Our main metric for translation performance evaluation is case-</text>
              <doc_id>105</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 http://www.statmt.org/moses/giza/GIZA++.html</text>
              <doc_id>106</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1.00</text>
              <doc_id>107</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Consistency</text>
              <doc_id>108</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.90</text>
              <doc_id>109</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.80</text>
              <doc_id>110</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.70</text>
              <doc_id>111</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.60</text>
              <doc_id>112</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Freq-Cutoff NoDep</text>
              <doc_id>113</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Dep CrossNoDep</text>
              <doc_id>114</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>CrossDep</text>
              <doc_id>115</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.298</text>
              <doc_id>116</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>BLEU 0.294</text>
              <doc_id>117</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.290</text>
              <doc_id>118</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0.286</text>
              <doc_id>119</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0% 10% 20% 30% 40% 50% 60% Phrase Reduction Ratio 0% 10% 20% 30% 40% 50% 60%</text>
              <doc_id>120</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Hierarchical Rule Reduction Ratio 0% 10% 20% 30% 40% 50% 60% Joint Reduction Ratio</text>
              <doc_id>121</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>insensitive BLEU-4 scores (Papineni et al., 2002).</text>
              <doc_id>122</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>As a baseline system, we chose the frequencybased cutoff method, which is one of the most widely used filtering methods.</text>
              <doc_id>123</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As shown in Figure 1, almost half of the phrases and hierarchical rules are pruned when cutoff=2, while the BLEU score is also deteriorated significantly.</text>
              <doc_id>124</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We introduced two methods for selecting the N pruning entries considering dependency relationships.</text>
              <doc_id>125</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The non-dependency method does not consider dependency relationships, while the dependency method prunes independent entries first.</text>
              <doc_id>126</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Each method can be combined with cross reduction.</text>
              <doc_id>127</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The performance is measured in three different reduction tasks: phrase reduction, hierarchical rule reduction, and joint reduction.</text>
              <doc_id>128</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>As the reduction ratio becomes higher, the model size, i.e., the number of entries, is reduced while BLEU scores and coverage are decreased.</text>
              <doc_id>129</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The results show that the translation performance is highly co-related with the consistency.</text>
              <doc_id>130</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>The co-relation scores measured between them on the phrase reduction and the hierarchical rule reduction tasks are 0.99 and 0.95, respectively, which indicates very strong positive relationship.</text>
              <doc_id>131</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For the phrase reduction task, the dependency method outperforms the non-dependency method in terms of BLEU score.</text>
              <doc_id>132</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>When the cross reduction technique was used for the phrase reduction task, BLEU score is not deteriorated even when more than half of phrase entries are pruned.</text>
              <doc_id>133</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This result implies that there is much redundant information stored in the hierarchical rule table.</text>
              <doc_id>134</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>On the other hand, for the hierarchical rule reduction task, the non-dependency method shows the better performance.</text>
              <doc_id>135</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The dependency method sometimes performs worse than the baseline method.</text>
              <doc_id>136</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We expect that this is caused by the unreliable estimation of dependency among hierarchical rules since the most of them are automatically generated from the phrases.</text>
              <doc_id>137</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The excessive dependency of these rules would cause overestimation of hierarchical rule redundancy score.</text>
              <doc_id>138</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Conclusion</title>
        <text>We present a novel method of reducing the size of translation model for SMT. The contributions of the proposed method are as follows: 1) our method is the first attempt to reduce the phrase table and the hierarchical rule table simultaneously. 2) our method is a safe reduction method since it considers the redundancy, which is the practical ineffectiveness of individual entry. 3) our method shows that almost the half size of the translation model can be reduced without significant performance degradation. It may be appropriate for the applications running on limited environment, e.g., mobile devices.
Acknowledgement
The first author performed this research during an internship at Microsoft Research Asia. This research was supported by the MKE(The Ministry of Knowledge Economy), Korea and Microsoft Research, under IT/SW Creative research program supervised by the NIPA(National IT Industry Promotion Agency). (NIPA-2010-C1810-1002-0025)
References
Michael Bloodgood and Chris Callison-Burch. 2010. Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 854&#8211;864. Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19:263&#8211;311, June. David Chiang. 2005. A Hierarchical Phrase-based Model for Statistical Machine Translation. In Proceedings of the 43th Annual Meeting on Association for Computational Linguistics, pages 263&#8211;270. David Chiang. 2007. Hierarchical Phrase-based Translation. Computational Linguistics, 33:201&#8211;228, June. Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, and
Tiejun Zhao. 2010. Hybrid Decoding: Decoding with Partial Hypotheses Combination Over Multiple SMT Systems. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING &#8217;10, pages 214&#8211;222, Stroudsburg, PA, USA. Association for Computational Linguistics. David Graff and Christopher Cieri. 2003. English Gigaword. In Linguistic Data Consortium, Philadelphia. David Graff. 2007. Chinese Gigaword Third Edition. In
Linguistic Data Consortium, Philadelphia. Fei Huang and Bing Xiang. 2010. Feature-Rich Discriminative Phrase Rescoring for SMT. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING &#8217;10, pages 492&#8211;500, Stroudsburg, PA, USA. Association for Computational Linguistics. Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving Translation Quality by Discarding Most of the Phrasetable. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 967&#8211; 975.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-based Translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL &#8217;03, pages 48&#8211;54, Stroudsburg, PA, USA. Association for Computational Linguistics. Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can Improve Statistical Translation Models. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: companion volume of the Proceedings of HLT- NAACL 2003&#8211;short papers - Volume 2, NAACL-Short &#8217;03, pages 46&#8211;48, Stroudsburg, PA, USA. Association for Computational Linguistics. Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL &#8217;03, pages 160&#8211; 167, Stroudsburg, PA, USA. Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &#8217;02, pages 311&#8211;318, Morristown, NJ, USA. Association for Computational Linguistics. Nadi Tomeh, Nicola Cancedda, and Marc Dymetman. 2009. Complexity-based Phrase-Table Filtering for Statistical Machine Translation. Mei Yang and Jing Zheng. 2009. Toward Smaller, Faster,
and Better Hierarchical Phrase-based SMT. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort &#8217;09, pages 237&#8211;240, Stroudsburg, PA, USA. Association for Computational Linguistics. Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A Systematic Comparison of Phrasebased, Hierarchical and Syntax-Augmented Statistical MT. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1145&#8211;1152, troudsburg, PA, USA. Association for Computational Linguistics.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We present a novel method of reducing the size of translation model for SMT.</text>
              <doc_id>139</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The contributions of the proposed method are as follows: 1) our method is the first attempt to reduce the phrase table and the hierarchical rule table simultaneously.</text>
              <doc_id>140</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>2) our method is a safe reduction method since it considers the redundancy, which is the practical ineffectiveness of individual entry.</text>
              <doc_id>141</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>3) our method shows that almost the half size of the translation model can be reduced without significant performance degradation.</text>
              <doc_id>142</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>It may be appropriate for the applications running on limited environment, e.g., mobile devices.</text>
              <doc_id>143</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Acknowledgement</text>
              <doc_id>144</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The first author performed this research during an internship at Microsoft Research Asia.</text>
              <doc_id>145</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This research was supported by the MKE(The Ministry of Knowledge Economy), Korea and Microsoft Research, under IT/SW Creative research program supervised by the NIPA(National IT Industry Promotion Agency).</text>
              <doc_id>146</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>(NIPA-2010-C1810-1002-0025)</text>
              <doc_id>147</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>References</text>
              <doc_id>148</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Michael Bloodgood and Chris Callison-Burch.</text>
              <doc_id>149</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>150</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation.</text>
              <doc_id>151</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 854&#8211;864.</text>
              <doc_id>152</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della</text>
              <doc_id>153</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Pietra, and Robert L. Mercer.</text>
              <doc_id>154</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1993.</text>
              <doc_id>155</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The Mathematics of Statistical Machine Translation: Parameter Estimation.</text>
              <doc_id>156</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 19:263&#8211;311, June.</text>
              <doc_id>157</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>David Chiang.</text>
              <doc_id>158</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>159</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>A Hierarchical Phrase-based Model for Statistical Machine Translation.</text>
              <doc_id>160</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 43th Annual Meeting on Association for Computational Linguistics, pages 263&#8211;270.</text>
              <doc_id>161</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>David Chiang.</text>
              <doc_id>162</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>163</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Hierarchical Phrase-based Translation.</text>
              <doc_id>164</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 33:201&#8211;228, June.</text>
              <doc_id>165</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, and</text>
              <doc_id>166</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Tiejun Zhao.</text>
              <doc_id>167</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>168</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Hybrid Decoding: Decoding with Partial Hypotheses Combination Over Multiple SMT Systems.</text>
              <doc_id>169</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING &#8217;10, pages 214&#8211;222, Stroudsburg, PA, USA.</text>
              <doc_id>170</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>171</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>David Graff and Christopher Cieri.</text>
              <doc_id>172</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>173</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>English Gigaword.</text>
              <doc_id>174</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>In Linguistic Data Consortium, Philadelphia.</text>
              <doc_id>175</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>David Graff.</text>
              <doc_id>176</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>177</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>Chinese Gigaword Third Edition.</text>
              <doc_id>178</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>In</text>
              <doc_id>179</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Linguistic Data Consortium, Philadelphia.</text>
              <doc_id>180</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Fei Huang and Bing Xiang.</text>
              <doc_id>181</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>182</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Feature-Rich Discriminative Phrase Rescoring for SMT.</text>
              <doc_id>183</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING &#8217;10, pages 492&#8211;500, Stroudsburg, PA, USA.</text>
              <doc_id>184</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>185</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Howard Johnson, Joel Martin, George Foster, and Roland</text>
              <doc_id>186</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Kuhn.</text>
              <doc_id>187</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>188</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Improving Translation Quality by Discarding Most of the Phrasetable.</text>
              <doc_id>189</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 967&#8211; 975.</text>
              <doc_id>190</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Philipp Koehn, Franz Josef Och, and Daniel Marcu.</text>
              <doc_id>191</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>192</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Statistical Phrase-based Translation.</text>
              <doc_id>193</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL &#8217;03, pages 48&#8211;54, Stroudsburg, PA, USA.</text>
              <doc_id>194</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>195</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.</text>
              <doc_id>196</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2003.</text>
              <doc_id>197</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Cognates can Improve Statistical Translation Models.</text>
              <doc_id>198</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: companion volume of the Proceedings of HLT- NAACL 2003&#8211;short papers - Volume 2, NAACL-Short &#8217;03, pages 46&#8211;48, Stroudsburg, PA, USA.</text>
              <doc_id>199</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>200</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Franz Josef Och.</text>
              <doc_id>201</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>202</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Minimum Error Rate Training</text>
              <doc_id>203</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>in Statistical Machine Translation.</text>
              <doc_id>204</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL &#8217;03, pages 160&#8211; 167, Stroudsburg, PA, USA.</text>
              <doc_id>205</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>206</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu.</text>
              <doc_id>207</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>208</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>BLEU: a Method for Automatic Evaluation of Machine Translation.</text>
              <doc_id>209</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &#8217;02, pages 311&#8211;318, Morristown, NJ, USA.</text>
              <doc_id>210</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>211</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.</text>
              <doc_id>212</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>213</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Complexity-based Phrase-Table Filtering for Statistical Machine Translation.</text>
              <doc_id>214</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>Mei Yang and Jing Zheng.</text>
              <doc_id>215</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>216</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>Toward Smaller, Faster,</text>
              <doc_id>217</doc_id>
              <sec_id>13</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and Better Hierarchical Phrase-based SMT.</text>
              <doc_id>218</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort &#8217;09, pages 237&#8211;240, Stroudsburg, PA, USA.</text>
              <doc_id>219</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>220</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Andreas Zollmann, Ashish Venugopal, Franz Och, and</text>
              <doc_id>221</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Jay Ponte.</text>
              <doc_id>222</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>223</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A Systematic Comparison of Phrasebased, Hierarchical and Syntax-Augmented Statistical MT.</text>
              <doc_id>224</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1145&#8211;1152, troudsburg, PA, USA.</text>
              <doc_id>225</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>226</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables/>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Michael Bloodgood</author>
          <author>Chris Callison-Burch</author>
        </authors>
        <title>Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation.</title>
        <publication>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>854--864</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Peter F Brown</author>
          <author>Vincent J Della Pietra</author>
          <author>Stephen A Della Pietra</author>
          <author>Robert L Mercer</author>
        </authors>
        <title>The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics,</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1993</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>A Hierarchical Phrase-based Model for Statistical Machine Translation.</title>
        <publication>In Proceedings of the 43th Annual Meeting on Association for Computational Linguistics,</publication>
        <pages>263--270</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>Hierarchical Phrase-based Translation.</title>
        <publication>None</publication>
        <pages>33--201</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Lei Cui</author>
          <author>Dongdong Zhang</author>
          <author>Mu Li</author>
          <author>Ming Zhou</author>
          <author>Tiejun Zhao</author>
        </authors>
        <title>Hybrid Decoding: Decoding with Partial Hypotheses Combination Over Multiple SMT Systems.</title>
        <publication>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING &#8217;10,</publication>
        <pages>214--222</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>David Graff</author>
          <author>Christopher Cieri</author>
        </authors>
        <title>None</title>
        <publication>English Gigaword. In Linguistic Data Consortium,</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>David Graff</author>
        </authors>
        <title>Chinese Gigaword Third Edition.</title>
        <publication>In Linguistic Data Consortium,</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Fei Huang</author>
          <author>Bing Xiang</author>
        </authors>
        <title>Feature-Rich Discriminative Phrase Rescoring for SMT.</title>
        <publication>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING &#8217;10,</publication>
        <pages>492--500</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Howard Johnson</author>
          <author>Joel Martin</author>
          <author>George Foster</author>
          <author>Roland Kuhn</author>
        </authors>
        <title>Improving Translation Quality by Discarding Most of the Phrasetable.</title>
        <publication>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</publication>
        <pages>967--975</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz Josef Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical Phrase-based Translation.</title>
        <publication>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL &#8217;03,</publication>
        <pages>48--54</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Grzegorz Kondrak</author>
          <author>Daniel Marcu</author>
          <author>Kevin Knight</author>
        </authors>
        <title>Cognates can Improve Statistical Translation Models.</title>
        <publication>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: companion volume of the Proceedings of HLTNAACL 2003&#8211;short papers -</publication>
        <pages>46--48</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum Error Rate Training in Statistical Machine Translation.</title>
        <publication>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL &#8217;03,</publication>
        <pages>160--167</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
        <publication>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &#8217;02,</publication>
        <pages>311--318</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Nadi Tomeh</author>
          <author>Nicola Cancedda</author>
          <author>Marc Dymetman</author>
        </authors>
        <title>Complexity-based Phrase-Table Filtering for Statistical Machine Translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Mei Yang</author>
          <author>Jing Zheng</author>
        </authors>
        <title>Toward Smaller, Faster, and Better Hierarchical Phrase-based SMT.</title>
        <publication>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort &#8217;09,</publication>
        <pages>237--240</pages>
        <date>2009</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Bloodgood and Callison-Burch (2010)</string>
        <sentence_id>36205</sentence_id>
        <char_offset>13</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Brown et al., 1993</string>
        <sentence_id>36297</sentence_id>
        <char_offset>71</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Chiang (2005)</string>
        <sentence_id>36298</sentence_id>
        <char_offset>69</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>2</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>36202</sentence_id>
        <char_offset>76</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>3</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>36202</sentence_id>
        <char_offset>90</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>4</reference_id>
        <string>Cui et al. (2010)</string>
        <sentence_id>36294</sentence_id>
        <char_offset>30</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>5</reference_id>
        <string>Graff and Cieri, 2003</string>
        <sentence_id>36299</sentence_id>
        <char_offset>97</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>6</reference_id>
        <string>Graff, 2007</string>
        <sentence_id>36300</sentence_id>
        <char_offset>55</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>7</reference_id>
        <string>Huang and Xiang, 2010</string>
        <sentence_id>36210</sentence_id>
        <char_offset>110</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>8</reference_id>
        <string>Johnson et al., 2007</string>
        <sentence_id>36211</sentence_id>
        <char_offset>123</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>9</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>36202</sentence_id>
        <char_offset>19</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>10</reference_id>
        <string>Kondrak et al., 2003</string>
        <sentence_id>36229</sentence_id>
        <char_offset>66</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>11</reference_id>
        <string>Och, 2003</string>
        <sentence_id>36236</sentence_id>
        <char_offset>154</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>12</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>36230</sentence_id>
        <char_offset>29</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>12</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>36318</sentence_id>
        <char_offset>27</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>13</reference_id>
        <string>Tomeh et al., 2009</string>
        <sentence_id>36211</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>14</reference_id>
        <string>Yang and Zheng, 2009</string>
        <sentence_id>36211</sentence_id>
        <char_offset>145</char_offset>
      </citation>
    </citations>
  </content>
</document>
