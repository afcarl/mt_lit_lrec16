<document>
  <filename>N13-1075</filename>
  <authors>
    <author>Daniel Andrade Masaaki Tsuchida Takashi Onishi Kai Ishikawa Knowledge Discovery Research Laboratories</author>
    <author>NEC Corporation</author>
  </authors>
  <title>Translation Acquisition Using Synonym Sets</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>We propose a new method for translation acquisition which uses a set of synonyms to acquire translations from comparable corpora. The motivation is that, given a certain query term, it is often possible for a user to specify one or more synonyms. Using the resulting set of query terms has the advantage that we can overcome the problem that a single query term&#8217;s context vector does not always reliably represent a terms meaning due to the context vector&#8217;s sparsity. Our proposed method uses a weighted average of the synonyms&#8217; context vectors, that is derived by inferring the mean vector of the von Mises-Fisher distribution. We evaluate our method, using the synsets from the cross-lingually aligned Japanese and English WordNet. The experiments show that our proposed method significantly improves translation accuracy when compared to a previous method for smoothing context vectors.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We propose a new method for translation acquisition which uses a set of synonyms to acquire translations from comparable corpora.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The motivation is that, given a certain query term, it is often possible for a user to specify one or more synonyms.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Using the resulting set of query terms has the advantage that we can overcome the problem that a single query term&#8217;s context vector does not always reliably represent a terms meaning due to the context vector&#8217;s sparsity.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Our proposed method uses a weighted average of the synonyms&#8217; context vectors, that is derived by inferring the mean vector of the von Mises-Fisher distribution.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We evaluate our method, using the synsets from the cross-lingually aligned Japanese and English WordNet.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The experiments show that our proposed method significantly improves translation accuracy when compared to a previous method for smoothing context vectors.</text>
              <doc_id>5</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Automatic translation acquisition is an important task for various applications. For example, finding term translations can be used to automatically update existing bilingual dictionaries, which are an indispensable resource for tasks such as cross-lingual information retrieval and text mining.
Various previous research like (Rapp, 1999; Fung, 1998) has shown that it is possible to acquire word translations from comparable corpora. We suggest here an extension of this approach which uses several query terms instead of a single query term. A user who searches a translation for a query term that is not listed in an existing bilingual dictionary, might first try to find a synonym of that term. For example, the user might look up a synonym in a thesaurus 1 or might use methods for automatic synonym acquisition like described in (Grefenstette, 1994). If the synonym is listed in the bilingual dictionary, we can consider the synonym&#8217;s translations as the translations of the query term. Otherwise, if the synonym is not listed in the dictionary either, we use the synonym together with the original query term to find a translation.
We claim that using a set of synonymous query terms to find a translation is better than using a single query term. The reason is that a single query term&#8217;s context vector is, in general, unreliable due to sparsity. For example, a low frequent query term tends to have many zero entries in its context vector. To mitigate this problem it has been proposed to smooth a query&#8217;s context vector by its nearest neighbors (Pekar et al., 2006). However, nearest neighbors, which context vectors are close the query&#8217;s context vector, can have different meanings and therefore might introduce noise.
The contributions of this paper are two-fold. First, we confirm experimentally that smoothing a query&#8217;s context vector with its synonyms leads in deed to higher translation accuracy, compared to smoothing with nearest neighbors. Second, we propose a simple method to combine a set of context vectors that performs in this setting better than a method previously proposed by (Pekar et al., 2006).
Our approach to combine a set of context vec-
1 Monolingual thesauri are, arguably, easier to construct than
bilingual dictionaries.
tors is derived by learning the mean vector of a von Mises-Fisher distribution. The combined context vector is a weighted-average of the original contextvectors, where the weights are determined by the word occurrence frequencies.
In the following section we briefly show the relation to other previous work. In Section 3, we explain our method in detail, followed by an empirical evaluation in Section 4. We summarize our results in Section 6.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Automatic translation acquisition is an important task for various applications.</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For example, finding term translations can be used to automatically update existing bilingual dictionaries, which are an indispensable resource for tasks such as cross-lingual information retrieval and text mining.</text>
              <doc_id>7</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Various previous research like (Rapp, 1999; Fung, 1998) has shown that it is possible to acquire word translations from comparable corpora.</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We suggest here an extension of this approach which uses several query terms instead of a single query term.</text>
              <doc_id>9</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A user who searches a translation for a query term that is not listed in an existing bilingual dictionary, might first try to find a synonym of that term.</text>
              <doc_id>10</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>For example, the user might look up a synonym in a thesaurus 1 or might use methods for automatic synonym acquisition like described in (Grefenstette, 1994).</text>
              <doc_id>11</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>If the synonym is listed in the bilingual dictionary, we can consider the synonym&#8217;s translations as the translations of the query term.</text>
              <doc_id>12</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Otherwise, if the synonym is not listed in the dictionary either, we use the synonym together with the original query term to find a translation.</text>
              <doc_id>13</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We claim that using a set of synonymous query terms to find a translation is better than using a single query term.</text>
              <doc_id>14</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The reason is that a single query term&#8217;s context vector is, in general, unreliable due to sparsity.</text>
              <doc_id>15</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For example, a low frequent query term tends to have many zero entries in its context vector.</text>
              <doc_id>16</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>To mitigate this problem it has been proposed to smooth a query&#8217;s context vector by its nearest neighbors (Pekar et al., 2006).</text>
              <doc_id>17</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>However, nearest neighbors, which context vectors are close the query&#8217;s context vector, can have different meanings and therefore might introduce noise.</text>
              <doc_id>18</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The contributions of this paper are two-fold.</text>
              <doc_id>19</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>First, we confirm experimentally that smoothing a query&#8217;s context vector with its synonyms leads in deed to higher translation accuracy, compared to smoothing with nearest neighbors.</text>
              <doc_id>20</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Second, we propose a simple method to combine a set of context vectors that performs in this setting better than a method previously proposed by (Pekar et al., 2006).</text>
              <doc_id>21</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Our approach to combine a set of context vec-</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 Monolingual thesauri are, arguably, easier to construct than</text>
              <doc_id>23</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>bilingual dictionaries.</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tors is derived by learning the mean vector of a von Mises-Fisher distribution.</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The combined context vector is a weighted-average of the original contextvectors, where the weights are determined by the word occurrence frequencies.</text>
              <doc_id>26</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the following section we briefly show the relation to other previous work.</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Section 3, we explain our method in detail, followed by an empirical evaluation in Section 4.</text>
              <doc_id>28</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We summarize our results in Section 6.</text>
              <doc_id>29</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Related Work</title>
        <text>There are several previous works on extracting translations from comparable corpora ranging from (Rapp, 1999; Fung, 1998), and more recently (Haghighi et al., 2008; Laroche and Langlais, 2010), among others. Essentially, all these methods calculate the similarity of a query term&#8217;s context vector with each translation candidate&#8217;s context vector. The context vectors are extracted from the comparable corpora, and mapped to a common vector space with the help of an existing bilingual dictionary. The work in (D&#233;jean et al., 2002) uses crosslingually aligned classes in a multilingual thesaurus to improve the translation accuracy. Their method uses the probability that the query term and a translation candidate are assigned to the same class. In contrast, our method does not need cross-lingually aligned classes.
Ismail and Manandhar (2010) proposes a method that tries to improve a query&#8217;s context vector by using in-domain terms. In-domain terms are the terms that are highly associated to the query, as well as highly associated to one of the query&#8217;s highly associated terms. Their method makes it necessary that the query term has enough highly associated context terms. 2 However, a low-frequent query term might not have enough highly associated terms.
In general if a query term has a low-frequency in the corpus, then its context vector is sparse. In that case, the chance of finding a correct translation is reduced (Pekar et al., 2006). Therefore, Pekar et al. (2006) suggest to use distance-based averaging to smooth the context vector of a low-frequent query
2 In their experiments, they require that a query word has at
least 100 associated terms.
term. Their smoothing strategy is dependent on the occurrence frequency of a query term and its close neighbors. Let us denote q the context vector of the query word, and K be the set of its close neighbors. The smoothed context vector q &#8242; is then derived by using:
q &#8242; = &#947; &#183; q + (1 &#8722; &#947;) &#183; &#8721; w x &#183; x , (1)
x&#8712;K
where w x is the weight of neighbor x, and all weights sum to one. The context vectors q and x are interpreted as probability vectors and therefore L1-normalized. The weight w x is a function of the distance between neighbor x and query q. The parameter &#947; determines the degree of smoothing, and is a function of the frequency of the query term and its neighbors:
&#947; = log f(q) log max x&#8712;K&#8746;{q} f(x) (2)
where f(x) is the frequency of term x. Their method forms the baseline for our proposed method.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>There are several previous works on extracting translations from comparable corpora ranging from (Rapp, 1999; Fung, 1998), and more recently (Haghighi et al., 2008; Laroche and Langlais, 2010), among others.</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Essentially, all these methods calculate the similarity of a query term&#8217;s context vector with each translation candidate&#8217;s context vector.</text>
              <doc_id>31</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The context vectors are extracted from the comparable corpora, and mapped to a common vector space with the help of an existing bilingual dictionary.</text>
              <doc_id>32</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The work in (D&#233;jean et al., 2002) uses crosslingually aligned classes in a multilingual thesaurus to improve the translation accuracy.</text>
              <doc_id>33</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Their method uses the probability that the query term and a translation candidate are assigned to the same class.</text>
              <doc_id>34</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In contrast, our method does not need cross-lingually aligned classes.</text>
              <doc_id>35</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Ismail and Manandhar (2010) proposes a method that tries to improve a query&#8217;s context vector by using in-domain terms.</text>
              <doc_id>36</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In-domain terms are the terms that are highly associated to the query, as well as highly associated to one of the query&#8217;s highly associated terms.</text>
              <doc_id>37</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Their method makes it necessary that the query term has enough highly associated context terms.</text>
              <doc_id>38</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2 However, a low-frequent query term might not have enough highly associated terms.</text>
              <doc_id>39</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In general if a query term has a low-frequency in the corpus, then its context vector is sparse.</text>
              <doc_id>40</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In that case, the chance of finding a correct translation is reduced (Pekar et al., 2006).</text>
              <doc_id>41</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, Pekar et al. (2006) suggest to use distance-based averaging to smooth the context vector of a low-frequent query</text>
              <doc_id>42</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2 In their experiments, they require that a query word has at</text>
              <doc_id>43</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>least 100 associated terms.</text>
              <doc_id>44</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>term.</text>
              <doc_id>45</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Their smoothing strategy is dependent on the occurrence frequency of a query term and its close neighbors.</text>
              <doc_id>46</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Let us denote q the context vector of the query word, and K be the set of its close neighbors.</text>
              <doc_id>47</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The smoothed context vector q &#8242; is then derived by using:</text>
              <doc_id>48</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>q &#8242; = &#947; &#183; q + (1 &#8722; &#947;) &#183; &#8721; w x &#183; x , (1)</text>
              <doc_id>49</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>x&#8712;K</text>
              <doc_id>50</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where w x is the weight of neighbor x, and all weights sum to one.</text>
              <doc_id>51</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The context vectors q and x are interpreted as probability vectors and therefore L1-normalized.</text>
              <doc_id>52</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The weight w x is a function of the distance between neighbor x and query q.</text>
              <doc_id>53</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The parameter &#947; determines the degree of smoothing, and is a function of the frequency of the query term and its neighbors:</text>
              <doc_id>54</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#947; = log f(q) log max x&#8712;K&#8746;{q} f(x) (2)</text>
              <doc_id>55</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where f(x) is the frequency of term x.</text>
              <doc_id>56</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Their method forms the baseline for our proposed method.</text>
              <doc_id>57</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Proposed Method</title>
        <text>Our goal is to combine the context vectors to one context vector which is less sparse and more reliable than the original context vector of query word q. We assume that for each occurrence of a word, its corresponding context vector was generated by a probabilistic model. Furthermore, we assume that synonyms are generated by the same probability distribution. Finally we use the mean vector of that distribution to represent the combined context vector. By using the assumption that each occurrence of a word corresponds to one sample of the probability distribution, our model places more weight on synonyms that are highly-frequent than synonyms that occur infrequently. This is motivated by the assumption that context vectors of synonyms that occur with high frequency in the corpus, are more reliable than the ones of low-frequency synonyms.
When comparing context vectors, work like Laroche and Langlais (2010) observed that often the cosine similarity performs superior to other distance-measures, like, for example, the euclidean distance. This suggests that context vectors tend to lie in the spherical vector space,
and therefore the von Mises-Fisher distribution is a natural choice for our probabilistic model. The von Mises-Fisher distribution was also successfully used in the work of (Basu et al., 2004) to cluster text data. The von Mises-Fisher distribution with location parameter &#181;, and concentration parameter &#954; is defined as:
p(x|&#181;, &#954;) = c(&#954;) &#183; e &#954;&#183;x&#183;&#181;T ,
where c(&#954;) is a normalization constant, and ||x|| = ||&#181;|| = 1, and &#954; &#8805; 0. || denotes here the L2-norm. The cosine-similarity measures the angle between two vectors, and the von Mises distribution defines a probability distribution over the possible angles. The parameter &#181; of the von Mises distribution is estimated as follows (Jammalamadaka and Sengupta, 2001): Given the words x 1 , ..., x n , we denote the corresponding context vectors as x 1 , ..., x n , and assume that each context vector is L2-normalized. Then, the mean vector &#181; is calculated as:
&#181; = 1 Z n&#8721;
i=1
where Z ensures that the resulting context vector is L2-normalized, i.e. Z is || &#8721; n
i=1 x i n
||. For our purpose, &#954; is irrelevant and is assumed to be any fixed positive constant.
Since we assume that each occurrence of a word x in the corpus corresponds to one observation of the corresponding word&#8217;s context vector x, we get the following formula:
&#181; = 1 Z &#8242; &#183; n&#8721;
i=1
where Z &#8242; is now || &#8721; n
i=1
x i n
f(x i ) &#8721; n
j=1 f(x j) &#183; x i
f(x i ) &#8721; n
j=1 f(x j) &#183; x i||. We then
use the vector &#181; as the combined vector of the words&#8217; context vectors x i . Our proposed procedure to combine the context vector of query word q and its synonyms can be summarized as follows:
1. Denote the context vectors of q and its synonyms as x 1 , ..., x n , and L2-normalize each context vector.
2. Calculate the weighted average of the vectors x 1 , ..., x n , whereas the weights correspond to the frequencies of each word x i .
3. L2-normalize the weighted average.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Our goal is to combine the context vectors to one context vector which is less sparse and more reliable than the original context vector of query word q.</text>
              <doc_id>58</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We assume that for each occurrence of a word, its corresponding context vector was generated by a probabilistic model.</text>
              <doc_id>59</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Furthermore, we assume that synonyms are generated by the same probability distribution.</text>
              <doc_id>60</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Finally we use the mean vector of that distribution to represent the combined context vector.</text>
              <doc_id>61</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>By using the assumption that each occurrence of a word corresponds to one sample of the probability distribution, our model places more weight on synonyms that are highly-frequent than synonyms that occur infrequently.</text>
              <doc_id>62</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This is motivated by the assumption that context vectors of synonyms that occur with high frequency in the corpus, are more reliable than the ones of low-frequency synonyms.</text>
              <doc_id>63</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>When comparing context vectors, work like Laroche and Langlais (2010) observed that often the cosine similarity performs superior to other distance-measures, like, for example, the euclidean distance.</text>
              <doc_id>64</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This suggests that context vectors tend to lie in the spherical vector space,</text>
              <doc_id>65</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and therefore the von Mises-Fisher distribution is a natural choice for our probabilistic model.</text>
              <doc_id>66</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The von Mises-Fisher distribution was also successfully used in the work of (Basu et al., 2004) to cluster text data.</text>
              <doc_id>67</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The von Mises-Fisher distribution with location parameter &#181;, and concentration parameter &#954; is defined as:</text>
              <doc_id>68</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>p(x|&#181;, &#954;) = c(&#954;) &#183; e &#954;&#183;x&#183;&#181;T ,</text>
              <doc_id>69</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where c(&#954;) is a normalization constant, and ||x|| = ||&#181;|| = 1, and &#954; &#8805; 0.</text>
              <doc_id>70</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>|| denotes here the L2-norm.</text>
              <doc_id>71</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The cosine-similarity measures the angle between two vectors, and the von Mises distribution defines a probability distribution over the possible angles.</text>
              <doc_id>72</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The parameter &#181; of the von Mises distribution is estimated as follows (Jammalamadaka and Sengupta, 2001): Given the words x 1 , ..., x n , we denote the corresponding context vectors as x 1 , ..., x n , and assume that each context vector is L2-normalized.</text>
              <doc_id>73</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Then, the mean vector &#181; is calculated as:</text>
              <doc_id>74</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#181; = 1 Z n&#8721;</text>
              <doc_id>75</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1</text>
              <doc_id>76</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where Z ensures that the resulting context vector is L2-normalized, i.e.</text>
              <doc_id>77</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Z is || &#8721; n</text>
              <doc_id>78</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1 x i n</text>
              <doc_id>79</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>||.</text>
              <doc_id>80</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For our purpose, &#954; is irrelevant and is assumed to be any fixed positive constant.</text>
              <doc_id>81</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Since we assume that each occurrence of a word x in the corpus corresponds to one observation of the corresponding word&#8217;s context vector x, we get the following formula:</text>
              <doc_id>82</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#181; = 1 Z &#8242; &#183; n&#8721;</text>
              <doc_id>83</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1</text>
              <doc_id>84</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where Z &#8242; is now || &#8721; n</text>
              <doc_id>85</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1</text>
              <doc_id>86</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>x i n</text>
              <doc_id>87</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>f(x i ) &#8721; n</text>
              <doc_id>88</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>j=1 f(x j) &#183; x i</text>
              <doc_id>89</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>f(x i ) &#8721; n</text>
              <doc_id>90</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>j=1 f(x j) &#183; x i||.</text>
              <doc_id>91</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We then</text>
              <doc_id>92</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>use the vector &#181; as the combined vector of the words&#8217; context vectors x i .</text>
              <doc_id>93</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our proposed procedure to combine the context vector of query word q and its synonyms can be summarized as follows:</text>
              <doc_id>94</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1.</text>
              <doc_id>95</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Denote the context vectors of q and its synonyms as x 1 , ..., x n , and L2-normalize each context vector.</text>
              <doc_id>96</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2.</text>
              <doc_id>97</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Calculate the weighted average of the vectors x 1 , ..., x n , whereas the weights correspond to the frequencies of each word x i .</text>
              <doc_id>98</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3.</text>
              <doc_id>99</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>L2-normalize the weighted average.</text>
              <doc_id>100</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Experiments</title>
        <text>As source and target language corpora we use a corpus extracted from a collection of complaints concerning automobiles compiled by the Japanese Ministry of Land, Infrastructure, Transport and Tourism (MLIT) 3 and the USA National Highway Traffic Safety Administration (NHTSA) 4 , respectively. The Japanese corpus contains 24090 sentences that were POS tagged using MeCab (Kudo et al., 2004). The English corpus contains 47613 sentences, that were POS tagged using Stepp Tagger (Tsuruoka et al., 2005), and use the Lemmatizer (Okazaki et al., 2008) to extract and stem content words (nouns, verbs, adjectives, adverbs).
For creating the context vectors, we calculate the association between two content words occurring in the same sentence, using the log-odds-ratio (Evert, 2004). It was shown in (Laroche and Langlais, 2010) that the log-odds-ratio in combination with the cosine-similarity performs superior to several other methods like PMI 5 and LLR 6 . For comparing two context vectors we use the cosine similarity. To transform the Japanese and English context vectors into the same vector space, we use a bilingual dictionary with around 1.6 million entries. 7 To express all context vectors in the same vector space, we map the context vectors in English to context vectors in Japanese. 8 First, for all the words which are listed in the bilingual dictionary we calculate word translation probabilities. These translation probabilities are calculated using the EM-algorithm described in (Koehn and Knight, 2000). We then create a translation matrix T which contains in each
3 http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html 4 http://www-odi.nhtsa.dot.gov/downloads/index.cfm 5 point-wise mutual information 6 log-likelihood ratio 7 The bilingual dictionary was developed in the course of our
Japanese language processing efforts described in (Sato et al., 2003). 8 Alternatively, we could, for example, use canonical correlation analysis to match the vectors to a common latent vector space, like described in (Haghighi et al., 2008).
column the translation probabilities for a word in English into any word in Japanese. Each context vector in English is then mapped into Japanese using the linear transformation described by the translation matrix T . For word x with context vector x in English, let x &#8242; be its context vector after transformation into Japanese, i.e. x &#8242; = T &#183; x.
The gold-standard was created by considering all nouns in the Japanese and English WordNet where synsets are aligned cross-lingually. This way we were able to create a gold-standard with 215 Japanese nouns, and their respective English translations that occur in our comparable corpora. 9 Note that the cross-lingual alignment is needed only for evaluation. For evaluation, we consider only the translations that occur in the corresponding English synset as correct.
Because all methods return a ranked list of translation candidates, the accuracy is measured using the rank of the translation listed in the gold-standard. The inverse rank is the sum of the inverse ranks of each translation in the gold-standard.
In Table 1, the first row shows the results when using no smoothing. Next, we smooth the query&#8217;s context vector by using Equation (1) and (2). The set of neighbors K is defined as the k-terms in the source language that are closest to the query word, with respect to the cosine similarity (sim). The weight w x for a neighbor x is set to w x = 10 0.13&#183;sim(x,q) in accordance to (Pekar et al., 2006). For k we tried values between 1 and 100, and got the best inverse rank when using k=19. The resulting method (Topk Smoothing) performs consistently better than the method using no smoothing, see Table 1, second row. Next, instead of smoothing the query word with its nearest neighbors, we use as the set K the set of synonyms of the query word (Syn Smoothing). Table 1 shows a clear improvement over the method that uses nearest neighbor-smoothing. This confirms our claim that using synonyms for smoothing can lead to better translation accuracy than using nearest neighbors. In the last row of Table 1, we compare our proposed method to combine context vectors of synonyms (Syn Mises-Combination), with the pre-
9 The resulting synsets in Japanese and English, contain in
average 2.2 and 2.8 words, respectively. The ambiguity of a query term in our gold-standard is low, since, in average, a query term belongs to only 1.2 different synsets.
vious method (Syn Smoothing). A pair-wise comparison of our proposed method with Syn Smoothing shows a statistically significant improvement (p &lt; 0.01). 10
Finally, we also show the result when simply adding each synonym vector to the query&#8217;s context vector to form a new combined context vector (Syn Sum). 11 Even though, this approach does not use the frequency information of a word, it performs better than Syn Smoothing. We suppose that this is due to the fact that it actually indirectly uses frequency information, since the log-odds-ratio tends to be higher for words which occur with high frequency in the corpus.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>As source and target language corpora we use a corpus extracted from a collection of complaints concerning automobiles compiled by the Japanese Ministry of Land, Infrastructure, Transport and Tourism (MLIT) 3 and the USA National Highway Traffic Safety Administration (NHTSA) 4 , respectively.</text>
              <doc_id>101</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The Japanese corpus contains 24090 sentences that were POS tagged using MeCab (Kudo et al., 2004).</text>
              <doc_id>102</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The English corpus contains 47613 sentences, that were POS tagged using Stepp Tagger (Tsuruoka et al., 2005), and use the Lemmatizer (Okazaki et al., 2008) to extract and stem content words (nouns, verbs, adjectives, adverbs).</text>
              <doc_id>103</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For creating the context vectors, we calculate the association between two content words occurring in the same sentence, using the log-odds-ratio (Evert, 2004).</text>
              <doc_id>104</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It was shown in (Laroche and Langlais, 2010) that the log-odds-ratio in combination with the cosine-similarity performs superior to several other methods like PMI 5 and LLR 6 .</text>
              <doc_id>105</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For comparing two context vectors we use the cosine similarity.</text>
              <doc_id>106</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>To transform the Japanese and English context vectors into the same vector space, we use a bilingual dictionary with around 1.6 million entries.</text>
              <doc_id>107</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>7 To express all context vectors in the same vector space, we map the context vectors in English to context vectors in Japanese.</text>
              <doc_id>108</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>8 First, for all the words which are listed in the bilingual dictionary we calculate word translation probabilities.</text>
              <doc_id>109</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>These translation probabilities are calculated using the EM-algorithm described in (Koehn and Knight, 2000).</text>
              <doc_id>110</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>We then create a translation matrix T which contains in each</text>
              <doc_id>111</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3 http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html 4 http://www-odi.nhtsa.dot.gov/downloads/index.cfm 5 point-wise mutual information 6 log-likelihood ratio 7 The bilingual dictionary was developed in the course of our</text>
              <doc_id>112</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Japanese language processing efforts described in (Sato et al., 2003).</text>
              <doc_id>113</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>8 Alternatively, we could, for example, use canonical correlation analysis to match the vectors to a common latent vector space, like described in (Haghighi et al., 2008).</text>
              <doc_id>114</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>column the translation probabilities for a word in English into any word in Japanese.</text>
              <doc_id>115</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each context vector in English is then mapped into Japanese using the linear transformation described by the translation matrix T .</text>
              <doc_id>116</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For word x with context vector x in English, let x &#8242; be its context vector after transformation into Japanese, i.e. x &#8242; = T &#183; x.</text>
              <doc_id>117</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The gold-standard was created by considering all nouns in the Japanese and English WordNet where synsets are aligned cross-lingually.</text>
              <doc_id>118</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This way we were able to create a gold-standard with 215 Japanese nouns, and their respective English translations that occur in our comparable corpora.</text>
              <doc_id>119</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>9 Note that the cross-lingual alignment is needed only for evaluation.</text>
              <doc_id>120</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>For evaluation, we consider only the translations that occur in the corresponding English synset as correct.</text>
              <doc_id>121</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Because all methods return a ranked list of translation candidates, the accuracy is measured using the rank of the translation listed in the gold-standard.</text>
              <doc_id>122</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The inverse rank is the sum of the inverse ranks of each translation in the gold-standard.</text>
              <doc_id>123</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In Table 1, the first row shows the results when using no smoothing.</text>
              <doc_id>124</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Next, we smooth the query&#8217;s context vector by using Equation (1) and (2).</text>
              <doc_id>125</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The set of neighbors K is defined as the k-terms in the source language that are closest to the query word, with respect to the cosine similarity (sim).</text>
              <doc_id>126</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The weight w x for a neighbor x is set to w x = 10 0.13&#183;sim(x,q) in accordance to (Pekar et al., 2006).</text>
              <doc_id>127</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>For k we tried values between 1 and 100, and got the best inverse rank when using k=19.</text>
              <doc_id>128</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The resulting method (Topk Smoothing) performs consistently better than the method using no smoothing, see Table 1, second row.</text>
              <doc_id>129</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Next, instead of smoothing the query word with its nearest neighbors, we use as the set K the set of synonyms of the query word (Syn Smoothing).</text>
              <doc_id>130</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Table 1 shows a clear improvement over the method that uses nearest neighbor-smoothing.</text>
              <doc_id>131</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>This confirms our claim that using synonyms for smoothing can lead to better translation accuracy than using nearest neighbors.</text>
              <doc_id>132</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>In the last row of Table 1, we compare our proposed method to combine context vectors of synonyms (Syn Mises-Combination), with the pre-</text>
              <doc_id>133</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>9 The resulting synsets in Japanese and English, contain in</text>
              <doc_id>134</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>average 2.2 and 2.8 words, respectively.</text>
              <doc_id>135</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The ambiguity of a query term in our gold-standard is low, since, in average, a query term belongs to only 1.2 different synsets.</text>
              <doc_id>136</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>vious method (Syn Smoothing).</text>
              <doc_id>137</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A pair-wise comparison of our proposed method with Syn Smoothing shows a statistically significant improvement (p &lt; 0.01).</text>
              <doc_id>138</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>10</text>
              <doc_id>139</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Finally, we also show the result when simply adding each synonym vector to the query&#8217;s context vector to form a new combined context vector (Syn Sum).</text>
              <doc_id>140</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>11 Even though, this approach does not use the frequency information of a word, it performs better than Syn Smoothing.</text>
              <doc_id>141</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We suppose that this is due to the fact that it actually indirectly uses frequency information, since the log-odds-ratio tends to be higher for words which occur with high frequency in the corpus.</text>
              <doc_id>142</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>5 Discussion</title>
        <text>We first discuss an example where the query terms are &#65533;&#65533;&#65533;&#65533; (cruise) and &#65533;&#65533; (cruise). Both words can have the same meaning. The resulting translation candidates suggested by the baseline methods and the proposed method is shown in Table 2. Using no smoothing, the baseline method outputs the correct translation for &#65533;&#65533;&#65533;&#65533; (cruise) and &#65533; &#65533; (cruise) at rank 10 and 15, respectively. When combining both queries to form one context vector our proposed method (Syn Mises-Combination) retrieves the correct translation at rank 2. Note that we considered all nouns that occur three or more times as possible translation candidates. As can be seen in Table 2, this also includes spelling mistakes like &#8221;sevice&#8221; and &#8221;infromation&#8221;.
10 We use the sign-test (Wilcox, 2009) to test the hypothesis
that the proposed method ranks higher than the baseline. 11 No normalization is performed before adding the context
vectors.
Finally, we note that some terms in our test set are ambiguous, and the ambiguity is not resolved by using the synonyms of only one synset. For example, the term &#65533;&#65533; (steering, guidance) belongs to the synset &#8221;steering, guidance&#8221; which includes the terms &#65533;&#65533;&#65533; (steering, guidance) and &#65533;&#65533;&#65533; (guidance), &#65533;&#65533; (guidance). Despite this conflation of senses in one synset, our proposed method can improve the finding of (one) correct translation. The baseline system using only &#65533;&#65533; (steering, guidance) outputs the correct translation &#8221;steering&#8221; at rank 4, whereas our method using all four terms outputs it at rank 2.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We first discuss an example where the query terms are &#65533;&#65533;&#65533;&#65533; (cruise) and &#65533;&#65533; (cruise).</text>
              <doc_id>143</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Both words can have the same meaning.</text>
              <doc_id>144</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The resulting translation candidates suggested by the baseline methods and the proposed method is shown in Table 2.</text>
              <doc_id>145</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Using no smoothing, the baseline method outputs the correct translation for &#65533;&#65533;&#65533;&#65533; (cruise) and &#65533; &#65533; (cruise) at rank 10 and 15, respectively.</text>
              <doc_id>146</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>When combining both queries to form one context vector our proposed method (Syn Mises-Combination) retrieves the correct translation at rank 2.</text>
              <doc_id>147</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Note that we considered all nouns that occur three or more times as possible translation candidates.</text>
              <doc_id>148</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>As can be seen in Table 2, this also includes spelling mistakes like &#8221;sevice&#8221; and &#8221;infromation&#8221;.</text>
              <doc_id>149</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>10 We use the sign-test (Wilcox, 2009) to test the hypothesis</text>
              <doc_id>150</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>that the proposed method ranks higher than the baseline.</text>
              <doc_id>151</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>11 No normalization is performed before adding the context</text>
              <doc_id>152</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>vectors.</text>
              <doc_id>153</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Finally, we note that some terms in our test set are ambiguous, and the ambiguity is not resolved by using the synonyms of only one synset.</text>
              <doc_id>154</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For example, the term &#65533;&#65533; (steering, guidance) belongs to the synset &#8221;steering, guidance&#8221; which includes the terms &#65533;&#65533;&#65533; (steering, guidance) and &#65533;&#65533;&#65533; (guidance), &#65533;&#65533; (guidance).</text>
              <doc_id>155</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Despite this conflation of senses in one synset, our proposed method can improve the finding of (one) correct translation.</text>
              <doc_id>156</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The baseline system using only &#65533;&#65533; (steering, guidance) outputs the correct translation &#8221;steering&#8221; at rank 4, whereas our method using all four terms outputs it at rank 2.</text>
              <doc_id>157</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>6 Conclusions</title>
        <text>We proposed a new method for translation acquisition which uses a set of synonyms to acquire translations. Our approach combines the query term&#8217;s context vector with all the context vectors of its synonyms. In order to combine the vectors we use a weighted average of each context vector, where the weights are determined by a term&#8217;s occurrence frequency. Our experiments, using the Japanese and English WordNet (Bond et al., 2009; Fellbaum, 1998), show that our proposed method can increase the translation accuracy, when compared to using only a single query term, or smoothing with nearest neighbours. Our results suggest that instead of directly searching for a translation, it is worth first looking for synonyms, for example by considering spelling variations or monolingual resources.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We proposed a new method for translation acquisition which uses a set of synonyms to acquire translations.</text>
              <doc_id>158</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our approach combines the query term&#8217;s context vector with all the context vectors of its synonyms.</text>
              <doc_id>159</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In order to combine the vectors we use a weighted average of each context vector, where the weights are determined by a term&#8217;s occurrence frequency.</text>
              <doc_id>160</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Our experiments, using the Japanese and English WordNet (Bond et al., 2009; Fellbaum, 1998), show that our proposed method can increase the translation accuracy, when compared to using only a single query term, or smoothing with nearest neighbours.</text>
              <doc_id>161</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Our results suggest that instead of directly searching for a translation, it is worth first looking for synonyms, for example by considering spelling variations or monolingual resources.</text>
              <doc_id>162</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Shows Top-n accuracy and mean inverse rank (MIR) for baseline methods which use no synonyms (No Smoothing, Top-k Smoothing), the proposed method (Syn Mises-Combination) which uses synonyms, and alternative methods that also use synonyms (Syn Smoothing, Syn Sum).</caption>
        <reference_text>In PAGE 4: ... The inverse rank is the sum of the inverse ranks of each translationin the gold-standard. In  Table1 , thefirstrow showstheresultswhenus- ing no smoothing.Next, we smooththe query?s con- text vectorby usingEquation(1) and (2)....  In PAGE 4: ...ccordanceto (Pekar et al., 2006). For k we tried values between1 and 100, and got the best inverse rank when using k=19. The resultingmethod(Top- k Smoothing)performsconsistentlybetter than the method using no smoothing, see  Table1 , second row. Next, insteadof smoothingthequerywordwith its nearestneighbors,we use as the set K the set of synonyms of the query word (Syn Smoothing)....  In PAGE 4: ...Thisconfirms our claim that using synonyms for smoothing can leadto bettertranslationaccuracy thanusingnearest neighbors. In the last row of  Table1 , we compare our proposedmethodto combinecontext vectors of synonyms (Syn Mises-Combination),with the pre- 9The resultingsynsets in Japaneseand English, contain in average 2....</reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>quency in the corpus.</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Method</cell>
              <cell>Top1</cell>
              <cell>Top5</cell>
              <cell>Top10</cell>
              <cell>MIR</cell>
            </row>
            <row>
              <cell>No Smoothing</cell>
              <cell>0.14</cell>
              <cell>0.30</cell>
              <cell>0.36</cell>
              <cell>0.23</cell>
            </row>
            <row>
              <cell>Top-kSmoothing</cell>
              <cell>0.16</cell>
              <cell>0.33</cell>
              <cell>0.43</cell>
              <cell>0.26</cell>
            </row>
            <row>
              <cell>SynSmoothing</cell>
              <cell>0.18</cell>
              <cell>0.35</cell>
              <cell>0.46</cell>
              <cell>0.28</cell>
            </row>
            <row>
              <cell>SynSum</cell>
              <cell>0.23</cell>
              <cell>0.46</cell>
              <cell>0.57</cell>
              <cell>0.35</cell>
            </row>
            <row>
              <cell>SynMises-Combination</cell>
              <cell>0.31</cell>
              <cell>0.46</cell>
              <cell>0.55</cell>
              <cell>0.40</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Shows the results for &#65533;&#65533;&#65533;&#65533; and &#65533;&#65533; which both have the same meaning &#8221;cruise&#8221;. The third column shows part of the ranked translation candidates separated by comma. The last column shows the rank of the correct translation &#8221;cruise&#8221;. Syn Smoothing uses Equation (1) with q corresponding to the context vector of the query word, and K contains only the context vector of the term that is used for smoothing.</caption>
        <reference_text>In PAGE 4: ... Notethatwe consideredall nouns that occur three or more times as possible translationcandidates. As can be seen in  Table2 , this also includesspelling mistakes like  sevice and  infromation . 10We use the sign-test(Wilcox, 2009) to test the hypothesis thatthe proposedmethodrankshigherthanthe baseline....</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>Method</cell>
              <cell>Query</cell>
              <cell>Output</cell>
              <cell>Rank</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>No Smoothing</cell>
              <cell>&#65533;&#65533;&#65533;&#65533;</cell>
              <cell>..., affinity, delco, cruise, sevice, sentrum,...</cell>
              <cell>10</cell>
            </row>
            <row>
              <cell>No Smoothing</cell>
              <cell>&#65533;&#65533;</cell>
              <cell>..., denali, attendant, cruise, abs, tactic,...</cell>
              <cell>15</cell>
            </row>
            <row>
              <cell>Top-k Smoothing</cell>
              <cell>&#65533;&#65533;&#65533;&#65533;</cell>
              <cell>pillar, multi, cruise, star, affinity,...</cell>
              <cell>3</cell>
            </row>
            <row>
              <cell>Top-k Smoothing</cell>
              <cell>&#65533;&#65533;</cell>
              <cell>..., burnout, dipstick, cruise, infromation, speed, ...</cell>
              <cell>8</cell>
            </row>
            <row>
              <cell>Syn Smoothing</cell>
              <cell>&#65533;&#65533;&#65533;&#65533; smoothed with &#65533;&#65533;</cell>
              <cell>..., affinity, delco, cruise, sevice, sentrum,...</cell>
              <cell>10</cell>
            </row>
            <row>
              <cell>Syn Smoothing</cell>
              <cell>&#65533;&#65533; smoothed with &#65533;&#65533;&#65533;&#65533;</cell>
              <cell>..., alldata, mode, cruise, expectancy, mph,...</cell>
              <cell>8</cell>
            </row>
            <row>
              <cell>Syn Sum</cell>
              <cell>&#65533;&#65533;&#65533;&#65533;, &#65533;&#65533;</cell>
              <cell>assumption, level, cruise, reimbursment, infromation,...</cell>
              <cell>3</cell>
            </row>
            <row>
              <cell>Syn Mises-Combination</cell>
              <cell>&#65533;&#65533;&#65533;&#65533;, &#65533;&#65533;</cell>
              <cell>pillar, cruise, assumption, level, speed,...</cell>
              <cell>2</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>S Basu</author>
          <author>M Bilenko</author>
          <author>R J Mooney</author>
        </authors>
        <title>A probabilistic framework for semi-supervised clustering.</title>
        <publication>In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</publication>
        <pages>59--68</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>F Bond</author>
          <author>H Isahara</author>
          <author>S Fujita</author>
          <author>K Uchimoto</author>
          <author>T Kuribayashi</author>
          <author>K Kanzaki</author>
        </authors>
        <title>Enhancing the japanese wordnet.</title>
        <publication>In Proceedings of the 7th Workshop on Asian Language Resources,</publication>
        <pages>1--8</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>H D&#233;jean</author>
          <author>&#201; Gaussier</author>
          <author>F Sadat</author>
        </authors>
        <title>An approach based on multilingual thesauri and model combination for bilingual lexicon extraction.</title>
        <publication>In Proceedings of the International Conference on Computational Linguistics,</publication>
        <pages>1--7</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>S Evert</author>
        </authors>
        <title>The statistics of word cooccurrences: word pairs and collocations. Doctoral dissertation, Institut f&#252;r maschinelle Sprachverarbeitung,</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>C Fellbaum</author>
        </authors>
        <title>Wordnet: an electronic lexical database. Cambrige,</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>P Fung</author>
        </authors>
        <title>A statistical view on bilingual lexicon extraction: from parallel corpora to non-parallel corpora.</title>
        <publication>Lecture Notes in Computer Science,</publication>
        <pages>1529--1</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>G Grefenstette</author>
        </authors>
        <title>Explorations in automatic thesaurus discovery.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1994</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>A Haghighi</author>
          <author>P Liang</author>
          <author>T Berg-Kirkpatrick</author>
          <author>D Klein</author>
        </authors>
        <title>Learning bilingual lexicons from monolingual corpora.</title>
        <publication>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>771--779</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>A Ismail</author>
          <author>S Manandhar</author>
        </authors>
        <title>Bilingual lexicon extraction from comparable corpora using in-domain terms.</title>
        <publication>In Proceedings of the International Conference on Computational Linguistics,</publication>
        <pages>481--489</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Jammalamadaka</author>
          <author>A Sengupta</author>
        </authors>
        <title>Topics in circular statistics, volume 5. World Scientific Pub Co Inc.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>P Koehn</author>
          <author>K Knight</author>
        </authors>
        <title>Estimating word translation probabilities from unrelated monolingual corpora using the em algorithm.</title>
        <publication>In Proceedings of the National Conference on Artificial Intelligence,</publication>
        <pages>711--715</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>T Kudo</author>
          <author>K Yamamoto</author>
          <author>Y Matsumoto</author>
        </authors>
        <title>Applying conditional random fields to Japanese morphological analysis.</title>
        <publication>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>230--237</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>A Laroche</author>
          <author>P Langlais</author>
        </authors>
        <title>Revisiting contextbased projection methods for term-translation spotting in comparable corpora.</title>
        <publication>In Proceedings of the International Conference on Computational Linguistics,</publication>
        <pages>617--625</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>N Okazaki</author>
          <author>Y Tsuruoka</author>
          <author>S Ananiadou</author>
          <author>J Tsujii</author>
        </authors>
        <title>A discriminative candidate generator for string transformations.</title>
        <publication>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>447--456</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>V Pekar</author>
          <author>R Mitkov</author>
          <author>D Blagoev</author>
          <author>A Mulloni</author>
        </authors>
        <title>Finding translations for low-frequency words in comparable corpora.</title>
        <publication>None</publication>
        <pages>266</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>R Rapp</author>
        </authors>
        <title>Automatic identification of word translations from unrelated English and German corpora.</title>
        <publication>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>519--526</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>K Sato</author>
          <author>T Ikeda</author>
          <author>T Nakata</author>
          <author>S Osada</author>
        </authors>
        <title>Introduction of a Japanese language processing middleware used for CRM.</title>
        <publication>In Annual Meeting of the Japanese Association for Natural Language Processing (in Japanese),</publication>
        <pages>109--112</pages>
        <date>2003</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Basu et al., 2004</string>
        <sentence_id>25506</sentence_id>
        <char_offset>77</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>2</reference_id>
        <string>D&#233;jean et al., 2002</string>
        <sentence_id>25472</sentence_id>
        <char_offset>13</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>3</reference_id>
        <string>Evert, 2004</string>
        <sentence_id>25543</sentence_id>
        <char_offset>147</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>5</reference_id>
        <string>Fung, 1998</string>
        <sentence_id>25447</sentence_id>
        <char_offset>44</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>5</reference_id>
        <string>Fung, 1998</string>
        <sentence_id>25469</sentence_id>
        <char_offset>110</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>6</reference_id>
        <string>Grefenstette, 1994</string>
        <sentence_id>25450</sentence_id>
        <char_offset>137</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>7</reference_id>
        <string>Haghighi et al., 2008</string>
        <sentence_id>25469</sentence_id>
        <char_offset>142</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>7</reference_id>
        <string>Haghighi et al., 2008</string>
        <sentence_id>25553</sentence_id>
        <char_offset>148</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>8</reference_id>
        <string>Ismail and Manandhar (2010)</string>
        <sentence_id>25475</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>9</reference_id>
        <string>Jammalamadaka and Sengupta, 2001</string>
        <sentence_id>25512</sentence_id>
        <char_offset>71</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>10</reference_id>
        <string>Koehn and Knight, 2000</string>
        <sentence_id>25549</sentence_id>
        <char_offset>84</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>11</reference_id>
        <string>Kudo et al., 2004</string>
        <sentence_id>25541</sentence_id>
        <char_offset>79</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>12</reference_id>
        <string>Laroche and Langlais (2010)</string>
        <sentence_id>25503</sentence_id>
        <char_offset>42</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>12</reference_id>
        <string>Laroche and Langlais, 2010</string>
        <sentence_id>25469</sentence_id>
        <char_offset>165</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>12</reference_id>
        <string>Laroche and Langlais, 2010</string>
        <sentence_id>25544</sentence_id>
        <char_offset>17</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>13</reference_id>
        <string>Okazaki et al., 2008</string>
        <sentence_id>25542</sentence_id>
        <char_offset>134</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>14</reference_id>
        <string>Pekar et al., 2006</string>
        <sentence_id>25456</sentence_id>
        <char_offset>107</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>14</reference_id>
        <string>Pekar et al., 2006</string>
        <sentence_id>25460</sentence_id>
        <char_offset>146</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>14</reference_id>
        <string>Pekar et al., 2006</string>
        <sentence_id>25480</sentence_id>
        <char_offset>70</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>14</reference_id>
        <string>Pekar et al., 2006</string>
        <sentence_id>25566</sentence_id>
        <char_offset>83</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>15</reference_id>
        <string>Rapp, 1999</string>
        <sentence_id>25447</sentence_id>
        <char_offset>32</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>15</reference_id>
        <string>Rapp, 1999</string>
        <sentence_id>25469</sentence_id>
        <char_offset>98</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>16</reference_id>
        <string>Sato et al., 2003</string>
        <sentence_id>25552</sentence_id>
        <char_offset>51</char_offset>
      </citation>
    </citations>
  </content>
</document>
