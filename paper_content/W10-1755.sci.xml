<PAPER>
  <FILENO/>
  <TITLE>The Parameter-optimized ATEC Metric for MT Evaluation</TITLE>
  <AUTHORS/>
  <ABSTRACT>
    <A-S ID="S-43731">This paper describes the latest version of the ATEC metric for automatic MT evaluation, with parameters optimized for word choice and word order, the two fundamental features of language that the metric relies on.</A-S>
    <A-S ID="S-43732">The former is assessed by matching at various linguistic levels and weighting the informativeness of both matched and unmatched words.</A-S>
    <A-S ID="S-43733">The latter is quantified in term of word position and information flow.</A-S>
    <A-S ID="S-43734">We also discuss those aspects of language not yet covered by other existing evaluation metrics but carefully considered in the formulation of our metric.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-43735">It is recognized that the proposal of the BLEU metric (<REF ID="R-03" RPTR="5">Papineni et al., 2002</REF>) has piloted a paradigm evolution to MT evaluation.</S>
        <S ID="S-43736">It provides a computable solution to the task and turns it into an engineering problem of measuring text similarity and simulating human judgments of translation quality.</S>
        <S ID="S-43737">Related studies in recent years have extensively revealed more essential characteristics of BLEU, including its strengths and weaknesses.</S>
        <S ID="S-43738">This has aroused the proposal of different new evaluation metrics aimed at addressing such weaknesses so as to find some other hopefully better alternatives for the task.</S>
        <S ID="S-43739">Effort in this direction brings up some advanced metrics such as METEOR (<REF ID="R-00" RPTR="0">Banerjee and Lavie, 2005</REF>) and TERp (<REF ID="R-04" RPTR="6">Snover et al., 2009</REF>) that seem to have already achieved considerably strong correlations with human judgments.</S>
        <S ID="S-43740">Nevertheless, few metrics have really nurtured our understanding of possible parameters involved in our language comprehension and text quality judgment.</S>
        <S ID="S-43741">This inadequacy limits, inevitably, the application of the existing metrics.</S>
      </P>
      <P>
        <S ID="S-43742">The ATEC metric (<REF ID="R-07" RPTR="9">Wong and Kit, 2008</REF>) was developed as a response to this inadequacy, with a focus to account for the process of human comprehension of sentences via two fundamental features of text, namely word choice and word order.</S>
        <S ID="S-43743">It integrates various explicit measures for these two features in order to provide an intuitive and informative evaluation result.</S>
        <S ID="S-43744">Its previous version (<REF ID="R-09" RPTR="10">Wong and Kit, 2009</REF><REF ID="R-11" RPTR="12">Wong and Kit, 2009</REF>b) has already illustrated a highly comparable performance to the few state-of-the-art evaluation metrics, showing a great improvement over its initial version for participation in MetricsMATR08 1 .</S>
        <S ID="S-43745">It is also applied to evaluate online MT systems for legal translation, to examine its applicability for lay users&#8217; use to select appropriate MT systems (<REF ID="R-09" RPTR="11">Wong and Kit, 2009</REF><REF ID="R-11" RPTR="13">Wong and Kit, 2009</REF>a).</S>
      </P>
      <P>
        <S ID="S-43746">In this paper we describe the formulation of ATEC, including its new features and optimization of parameters.</S>
        <S ID="S-43747">In particular we will discuss how the design of this metric can complement the inadequacies of other metrics in terms of its treatment of word choice and word order and its utilization of multiple references in the evaluation process.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 The ATEC Metric</HEADER>
      <P>
        <S ID="S-43910"></S>
      </P>
      <DIV DEPTH="1">
        <HEADER>2.1 Word Choice</HEADER>
        <P>
          <S ID="S-43748">In general, word is the basic meaning bearing unit of language.</S>
          <S ID="S-43749">In a semantic theory such as Latent Semantic Analysis (LSA) (<REF ID="R-02" RPTR="2">Landauer et al., 1998</REF>), lexical selection is even the sole consideration of the meaning of a text.</S>
          <S ID="S-43750">A recent study of the major errors in MT outputs by <REF ID="R-05" RPTR="7">Vilar et al. (2006)</REF> also reveals that different kinds of error related to word choices constitute a majority of error types.</S>
          <S ID="S-43751">It is therefore of prime importance</S>
        </P>
        <P>
          <S ID="S-43752">1 http://www.itl.nist.gov/iad/mig/tests/metricsmatr/2008/</S>
        </P>
        <P>
          <S ID="S-43753">for MT evaluation metrics to diagnose the adequacy of word selection by an MT system.</S>
        </P>
        <P>
          <S ID="S-43754">It is a general consensus that the performance of an evaluation metric can be improved by matching more words between MT outputs and human references.</S>
          <S ID="S-43755">Linguistic resources like stemmer and WordNet are widely applied by many metrics for matching word stems and synonyms.</S>
          <S ID="S-43756">ATEC is equipped with these two modules as well, and furthermore, with two measures for word similarity, including a WordNet-based (Wu and Palmer, 1994) and a corpus-based measure (<REF ID="R-02" RPTR="3">Landauer et al., 1998</REF>) for matching word pairs of similar meanings.</S>
          <S ID="S-43757">Our previous work (<REF ID="R-06" RPTR="8">Wong, 2010</REF>) shows that the inclusion of semantically similar words results in a positive correlation gain comparable to the use of Word- Net for synonym identification.</S>
        </P>
        <P>
          <S ID="S-43758">In addition to increasing the number of legitimate matches, we also consider the importance of each match.</S>
          <S ID="S-43759">Although most metrics score every matched word with equal weight, different words indeed contribute different amount of information to the meaning of a sentence.</S>
          <S ID="S-43760">In Example 1 below, both C1 and C2 contain the same number of words matched with Ref, but the matches in C1 are more informative and therefore should be assigned higher weights.</S>
        </P>
        <P>
          <S ID="S-43761">Example 1 C1: it was not first time that prime minister confronts northern league &#8230; C2: this is not the prime the operation with the</S>
        </P>
        <P>
          <S ID="S-43762">north &#8230; Ref: this is not the first time the prime minister</S>
        </P>
        <P>
          <S ID="S-43763">has faced the northern league &#8230;</S>
        </P>
        <P>
          <S ID="S-43764">The informativeness of a match is weighted by the tf-idf measure, which has been widely used in information retrieval to assess the relative importance of a word as an indexing term for a document.</S>
          <S ID="S-43765">A word is more important to a document when it occurs more frequently in this document and less in others.</S>
          <S ID="S-43766">In ATEC, we have &#8220;document&#8221; to refer to &#8220;sentence&#8221;, the basic text unit in MT evaluation.</S>
          <S ID="S-43767">This allows a more sensitive measure for words in different sentences, and gets around the problem of an evaluation dataset containing only one or a few long documents.</S>
          <S ID="S-43768">Accordingly, the tf-idf measure is formulated as: N tfidf ( i, j) = tfi, j &#8901;log( )</S>
        </P>
        <P>
          <S ID="S-43769">sfi where tf i,j is the occurrences of word w i in sentence s j , sf i the number of sentences containing word w i , and N the total number of sentences in</S>
        </P>
        <P>
          <S ID="S-43770">the evaluation set.</S>
          <S ID="S-43771">In case of a high-frequency word whose tf-idf weight is less than 1, it is then rounded up to 1.</S>
        </P>
        <P>
          <S ID="S-43772">In addition to matched words, unmatched words are also considered to have a role to play in determining the quality of word choices of an MT output.</S>
          <S ID="S-43773">As illustrated in Example 1, the unmatched words in Ref for C1 and C2 are [this | is | the | the | has | faced | the] and [first | time | minister | has | faced | northern | league] respectively.</S>
          <S ID="S-43774">One can see that the words missing in C2 are more significant.</S>
          <S ID="S-43775">It is therefore necessary to apply the tf-idf weighting to unmatched reference words so as to quantify the information missed in the MT outputs in question.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.2 Word Order</HEADER>
        <P>
          <S ID="S-43776">In MT evaluation, word order refers to the extent to which an MT output is interpretable following the information flow of its reference translation.</S>
          <S ID="S-43777">It is not rare that an MT output has many matched words but does not make sense because of a problematic word order.</S>
          <S ID="S-43778">Currently it is observed that consecutive matches represent a legitimate local ordering, causing some metrics to extend the unit of matching from word to phrase.</S>
          <S ID="S-43779"><REF ID="R-01" RPTR="1">Birch et al. (2010)</REF> show, however, that the current metrics including BLEU, METEOR and TER are highly lexical oriented and still cannot distinguish between sentences of different word orders.</S>
          <S ID="S-43780">This is a serious problem in MT evaluation, for many MT systems have become capable of generating more and more suitable words in translations, resulting in that the quality difference of their outputs lies more and more crucially in the variances of word order.</S>
        </P>
        <P>
          <S ID="S-43781">ATEC uses three explicit features for word order, namely position distance, order distance and phrase size.</S>
          <S ID="S-43782">Position distance refers to the divergence of the locations of matches in an MT output and its reference.</S>
          <S ID="S-43783">Example 2 illustrates two candidates with the same match, whose position in C1 is closer to its corresponding position in Ref than that in C2.</S>
          <S ID="S-43784">We conceive this as a significant indicator of the accuracy of word order: the closer the positions of a matched word in the candidate and reference, the better match it is.</S>
        </P>
        <P>
          <S ID="S-43785">Example 2 C1: non-signatories these acts victims but it</S>
        </P>
        <P>
          <S ID="S-43786">caused to incursion transcendant C2: non-signatories but it caused to incursion</S>
        </P>
        <P>
          <S ID="S-43787">transcendant these acts victims Ref: there were no victims in this incident but</S>
        </P>
        <P>
          <S ID="S-43788">they did cause massive damage</S>
        </P>
        <P>
          <S ID="S-43789">The calculation of position distance is based on the position indices of words in a sentence.</S>
          <S ID="S-43790">In particular, we align every word in a candidate to its closest counterpart in a reference.</S>
          <S ID="S-43791">In Example 3, all the candidate words have a match in the reference.</S>
          <S ID="S-43792">As illustrated by the two &#8220;a&#8221; in the candidate, the shortest alignments (strict lines) are preferred over any farther alternatives (dash lines).</S>
          <S ID="S-43793">In a case like this, only two matches, i.e., thief and police, vary in position by a distance of 3.</S>
        </P>
        <P>
          <S ID="S-43794">Reference: a police chases a thief Pos index: 1 2 3 4 5</S>
        </P>
        <P>
          <S ID="S-43795">This position distance is sensitive to sentence length as it simply makes use of word position indices without any normalization.</S>
          <S ID="S-43796">Example 4 illustrates two cases of different lengths.</S>
          <S ID="S-43797">The position distance of the bold matched words is 3 in C1 but 14 in C2.</S>
          <S ID="S-43798">Indeed, the divergence of word order in C1 does not hinder our understanding, but in C2 it poses a serious problem.</S>
          <S ID="S-43799">This excessive length inevitably magnifies the interference effect of word order divergence.</S>
        </P>
        <P>
          <S ID="S-43800">Example 4 C1: Short 1 and 2 various 3 international 4 news 5 R1: International 1 news 2 brief 3 C2: Is 1 on 2 a 3 popular 4 the 5 very 6 in 7 Iraq 8 to 9</S>
        </P>
        <P>
          <S ID="S-43801">those 10 just 11 like 12 other 13 world 14 in 15 which 16 young 17 people 18 with 19 the 20 and 21 flowers 22 while 23 awareness 24 by 25 other 26 times 27 of 28 the 29 countries 30 of 31 the 32 R2: Valentine&#8217;s 1 day 2 is 3 a 4 very 5 popular 6 day 7</S>
        </P>
        <P>
          <S ID="S-43802">in 8 Iraq 9 as 10 it 11 is 12 in 13 the 14 other 15 countries 16 of 17 the 18 world 19 .</S>
          <S ID="S-43803">Young 20 men 21 exchange 22 with 23 their 24 girlfriends 25 sweets 26 , flowers 27 , perfumes 28 and 29 other 30 gifts 31 .</S>
        </P>
        <P>
          <S ID="S-43804">Another feature, the order distance, concerns the information flow of a sentence in the form of the sequence of matches.</S>
          <S ID="S-43805">Each match in a candidate and a reference is first assigned an order index in a sequential manner.</S>
          <S ID="S-43806">Then, the difference of two counterpart indices is measured, so as to see if a variance exists.</S>
          <S ID="S-43807">Examples 5a and 5b exemplify two kinds of order distance and their corresponding position distance.</S>
          <S ID="S-43808">Both cases have two matches with the same sum of position distance.</S>
          <S ID="S-43809">However, the matches are in an identical sequence in 5a but cause a cross in 5b, resulting in a larger order distance for the latter.</S>
        </P>
        <P>
          <S ID="S-43810">Example 5a Position index Order index Candidate:</S>
        </P>
        <P>
          <S ID="S-43811">Reference: Order index Position index</S>
        </P>
        <P>
          <S ID="S-43812">Position distance Order distance</S>
        </P>
        <P>
          <S ID="S-43813">Example 5b Position index Order index Candidate:</S>
        </P>
        <P>
          <S ID="S-43814">Reference: Order index Position index</S>
        </P>
        <P>
          <S ID="S-43815">Position distance Order distance</S>
        </P>
        <P>
          <S ID="S-43816">(2-1) + (4-3) = 2 (1-1) + (2-2) = 0</S>
        </P>
        <P>
          <S ID="S-43817">(2-2) + (3-1) = 2 (2-1) + (2-1) = 2</S>
        </P>
        <P>
          <S ID="S-43818">In practice, ATEC operates on phrases like many other metrics.</S>
          <S ID="S-43819">But unlike these metrics that count only the number of matched phrases, ATEC gives extra credit to a longer phrase to reward its valid word sequence.</S>
          <S ID="S-43820">In Example 6, C1 and C2 represent two MT outputs of the same length, with matched words underlined.</S>
          <S ID="S-43821">Both have 10 matches in 3 phrases, and will receive the same evaluation score from a metric like METEOR or TERp, ignoring the subtle difference in the sizes of the matched phrases, which are [8,1,1] and [4,3,3] words for C1 and C2 respectively.</S>
          <S ID="S-43822">In contrast, ATEC uses the size of a phrase as a reduction factor to its position distance, so as to raise the contribution of a larger phrase to the metric score.</S>
        </P>
        <P>
          <S ID="S-43823">Example 6 C1: w 1 w 2 w 3 w 4 w 5 w 6 w 7 w 8 w 9 w 10 w 11 w 12 w 13 C2: w 1 w 2 w 3 w 4 w 5 w 6 w 7 w 8 w 9 w 10 w 11 w 12 w 13</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.3 Multiple References</HEADER>
        <P>
          <S ID="S-43824">The availability of multiple references allows more legitimate word choices and word order of an MT output to be accounted.</S>
          <S ID="S-43825">Some existing metrics only compute the scores of a candidate against each reference and select the highest one.</S>
        </P>
        <P>
          <S ID="S-43826">This deficit can be illustrated by a well-known example from <REF ID="R-03" RPTR="4">Papineni et al. (2002)</REF>, as replicated in Example 7 with slight modification.</S>
          <S ID="S-43827">It shows that nearly all candidate words can find their matches in either reference.</S>
          <S ID="S-43828">However, if we resort to single reference, only around half of them can have a match, which would seriously underrate the quality of the candidate.</S>
        </P>
        <P>
          <S ID="S-43829">Example 7 C: It is a guide to action which ensures that the</S>
        </P>
        <P>
          <S ID="S-43830">military always obeys the commands of the party.</S>
        </P>
        <P>
          <S ID="S-43831">R1: It is a guide to action that ensures that the military will forever heed Party commands.</S>
        </P>
        <P>
          <S ID="S-43832">R2: It is the guiding principle which guarantees the military forces always being under the commands of the party.</S>
        </P>
        <P>
          <S ID="S-43833">ATEC exploits multiple references in this fashion to maximize the number of matches in a candidate.</S>
          <S ID="S-43834">It begins with aligning the longest matches with either reference.</S>
          <S ID="S-43835">The one with the shortest position distance is preferred if more than one alternative available in the same phrase size.</S>
          <S ID="S-43836">This process repeats until no more candidate word can find a match.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.4 Formulation of ATEC</HEADER>
        <P>
          <S ID="S-43837">The computation of an ATEC score begins with alignment of phrases, as described above.</S>
          <S ID="S-43838">For each matched phase, we first sum up the score of each word i in the phrase as</S>
        </P>
        <P>
          <S ID="S-43839">W</S>
        </P>
        <P>
          <S ID="S-43840">match</S>
        </P>
        <P>
          <S ID="S-43841">=</S>
        </P>
        <P>
          <S ID="S-43842">&#8721;</S>
        </P>
        <P>
          <S ID="S-43843">i&#8712;{ phrase}</S>
        </P>
        <P>
          <S ID="S-43844">( w</S>
        </P>
        <P>
          <S ID="S-43845">type</S>
        </P>
        <P>
          <S ID="S-43846">Info &#8722;</S>
        </P>
        <P>
          <S ID="S-43847">tfidf</S>
        </P>
        <P>
          <S ID="S-43848">match</S>
        </P>
        <P>
          <S ID="S-43849">where w type refers to a basic score of a matched word depending on its match type.</S>
          <S ID="S-43850">It is then minus its information load, i.e., the tf-idf score of the matched word with a weight factor, Info match .</S>
        </P>
        <P>
          <S ID="S-43851">There is also a distance penalty for a phrase,</S>
        </P>
        <P>
          <S ID="S-43852">e</S>
        </P>
        <P>
          <S ID="S-43853">| p | Dis = wposdis pos (1 &#8722; ) + w</S>
        </P>
        <P>
          <S ID="S-43854">| c |</S>
        </P>
        <P>
          <S ID="S-43855">i</S>
        </P>
        <P>
          <S ID="S-43856">order</S>
        </P>
        <P>
          <S ID="S-43857">)</S>
        </P>
        <P>
          <S ID="S-43858">dis</S>
        </P>
        <P>
          <S ID="S-43859">order</S>
        </P>
        <P>
          <S ID="S-43860">where dis pos and dis order refer to the position distance and order distance, and w pos and w order are their corresponding weight factors, respectively.</S>
          <S ID="S-43861">The position distance is further weighted according to the size of phrase |p| with ,</S>
        </P>
        <P>
          <S ID="S-43862">an exponential factor e, in proportion to the length of candidate |c|.</S>
        </P>
        <P>
          <S ID="S-43863">The score of a matched phrase is then computed by</S>
        </P>
        <P>
          <S ID="S-43864">&#9127;W Phrase = &#9128;</S>
        </P>
        <P>
          <S ID="S-43865">&#9129;W</S>
        </P>
        <P>
          <S ID="S-43866">match</S>
        </P>
        <P>
          <S ID="S-43867">match</S>
        </P>
        <P>
          <S ID="S-43868">&#8901; Limit</S>
        </P>
        <P>
          <S ID="S-43869">&#8722; Dis,</S>
        </P>
        <P>
          <S ID="S-43870">dis</S>
        </P>
        <P>
          <S ID="S-43871">,</S>
        </P>
        <P>
          <S ID="S-43872">if Dis &gt; W match&#183;Limit dis ;</S>
        </P>
        <P>
          <S ID="S-43873">otherwise,</S>
        </P>
        <P>
          <S ID="S-43874">Limit dis is an upper limit for the distance penalty.</S>
          <S ID="S-43875">Accordingly, the score C of all phrases in a candidate is</S>
        </P>
        <P>
          <S ID="S-43876">C = &#8721; Phrase j</S>
        </P>
        <P>
          <S ID="S-43877">j&#8712;{candidate }</S>
        </P>
        <P>
          <S ID="S-43878">.</S>
        </P>
        <P>
          <S ID="S-43879">Then, we move on to calculating the information load of unmatched reference words W unmatch , approximated as</S>
        </P>
        <P>
          <S ID="S-43880">W</S>
        </P>
        <P>
          <S ID="S-43881">unmatch</S>
        </P>
        <P>
          <S ID="S-43882">=</S>
        </P>
        <P>
          <S ID="S-43883">&#8721;</S>
        </P>
        <P>
          <S ID="S-43884">k&#8712;{ unmatch}</S>
        </P>
        <P>
          <S ID="S-43885">( w</S>
        </P>
        <P>
          <S ID="S-43886">type</S>
        </P>
        <P>
          <S ID="S-43887">Info &#8722;</S>
        </P>
        <P>
          <S ID="S-43888">tfidf</S>
        </P>
        <P>
          <S ID="S-43889">unmatch</S>
        </P>
        <P>
          <S ID="S-43890">k</S>
        </P>
        <P>
          <S ID="S-43891">) .</S>
        </P>
        <P>
          <S ID="S-43892">The overall score M accounting for both the matched and unmatched is defined as</S>
        </P>
        <P>
          <S ID="S-43893">&#9127;C &#8901; Limit M = &#9128;</S>
        </P>
        <P>
          <S ID="S-43894">&#9129;C &#8722; W</S>
        </P>
        <P>
          <S ID="S-43895">Info</S>
        </P>
        <P>
          <S ID="S-43896">unmatch</S>
        </P>
        <P>
          <S ID="S-43897">,</S>
        </P>
        <P>
          <S ID="S-43898">,</S>
        </P>
        <P>
          <S ID="S-43899">if W unmatch &gt; C&#183;Limit Info ;</S>
        </P>
        <P>
          <S ID="S-43900">otherwise,</S>
        </P>
        <P>
          <S ID="S-43901">Limit Info is an upper limit for the information penalty of the unmatched words.</S>
        </P>
        <P>
          <S ID="S-43902">Finally, the ATEC score is computed using the conventional F-measure in terms of precision P and recall R as PR ATEC = &#945;P + ( 1&#8722;&#945;) R</S>
        </P>
        <P>
          <S ID="S-43903">M P = where | c | ,</S>
        </P>
        <P>
          <S ID="S-43904">M R =</S>
        </P>
        <P>
          <S ID="S-43905">| r |</S>
        </P>
        <P>
          <S ID="S-43906">The parameter &#945; adjusts the weights of P and R, and |c| and |r| refer to the length of candidate and reference, respectively.</S>
          <S ID="S-43907">In the case of multiple references, |r| refers to the average length of references.</S>
        </P>
        <P>
          <S ID="S-43908">We have derived the optimized values for the parameters involved in ATEC calculation using the development data of NIST MetricsMATR10 with adequacy assessments by a simple hill climbing approach.</S>
          <S ID="S-43909">The optimal parameter setting is presented in Table 1 below.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Conclusion</HEADER>
      <P>
        <S ID="S-43911">In the above sections we have presented the latest version of our ATEC metric with particular emphasis on word choice and word order as two fundamental features of language.</S>
        <S ID="S-43912">Each of these features contains multiple parameters intended to</S>
      </P>
      <P>
        <S ID="S-43913">have a comprehensive coverage of different textual factors involved in our interpretation of a sentence.</S>
        <S ID="S-43914">The optimal offsetting for the parameters is expected to report an empirical observation of the relative merits of each factor in adequacy assessment.</S>
        <S ID="S-43915">We are currently exploring their relation with the errors of MT outputs, to examine the potential of automatic error analysis.</S>
        <S ID="S-43916">The ATEC package is obtainable at: http://mega.ctl.cityu.edu.hk/ctbwong/ATEC/</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS>
    <P>
      <S ID="S-43917">The research work described in this paper was supported by City University of Hong Kong through the Strategic Research Grant (SRG) 7002267.</S>
    </P>
  </ACKNOWLEDGMENTS>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>Satanjeev Banerjee</RAUTHOR>
      <REFTITLE>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>Alexandra Birch</RAUTHOR>
      <REFTITLE>Metrics for MT Evaluation: Evaluating Reordering. Machine Translation (forthcoming).</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>Thomas Landauer</RAUTHOR>
      <REFTITLE>Introduction to Latent Semantic Analysis.</REFTITLE>
      <DATE>1998</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>Kishore Papineni</RAUTHOR>
      <REFTITLE>BLEU: A Method for Automatic Evaluation of Machine Translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>Matthew Snover</RAUTHOR>
      <REFTITLE>Fluency, Adequacy, or HTER? Exploring Different Human Judgments with a Tunable MT Metric.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>David Vilar</RAUTHOR>
      <REFTITLE>Error Analysis of Statistical Machine Translation Output.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>Billy T-M Wong</RAUTHOR>
      <REFTITLE>Semantic Evaluation of Machine Translation.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>Billy T-M Wong</RAUTHOR>
      <REFTITLE>Word choice and Word Position for Automatic MT Evaluation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>AMTA</RAUTHOR>
      <REFTITLE>Workshop: MetricsMATR,</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>Billy T-M Wong</RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR></RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>Billy Wong</RAUTHOR>
      <REFTITLE>ATEC: Automatic Evaluation of Machine Translation via Word Choice and Word Order.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
