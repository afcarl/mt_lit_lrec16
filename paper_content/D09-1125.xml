<document>
  <filename>D09-1125</filename>
  <authors>
    <author>Xiaodong He</author>
  </authors>
  <title>Joint Optimization for Machine Translation System Combination</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>System combination has emerged as a powerful method for machine translation (MT). This paper pursues a joint optimization strategy for combining outputs from multiple MT systems, where word alignment, ordering, and lexical selection decisions are made jointly according to a set of feature functions combined in a single log-linear model. The decoding algorithm is described in detail and a set of new features that support this joint decoding approach is proposed. The approach is evaluated in comparison to state-of-the-art confusion-network-based system combination methods using equivalent features and shown to outperform them significantly.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>System combination has emerged as a powerful method for machine translation (MT).</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This paper pursues a joint optimization strategy for combining outputs from multiple MT systems, where word alignment, ordering, and lexical selection decisions are made jointly according to a set of feature functions combined in a single log-linear model.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The decoding algorithm is described in detail and a set of new features that support this joint decoding approach is proposed.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The approach is evaluated in comparison to state-of-the-art confusion-network-based system combination methods using equivalent features and shown to outperform them significantly.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>System combination for machine translation (MT) has emerged as a powerful method of combining the strengths of multiple MT systems and achieving results which surpass those of each individual system (e.g. Bangalore, et. al., 2001, Matusov, et. al., 2006, Rosti, et. al., 2007a). Most state-of-the-art system combination methods are based on constructing a confusion network (CN) from several input translation hypotheses, and choosing the best output from the CN based on several scoring functions (e.g. Rosti et. al., 2007a, He et. al., 2008, Matusov et al. 2008). Confusion networks allow word-level system combination, which was shown to outperform sentence re-ranking methods and phrase-level combination (Rosti, et. al. 2007a).
We will review confusion-network-based system combination with the help of the examples in Figures 1 and 2. Figure 1 shows translation hypotheses from three Chinese-to- English MT systems. The general idea is to combine hypotheses in a representation such as the ones in Figure 2, where for each word position there is a set of possible words, shown in columns. 1 The final output is determined by choosing one word from each column, which can be a real word or the empty word &#949;. For example, the CN in Figure 2a) can generate eight distinct sequences of words, including e.g. &#8220;she bought the Jeep&#8221; and &#8220;she bought the SUV Jeep&#8221;. The choice is performed to maximize a scoring function using a set of features and a log-linear model (Matusov, et. al 2006, Rosti, et al. 2007a).
We can view a confusion network as an ordered sequence of columns (correspondence sets). Each word from each input hypothesis belongs to exactly one correspondence set. Each correspondence set contains at most one word from each input hypothesis and contributes exactly one of its words (including the possible &#949;) to the final output. Final words are output in the order of correspondence sets. In order to construct such a representation, we need to solve the following two sub-problems: arrange words from all input hypotheses into correspondence sets (alignment problem) and order correspondence sets (ordering problem). After constructing the confusion network we need to solve a third sub-problem: decide which words to output from each correspondence set (lexical choice problem).
In current state-of-the-art approaches, the construction of the confusion network is performed as follows: first, a backbone hypothesis is selected. The backbone hypothesis determines the order of words in the final system output, and guides word-level alignments for construction of columns of possible words at each position. Let us assume that for our example in Figure 1, the second hypothesis is selected as a backbone. All other hypotheses are aligned to the backbone such that these alignments are one-to-one; empty words are inserted where necessary to make one-to-one
1 This representation is alternative to directed acyclic
graph representations of confusion networks.
alignment possible. Words in all hypotheses are sorted by the position of the backbone word they align to and the confusion network is determined.
It is clear that the quality of selection of the backbone and alignments has a large impact on the performance, because the word order is determined by the backbone, and the set of possible words at each position is determined by alignment. Since the space of possible alignments is extremely large, approximate and heuristic techniques have been employed to derive them. In pair-wise alignment, each hypothesis is aligned to the backbone in turn, with separate processing to combine the multiple alignments. Several models have been used for pair-wise alignment, starting with TER and proceeding with more sophisticated techniques such as HMM models, ITG, and IHMM (Rosti et. al 2007a, Matusov et al 2008, Krakos et al. 2008, He et al. 2008). A major problem with such methods is that each hypothesis is aligned to the backbone independently, leading to suboptimal behavior. For example, suppose that we use a state-of-the-art word alignment model for pairs of hypotheses, such as the IHMM. Figure 1 shows likely alignment links between every pair of hypotheses. If Hypothesis 1 is aligned to Hypothesis 2 (the backbone), Jeep is likely to align to SUV because they express similar Chinese content. Hypothesis 3 is separately aligned to the backbone and since the alignment is constrained to be one-to-one, SUV is aligned to SUV and Jeep to an empty word which is inserted after SUV. The network in Figure 2a) is the result of this process. An undesirable property of this CN is that the two instances of Jeep are placed in separate columns and cannot vote to reinforce each other.
Incremental alignment methods have been proposed to relax the independence assumption of pair-wise alignment (Rosti et al. 2008, Li et al. 2009). Such methods align hypotheses to a partially constructed CN in some order. For example, if in such method, Hypothesis 3 is first aligned to the backbone, followed by Hypothesis 1, we are likely to arrive at the CN in Figure 2b) in which the two instances of Jeep are aligned. However, if Hypothesis 1 is aligned to the backbone first, we would still get the CN in Figure 2a). Notice that the desirable output &#8220;She bought the Jeep SUV&#8221; cannot be generated from either of the confusion networks because a rereordering of columns would be required.
A common characteristic of CN-based approaches is that the order of words (backbone) and the alignment of words (correspondence sets) are decided as greedy steps independently of the lexical choice for the final output. The backbone and alignment are optimized according to auxiliary scoring functions and heuristics which may or may not be optimal with respect to producing CNs leading to good translations. In some recent approaches, these assumptions are relaxed to allow each input hypothesis as a backbone. Each backbone produces a separate CN and the decision of which CN to choose is taken at a later decoding stage, but this still restricts the possible orders and alignments greatly (Rosti et al. 2008, Matusov et al. 2008).
In this paper, we present a joint optimization method for system combination. In this method, the alignment, ordering and lexical selection subproblems are solved jointly in a single decoding framework based on a log-linear model.
she
she
she
bought
buys
bought
the
the
the
&#949;
&#949;
Jeep
SUV
SUV
&#949;
Jeep
Figure 1. Three MT system hypotheses with pairwise alignments.
a) Confusion network with pair-wise alignment.
b) Confusion network with incremental alignment.
Figure 2. Correspondence sets of confusion networks under pair-wise and incremental alignment, using the second hypothesis as a backbone.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>System combination for machine translation (MT) has emerged as a powerful method of combining the strengths of multiple MT systems and achieving results which surpass those of each individual system (e.g. Bangalore, et.</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>al., 2001, Matusov, et.</text>
              <doc_id>5</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>al., 2006, Rosti, et.</text>
              <doc_id>6</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>al., 2007a).</text>
              <doc_id>7</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Most state-of-the-art system combination methods are based on constructing a confusion network (CN) from several input translation hypotheses, and choosing the best output from the CN based on several scoring functions (e.g. Rosti et.</text>
              <doc_id>8</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>al., 2007a, He et.</text>
              <doc_id>9</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>al., 2008, Matusov et al. 2008).</text>
              <doc_id>10</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Confusion networks allow word-level system combination, which was shown to outperform sentence re-ranking methods and phrase-level combination (Rosti, et.</text>
              <doc_id>11</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>al. 2007a).</text>
              <doc_id>12</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We will review confusion-network-based system combination with the help of the examples in Figures 1 and 2.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Figure 1 shows translation hypotheses from three Chinese-to- English MT systems.</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The general idea is to combine hypotheses in a representation such as the ones in Figure 2, where for each word position there is a set of possible words, shown in columns.</text>
              <doc_id>15</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>1 The final output is determined by choosing one word from each column, which can be a real word or the empty word &#949;.</text>
              <doc_id>16</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>For example, the CN in Figure 2a) can generate eight distinct sequences of words, including e.g. &#8220;she bought the Jeep&#8221; and &#8220;she bought the SUV Jeep&#8221;.</text>
              <doc_id>17</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The choice is performed to maximize a scoring function using a set of features and a log-linear model (Matusov, et.</text>
              <doc_id>18</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>al 2006, Rosti, et al. 2007a).</text>
              <doc_id>19</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We can view a confusion network as an ordered sequence of columns (correspondence sets).</text>
              <doc_id>20</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each word from each input hypothesis belongs to exactly one correspondence set.</text>
              <doc_id>21</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Each correspondence set contains at most one word from each input hypothesis and contributes exactly one of its words (including the possible &#949;) to the final output.</text>
              <doc_id>22</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Final words are output in the order of correspondence sets.</text>
              <doc_id>23</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In order to construct such a representation, we need to solve the following two sub-problems: arrange words from all input hypotheses into correspondence sets (alignment problem) and order correspondence sets (ordering problem).</text>
              <doc_id>24</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>After constructing the confusion network we need to solve a third sub-problem: decide which words to output from each correspondence set (lexical choice problem).</text>
              <doc_id>25</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In current state-of-the-art approaches, the construction of the confusion network is performed as follows: first, a backbone hypothesis is selected.</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The backbone hypothesis determines the order of words in the final system output, and guides word-level alignments for construction of columns of possible words at each position.</text>
              <doc_id>27</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Let us assume that for our example in Figure 1, the second hypothesis is selected as a backbone.</text>
              <doc_id>28</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>All other hypotheses are aligned to the backbone such that these alignments are one-to-one; empty words are inserted where necessary to make one-to-one</text>
              <doc_id>29</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 This representation is alternative to directed acyclic</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>graph representations of confusion networks.</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>alignment possible.</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Words in all hypotheses are sorted by the position of the backbone word they align to and the confusion network is determined.</text>
              <doc_id>33</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>It is clear that the quality of selection of the backbone and alignments has a large impact on the performance, because the word order is determined by the backbone, and the set of possible words at each position is determined by alignment.</text>
              <doc_id>34</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Since the space of possible alignments is extremely large, approximate and heuristic techniques have been employed to derive them.</text>
              <doc_id>35</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In pair-wise alignment, each hypothesis is aligned to the backbone in turn, with separate processing to combine the multiple alignments.</text>
              <doc_id>36</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Several models have been used for pair-wise alignment, starting with TER and proceeding with more sophisticated techniques such as HMM models, ITG, and IHMM (Rosti et.</text>
              <doc_id>37</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>al 2007a, Matusov et al 2008, Krakos et al. 2008, He et al. 2008).</text>
              <doc_id>38</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>A major problem with such methods is that each hypothesis is aligned to the backbone independently, leading to suboptimal behavior.</text>
              <doc_id>39</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>For example, suppose that we use a state-of-the-art word alignment model for pairs of hypotheses, such as the IHMM.</text>
              <doc_id>40</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Figure 1 shows likely alignment links between every pair of hypotheses.</text>
              <doc_id>41</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>If Hypothesis 1 is aligned to Hypothesis 2 (the backbone), Jeep is likely to align to SUV because they express similar Chinese content.</text>
              <doc_id>42</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Hypothesis 3 is separately aligned to the backbone and since the alignment is constrained to be one-to-one, SUV is aligned to SUV and Jeep to an empty word which is inserted after SUV.</text>
              <doc_id>43</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>The network in Figure 2a) is the result of this process.</text>
              <doc_id>44</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>An undesirable property of this CN is that the two instances of Jeep are placed in separate columns and cannot vote to reinforce each other.</text>
              <doc_id>45</doc_id>
              <sec_id>11</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Incremental alignment methods have been proposed to relax the independence assumption of pair-wise alignment (Rosti et al. 2008, Li et al. 2009).</text>
              <doc_id>46</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Such methods align hypotheses to a partially constructed CN in some order.</text>
              <doc_id>47</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For example, if in such method, Hypothesis 3 is first aligned to the backbone, followed by Hypothesis 1, we are likely to arrive at the CN in Figure 2b) in which the two instances of Jeep are aligned.</text>
              <doc_id>48</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>However, if Hypothesis 1 is aligned to the backbone first, we would still get the CN in Figure 2a).</text>
              <doc_id>49</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Notice that the desirable output &#8220;She bought the Jeep SUV&#8221; cannot be generated from either of the confusion networks because a rereordering of columns would be required.</text>
              <doc_id>50</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A common characteristic of CN-based approaches is that the order of words (backbone) and the alignment of words (correspondence sets) are decided as greedy steps independently of the lexical choice for the final output.</text>
              <doc_id>51</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The backbone and alignment are optimized according to auxiliary scoring functions and heuristics which may or may not be optimal with respect to producing CNs leading to good translations.</text>
              <doc_id>52</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In some recent approaches, these assumptions are relaxed to allow each input hypothesis as a backbone.</text>
              <doc_id>53</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Each backbone produces a separate CN and the decision of which CN to choose is taken at a later decoding stage, but this still restricts the possible orders and alignments greatly (Rosti et al. 2008, Matusov et al. 2008).</text>
              <doc_id>54</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this paper, we present a joint optimization method for system combination.</text>
              <doc_id>55</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this method, the alignment, ordering and lexical selection subproblems are solved jointly in a single decoding framework based on a log-linear model.</text>
              <doc_id>56</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>she</text>
              <doc_id>57</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>she</text>
              <doc_id>58</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>she</text>
              <doc_id>59</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>bought</text>
              <doc_id>60</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>buys</text>
              <doc_id>61</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>bought</text>
              <doc_id>62</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the</text>
              <doc_id>63</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the</text>
              <doc_id>64</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the</text>
              <doc_id>65</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#949;</text>
              <doc_id>66</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#949;</text>
              <doc_id>67</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Jeep</text>
              <doc_id>68</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>SUV</text>
              <doc_id>69</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>SUV</text>
              <doc_id>70</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#949;</text>
              <doc_id>71</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Jeep</text>
              <doc_id>72</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Figure 1.</text>
              <doc_id>73</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Three MT system hypotheses with pairwise alignments.</text>
              <doc_id>74</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>a) Confusion network with pair-wise alignment.</text>
              <doc_id>75</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>b) Confusion network with incremental alignment.</text>
              <doc_id>76</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Figure 2.</text>
              <doc_id>77</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Correspondence sets of confusion networks under pair-wise and incremental alignment, using the second hypothesis as a backbone.</text>
              <doc_id>78</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Related Work</title>
        <text>There has been a large body of work on MT system combination. Among confusion-networkbased algorithms, most relevant to our work are state-of-the-art methods for constructing word alignments (correspondence sets) and methods for improving the selection of a backbone hypothesis. We have already reviewed such work in the introduction and will note relation to
specific models throughout the paper as we discuss specifics of our scoring functions.
In confusion network algorithms which use pair-wise (or incremental) word-level alignment algorithms for correspondence set construction, problems of converting many-to-many alignments and handling multiple insertions and deletions need to be addressed. Prior work has used a number of heuristics to deal with these problems (Matusov, et. al., 2006, He et al 08). Some work has made such decisions in a more principled fashion by computing model-based scores (Matusov et al. 2008), but still specialpurpose algorithms and heuristics are needed and a single alignment is fixed.
In our approach, no heuristics are used to convert alignments and no concept of a backbone is used. Instead, the globally highest scoring combination of alignment, order, and lexical choice is selected (subject to search error).
Other than confusion-network-based algorithms, work most closely related to ours is the method of MT system combination proposed in (Jayaraman and Lavie 2005), which we will refer to as J&amp;L. Like our method, this approach performs word-level system combination and is not limited to following the word order of a single backbone hypothesis; it also allows more flexibility in the selection of correspondence sets during decoding, compared to a confusionnetwork-based approach. Even though their algorithm and ours are broadly similar, there are several important differences. Firstly, the J&amp;L approach is based on pairwise alignments between words in different hypotheses, which are hard and do not have associated probabilities. Every word in every hypothesis is aligned to at most one word from each of the remaining hypotheses. Thus there is no uncertainty about which words should belong to the correspondence set of an aligned word w, once that word is selected to extend a partial hypothesis during search. If words do not have corresponding matching words in some hypotheses, heuristic matching to currently unused words is attempted.
In contrast, our algorithm is based on the definition of a joint scoring model, which takes into account alignment uncertainty and combines information from word-level alignment models, ordering and lexical selection models, to address the three sub-problems of word-level system combination. In addition to the language model and word-voting features used by the J&amp;L model, we incorporate features which measure alignment confidence via word-level alignment models and features which evaluate re-ordering via distortion models with respect to original hypotheses. While the J&amp;L search algorithm incorporates a number of special-purpose heuristics to address phenomena of unused words lagging behind the last used words, the goal in our work is to minimize heuristics and perform search to jointly optimize the assignment of hidden variables (ordered correspondence sets) and observed output variables (words in final translations).
Finally, the J&amp;L method has not been evaluated in comparison to confusion-networkbased methods to study the impact of performing joint decoding for the three sub-problems.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>There has been a large body of work on MT system combination.</text>
              <doc_id>79</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Among confusion-networkbased algorithms, most relevant to our work are state-of-the-art methods for constructing word alignments (correspondence sets) and methods for improving the selection of a backbone hypothesis.</text>
              <doc_id>80</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We have already reviewed such work in the introduction and will note relation to</text>
              <doc_id>81</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>specific models throughout the paper as we discuss specifics of our scoring functions.</text>
              <doc_id>82</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In confusion network algorithms which use pair-wise (or incremental) word-level alignment algorithms for correspondence set construction, problems of converting many-to-many alignments and handling multiple insertions and deletions need to be addressed.</text>
              <doc_id>83</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Prior work has used a number of heuristics to deal with these problems (Matusov, et.</text>
              <doc_id>84</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>al., 2006, He et al 08).</text>
              <doc_id>85</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Some work has made such decisions in a more principled fashion by computing model-based scores (Matusov et al. 2008), but still specialpurpose algorithms and heuristics are needed and a single alignment is fixed.</text>
              <doc_id>86</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In our approach, no heuristics are used to convert alignments and no concept of a backbone is used.</text>
              <doc_id>87</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Instead, the globally highest scoring combination of alignment, order, and lexical choice is selected (subject to search error).</text>
              <doc_id>88</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Other than confusion-network-based algorithms, work most closely related to ours is the method of MT system combination proposed in (Jayaraman and Lavie 2005), which we will refer to as J&amp;L.</text>
              <doc_id>89</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Like our method, this approach performs word-level system combination and is not limited to following the word order of a single backbone hypothesis; it also allows more flexibility in the selection of correspondence sets during decoding, compared to a confusionnetwork-based approach.</text>
              <doc_id>90</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Even though their algorithm and ours are broadly similar, there are several important differences.</text>
              <doc_id>91</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Firstly, the J&amp;L approach is based on pairwise alignments between words in different hypotheses, which are hard and do not have associated probabilities.</text>
              <doc_id>92</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Every word in every hypothesis is aligned to at most one word from each of the remaining hypotheses.</text>
              <doc_id>93</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Thus there is no uncertainty about which words should belong to the correspondence set of an aligned word w, once that word is selected to extend a partial hypothesis during search.</text>
              <doc_id>94</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>If words do not have corresponding matching words in some hypotheses, heuristic matching to currently unused words is attempted.</text>
              <doc_id>95</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In contrast, our algorithm is based on the definition of a joint scoring model, which takes into account alignment uncertainty and combines information from word-level alignment models, ordering and lexical selection models, to address the three sub-problems of word-level system combination.</text>
              <doc_id>96</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In addition to the language model and word-voting features used by the J&amp;L model, we incorporate features which measure alignment confidence via word-level alignment models and features which evaluate re-ordering via distortion models with respect to original hypotheses.</text>
              <doc_id>97</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>While the J&amp;L search algorithm incorporates a number of special-purpose heuristics to address phenomena of unused words lagging behind the last used words, the goal in our work is to minimize heuristics and perform search to jointly optimize the assignment of hidden variables (ordered correspondence sets) and observed output variables (words in final translations).</text>
              <doc_id>98</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Finally, the J&amp;L method has not been evaluated in comparison to confusion-networkbased methods to study the impact of performing joint decoding for the three sub-problems.</text>
              <doc_id>99</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Notation</title>
        <text>Before elaborating the models and decoding algorithms, we first clarify the notation that will be used in the paper.
We denote by H =  1 , &#8230; ,  N the set of hypotheses from multiple MT systems, where  i is the hypothesis from the i-th system and  i is a word sequence w i,1 , &#8230; , w i,L(i) with length L(i). For simplicity, we assume that each system contributes only its 1-best hypothesis for combination. Accordingly, the i-th hypothesis  i will be associated with a weight W(i) which is the weight of the i-th system. In the scenario that N-best lists are available from individual systems for combination, the weight of each hypothesis can be computed based on its rank in the N-best list (Rosti et. al. 2007a).
Like in CN-based system combination, we construct a set of ordered correspondence sets (CS) from input hypotheses, and select one word from each CS to form the final output. A CS is defined as a set of (possibly empty) words, one from each hypothesis, that implicitly align to each other and that contributes exactly one of its words to the final output. A valid complete set of CS includes each non-empty word from each hypothesis in exactly one CS. As opposed to CNbased algorithms, our ordered correspondence sets are constructed during a joint decoding process which performs lexical selection at the same time.
To facilitate the presentation of our features, we define notation for ordered CS. A sequence of correspondence sets is denoted by C= CS 1 , &#8230; , CS m . Each correspondence set is specified by listing the positions of each of the words in the CS in their respective input
hypotheses. Each input hypothesis is assumed to have one special empty word &#949; at position 0. A CS is denoted by CS l 1 , &#8230; , l N = w 1,l1 , &#8230; , w N,lN , where w i,li is the l i -th word in the i-th hypothesis and the word position vector v = l 1 , &#8230; , l N T specifies the position of each word in its original hypothesis. Correspondingly, word w i,li has the same weight W(i) as its original hypothesis i . As an example, the last two correspondence sets specified by the CN in Figure 2a) would be specified as CS 4 = CS 4,4,4 = {Jeep, SUV, SUV} and CS 5 = CS 0,0,5 = {&#949;, &#949;, Jeep}.
As opposed to the CS defined in a conventional CN, words that have the same surface form but come from different hypotheses are not collapsed to be one single candidate since they have different original word positions. We need to trace each of them separately during the decoding process.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Before elaborating the models and decoding algorithms, we first clarify the notation that will be used in the paper.</text>
              <doc_id>100</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We denote by H =  1 , &#8230; ,  N the set of hypotheses from multiple MT systems, where  i is the hypothesis from the i-th system and  i is a word sequence w i,1 , &#8230; , w i,L(i) with length L(i).</text>
              <doc_id>101</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For simplicity, we assume that each system contributes only its 1-best hypothesis for combination.</text>
              <doc_id>102</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Accordingly, the i-th hypothesis  i will be associated with a weight W(i) which is the weight of the i-th system.</text>
              <doc_id>103</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In the scenario that N-best lists are available from individual systems for combination, the weight of each hypothesis can be computed based on its rank in the N-best list (Rosti et.</text>
              <doc_id>104</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>al. 2007a).</text>
              <doc_id>105</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Like in CN-based system combination, we construct a set of ordered correspondence sets (CS) from input hypotheses, and select one word from each CS to form the final output.</text>
              <doc_id>106</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A CS is defined as a set of (possibly empty) words, one from each hypothesis, that implicitly align to each other and that contributes exactly one of its words to the final output.</text>
              <doc_id>107</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A valid complete set of CS includes each non-empty word from each hypothesis in exactly one CS.</text>
              <doc_id>108</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As opposed to CNbased algorithms, our ordered correspondence sets are constructed during a joint decoding process which performs lexical selection at the same time.</text>
              <doc_id>109</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To facilitate the presentation of our features, we define notation for ordered CS.</text>
              <doc_id>110</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A sequence of correspondence sets is denoted by C= CS 1 , &#8230; , CS m .</text>
              <doc_id>111</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Each correspondence set is specified by listing the positions of each of the words in the CS in their respective input</text>
              <doc_id>112</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>hypotheses.</text>
              <doc_id>113</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each input hypothesis is assumed to have one special empty word &#949; at position 0.</text>
              <doc_id>114</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A CS is denoted by CS l 1 , &#8230; , l N = w 1,l1 , &#8230; , w N,lN , where w i,li is the l i -th word in the i-th hypothesis and the word position vector v = l 1 , &#8230; , l N T specifies the position of each word in its original hypothesis.</text>
              <doc_id>115</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Correspondingly, word w i,li has the same weight W(i) as its original hypothesis i .</text>
              <doc_id>116</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>As an example, the last two correspondence sets specified by the CN in Figure 2a) would be specified as CS 4 = CS 4,4,4 = {Jeep, SUV, SUV} and CS 5 = CS 0,0,5 = {&#949;, &#949;, Jeep}.</text>
              <doc_id>117</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>As opposed to the CS defined in a conventional CN, words that have the same surface form but come from different hypotheses are not collapsed to be one single candidate since they have different original word positions.</text>
              <doc_id>118</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We need to trace each of them separately during the decoding process.</text>
              <doc_id>119</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 A Joint Optimization Framework For System Combination</title>
        <text>The joint decoding framework chooses optimal output according to the following log-linear model:
F
i=1
&#945; i &#8901; f i (w, O, C, H)
where we denote by C the set of all possible valid arrangements of CS, O the set of all possible orders of CS, W the set of all possible word sequences, consisting of words from the input hypotheses. {f i (w, O, C, H)} are the features and {&#945; i } are the feature weights in the log-linear model, respectively.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The joint decoding framework chooses optimal output according to the following log-linear model:</text>
              <doc_id>120</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>F</text>
              <doc_id>121</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1</text>
              <doc_id>122</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#945; i &#8901; f i (w, O, C, H)</text>
              <doc_id>123</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where we denote by C the set of all possible valid arrangements of CS, O the set of all possible orders of CS, W the set of all possible word sequences, consisting of words from the input hypotheses.</text>
              <doc_id>124</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>{f i (w, O, C, H)} are the features and {&#945; i } are the feature weights in the log-linear model, respectively.</text>
              <doc_id>125</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Features</title>
            <text>A set of features are used in this framework. Each of them models one or more of the alignment, ordering, and lexical selection subproblems. Features are defined as follows.
Word posterior model:
The word posterior feature is the same as the one proposed by Rosti et. al. (2007a). i.e.,
f wp w, O, C, H =
M
m =1
log P w m CS m
where the posterior of a single word in a CS is
computed based on a weighted voting score:
P w i,li CS = P w i,li CS l 1 , &#8230; , l N
N
= W(k) &#948;(w k,lk = w i,li )
k=1
and M is the number of CS generated. Note that M may be larger than the length of the output word sequence w since some CS may generate empty words.
Bi-gram voting model:
The second feature we used is a bi-gram voting feature proposed by Zhao and He (2009), i.e., for each bi-gram w i , w i+1 , a weighted position-independent voting score is computed:
P w i , w i+1 H =
N
k=1
W(k) &#948;( w i , w i+1 &#8712;  i )
And the global bi-gram voting feature is defined as:
f bgv w, O, C, H =
|w|&#8722;1
i=1
log P w i , w i+1 H
Distortion model:
Unlike in the conventional CN-based system combination, flexible orders of CS are allowed in this joint decoding framework. In order to model the distortion of different orderings, a distortion model between two CS is defined as follows:
First we define the distortion cost between two words at a single hypothesis. Similarly to the distortion penalty in the conventional phrasebased decoder (Koehn 2004b), the distortion cost of jumping from a word at position i to another word at position j, d(i,j), is proportional to the distance between i and j, e.g., |i-j|. Then, the distortion cost of jumping from one CS, which has a position vector recording the original position of each word in that CS, to another CS is a weighted sum of single-hypothesis-based distortion costs:
d(CS m , CS m+1 ) =
N
k=1
W(k) &#8729; |l m ,k &#8722; l m+1,k |
where l m,k and l m+1,k are the k-th element of the word position vector of CS m and CS m+1 , respectively. For the purpose of computing the distortion feature, the position of an empty word is taken to be the same as the position of
the last visited non-empty word from the same hypothesis.
The overall ordering feature can then be computed based on d(CS m , CS m+1 ):
p(j|CS) =
N
k=1 k&#8800;j
p w j ,lj , w k,lk
M&#8722;1
f dis w, O, C, H = &#8722; d(CS m , CS m+1 )
m=1
It is worth noting that this is not the only feature modeling the re-ordering behavior. Under the joint decoding framework, other features such as the language model and bi-gram voting affect the ordering as well.
Alignment model:
Each CS consists of a set of words, one from each hypothesis, that are implicitly aligned to each other. Therefore, a valid complete set of CS defines the word alignment among different hypotheses. In this paper, we derive an alignment score of a CS based on alignment scores of word pairs in that CS. To compute scores for word pairs, we perform pair-wise hypothesis alignment using the indirect HMM (He et al. 2008) for every pair of input hypotheses. Note that this involves a total of N by (N-1)/2 bi-directional hypothesis alignments. The alignment score for a pair of words w j ,lj and w k,lk is defined as the average of posterior probabilities of alignment links in both directions and is thus direction independent:
p w j ,lj , w k,lk = 1 2 p(a l j = l k | j ,  k ) + p(a lk = l j | k ,  j )
If one of the two words is &#949;, the posterior of aligning word &#949; to state j is computed as suggested by Liang et al. (2006), i.e.,
p a 0 = l j  k ,  j =
L(k)
i=1
1 &#8722; p a i = l j  k ,  j
And p(a lj = 0| j ,  k ) can be computed by the HMM directly.
If both words are &#949;, then a pre-defined p &#949;&#949; is assigned, i.e., p a 0 = 0  k ,  j = p &#949;&#949; , where p &#949;&#949; can be optimized on a held-out validation set.
For a CS of words, if we set the j-th word as an anchor word, the probability that all other words align to that word is:
The alignment score of the whole CS is a weighted sum of the logarithm of the above alignment probabilities, i.e.,
S aln (CS) =
N
j =1
W(j) log P(j|CS)
and the global alignment score is computed as:
f aln w, O, C, H = S aln (CS m )
M
m=1
Entropy model:
In general, it is preferable to align the same word from different hypotheses into a common CS. Therefore, we use entropy to model the purity of a CS. The entropy of a CS is defined as:
Ent CS = Ent(CS l 1 , &#8230; , l N ) =
N &#8242;
i=1
P w i,li CS logP w i,li CS
where the sum is taken over all distinct words in the CS. Then the global entropy score is computed as:
f ent w, O, C, H =
M
m =1
Ent( CS m )
Other features used in our log-linear model include the count of real words |w|, a n-gram language model, and the count M of CS sets.
These features address one or more of the three sub-problems of MT system combination. By performing joint decoding with all these features working together, we hope to derive better decisions on alignment, ordering and lexical selection.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>A set of features are used in this framework.</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Each of them models one or more of the alignment, ordering, and lexical selection subproblems.</text>
                  <doc_id>127</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Features are defined as follows.</text>
                  <doc_id>128</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Word posterior model:</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The word posterior feature is the same as the one proposed by Rosti et.</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>al. (2007a).</text>
                  <doc_id>131</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>i.e.,</text>
                  <doc_id>132</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f wp w, O, C, H =</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>M</text>
                  <doc_id>134</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>m =1</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>log P w m CS m</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where the posterior of a single word in a CS is</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>computed based on a weighted voting score:</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P w i,li CS = P w i,li CS l 1 , &#8230; , l N</text>
                  <doc_id>139</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>N</text>
                  <doc_id>140</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>= W(k) &#948;(w k,lk = w i,li )</text>
                  <doc_id>141</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k=1</text>
                  <doc_id>142</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and M is the number of CS generated.</text>
                  <doc_id>143</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Note that M may be larger than the length of the output word sequence w since some CS may generate empty words.</text>
                  <doc_id>144</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Bi-gram voting model:</text>
                  <doc_id>145</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The second feature we used is a bi-gram voting feature proposed by Zhao and He (2009), i.e., for each bi-gram w i , w i+1 , a weighted position-independent voting score is computed:</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P w i , w i+1 H =</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>N</text>
                  <doc_id>148</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k=1</text>
                  <doc_id>149</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>W(k) &#948;( w i , w i+1 &#8712;  i )</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>And the global bi-gram voting feature is defined as:</text>
                  <doc_id>151</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f bgv w, O, C, H =</text>
                  <doc_id>152</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>|w|&#8722;1</text>
                  <doc_id>153</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>154</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>log P w i , w i+1 H</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Distortion model:</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Unlike in the conventional CN-based system combination, flexible orders of CS are allowed in this joint decoding framework.</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In order to model the distortion of different orderings, a distortion model between two CS is defined as follows:</text>
                  <doc_id>158</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>First we define the distortion cost between two words at a single hypothesis.</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Similarly to the distortion penalty in the conventional phrasebased decoder (Koehn 2004b), the distortion cost of jumping from a word at position i to another word at position j, d(i,j), is proportional to the distance between i and j, e.g., |i-j|.</text>
                  <doc_id>160</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Then, the distortion cost of jumping from one CS, which has a position vector recording the original position of each word in that CS, to another CS is a weighted sum of single-hypothesis-based distortion costs:</text>
                  <doc_id>161</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>d(CS m , CS m+1 ) =</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>N</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k=1</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>W(k) &#8729; |l m ,k &#8722; l m+1,k |</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where l m,k and l m+1,k are the k-th element of the word position vector of CS m and CS m+1 , respectively.</text>
                  <doc_id>166</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For the purpose of computing the distortion feature, the position of an empty word is taken to be the same as the position of</text>
                  <doc_id>167</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the last visited non-empty word from the same hypothesis.</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The overall ordering feature can then be computed based on d(CS m , CS m+1 ):</text>
                  <doc_id>169</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(j|CS) =</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>N</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k=1 k&#8800;j</text>
                  <doc_id>172</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p w j ,lj , w k,lk</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>M&#8722;1</text>
                  <doc_id>174</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f dis w, O, C, H = &#8722; d(CS m , CS m+1 )</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>m=1</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It is worth noting that this is not the only feature modeling the re-ordering behavior.</text>
                  <doc_id>177</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Under the joint decoding framework, other features such as the language model and bi-gram voting affect the ordering as well.</text>
                  <doc_id>178</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Alignment model:</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Each CS consists of a set of words, one from each hypothesis, that are implicitly aligned to each other.</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, a valid complete set of CS defines the word alignment among different hypotheses.</text>
                  <doc_id>181</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In this paper, we derive an alignment score of a CS based on alignment scores of word pairs in that CS.</text>
                  <doc_id>182</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>To compute scores for word pairs, we perform pair-wise hypothesis alignment using the indirect HMM (He et al. 2008) for every pair of input hypotheses.</text>
                  <doc_id>183</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Note that this involves a total of N by (N-1)/2 bi-directional hypothesis alignments.</text>
                  <doc_id>184</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The alignment score for a pair of words w j ,lj and w k,lk is defined as the average of posterior probabilities of alignment links in both directions and is thus direction independent:</text>
                  <doc_id>185</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p w j ,lj , w k,lk = 1 2 p(a l j = l k | j ,  k ) + p(a lk = l j | k ,  j )</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>If one of the two words is &#949;, the posterior of aligning word &#949; to state j is computed as suggested by Liang et al. (2006), i.e.,</text>
                  <doc_id>187</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p a 0 = l j  k ,  j =</text>
                  <doc_id>188</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(k)</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 &#8722; p a i = l j  k ,  j</text>
                  <doc_id>191</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>And p(a lj = 0| j ,  k ) can be computed by the HMM directly.</text>
                  <doc_id>192</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>If both words are &#949;, then a pre-defined p &#949;&#949; is assigned, i.e., p a 0 = 0  k ,  j = p &#949;&#949; , where p &#949;&#949; can be optimized on a held-out validation set.</text>
                  <doc_id>193</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For a CS of words, if we set the j-th word as an anchor word, the probability that all other words align to that word is:</text>
                  <doc_id>194</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The alignment score of the whole CS is a weighted sum of the logarithm of the above alignment probabilities, i.e.,</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>S aln (CS) =</text>
                  <doc_id>196</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>N</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j =1</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>W(j) log P(j|CS)</text>
                  <doc_id>199</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and the global alignment score is computed as:</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f aln w, O, C, H = S aln (CS m )</text>
                  <doc_id>201</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>M</text>
                  <doc_id>202</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>m=1</text>
                  <doc_id>203</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Entropy model:</text>
                  <doc_id>204</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In general, it is preferable to align the same word from different hypotheses into a common CS.</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, we use entropy to model the purity of a CS.</text>
                  <doc_id>206</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The entropy of a CS is defined as:</text>
                  <doc_id>207</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Ent CS = Ent(CS l 1 , &#8230; , l N ) =</text>
                  <doc_id>208</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>N &#8242;</text>
                  <doc_id>209</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>210</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P w i,li CS logP w i,li CS</text>
                  <doc_id>211</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where the sum is taken over all distinct words in the CS.</text>
                  <doc_id>212</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Then the global entropy score is computed as:</text>
                  <doc_id>213</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f ent w, O, C, H =</text>
                  <doc_id>214</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>M</text>
                  <doc_id>215</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>m =1</text>
                  <doc_id>216</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Ent( CS m )</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Other features used in our log-linear model include the count of real words |w|, a n-gram language model, and the count M of CS sets.</text>
                  <doc_id>218</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>These features address one or more of the three sub-problems of MT system combination.</text>
                  <doc_id>219</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>By performing joint decoding with all these features working together, we hope to derive better decisions on alignment, ordering and lexical selection.</text>
                  <doc_id>220</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Joint Decoding</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>221</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Core algorithm</title>
            <text>Decoding is based on a beam search algorithm similar to that of the phrase-based MT decoder (Koehn 2004b). The input is a set of translation hypotheses to be combined, and the final output
sentence is generated left to right. Figure 3 illustrates the decoding process, using the example input hypotheses from Figure 1. Each decoding state represents a partial sequence of correspondence sets covering some of the words in the input hypotheses and a sequence of words selected from the CS to form a partial output hypothesis. The initial decoding state has an empty sequence of CS and an empty output sequence. A state corresponds to a complete output candidate if its CS covers all input words.
lm: &#8230; bought the
a) a decoding state
lm: &#8230; bought the
b) seed states
lm: &#8230; bought the
c) correspondence set states
lm: &#8230; the Jeep
d) decoding states
lm: &#8230; bought the
lm: &#8230; bought the
lm: &#8230; the Jeep
Figure 3. Illustration of the decoding process.
In practice, because the features over hypotheses can be decomposed, we do not need to encode all of this information in a decoding state. It suffices to store a few attributes. They include positions of words from each input hypothesis that have been visited, the last two non-empty words generated (if a tri-gram LM is used), and an "end position vector (EPV)" recording positions of words in the last CS, which were just visited. In the figure, the visited words are shown with filled circles and the EPV is shown with a dotted pattern in the filled circles. Words specified by the EPV are implicitly aligned. In the state in Figure 3 a) the first three words of each hypothesis have been visited, the third word of each hypothesis is the last word visited (in the EPV), and the last two words produced are &#8220;bought the&#8221;. The states also record the decoding score accumulated so far and an estimated future score to cover words that have not been visited yet (not shown).
The expansion from one decoding state to a set of new decoding states is illustrated in Figure 3. The expansion is done in three steps with the help of intermediate states. Starting from a decoding state as shown in Figure 3a), first a set of &#8220;seed states&#8221; as shown in Figure 3b) are generated. Each seed state represents a choice of one of unvisited words, called a &#8220;seed word&#8221; which is selected and marked as visited. For example, the word Jeep from the first hypothesis and the word SUV from the second hypothesis are selected in the two seed states shown in Figure 3b), respectively. These seed states further expand into a set of "CS states" as shown in Figure 3c). I.e., a CS is formed by picking one word from each of the other hypotheses which is unvisited and has a valid alignment link to the seed word. Figure 3c) shows two CS states expanded from the first seed state of Figure 3b), using Jeep from the first hypothesis as a seed word. In one of them the empty word from the second hypothesis is chosen, and in the other, the word SUV is chosen. Both are allowed by the alignments illustrated in Figure 1. Finally, each CS state generates one or more complete decoding states, in which a word is chosen from the current CS and the EPV vector is advanced to reflect the last newly visited words. Figure 3d) shows two such states, descending from the corresponding CS states in 3c). After one more expansion the state in 3d) on the left can generate the translation &#8220;She bought the Jeep SUV&#8221;, which cannot be produced by either confusion network in Figure 2.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Decoding is based on a beam search algorithm similar to that of the phrase-based MT decoder (Koehn 2004b).</text>
                  <doc_id>222</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The input is a set of translation hypotheses to be combined, and the final output</text>
                  <doc_id>223</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>sentence is generated left to right.</text>
                  <doc_id>224</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 3 illustrates the decoding process, using the example input hypotheses from Figure 1.</text>
                  <doc_id>225</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Each decoding state represents a partial sequence of correspondence sets covering some of the words in the input hypotheses and a sequence of words selected from the CS to form a partial output hypothesis.</text>
                  <doc_id>226</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The initial decoding state has an empty sequence of CS and an empty output sequence.</text>
                  <doc_id>227</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>A state corresponds to a complete output candidate if its CS covers all input words.</text>
                  <doc_id>228</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>lm: &#8230; bought the</text>
                  <doc_id>229</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>a) a decoding state</text>
                  <doc_id>230</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>lm: &#8230; bought the</text>
                  <doc_id>231</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>b) seed states</text>
                  <doc_id>232</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>lm: &#8230; bought the</text>
                  <doc_id>233</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c) correspondence set states</text>
                  <doc_id>234</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>lm: &#8230; the Jeep</text>
                  <doc_id>235</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>d) decoding states</text>
                  <doc_id>236</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>lm: &#8230; bought the</text>
                  <doc_id>237</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>lm: &#8230; bought the</text>
                  <doc_id>238</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>lm: &#8230; the Jeep</text>
                  <doc_id>239</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 3.</text>
                  <doc_id>240</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Illustration of the decoding process.</text>
                  <doc_id>241</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In practice, because the features over hypotheses can be decomposed, we do not need to encode all of this information in a decoding state.</text>
                  <doc_id>242</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It suffices to store a few attributes.</text>
                  <doc_id>243</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>They include positions of words from each input hypothesis that have been visited, the last two non-empty words generated (if a tri-gram LM is used), and an "end position vector (EPV)" recording positions of words in the last CS, which were just visited.</text>
                  <doc_id>244</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In the figure, the visited words are shown with filled circles and the EPV is shown with a dotted pattern in the filled circles.</text>
                  <doc_id>245</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Words specified by the EPV are implicitly aligned.</text>
                  <doc_id>246</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In the state in Figure 3 a) the first three words of each hypothesis have been visited, the third word of each hypothesis is the last word visited (in the EPV), and the last two words produced are &#8220;bought the&#8221;.</text>
                  <doc_id>247</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The states also record the decoding score accumulated so far and an estimated future score to cover words that have not been visited yet (not shown).</text>
                  <doc_id>248</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The expansion from one decoding state to a set of new decoding states is illustrated in Figure 3.</text>
                  <doc_id>249</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The expansion is done in three steps with the help of intermediate states.</text>
                  <doc_id>250</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Starting from a decoding state as shown in Figure 3a), first a set of &#8220;seed states&#8221; as shown in Figure 3b) are generated.</text>
                  <doc_id>251</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Each seed state represents a choice of one of unvisited words, called a &#8220;seed word&#8221; which is selected and marked as visited.</text>
                  <doc_id>252</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For example, the word Jeep from the first hypothesis and the word SUV from the second hypothesis are selected in the two seed states shown in Figure 3b), respectively.</text>
                  <doc_id>253</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>These seed states further expand into a set of "CS states" as shown in Figure 3c).</text>
                  <doc_id>254</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>I.e., a CS is formed by picking one word from each of the other hypotheses which is unvisited and has a valid alignment link to the seed word.</text>
                  <doc_id>255</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 3c) shows two CS states expanded from the first seed state of Figure 3b), using Jeep from the first hypothesis as a seed word.</text>
                  <doc_id>256</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>In one of them the empty word from the second hypothesis is chosen, and in the other, the word SUV is chosen.</text>
                  <doc_id>257</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>Both are allowed by the alignments illustrated in Figure 1.</text>
                  <doc_id>258</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, each CS state generates one or more complete decoding states, in which a word is chosen from the current CS and the EPV vector is advanced to reflect the last newly visited words.</text>
                  <doc_id>259</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 3d) shows two such states, descending from the corresponding CS states in 3c).</text>
                  <doc_id>260</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
                <sentence>
                  <text>After one more expansion the state in 3d) on the left can generate the translation &#8220;She bought the Jeep SUV&#8221;, which cannot be produced by either confusion network in Figure 2.</text>
                  <doc_id>261</doc_id>
                  <sec_id>12</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Pruning</title>
            <text>The full search space of joint decoding is a product of the alignment, ordering, and lexical selection spaces. Its size is exponential in the length of the sentence and the number of hypotheses involved in combination. Therefore, pruning techniques are necessary to reduce the search space.
First we will prune down the alignment space. Instead of allowing any alignment link between
arbitrary words of two hypotheses, only links that have alignment score higher than a threshold are allowed, plus links in the union of the Viterbi alignments in both directions. In order to prevent the garbage collection problem where many words align to a rare word at the other side (Moore, 2004), we further impose the limit that if one word is aligned to more than T words, these links are sorted by their alignment score and only the top T links are kept. Meanwhile, alignments between a real word and &#949; are always allowed.
We then prune down the ordering space by limiting the expansion of new states. Only states that are adjacent to their preceding states are created. Two states are called adjacent if their EPVs are adjacent, i.e., given the EPV of the preceding state m as l m,1 , &#8230; , l T m,N and the EPV of the next state m+1 as
l m+1,1 , &#8230; , l m+1,N T , if at least at one dimension k, l m +1,k = l m,k +1, then these two states are adjacent. When checking the adjacency of two states, the position of an empty word is taken to be the same as the position of the last visited non-empty word from the same hypothesis.
The number of possible CS states expanded from a decoding state is exponential in the number of hypotheses. In decoding, these CS states are sorted by their alignment scores and only the top K CS states are kept.
The search space can be further pruned down by the widely used technique of path recombination and by best-first pruning.
Path recombination is a risk-free pruning method. Two paths can be recombined if they agree on a) words from each hypothesis that have been visited so far, b) the last two real words generated, and c) their EPVs. In such case, we only need to keep the path with the higher score.
Best-first pruning can help to reduce the search space even further. In the decoding process we compare paths that have generated the same number of words (both real and empty words) and only keep a certain number of most promising paths. Pruning is based on an estimated overall score of each path, which is the sum of the decoding score accumulated so far and an estimated future score to cover the words that have not been visited. Next we discuss the future score computation.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The full search space of joint decoding is a product of the alignment, ordering, and lexical selection spaces.</text>
                  <doc_id>262</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Its size is exponential in the length of the sentence and the number of hypotheses involved in combination.</text>
                  <doc_id>263</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, pruning techniques are necessary to reduce the search space.</text>
                  <doc_id>264</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>First we will prune down the alignment space.</text>
                  <doc_id>265</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Instead of allowing any alignment link between</text>
                  <doc_id>266</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>arbitrary words of two hypotheses, only links that have alignment score higher than a threshold are allowed, plus links in the union of the Viterbi alignments in both directions.</text>
                  <doc_id>267</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In order to prevent the garbage collection problem where many words align to a rare word at the other side (Moore, 2004), we further impose the limit that if one word is aligned to more than T words, these links are sorted by their alignment score and only the top T links are kept.</text>
                  <doc_id>268</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Meanwhile, alignments between a real word and &#949; are always allowed.</text>
                  <doc_id>269</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We then prune down the ordering space by limiting the expansion of new states.</text>
                  <doc_id>270</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Only states that are adjacent to their preceding states are created.</text>
                  <doc_id>271</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Two states are called adjacent if their EPVs are adjacent, i.e., given the EPV of the preceding state m as l m,1 , &#8230; , l T m,N and the EPV of the next state m+1 as</text>
                  <doc_id>272</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>l m+1,1 , &#8230; , l m+1,N T , if at least at one dimension k, l m +1,k = l m,k +1, then these two states are adjacent.</text>
                  <doc_id>273</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>When checking the adjacency of two states, the position of an empty word is taken to be the same as the position of the last visited non-empty word from the same hypothesis.</text>
                  <doc_id>274</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The number of possible CS states expanded from a decoding state is exponential in the number of hypotheses.</text>
                  <doc_id>275</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In decoding, these CS states are sorted by their alignment scores and only the top K CS states are kept.</text>
                  <doc_id>276</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The search space can be further pruned down by the widely used technique of path recombination and by best-first pruning.</text>
                  <doc_id>277</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Path recombination is a risk-free pruning method.</text>
                  <doc_id>278</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Two paths can be recombined if they agree on a) words from each hypothesis that have been visited so far, b) the last two real words generated, and c) their EPVs.</text>
                  <doc_id>279</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In such case, we only need to keep the path with the higher score.</text>
                  <doc_id>280</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Best-first pruning can help to reduce the search space even further.</text>
                  <doc_id>281</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the decoding process we compare paths that have generated the same number of words (both real and empty words) and only keep a certain number of most promising paths.</text>
                  <doc_id>282</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Pruning is based on an estimated overall score of each path, which is the sum of the decoding score accumulated so far and an estimated future score to cover the words that have not been visited.</text>
                  <doc_id>283</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Next we discuss the future score computation.</text>
                  <doc_id>284</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>5.3 Computing the future score</title>
            <text>In order to estimate the future cost of an unfinished path, we treat the unvisited words of one input hypothesis as a backbone, and apply a greedy search for alignment based on it; i.e., for each word of this backbone, the most likely words (based on the alignment link scores) from other hypotheses, one word from each hypothesis, are collected to form a CS. These CS are ordered according to the word order of the backbone and form a CN. Then, a light decoding process with a search beam of size one is applied to decode this CN and find the approximate future path, with future feature scores computed during the decoding process. If there are leftover words not included in this CN, they are treated in the way described in section 5.4. Additionally, caching techniques are applied to speed up the computation of future scores further.
Given the method discussed above, we can estimate a future score based on each input hypothesis, and the final future score is estimated as the best of these hypothesis-dependent scores.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In order to estimate the future cost of an unfinished path, we treat the unvisited words of one input hypothesis as a backbone, and apply a greedy search for alignment based on it; i.e., for each word of this backbone, the most likely words (based on the alignment link scores) from other hypotheses, one word from each hypothesis, are collected to form a CS.</text>
                  <doc_id>285</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These CS are ordered according to the word order of the backbone and form a CN.</text>
                  <doc_id>286</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Then, a light decoding process with a search beam of size one is applied to decode this CN and find the approximate future path, with future feature scores computed during the decoding process.</text>
                  <doc_id>287</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>If there are leftover words not included in this CN, they are treated in the way described in section 5.4.</text>
                  <doc_id>288</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Additionally, caching techniques are applied to speed up the computation of future scores further.</text>
                  <doc_id>289</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Given the method discussed above, we can estimate a future score based on each input hypothesis, and the final future score is estimated as the best of these hypothesis-dependent scores.</text>
                  <doc_id>290</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>5.4 Dealing with leftover input words</title>
            <text>At a certain point a path will reach the end, i.e., no more states can be generated from it according to the state expansion requirement. Then it is marked as a finished path. However, sometimes the state may contain a few input words that have not been visited. An example of this situation is the second state in Figure 3d). The word SUV in the third input hypothesis is left unvisited and it cannot be selected next because there is no adjacent state that can be generated. For such cases, we need to compute an extra score of covering these leftover words. Our approach is to create a state that produces the same output translation, but also covers all remaining words. For each leftover word, we create a pseudo CS that contains just that word plus &#949;&#8217;s from all other hypotheses, and let it output &#949;. Moreover, that CS is inserted at a place such that no extra distortion cost is incurred. Figure 4 shows an example using the second state in Figure 3d). The last two words from the first two MT hypotheses &#8220;the Jeep&#8221; and &#8220;the SUV&#8221; align to the third and fifth words of the third hypothesis &#8220;the Jeep&#8221;; the word w 3,4 from the third hypothesis is left unvisited. The original path has two CS and one left-over word w 3,4 . It is expanded to have three CS, with a pseudo CS inserted between the two CS.
It is worth noting that the new inserted pseudo CS will not affect the word count feature and contextually dependent feature scores such as the LM and bi-gram voting, since it only generates an empty word. Moreover, it will not affect the
distortion score either. For example, as shown in Figure 4, the distortion cost of jumping from word w 2,3 to &#949; 2 and then to w 2,4 is the same as the cost of jumping from w 2,3 to w 2,4 given the way we assign position to empty word and the fact that the distortion cost is proportional to the difference between word positions.
Scores of other features for this pseudo CS such as word posterior (of &#949;), alignment score, CS entropy, and CS count are all local scores and can be computed easily. Unlike future scores which are approximate, the score computed in this process is exact. Adding this extra score to the existing score accumulated in the final state gives the complete score of this finished path. When all paths are finished, the one with the best complete score is returned as the final output sentence.
w 1,3 w 1,4 w 1,3 &#949; 1 w 1,4
w 2,3 w 2,4 =&gt; w 2,3 &#949; 2 w 2,4
w 3,3 w 3,4 w 3,5 w 3,3 w 3,4 w 3,5
Figure 4. Expanding a leftover word to a pseudo correspondence set.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>At a certain point a path will reach the end, i.e., no more states can be generated from it according to the state expansion requirement.</text>
                  <doc_id>291</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Then it is marked as a finished path.</text>
                  <doc_id>292</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, sometimes the state may contain a few input words that have not been visited.</text>
                  <doc_id>293</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>An example of this situation is the second state in Figure 3d).</text>
                  <doc_id>294</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The word SUV in the third input hypothesis is left unvisited and it cannot be selected next because there is no adjacent state that can be generated.</text>
                  <doc_id>295</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For such cases, we need to compute an extra score of covering these leftover words.</text>
                  <doc_id>296</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Our approach is to create a state that produces the same output translation, but also covers all remaining words.</text>
                  <doc_id>297</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>For each leftover word, we create a pseudo CS that contains just that word plus &#949;&#8217;s from all other hypotheses, and let it output &#949;. Moreover, that CS is inserted at a place such that no extra distortion cost is incurred.</text>
                  <doc_id>298</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 4 shows an example using the second state in Figure 3d).</text>
                  <doc_id>299</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>The last two words from the first two MT hypotheses &#8220;the Jeep&#8221; and &#8220;the SUV&#8221; align to the third and fifth words of the third hypothesis &#8220;the Jeep&#8221;; the word w 3,4 from the third hypothesis is left unvisited.</text>
                  <doc_id>300</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>The original path has two CS and one left-over word w 3,4 .</text>
                  <doc_id>301</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>It is expanded to have three CS, with a pseudo CS inserted between the two CS.</text>
                  <doc_id>302</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It is worth noting that the new inserted pseudo CS will not affect the word count feature and contextually dependent feature scores such as the LM and bi-gram voting, since it only generates an empty word.</text>
                  <doc_id>303</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Moreover, it will not affect the</text>
                  <doc_id>304</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>distortion score either.</text>
                  <doc_id>305</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For example, as shown in Figure 4, the distortion cost of jumping from word w 2,3 to &#949; 2 and then to w 2,4 is the same as the cost of jumping from w 2,3 to w 2,4 given the way we assign position to empty word and the fact that the distortion cost is proportional to the difference between word positions.</text>
                  <doc_id>306</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Scores of other features for this pseudo CS such as word posterior (of &#949;), alignment score, CS entropy, and CS count are all local scores and can be computed easily.</text>
                  <doc_id>307</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Unlike future scores which are approximate, the score computed in this process is exact.</text>
                  <doc_id>308</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Adding this extra score to the existing score accumulated in the final state gives the complete score of this finished path.</text>
                  <doc_id>309</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>When all paths are finished, the one with the best complete score is returned as the final output sentence.</text>
                  <doc_id>310</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w 1,3 w 1,4 w 1,3 &#949; 1 w 1,4</text>
                  <doc_id>311</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w 2,3 w 2,4 =&gt; w 2,3 &#949; 2 w 2,4</text>
                  <doc_id>312</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w 3,3 w 3,4 w 3,5 w 3,3 w 3,4 w 3,5</text>
                  <doc_id>313</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 4.</text>
                  <doc_id>314</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Expanding a leftover word to a pseudo correspondence set.</text>
                  <doc_id>315</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>6</index>
        <title>6 Evaluation</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>316</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>6.1 Experimental conditions</title>
            <text>For the joint decoding method, the threshold for alignment-score-based pruning is set to 0.25 and the maximum number of words that can align to the same word is limited to 3. We call this the standard setting. The joint decoding approach is evaluated on the Chinese-to-English (C2E) test set of the 2008 NIST Open MT Evaluation (NIST 2008). Results are reported in case insensitive BLEU score in percentages (Papineni et. al., 2002).
The NIST MT08 C2E test set contains 691 and 666 sentences of data from two genres, newswire and web-data, respectively. Each test sentence has four references provided by human translators. Individual systems in our experiments belong to the official submissions of the MT08 C2E constraint-training track. Each submission provides 1-best translation of the whole test set. In order to train feature weights, the original test set is divided into two parts, called the dev and test set, respectively. The dev set consists of the first half of both newswire and web-data, and the test set consists of the second half of data of both genres.
There are 20 individual systems available. We ranked them by their BLEU score results on the dev set and picked the top five systems, excluding systems ranked 5th and 6th since they are subsets of the first entry (NIST 2008). Performance of these systems on the dev and test sets is shown in Table 1.
The baselines include a pair-wise hypothesis alignment approach using the indirect HMM (IHMM) proposed by He et al. (2008), and an incremental hypothesis alignment approach using the incremental HMM (IncHMM) proposed by Li et al. (2009). The lexical translation model used to compute the semantic similarity is estimated from two million parallel sentencepairs selected from the training corpus of MT08. The backbone for the IHMM-based approach is selected based on Minimum Bayes Risk (MBR) using a BLEU-based loss function. The various parameters of the IHMM and the IncHMM are tuned on the dev set. The same IHMM is used to compute the alignment feature score for the joint decoding approach.
The final combination output can be obtained by decoding the CN with a set of features. The features used for the baseline systems are the same as the features used by the joint decoding approach. Some of these features are constant across decoding hypotheses and can be ignored. The non-constant features are word posterior, bigram voting, language model score, and word count. They are computed in the same way as for the joint decoding approach.
System weights and feature weights are trained together using Powell's search for the IHMM-based approach. Then the same system weights are applied to both IncHMM and Joint Decoding -based approaches, and the feature weights of them are trained using the max-BLEU training method proposed by Och (2003) and refined by Moore and Quirk (2008).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>For the joint decoding method, the threshold for alignment-score-based pruning is set to 0.25 and the maximum number of words that can align to the same word is limited to 3.</text>
                  <doc_id>317</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We call this the standard setting.</text>
                  <doc_id>318</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The joint decoding approach is evaluated on the Chinese-to-English (C2E) test set of the 2008 NIST Open MT Evaluation (NIST 2008).</text>
                  <doc_id>319</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Results are reported in case insensitive BLEU score in percentages (Papineni et.</text>
                  <doc_id>320</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>al., 2002).</text>
                  <doc_id>321</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The NIST MT08 C2E test set contains 691 and 666 sentences of data from two genres, newswire and web-data, respectively.</text>
                  <doc_id>322</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Each test sentence has four references provided by human translators.</text>
                  <doc_id>323</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Individual systems in our experiments belong to the official submissions of the MT08 C2E constraint-training track.</text>
                  <doc_id>324</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Each submission provides 1-best translation of the whole test set.</text>
                  <doc_id>325</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In order to train feature weights, the original test set is divided into two parts, called the dev and test set, respectively.</text>
                  <doc_id>326</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The dev set consists of the first half of both newswire and web-data, and the test set consists of the second half of data of both genres.</text>
                  <doc_id>327</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>There are 20 individual systems available.</text>
                  <doc_id>328</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We ranked them by their BLEU score results on the dev set and picked the top five systems, excluding systems ranked 5th and 6th since they are subsets of the first entry (NIST 2008).</text>
                  <doc_id>329</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Performance of these systems on the dev and test sets is shown in Table 1.</text>
                  <doc_id>330</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The baselines include a pair-wise hypothesis alignment approach using the indirect HMM (IHMM) proposed by He et al. (2008), and an incremental hypothesis alignment approach using the incremental HMM (IncHMM) proposed by Li et al. (2009).</text>
                  <doc_id>331</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The lexical translation model used to compute the semantic similarity is estimated from two million parallel sentencepairs selected from the training corpus of MT08.</text>
                  <doc_id>332</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The backbone for the IHMM-based approach is selected based on Minimum Bayes Risk (MBR) using a BLEU-based loss function.</text>
                  <doc_id>333</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The various parameters of the IHMM and the IncHMM are tuned on the dev set.</text>
                  <doc_id>334</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The same IHMM is used to compute the alignment feature score for the joint decoding approach.</text>
                  <doc_id>335</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The final combination output can be obtained by decoding the CN with a set of features.</text>
                  <doc_id>336</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The features used for the baseline systems are the same as the features used by the joint decoding approach.</text>
                  <doc_id>337</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Some of these features are constant across decoding hypotheses and can be ignored.</text>
                  <doc_id>338</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The non-constant features are word posterior, bigram voting, language model score, and word count.</text>
                  <doc_id>339</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>They are computed in the same way as for the joint decoding approach.</text>
                  <doc_id>340</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>System weights and feature weights are trained together using Powell's search for the IHMM-based approach.</text>
                  <doc_id>341</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Then the same system weights are applied to both IncHMM and Joint Decoding -based approaches, and the feature weights of them are trained using the max-BLEU training method proposed by Och (2003) and refined by Moore and Quirk (2008).</text>
                  <doc_id>342</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>6.2 Comparison against baselines</title>
            <text>Table 2 lists the BLEU scores achieved by the two baselines and the joint decoding approach. Both baselines surpass the best individual system
significantly. However, the gain of incremental HMM over IHMM is smaller than that reported in Li et al. (2009). One possible reason of such discrepancy could be that fewer hypotheses are used for combination in this experiment compared to that of Li et al. (2009), so the performance difference between them is narrowed accordingly. Despite that, the proposed joint decoding method outperforms both IHMM and IncHMM baselines significantly.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Table 2 lists the BLEU scores achieved by the two baselines and the joint decoding approach.</text>
                  <doc_id>343</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Both baselines surpass the best individual system</text>
                  <doc_id>344</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>significantly.</text>
                  <doc_id>345</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, the gain of incremental HMM over IHMM is smaller than that reported in Li et al. (2009).</text>
                  <doc_id>346</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>One possible reason of such discrepancy could be that fewer hypotheses are used for combination in this experiment compared to that of Li et al. (2009), so the performance difference between them is narrowed accordingly.</text>
                  <doc_id>347</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Despite that, the proposed joint decoding method outperforms both IHMM and IncHMM baselines significantly.</text>
                  <doc_id>348</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>6.3 Comparison of alignment pruning</title>
            <text>The effect of alignment pruning is also studied. We tested with limiting the allowable links to just those that in the union of bi-directional Viterbi alignments.
The results are presented in Table 3. Compared to the standard setting, allowing only links in the union of the bi-directional Viterbi alignments causes slight performance degradation. On the other hand, it still outperforms the IHMM baseline by a fair margin. This is because the joint decoding approach is effectively resolving the ambiguous 1-to-many alignments and deciding proper places to insert empty words during decoding.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The effect of alignment pruning is also studied.</text>
                  <doc_id>349</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We tested with limiting the allowable links to just those that in the union of bi-directional Viterbi alignments.</text>
                  <doc_id>350</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The results are presented in Table 3.</text>
                  <doc_id>351</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Compared to the standard setting, allowing only links in the union of the bi-directional Viterbi alignments causes slight performance degradation.</text>
                  <doc_id>352</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>On the other hand, it still outperforms the IHMM baseline by a fair margin.</text>
                  <doc_id>353</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This is because the joint decoding approach is effectively resolving the ambiguous 1-to-many alignments and deciding proper places to insert empty words during decoding.</text>
                  <doc_id>354</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>6.4 Comparison of ordering constraints</title>
            <text>In order to investigate the effect of allowing flexible word ordering, we conducted experiments using different constraints on the ordering of CS in the decoding process. In the first case, we restrict the order of CS to follow the word order of a backbone, which is one of the input hypotheses selected by MBR-BLEU. In the second case, the order of CS is constrained to follow the word order of at least one of the input hypotheses. As shown in Table 4, in comparison to the standard setting that allows backbone-free word ordering, the constrained settings did not lead to significant performance degradation. This indicates that most of the gain due to the joint decoding approach comes from the joint optimization of alignment and word selection. It is possible, though, that if we lift the CS adjacency constraint during search, we might derive more benefit from flexible word ordering.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In order to investigate the effect of allowing flexible word ordering, we conducted experiments using different constraints on the ordering of CS in the decoding process.</text>
                  <doc_id>355</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the first case, we restrict the order of CS to follow the word order of a backbone, which is one of the input hypotheses selected by MBR-BLEU.</text>
                  <doc_id>356</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In the second case, the order of CS is constrained to follow the word order of at least one of the input hypotheses.</text>
                  <doc_id>357</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>As shown in Table 4, in comparison to the standard setting that allows backbone-free word ordering, the constrained settings did not lead to significant performance degradation.</text>
                  <doc_id>358</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This indicates that most of the gain due to the joint decoding approach comes from the joint optimization of alignment and word selection.</text>
                  <doc_id>359</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>It is possible, though, that if we lift the CS adjacency constraint during search, we might derive more benefit from flexible word ordering.</text>
                  <doc_id>360</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>7</index>
        <title>7 Discussion</title>
        <text>This paper proposed a joint optimization approach for word-level combination of translation hypotheses from multiple machine translation systems. Unlike conventional confusion-network-based methods, alignments between words from different hypotheses are not pre-determined and flexible word orderings are allowed. Decisions on word alignment between hypotheses, word ordering, and the lexical choice of the final output are made jointly according to a set of features in the decoding process. A new set of features to model alignment and reordering behavior is also proposed. The method is evaluated against state-of-the-art baselines on the NIST MT08 C2E task. The joint decoding approach is shown to outperform baselines significantly. Because of the complexity of search, a challenge for our approach is combining a large number of input hypotheses. When N-best hypotheses from the same system are added, it is possible to pre-compute and fix the one-to-one word alignment among the same-system hypotheses; such pre-computation is reasonable given our observation that the disagreement among hypotheses from different systems is larger than that among hypotheses from the same system. This will reduce the alignment search space to be the same as that for 1-best case. We plan to study this setting in future work. To further improve the performance of our approach we see the biggest opportunity in developing better estimates of future scores and incorporating additional features. Beside potential performance improvement, they may help on more effective pruning and speed up the overall decoding process as well.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This paper proposed a joint optimization approach for word-level combination of translation hypotheses from multiple machine translation systems.</text>
              <doc_id>361</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Unlike conventional confusion-network-based methods, alignments between words from different hypotheses are not pre-determined and flexible word orderings are allowed.</text>
              <doc_id>362</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Decisions on word alignment between hypotheses, word ordering, and the lexical choice of the final output are made jointly according to a set of features in the decoding process.</text>
              <doc_id>363</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>A new set of features to model alignment and reordering behavior is also proposed.</text>
              <doc_id>364</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The method is evaluated against state-of-the-art baselines on the NIST MT08 C2E task.</text>
              <doc_id>365</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The joint decoding approach is shown to outperform baselines significantly.</text>
              <doc_id>366</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Because of the complexity of search, a challenge for our approach is combining a large number of input hypotheses.</text>
              <doc_id>367</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>When N-best hypotheses from the same system are added, it is possible to pre-compute and fix the one-to-one word alignment among the same-system hypotheses; such pre-computation is reasonable given our observation that the disagreement among hypotheses from different systems is larger than that among hypotheses from the same system.</text>
              <doc_id>368</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>This will reduce the alignment search space to be the same as that for 1-best case.</text>
              <doc_id>369</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>We plan to study this setting in future work.</text>
              <doc_id>370</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>To further improve the performance of our approach we see the biggest opportunity in developing better estimates of future scores and incorporating additional features.</text>
              <doc_id>371</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>Beside potential performance improvement, they may help on more effective pruning and speed up the overall decoding process as well.</text>
              <doc_id>372</doc_id>
              <sec_id>11</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Comparison between the joint decoding approach and the two baselines</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>System ID</cell>
              <cell>dev</cell>
              <cell>test</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System A</cell>
              <cell>32.88</cell>
              <cell>31.81</cell>
            </row>
            <row>
              <cell>System B</cell>
              <cell>32.82</cell>
              <cell>32.03</cell>
            </row>
            <row>
              <cell>System C</cell>
              <cell>32.16</cell>
              <cell>31.87</cell>
            </row>
            <row>
              <cell>System D</cell>
              <cell>31.40</cell>
              <cell>31.32</cell>
            </row>
            <row>
              <cell>System E</cell>
              <cell>27.44</cell>
              <cell>27.67</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 2: Comparison between the joint decoding  approach and the two baselines#@#@Table 3: Comparison between different settings of alignment pruning</caption>
        <reference_text>In PAGE 8: ...67  6.2  Comparison against baselines   Table2   lists  the  BLEU  scores  achieved  by  the  two  baselines  and  the  joint  decoding  approach.  Both baselines surpass the best individual system ...</reference_text>
        <page_num>9</page_num>
        <head>
          <rows>
            <row>
              <cell>method</cell>
              <cell>dev</cell>
              <cell>test</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>IHMM</cell>
              <cell>36.91</cell>
              <cell>35.85</cell>
            </row>
            <row>
              <cell>IncHMM</cell>
              <cell>37.32</cell>
              <cell>36.38</cell>
            </row>
            <row>
              <cell>Joint Decoding</cell>
              <cell>37.94</cell>
              <cell>37.20 *</cell>
            </row>
            <row>
              <cell>* The gains of Joint Decoding over IHMM and</cell>
            </row>
            <row>
              <cell>IncHMM are both with a statistical significance level &gt;</cell>
            </row>
            <row>
              <cell>99%, measured based on the paired bootstrap resampling</cell>
            </row>
            <row>
              <cell>method (Koehn 2004a)</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: Effect of ordering constraints</caption>
        <reference_text>In PAGE 9: ... In  the second case, the order of CS is constrained to  follow the word order of at least one of the input  hypotheses.  As shown in  Table4 , in comparison  to the standard setting that allows backbone-free  word  ordering,  the  constrained  settings  did  not  lead to significant performance degradation. This  indicates  that  most  of  the  gain  due  to  the  joint  decoding  approach  comes  from  the  joint  optimization of alignment and word selection....</reference_text>
        <page_num>9</page_num>
        <head>
          <rows>
            <row>
              <cell>Setting  standard settings</cell>
              <cell>Setting    standard settings</cell>
              <cell>standard settings</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>test     37.20</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>monotone w.r.t. backbone</cell>
              <cell>monotone w.r.t. backbone</cell>
              <cell>monotone w.r.t. backbone</cell>
              <cell>monotone w.r.t. backbone</cell>
              <cell>monotone w.r.t. backbone</cell>
              <cell>37.22</cell>
            </row>
            <row>
              <cell>monotone w.r.t. any hyp.</cell>
              <cell>monotone w.r.t. any hyp.</cell>
              <cell>monotone w.r.t. any hyp.</cell>
              <cell>monotone w.r.t. any hyp.</cell>
              <cell>None</cell>
              <cell>37.12</cell>
            </row>
            <row>
              <cell>7</cell>
              <cell>Discussion</cell>
              <cell>Discussion</cell>
            </row>
            <row>
              <cell>This</cell>
              <cell>paper</cell>
              <cell>proposed</cell>
              <cell>a</cell>
              <cell>joint</cell>
              <cell>optimization</cell>
            </row>
            <row>
              <cell>approach</cell>
              <cell>approach</cell>
              <cell>for</cell>
              <cell>word-level</cell>
              <cell>combination</cell>
              <cell>combination</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Srinivas Bangalore</author>
          <author>German Bordel</author>
          <author>Giuseppe Riccardi</author>
        </authors>
        <title>Computing consensus translation from multiple machine translation systems.</title>
        <publication>In Proceedings of IEEE ASRU.</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Xiaodong He</author>
          <author>Mei Yang</author>
          <author>Jianfeng Gao</author>
          <author>Patrick Nguyen</author>
          <author>Robert Moore</author>
        </authors>
        <title>Indirect HMM based Hypothesis Alignment for Combining Outputs from Machine Translation Systems.</title>
        <publication>In Proceedings of EMNLP.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Shyamsundar Jayaraman</author>
          <author>Alon Lavie</author>
        </authors>
        <title>Multi-engine machine translation guided by explicit word matching.</title>
        <publication>In Proceedings of EAMT.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Damianos Karakos</author>
          <author>Jason Eisner</author>
          <author>Sanjeev Khudanpur</author>
          <author>Markus Dreyer</author>
        </authors>
        <title>Machine Translation System Combination using ITG-based Alignments.</title>
        <publication>In Proceedings of ACL.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Philipp Koehn</author>
        </authors>
        <title>2004a, Statistical Significance Tests for Machine Translation Evaluation.</title>
        <publication>In Proceedings of EMNLP.</publication>
        <pages>None</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Philipp Koehn</author>
        </authors>
        <title>Pharaoh: A Beam Search Decoder For Phrase Based Statistical Machine Translation Models.</title>
        <publication>In Proceedings of AMTA.</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Chi-Ho Li</author>
        </authors>
        <title>Xiaodong He, Yupeng Liu and Ning Xi,</title>
        <publication>In Proceedings of ACL.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>NIST</author>
        </authors>
        <title>The NIST Open Machine Translation Evaluation.www.nist.gov/speech/tests/mt/2008/doc/ Franz</title>
        <publication>In Proceedings of ACL.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>Wei- Jing Zhu</author>
        </authors>
        <title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
        <publication>In Proceedings of ACL. Antti-Veikko</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>2007a</author>
        </authors>
        <title>Combining outputs from multiple machine translation systems.</title>
        <publication>In Proceedings of NAACL-HLT.</publication>
        <pages>None</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Antti-Veikko I Rosti</author>
        </authors>
        <title>Spyros Matsoukas, and Richard Schwartz 2007b. Improved Word-level System Combination for Machine Translation.</title>
        <publication>In Proceedings of ACL.</publication>
        <pages>None</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Antti-Veikko I Rosti</author>
          <author>Bing Zhang</author>
          <author>Spyros Matsoukas</author>
          <author>Richard Schwartz</author>
        </authors>
        <title>Incremental Hypothesis Alignment for Building Confusion Networks with Application to Machine Translation System Combination.</title>
        <publication>In Proceedings of the 3rd ACL Workshop on SMT.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Yong Zhao</author>
          <author>Xiaodong He</author>
        </authors>
        <title>Using N-gram based Features for Machine Translation System Combination.</title>
        <publication>In Proceedings of NAACL-HLT</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Personal Communication Evgeny Matusov</author>
          <author>Nicola Ueffing</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Computing Consensus Translation from Multiple Machine Translation Systems using Enhanced Hypothesis Alignment.</title>
        <publication>In Proceedings of EACL. Evgeny Matusov,</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Matthias Paulik Mari&#241;o</author>
          <author>Salim Roukos</author>
          <author>Holger Schwenk</author>
          <author>Hermann Ney</author>
        </authors>
        <title>System combination for machine translation of spoken and written language. IEEE transactions on audio speech and language processing 16(7).</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>1</reference_id>
        <string>He et al. 2008</string>
        <sentence_id>7716</sentence_id>
        <char_offset>50</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>He et al. 2008</string>
        <sentence_id>7855</sentence_id>
        <char_offset>100</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>He et al. (2008)</string>
        <sentence_id>8008</sentence_id>
        <char_offset>106</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>2</reference_id>
        <string>Jayaraman and Lavie 2005</string>
        <sentence_id>7767</sentence_id>
        <char_offset>133</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>5</reference_id>
        <string>Koehn 2004</string>
        <sentence_id>7832</sentence_id>
        <char_offset>77</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>5</reference_id>
        <string>Koehn 2004</string>
        <sentence_id>7899</sentence_id>
        <char_offset>93</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>7</reference_id>
        <string>NIST 2008</string>
        <sentence_id>7996</sentence_id>
        <char_offset>119</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>7</reference_id>
        <string>NIST 2008</string>
        <sentence_id>8006</sentence_id>
        <char_offset>171</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>7</reference_id>
        <string>NIST 2008</string>
        <sentence_id>8074</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>11</reference_id>
        <string>Rosti et al. 2008</string>
        <sentence_id>7724</sentence_id>
        <char_offset>110</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>11</reference_id>
        <string>Rosti et al. 2008</string>
        <sentence_id>7732</sentence_id>
        <char_offset>181</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>12</reference_id>
        <string>Zhao and He (2009)</string>
        <sentence_id>7818</sentence_id>
        <char_offset>67</char_offset>
      </citation>
    </citations>
  </content>
</document>
