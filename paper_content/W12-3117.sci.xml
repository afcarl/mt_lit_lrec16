<PAPER>
  <FILENO/>
  <TITLE>DCU-Symantec Submission for the WMT 2012 Quality Estimation Task</TITLE>
  <AUTHORS/>
  <ABSTRACT>
    <A-S ID="S-49684">This paper describes the features and the machine learning methods used by Dublin City University (DCU) and SYMANTEC for the WMT 2012 quality estimation task.</A-S>
    <A-S ID="S-49685">Two sets of features are proposed: one constrained, i.e. respecting the data limitation suggested by the workshop organisers, and one unconstrained, i.e. using data or tools trained on data that was not provided by the workshop organisers.</A-S>
    <A-S ID="S-49686">In total, more than 300 features were extracted and used to train classifiers in order to predict the translation quality of unseen data.</A-S>
    <A-S ID="S-49687">In this paper, we focus on a subset of our feature set that we consider to be relatively novel: features based on a topic model built using the Latent Dirichlet Allocation approach, and features based on source and target language syntax extracted using part-of-speech (POS) taggers and parsers.</A-S>
    <A-S ID="S-49688">We evaluate nine feature combinations using four classification-based and four regression-based machine learning techniques.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-49689">For the first time, the WMT organisers this year propose a Quality Estimation (QE) shared task, which is divided into two sub-tasks: scoring and ranking automatic translations.</S>
        <S ID="S-49690">The aim of this workshop is to define useful sets of features and machine learning techniques in order to predict the quality of a machine translation (MT) output T (Spanish) given a source segment S (English).</S>
        <S ID="S-49691">Quality is measured using a 5-point likert scale which is based on postediting effort, following the scoring scheme:</S>
      </P>
      <P>
        <S ID="S-49692">1.</S>
        <S ID="S-49693">The MT output is incomprehensible 2.</S>
        <S ID="S-49694">About 50-70% of the MT output needs to be</S>
      </P>
      <P>
        <S ID="S-49695">edited 3.</S>
        <S ID="S-49696">About 25-50% of the MT output needs to be</S>
      </P>
      <P>
        <S ID="S-49697">edited 4.</S>
        <S ID="S-49698">About 10-25% of the MT output needs to be</S>
      </P>
      <P>
        <S ID="S-49699">edited 5.</S>
        <S ID="S-49700">The MT output is perfectly clear and intelligible</S>
      </P>
      <P>
        <S ID="S-49701">The final score is a combination of the scores assigned by three evaluators.</S>
        <S ID="S-49702">The use of a 5-point scale makes the scoring task more difficult than a binary classification task where a translation is considered to be either good or bad.</S>
        <S ID="S-49703">However, if the task is successfully carried out, the score produced is more useful.</S>
      </P>
      <P>
        <S ID="S-49704">Dublin City University and Symantec jointly address the scoring task.</S>
        <S ID="S-49705">For each pair (S, T ) of source segment S and machine translation T , we train three classifiers and one classifier combination using the training data provided by the organisers to predict 5-point Likert scores.</S>
        <S ID="S-49706">In this paper, we present the classification results on the test set along with additional results obtained using regression techniques.</S>
        <S ID="S-49707">We evaluate the usefulness of two new sets of features:</S>
      </P>
      <P>
        <S ID="S-49708">1. topic-based features using Latent Dirichlet Allocation (LDA (<REF ID="R-01" RPTR="1">Blei et al., 2003</REF>)), 2. syntax-based features using POS taggers and</S>
      </P>
      <P>
        <S ID="S-49709">parsers (Wagner et al., 2009)</S>
      </P>
      <P>
        <S ID="S-49710">The remainder of this paper is organised as follows.</S>
        <S ID="S-49711">In Section 2, we give an overview of all the</S>
      </P>
      <P>
        <S ID="S-49712">features employed in our QE system.</S>
        <S ID="S-49713">Then, in Section 3, we describe the topic and syntax-based features in more detail.</S>
        <S ID="S-49714">Section 4 presents the various classification and regression techniques we explored.</S>
        <S ID="S-49715">Our results are presented and discussed in Section 5.</S>
        <S ID="S-49716">Finally, we summarise and outline our plans in Section 6.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Features Overview</HEADER>
      <P>
        <S ID="S-49754">In this section, we describe the features used in our QE system.</S>
        <S ID="S-49755">In the first subsection, the features included in our constrained system are presented.</S>
        <S ID="S-49756">In the second subsection, we detail the features included in our unconstrained system.</S>
        <S ID="S-49757">Both of these systems include the 17 baseline features provided for the shared task.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>2.1 Constrained System</HEADER>
        <P>
          <S ID="S-49717">The constrained system is based only on the data provided by the organisers.</S>
          <S ID="S-49718">We extracted 70 features in total (including the baseline features) and we present them here according to the type of information they capture.</S>
        </P>
        <P>
          <S ID="S-49719">Word and Phrase-Level Features</S>
        </P>
        <P>
          <S ID="S-49720">&#8226; Ratio of source and target segment length: the number of source words divided by the number of target words &#8226; Ratio of source and target number of punctuation marks: the number of source punctuation marks divided by the number of target ones &#8226; Number of phrases comprising the MT output: given a phrase-table, we assume that a sentence composed of several phrases indicates uncertainty on the part of the MT system.</S>
          <S ID="S-49721">&#8226; Average length of source and target phrases: concatenating short phrases may result in lower fluency compared to the use of longer ones.</S>
          <S ID="S-49722">&#8226; Ratio of source and target averaged phrase length &#8226; Number of source prepositions and conjunctions word: our assumption here is that segments containing a relatively high number of prepositions and conjunctions may be more complex and difficult to translate.</S>
          <S ID="S-49723">&#8226; Number of source out-of-vocabulary words</S>
        </P>
        <P>
          <S ID="S-49724">Language Model Features</S>
        </P>
        <P>
          <S ID="S-49725">All the language models (LMs) used in our work are n-gram LMs with Kneser-Ney smoothing built with the SRI Toolkit (<REF ID="R-29" RPTR="26">Stolcke, 2002</REF>).</S>
        </P>
        <P>
          <S ID="S-49726">&#8226; Backward 2-gram and 3-gram source and target log probabilities: as proposed by <REF ID="R-08" RPTR="8">Duchateau et al. (2002)</REF> &#8226; Log probability of target segments on 5-gram MT-output-based LM: using</S>
        </P>
        <P>
          <S ID="S-49727">MOSES (<REF ID="R-14" RPTR="12">Koehn et al., 2007</REF>) trained on the</S>
        </P>
        <P>
          <S ID="S-49728">provided parallel corpus, we translated the English side of this corpus into Spanish, assuming that the MT output contains mistakes.</S>
          <S ID="S-49729">This MT output is used to build a LM that models the behavior of the MT system.</S>
          <S ID="S-49730">We assume that for a given MT output, a high n-gram probability (or a low perplexity) of the LM indicates that the MT output contains mistakes.</S>
        </P>
        <P>
          <S ID="S-49731">MT-system Features</S>
        </P>
        <P>
          <S ID="S-49732">&#8226; 15 scores provided by Moses: phrase-table, language model, reordering model and word penalty (weighted and unweighted) &#8226; Number of n-bests for each source segment &#8226; MT output back-translation: from Spanish to English using MOSES trained on the provided parallel corpus, scored with TER (<REF ID="R-28" RPTR="25">Snover et al., 2006</REF>), BLEU (<REF ID="R-22" RPTR="19">Papineni et al., 2002</REF>) and the Levenshtein distance (<REF ID="R-15" RPTR="13">Levenshtein, 1966</REF>), based on the source segments as a translation reference</S>
        </P>
        <P>
          <S ID="S-49733">Topic Model Features</S>
        </P>
        <P>
          <S ID="S-49734">&#8226; Probability distribution over topics: Source and target segment probability distribution over topics for a 10-dimension topic model &#8226; Cosine distance between source and target topic vectors</S>
        </P>
        <P>
          <S ID="S-49735">More details about these two features are provided in Section 3.1.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.2 Unconstrained System</HEADER>
        <P>
          <S ID="S-49736">In addition to the features used for the constrained system, a further 238 unconstrained features were included in our unconstrained system.</S>
        </P>
        <P>
          <S ID="S-49737">MT System Features</S>
        </P>
        <P>
          <S ID="S-49738">As for our constrained system, we use MT output back-translation from Spanish to English, but this time using Bing Translator 1 in addition to Moses.</S>
          <S ID="S-49739">Each back-translated segment is scored with TER, BLEU and the Levenshtein distance, based on the source segments as a translation reference.</S>
        </P>
        <P>
          <S ID="S-49740">Source Syntax Features</S>
        </P>
        <P>
          <S ID="S-49741">Wagner et al. (2007; 2009) propose a series of features to measure sentence grammaticality.</S>
          <S ID="S-49742">These features rely on a part-of-speech tagger, a probabilistic parser and a precision grammar/parser.</S>
          <S ID="S-49743">We have at our disposal these tools for English and so we apply them to the source data.</S>
          <S ID="S-49744">The features themselves are described in more detail in Section 3.2.</S>
        </P>
        <P>
          <S ID="S-49745">Target Syntax Features</S>
        </P>
        <P>
          <S ID="S-49746">We use a part-of-speech tagger trained on Spanish to extract from the target data the subset of grammaticality features proposed by Wagner et al. (2007; 2009) that are based on POS n-grams.</S>
          <S ID="S-49747">In addition we extract features which reflect the prevalence of particular POS tags in each target segment.</S>
          <S ID="S-49748">These are explained in more detail in Section 3.2 below.</S>
        </P>
        <P>
          <S ID="S-49749">Grammar Checker Features</S>
        </P>
        <P>
          <S ID="S-49750">LANGUAGETOOL (based on (<REF ID="R-20" RPTR="18">Naber, 2003</REF>)) is an</S>
        </P>
        <P>
          <S ID="S-49751">open-source grammar and style proofreading tool that finds errors based on pre-defined, languagespecific rules.</S>
          <S ID="S-49752">The latest version of the tool can be run in server mode, so individual sentences can be checked and assigned a total number of errors (which may or may not be true positives).</S>
          <S ID="S-49753">2 This number is used as a feature for each source segment and its corresponding MT output.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Topic and Syntax-based Features</HEADER>
      <P>
        <S ID="S-49811">In this section, we focus on the set of features that aim to capture adequacy using topic modelling and grammaticality using POS tagging and syntactic parsing.</S>
      </P>
      <P>
        <S ID="S-49812">1 http://www.microsofttranslator.com/ 2 The list of English and Spanish rules is available at:</S>
      </P>
      <P>
        <S ID="S-49813">http://languagetool.org/languages.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>3.1 Topic-based Features</HEADER>
        <P>
          <S ID="S-49758">We extract source and target features based on a topic model built using LDA.</S>
          <S ID="S-49759">The main idea in topic modelling is to produce a set of thematic word clusters from a collection of documents.</S>
          <S ID="S-49760">Using the parallel corpus provided for the task, a bilingual corpus is built where each line is composed of a source segment and its translation separated by a space.</S>
          <S ID="S-49761">Each pair of segments is considered as a bilingual document.</S>
          <S ID="S-49762">This corpus is used to train a bilingual topic model after stopwords removal.</S>
          <S ID="S-49763">The resulting model is one set of bilingual topics z containing words w with a probability p(w n |z n , &#946;) (with n equal to the vocabulary size in the whole parallel corpus).</S>
          <S ID="S-49764">This model can be used to infer the probability distribution of unseen source and target segments over bilingual topics.</S>
          <S ID="S-49765">During the test step, each source segment and its translation are considered individually, as two monolingual documents.</S>
          <S ID="S-49766">This method allows us to compare the source and target topic distributions.</S>
          <S ID="S-49767">We assume that a source segment and its translation share topic similarities.</S>
        </P>
        <P>
          <S ID="S-49768">We propose two ways of using topic-based features for quality estimation: keeping source and target topic vectors as two sets of k features, or computing a vector distance between these two vectors and using one feature only.</S>
          <S ID="S-49769">To measure the proximity of two vectors, we decided to used the Cosine distance, as it leads to the best results in terms of classification accuracy.</S>
          <S ID="S-49770">However, we plan to study different metrics in further experiments, like the Manhattan or the Euclidean distances.</S>
          <S ID="S-49771">Some parameters related to LDA have to be studied more carefully too, such as the number of topics (dimensions in the topic space), the number of words per topic, the Dirichlet hyperparameter &#945;, etc.</S>
          <S ID="S-49772">In our experiments, we built a topic model composed of 10 dimensions using Gibbs sampling with 1000 iterations.</S>
          <S ID="S-49773">We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics.</S>
        </P>
        <P>
          <S ID="S-49774">Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (<REF ID="R-19" RPTR="17">Mimno et al., 2009</REF>) or multilingual topic models for unaligned text (<REF ID="R-02" RPTR="2">Boyd-Graber and Blei, 2009</REF>).</S>
          <S ID="S-49775">In the field of machine translation, <REF ID="R-30" RPTR="27">Tam et al. (2007)</REF> propose to adapt a translation and a lan-</S>
        </P>
        <P>
          <S ID="S-49776">guage model to a specific topic using Latent Semantic Analysis (LSA, or Latent Semantic Indexing, LSI (<REF ID="R-07" RPTR="7">Deerwester et al., 1990</REF>)).</S>
          <S ID="S-49777">More recently, some studies were conducted on the use of LDA to adapt SMT systems to specific domains (Gong et al., 2010; <REF ID="R-11" RPTR="9">Gong et al., 2011</REF>) or to extract bilingual lexicon from comparable corpora (<REF ID="R-26" RPTR="23">Rubino and Linar&#232;s, 2011</REF>).</S>
          <S ID="S-49778">Extracting features from a topic model is, to the best of our knowledge, the first attempt in machine translation quality estimation.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.2 Syntax-based Features</HEADER>
        <P>
          <S ID="S-49779">Syntactic features have previously been used in MT for confidence estimation and for building automatic evaluation measures.</S>
          <S ID="S-49780"><REF ID="R-06" RPTR="6">Corston-Oliver et al. (2001)</REF> build a classifier using 46 parse tree features to predict whether a sentence is a human translation or MT output.</S>
          <S ID="S-49781"><REF ID="R-25" RPTR="22">Quirk (2004)</REF> uses a single parse tree feature in the quality estimation task with a 4-point scale, namely whether a spanning parse can be found, in addition to LM perplexity and sentence length.</S>
          <S ID="S-49782"><REF ID="R-16" RPTR="14">Liu and Gildea (2005)</REF> measure the syntactic similarity between MT output and reference translation.</S>
          <S ID="S-49783"><REF ID="R-00" RPTR="0">Albrecht and Hwa (2007)</REF> measure the syntactic similarity between MT output and reference translation and between MT output and a large monolingual corpus.</S>
          <S ID="S-49784">Gimenez and Marquez (2007) explore lexical, syntactic and shallow semantic features and focus on measuring the similarity of MT output to reference translation.</S>
          <S ID="S-49785">Owczarzak et al. (2007) use labelled dependencies together with WordNet to avoid penalising valid syntactic and lexical variations in MT evaluation.</S>
          <S ID="S-49786">In what follows, we describe how we make use of syntactic information in the QE task, i.e. evaluating MT output without a reference translation.</S>
          <S ID="S-49787">Wagner et al. (2007; 2009) use three sources of linguistic information in order to extract features which they use to judge the grammaticality of English sentences:</S>
        </P>
        <P>
          <S ID="S-49788">1.</S>
          <S ID="S-49789">For each POS n-gram (with n ranging from 2 to 7), a feature is extracted which represents the frequency of the least frequent n-gram in the sentence according to some reference corpus.</S>
          <S ID="S-49790">TreeTagger (<REF ID="R-27" RPTR="24">Schmidt, 1994</REF>) is used to produce POS tags.</S>
          <S ID="S-49791">2.</S>
          <S ID="S-49792">Features provided by a hand-crafted, broadcoverage precision grammar of English (<REF ID="R-04" RPTR="4">Butt et al., 2002</REF>) and a Lexical Functional Grammar parser (<REF ID="R-18" RPTR="16">Maxwell and Kaplan, 1996</REF>).</S>
          <S ID="S-49793">These include whether or not a sentence could be parsed without resorting to robustness measures, the number of analyses found and the parsing time.</S>
          <S ID="S-49794">3.</S>
          <S ID="S-49795">Features extracted from the output of three</S>
        </P>
        <P>
          <S ID="S-49796">probabilistic parsers of English (<REF ID="R-05" RPTR="5">Charniak and Johnson, 2005</REF>), one trained on Wall Street Journal trees (<REF ID="R-17" RPTR="15">Marcus et al., 1993</REF>), one trained on a distorted version of the treebank obtained by automatically creating grammatical error and adjusting the parse trees, and the third trained on the union of the original and distorted versions.</S>
        </P>
        <P>
          <S ID="S-49797">These features were originally designed to distinguish grammatical sentences from ungrammatical ones and were tested on sentences from learner corpora by Wagner et al. (2009) and <REF ID="R-33" RPTR="28">Wagner (2012)</REF>.</S>
          <S ID="S-49798">In this work we extract all three sets of features from the source side of our data and the POS-based subset from the target side.</S>
          <S ID="S-49799">3 We use the publicly available pre-trained TreeTagger models for English and Spanish 4 .</S>
          <S ID="S-49800">The reference corpus used to obtain POS n-gram frequences is the MT translation model training data.</S>
          <S ID="S-49801">5</S>
        </P>
        <P>
          <S ID="S-49802">In addition to the POS-based features described in Wagner et al. (2007; 2009), we also extract the following features from the Spanish POS-tagged data: for each POS tag P and target segment T , we extract a feature which is the proportion of words in T that are tagged as P .</S>
          <S ID="S-49803">Two additional features are extracted to represent the proportion of words in T that are assigned more than one tag by the tagger,</S>
        </P>
        <P>
          <S ID="S-49804">3 Unfortunately, due to time constraints, we were unable to</S>
        </P>
        <P>
          <S ID="S-49805">source a suitable probabilistic phrase-structure parser and a precision grammar for Spanish and were thus unable to extract parser-based features for Spanish.</S>
          <S ID="S-49806">We expect that these features would be more useful on the target side than the source side.</S>
          <S ID="S-49807">4 http://www.ims.uni-stuttgart.de/</S>
        </P>
        <P>
          <S ID="S-49808">projekte/corplex/TreeTagger/ 5 To aid machine learning methods that linearly combine feature values, we add binarised features derived from the raw XLE and POS n-gram features described above, for example we add a feature indicating whether the frequency of the least frequent POS 5-gram is below 10.</S>
          <S ID="S-49809">We base the choice of binary features on (a) decision rules observed in decision trees trained for a binary scoring task and (b) decision rules of simple classifiers (decision trees with just one decision node and 2 leaf nodes) that form a convex hull of optimal classifiers in ROC space.</S>
        </P>
        <P>
          <S ID="S-49810">and the proportion of words in T that are unknown to the tagger.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 Machine Learning</HEADER>
      <P>
        <S ID="S-49824">In this section, we describe the machine learning methods that we experimented with.</S>
        <S ID="S-49825">Our final systems submitted for the shared task are based on classification methods.</S>
        <S ID="S-49826">However, we also performed some experiments with regression methods.</S>
      </P>
      <P>
        <S ID="S-49827">We evaluate the systems on the test set using the official evaluation script and the reference scores.</S>
        <S ID="S-49828">We report the evaluation results as Mean Average Error (MAE) and Root Mean Squared Error (RMSE).</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>4.1 Classification</HEADER>
        <P>
          <S ID="S-49814">In order to apply classification algorithms to the set of features associated with each source and target segment, we rounded the training data scores to the closest integer.</S>
          <S ID="S-49815">We tested several classifiers and empirically chose three algorithms: Support Vector Machine using sequential minimal optimization and RBF kernel (parameters optimized by gridsearch) (<REF ID="R-23" RPTR="20">Platt, 1999</REF>), Naive Bayes (<REF ID="R-13" RPTR="11">John and Langley, 1995</REF>) and Random Forest (<REF ID="R-03" RPTR="3">Breiman, 2001</REF>) (the latter two techniques were applied with default parameters).</S>
          <S ID="S-49816">We use the Weka toolkit (<REF ID="R-12" RPTR="10">Hall et al., 2009</REF>) to train the classifiers and predict the scores on the test set.</S>
          <S ID="S-49817">Each method is evaluated individually and then combined by averaging the predicted scores.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>4.2 Regression</HEADER>
        <P>
          <S ID="S-49818">We applied three different regression techniques: SVM epsilon-SVR with RBF kernel, Linear Regression and M5P (<REF ID="R-24" RPTR="21">Quinlan, 1992</REF>; Wang and Witten, 1997).</S>
          <S ID="S-49819">The two latter algorithms were used with default parameters, whereas SVM parameters (&#947;, c and &#603;) were optimized by grid-search.</S>
          <S ID="S-49820">We also performed a combination of the three algorithms by averaging the predicted scores.</S>
          <S ID="S-49821">We apply a linear function on the predicted scores S in order to keep them in the correct range (from 1 to 5) as detailed in (1), where S &#8242; is the rescaled sentence score, S min is the lowest predicted score and S max is the highest predicted score.</S>
        </P>
        <P>
          <S ID="S-49822">S &#8242; = 1 + 4 &#215;</S>
        </P>
        <P>
          <S ID="S-49823">S &#8722; S min S max &#8722; S min (1)</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Evaluation</HEADER>
      <P>
        <S ID="S-49829">Table 1 shows the results obtained by our classification approach on various feature subsets.</S>
        <S ID="S-49830">Note that the two submitted systems used the combined classifier approach with the constrained and unconstrained feature sets.</S>
        <S ID="S-49831">Table 2 shows the results for the same feature combinations, this time using regression rather than classification.</S>
      </P>
      <P>
        <S ID="S-49832">The results of quality estimation using classification methods show that the baseline and the syntaxbased features with the classifier combination leads to the best results with an MAE of 0.71 and an RMSE of 0.87.</S>
        <S ID="S-49833">However, these scores are substantially lower than the ones obtained using regression, where the unconstrained set of features with SVM leads to an MAE of 0.62 and an RMSE of 0.78.</S>
        <S ID="S-49834">It seems that the classification methods are not suitable for this task according to the different sets of features studied.</S>
        <S ID="S-49835">Furthermore, the topic-distance feature is not correlated with the quality scores, according to the regression results.</S>
        <S ID="S-49836">On the other hand, the syntax-based features appear to be the most informative and lead to an MAE of 0.70.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>6 Conclusion</HEADER>
      <P>
        <S ID="S-49837">We presented in this paper our submission for the WMT12 Quality Estimation shared task.</S>
        <S ID="S-49838">We also presented further experiments using different machine learning techniques and we evaluated the impact of two sets of features - one set which is based on linguistic features extracted using POS tagging and parsing, and a second set which is based on topic modelling.</S>
        <S ID="S-49839">The best results are obtained by our unconstrained system containing all features and using an &#603;-SVR regression method with a Radial Basis Function kernel.</S>
        <S ID="S-49840">This setup leads to a Mean Average Error of 0.62 and a Root Mean Squared Error of 0.78.</S>
        <S ID="S-49841">Unfortunately, we did not submit our best configuration for the shared task.</S>
        <S ID="S-49842">We plan to continue working on the task of machine translation quality estimation.</S>
        <S ID="S-49843">Our immediate next steps are to continue to investigate the contribution of individual features, to explore feature selection in a more detailed fashion and to apply our best system to other types of data including sentences taken from an online discussion forum.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS/>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>J Albrecht</RAUTHOR>
      <REFTITLE>A re-examination of machine learning approaches for sentence-level MT evaluation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>D M Blei</RAUTHOR>
      <REFTITLE>Latent Dirichlet Allocation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>Jordan Boyd-Graber</RAUTHOR>
      <REFTITLE>Multilingual topic models for unaligned text.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>L Breiman</RAUTHOR>
      <REFTITLE>Random forests.</REFTITLE>
      <DATE>2001</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>M Butt</RAUTHOR>
      <REFTITLE>The parallel grammar project.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>E Charniak</RAUTHOR>
      <REFTITLE>Course-to-fine nbest-parsing and maxent discriminative reranking.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>S Corston-Oliver</RAUTHOR>
      <REFTITLE>A machine learning approach to the automatic evaluation of machine translation.</REFTITLE>
      <DATE>2001</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>S Deerwester</RAUTHOR>
      <REFTITLE>Indexing by Latent Semantic Analysis.</REFTITLE>
      <DATE>1990</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>J Duchateau</RAUTHOR>
      <REFTITLE>Confidence scoring based on backward language models.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>J Gim&#233;nez</RAUTHOR>
      <REFTITLE>Linguistic features for automatic evaluation of heterogenous MT systems.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>Y Zhang Gong</RAUTHOR>
      <REFTITLE>Statistical machine translation based on lda.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>Z Gong</RAUTHOR>
      <REFTITLE>Improve smt with source-side &#8221;topic-document&#8221; distributions.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>M Hall</RAUTHOR>
      <REFTITLE>The weka data mining software: an update.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>G H John</RAUTHOR>
      <REFTITLE>Estimating continuous distributions in bayesian classifiers.</REFTITLE>
      <DATE>1995</DATE>
    </REFERENCE>
    <REFERENCE ID="14">
      <RAUTHOR>P Koehn</RAUTHOR>
      <REFTITLE>Moses: Open source toolkit for statistical machine translation. In</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="15">
      <RAUTHOR>V I Levenshtein</RAUTHOR>
      <REFTITLE>Binary codes capable of correcting deletions, insertions, and reversals.</REFTITLE>
      <DATE>1966</DATE>
    </REFERENCE>
    <REFERENCE ID="16">
      <RAUTHOR>D Liu</RAUTHOR>
      <REFTITLE>Syntactic features for evaluation of machine translation.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="17">
      <RAUTHOR>M P Marcus</RAUTHOR>
      <REFTITLE>Building a large annotated corpus of english: the penn treebank.</REFTITLE>
      <DATE>1993</DATE>
    </REFERENCE>
    <REFERENCE ID="18">
      <RAUTHOR>John Maxwell</RAUTHOR>
      <REFTITLE>An Efficient Parser for LFG.</REFTITLE>
      <DATE>1996</DATE>
    </REFERENCE>
    <REFERENCE ID="19">
      <RAUTHOR>D Mimno</RAUTHOR>
      <REFTITLE>Polylingual topic models.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="20">
      <RAUTHOR>D Naber</RAUTHOR>
      <REFTITLE>A rule-based style and grammar checker.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="21">
      <RAUTHOR>K Owczarzak</RAUTHOR>
      <REFTITLE>Labelled dependencies in machine translation evaluation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="22">
      <RAUTHOR>K Papineni</RAUTHOR>
      <REFTITLE>Bleu: a method for automatic evaluation of machine translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="23">
      <RAUTHOR>J C Platt</RAUTHOR>
      <REFTITLE>Fast training of support vector machines using sequential minimal optimization.</REFTITLE>
      <DATE>1999</DATE>
    </REFERENCE>
    <REFERENCE ID="24">
      <RAUTHOR>R J Quinlan</RAUTHOR>
      <REFTITLE>Learning with continuous classes.</REFTITLE>
      <DATE>1992</DATE>
    </REFERENCE>
    <REFERENCE ID="25">
      <RAUTHOR>C Quirk</RAUTHOR>
      <REFTITLE>Training a sentence-level machine translation confidence measure.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="26">
      <RAUTHOR>R Rubino</RAUTHOR>
      <REFTITLE>A multi-view approach for term translation spotting.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="27">
      <RAUTHOR>H Schmidt</RAUTHOR>
      <REFTITLE>Probabilistic part-of-speech tagging using decision trees.</REFTITLE>
      <DATE>1994</DATE>
    </REFERENCE>
    <REFERENCE ID="28">
      <RAUTHOR>M Snover</RAUTHOR>
      <REFTITLE>A study of translation edit rate with targeted human annotation.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="29">
      <RAUTHOR>A Stolcke</RAUTHOR>
      <REFTITLE>SRILM-an extensible language modeling toolkit.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="30">
      <RAUTHOR>Y C Tam</RAUTHOR>
      <REFTITLE>Bilingual lsa-based adaptation for statistical machine translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="31">
      <RAUTHOR>J Wagner</RAUTHOR>
      <REFTITLE>A comparative evaluation of deep and shallow approaches to the automatic detection of common grammatical errors.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="32">
      <RAUTHOR>J Wagner</RAUTHOR>
      <REFTITLE>Judging grammaticality: Experiments in sentence classification.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="33">
      <RAUTHOR>J Wagner</RAUTHOR>
      <REFTITLE>Detecting grammatical errors with treebank-induced probabilistic parsers.</REFTITLE>
      <DATE>2012</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
