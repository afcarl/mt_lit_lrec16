<document>
  <filename>W13-2239</filename>
  <authors>
    <author>Daniel Cer</author>
    <author>Christopher D Manning</author>
  </authors>
  <title>Positive Diversity Tuning for Machine Translation System Combination</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>We present Positive Diversity Tuning, a new method for tuning machine translation models specifically for improved performance during system combination. System combination gains are often limited by the fact that the translations produced by the different component systems are too similar to each other. We propose a method for reducing excess cross-system similarity by optimizing a joint objective that simultaneously rewards models for producing translations that are similar to reference translations, while also punishing them for translations that are too similar to those produced by other systems. The formulation of the Positive Diversity objective is easy to implement and allows for its quick integration with most machine translation tuning pipelines. We find that individual systems tuned on the same data to Positive Diversity can be even more diverse than systems built using different data sets, while still obtaining good BLEU scores. When these individual systems are used together for system combination, our approach allows for significant gains of 0.8 BLEU even when the combination is performed using a small number of otherwise identical individual systems.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We present Positive Diversity Tuning, a new method for tuning machine translation models specifically for improved performance during system combination.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>System combination gains are often limited by the fact that the translations produced by the different component systems are too similar to each other.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We propose a method for reducing excess cross-system similarity by optimizing a joint objective that simultaneously rewards models for producing translations that are similar to reference translations, while also punishing them for translations that are too similar to those produced by other systems.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The formulation of the Positive Diversity objective is easy to implement and allows for its quick integration with most machine translation tuning pipelines.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We find that individual systems tuned on the same data to Positive Diversity can be even more diverse than systems built using different data sets, while still obtaining good BLEU scores.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>When these individual systems are used together for system combination, our approach allows for significant gains of 0.8 BLEU even when the combination is performed using a small number of otherwise identical individual systems.</text>
              <doc_id>5</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>The best performing machine translation systems are typically not individual decoders but rather are ensembles of two or more systems whose output is then merged using system combination algorithms. Since combining multiple distinct equally good translation systems reliably produces gains over any one of the systems in isolation, it is widely used in situations where high quality is essential.
Exploiting system combination brings significant cost: Macherey and Och (2007) showed that successful system combination requires the construction of multiple systems that are simultaneously diverse and well-performing. If the systems are not distinct enough, they will bring very little value during system combination. However, if some of the systems produce diverse translations but achieve lower overall translation quality, their contributions risk being ignored during system combination.
Prior work has approached the need for diverse systems by using different system architectures, model components, system build parameters, decoder hyperparameters, as well as data selection and weighting (Macherey and Och, 2007; DeNero et al., 2010; Xiao et al., 2013). However, during tuning, each individual system is still just trained to maximize its own isolated performance on a tune set, or at best an error-driven reweighting of the tune set, without explicitly taking into account the diversity of the resulting translations. Such tuning does not encourage systems to rigorously explore model variations that achieve both good translation quality and diversity with respect to the other systems. It is reasonable to suspect that this results in individual systems that under exploit the amount of diversity possible, given the characteristics of the individual systems.
For better system combination, we propose building individual systems to attempt to simultaneously maximize the overall quality of the individual systems and the amount of diversity across systems. We operationalize this problem formulation by devising a new heuristic measure called Positive Diversity that estimates the potential usefulness of individual systems during system combination. We find that optimizing systems toward Positive Diversity leads to significant performance gains during system combination even when the combination is performed using a small number of
otherwise identical individual translation systems.
The remainder of this paper is organized as follows. Section 2 and 3 briefly review the tuning of individual machine translation systems and how system combination merges the output of multiple systems into an improved combined translation. Section 4 introduces our Positive Diversity measure. Section 5 introduces an algorithm for training a collection of translation systems toward Positive Diversity. Experiments are presented in sections 6 and 7. Sections 8 and 9 conclude with discussions of prior work and directions for future research.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The best performing machine translation systems are typically not individual decoders but rather are ensembles of two or more systems whose output is then merged using system combination algorithms.</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Since combining multiple distinct equally good translation systems reliably produces gains over any one of the systems in isolation, it is widely used in situations where high quality is essential.</text>
              <doc_id>7</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Exploiting system combination brings significant cost: Macherey and Och (2007) showed that successful system combination requires the construction of multiple systems that are simultaneously diverse and well-performing.</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>If the systems are not distinct enough, they will bring very little value during system combination.</text>
              <doc_id>9</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, if some of the systems produce diverse translations but achieve lower overall translation quality, their contributions risk being ignored during system combination.</text>
              <doc_id>10</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Prior work has approached the need for diverse systems by using different system architectures, model components, system build parameters, decoder hyperparameters, as well as data selection and weighting (Macherey and Och, 2007; DeNero et al., 2010; Xiao et al., 2013).</text>
              <doc_id>11</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, during tuning, each individual system is still just trained to maximize its own isolated performance on a tune set, or at best an error-driven reweighting of the tune set, without explicitly taking into account the diversity of the resulting translations.</text>
              <doc_id>12</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Such tuning does not encourage systems to rigorously explore model variations that achieve both good translation quality and diversity with respect to the other systems.</text>
              <doc_id>13</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>It is reasonable to suspect that this results in individual systems that under exploit the amount of diversity possible, given the characteristics of the individual systems.</text>
              <doc_id>14</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For better system combination, we propose building individual systems to attempt to simultaneously maximize the overall quality of the individual systems and the amount of diversity across systems.</text>
              <doc_id>15</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We operationalize this problem formulation by devising a new heuristic measure called Positive Diversity that estimates the potential usefulness of individual systems during system combination.</text>
              <doc_id>16</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We find that optimizing systems toward Positive Diversity leads to significant performance gains during system combination even when the combination is performed using a small number of</text>
              <doc_id>17</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>otherwise identical individual translation systems.</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The remainder of this paper is organized as follows.</text>
              <doc_id>19</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Section 2 and 3 briefly review the tuning of individual machine translation systems and how system combination merges the output of multiple systems into an improved combined translation.</text>
              <doc_id>20</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Section 4 introduces our Positive Diversity measure.</text>
              <doc_id>21</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Section 5 introduces an algorithm for training a collection of translation systems toward Positive Diversity.</text>
              <doc_id>22</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Experiments are presented in sections 6 and 7.</text>
              <doc_id>23</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Sections 8 and 9 conclude with discussions of prior work and directions for future research.</text>
              <doc_id>24</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Tuning Individual Translation Systems</title>
        <text>Machine translation systems are tuned toward some measure of the correctness of the translations produced by the system according to one or more manually translated references. As shown in equation (1), this can be written as finding parameter values &#920; that produce translations sys &#920; that in turn achieve a high score on some correctness measure:
arg max
&#920;
Correctness(ref[], sys &#920; ) (1)
The correctness measure that systems are typically tuned toward is BLEU (Papineni et al., 2002), which measures the fraction of the n-grams that are both present in the reference translations and the translations produced by a system. The BLEU score is computed as the geometric mean of the resulting n-gram precisions scaled by a brevity penalty.
The most widely used machine translation tuning algorithm, minimum error rate training (MERT) (Och, 2003), attempts to maximize the correctness objective directly. Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization objectives that indirectly attempt to maximize the correctness function by using it to select targets for training discriminative classification models. In practice, either optimizing correctness directly or optimizing a surrogate objective that uses correctness to choose optimization targets results in roughly equivalent translation performance (Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maximize the potential usefulness of their contribution during system combination. 1 To our knowledge, no effort has been made to explicitly tune toward criteria that attempts to simultaneously maximize the translation quality of individual systems and their mutual diversity. This is unfortunate since the most valuable component systems for system combination should not only obtain good translation performance, but also produce translations that are different from those produced by other systems.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Machine translation systems are tuned toward some measure of the correctness of the translations produced by the system according to one or more manually translated references.</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As shown in equation (1), this can be written as finding parameter values &#920; that produce translations sys &#920; that in turn achieve a high score on some correctness measure:</text>
              <doc_id>26</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>arg max</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#920;</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Correctness(ref[], sys &#920; ) (1)</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The correctness measure that systems are typically tuned toward is BLEU (Papineni et al., 2002), which measures the fraction of the n-grams that are both present in the reference translations and the translations produced by a system.</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The BLEU score is computed as the geometric mean of the resulting n-gram precisions scaled by a brevity penalty.</text>
              <doc_id>31</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The most widely used machine translation tuning algorithm, minimum error rate training (MERT) (Och, 2003), attempts to maximize the correctness objective directly.</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization objectives that indirectly attempt to maximize the correctness function by using it to select targets for training discriminative classification models.</text>
              <doc_id>33</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In practice, either optimizing correctness directly or optimizing a surrogate objective that uses correctness to choose optimization targets results in roughly equivalent translation performance (Cherry and Foster, 2012).</text>
              <doc_id>34</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maximize the potential usefulness of their contribution during system combination.</text>
              <doc_id>35</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>1 To our knowledge, no effort has been made to explicitly tune toward criteria that attempts to simultaneously maximize the translation quality of individual systems and their mutual diversity.</text>
              <doc_id>36</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This is unfortunate since the most valuable component systems for system combination should not only obtain good translation performance, but also produce translations that are different from those produced by other systems.</text>
              <doc_id>37</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 System Combination</title>
        <text>Similar to speech recognition&#8217;s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a). 2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model&#8217;s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie, 2010b).
Both system confidence model features and n- gram consensus features score contributions based in part on how confident the system combination model is in each individual machine translation system. This means that little or no gains will typically be seen when combining a good system with poor performing systems even if the systems col-
1 The exception being Xiao et al. (2013)&#8217;s work using
boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009), re-decoding informed by the decoding paths taken by other systems (Huang and Papineni, 2007), and decoding model combination (DeNero et al., 2010).
Input : systems [], tune(), source, refs [], &#945;, EvalMetric (), SimMetric () Output: models []
// start with an empty set of translations from prior iterations other_sys [] &#8592; []
for i &#8592; 1 to len(systems []) do // new Positive Diversity measure using prior translations PD &#945;,i () &#8592; new PD(&#945;, EvalMetric(), SimMetric(), refs [], other_sys [])
// tune a new model to fit PD &#945;,i // e.g., using MERT, PRO, MIRA, RAMPION, etc. models [i] &#8592; tune(systems [i], source, PD &#945;,i ())
// Save translations from tuned model i for use during // the diversity computation for subsequent systems push(other_sys [], translate(systems [i], models [i], source)) end
return models [] Algorithm 1: Positive Diversity Tuning (PDT)
lectively produce very diverse translations. 3
The requirement that the systems used for system combination be both of high quality and diverse can be and often is met by building several different systems using different system architectures, model components or tuning data. However, as will be shown in the next few sections, by explicitly optimizing an objective that targets both translation quality and diversity, it is possible to obtain meaningful system combination gains even using a single system architecture with identical model components and the same tuning set.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Similar to speech recognition&#8217;s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011).</text>
              <doc_id>39</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The piecewise selection of material from the original translations is performed using the combination model&#8217;s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie, 2010b).</text>
              <doc_id>40</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Both system confidence model features and n- gram consensus features score contributions based in part on how confident the system combination model is in each individual machine translation system.</text>
              <doc_id>41</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This means that little or no gains will typically be seen when combining a good system with poor performing systems even if the systems col-</text>
              <doc_id>42</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 The exception being Xiao et al. (2013)&#8217;s work using</text>
              <doc_id>43</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009), re-decoding informed by the decoding paths taken by other systems (Huang and Papineni, 2007), and decoding model combination (DeNero et al., 2010).</text>
              <doc_id>44</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Input : systems [], tune(), source, refs [], &#945;, EvalMetric (), SimMetric () Output: models []</text>
              <doc_id>45</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>// start with an empty set of translations from prior iterations other_sys [] &#8592; []</text>
              <doc_id>46</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>for i &#8592; 1 to len(systems []) do // new Positive Diversity measure using prior translations PD &#945;,i () &#8592; new PD(&#945;, EvalMetric(), SimMetric(), refs [], other_sys [])</text>
              <doc_id>47</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>// tune a new model to fit PD &#945;,i // e.g., using MERT, PRO, MIRA, RAMPION, etc. models [i] &#8592; tune(systems [i], source, PD &#945;,i ())</text>
              <doc_id>48</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>// Save translations from tuned model i for use during // the diversity computation for subsequent systems push(other_sys [], translate(systems [i], models [i], source)) end</text>
              <doc_id>49</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>return models [] Algorithm 1: Positive Diversity Tuning (PDT)</text>
              <doc_id>50</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>lectively produce very diverse translations.</text>
              <doc_id>51</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>3</text>
              <doc_id>52</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The requirement that the systems used for system combination be both of high quality and diverse can be and often is met by building several different systems using different system architectures, model components or tuning data.</text>
              <doc_id>53</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, as will be shown in the next few sections, by explicitly optimizing an objective that targets both translation quality and diversity, it is possible to obtain meaningful system combination gains even using a single system architecture with identical model components and the same tuning set.</text>
              <doc_id>54</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Positive Diversity</title>
        <text>We propose Positive Diversity as a heuristic measurement of the value of potential contributions from an individual system to system combination. As given in equation (2), Positive Diversity is defined as the correctness of the translations produced by a system minus a penalty term that scores how similar the systems translations are with those produced by other systems:
P D &#945; = &#945; Correctness(ref[], sys &#920; )&#8722; (1 &#8722; &#945;) Similarity(other_sys[], sys &#920; ) (2)
The hyperparameter &#945; explicitly trades-off the preference for a well performing individual sys-
3 The machine learning theory behind boosting suggests
that it should be possible to combine a very large number of poor performing systems into a single good system. However, for machine translation, using a very large number of individual systems brings with it difficult computational challenges.
tem with system combination diversity. Higher &#945; values result in a Positive Diversity metric that mostly favors good quality translations. However, even for large &#945; values, if two translations are of approximately the same quality, the Positive Diversity metric will prefer the one that is the most diverse given the translations being produced by other systems.
The Correctness() and Similarity() measures are any function that can score translations from a single system against other translations. This includes traditional machine translation evaluation metrics (e.g, BLEU, TER, METEOR) as well as any other measure of textual similarity.
For the remainder of this paper, we use BLEU to measure both correctness and the similarity of the translations produced by the individual systems. When tuning individual translation systems toward Positive Diversity, our task is then to maximize equation (3) rather than equation (1):
arg max &#920;
&#945; BLEU(ref[], sys)&#8722; (1 &#8722; &#945;) BLEU(other_sys[], sys)
Since this learning objective is simply the difference between two BLEU scores, it should be easy to integrate into most existing machine translation tuning pipelines that are already designed to optimize performance on translation evaluation metrics.
(3)</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We propose Positive Diversity as a heuristic measurement of the value of potential contributions from an individual system to system combination.</text>
              <doc_id>55</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As given in equation (2), Positive Diversity is defined as the correctness of the translations produced by a system minus a penalty term that scores how similar the systems translations are with those produced by other systems:</text>
              <doc_id>56</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P D &#945; = &#945; Correctness(ref[], sys &#920; )&#8722; (1 &#8722; &#945;) Similarity(other_sys[], sys &#920; ) (2)</text>
              <doc_id>57</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The hyperparameter &#945; explicitly trades-off the preference for a well performing individual sys-</text>
              <doc_id>58</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3 The machine learning theory behind boosting suggests</text>
              <doc_id>59</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>that it should be possible to combine a very large number of poor performing systems into a single good system.</text>
              <doc_id>60</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, for machine translation, using a very large number of individual systems brings with it difficult computational challenges.</text>
              <doc_id>61</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tem with system combination diversity.</text>
              <doc_id>62</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Higher &#945; values result in a Positive Diversity metric that mostly favors good quality translations.</text>
              <doc_id>63</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, even for large &#945; values, if two translations are of approximately the same quality, the Positive Diversity metric will prefer the one that is the most diverse given the translations being produced by other systems.</text>
              <doc_id>64</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The Correctness() and Similarity() measures are any function that can score translations from a single system against other translations.</text>
              <doc_id>65</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This includes traditional machine translation evaluation metrics (e.g, BLEU, TER, METEOR) as well as any other measure of textual similarity.</text>
              <doc_id>66</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For the remainder of this paper, we use BLEU to measure both correctness and the similarity of the translations produced by the individual systems.</text>
              <doc_id>67</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>When tuning individual translation systems toward Positive Diversity, our task is then to maximize equation (3) rather than equation (1):</text>
              <doc_id>68</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>arg max &#920;</text>
              <doc_id>69</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#945; BLEU(ref[], sys)&#8722; (1 &#8722; &#945;) BLEU(other_sys[], sys)</text>
              <doc_id>70</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Since this learning objective is simply the difference between two BLEU scores, it should be easy to integrate into most existing machine translation tuning pipelines that are already designed to optimize performance on translation evaluation metrics.</text>
              <doc_id>71</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(3)</text>
              <doc_id>72</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>5 Tuning to Positive Diversity</title>
        <text>To tune a collection of machine translation systems using Positive Diversity, we propose a staged process, whereby systems are tuned one-by-one to maximize equation (2) using the translations produced by previously trained systems to compute the diversity term, Similarity(other_sys[], sys &#920; ). As shown in Algorithm 1, Positive Diversity Tuning (PDT) takes as input: a list of machine translation systems, systems[]; a tuning procedure for training individual systems, tune(); a tuning data set with source and reference translations, source and refs; a hyperparameter &#945; to adjust the trade-off between fitting the reference translations and diversity between the systems; and metrics to measure correctness and cross-system similarity, Correctness() and Similarity().
The list of systems can contain any translation system that can be parameterized using tune(). This can be a heterogeneous collection of substantially different systems (e.g., phrase-based, hierarchical, syntactic, or tunable hybrid systems) or even multiple copies of a single machine translation system. In all cases, systems later in the list will be trained to produce translations that both fit the references and are encouraged to be distinct from the systems earlier in the list. During each iteration, the system constructs a new Positive Diversity measure PD &#945;,i using the translations produced during prior iterations of training. This PD &#945;,i measure is then given to tune() as the the training criteria for model i of system i . The function tune() is any algorithm that allows a translation system&#8217;s performance to be fit to an evaluation metric. This includes both minimum error rate training algorithms (MERT) that attempt to directly optimize a system&#8217;s performance on a metric, as well as other techniques such as Pairwaise Ranking Objective (PRO), MIRA, and RAMPION that optimize a surrogate loss based on the preferences of an evaluation metric.
After training a model for each system, the resulting model-system pairs can be combined using any arbitrary system combination strategy.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>To tune a collection of machine translation systems using Positive Diversity, we propose a staged process, whereby systems are tuned one-by-one to maximize equation (2) using the translations produced by previously trained systems to compute the diversity term, Similarity(other_sys[], sys &#920; ).</text>
              <doc_id>73</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As shown in Algorithm 1, Positive Diversity Tuning (PDT) takes as input: a list of machine translation systems, systems[]; a tuning procedure for training individual systems, tune(); a tuning data set with source and reference translations, source and refs; a hyperparameter &#945; to adjust the trade-off between fitting the reference translations and diversity between the systems; and metrics to measure correctness and cross-system similarity, Correctness() and Similarity().</text>
              <doc_id>74</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The list of systems can contain any translation system that can be parameterized using tune().</text>
              <doc_id>75</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This can be a heterogeneous collection of substantially different systems (e.g., phrase-based, hierarchical, syntactic, or tunable hybrid systems) or even multiple copies of a single machine translation system.</text>
              <doc_id>76</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In all cases, systems later in the list will be trained to produce translations that both fit the references and are encouraged to be distinct from the systems earlier in the list.</text>
              <doc_id>77</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>During each iteration, the system constructs a new Positive Diversity measure PD &#945;,i using the translations produced during prior iterations of training.</text>
              <doc_id>78</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This PD &#945;,i measure is then given to tune() as the the training criteria for model i of system i .</text>
              <doc_id>79</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The function tune() is any algorithm that allows a translation system&#8217;s performance to be fit to an evaluation metric.</text>
              <doc_id>80</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>This includes both minimum error rate training algorithms (MERT) that attempt to directly optimize a system&#8217;s performance on a metric, as well as other techniques such as Pairwaise Ranking Objective (PRO), MIRA, and RAMPION that optimize a surrogate loss based on the preferences of an evaluation metric.</text>
              <doc_id>81</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>After training a model for each system, the resulting model-system pairs can be combined using any arbitrary system combination strategy.</text>
              <doc_id>82</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>6 Experiments</title>
        <text>Experiments are performed using a single phrase-based Chinese-to-English translation system, built with the Stanford Phrasal machine translation toolkit (Cer et al., 2010). The system was built using all of the parallel data available for Phase 2 of the DARPA BOLT program. The Chinese data was segmented to the Chinese Tree- Bank (CTB) standard using a maximum match word segmenter, trained on the output of a CRF segmenter (Xiang et al., 2013). The bitext was word aligned using the Berkeley aligner (Liang et al., 2006). Standard phrase-pair extraction heuris-
tics were used to extract a phrase-table over word alignments symmetrized using grow-diag (Koehn et al., 2003). We made use of a hierarchical reordering model (Galley and Manning, 2008) as well as a 5-gram language model trained on the target side of the bi-text and smoothed using modified Kneeser-Ney (Chen and Goodman, 1996).
Individual PDT systems were tuned on the GALE dev10 web tune set using online-PRO (Green et al., 2013; Hopkins and May, 2011) to the Positive Diversity Tuning criterion. 4 The Multi-Engine Machine Translation (MEMT) package was used for system combination (Heafield and Lavie, 2010a). We used BOLT dev12 dev as a development test set to explore different &#945; parameterizations of the Positive Diversity criteria.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Experiments are performed using a single phrase-based Chinese-to-English translation system, built with the Stanford Phrasal machine translation toolkit (Cer et al., 2010).</text>
              <doc_id>83</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The system was built using all of the parallel data available for Phase 2 of the DARPA BOLT program.</text>
              <doc_id>84</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The Chinese data was segmented to the Chinese Tree- Bank (CTB) standard using a maximum match word segmenter, trained on the output of a CRF segmenter (Xiang et al., 2013).</text>
              <doc_id>85</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The bitext was word aligned using the Berkeley aligner (Liang et al., 2006).</text>
              <doc_id>86</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Standard phrase-pair extraction heuris-</text>
              <doc_id>87</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tics were used to extract a phrase-table over word alignments symmetrized using grow-diag (Koehn et al., 2003).</text>
              <doc_id>88</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We made use of a hierarchical reordering model (Galley and Manning, 2008) as well as a 5-gram language model trained on the target side of the bi-text and smoothed using modified Kneeser-Ney (Chen and Goodman, 1996).</text>
              <doc_id>89</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Individual PDT systems were tuned on the GALE dev10 web tune set using online-PRO (Green et al., 2013; Hopkins and May, 2011) to the Positive Diversity Tuning criterion.</text>
              <doc_id>90</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>4 The Multi-Engine Machine Translation (MEMT) package was used for system combination (Heafield and Lavie, 2010a).</text>
              <doc_id>91</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We used BOLT dev12 dev as a development test set to explore different &#945; parameterizations of the Positive Diversity criteria.</text>
              <doc_id>92</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>7</index>
        <title>7 Results</title>
        <text>Table 1 illustrates the amount of diversity achieved by individual PDT systems on the BOLT dev12 dev evaluation set for &#945; values 0.95, 0.97, and 0.99. 5 Using different tuning sets is one of the common strategies for producing diverse component systems for system combination. Thus, as a baseline, Table 2 gives the diversity of a system tuned to BLEU using a different tuning set, BOLT dev12 tune, with respect to the PDT systems available at each iteration. As in Table 1, the diversity computation is performed using translations of BOLT dev12 dev.
Like the cross-system diversity term in the formulation of Positive Diversity using BLEU in
4 Preliminary experiments performed using MERT to train
the individual systems produced similar results to those seen here. However, we switched to online-PRO since it dramatically reduced the amount time required to train each individual system. We expect similar results when using other tuning algorithms for the individual systems, such as MIRA or RAMPION. 5 Due to time constraints, we were not able to try additional
&#945; values. Given that our results suggest the lowest &#945; value from the ones we tried works best (i.e., &#945; = 0.95), it would be worth trying additional smaller &#945; values such as 0.90
equation (3), we measure the diversity of translations produced by an individual system as the negative BLEU score of the translations with respect to the translations from systems built during prior iterations. For clarity of presentation, these diversity scores are reported as 1.0&#8722;BLEU. Using 1.0&#8722;BLEU to score cross-system diversity, means that the reported numbers can be roughly interpreted as the fraction of n-grams from the individual systems built during iteration i that have not been previously produced by other systems built during any iteration &lt; i. 6
In our experiments, we find that for &#945; &#8804; 0.97, during the first three iterations of PDT, there is more diversity among the PDT systems tuned on a single data set (GALE dev10 web tune) than there is between systems tuned on different datasets (BOLT dev12 tune vs. GALE dev10 wb tune). This is significant since using different tuning sets is a common strategy for increasing diversity during system combination. These results suggest PDT is better at producing additional diversity than using different tuning sets. The PDT systems also achieve good coverage of the n-grams present in the baseline system that was tuned using different data. At iteration 10 and using &#945; = 0.95, the baseline systems receive a diversity score of only 7.9% when measured against the PDT systems. 7
As PDT progresses, it becomes more difficult to tune systems to produce high quality translations that are substantially different from those already being produced by other systems. This is seen in the per iteration diversity scores, whereby during iteration 5, the individual PDT translation systems have a 1.0&#8722;BLEU diversity score with prior systems ranging from 11.9%, when using an &#945; value
6 This intuitive interpretation assumes a brevity penalty
that is approximately 1.0. 7 For this diversity score, the brevity penalty is 1.0, meaning the diversity score is based purely on the n-grams present in the baseline system that are not present in translations produced by one or more of the PDT systems
of 0.95, to 3.2% when using an &#945; value of 0.99. A diversity score of 3.2% when using &#945; = 0.99 suggests that by iteration 5, very high &#945; values put insufficient pressure on learning to find models that produce diverse translations. When using an &#945; of 0.95, a sizable amount of diversity still exists across the systems translations all the way to iteration 7. By iteration 10, only a small amount of additional diversity is contributed by each additional system for all of the alpha values (&lt; 3%). 8
Table 3 shows the BLEU scores obtained on the BOLT dev12 dev evaluation set by the individual systems tuned during each iteration of PDT. The 0 th iteration for each &#945; value has an empty set of translations for the diversity term. This means the resulting systems are effectively tuned to just maximize BLEU. Differences in system performance during this iteration are only due to differences in the random seeds used during training. Starting at iteration 1, the individual systems are optimized to produce translations that both score well on BLEU
8 We speculate that if heterogeneous translation systems
were used with PDT, it could be possible to run with higher &#945; values and still obtain diverse translations after a large number of PDT iterations
and are diverse from the systems produced during prior iterations. It is interesting to note that the systems trained during these subsequent iterations obtain BLEU scores that are usually competitive with those obtained by the iteration 0 systems. Taken together with the diversity scores in Table 1, this strongly suggests that PDT is succeeding at increasing diversity while still producing high quality individual translation systems.
Figure 1 graphs the system combination BLEU score achieved by using varying numbers of Positive Diversity Tuned translation systems and different &#945; values to trade-off translation quality with translation diversity. After running 4 iterations of PDT, the best configuration, &#945; = 0.95, achieves a BLEU score that is 0.8 BLEU higher than the corresponding BLEU trained iteration 0 system. 9 From the graph, it appears that PDT performance initially increases as additional systems are added to the system combination and then later plateaus or even drops after too many systems are included. The combinations using PDT systems
9 Recall that the iteration 0 system is effectively just tuned
to maximize BLEU since we have an empty set of translations from other systems that are used to compute diversity
built with higher &#945; values reach the point of diminishing returns faster than combinations using systems built with lower alpha values. For instance, &#945; = 0.99 plateaus on iteration 2, while &#945; = 0.95 peaks on iteration 4. It might be possible to identify the point at which additional systems will likely not be useful by using the diversity scores in Table 1. Scoring about 10% or less on the 1&#8722;BLEU diversity measure, with respect to the other systems being used within the system combination, seems to suggest the individual system will not be very helpful to add into the combination.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Table 1 illustrates the amount of diversity achieved by individual PDT systems on the BOLT dev12 dev evaluation set for &#945; values 0.95, 0.97, and 0.99.</text>
              <doc_id>93</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>5 Using different tuning sets is one of the common strategies for producing diverse component systems for system combination.</text>
              <doc_id>94</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Thus, as a baseline, Table 2 gives the diversity of a system tuned to BLEU using a different tuning set, BOLT dev12 tune, with respect to the PDT systems available at each iteration.</text>
              <doc_id>95</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As in Table 1, the diversity computation is performed using translations of BOLT dev12 dev.</text>
              <doc_id>96</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Like the cross-system diversity term in the formulation of Positive Diversity using BLEU in</text>
              <doc_id>97</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>4 Preliminary experiments performed using MERT to train</text>
              <doc_id>98</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the individual systems produced similar results to those seen here.</text>
              <doc_id>99</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, we switched to online-PRO since it dramatically reduced the amount time required to train each individual system.</text>
              <doc_id>100</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We expect similar results when using other tuning algorithms for the individual systems, such as MIRA or RAMPION.</text>
              <doc_id>101</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>5 Due to time constraints, we were not able to try additional</text>
              <doc_id>102</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#945; values.</text>
              <doc_id>103</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Given that our results suggest the lowest &#945; value from the ones we tried works best (i.e., &#945; = 0.95), it would be worth trying additional smaller &#945; values such as 0.90</text>
              <doc_id>104</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>equation (3), we measure the diversity of translations produced by an individual system as the negative BLEU score of the translations with respect to the translations from systems built during prior iterations.</text>
              <doc_id>105</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For clarity of presentation, these diversity scores are reported as 1.0&#8722;BLEU.</text>
              <doc_id>106</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Using 1.0&#8722;BLEU to score cross-system diversity, means that the reported numbers can be roughly interpreted as the fraction of n-grams from the individual systems built during iteration i that have not been previously produced by other systems built during any iteration &lt; i.</text>
              <doc_id>107</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>6</text>
              <doc_id>108</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In our experiments, we find that for &#945; &#8804; 0.97, during the first three iterations of PDT, there is more diversity among the PDT systems tuned on a single data set (GALE dev10 web tune) than there is between systems tuned on different datasets (BOLT dev12 tune vs. GALE dev10 wb tune).</text>
              <doc_id>109</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This is significant since using different tuning sets is a common strategy for increasing diversity during system combination.</text>
              <doc_id>110</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>These results suggest PDT is better at producing additional diversity than using different tuning sets.</text>
              <doc_id>111</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The PDT systems also achieve good coverage of the n-grams present in the baseline system that was tuned using different data.</text>
              <doc_id>112</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>At iteration 10 and using &#945; = 0.95, the baseline systems receive a diversity score of only 7.9% when measured against the PDT systems.</text>
              <doc_id>113</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>7</text>
              <doc_id>114</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>As PDT progresses, it becomes more difficult to tune systems to produce high quality translations that are substantially different from those already being produced by other systems.</text>
              <doc_id>115</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This is seen in the per iteration diversity scores, whereby during iteration 5, the individual PDT translation systems have a 1.0&#8722;BLEU diversity score with prior systems ranging from 11.9%, when using an &#945; value</text>
              <doc_id>116</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>6 This intuitive interpretation assumes a brevity penalty</text>
              <doc_id>117</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>that is approximately 1.0.</text>
              <doc_id>118</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>7 For this diversity score, the brevity penalty is 1.0, meaning the diversity score is based purely on the n-grams present in the baseline system that are not present in translations produced by one or more of the PDT systems</text>
              <doc_id>119</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>of 0.95, to 3.2% when using an &#945; value of 0.99.</text>
              <doc_id>120</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A diversity score of 3.2% when using &#945; = 0.99 suggests that by iteration 5, very high &#945; values put insufficient pressure on learning to find models that produce diverse translations.</text>
              <doc_id>121</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>When using an &#945; of 0.95, a sizable amount of diversity still exists across the systems translations all the way to iteration 7.</text>
              <doc_id>122</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>By iteration 10, only a small amount of additional diversity is contributed by each additional system for all of the alpha values (&lt; 3%).</text>
              <doc_id>123</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>8</text>
              <doc_id>124</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Table 3 shows the BLEU scores obtained on the BOLT dev12 dev evaluation set by the individual systems tuned during each iteration of PDT.</text>
              <doc_id>125</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The 0 th iteration for each &#945; value has an empty set of translations for the diversity term.</text>
              <doc_id>126</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This means the resulting systems are effectively tuned to just maximize BLEU.</text>
              <doc_id>127</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Differences in system performance during this iteration are only due to differences in the random seeds used during training.</text>
              <doc_id>128</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Starting at iteration 1, the individual systems are optimized to produce translations that both score well on BLEU</text>
              <doc_id>129</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>8 We speculate that if heterogeneous translation systems</text>
              <doc_id>130</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>were used with PDT, it could be possible to run with higher &#945; values and still obtain diverse translations after a large number of PDT iterations</text>
              <doc_id>131</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and are diverse from the systems produced during prior iterations.</text>
              <doc_id>132</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It is interesting to note that the systems trained during these subsequent iterations obtain BLEU scores that are usually competitive with those obtained by the iteration 0 systems.</text>
              <doc_id>133</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Taken together with the diversity scores in Table 1, this strongly suggests that PDT is succeeding at increasing diversity while still producing high quality individual translation systems.</text>
              <doc_id>134</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Figure 1 graphs the system combination BLEU score achieved by using varying numbers of Positive Diversity Tuned translation systems and different &#945; values to trade-off translation quality with translation diversity.</text>
              <doc_id>135</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>After running 4 iterations of PDT, the best configuration, &#945; = 0.95, achieves a BLEU score that is 0.8 BLEU higher than the corresponding BLEU trained iteration 0 system.</text>
              <doc_id>136</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>9 From the graph, it appears that PDT performance initially increases as additional systems are added to the system combination and then later plateaus or even drops after too many systems are included.</text>
              <doc_id>137</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The combinations using PDT systems</text>
              <doc_id>138</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>9 Recall that the iteration 0 system is effectively just tuned</text>
              <doc_id>139</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>to maximize BLEU since we have an empty set of translations from other systems that are used to compute diversity</text>
              <doc_id>140</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>built with higher &#945; values reach the point of diminishing returns faster than combinations using systems built with lower alpha values.</text>
              <doc_id>141</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For instance, &#945; = 0.99 plateaus on iteration 2, while &#945; = 0.95 peaks on iteration 4.</text>
              <doc_id>142</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>It might be possible to identify the point at which additional systems will likely not be useful by using the diversity scores in Table 1.</text>
              <doc_id>143</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Scoring about 10% or less on the 1&#8722;BLEU diversity measure, with respect to the other systems being used within the system combination, seems to suggest the individual system will not be very helpful to add into the combination.</text>
              <doc_id>144</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>8</index>
        <title>8 Related Work</title>
        <text>While the idea of encouraging diversity in individual systems that will be used for system combination has been proven effective in speech recognition and document summarization (Hinton, 2002; Breslin and Gales, 2007; Carbonell and Goldstein, 1998; Goldstein et al., 2000), there has only been a modest amount of prior work exploring such approaches for machine translation. Prior work within machine translation has investigated adapting machine learning techniques for building ensembles of classifiers to translation system tuning, encouraging diversity by varying both the hyperparameters and the data used to build the individual systems, and chaining together individual translation systems. Xiao et al. (2013) explores using boosting to train an ensemble of machine translation systems. Following the standard Adaboost algorithm, each system was trained in sequence on an error-driven reweighting of the tuning set that focuses learning on the material that is the most problematic for the current ensemble. They found that using a single system to tune a large number of decoding models to different Adaboost guided weightings of the tuning data results in significant gains during system combination.
Macherey and Och (2007) investigated system combination using automatic generation of diverse individual systems. They programmatically generated variations of systems using different build and decoder hyperparameters such as choice of wordalignment algorithm, distortion limit, variations of model feature function weights, and the set of language models used. Then, in a process similar to forward feature selection, they constructed a combined system by iteratively adding the individual automatically generated system that produced the largest increase in quality when used in conjunction with the systems already selected for the combined system. They also explored producing variation by using different samplings of the the training data. The individual and combined systems produced by sampling the training data were inferior to systems that used all of the available data. However, the experiments facilitated insightful analysis on what properties an individual system must have in order to be useful during system combination. They found that in order to be useful within a combination, individual systems need to produce translations of similar quality to other individual systems within the system combination while also being as uncorrelated as possible from the other systems. The Positive Diversity Tuning method introduced in our work is an explicit attempt to build individual translation systems that meet this criteria, while being less computationally demanding than the diversity generating techniques explored by Macherey and Och (2007). Aikawa and Ruopp (2009) investigated building machine translations systems specifically for use in sequential combination with other systems. They constructed chains of systems whereby the output of one decoder is feed as input to the next decoder in the pipeline. The downstream systems are built and tuned to correct errors produced by the preceding system. In this approach, the downstream decoder acts as a machine learning based post editing system.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>While the idea of encouraging diversity in individual systems that will be used for system combination has been proven effective in speech recognition and document summarization (Hinton, 2002; Breslin and Gales, 2007; Carbonell and Goldstein, 1998; Goldstein et al., 2000), there has only been a modest amount of prior work exploring such approaches for machine translation.</text>
              <doc_id>145</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Prior work within machine translation has investigated adapting machine learning techniques for building ensembles of classifiers to translation system tuning, encouraging diversity by varying both the hyperparameters and the data used to build the individual systems, and chaining together individual translation systems.</text>
              <doc_id>146</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Xiao et al. (2013) explores using boosting to train an ensemble of machine translation systems.</text>
              <doc_id>147</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Following the standard Adaboost algorithm, each system was trained in sequence on an error-driven reweighting of the tuning set that focuses learning on the material that is the most problematic for the current ensemble.</text>
              <doc_id>148</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>They found that using a single system to tune a large number of decoding models to different Adaboost guided weightings of the tuning data results in significant gains during system combination.</text>
              <doc_id>149</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Macherey and Och (2007) investigated system combination using automatic generation of diverse individual systems.</text>
              <doc_id>150</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>They programmatically generated variations of systems using different build and decoder hyperparameters such as choice of wordalignment algorithm, distortion limit, variations of model feature function weights, and the set of language models used.</text>
              <doc_id>151</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Then, in a process similar to forward feature selection, they constructed a combined system by iteratively adding the individual automatically generated system that produced the largest increase in quality when used in conjunction with the systems already selected for the combined system.</text>
              <doc_id>152</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>They also explored producing variation by using different samplings of the the training data.</text>
              <doc_id>153</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The individual and combined systems produced by sampling the training data were inferior to systems that used all of the available data.</text>
              <doc_id>154</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>However, the experiments facilitated insightful analysis on what properties an individual system must have in order to be useful during system combination.</text>
              <doc_id>155</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>They found that in order to be useful within a combination, individual systems need to produce translations of similar quality to other individual systems within the system combination while also being as uncorrelated as possible from the other systems.</text>
              <doc_id>156</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The Positive Diversity Tuning method introduced in our work is an explicit attempt to build individual translation systems that meet this criteria, while being less computationally demanding than the diversity generating techniques explored by Macherey and Och (2007).</text>
              <doc_id>157</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Aikawa and Ruopp (2009) investigated building machine translations systems specifically for use in sequential combination with other systems.</text>
              <doc_id>158</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>They constructed chains of systems whereby the output of one decoder is feed as input to the next decoder in the pipeline.</text>
              <doc_id>159</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>The downstream systems are built and tuned to correct errors produced by the preceding system.</text>
              <doc_id>160</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>In this approach, the downstream decoder acts as a machine learning based post editing system.</text>
              <doc_id>161</doc_id>
              <sec_id>11</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>9</index>
        <title>9 Conclusion</title>
        <text>We have presented Positive Diversity as a new way of jointly measuring the quality and diversity of the contribution of individual machine translation systems to system combination. This method heuristically assesses the value of individual translation systems by measuring their similarity to the reference translations as well as their dissimilarity from the other systems being combined. We operationalize this metric by reusing existing techniques from machine translation evaluation to assess translation quality and the degree of similarity between systems. We also give a straightforward algorithm for training a collection of individual systems to optimize Positive Diversity. Our experimental results suggest that tuning to Positive Diversity leads to improved cross-system diversity and system combination performance even when combining otherwise identical machine translation 326 systems. The Positive Diversity Tuning method explored in this work can be used to tune individual systems for any ensemble in which individual models can be fit to multiple extrinsic loss functions. Since Hall et al. (2011) demonstrated the general purpose application of multiple extrinsic loss functions to training structured prediction models, Positive Diversity Tuning could be broadly useful within natural language processing and for other machine learning tasks. In future work within machine translation, it may prove fruitful to examine more sophisticated measures of dissimilarity. For example, one could imagine a metric that punishes instances of similar material in proportion to some measure of the expected diversity of the material. It might also be useful to explore joint rather than sequential training of the individual translation systems.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have presented Positive Diversity as a new way of jointly measuring the quality and diversity of the contribution of individual machine translation systems to system combination.</text>
              <doc_id>162</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This method heuristically assesses the value of individual translation systems by measuring their similarity to the reference translations as well as their dissimilarity from the other systems being combined.</text>
              <doc_id>163</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We operationalize this metric by reusing existing techniques from machine translation evaluation to assess translation quality and the degree of similarity between systems.</text>
              <doc_id>164</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We also give a straightforward algorithm for training a collection of individual systems to optimize Positive Diversity.</text>
              <doc_id>165</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Our experimental results suggest that tuning to Positive Diversity leads to improved cross-system diversity and system combination performance even when combining otherwise identical machine translation 326 systems.</text>
              <doc_id>166</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The Positive Diversity Tuning method explored in this work can be used to tune individual systems for any ensemble in which individual models can be fit to multiple extrinsic loss functions.</text>
              <doc_id>167</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Since Hall et al. (2011) demonstrated the general purpose application of multiple extrinsic loss functions to training structured prediction models, Positive Diversity Tuning could be broadly useful within natural language processing and for other machine learning tasks.</text>
              <doc_id>168</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In future work within machine translation, it may prove fruitful to examine more sophisticated measures of dissimilarity.</text>
              <doc_id>169</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>For example, one could imagine a metric that punishes instances of similar material in proportion to some measure of the expected diversity of the material.</text>
              <doc_id>170</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>It might also be useful to explore joint rather than sequential training of the individual translation systems.</text>
              <doc_id>171</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>10</index>
        <title>Acknowledgments</title>
        <text>We thank the reviewers and the members of the Stanford NLP group for their helpful comments and suggestions. This work was supported by the Defense Advanced Research Projects Agency (DARPA) Broad Operational Language Translation (BOLT) program through IBM and a fellowship to one of the authors from the Center for Advanced Study in the Behavioral Sciences. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of DARPA or the US government.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We thank the reviewers and the members of the Stanford NLP group for their helpful comments and suggestions.</text>
              <doc_id>172</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This work was supported by the Defense Advanced Research Projects Agency (DARPA) Broad Operational Language Translation (BOLT) program through IBM and a fellowship to one of the authors from the Center for Advanced Study in the Behavioral Sciences.</text>
              <doc_id>173</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of DARPA or the US government.</text>
              <doc_id>174</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Diversity scores for PDT individual systems on BOLT dev12 dev. Individual systems are tuned to Positive Diversity on GALE dev10 web tune. A system&#8217;s diversity score is measured as its 1.0&#8722;BLEU score on the translations produced by PDT systems from earlier iterations. Higher scores mean more diversity.</caption>
        <reference_text></reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>PDT Individual System Diversity</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System \ Iteration</cell>
              <cell>1</cell>
              <cell>2</cell>
              <cell>3</cell>
              <cell>4</cell>
              <cell>5</cell>
              <cell>6</cell>
              <cell>7</cell>
              <cell>8</cell>
              <cell>9</cell>
              <cell>10</cell>
            </row>
            <row>
              <cell>&#945; = 0.95</cell>
              <cell>36.6</cell>
              <cell>32.0</cell>
              <cell>19.0</cell>
              <cell>13.6</cell>
              <cell>11.9</cell>
              <cell>8.2</cell>
              <cell>15.9</cell>
              <cell>8.7</cell>
              <cell>7.3</cell>
              <cell>2.3</cell>
            </row>
            <row>
              <cell>&#945; = 0.97</cell>
              <cell>32.9</cell>
              <cell>21.7</cell>
              <cell>17.7</cell>
              <cell>10.4</cell>
              <cell>2.7</cell>
              <cell>7.4</cell>
              <cell>2.3</cell>
              <cell>7.3</cell>
              <cell>2.1</cell>
              <cell>2.9</cell>
            </row>
            <row>
              <cell>&#945; = 0.99</cell>
              <cell>23.9</cell>
              <cell>13.1</cell>
              <cell>7.9</cell>
              <cell>2.3</cell>
              <cell>3.2</cell>
              <cell>2.6</cell>
              <cell>2.2</cell>
              <cell>1.5</cell>
              <cell>3.4</cell>
              <cell>0.7</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 1: Diversity scores for PDT individual systems on BOLT dev12 dev. Individual systems are tuned to Positive Diversity on GALE dev10 web tune. A system?s diversity score is measured as its 1.0?BLEU score on the translations produced by PDT systems from earlier iterations. Higher scores mean more#@#@Table 2: Diversity scores of a baseline system tuned to BOLT dev12 tune, a different tuning set than what was used for the PDT individual systems. The baseline system diversity is scored against all of the PDT individual systems available at iteration i for a given &#945; value and over translations of BOLT dev12 dev.</caption>
        <reference_text>In PAGE 5: ... Thus, as a baseline, Table 2 gives the diversity of a system tuned to BLEU using a different tuning set, BOLT dev12 tune, with respect to the PDT systems avail- able at each iteration. As in  Table1 , the diver- sity computation is performed using translations of BOLT dev12 dev. Like the cross-system diversity term in the for- mulation of Positive Diversity using BLEU in 4 Preliminary experiments performed using MERT to train the individual systems produced similar results to those seen here....</reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>Diversity of Baseline System vs. Individual PDT Systems Available at Iteration i</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>PDT Systems \ Iteration</cell>
              <cell>0</cell>
              <cell>1</cell>
              <cell>2</cell>
              <cell>3</cell>
              <cell>4</cell>
              <cell>5</cell>
              <cell>6</cell>
              <cell>7</cell>
              <cell>8</cell>
              <cell>9</cell>
              <cell>10</cell>
            </row>
            <row>
              <cell>&#945; = 0.95</cell>
              <cell>27.3</cell>
              <cell>20.4</cell>
              <cell>16.8</cell>
              <cell>14.9</cell>
              <cell>12.8</cell>
              <cell>11.4</cell>
              <cell>9.4</cell>
              <cell>8.6</cell>
              <cell>8.3</cell>
              <cell>8.1</cell>
              <cell>7.9</cell>
            </row>
            <row>
              <cell>&#945; = 0.97</cell>
              <cell>28.4</cell>
              <cell>21.3</cell>
              <cell>15.8</cell>
              <cell>14.7</cell>
              <cell>13.3</cell>
              <cell>13.0</cell>
              <cell>12.5</cell>
              <cell>12.2</cell>
              <cell>10.3</cell>
              <cell>10.0</cell>
              <cell>9.7</cell>
            </row>
            <row>
              <cell>&#945; = 0.99</cell>
              <cell>27.5</cell>
              <cell>22.6</cell>
              <cell>18.5</cell>
              <cell>17.1</cell>
              <cell>16.8</cell>
              <cell>15.9</cell>
              <cell>15.4</cell>
              <cell>14.6</cell>
              <cell>14.3</cell>
              <cell>13.5</cell>
              <cell>13.4</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TET</source>
        <caption>Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE dev10 web tune. Scores report individual system performance before system combination.</caption>
        <reference_text></reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>BLEU scores from individual systems</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>tuned during iteration i of PDT</cell>
            </row>
            <row>
              <cell>PDT System</cell>
              <cell>0</cell>
              <cell>1</cell>
              <cell>2</cell>
              <cell>3</cell>
              <cell>4</cell>
              <cell>5</cell>
              <cell>6</cell>
              <cell>7</cell>
              <cell>8</cell>
              <cell>9</cell>
              <cell>10</cell>
            </row>
            <row>
              <cell>&#945; = 0.95</cell>
              <cell>16.2</cell>
              <cell>16.0</cell>
              <cell>15.7</cell>
              <cell>15.9</cell>
              <cell>16.1</cell>
              <cell>16.1</cell>
              <cell>15.9</cell>
              <cell>15.4</cell>
              <cell>16.1</cell>
              <cell>15.9</cell>
              <cell>16.2</cell>
            </row>
            <row>
              <cell>&#945; = 0.97</cell>
              <cell>16.4</cell>
              <cell>15.8</cell>
              <cell>15.8</cell>
              <cell>15.9</cell>
              <cell>16.0</cell>
              <cell>16.2</cell>
              <cell>16.1</cell>
              <cell>16.2</cell>
              <cell>16.2</cell>
              <cell>16.4</cell>
              <cell>16.1</cell>
            </row>
            <row>
              <cell>&#945; = 0.99</cell>
              <cell>16.3</cell>
              <cell>16.1</cell>
              <cell>16.2</cell>
              <cell>15.9</cell>
              <cell>16.3</cell>
              <cell>16.4</cell>
              <cell>16.4</cell>
              <cell>16.3</cell>
              <cell>16.4</cell>
              <cell>16.5</cell>
              <cell>16.3</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Takako Aikawa</author>
          <author>Achim Ruopp</author>
        </authors>
        <title>Chained system: A linear combination of different types of statistical machine translation systems.</title>
        <publication>In Proceedings of MT Summit XII.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>S Bangalore</author>
          <author>G Bordel</author>
          <author>Giuseppe Riccardi</author>
        </authors>
        <title>Computing consensus translation from multiple machine translation systems.</title>
        <publication>In ASRU.</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>C Breslin</author>
          <author>M J F Gales</author>
        </authors>
        <title>Complementary system generation using directed decision trees.</title>
        <publication>In ICASSP.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Jaime Carbonell</author>
          <author>Jade Goldstein</author>
        </authors>
        <title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
        <publication>In SIGIR.</publication>
        <pages>None</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Daniel Cer</author>
          <author>Michel Galley</author>
          <author>Daniel Jurafsky</author>
          <author>Christopher D Manning</author>
        </authors>
        <title>Phrasal: A statistical machine translation toolkit for Exploring new model features.</title>
        <publication>In NAACL/HLT.</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Stanley F Chen</author>
          <author>Joshua Goodman</author>
        </authors>
        <title>An empirical study of smoothing techniques for language modeling.</title>
        <publication>In ACL.</publication>
        <pages>None</pages>
        <date>1996</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Colin Cherry</author>
          <author>George Foster</author>
        </authors>
        <title>Batch tuning strategies for statistical machine translation.</title>
        <publication>In NAACL/HLT.</publication>
        <pages>None</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>David Chiang</author>
          <author>Yuval Marton</author>
          <author>Philip Resnik</author>
        </authors>
        <title>Online large-margin training of syntactic and structural translation features.</title>
        <publication>In EMNLP.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>John DeNero</author>
          <author>Shankar Kumar</author>
          <author>Ciprian Chelba</author>
          <author>Franz Och</author>
        </authors>
        <title>Model combination for machine translation.</title>
        <publication>In NAACL/HLT.</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Michael Denkowski</author>
          <author>Alon Lavie</author>
        </authors>
        <title>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</title>
        <publication>In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</publication>
        <pages>None</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>J G Fiscus</author>
        </authors>
        <title>A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER).</title>
        <publication>In ASRU.</publication>
        <pages>None</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Michel Galley</author>
          <author>Christopher D Manning</author>
        </authors>
        <title>A simple and effective hierarchical phrase reordering model.</title>
        <publication>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Kevin Gimpel</author>
          <author>Noah A Smith</author>
        </authors>
        <title>Structured ramp loss minimization for machine translation.</title>
        <publication>In NAACL/HLT.</publication>
        <pages>None</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>J Goldstein</author>
          <author>V Mittal</author>
          <author>J Carbonell</author>
          <author>M Kantrowitz</author>
        </authors>
        <title>Multi-document summarization by sentence extraction.</title>
        <publication>In ANLP/NAACL Workshop on Automatic Summarization.</publication>
        <pages>None</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Spence Green</author>
          <author>Sida Wang</author>
          <author>Daniel Cer</author>
          <author>Christopher D Manning</author>
        </authors>
        <title>Fast and adaptive online training of feature-rich translation models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2013</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Keith Hall</author>
          <author>Ryan McDonald</author>
          <author>Slav Petrov</author>
        </authors>
        <title>Training structured prediction models with extrinsic loss functions.</title>
        <publication>In Domain Adaptation Workshop at NIPS.</publication>
        <pages>None</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Kenneth Heafield</author>
          <author>Alon Lavie</author>
        </authors>
        <title>CMU multiengine machine translation for WMT</title>
        <publication>In WMT.</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Kenneth Heafield</author>
          <author>Alon Lavie</author>
        </authors>
        <title>Voting on ngrams for machine translation system combination.</title>
        <publication>In AMTA.</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Geoffrey E Hinton</author>
        </authors>
        <title>Training products of experts by minimizing contrastive divergence.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Mark Hopkins</author>
          <author>Jonathan May</author>
        </authors>
        <title>Tuning as ranking.</title>
        <publication>In EMNLP.</publication>
        <pages>None</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Fei Huang</author>
          <author>Kishore Papineni</author>
        </authors>
        <title>Hierarchical system combination for machine translation.</title>
        <publication>In EMNLP-CoNLL.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Jason Eisner Karakos</author>
          <author>Sanjeev Khudanpur</author>
          <author>Markus Dreyer</author>
        </authors>
        <title>Machine translation system combination using ITG-based alignments.</title>
        <publication>In ACL/HLT.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Simon Zwarts</author>
          <author>Mark Dras</author>
        </authors>
        <title>Choosing the right translation: A syntactically informed classification approach.</title>
        <publication>In CoLING.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz Josef Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical phrase-based translation.</title>
        <publication>In NAACL.</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>Gregor Leusch</author>
          <author>Nicola Ueffing</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A novel string-to-string distance measure with applications to machine translation evaluation.</title>
        <publication>In MT Summit.</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Percy Liang</author>
          <author>Ben Taskar</author>
          <author>Dan Klein</author>
        </authors>
        <title>Alignment by agreement.</title>
        <publication>In NAACL/HLT.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>Wolfgang Macherey</author>
          <author>Franz J Och</author>
        </authors>
        <title>An empirical study on computing consensus translations from multiple machine translation systems.</title>
        <publication>In EMNLP/CoNLL.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>27</id>
        <authors>
          <author>Evgeny Matusov</author>
          <author>Nicola Ueffing</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
        <publication>In EMNLP.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>28</id>
        <authors>
          <author>Tadashi Nomoto</author>
        </authors>
        <title>Multi-engine machine translation with voted language model.</title>
        <publication>In ACL.</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>29</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>In ACL.</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>30</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>Bleu: a method for automatic evaluation of machine translation.</title>
        <publication>In ACL.</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>31</id>
        <authors>
          <author>Michael Paul</author>
        </authors>
        <title>Takao Doi, Youngsook Hwang, Kenji Imamura, Hideo Okuma, and Eiichiro Sumita.</title>
        <publication>In IWSLT.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>32</id>
        <authors>
          <author>Antti-Veikko Rosti</author>
          <author>Necip Fazil Ayan</author>
          <author>Bing Xiang</author>
          <author>Spyros Matsoukas</author>
          <author>Richard Schwartz</author>
          <author>Bonnie Dorr</author>
        </authors>
        <title>Combining outputs from multiple machine translation systems.</title>
        <publication>In NAACL/HLT.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>33</id>
        <authors>
          <author>Antti-Veikko Rosti</author>
          <author>Spyros Matsoukas</author>
          <author>Richard Schwartz</author>
        </authors>
        <title>Improved word-level system combination for machine translation.</title>
        <publication>In ACL.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>34</id>
        <authors>
          <author>Matthew Snover</author>
          <author>Nitin Madnani</author>
          <author>Bonnie J Dorr</author>
          <author>Richard Schwartz</author>
        </authors>
        <title>Fluency, adequacy, or HTER?: exploring different human judgments with a tunable MT metric.</title>
        <publication>In WMT.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>35</id>
        <authors>
          <author>Bing Xiang</author>
          <author>Xiaoqiang Luo</author>
          <author>Bowen Zhou</author>
        </authors>
        <title>Enlisting the ghost: Modeling empty categories for machine translation.</title>
        <publication>In ACL.</publication>
        <pages>None</pages>
        <date>2013</date>
      </reference>
      <reference>
        <id>36</id>
        <authors>
          <author>Tong Xiao</author>
          <author>Jingbo Zhu</author>
          <author>Tongran Liu</author>
        </authors>
        <title>Bagging and boosting statistical machine translation systems.</title>
        <publication>None</publication>
        <pages>195--496</pages>
        <date>2013</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Aikawa and Ruopp (2009)</string>
        <sentence_id>54742</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>0</reference_id>
        <string>Aikawa and Ruopp, 2009</string>
        <sentence_id>54628</sentence_id>
        <char_offset>329</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>Bangalore et al., 2001</string>
        <sentence_id>54622</sentence_id>
        <char_offset>436</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>2</reference_id>
        <string>Breslin and Gales, 2007</string>
        <sentence_id>54729</sentence_id>
        <char_offset>193</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>3</reference_id>
        <string>Carbonell and Goldstein, 1998</string>
        <sentence_id>54729</sentence_id>
        <char_offset>218</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>4</reference_id>
        <string>Cer et al., 2010</string>
        <sentence_id>54667</sentence_id>
        <char_offset>154</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>5</reference_id>
        <string>Chen and Goodman, 1996</string>
        <sentence_id>54673</sentence_id>
        <char_offset>192</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>6</reference_id>
        <string>Cherry and Foster, 2012</string>
        <sentence_id>54618</sentence_id>
        <char_offset>196</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>7</reference_id>
        <string>Chiang et al., 2008</string>
        <sentence_id>54617</sentence_id>
        <char_offset>93</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>8</reference_id>
        <string>DeNero et al., 2010</string>
        <sentence_id>54595</sentence_id>
        <char_offset>229</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>8</reference_id>
        <string>DeNero et al., 2010</string>
        <sentence_id>54628</sentence_id>
        <char_offset>480</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>9</reference_id>
        <string>Denkowski and Lavie, 2011</string>
        <sentence_id>54623</sentence_id>
        <char_offset>185</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>10</reference_id>
        <string>Fiscus, 1997</string>
        <sentence_id>54622</sentence_id>
        <char_offset>92</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>11</reference_id>
        <string>Galley and Manning, 2008</string>
        <sentence_id>54673</sentence_id>
        <char_offset>48</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>12</reference_id>
        <string>Gimpel and Smith, 2012</string>
        <sentence_id>54617</sentence_id>
        <char_offset>128</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>13</reference_id>
        <string>Goldstein et al., 2000</string>
        <sentence_id>54729</sentence_id>
        <char_offset>249</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>14</reference_id>
        <string>Green et al., 2013</string>
        <sentence_id>54674</sentence_id>
        <char_offset>83</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>15</reference_id>
        <string>Hall et al. (2011)</string>
        <sentence_id>54752</sentence_id>
        <char_offset>6</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>16</reference_id>
        <string>Heafield and Lavie, 2010</string>
        <sentence_id>54622</sentence_id>
        <char_offset>546</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>16</reference_id>
        <string>Heafield and Lavie, 2010</string>
        <sentence_id>54624</sentence_id>
        <char_offset>364</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>16</reference_id>
        <string>Heafield and Lavie, 2010</string>
        <sentence_id>54675</sentence_id>
        <char_offset>87</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>17</reference_id>
        <string>Heafield and Lavie, 2010</string>
        <sentence_id>54622</sentence_id>
        <char_offset>546</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>17</reference_id>
        <string>Heafield and Lavie, 2010</string>
        <sentence_id>54624</sentence_id>
        <char_offset>364</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>17</reference_id>
        <string>Heafield and Lavie, 2010</string>
        <sentence_id>54675</sentence_id>
        <char_offset>87</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>18</reference_id>
        <string>Hinton, 2002</string>
        <sentence_id>54729</sentence_id>
        <char_offset>179</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>19</reference_id>
        <string>Hopkins and May, 2011</string>
        <sentence_id>54617</sentence_id>
        <char_offset>63</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>19</reference_id>
        <string>Hopkins and May, 2011</string>
        <sentence_id>54674</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>20</reference_id>
        <string>Huang and Papineni, 2007</string>
        <sentence_id>54628</sentence_id>
        <char_offset>421</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>21</reference_id>
        <string>Karakos et al., 2008</string>
        <sentence_id>54622</sentence_id>
        <char_offset>524</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>22</reference_id>
        <string>Zwarts and Dras, 2008</string>
        <sentence_id>54628</sentence_id>
        <char_offset>287</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>23</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>54672</sentence_id>
        <char_offset>91</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>24</reference_id>
        <string>Leusch et al., 2003</string>
        <sentence_id>54623</sentence_id>
        <char_offset>143</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>25</reference_id>
        <string>Liang et al., 2006</string>
        <sentence_id>54670</sentence_id>
        <char_offset>56</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>26</reference_id>
        <string>Macherey and Och (2007)</string>
        <sentence_id>54592</sentence_id>
        <char_offset>55</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>26</reference_id>
        <string>Macherey and Och (2007)</string>
        <sentence_id>54734</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>26</reference_id>
        <string>Macherey and Och (2007)</string>
        <sentence_id>54741</sentence_id>
        <char_offset>244</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>27</reference_id>
        <string>Matusov et al., 2006</string>
        <sentence_id>54622</sentence_id>
        <char_offset>460</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>28</reference_id>
        <string>Nomoto, 2004</string>
        <sentence_id>54628</sentence_id>
        <char_offset>273</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>29</reference_id>
        <string>Och, 2003</string>
        <sentence_id>54616</sentence_id>
        <char_offset>95</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>30</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>54614</sentence_id>
        <char_offset>73</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>32</reference_id>
        <string>Rosti et al., 2007</string>
        <sentence_id>54622</sentence_id>
        <char_offset>482</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>32</reference_id>
        <string>Rosti et al., 2007</string>
        <sentence_id>54622</sentence_id>
        <char_offset>503</char_offset>
      </citation>
      <citation>
        <id>42</id>
        <reference_id>32</reference_id>
        <string>Rosti et al., 2007</string>
        <sentence_id>54624</sentence_id>
        <char_offset>324</char_offset>
      </citation>
      <citation>
        <id>43</id>
        <reference_id>33</reference_id>
        <string>Rosti et al., 2007</string>
        <sentence_id>54622</sentence_id>
        <char_offset>482</char_offset>
      </citation>
      <citation>
        <id>44</id>
        <reference_id>33</reference_id>
        <string>Rosti et al., 2007</string>
        <sentence_id>54622</sentence_id>
        <char_offset>503</char_offset>
      </citation>
      <citation>
        <id>45</id>
        <reference_id>33</reference_id>
        <string>Rosti et al., 2007</string>
        <sentence_id>54624</sentence_id>
        <char_offset>324</char_offset>
      </citation>
      <citation>
        <id>46</id>
        <reference_id>34</reference_id>
        <string>Snover et al., 2009</string>
        <sentence_id>54623</sentence_id>
        <char_offset>164</char_offset>
      </citation>
      <citation>
        <id>47</id>
        <reference_id>35</reference_id>
        <string>Xiang et al., 2013</string>
        <sentence_id>54669</sentence_id>
        <char_offset>152</char_offset>
      </citation>
      <citation>
        <id>48</id>
        <reference_id>36</reference_id>
        <string>Xiao et al., 2013</string>
        <sentence_id>54595</sentence_id>
        <char_offset>250</char_offset>
      </citation>
      <citation>
        <id>49</id>
        <reference_id>36</reference_id>
        <string>Xiao et al. (2013)</string>
        <sentence_id>54627</sentence_id>
        <char_offset>22</char_offset>
      </citation>
      <citation>
        <id>50</id>
        <reference_id>36</reference_id>
        <string>Xiao et al. (2013)</string>
        <sentence_id>54731</sentence_id>
        <char_offset>0</char_offset>
      </citation>
    </citations>
  </content>
</document>
