<PAPER>
  <FILENO/>
  <TITLE>Leveraging Multiple MT Engines for Paraphrase Generation</TITLE>
  <AUTHORS/>
  <ABSTRACT>
    <A-S ID="S-1831">This paper proposes a method that leverages multiple machine translation (MT) engines for paraphrase generation (PG).</A-S>
    <A-S ID="S-1832">The method includes two stages.</A-S>
    <A-S ID="S-1833">Firstly, we use a multi-pivot approach to acquire a set of candidate paraphrases for a source sentence S.</A-S>
    <A-S ID="S-1834">Then, we employ two kinds of techniques, namely the selection-based technique and the decoding-based technique, to produce a best paraphrase T for S using the candidates acquired in the first stage.</A-S>
    <A-S ID="S-1835">Experimental results show that: (1) The multi-pivot approach is effective for obtaining plenty of valuable candidate paraphrases.</A-S>
    <A-S ID="S-1836">(2) Both the selectionbased and decoding-based techniques can make good use of the candidates and produce high-quality paraphrases.</A-S>
    <A-S ID="S-1837">Moreover, these two techniques are complementary.</A-S>
    <A-S ID="S-1838">(3) The proposed method outperforms a state-of-the-art paraphrase generation approach.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-1839">This paper addresses the problem of paraphrase generation (PG), which seeks to generate paraphrases for sentences.</S>
        <S ID="S-1840">PG is important in many natural language processing (NLP) applications.</S>
        <S ID="S-1841">For example, in machine translation (MT), a sentence can be paraphrased so as to make it more translatable (<REF ID="R-19" RPTR="18">Zhang and Yamamoto, 2002</REF>; <REF ID="R-05" RPTR="5">Callison-Burch et al., 2006</REF>).</S>
        <S ID="S-1842">In question answering (QA), a question can be paraphrased to improve the coverage of answer extraction (<REF ID="R-06" RPTR="6">Duboue and Chu-Carroll, 2006</REF>; Riezler et al., 2007).</S>
        <S ID="S-1843">In natural language generation (NLG), paraphrasing can help to increase the expressive power of the NLG systems (<REF ID="R-08" RPTR="8">Iordanskaja et al., 1991</REF>).</S>
      </P>
      <P>
        <S ID="S-1844">In this paper, we propose a novel PG method.</S>
        <S ID="S-1845">For an English sentence S, the method first acquires a set of candidate paraphrases with a multipivot approach, which uses MT engines to automatically translate S into multiple pivot languages and then translate them back into English.</S>
        <S ID="S-1846">Furthermore, the method employs two kinds of techniques to produce a best paraphrase T for S using the candidates, i.e., the selection-based and decoding-based techniques.</S>
        <S ID="S-1847">The former selects a best paraphrase from the candidates based on Minimum Bayes Risk (MBR), while the latter trains a MT model using the candidates and generates paraphrases with a MT decoder.</S>
      </P>
      <P>
        <S ID="S-1848">We evaluate our method on a set of 1182 English sentences.</S>
        <S ID="S-1849">The results show that: (1) although the candidate paraphrases acquired by MT engines are noisy, they provide good raw materials for further paraphrase generation; (2) the selection-based technique is effective, which results in the best performance; (3) the decodingbased technique is promising, which can generate paraphrases that are different from the candidates; (4) both the selection-based and decoding-based techniques outperform a state-of-the-art approach SPG (<REF ID="R-20" RPTR="19">Zhao et al., 2009</REF>).</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Related Work</HEADER>
      <P>
        <S ID="S-1870"></S>
      </P>
      <DIV DEPTH="1">
        <HEADER>2.1 Methods for Paraphrase Generation</HEADER>
        <P>
          <S ID="S-1850">MT-based method is the mainstream method on PG.</S>
          <S ID="S-1851">It regards PG as a monolingual machine translation problem, i.e., &#8220;translating&#8221; a sentence S into another sentence T in the same language.</S>
        </P>
        <P>
          <S ID="S-1852"><REF ID="R-17" RPTR="16">Quirk et al. (2004)</REF> first presented the MT-based method.</S>
          <S ID="S-1853">They trained a statistical MT (SMT) model on a monolingual parallel corpus extracted from comparable news articles and applied the model to generate paraphrases.</S>
          <S ID="S-1854">Their work shows that SMT techniques can be extended to PG.</S>
          <S ID="S-1855">However, its usefulness is limited by the scarcity of monolingual parallel data.</S>
        </P>
        <P>
          <S ID="S-1856">To overcome the data sparseness problem, <REF ID="R-22" RPTR="26">Zhao et al. (2008</REF>a) improved the MT-based PG method by training the paraphrase model using multiple resources, including monolingual parallel corpora, monolingual comparable corpora, bilingual parallel corpora, etc. Their results show that bilingual parallel corpora are the most useful among the exploited resources.</S>
          <S ID="S-1857">Zhao et al. <REF ID="R-21" RPTR="24">(2009)</REF> further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications.</S>
        </P>
        <P>
          <S ID="S-1858">The main disadvantage of the MT-based method is that its performance heavily depends on the fine-grained paraphrases, such as paraphrase phrases and patterns, which provide paraphrase options in decoding.</S>
          <S ID="S-1859">Hence one has to first extract fine-grained paraphrases from various corpora with different methods (<REF ID="R-22" RPTR="28">Zhao et al., 2008</REF>a; <REF ID="R-20" RPTR="20">Zhao et al., 2009</REF>), which is difficult and timeconsuming.</S>
        </P>
        <P>
          <S ID="S-1860">In addition to the MT-based method, researchers have also investigated other methods for paraphrase generation, such as the pattern-based methods (<REF ID="R-01" RPTR="1">Barzilay and Lee, 2003</REF>; <REF ID="R-14" RPTR="13">Pang et al., 2003</REF>), thesaurus-based methods (<REF ID="R-03" RPTR="3">Bolshakov and Gelbukh, 2004</REF>; <REF ID="R-09" RPTR="9">Kauchak and Barzilay, 2006</REF>), and NLG-based methods (Kozlowski et al., 2003; <REF ID="R-16" RPTR="15">Power and Scott, 2005</REF>).</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.2 Pivot Approach for Paraphrasing</HEADER>
        <P>
          <S ID="S-1861"><REF ID="R-00" RPTR="0">Bannard and Callison-Burch (2005)</REF> introduced the pivot approach to extracting paraphrase phrases from bilingual parallel corpora.</S>
          <S ID="S-1862">Their basic assumption is that two English phrases aligned with the same phrase in a foreign language (also called a pivot language) are potential paraphrases.</S>
          <S ID="S-1863"><REF ID="R-22" RPTR="27">Zhao et al. (2008</REF>b) extended the approach and used it to extract paraphrase patterns.</S>
          <S ID="S-1864">Both of the above works have proved the effectiveness of the pivot approach in paraphrase extraction.</S>
        </P>
        <P>
          <S ID="S-1865">Pivot approach can also be used in paraphrase generation.</S>
          <S ID="S-1866">It generates paraphrases by translating sentences from a source language to one (singlepivot) or more (multi-pivot) pivot languages and then translating them back to the source language.</S>
          <S ID="S-1867">Duboue et al. (2006) first proposed the multipivot approach for paraphrase generation, which was specially designed for question expansion in QA.</S>
          <S ID="S-1868">In addition, <REF ID="R-12" RPTR="11">Max (2009)</REF><REF ID="R-21" RPTR="25">(2009)</REF> presented a singlepivot approach for generating sub-sentential paraphrases.</S>
          <S ID="S-1869">A clear difference between our method and the above works is that we propose selectionbased and decoding-based techniques to generate high-quality paraphrases using the candidates yielded from the pivot approach.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Multi-pivot Approach for Acquiring Candidate Paraphrases</HEADER>
      <P>
        <S ID="S-1871">A single-pivot PG approach paraphrases a sentence S by translating it into a pivot language P L with a MT engine MT 1 and then translating it back into the source language with MT 2 .</S>
        <S ID="S-1872">In this paper, a single-pivot PG system is represented as a triple (MT 1 , P L, MT 2 ).</S>
        <S ID="S-1873">A multipivot PG system is made up of a set of single-pivot systems with various pivot languages and MT engines.</S>
        <S ID="S-1874">Given m pivot languages and n MT engines, we can build a multi-pivot PG system consisting of N (N &#8804; n &#8727; m &#8727; n) single-pivot ones, where N = n &#8727; m &#8727; n iff all the n MT engines can perform bidirectional translation between the source and each pivot language.</S>
      </P>
      <P>
        <S ID="S-1875">In this work, we experiment with 6 pivot languages (Table 1) and 3 MT engines (Table 2) in the multi-pivot approach.</S>
        <S ID="S-1876">All the 3 MT engines are off-the-shelf systems, in which Google and Microsoft translators are SMT engines, while Systran translator is a rule-based MT engine.</S>
        <S ID="S-1877">Each MT engine can translate English to all the 6 pivot languages and back to English.</S>
        <S ID="S-1878">We thereby construct a multi-pivot PG system consisting of 54 (3*6*3) single-pivot systems.</S>
      </P>
      <P>
        <S ID="S-1879">The advantages of the multi-pivot PG approach lie in two aspects.</S>
        <S ID="S-1880">First, it effectively makes use of the vast bilingual data and translation rules underlying the MT engines.</S>
        <S ID="S-1881">Second, the approach is simple, which just sends sentences to the online MT engines and gets the translations back.</S>
      </P>
      <P>
        <S ID="S-1882">1 Google Translate (GG)</S>
      </P>
      <P>
        <S ID="S-1883">(translate.google.com) 2 Microsoft Translator (MS)</S>
      </P>
      <P>
        <S ID="S-1884">(www.microsofttranslator.com) 3 Systran Online Translation (ST)</S>
      </P>
      <P>
        <S ID="S-1885">(www.systransoft.com)</S>
      </P>
      <P>
        <S ID="S-1886">4 Producing High-quality Paraphrases using the Candidates</S>
      </P>
      <P>
        <S ID="S-1887">Table 3 shows some examples of candidate paraphrases for a sentence.</S>
        <S ID="S-1888">As can be seen, the candidates do provide some correct and useful paraphrase substitutes (in bold) for the source sentence.</S>
        <S ID="S-1889">However, they also contain quite a few errors (in italic) due to the limited translation quality of the MT engines.</S>
        <S ID="S-1890">The problem is even worse when the source sentences get longer and more complicated.</S>
        <S ID="S-1891">Therefore, we need to combine the outputs of the multiple single-pivot PG systems and produce high-quality paraphrases out of them.</S>
        <S ID="S-1892">To this end, we investigate two techniques, namely, the selection-based and decodingbased techniques.</S>
      </P>
      <P>
        <S ID="S-1893">4.1 Selection-based Technique</S>
      </P>
      <P>
        <S ID="S-1894">Given a source sentence S along with a set D of candidate paraphrases {T 1 , T 2 , ..., T i , ...T N }, the goal of the selection-based technique is to select the best paraphrase &#710;T i for S from D.</S>
        <S ID="S-1895">The paraphrase selection technique we propose is based on Minimum Bayes Risk (MBR).</S>
        <S ID="S-1896">In detail, the MBR based technique first measures the quality of each candidate paraphrase T i &#8712; D in terms of Bayes risk (BR), and then selects the one with the minimum BR as the best paraphrase.</S>
        <S ID="S-1897">In detail, given S, a candidate T i &#8712; D, a reference paraphrase T 1 , and a loss function L(T, T i ) that measures the quality of T i relative to T , we define the Bayes risk as follows:</S>
      </P>
      <P>
        <S ID="S-1898">BR(T i ) = E P (T,S) [L(T, T i )], (1)</S>
      </P>
      <P>
        <S ID="S-1899">where the expectation is taken under the true distribution P (T, S) of the paraphrases.</S>
        <S ID="S-1900">According to (<REF ID="R-02" RPTR="2">Bickel and Doksum, 1977</REF>), the candidate paraphrase that minimizes the Bayes risk can be found as follows:</S>
      </P>
      <P>
        <S ID="S-1901">&#710;T i = arg min</S>
      </P>
      <P>
        <S ID="S-1902">T i &#8712;D T &#8712;T</S>
      </P>
      <P>
        <S ID="S-1903">&#8721; L(T, T i )P (T |S), (2)</S>
      </P>
      <P>
        <S ID="S-1904">where T represents the space of reference paraphrases.</S>
        <S ID="S-1905">In practice, however, the collection of reference paraphrases is not available.</S>
        <S ID="S-1906">We thus construct a set D &#8242; = D &#8746; {S} to approximate T 2 .</S>
        <S ID="S-1907">In addition, we cannot estimate P (T |S) in Equation (2), either.</S>
        <S ID="S-1908">Therefore, we make a simplification by assigning a constant c to P (T |S) for each T &#8712; D &#8242; , which can then be removed:</S>
      </P>
      <P>
        <S ID="S-1909">&#710;T i = arg min</S>
      </P>
      <P>
        <S ID="S-1910">T i &#8712;D</S>
      </P>
      <P>
        <S ID="S-1911">&#8721;</S>
      </P>
      <P>
        <S ID="S-1912">T &#8712;D &#8242; L(T, T i ).</S>
        <S ID="S-1913">(3)</S>
      </P>
      <P>
        <S ID="S-1914">Equation (3) can be further rewritten using a gain function G(T, T i ) instead of the loss function:</S>
      </P>
      <P>
        <S ID="S-1915">1 Here we assume that we have the collection of all possible paraphrases of S, which are used as references.</S>
        <S ID="S-1916">2 The source sentence S is included in D &#8242; based on the</S>
      </P>
      <P>
        <S ID="S-1917">consideration that a sentence is allowed to keep unchanged during paraphrasing.</S>
      </P>
      <P>
        <S ID="S-1918">&#8721; &#710;T i = arg max G(T, T i ).</S>
        <S ID="S-1919">(4)</S>
      </P>
      <P>
        <S ID="S-1920">T i &#8712;D T &#8712;D &#8242;</S>
      </P>
      <P>
        <S ID="S-1921">We define the gain function based on BLEU: G(T, T i ) = BLEU(T, T i ).</S>
        <S ID="S-1922">BLEU is a widely used metric in the automatic evaluation of MT (<REF ID="R-15" RPTR="14">Papineni et al., 2002</REF>).</S>
        <S ID="S-1923">It measures the similarity of two sentences by counting the overlapping n-grams (n=1,2,3,4 in our experiments):</S>
      </P>
      <P>
        <S ID="S-1924">BLEU(T, T i ) = BP &#183;exp( 4&#8721;</S>
      </P>
      <P>
        <S ID="S-1925">w n log p n (T, T i )),</S>
      </P>
      <P>
        <S ID="S-1926">n=1</S>
      </P>
      <P>
        <S ID="S-1927">where p n (T, T i ) is the n-gram precision of T i and w n = 1/4.</S>
        <S ID="S-1928">BP (&#8804; 1) is a brevity penalty that penalizes T i if it is shorter than T .</S>
        <S ID="S-1929">In summary, for each sentence S, the MBR based technique selects a paraphrase that is the most similar to all candidates and the source sentence.</S>
        <S ID="S-1930">The underlying assumption is that correct paraphrase substitutes should be common among the candidates, while errors committed by the single-pivot PG systems should be all different.</S>
        <S ID="S-1931">We denote this approach as S-1 hereafter.</S>
      </P>
      <P>
        <S ID="S-1932">Approaches for comparison.</S>
        <S ID="S-1933">In the experiments, we also design another two paraphrase selection approaches S-2 and S-3 for comparison with S-1.</S>
      </P>
      <P>
        <S ID="S-1934">S-2: S-2 selects the best single-pivot PG system from all the 54 ones.</S>
        <S ID="S-1935">The selection is also based on MBR and BLEU.</S>
        <S ID="S-1936">For each single-pivot PG system, we sum up its gain function</S>
      </P>
      <P>
        <S ID="S-1937">&#8721; values &#8721;</S>
      </P>
      <P>
        <S ID="S-1938">over a set of source sentences (i.e.,</S>
      </P>
      <P>
        <S ID="S-1939">S T S &#8712;D S &#8242; G(T S , T Si )).</S>
        <S ID="S-1940">Then we select the one with the maximum gain value as the best single-pivot system.</S>
        <S ID="S-1941">In our experiments, the selected best single-pivot PG system is (ST, P, GG), the candidate paraphrases acquired by which are then returned as the best paraphrases in S-2.</S>
      </P>
      <P>
        <S ID="S-1942">S-3: S-3 is a simple baseline, which just randomly selects a paraphrase from the 54 candidates for each source sentence S.</S>
      </P>
      <P>
        <S ID="S-1943">4.2 Decoding-based Technique</S>
      </P>
      <P>
        <S ID="S-1944">The selection-based technique introduced above has an inherent limitation that it can only select a paraphrase from the candidates.</S>
        <S ID="S-1945">That is to say, it major cuts significant cuts major cuts* important cuts big cuts great cuts</S>
      </P>
      <P>
        <S ID="S-1946">high-level civil servants senior officials high-level officials senior civil servants</S>
      </P>
      <P>
        <S ID="S-1947">can never produce a perfect paraphrase if all the candidates have some tiny flaws.</S>
        <S ID="S-1948">To solve this problem, we propose the decoding-based technique, which trains a MT model using the candidate paraphrases of each source sentence S and generates a new paraphrase T for S with a MT decoder.</S>
      </P>
      <P>
        <S ID="S-1949">In this work, we implement the decoding-based technique using Giza++ (<REF ID="R-13" RPTR="12">Och and Ney, 2000</REF>) and Moses (<REF ID="R-07" RPTR="7">Hoang and Koehn, 2008</REF>), both of which are commonly used SMT tools.</S>
        <S ID="S-1950">For a sentence S, we first construct a set of parallel sentences by pairing S with each of its candidate paraphrases: {(S,T 1 ),(S,T 2 ),...,(S,T N )} (N = 54).</S>
        <S ID="S-1951">We then run word alignment on the set using Giza++ and extract aligned phrase pairs as described in (<REF ID="R-10" RPTR="10">Koehn, 2004</REF>).</S>
        <S ID="S-1952">Here we only keep the phrase pairs that are aligned &#8805;3 times on the set, so as to filter errors brought by the noisy sentence pairs.</S>
        <S ID="S-1953">The extracted phrase pairs are stored in a phrase table.</S>
        <S ID="S-1954">Table 4 shows some extracted phrase pairs.</S>
      </P>
      <P>
        <S ID="S-1955">Note that Giza++ is sensitive to the data size.</S>
        <S ID="S-1956">Hence it is interesting to examine if the alignment can be improved by augmenting the parallel sentence pairs.</S>
        <S ID="S-1957">To this end, we have tried augmenting the parallel set for each sentence S by pairing any two candidate paraphrases.</S>
        <S ID="S-1958">In this manner, C 2 N sentence pairs are augmented for each S.</S>
        <S ID="S-1959">We conduct word alignment using the (N +C 2 N ) sentence pairs and extract aligned phrases from the original N pairs.</S>
        <S ID="S-1960">However, we have not found clear improvement after observing the results.</S>
        <S ID="S-1961">Therefore, we do not adopt the augmentation strategy in our experiments.</S>
      </P>
      <P>
        <S ID="S-1962">Using the extracted phrasal paraphrases, we conduct decoding for the sentence S with Moses, which is based on a log-linear model.</S>
        <S ID="S-1963">The default setting of Moses is used, except that the distortion model for phrase reordering is turned off 3 .</S>
        <S ID="S-1964">The language model in Moses is trained using a 9 GB English corpus.</S>
        <S ID="S-1965">We denote the above approach as D-1 in what follows.</S>
      </P>
      <P>
        <S ID="S-1966">Approach for comparison.</S>
        <S ID="S-1967">The main advantage of the decoding-based technique is that it allows us to customize the paraphrases for different requirements through tailoring the phrase table or tuning the model parameters.</S>
        <S ID="S-1968">As a case study, this paper shows how to generate paraphrases with varied paraphrase rates 4 .</S>
      </P>
      <P>
        <S ID="S-1969">D-2: The extracted phrasal paraphrases (including self-paraphrases) are stored in a phrase table, in which each phrase pair has 4 scores measuring their alignment confidence (Koehn et al., 2003).</S>
        <S ID="S-1970">Our basic idea is to control the paraphrase rate by tuning the scores of the self-paraphrases.</S>
        <S ID="S-1971">We thus extend D-1 to D-2, which assigns a weight &#955; (&#955; &gt; 0) to the scores of the selfparaphrase pairs.</S>
        <S ID="S-1972">Obviously, if we set &#955; &lt; 1, the self-paraphrases will be penalized and the decoder will prefer to generate a paraphrase with more changes.</S>
        <S ID="S-1973">If we set &#955; &gt; 1, the decoder will tend to generate a paraphrase that is more similar to the source sentence.</S>
        <S ID="S-1974">In our experiments, we set &#955; = 0.1 in D-2.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Experimental Setup</HEADER>
      <P>
        <S ID="S-1975">Our test sentences are extracted from the parallel reference translations of a Chinese-to-English MT evaluation 5 , in which each Chinese sentence c has 4 English reference translations, namely e 1 , e 2 , e 3 , and e 4 .</S>
        <S ID="S-1976">We use e 1 as a test sentence to paraphrase and e 2 , e 3 , e 4 as human paraphrases of e 1 for comparison with the automatically generated paraphrases.</S>
        <S ID="S-1977">We process the test set by manually filtering ill-formed sentences, such as the ungrammatical or incomplete ones.</S>
        <S ID="S-1978">1182 out of 1357</S>
      </P>
      <P>
        <S ID="S-1979">3 We conduct monotone decoding as previous work</S>
      </P>
      <P>
        <S ID="S-1980">(<REF ID="R-17" RPTR="17">Quirk et al., 2004</REF>; <REF ID="R-22" RPTR="29">Zhao et al., 2008</REF>a, <REF ID="R-20" RPTR="21">Zhao et al., 2009</REF>).</S>
        <S ID="S-1981">4 The paraphrase rate reflects how different a paraphrase</S>
      </P>
      <P>
        <S ID="S-1982">is from the source sentence.</S>
        <S ID="S-1983">5 2008 NIST Open Machine Translation Evaluation: Chinese to English Task.</S>
      </P>
      <P>
        <S ID="S-1984">test sentences are retained after filtering.</S>
        <S ID="S-1985">Statistics show that about half of the test sentences are from news and the other half are from essays.</S>
        <S ID="S-1986">The average length of the test sentences is 34.12 (words).</S>
      </P>
      <P>
        <S ID="S-1987">Manual evaluation is used in this work.</S>
        <S ID="S-1988">A paraphrase T of a sentence S is manually scored based on a five point scale, which measures both the &#8220;adequacy&#8221; (i.e., how much of the meaning of S is preserved in T ) and &#8220;fluency&#8221; of T (See Table 5).</S>
        <S ID="S-1989">The five point scale used here is similar to that in the human evaluation of MT (<REF ID="R-04" RPTR="4">Callison-Burch et al., 2007</REF>).</S>
        <S ID="S-1990">In MT, adequacy and fluency are evaluated separately.</S>
        <S ID="S-1991">However, we find that there is a high correlation between the two aspects, which makes it difficult to separate them.</S>
        <S ID="S-1992">Thus we combine them in this paper.</S>
      </P>
      <P>
        <S ID="S-1993">We compare our method with a state-of-theart approach SPG 6 (<REF ID="R-20" RPTR="22">Zhao et al., 2009</REF>), which is a statistical approach specially designed for PG.</S>
        <S ID="S-1994">The approach first collects a large volume of fine-grained paraphrase resources, including paraphrase phrases, patterns, and collocations, from various corpora using different methods.</S>
        <S ID="S-1995">Then it generates paraphrases using these resources with a statistical model 7 .</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>6 Experimental Results</HEADER>
      <P>
        <S ID="S-2049">We evaluate six approaches, i.e., S-1, S-2, S-3, D- 1, D-2 and SPG, in the experiments.</S>
        <S ID="S-2050">Each approach generates a 1-best paraphrase for a test sentence S.</S>
        <S ID="S-2051">We randomize the order of the 6 paraphrases of each S to avoid bias of the raters.</S>
      </P>
      <P>
        <S ID="S-2052">6 SPG: Statistical Paraphrase Generation.</S>
        <S ID="S-2053">7 We ran SPG under the setting of baseline-2 as described</S>
      </P>
      <P>
        <S ID="S-2054">in (<REF ID="R-20" RPTR="23">Zhao et al., 2009</REF>).</S>
      </P>
      <P>
        <S ID="S-2055">4.5</S>
      </P>
      <P>
        <S ID="S-2056">3.5</S>
      </P>
      <P>
        <S ID="S-2057">2.5</S>
      </P>
      <P>
        <S ID="S-2058">1.5</S>
      </P>
      <P>
        <S ID="S-2059">0.5</S>
      </P>
      <P>
        <S ID="S-2060">0</S>
      </P>
      <P>
        <S ID="S-2061">S-1 S-2 S-3 D-1 D-2 SPG</S>
      </P>
      <P>
        <S ID="S-2062">score 3.92 3.52 2.78 3.62 3.36 3.47</S>
      </P>
      <P>
        <S ID="S-2063">4.5</S>
      </P>
      <P>
        <S ID="S-2064">3.5</S>
      </P>
      <P>
        <S ID="S-2065">2.5</S>
      </P>
      <P>
        <S ID="S-2066">1.5</S>
      </P>
      <P>
        <S ID="S-2067">0.5</S>
      </P>
      <P>
        <S ID="S-2068">0</S>
      </P>
      <P>
        <S ID="S-2069">S-1 S-2 S-3 D-1 D-2 SPG</S>
      </P>
      <P>
        <S ID="S-2070">r1 r2 r3 r4 r5 r6</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>6.1 Human Evaluation Results</HEADER>
        <P>
          <S ID="S-1996">We have 6 raters in the evaluation, all of whom are postgraduate students.</S>
          <S ID="S-1997">In particular, 3 raters major in English, while the other 3 major in computer science.</S>
          <S ID="S-1998">Each rater scores the paraphrases of 1/6 test sentences, whose results are then combined to form the final scoring result.</S>
          <S ID="S-1999">The average scores of the six approaches are shown in Figure 1.</S>
          <S ID="S-2000">We can find that among the selectionbased approaches, the performance of S-3 is the worst, which indicates that randomly selecting a paraphrase from the candidates works badly.</S>
          <S ID="S-2001">S- 2 performs much better than S-3, suggesting that the quality of the paraphrases acquired with the best single-pivot PG system are much higher than the randomly selected ones.</S>
          <S ID="S-2002">S-1 performs the best in all the six approaches, which demonstrates the effectiveness of the MBR-based selection technique.</S>
          <S ID="S-2003">Additionally, the fact that S-1 evidently outperforms S-2 suggests that it is necessary to extend a single-pivot approach to a multi-pivot one.</S>
        </P>
        <P>
          <S ID="S-2004">To get a deeper insight of S-1, we randomly sample 100 test sentences and manually score all of their candidates.</S>
          <S ID="S-2005">We find that S-1 successfully picks out a paraphrase with the highest score for 72 test sentences.</S>
          <S ID="S-2006">We further analyze the remaining 28 sentences for which S-1 fails and find that the failures are mainly due to the BLEU-based gain function.</S>
          <S ID="S-2007">For example, S-1 sometimes selects paraphrases that have correct phrases but incorrect phrase orders, since BLEU is weak in evaluating phrase orders and sentence structures.</S>
          <S ID="S-2008">In the next step we shall improve the gain function by investigating other features besides BLEU.</S>
          <S ID="S-2009">In the decoding-based approaches, D-1 ranks the second in the six approaches only behind S-1.</S>
          <S ID="S-2010">Figure 2: Evaluation results from each rater.</S>
        </P>
        <P>
          <S ID="S-2011">We will further improve D-1 in the future rather than simply use Moses in decoding with the default setting.</S>
          <S ID="S-2012">However, the value of D-1 lies in that it enables us to break down the candidates and generate new paraphrases flexibly.</S>
          <S ID="S-2013">The performance decreases when we extend D-1 to D-2 to achieve a larger paraphrase rate.</S>
          <S ID="S-2014">This is mainly because more errors are brought in when more parts of a sentence are paraphrased.</S>
        </P>
        <P>
          <S ID="S-2015">We can also find from Figure 1 that S-1, S-2, and D-1 all get higher scores than SPG, which shows that our method outperforms this state-ofthe-art approach.</S>
          <S ID="S-2016">This is more important if we consider that our method is lightweight, which makes no effort to collect fine-grained paraphrase resources beforehand.</S>
          <S ID="S-2017">After observing the results, we believe that the outperformance of our method can be mainly ascribed to the selection-based and decoding-based techniques, since we avoid many errors by voting among the candidates.</S>
          <S ID="S-2018">For instance, an ambiguous phrase may be incorrectly paraphrased by some of the single-pivot PG systems or the SPG approach.</S>
          <S ID="S-2019">However, our method may obtain the correct paraphrase through statistics over all candidates and selecting the most credible one.</S>
        </P>
        <P>
          <S ID="S-2020">The human evaluation of paraphrases is subjective.</S>
          <S ID="S-2021">Hence it is necessary to examine the coherence among the raters.</S>
          <S ID="S-2022">The scoring results from the six raters are depicted in Figure 2.</S>
          <S ID="S-2023">As it can be seen, they show similar trends though the raters have different degrees of strictness.</S>
        </P>
        <P>
          <S ID="S-2024">0.8</S>
        </P>
        <P>
          <S ID="S-2025">0.7</S>
        </P>
        <P>
          <S ID="S-2026">0.6</S>
        </P>
        <P>
          <S ID="S-2027">0.5</S>
        </P>
        <P>
          <S ID="S-2028">0.4</S>
        </P>
        <P>
          <S ID="S-2029">0.3</S>
        </P>
        <P>
          <S ID="S-2030">0.2</S>
        </P>
        <P>
          <S ID="S-2031">0.1</S>
        </P>
        <P>
          <S ID="S-2032">0</S>
        </P>
        <P>
          <S ID="S-2033">S-1 S-2 S-3 D-1 D-2 SPG HP1 HP2 HP3</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>6.2 Paraphrase Rate</HEADER>
        <P>
          <S ID="S-2034">Human evaluation assesses the quality of paraphrases.</S>
          <S ID="S-2035">However, the paraphrase rates cannot be reflected.</S>
          <S ID="S-2036">A paraphrase that is totally transformed from the source sentence and another that is almost unchanged may get the same score.</S>
          <S ID="S-2037">Therefore, we propose two strategies, i.e., PR1 and PR2, to compute the paraphrase rate:</S>
        </P>
        <P>
          <S ID="S-2038">P R1(T ) = 1 &#8722; OL(S, T ) L(S) ; P R2(T ) = ED(S, T ) .</S>
          <S ID="S-2039">L(S)</S>
        </P>
        <P>
          <S ID="S-2040">Here, PR1 is defined based on word overlapping rate, in which OL(S, T ) denotes the number of overlapping words between a paraphrase T and its source sentence S, L(S) denotes the number of words in S. PR2 is defined based on edit distance, in which ED(S, T ) denotes the edit distance between T and S.</S>
          <S ID="S-2041">Obviously, PR1 only measures the percentage of words that are changed from S to T , whereas PR2 further takes word order changes into consideration.</S>
          <S ID="S-2042">It should be noted that PR1 and PR2 not only count the correct changes between S and T , but also count the incorrect ones.</S>
          <S ID="S-2043">We compute the paraphrase rate for each of the six approaches by averaging the paraphrase rates over the whole test set.</S>
          <S ID="S-2044">The results are shown in the left part of Figure 3.</S>
        </P>
        <P>
          <S ID="S-2045">On the whole, the paraphrase rates of the approaches are not high.</S>
          <S ID="S-2046">In particular, we can see that the paraphrase rate of D-2 is clearly higher than D-1, which is in line with our intention of designing D-2.</S>
          <S ID="S-2047">We can also see that the paraphrase rate of S-3 is the highest among the approaches.</S>
          <S ID="S-2048">We find it is mainly because the paraphrases generated with S-3 contain quite a lot of errors, which contribute most of the changes.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>7 Analysis</HEADER>
      <P>
        <S ID="S-2099"></S>
      </P>
      <DIV DEPTH="1">
        <HEADER>7.1 Effectiveness of the Proposed Method</HEADER>
        <P>
          <S ID="S-2071">Our analysis starts from the candidate paraphrases acquired with the multi-pivot approach.</S>
          <S ID="S-2072">Actually, the results of S-3 reflect the average quality of the candidate paraphrases.</S>
          <S ID="S-2073">A score of 2.78 (See Figure 1) indicates that the candidates are unacceptable according to the human evaluation metrics.</S>
          <S ID="S-2074">This is in line with our expectation that the automatically acquired paraphrases through a two-way translation are noisy.</S>
          <S ID="S-2075">However, the results of S-1 and D-1 demonstrate that, using the selection-based and decoding-based techniques, we can produce paraphrases of good quality.</S>
          <S ID="S-2076">Especially, S-1 gets a score of nearly 4, which suggests that the paraphrases are pretty good according to our metrics.</S>
          <S ID="S-2077">Moreover, our method outperforms SPG built on pre-extracted fine-grained paraphrases.</S>
          <S ID="S-2078">It shows that our method makes good use of the paraphrase knowledge from the large volume of bilingual data underlying the multiple MT engines.</S>
        </P>
        <P>
          <S ID="S-2079">7.2 How to Choose Pivot Languages and MT Engines in the Multi-pivot Approach</S>
        </P>
        <P>
          <S ID="S-2080">In our experiments, besides the six pivot languages used in the multi-pivot system, we have also tried another five pivot languages, including Arabic, Japanese, Korean, Russian, and Dutch.</S>
          <S ID="S-2081">They are finally abandoned since we find that they perform badly.</S>
          <S ID="S-2082">Our experience on choosing pivot languages is that: (1) a pivot language should be a language whose translation quality can be well guaranteed by the MT engines; (2) it is better to choose a pivot language similar to the source language (e.g., French - English), which is easier to translate; (3) the translation quality of a pivot language should not vary a lot among the MT engines.</S>
          <S ID="S-2083">On the other hand, it is better to choose MT engines built on diverse models and corpora, which can provide different paraphrase options.</S>
          <S ID="S-2084">We plan to employ a syntax-based MT engine in our further experiments besides the currently used phrase-based SMT and rule-based MT engines.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>7.3 Comparing the Selection-based and Decoding-based Techniques</HEADER>
        <P>
          <S ID="S-2085">It is necessary to compare the paraphrases generated via the selection-based and decoding-based techniques.</S>
          <S ID="S-2086">As stated above, the selection-based technique can only select a paraphrase from the candidates, while the decoding-based technique can generate a paraphrase different from all candidates.</S>
          <S ID="S-2087">In our experiments, we find that for about 90% test sentences, the paraphrases generated by the decoding-based approach D-1 are outside the candidates.</S>
          <S ID="S-2088">In particular, we compare the paraphrases generated by S-1 and D-1 and find that, for about 40% test sentences, S-1 gets higher scores than D-1, while for another 21% test sentences, D-1 gets higher scores than S-1 8 .</S>
          <S ID="S-2089">This indicates that the selection-based and decodingbased techniques are complementary.</S>
          <S ID="S-2090">In addition, we find examples in which the decoding-based technique can generate a perfect paraphrase for the source sentence, even if all the candidate paraphrases have obvious errors.</S>
          <S ID="S-2091">This also shows that the decoding-based technique is promising.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>7.4 Comparing Automatically Generated Paraphrases with Human Paraphrases</HEADER>
        <P>
          <S ID="S-2092">We also analyze the characteristics of the generated paraphrases and compare them with the human paraphrases (i.e., the other 3 reference translations in the MT evaluation, see Section 5, which are denoted as HP1, HP2, and HP3).</S>
          <S ID="S-2093">We find that, compared with the automatically generated paraphrases, the human paraphrases are more com-</S>
        </P>
        <P>
          <S ID="S-2094">8 For the rest 39%, S-1 and D-1 get identical scores.</S>
        </P>
        <P>
          <S ID="S-2095">plicated, which involve not only phrase replacements, but also structure reformulations and even inferences.</S>
          <S ID="S-2096">Their paraphrase rates are also much higher, which can be seen in the right part of Figure 3.</S>
          <S ID="S-2097">We show the automatic and human paraphrases for the example sentence of this paper in Table 6.</S>
          <S ID="S-2098">To narrow the gap between the automatic and human paraphrases, it is necessary to learn structural paraphrase knowledge from the candidates in the future work.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>8 Conclusions and Future Work</HEADER>
      <P>
        <S ID="S-2100">We put forward an effective method for paraphrase generation, which has the following contributions.</S>
        <S ID="S-2101">First, it acquires a rich fund of paraphrase knowledge through the use of multiple MT engines and pivot languages.</S>
        <S ID="S-2102">Second, it presents a MBR-based technique that effectively selects high-quality paraphrases from the noisy candidates.</S>
        <S ID="S-2103">Third, it proposes a decoding-based technique, which can generate paraphrases that are different from the candidates.</S>
        <S ID="S-2104">Experimental results show that the proposed method outperforms a state-of-the-art approach SPG.</S>
        <S ID="S-2105">In the future work, we plan to improve the selection-based and decoding-based techniques.</S>
        <S ID="S-2106">We will try some standard system combination strategies, like confusion networks and consensus decoding.</S>
        <S ID="S-2107">In addition, we will refine our evaluation metrics.</S>
        <S ID="S-2108">In the current experiments, paraphrase correctness (adequacy and fluency) and paraphrase rate are evaluated separately, which seem to be incompatible.</S>
        <S ID="S-2109">We plan to combine them together and propose a uniform metric.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS/>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>Colin Bannard</RAUTHOR>
      <REFTITLE>Paraphrasing with Bilingual Parallel Corpora.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>Regina Barzilay</RAUTHOR>
      <REFTITLE>Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>Peter J Bickel</RAUTHOR>
      <REFTITLE>Mathematical Statistics: Basic Ideas and Selected Topics.</REFTITLE>
      <DATE>1977</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>Igor A Bolshakov</RAUTHOR>
      <REFTITLE>Synonymous Paraphrasing Using WordNet and Internet.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>Chris Callison-Burch</RAUTHOR>
      <REFTITLE>Evaluation of Machine Translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>Chris Callison-Burch</RAUTHOR>
      <REFTITLE>Improved Statistical Machine Translation Using Paraphrases.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>Pablo Ariel Duboue</RAUTHOR>
      <REFTITLE>Answering the Question You Wish They Had Asked: The Impact of Paraphrasing for Question Answering.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>Hieu Hoang</RAUTHOR>
      <REFTITLE>Design of the Moses Decoder for Statistical Machine Translation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>Lidija Iordanskaja</RAUTHOR>
      <REFTITLE>Lexical Selection and Paraphrase in a Meaning-Text Generation Model. In C&#233;cile</REFTITLE>
      <DATE>1991</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>David Kauchak</RAUTHOR>
      <REFTITLE>Paraphrasing for Automatic Evaluation.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models: User Manual and Description for Version 1.2. Philipp Koehn,</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>Vijay-Shanker</RAUTHOR>
      <REFTITLE>Generation of singlesentence paraphrases from predicate/argument structure using lexico-grammatical resources.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>Aur&#233;lien Max</RAUTHOR>
      <REFTITLE>Sub-sentential Paraphrasing by Contextual Pivot Translation.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>Franz Josef Och</RAUTHOR>
      <REFTITLE>Improved Statistical Alignment Models.</REFTITLE>
      <DATE>2000</DATE>
    </REFERENCE>
    <REFERENCE ID="14">
      <RAUTHOR>Bo Pang</RAUTHOR>
      <REFTITLE>Syntax-based Alignment of Multiple Translations: Extracting Paraphrases and Generating New Sentences.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="15">
      <RAUTHOR>Kishore Papineni</RAUTHOR>
      <REFTITLE>BLEU: a Method for Automatic Evaluation of Machine Translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="16">
      <RAUTHOR>Richard Power</RAUTHOR>
      <REFTITLE>Automatic generation of large-scale paraphrases.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="17">
      <RAUTHOR>Chris Quirk</RAUTHOR>
      <REFTITLE>Monolingual Machine Translation for Paraphrase Generation.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="18">
      <RAUTHOR>Stefan Riezler</RAUTHOR>
      <REFTITLE>Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="19">
      <RAUTHOR>Yujie Zhang</RAUTHOR>
      <REFTITLE>Paraphrasing of Chinese Utterances.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="20">
      <RAUTHOR>Shiqi Zhao</RAUTHOR>
      <REFTITLE>Application-driven Statistical Paraphrase Generation.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="21">
      <RAUTHOR></RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="22">
      <RAUTHOR>Shiqi Zhao</RAUTHOR>
      <REFTITLE>Combining Multiple Resources to Improve SMT-based Paraphrasing Model.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
