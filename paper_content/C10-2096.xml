<document>
  <filename>C10-2096</filename>
  <authors/>
  <title>None</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>1 Introduction</title>
        <text>Statistical machine translation (SMT) has witnessed promising progress in recent years. Typically, conventional SMT is characterized as a 1- best pipeline system (Figure 1(a)), whose modules are independent of each other and only take as input 1-best results from the previous module. Though this assumption is convenient to reduce the complexity of SMT systems. It also bring a major drawback of error propagation. The errors of 1-best outputs, introduced inevitably in each phase, will propagate and accumulate along the pipeline. Not recoverable in the final decoding source
source
1-best segmentation
1-best tree
segmentation lattice
parse forest
target
target
(a)
(b)
step. These errors will severely hurt the translation quality. For example, if the accuracy of each module is 90%, the final accuracy will drop to 73% after three separate phases. To alleviate this problem, an obvious solution is to widen the pipeline with k-best lists rather than 1-best results. For example Venugopal et al. (2008) use k-best alignments and parses in the training phase. However, with limited scope and too many redundancies, it is inefficient to search separately on each of these similar lists (Huang, 2008).
Another efficient method is to use compact data structures instead of k-bestlists. A lattice or forest, compactly encoded exponentially many derivations, have proven to be a promising technique. For example, Mi and Huang (2008), Mi et al. (2008), Liu et al. (2009) and Zhang et al. (2009) use forests in rule extraction and decoding phases to extract more general rules and weaken the influence of parsing errors; Dyer et al. (2008) use word lattice in Chinese word segmentation and Arabic morphological variation phases to weaken the influence of segmentation errors; Huang (2008) and
Coling 2010: Poster Volume, pages 837&#8211;845, Beijing, August 2010
(0, 2, NR) (2,3,CC) (3,5,NR) (5, 6, VV) (6, 8, NN) (8,9,NN) 0 c 0 :B&#249; 1 c 1 :sh&#237; 2 c 2 :y&#468; 3 c 3 :Sh&#257; 4 c 4 :l&#243;ng 5 c 5 :j&#468; 6 c 6 :x&#237;ng 7 c 7 :t&#462;o 8 c 8 :l&#249;n 9 (2,3,P) (5,7,VV) (7,9,NN)
Jiang et al. (2008b) stress the problems in reranking phase. Both lattices and forests have become popular in machine translation literature.
However, to the best of our knowledge, previous work only focused on one module at a time. In this paper, we investigate the combination of lattice and forest (Section 2), as shown in Figure 1(b). We explore the algorithms of lattice parsing (Section 3.2), rule extraction (Section 4) and decoding (Section 5). More importantly, in the decoding step, our model can search among not only more parse-trees but also more segmentations encoded in the lattice-forests and can take into account all the probabilities of segmentations and parse-trees. In other words, our model postpones the disambiguition of segmentation and parsing into the final translation step, so that we can do global search for the best segmentation, parse-tree and translation in one step. When we integrate a lattice into a forest system, medium-scale experiments (Section 6) show another improvement of +0.9 BLEU points over a state-of-the-art forest-based system.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Statistical machine translation (SMT) has witnessed promising progress in recent years.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Typically, conventional SMT is characterized as a 1- best pipeline system (Figure 1(a)), whose modules are independent of each other and only take as input 1-best results from the previous module.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Though this assumption is convenient to reduce the complexity of SMT systems.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>It also bring a major drawback of error propagation.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The errors of 1-best outputs, introduced inevitably in each phase, will propagate and accumulate along the pipeline.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Not recoverable in the final decoding source</text>
              <doc_id>5</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>source</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1-best segmentation</text>
              <doc_id>7</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1-best tree</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>segmentation lattice</text>
              <doc_id>9</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>parse forest</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>target</text>
              <doc_id>11</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>target</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(a)</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(b)</text>
              <doc_id>14</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>step.</text>
              <doc_id>15</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>These errors will severely hurt the translation quality.</text>
              <doc_id>16</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For example, if the accuracy of each module is 90%, the final accuracy will drop to 73% after three separate phases.</text>
              <doc_id>17</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>To alleviate this problem, an obvious solution is to widen the pipeline with k-best lists rather than 1-best results.</text>
              <doc_id>18</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>For example Venugopal et al. (2008) use k-best alignments and parses in the training phase.</text>
              <doc_id>19</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>However, with limited scope and too many redundancies, it is inefficient to search separately on each of these similar lists (Huang, 2008).</text>
              <doc_id>20</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Another efficient method is to use compact data structures instead of k-bestlists.</text>
              <doc_id>21</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A lattice or forest, compactly encoded exponentially many derivations, have proven to be a promising technique.</text>
              <doc_id>22</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For example, Mi and Huang (2008), Mi et al. (2008), Liu et al. (2009) and Zhang et al. (2009) use forests in rule extraction and decoding phases to extract more general rules and weaken the influence of parsing errors; Dyer et al. (2008) use word lattice in Chinese word segmentation and Arabic morphological variation phases to weaken the influence of segmentation errors; Huang (2008) and</text>
              <doc_id>23</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Coling 2010: Poster Volume, pages 837&#8211;845, Beijing, August 2010</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(0, 2, NR) (2,3,CC) (3,5,NR) (5, 6, VV) (6, 8, NN) (8,9,NN) 0 c 0 :B&#249; 1 c 1 :sh&#237; 2 c 2 :y&#468; 3 c 3 :Sh&#257; 4 c 4 :l&#243;ng 5 c 5 :j&#468; 6 c 6 :x&#237;ng 7 c 7 :t&#462;o 8 c 8 :l&#249;n 9 (2,3,P) (5,7,VV) (7,9,NN)</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Jiang et al. (2008b) stress the problems in reranking phase.</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Both lattices and forests have become popular in machine translation literature.</text>
              <doc_id>27</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>However, to the best of our knowledge, previous work only focused on one module at a time.</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, we investigate the combination of lattice and forest (Section 2), as shown in Figure 1(b).</text>
              <doc_id>29</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We explore the algorithms of lattice parsing (Section 3.2), rule extraction (Section 4) and decoding (Section 5).</text>
              <doc_id>30</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>More importantly, in the decoding step, our model can search among not only more parse-trees but also more segmentations encoded in the lattice-forests and can take into account all the probabilities of segmentations and parse-trees.</text>
              <doc_id>31</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In other words, our model postpones the disambiguition of segmentation and parsing into the final translation step, so that we can do global search for the best segmentation, parse-tree and translation in one step.</text>
              <doc_id>32</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>When we integrate a lattice into a forest system, medium-scale experiments (Section 6) show another improvement of +0.9 BLEU points over a state-of-the-art forest-based system.</text>
              <doc_id>33</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>2 Compact Structures</title>
        <text>A word lattice (Figure 2) is a compact representation of all the possible of segmentations and POS tags, while a parse forest (Figure 5) is a compact representation of all parse trees.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>A word lattice (Figure 2) is a compact representation of all the possible of segmentations and POS tags, while a parse forest (Figure 5) is a compact representation of all parse trees.</text>
              <doc_id>34</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Word Lattice</title>
            <text>For a given input sentence C = c 0 ..c n&#8722;1 ,where c i denotes a character at position i, andn is the length of the sentence.
A word lattice (Figure 2), or lattice in short, is asetofedges L, where each edge is in the form of (i, j, X), which denotes a word of tag X, covering characters c i through c j&#8722;1 . For example, in Figure 2, (7, 9, NN) is a noun &#8220;t&#462;ol&#249;n&#8221; of two characters.
The lattice in Figure 2 shows result of the example:&#8220; B&#249; sh&#237;y&#468;Sh&#257;l&#243;ng j&#468; x&#237;ng t&#462;o l&#249;n &#8221;. One ambiguity comes from the POS tag of word &#8220;y&#468;&#8221; (preposition (P) or conjunction (CC)). The other one is the segmentation ambiguity of the last four characters, we can segment into either &#8220;j&#468; x&#237;ngt&#462;o l&#249;n&#8221; (solid lines), which means lift, begging and argument separately for each word or &#8220;j&#468;x&#237;ng t&#462;ol&#249;n&#8221; (dashed lines), which means hold a discussion.
lift begging argument
5 j&#468; 6 x&#237;ng 7 t&#462;o 8 l&#249;n 9
hold a discussion
The solid lines above (and also in Figure 2) show the 1-best result, which is obviously wrong. If we feed it into the next modules in the SMT pipeline, parsing and translation will be become much more difficult, since the segmentation is not recoverable. So it is necessary to postpone error segmentation decisions to the final translation step.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>For a given input sentence C = c 0 ..c n&#8722;1 ,where c i denotes a character at position i, andn is the length of the sentence.</text>
                  <doc_id>35</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A word lattice (Figure 2), or lattice in short, is asetofedges L, where each edge is in the form of (i, j, X), which denotes a word of tag X, covering characters c i through c j&#8722;1 .</text>
                  <doc_id>36</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For example, in Figure 2, (7, 9, NN) is a noun &#8220;t&#462;ol&#249;n&#8221; of two characters.</text>
                  <doc_id>37</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The lattice in Figure 2 shows result of the example:&#8220; B&#249; sh&#237;y&#468;Sh&#257;l&#243;ng j&#468; x&#237;ng t&#462;o l&#249;n &#8221;.</text>
                  <doc_id>38</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>One ambiguity comes from the POS tag of word &#8220;y&#468;&#8221; (preposition (P) or conjunction (CC)).</text>
                  <doc_id>39</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The other one is the segmentation ambiguity of the last four characters, we can segment into either &#8220;j&#468; x&#237;ngt&#462;o l&#249;n&#8221; (solid lines), which means lift, begging and argument separately for each word or &#8220;j&#468;x&#237;ng t&#462;ol&#249;n&#8221; (dashed lines), which means hold a discussion.</text>
                  <doc_id>40</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>lift begging argument</text>
                  <doc_id>41</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5 j&#468; 6 x&#237;ng 7 t&#462;o 8 l&#249;n 9</text>
                  <doc_id>42</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>hold a discussion</text>
                  <doc_id>43</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The solid lines above (and also in Figure 2) show the 1-best result, which is obviously wrong.</text>
                  <doc_id>44</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If we feed it into the next modules in the SMT pipeline, parsing and translation will be become much more difficult, since the segmentation is not recoverable.</text>
                  <doc_id>45</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>So it is necessary to postpone error segmentation decisions to the final translation step.</text>
                  <doc_id>46</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Parse Forest</title>
            <text>In parsing scenario, a parse forest (Figrure 5), or forest for short, can be formalized as a hypergraph H, a pair &#12296;V,E&#12297;, where node v &#8712; V is in the form of X i,j , which denotes the recognition of nonterminal X spanning the substring c i:j&#8722;1 from positions c i through c j&#8722;1 . Each hyperedge e &#8712; E is a pair &#12296;tails(e), head(e)&#12297;, wherehead(e) &#8712; V is the consequent node in an instantiated deductive step, and tails(e) &#8712; (V ) &#8727; is the list of antecedent nodes.
For the following deduction:
NR 0,2 CC 2,3 NR 3,5 NP 0,5 (*)
its hyperedge e &#8727; is notated:
&#12296;(NR 0,2 , CC 2,3 , NR 3,5 ), NP 0,5 &#12297;.
where
head(e &#8727; )={NP 0,5 },and
tails(e &#8727; )={NR 0,2 , CC 2,3 , NR 3,5 }.
We also denote IN (v) to be the set of incoming hyperedges of node v, which represents the different ways of deriving v. For simplicity, we only show a tree in Figure 5(a) over 1-best segmentation and POS tagging result in Figure 2. So the IN (NP 0,5 ) is {e &#8727; }.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In parsing scenario, a parse forest (Figrure 5), or forest for short, can be formalized as a hypergraph H, a pair &#12296;V,E&#12297;, where node v &#8712; V is in the form of X i,j , which denotes the recognition of nonterminal X spanning the substring c i:j&#8722;1 from positions c i through c j&#8722;1 .</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Each hyperedge e &#8712; E is a pair &#12296;tails(e), head(e)&#12297;, wherehead(e) &#8712; V is the consequent node in an instantiated deductive step, and tails(e) &#8712; (V ) &#8727; is the list of antecedent nodes.</text>
                  <doc_id>48</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For the following deduction:</text>
                  <doc_id>49</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NR 0,2 CC 2,3 NR 3,5 NP 0,5 (*)</text>
                  <doc_id>50</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>its hyperedge e &#8727; is notated:</text>
                  <doc_id>51</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#12296;(NR 0,2 , CC 2,3 , NR 3,5 ), NP 0,5 &#12297;.</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>head(e &#8727; )={NP 0,5 },and</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>tails(e &#8727; )={NR 0,2 , CC 2,3 , NR 3,5 }.</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We also denote IN (v) to be the set of incoming hyperedges of node v, which represents the different ways of deriving v.</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For simplicity, we only show a tree in Figure 5(a) over 1-best segmentation and POS tagging result in Figure 2.</text>
                  <doc_id>57</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>So the IN (NP 0,5 ) is {e &#8727; }.</text>
                  <doc_id>58</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>2</index>
        <title>3 Lattice Parsing</title>
        <text>In this section, we first briefly review the conventional CYK parsing, and then extend to lattice parsing. More importantly, we propose a more efficient parsing paradigm in Section 3.3.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this section, we first briefly review the conventional CYK parsing, and then extend to lattice parsing.</text>
              <doc_id>59</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>More importantly, we propose a more efficient parsing paradigm in Section 3.3.</text>
              <doc_id>60</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Conventional Parsing</title>
            <text>The conventional CYK parsing algorithm in Figure 3(a) usually takes as input a single sequence of words, so the CYK cells are organized over words. This algorithm consists of two steps: initialization and parsing. The first step is to initialize the CYK cells, whose span size is one, with POS tags produced by a POS tagger or defined by the input string 1 . For example, the top line in Figure 3(a) is initialized with a series of POS tags in 1-best segmentation. The second step is to search for the best syntactic tree under a context-free grammar. For example, the tree composed by the solid lines in Figure 5(a) shows the parsing tree for the 1-best segmentation and POS tagging results.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The conventional CYK parsing algorithm in Figure 3(a) usually takes as input a single sequence of words, so the CYK cells are organized over words.</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This algorithm consists of two steps: initialization and parsing.</text>
                  <doc_id>62</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The first step is to initialize the CYK cells, whose span size is one, with POS tags produced by a POS tagger or defined by the input string 1 .</text>
                  <doc_id>63</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For example, the top line in Figure 3(a) is initialized with a series of POS tags in 1-best segmentation.</text>
                  <doc_id>64</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The second step is to search for the best syntactic tree under a context-free grammar.</text>
                  <doc_id>65</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For example, the tree composed by the solid lines in Figure 5(a) shows the parsing tree for the 1-best segmentation and POS tagging results.</text>
                  <doc_id>66</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Lattice Parsing</title>
            <text>The main differences of our lattice parsing in Figure 3(b) from conventional approach are listed in following: First, the CYK cells are organized over characters rather than words. Second, in the initialization step, we only initialize the cells with all edges L in the lattice. Take the edge (7, 9, NN) in Figure 2 for example, the corresponding cell should be (7, 9), then we add a leaf node v = NN 7,9 with a word t&#462;ol&#249;n. The final initialization is shown in Figure 3(b), which shows that
1 For simplicity, we assume the input of a parser is a segmentation and POS tagging result
0 B&#249; 1 sh&#237; 2 y&#468; 3 Sh&#257; 4 l&#243;ng 5 j&#468; 6 x&#237;ng 7 t&#462;o 8 l&#249;n 9 NR CC NR VV NN NN
NP
IP
VPB
O(n 3 w)
(a): Parsing over 1-best segmentation
0 B&#249; 1 sh&#237; 2 y&#468; 3 Sh&#257; 4 l&#243;ng 5 j&#468; 6 x&#237;ng 7 t&#462;o 8 l&#249;n 9 CC,P VV NN NR NR VV NN NN PP NP VPB
IP
VP
(b): Parsing over characters
O(n 3 )
0 B&#249; 1 sh&#237; 2 y&#468; 3 Sh&#257; 4 l&#243;ng 5 j&#468; 6 x&#237;ng 7 t&#462;o 8 l&#249;n 9 NR CC,P NR VV NN VV NN NN PP
NP VPB
IP
VP
O(n 3 r)
(c): Parsing over most-refined segmentation
lattice parsing can initialize the cells, whose span size is larger than one. Third, in the deduction step of the parsing algorithm i, j, k are the indexes between characters rather than words. We formalize our lattice parser as a deductive proof system (Shieber et al., 1994) in Figure 4. Following the definitions of the previous Sec-
tion, given a set of edges L of a lattice for an input sentence C = c 0 ..c n&#8722;1 and a PCFG grammar: a 4-tuple &#12296;N,&#931;,P,S&#12297;, whereN is a set of nonterminals, &#931; is a set of terminal symbols, P is a set of inference rules, each of which is in the form of X &#8594; &#945; : p for X &#8712; N, &#945; &#8712; (N &#8746; &#931;) &#8727; and p is the probability, and S &#8712; N is the start symbol. The deductive proof system (Figure 4) consists of axioms, goals and inference rules. The axioms are converted by edges in L. Take the (5, 7, NN) associated with a weight p 1 for example, the corresponding axiom is NN &#8594; t&#462;ol&#249;n : p 1 . All axioms converted from the lattice are shown in Figure 3(b) exclude the italic non-terminals. Please note that all the probabilities of the edges L in a lattice are taken into account in the parsing step. The goals are the recognition X 0,n &#8712; S of the whole sentence. The inference rules are the deductions in parsing. Take the deduction (*) for example, it will prove a new item NP 0,5 (italic NP in Figure 3(b)) and generate a new hyper-edge e &#8727; (in Figure 5(b)). So the parsing algorithm starts with the axioms, and then applies the inference rules to prove new items until a goal item is proved. The final whole forest for the input lattice (Figure 2) is shown in Figure 5(b). The extra hyper-edges of lattice-forest are highlighted with dashed lines, which can inference the input sentence correctly. For example: &#8220;y&#468;&#8221; is tagged into P rather than CC.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The main differences of our lattice parsing in Figure 3(b) from conventional approach are listed in following: First, the CYK cells are organized over characters rather than words.</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Second, in the initialization step, we only initialize the cells with all edges L in the lattice.</text>
                  <doc_id>68</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Take the edge (7, 9, NN) in Figure 2 for example, the corresponding cell should be (7, 9), then we add a leaf node v = NN 7,9 with a word t&#462;ol&#249;n.</text>
                  <doc_id>69</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The final initialization is shown in Figure 3(b), which shows that</text>
                  <doc_id>70</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 For simplicity, we assume the input of a parser is a segmentation and POS tagging result</text>
                  <doc_id>71</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0 B&#249; 1 sh&#237; 2 y&#468; 3 Sh&#257; 4 l&#243;ng 5 j&#468; 6 x&#237;ng 7 t&#462;o 8 l&#249;n 9 NR CC NR VV NN NN</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NP</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>IP</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VPB</text>
                  <doc_id>75</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>O(n 3 w)</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(a): Parsing over 1-best segmentation</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0 B&#249; 1 sh&#237; 2 y&#468; 3 Sh&#257; 4 l&#243;ng 5 j&#468; 6 x&#237;ng 7 t&#462;o 8 l&#249;n 9 CC,P VV NN NR NR VV NN NN PP NP VPB</text>
                  <doc_id>78</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>IP</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VP</text>
                  <doc_id>80</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(b): Parsing over characters</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>O(n 3 )</text>
                  <doc_id>82</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0 B&#249; 1 sh&#237; 2 y&#468; 3 Sh&#257; 4 l&#243;ng 5 j&#468; 6 x&#237;ng 7 t&#462;o 8 l&#249;n 9 NR CC,P NR VV NN VV NN NN PP</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NP VPB</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>IP</text>
                  <doc_id>85</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VP</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>O(n 3 r)</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(c): Parsing over most-refined segmentation</text>
                  <doc_id>88</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>lattice parsing can initialize the cells, whose span size is larger than one.</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Third, in the deduction step of the parsing algorithm i, j, k are the indexes between characters rather than words.</text>
                  <doc_id>90</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We formalize our lattice parser as a deductive proof system (Shieber et al., 1994) in Figure 4.</text>
                  <doc_id>91</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Following the definitions of the previous Sec-</text>
                  <doc_id>92</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>tion, given a set of edges L of a lattice for an input sentence C = c 0 ..c n&#8722;1 and a PCFG grammar: a 4-tuple &#12296;N,&#931;,P,S&#12297;, whereN is a set of nonterminals, &#931; is a set of terminal symbols, P is a set of inference rules, each of which is in the form of X &#8594; &#945; : p for X &#8712; N, &#945; &#8712; (N &#8746; &#931;) &#8727; and p is the probability, and S &#8712; N is the start symbol.</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The deductive proof system (Figure 4) consists of axioms, goals and inference rules.</text>
                  <doc_id>94</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The axioms are converted by edges in L.</text>
                  <doc_id>95</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Take the (5, 7, NN) associated with a weight p 1 for example, the corresponding axiom is NN &#8594; t&#462;ol&#249;n : p 1 .</text>
                  <doc_id>96</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>All axioms converted from the lattice are shown in Figure 3(b) exclude the italic non-terminals.</text>
                  <doc_id>97</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Please note that all the probabilities of the edges L in a lattice are taken into account in the parsing step.</text>
                  <doc_id>98</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The goals are the recognition X 0,n &#8712; S of the whole sentence.</text>
                  <doc_id>99</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>The inference rules are the deductions in parsing.</text>
                  <doc_id>100</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>Take the deduction (*) for example, it will prove a new item NP 0,5 (italic NP in Figure 3(b)) and generate a new hyper-edge e &#8727; (in Figure 5(b)).</text>
                  <doc_id>101</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>So the parsing algorithm starts with the axioms, and then applies the inference rules to prove new items until a goal item is proved.</text>
                  <doc_id>102</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>The final whole forest for the input lattice (Figure 2) is shown in Figure 5(b).</text>
                  <doc_id>103</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>The extra hyper-edges of lattice-forest are highlighted with dashed lines, which can inference the input sentence correctly.</text>
                  <doc_id>104</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
                <sentence>
                  <text>For example: &#8220;y&#468;&#8221; is tagged into P rather than CC.</text>
                  <doc_id>105</doc_id>
                  <sec_id>12</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Faster Parsing with Most-refined Lattice</title>
            <text>However, our statistics show that the average number of characters n in a sentence is 1.6 times than the number of words n w in its 1-best segmentation. As a result, the parsing time over the characters will grow more than 4 times than parsing over the 1-best segmentation, since the time complexity is O(n 3 ). In order to alleviate this problem, we reduce the parsing time by using most-refined segmentation for a lattice, whose number of tokens is n r and has the property n w &#8804; n r &#8804; n.
Given a lattice with its edges L over indexes (0, .., n), a index i is a split point, if and only if there exists some edge (i, j, X) &#8712; L or (k, i, X) &#8712; L. The most-refined segmentation, or ms for short, is the segmentation result by using all split points in a lattice. For example, the corresponding ms of the example is &#8220;B&#249;sh&#237; y&#468;Sh&#257;l&#243;ng j&#468; x&#237;ng t&#462;o l&#249;n&#8221; since points 1 and 4 are not split points.
Item form:
Axioms:
Infer. rules:
X i,j
(i, j, X) &#8712; L X i,j : p(i, j, X)
X i,k : p 1 Y k,j : p 2 Z &#8594; XY : p &#8712; P
Z i,j : pp 1 p 2
Goals: X 0,n
Figure 3(c) shows the CKY parsing cells over most-refined segmentation, the average number of tokens n r is reduced by combining columns, which are shown with red dashed boxes. As a result, the search space is reduced without losing any derivations. Theoretically, the parsing over fs will speed up in O((n/n r ) 3 ). And our experiments in Section 6 show the efficiency of our new approach.
It turns out that the parsing algorithm developed in lattice-parsing Section 3.2 can be used here without any change. The non-terminals inducted are also shown in Figure 3(c) in italic style.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>However, our statistics show that the average number of characters n in a sentence is 1.6 times than the number of words n w in its 1-best segmentation.</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As a result, the parsing time over the characters will grow more than 4 times than parsing over the 1-best segmentation, since the time complexity is O(n 3 ).</text>
                  <doc_id>107</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In order to alleviate this problem, we reduce the parsing time by using most-refined segmentation for a lattice, whose number of tokens is n r and has the property n w &#8804; n r &#8804; n.</text>
                  <doc_id>108</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Given a lattice with its edges L over indexes (0, .., n), a index i is a split point, if and only if there exists some edge (i, j, X) &#8712; L or (k, i, X) &#8712; L.</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The most-refined segmentation, or ms for short, is the segmentation result by using all split points in a lattice.</text>
                  <doc_id>110</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For example, the corresponding ms of the example is &#8220;B&#249;sh&#237; y&#468;Sh&#257;l&#243;ng j&#468; x&#237;ng t&#462;o l&#249;n&#8221; since points 1 and 4 are not split points.</text>
                  <doc_id>111</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Item form:</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Axioms:</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Infer.</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>rules:</text>
                  <doc_id>115</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>X i,j</text>
                  <doc_id>116</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(i, j, X) &#8712; L X i,j : p(i, j, X)</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>X i,k : p 1 Y k,j : p 2 Z &#8594; XY : p &#8712; P</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Z i,j : pp 1 p 2</text>
                  <doc_id>119</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Goals: X 0,n</text>
                  <doc_id>120</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 3(c) shows the CKY parsing cells over most-refined segmentation, the average number of tokens n r is reduced by combining columns, which are shown with red dashed boxes.</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As a result, the search space is reduced without losing any derivations.</text>
                  <doc_id>122</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Theoretically, the parsing over fs will speed up in O((n/n r ) 3 ).</text>
                  <doc_id>123</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>And our experiments in Section 6 show the efficiency of our new approach.</text>
                  <doc_id>124</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It turns out that the parsing algorithm developed in lattice-parsing Section 3.2 can be used here without any change.</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The non-terminals inducted are also shown in Figure 3(c) in italic style.</text>
                  <doc_id>126</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>4 Rule Extraction with Lattice &amp; Forest</title>
        <text>We now explore the extraction algorithm from aligned source lattice-forest and target string 2 , which is a tuple &#12296;F, &#964;, a&#12297; in Figure 5(b). Following Mi and Huang (2008), we extract minimal rules from a lattice-forest also in two steps:
(1) frontier set computation
(2) fragmentation
Following the algorithms developed by Mi and Huang (2008) in Algorithm 1, all the nodes in frontier set (fs) are highlighted with gray in Figure 5(b). Our process of fragmentation (lines 1- 13) is to visit each frontier node v and initial a queue (open) of growing fragments with a pair of empty fragment and node v (line 3). Each fragment is associated with a list of expansion sites (front) being
2 For simplicity and consistency, we use character-based
lattice-forest for the running example. The &#8220;B&#249;&#8221; and&#8220;sh&#237;&#8221; are aligned to the same word &#8220;Bush&#8221;. In our experiment, we use most-refined segmentation to run lattice-parsing and word alignment.
IP 0,9
NP 0,5 VPB 5,9 (a)
e &#8727;
.NR 0,2 .CC 2,3 .NR 3,5 .VV 5,6 .NN 6,8 .NN 8,9
0 .B&#249; 1 .sh&#237; 2 .y&#468; 3 .Sh&#257; 4 .l&#243;ng 5 .j&#468; 6 .x&#237;ng 7 .t&#462;o 8 .l&#249;n 9
IP 0,9
NP 0,5 VP 2,9
e &#8727; (b)
PP 2,5 VPB 5,9
. NR 0,2 . CC 2,3 . P 2,3 . NR 3,5 .VV 5,6 . VV 5,7 .NN 6,8 .NN 8,9 . NN 7,9
0 .B&#249; 1 .sh&#237; 2 .y&#468; 3 .Sh&#257; 4 .l&#243;ng 5 .j&#468; 6 .x&#237;ng 7 .t&#462;o 8 .l&#249;n 9
Bush held a discussion with Sharon
(c)
the subset of leaf nodes of the current fragment that are not in the fs except for the initial node v. Then we keep expanding fragments in open in following way. If current fragment is complete, whose expansion sites is empty, we extract rule corresponding to the fragment and its target string
(line 7) . Otherwise we pop one expansion node u to grow and spin-off new fragments by IN (u), adding new expansion sites (lines 11- 13), until all active fragments are complete and open queue is empty. The extra minimal rules extracted on latticeforest are listed at the right bottom of Figure 5(c). Compared with the forest-only approach, we can extract smaller and more general rules.
After we get all the minimal rules, we compose two or more minimal rules into composed rules (Galley et al., 2006), which will be used in our experiments.
For each rule r extracted, we also assign a fractional count which is computed by using insideoutside probabilities:
Q &#945;(root(r)) &#183; P(lhs(r)) &#183;
v&#8712;yield(root(r))
c(r) = &#946;(v) , &#946;(TOP) (1)
where root(r) is the root of the rule, lhs(r) is the left-hand-side of rule, rhs(r) is the righthand-side of rule, P(lhs(r)) is the product of all probabilities of hyperedges involved in lhs(r), yield(root(r)) is the leave nodes, TOP is the root node of the forest, &#945;(v) and &#946;(v) are outside and inside probabilities, respectively. Then we compute three conditional probabilities for each rule:
c(r) P(r | lhs(r)) = &#8721;
r &#8242; :lhs(r &#8242; )=lhs(r) c(r&#8242; )
(2)
c(r) P(r | rhs(r)) = &#8721;
r &#8242; :rhs(r &#8242; )=rhs(r) c(r&#8242; )
P(r | root(r)) =
(3) c(r)
&#8721;r &#8242; :root(r &#8242; )=root(r) c(r&#8242; ) . (4)
All these probabilities are used in decoding step (Section 5). For more detail, we refer to the algorithms of Mi and Huang (2008).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We now explore the extraction algorithm from aligned source lattice-forest and target string 2 , which is a tuple &#12296;F, &#964;, a&#12297; in Figure 5(b).</text>
              <doc_id>127</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Following Mi and Huang (2008), we extract minimal rules from a lattice-forest also in two steps:</text>
              <doc_id>128</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(1) frontier set computation</text>
              <doc_id>129</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(2) fragmentation</text>
              <doc_id>130</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Following the algorithms developed by Mi and Huang (2008) in Algorithm 1, all the nodes in frontier set (fs) are highlighted with gray in Figure 5(b).</text>
              <doc_id>131</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our process of fragmentation (lines 1- 13) is to visit each frontier node v and initial a queue (open) of growing fragments with a pair of empty fragment and node v (line 3).</text>
              <doc_id>132</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Each fragment is associated with a list of expansion sites (front) being</text>
              <doc_id>133</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2 For simplicity and consistency, we use character-based</text>
              <doc_id>134</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>lattice-forest for the running example.</text>
              <doc_id>135</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The &#8220;B&#249;&#8221; and&#8220;sh&#237;&#8221; are aligned to the same word &#8220;Bush&#8221;.</text>
              <doc_id>136</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In our experiment, we use most-refined segmentation to run lattice-parsing and word alignment.</text>
              <doc_id>137</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>IP 0,9</text>
              <doc_id>138</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>NP 0,5 VPB 5,9 (a)</text>
              <doc_id>139</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e &#8727;</text>
              <doc_id>140</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>.NR 0,2 .CC 2,3 .NR 3,5 .VV 5,6 .NN 6,8 .NN 8,9</text>
              <doc_id>141</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0 .B&#249; 1 .sh&#237; 2 .y&#468; 3 .Sh&#257; 4 .l&#243;ng 5 .j&#468; 6 .x&#237;ng 7 .t&#462;o 8 .l&#249;n 9</text>
              <doc_id>142</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>IP 0,9</text>
              <doc_id>143</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>NP 0,5 VP 2,9</text>
              <doc_id>144</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e &#8727; (b)</text>
              <doc_id>145</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>PP 2,5 VPB 5,9</text>
              <doc_id>146</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>.</text>
              <doc_id>147</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>NR 0,2 .</text>
              <doc_id>148</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>CC 2,3 .</text>
              <doc_id>149</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>P 2,3 .</text>
              <doc_id>150</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>NR 3,5 .VV 5,6 .</text>
              <doc_id>151</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>VV 5,7 .NN 6,8 .NN 8,9 .</text>
              <doc_id>152</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>NN 7,9</text>
              <doc_id>153</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0 .B&#249; 1 .sh&#237; 2 .y&#468; 3 .Sh&#257; 4 .l&#243;ng 5 .j&#468; 6 .x&#237;ng 7 .t&#462;o 8 .l&#249;n 9</text>
              <doc_id>154</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Bush held a discussion with Sharon</text>
              <doc_id>155</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(c)</text>
              <doc_id>156</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the subset of leaf nodes of the current fragment that are not in the fs except for the initial node v.</text>
              <doc_id>157</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Then we keep expanding fragments in open in following way.</text>
              <doc_id>158</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>If current fragment is complete, whose expansion sites is empty, we extract rule corresponding to the fragment and its target string</text>
              <doc_id>159</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(line 7) .</text>
              <doc_id>160</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Otherwise we pop one expansion node u to grow and spin-off new fragments by IN (u), adding new expansion sites (lines 11- 13), until all active fragments are complete and open queue is empty.</text>
              <doc_id>161</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The extra minimal rules extracted on latticeforest are listed at the right bottom of Figure 5(c).</text>
              <doc_id>162</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Compared with the forest-only approach, we can extract smaller and more general rules.</text>
              <doc_id>163</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>After we get all the minimal rules, we compose two or more minimal rules into composed rules (Galley et al., 2006), which will be used in our experiments.</text>
              <doc_id>164</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For each rule r extracted, we also assign a fractional count which is computed by using insideoutside probabilities:</text>
              <doc_id>165</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Q &#945;(root(r)) &#183; P(lhs(r)) &#183;</text>
              <doc_id>166</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>v&#8712;yield(root(r))</text>
              <doc_id>167</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>c(r) = &#946;(v) , &#946;(TOP) (1)</text>
              <doc_id>168</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where root(r) is the root of the rule, lhs(r) is the left-hand-side of rule, rhs(r) is the righthand-side of rule, P(lhs(r)) is the product of all probabilities of hyperedges involved in lhs(r), yield(root(r)) is the leave nodes, TOP is the root node of the forest, &#945;(v) and &#946;(v) are outside and inside probabilities, respectively.</text>
              <doc_id>169</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Then we compute three conditional probabilities for each rule:</text>
              <doc_id>170</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>c(r) P(r | lhs(r)) = &#8721;</text>
              <doc_id>171</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>r &#8242; :lhs(r &#8242; )=lhs(r) c(r&#8242; )</text>
              <doc_id>172</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(2)</text>
              <doc_id>173</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>c(r) P(r | rhs(r)) = &#8721;</text>
              <doc_id>174</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>r &#8242; :rhs(r &#8242; )=rhs(r) c(r&#8242; )</text>
              <doc_id>175</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P(r | root(r)) =</text>
              <doc_id>176</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(3) c(r)</text>
              <doc_id>177</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8721;r &#8242; :root(r &#8242; )=root(r) c(r&#8242; ) .</text>
              <doc_id>178</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>(4)</text>
              <doc_id>179</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>All these probabilities are used in decoding step (Section 5).</text>
              <doc_id>180</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For more detail, we refer to the algorithms of Mi and Huang (2008).</text>
              <doc_id>181</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>5 Decoding with Lattice &amp; Forest</title>
        <text>Given a source-side lattice-forest F , our decoder searches for the best derivation d &#8727; among the set of all possible derivation D, each of which converts a tree in lattice-forest into a target string &#964;:
d &#8727; =argmaxP (d|T ) &#955;0 &#183; e &#955; 1|d|
d&#8712;D,T&#8712;F
&#183; LM(&#964;(d)) &#955;2 &#183; e &#955; 3|&#964;(d)| , (5)
where |d| is the penalty term on the number of rules in a derivation, LM(&#964;(d)) is the language model and e &#955; 3|&#964;(d)| is the length penalty term on target translation. The P (d|T ) decomposes into the product of rule probabilities P (r), each of which is decomposed further into
P (d|T )= &#8719; r&#8712;d P (r). (6)
Each P (r) in Equation 6 is decomposed further into the production of five probabilities:
P(r) =P(r|lhs(r)) &#955; 4
&#183; P(r|rhs(r)) &#955; 5
&#183; P(r|root(lhs(r)) &#955; 6
&#183; P lex (lhs(r)|rhs(r)) &#955; 7
&#183; P lex (rhs(r)|lhs(r)) &#955; 8 ,
(7)
where the last two are the lexical probabilities between the terminals of lhs(r) and rhs(r). All the weights of those features are tuned by using Minimal Error Rate Training (Och, 2003).
Following Mi et al. (2008), we first convert the lattice-forestintolattice translation forest with the conversion algorithm proposed by Mi et al. (2008),
and then the decoder finds the best derivation on the lattice translation forest. For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) to incrementally compute the second, third, through the kth best alternatives.
For more detail, we refer to the algorithms of Mi et al. (2008).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Given a source-side lattice-forest F , our decoder searches for the best derivation d &#8727; among the set of all possible derivation D, each of which converts a tree in lattice-forest into a target string &#964;:</text>
              <doc_id>182</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>d &#8727; =argmaxP (d|T ) &#955;0 &#183; e &#955; 1|d|</text>
              <doc_id>183</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>d&#8712;D,T&#8712;F</text>
              <doc_id>184</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#183; LM(&#964;(d)) &#955;2 &#183; e &#955; 3|&#964;(d)| , (5)</text>
              <doc_id>185</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where |d| is the penalty term on the number of rules in a derivation, LM(&#964;(d)) is the language model and e &#955; 3|&#964;(d)| is the length penalty term on target translation.</text>
              <doc_id>186</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The P (d|T ) decomposes into the product of rule probabilities P (r), each of which is decomposed further into</text>
              <doc_id>187</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (d|T )= &#8719; r&#8712;d P (r).</text>
              <doc_id>188</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>(6)</text>
              <doc_id>189</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Each P (r) in Equation 6 is decomposed further into the production of five probabilities:</text>
              <doc_id>190</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P(r) =P(r|lhs(r)) &#955; 4</text>
              <doc_id>191</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#183; P(r|rhs(r)) &#955; 5</text>
              <doc_id>192</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#183; P(r|root(lhs(r)) &#955; 6</text>
              <doc_id>193</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#183; P lex (lhs(r)|rhs(r)) &#955; 7</text>
              <doc_id>194</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#183; P lex (rhs(r)|lhs(r)) &#955; 8 ,</text>
              <doc_id>195</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(7)</text>
              <doc_id>196</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where the last two are the lexical probabilities between the terminals of lhs(r) and rhs(r).</text>
              <doc_id>197</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>All the weights of those features are tuned by using Minimal Error Rate Training (Och, 2003).</text>
              <doc_id>198</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Following Mi et al. (2008), we first convert the lattice-forestintolattice translation forest with the conversion algorithm proposed by Mi et al. (2008),</text>
              <doc_id>199</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and then the decoder finds the best derivation on the lattice translation forest.</text>
              <doc_id>200</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM.</text>
              <doc_id>201</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) to incrementally compute the second, third, through the kth best alternatives.</text>
              <doc_id>202</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For more detail, we refer to the algorithms of Mi et al. (2008).</text>
              <doc_id>203</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>6 Experiments</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>204</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>6.1 Data Preparation</title>
            <text>Our experiments are on Chinese-to-English translation. Our training corpus is FBIS corpus with about 6.9M/8.9M words in Chinese/English respectively.
We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Gigaword corpus.
We use the 2002 NIST MT Evaluation test set as development set and the 2005 NIST MT Evaluation test set as test set. We evaluate the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002). We use the standard MERT (Och, 2003) to tune the weights.
6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al., 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results. Then we parse the segmentation results into forest by using the parser of Xiong et al. (2005). Actually, the parser will assign multiple POS tags to each word rather than one. As a result, our baseline system has already postponed the POS tagging disambiguition to the decoding step. Forest is pruned by using a marginal probabilitybased pruning algorithm similar to Huang (2008). The pruning threshold are p f =5and p f =10at rule extraction and decoding steps respectively.
We word-align the strings of 1-best segmentations and target strings with GIZA++ (Och and Ney, 2000) and apply the refinement method &#8220;grow-diag-final-and&#8221; (Koehn et al., 2003) to get the final alignments. Following Mi and Huang (2008) and Mi et al. (2008), we also extract rules from forest-string pairs and translate forest to string.
6.1.2 Lattice-forest System
We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al., 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm.
Then, as current GIZA++ (Och and Ney, 2000) can only handle alignment between string-string pairs, and word-alingment with the pairs of Chinese characters and target-string will obviously result in worse alignment quality. So a much better way to utilize GIZA++ is to use the most-refined segmentation for each lattice instead of the character sequence. This approach can be viewed as a compromise between character-string and latticestring word-alignment paradigms. In our experiments, we construct the most-refined segmentations for lattices and word-align them against the English sentences. We again apply the refinement method &#8220;grow-diag-final-and&#8221; (Koehn et al., 2003) to get the final alignments.
In order to get the lattice-forests, we modified Xiong et al. (2005)&#8217;s parser into a lattice parser, which produces the pruned lattice forests for both training, dev and test sentences. Finally, we apply the rule extraction algorithm proposed in this paper to obtain the rule set. Both lattices and forests are pruned using a marginal probabilitybased pruning algorithm similar to Huang (2008). The pruning threshold of lattice is p l =20at both the rule extraction and decoding steps, the thresholds for the latice-forests are p f =5and p f =10 at rule extraction and decoding steps respectively.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Our experiments are on Chinese-to-English translation.</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Our training corpus is FBIS corpus with about 6.9M/8.9M words in Chinese/English respectively.</text>
                  <doc_id>206</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We use SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram language model with Kneser-Ney smoothing on the first 1/3 of the Xinhua portion of Gigaword corpus.</text>
                  <doc_id>207</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We use the 2002 NIST MT Evaluation test set as development set and the 2005 NIST MT Evaluation test set as test set.</text>
                  <doc_id>208</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We evaluate the translation quality using the case-insensitive BLEU-4 metric (Papineni et al., 2002).</text>
                  <doc_id>209</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We use the standard MERT (Och, 2003) to tune the weights.</text>
                  <doc_id>210</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al., 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results.</text>
                  <doc_id>211</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Then we parse the segmentation results into forest by using the parser of Xiong et al. (2005).</text>
                  <doc_id>212</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Actually, the parser will assign multiple POS tags to each word rather than one.</text>
                  <doc_id>213</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>As a result, our baseline system has already postponed the POS tagging disambiguition to the decoding step.</text>
                  <doc_id>214</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Forest is pruned by using a marginal probabilitybased pruning algorithm similar to Huang (2008).</text>
                  <doc_id>215</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The pruning threshold are p f =5and p f =10at rule extraction and decoding steps respectively.</text>
                  <doc_id>216</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We word-align the strings of 1-best segmentations and target strings with GIZA++ (Och and Ney, 2000) and apply the refinement method &#8220;grow-diag-final-and&#8221; (Koehn et al., 2003) to get the final alignments.</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Following Mi and Huang (2008) and Mi et al. (2008), we also extract rules from forest-string pairs and translate forest to string.</text>
                  <doc_id>218</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>6.1.2 Lattice-forest System</text>
                  <doc_id>219</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al., 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm.</text>
                  <doc_id>220</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Then, as current GIZA++ (Och and Ney, 2000) can only handle alignment between string-string pairs, and word-alingment with the pairs of Chinese characters and target-string will obviously result in worse alignment quality.</text>
                  <doc_id>221</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>So a much better way to utilize GIZA++ is to use the most-refined segmentation for each lattice instead of the character sequence.</text>
                  <doc_id>222</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This approach can be viewed as a compromise between character-string and latticestring word-alignment paradigms.</text>
                  <doc_id>223</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In our experiments, we construct the most-refined segmentations for lattices and word-align them against the English sentences.</text>
                  <doc_id>224</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We again apply the refinement method &#8220;grow-diag-final-and&#8221; (Koehn et al., 2003) to get the final alignments.</text>
                  <doc_id>225</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In order to get the lattice-forests, we modified Xiong et al. (2005)&#8217;s parser into a lattice parser, which produces the pruned lattice forests for both training, dev and test sentences.</text>
                  <doc_id>226</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, we apply the rule extraction algorithm proposed in this paper to obtain the rule set.</text>
                  <doc_id>227</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Both lattices and forests are pruned using a marginal probabilitybased pruning algorithm similar to Huang (2008).</text>
                  <doc_id>228</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The pruning threshold of lattice is p l =20at both the rule extraction and decoding steps, the thresholds for the latice-forests are p f =5and p f =10 at rule extraction and decoding steps respectively.</text>
                  <doc_id>229</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>6.2 Results and Analysis</title>
            <text>Table 1 shows results of two systems. Our latticeforest (LF) system achieves a BLEU score of 29.65, which is an absolute improvement of 0.9 points over the forest (F) baseline system, and the improvement is statistically significant at p &lt; 0.01 using the sign-test of Collins et al. (2005). The average number of tokens for the 1-best and most-refined segmentations are shown in second column. The average number of characters is 46.7, which is not shown in Table 1. Com-
Sys
Avg # of Rules tokens links All dev&amp;tst BLEU
pared with the characters-based lattice parsing, our most-refined lattice parsing speeds up parsing by (37.1/46.7) 3 &#8776; 2 times, since parsing complexity is O(n 3 ).
More interestingly, our lattice-forest model only extracts 23.5M rules, which is 79.4% percent of the rules extracted by the baseline system. The main reason lies in the larger average number of words for most-refined segmentations over lattices being 37.1 words vs 28.7 words over 1-best segmentations. With much finer granularity, more word aligned links and restrictions are introduced during the rule extraction step by GIZA++. However, more rules can be used in the decoding step for the lattice-forest system, since the lattice-forest is larger than the forest over 1-best segmentation. We also investigate the question of how often the non 1-best segmentations are picked in the final translation. The statistic on our dev set suggests 33% of sentences choose non 1-best segmentations. So our lattice-forest model can do global search for the best segmentation and parse-tree to direct the final translation. More importantly, we can use more translation rules in the translation step.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Table 1 shows results of two systems.</text>
                  <doc_id>230</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Our latticeforest (LF) system achieves a BLEU score of 29.65, which is an absolute improvement of 0.9 points over the forest (F) baseline system, and the improvement is statistically significant at p &lt; 0.01 using the sign-test of Collins et al. (2005).</text>
                  <doc_id>231</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The average number of tokens for the 1-best and most-refined segmentations are shown in second column.</text>
                  <doc_id>232</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The average number of characters is 46.7, which is not shown in Table 1.</text>
                  <doc_id>233</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Com-</text>
                  <doc_id>234</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Sys</text>
                  <doc_id>235</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Avg # of Rules tokens links All dev&amp;tst BLEU</text>
                  <doc_id>236</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>pared with the characters-based lattice parsing, our most-refined lattice parsing speeds up parsing by (37.1/46.7) 3 &#8776; 2 times, since parsing complexity is O(n 3 ).</text>
                  <doc_id>237</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>More interestingly, our lattice-forest model only extracts 23.5M rules, which is 79.4% percent of the rules extracted by the baseline system.</text>
                  <doc_id>238</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The main reason lies in the larger average number of words for most-refined segmentations over lattices being 37.1 words vs 28.7 words over 1-best segmentations.</text>
                  <doc_id>239</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>With much finer granularity, more word aligned links and restrictions are introduced during the rule extraction step by GIZA++.</text>
                  <doc_id>240</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>However, more rules can be used in the decoding step for the lattice-forest system, since the lattice-forest is larger than the forest over 1-best segmentation.</text>
                  <doc_id>241</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We also investigate the question of how often the non 1-best segmentations are picked in the final translation.</text>
                  <doc_id>242</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The statistic on our dev set suggests 33% of sentences choose non 1-best segmentations.</text>
                  <doc_id>243</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>So our lattice-forest model can do global search for the best segmentation and parse-tree to direct the final translation.</text>
                  <doc_id>244</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>More importantly, we can use more translation rules in the translation step.</text>
                  <doc_id>245</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>6</index>
        <title>7 Related Works</title>
        <text>Compactly encoding exponentially many derivations, lattice and forest have been used in some previous works on SMT. To alleviate the problem of parsing error in 1-best tree-to-string translation model, Mi et al. (2008) first use forest to direct translation. Then Mi and Huang (2008) use forest in rule extraction step. Following the same direction, Liu et al. (2009) use forest in treeto-tree model, and improve 1-best system by 3 BLEU points. Zhang et al. (2009) use forest in tree-sequence-to-string model and also achieve a promising improvement. Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009). Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Compactly encoding exponentially many derivations, lattice and forest have been used in some previous works on SMT.</text>
              <doc_id>246</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To alleviate the problem of parsing error in 1-best tree-to-string translation model, Mi et al. (2008) first use forest to direct translation.</text>
              <doc_id>247</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Then Mi and Huang (2008) use forest in rule extraction step.</text>
              <doc_id>248</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Following the same direction, Liu et al. (2009) use forest in treeto-tree model, and improve 1-best system by 3 BLEU points.</text>
              <doc_id>249</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Zhang et al. (2009) use forest in tree-sequence-to-string model and also achieve a promising improvement.</text>
              <doc_id>250</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Dyer et al. (2008) combine multiple segmentations into word lattice and then use lattice to direct a phrase-based translation decoder.</text>
              <doc_id>251</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system.</text>
              <doc_id>252</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Lattices and forests can also be used in Minimal Error Rate Training and Minimum Bayes Risk Decoding phases (Macherey et al., 2008; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009; Li and Eisner, 2009).</text>
              <doc_id>253</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Different from the works listed above, we mainly focus on how to combine lattice and forest into a single tree-to-string system.</text>
              <doc_id>254</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>7</index>
        <title>8 Conclusion and Future Work</title>
        <text>In this paper, we have proposed a lattice-forest based model to alleviate the problem of error propagation in traditional single-best pipeline framework. Unlike previous works, which only focus on one module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-tostring system. We have explored the algorithms of lattice parsing, rule extraction and decoding. Our model postpones the disambiguition of segmentation and parsing into the final translation step, so that we can make a more global decision to search for the best segmentation, parse-tree and translation in one step. The experimental results show that our lattice-forest approach achieves an absolute improvement of +0.9 points in term of BLEU score over a state-of-the-art forest-based model.
For future work, we would like to pay more attention to word alignment between lattice pairs and forest pairs, which would be more principled than our current method of word alignment between most-refined segmentation and string.
Acknowledgement
We thank Steve DeNeefe and the three anonymous reviewers for comments. The work is supported by National Natural Science Foundation of China, Contracts 90920004 and 60736014, and 863 State Key Project No. 2006AA010108 (H. M and Q. L.), and in part by DARPA GALE Contract No. HR0011-06-C-0022, and DARPA under DOI-NBC Grant N10AP20031 (L. H and H. M).
References
David Chiang. 2007. Hierarchical phrase-based translation. Comput. Linguist., 33(2):201&#8211;228.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings of ACL, pages 531&#8211;540, Ann Arbor, Michigan, June.
John DeNero, David Chiang, and Kevin Knight. 2009. Fast consensus decoding over translation forests. In Proceedings of ACL/IJCNLP.
Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings of ACL-08: HLT, pages 1012&#8211;1020, Columbus, Ohio, June.
C. Dyer. 2009. Using a maximum entropy model to build segmentation lattices for mt. In Proceedings of NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING-ACL, pages 961&#8211;968, Sydney, Australia, July.
Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of IWPT.
Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of ACL, pages 144&#8211;151, June.
Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L&#252;. 2008a. A cascaded linear model for joint chinese word segmentation and part-of-speech tagging. In Proceedings of ACL-08: HLT.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word lattice reranking for chinese word segmentation and part-of-speech tagging. In Proceedings of Coling 2008.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL, pages 127&#8211;133, Edmonton, Canada, May.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices. In Proceedings of the ACL/IJCNLP 2009.
Zhifei Li and Jason Eisner. 2009. First- and secondorder expectation semirings with applications to minimum-risk training on translation forests. In Proceedings of EMNLP, pages 40&#8211;51, Singapore, August. Association for Computational Linguistics.
Yang Liu, Yajuan L&#252;, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proceedings of ACL/IJCNLP, August.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based minimum error rate training for statistical machine translation. In Proceedings of EMNLP 2008.
Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of EMNLP 2008, pages 206&#8211;214, Honolulu, Hawaii, October.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of ACL-08:HLT, pages 192&#8211;199, Columbus, Ohio, June.
Franz J. Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of ACL, pages 440&#8211;447.
Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160&#8211;167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311&#8211;318, Philadephia, USA, July.
Stuart M. Shieber, Yves Schabes, and Fernando C. N. Pereira. 1994. Principles and implementation of deductive parsing.
Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of ICSLP, volume 30, pages 901&#8211;904.
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum Bayes- Risk decoding for statistical machine translation. In Proceedings of EMNLP 2008.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith, and Stephan Vogel. 2008. Wider pipelines: N-best alignments and parses in MT training. In Proceedings of AMTA.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin. 2005. Parsing the Penn Chinese Treebank with Semantic Knowledge. In Proceedings of IJCNLP 2005, pages 70&#8211;81.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and Chew Lim Tan. 2009. Forest-based tree sequence to string translation model. In Proceedings of the ACL/IJCNLP 2009.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper, we have proposed a lattice-forest based model to alleviate the problem of error propagation in traditional single-best pipeline framework.</text>
              <doc_id>255</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Unlike previous works, which only focus on one module at a time, our model successfully integrates lattice into a state-of-the-art forest tree-tostring system.</text>
              <doc_id>256</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We have explored the algorithms of lattice parsing, rule extraction and decoding.</text>
              <doc_id>257</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Our model postpones the disambiguition of segmentation and parsing into the final translation step, so that we can make a more global decision to search for the best segmentation, parse-tree and translation in one step.</text>
              <doc_id>258</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The experimental results show that our lattice-forest approach achieves an absolute improvement of +0.9 points in term of BLEU score over a state-of-the-art forest-based model.</text>
              <doc_id>259</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For future work, we would like to pay more attention to word alignment between lattice pairs and forest pairs, which would be more principled than our current method of word alignment between most-refined segmentation and string.</text>
              <doc_id>260</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Acknowledgement</text>
              <doc_id>261</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We thank Steve DeNeefe and the three anonymous reviewers for comments.</text>
              <doc_id>262</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The work is supported by National Natural Science Foundation of China, Contracts 90920004 and 60736014, and 863 State Key Project No.</text>
              <doc_id>263</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>2006AA010108 (H. M and Q.</text>
              <doc_id>264</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>L.</text>
              <doc_id>265</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>), and in part by DARPA GALE Contract No.</text>
              <doc_id>266</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>HR0011-06-C-0022, and DARPA under DOI-NBC Grant N10AP20031 (L.</text>
              <doc_id>267</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>H and H. M).</text>
              <doc_id>268</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>References</text>
              <doc_id>269</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>David Chiang.</text>
              <doc_id>270</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>271</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Hierarchical phrase-based translation.</text>
              <doc_id>272</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Comput.</text>
              <doc_id>273</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Linguist., 33(2):201&#8211;228.</text>
              <doc_id>274</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Michael Collins, Philipp Koehn, and Ivona Kucerova.</text>
              <doc_id>275</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>276</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Clause restructuring for statistical machine translation.</text>
              <doc_id>277</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL, pages 531&#8211;540, Ann Arbor, Michigan, June.</text>
              <doc_id>278</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>John DeNero, David Chiang, and Kevin Knight.</text>
              <doc_id>279</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>280</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Fast consensus decoding over translation forests.</text>
              <doc_id>281</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL/IJCNLP.</text>
              <doc_id>282</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Christopher Dyer, Smaranda Muresan, and Philip Resnik.</text>
              <doc_id>283</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>284</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Generalizing word lattice translation.</text>
              <doc_id>285</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL-08: HLT, pages 1012&#8211;1020, Columbus, Ohio, June.</text>
              <doc_id>286</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>C. Dyer.</text>
              <doc_id>287</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>288</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Using a maximum entropy model to build segmentation lattices for mt.</text>
              <doc_id>289</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of NAACL.</text>
              <doc_id>290</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.</text>
              <doc_id>291</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>292</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Scalable inference and training of context-rich syntactic translation models.</text>
              <doc_id>293</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of COLING-ACL, pages 961&#8211;968, Sydney, Australia, July.</text>
              <doc_id>294</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Liang Huang and David Chiang.</text>
              <doc_id>295</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>296</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Better k-best parsing.</text>
              <doc_id>297</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of IWPT.</text>
              <doc_id>298</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Liang Huang and David Chiang.</text>
              <doc_id>299</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>300</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Forest rescoring: Faster decoding with integrated language models.</text>
              <doc_id>301</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL, pages 144&#8211;151, June.</text>
              <doc_id>302</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Liang Huang.</text>
              <doc_id>303</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>304</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Forest reranking: Discriminative parsing with non-local features.</text>
              <doc_id>305</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL.</text>
              <doc_id>306</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L&#252;.</text>
              <doc_id>307</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008a.</text>
              <doc_id>308</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A cascaded linear model for joint chinese word segmentation and part-of-speech tagging.</text>
              <doc_id>309</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL-08: HLT.</text>
              <doc_id>310</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Wenbin Jiang, Haitao Mi, and Qun Liu.</text>
              <doc_id>311</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008b.</text>
              <doc_id>312</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Word lattice reranking for chinese word segmentation and part-of-speech tagging.</text>
              <doc_id>313</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of Coling 2008.</text>
              <doc_id>314</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Philipp Koehn, Franz Joseph Och, and Daniel Marcu.</text>
              <doc_id>315</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>316</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Statistical phrase-based translation.</text>
              <doc_id>317</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of HLT-NAACL, pages 127&#8211;133, Edmonton, Canada, May.</text>
              <doc_id>318</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och.</text>
              <doc_id>319</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>320</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Efficient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.</text>
              <doc_id>321</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the ACL/IJCNLP 2009.</text>
              <doc_id>322</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Zhifei Li and Jason Eisner.</text>
              <doc_id>323</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>324</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>First- and secondorder expectation semirings with applications to minimum-risk training on translation forests.</text>
              <doc_id>325</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of EMNLP, pages 40&#8211;51, Singapore, August.</text>
              <doc_id>326</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>327</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Yang Liu, Yajuan L&#252;, and Qun Liu.</text>
              <doc_id>328</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>329</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Improving tree-to-tree translation with packed forests.</text>
              <doc_id>330</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL/IJCNLP, August.</text>
              <doc_id>331</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit.</text>
              <doc_id>332</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>333</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Lattice-based minimum error rate training for statistical machine translation.</text>
              <doc_id>334</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of EMNLP 2008.</text>
              <doc_id>335</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Haitao Mi and Liang Huang.</text>
              <doc_id>336</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>337</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Forest-based translation rule extraction.</text>
              <doc_id>338</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of EMNLP 2008, pages 206&#8211;214, Honolulu, Hawaii, October.</text>
              <doc_id>339</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Haitao Mi, Liang Huang, and Qun Liu.</text>
              <doc_id>340</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>341</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Forestbased translation.</text>
              <doc_id>342</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL-08:HLT, pages 192&#8211;199, Columbus, Ohio, June.</text>
              <doc_id>343</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Franz J. Och and Hermann Ney.</text>
              <doc_id>344</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2000.</text>
              <doc_id>345</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Improved statistical alignment models.</text>
              <doc_id>346</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL, pages 440&#8211;447.</text>
              <doc_id>347</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Franz J. Och.</text>
              <doc_id>348</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>349</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Minimum error rate training in statistical machine translation.</text>
              <doc_id>350</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL, pages 160&#8211;167.</text>
              <doc_id>351</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu.</text>
              <doc_id>352</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>353</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Bleu: a method for automatic evaluation of machine translation.</text>
              <doc_id>354</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL, pages 311&#8211;318, Philadephia, USA, July.</text>
              <doc_id>355</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Stuart M. Shieber, Yves Schabes, and Fernando C.</text>
              <doc_id>356</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>N. Pereira.</text>
              <doc_id>357</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>1994.</text>
              <doc_id>358</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Principles and implementation of deductive parsing.</text>
              <doc_id>359</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Andreas Stolcke.</text>
              <doc_id>360</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>361</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>SRILM - an extensible language modeling toolkit.</text>
              <doc_id>362</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ICSLP, volume 30, pages 901&#8211;904.</text>
              <doc_id>363</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey.</text>
              <doc_id>364</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>365</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Lattice Minimum Bayes- Risk decoding for statistical machine translation.</text>
              <doc_id>366</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of EMNLP 2008.</text>
              <doc_id>367</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Ashish Venugopal, Andreas Zollmann, Noah A. Smith, and Stephan Vogel.</text>
              <doc_id>368</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>369</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Wider pipelines: N-best alignments and parses in MT training.</text>
              <doc_id>370</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of AMTA.</text>
              <doc_id>371</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin.</text>
              <doc_id>372</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>373</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Parsing the Penn Chinese Treebank with Semantic Knowledge.</text>
              <doc_id>374</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of IJCNLP 2005, pages 70&#8211;81.</text>
              <doc_id>375</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and Chew Lim Tan.</text>
              <doc_id>376</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>377</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Forest-based tree sequence to string translation model.</text>
              <doc_id>378</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the ACL/IJCNLP 2009.</text>
              <doc_id>379</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Results of forest (F) and lattice-forest (LF) systems. Please note that lattice-forest system only extracts 23.5M rules, which is only 79.4% of the rules extracted by forest system. However, in decoding step, lattice-forest system can use more rules after filtered on dev and test sets.</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>F</cell>
              <cell>28.7</cell>
              <cell>35.1</cell>
              <cell>29.6M</cell>
              <cell>3.3M</cell>
              <cell>28.75</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>LF</cell>
              <cell>37.1</cell>
              <cell>37.1</cell>
              <cell>23.5M</cell>
              <cell>3.4M</cell>
              <cell>29.65</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references/>
    <citations/>
  </content>
</document>
