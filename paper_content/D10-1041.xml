<document>
  <filename>D10-1041</filename>
  <authors>
    <author>Jinhua Du</author>
    <author>Jie Jiang</author>
  </authors>
  <title>Facilitating Translation Using Source Language Paraphrase Lattices</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>For resource-limited language pairs, coverage of the test set by the parallel corpus is an important factor that affects translation quality in two respects: 1) out of vocabulary words; 2) the same information in an input sentence can be expressed in different ways, while current phrase-based SMT systems cannot automatically select an alternative way to transfer the same information. Therefore, given limited data, in order to facilitate translation from the input side, this paper proposes a novel method to reduce the translation difficulty using source-side lattice-based paraphrases. We utilise the original phrases from the input sentence and the corresponding paraphrases to build a lattice with estimated weights for each edge to improve translation quality. Compared to the baseline system, our method achieves relative improvements of 7.07%, 6.78% and 3.63% in terms of BLEU score on small, medium and largescale English-to-Chinese translation tasks respectively. The results show that the proposed method is effective not only for resourcelimited language pairs, but also for resourcesufficient pairs to some extent.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>For resource-limited language pairs, coverage of the test set by the parallel corpus is an important factor that affects translation quality in two respects: 1) out of vocabulary words; 2) the same information in an input sentence can be expressed in different ways, while current phrase-based SMT systems cannot automatically select an alternative way to transfer the same information.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, given limited data, in order to facilitate translation from the input side, this paper proposes a novel method to reduce the translation difficulty using source-side lattice-based paraphrases.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We utilise the original phrases from the input sentence and the corresponding paraphrases to build a lattice with estimated weights for each edge to improve translation quality.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Compared to the baseline system, our method achieves relative improvements of 7.07%, 6.78% and 3.63% in terms of BLEU score on small, medium and largescale English-to-Chinese translation tasks respectively.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The results show that the proposed method is effective not only for resourcelimited language pairs, but also for resourcesufficient pairs to some extent.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>In recent years, statistical MT systems have been easy to develop due to the rapid explosion in data availability, especially parallel data. However, in reality there are still many language pairs which lack parallel data, such as Urdu&#8211;English, Chinese&#8211; Italian, where large amounts of speakers exist for both languages; of course, the problem is far worse for pairs such as Catalan&#8211;Irish. For such resourcelimited language pairs, sparse amounts of parallel data would cause the word alignment to be inaccurate, which would in turn lead to an inaccurate phrase alignment, and bad translations would result. Callison-Burch et al. (2006) argue that limited amounts of parallel training data can lead to the problem of low coverage in that many phrases encountered at run-time are not observed in the training data and so their translations will not be learned. Thus, in recent years, research on addressing the problem of unknown words or phrases has become more and more evident for resource-limited language pairs.
Callison-Burch et al. (2006) proposed a novel method which substitutes a paraphrase for an unknown source word or phrase in the input sentence, and then proceeds to use the translation of that paraphrase in the production of the target-language result. Their experiments showed that by translating paraphrases a marked improvement was achieved in coverage and translation quality, especially in the case of unknown words which previously had been left untranslated. However, on a large-scale data set, they did not achieve improvements in terms of automatic evaluation.
Nakov (2008) proposed another way to use paraphrases in SMT. He generates nearly-equivalent syntactic paraphrases of the source-side training sentences, then pairs each paraphrased sentence with the target translation associated with the original sentence in the training data. Essentially, this method generates new training data using paraphrases to train a new model and obtain more useful
phrase pairs. However, he reported that this method results in bad system performance. By contrast, real improvements can be achieved by merging the phrase tables of the paraphrase model and the original model, giving priority to the latter. Schroeder et al. (2009) presented the use of word lattices for multi-source translation, in which the multiple source input texts are compiled into a compact lattice, over which a single decoding pass is then performed. This lattice-based method achieved positive results across all data conditions. In this paper, we propose a novel method using paraphrases to facilitate translation, especially for resource-limited languages. Our method does not distinguish unknown words in the input sentence, but uses paraphrases of all possible words and phrases in the source input sentence to build a source-side lattice to provide a diverse and flexible list of source-side candidates to the SMT decoder so that it can search for a best path and deliver the translation with the highest probability. In this case, we neither need to change the phrase table, nor add new features in the log-linear model, nor add new sentences in the training data.
The remainder of this paper is organised as follows. In Section 2, we define the &#8220;translation difficulty&#8221; from the perspective of the source side, and then examine how well the test set is covered by the phrase table and the parallel training data . Section 3 describes our paraphrase lattice method and discusses how to set the weights for the edges in the lattice network. In Section 4, we report comparative experiments conducted on small, medium and largescale English-to-Chinese data sets. In Section 5, we analyse the influence of our paraphrase lattice method. Section 6 concludes and gives avenues for future work.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In recent years, statistical MT systems have been easy to develop due to the rapid explosion in data availability, especially parallel data.</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, in reality there are still many language pairs which lack parallel data, such as Urdu&#8211;English, Chinese&#8211; Italian, where large amounts of speakers exist for both languages; of course, the problem is far worse for pairs such as Catalan&#8211;Irish.</text>
              <doc_id>6</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For such resourcelimited language pairs, sparse amounts of parallel data would cause the word alignment to be inaccurate, which would in turn lead to an inaccurate phrase alignment, and bad translations would result.</text>
              <doc_id>7</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Callison-Burch et al. (2006) argue that limited amounts of parallel training data can lead to the problem of low coverage in that many phrases encountered at run-time are not observed in the training data and so their translations will not be learned.</text>
              <doc_id>8</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Thus, in recent years, research on addressing the problem of unknown words or phrases has become more and more evident for resource-limited language pairs.</text>
              <doc_id>9</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Callison-Burch et al. (2006) proposed a novel method which substitutes a paraphrase for an unknown source word or phrase in the input sentence, and then proceeds to use the translation of that paraphrase in the production of the target-language result.</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Their experiments showed that by translating paraphrases a marked improvement was achieved in coverage and translation quality, especially in the case of unknown words which previously had been left untranslated.</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, on a large-scale data set, they did not achieve improvements in terms of automatic evaluation.</text>
              <doc_id>12</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Nakov (2008) proposed another way to use paraphrases in SMT.</text>
              <doc_id>13</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>He generates nearly-equivalent syntactic paraphrases of the source-side training sentences, then pairs each paraphrased sentence with the target translation associated with the original sentence in the training data.</text>
              <doc_id>14</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Essentially, this method generates new training data using paraphrases to train a new model and obtain more useful</text>
              <doc_id>15</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>phrase pairs.</text>
              <doc_id>16</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, he reported that this method results in bad system performance.</text>
              <doc_id>17</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>By contrast, real improvements can be achieved by merging the phrase tables of the paraphrase model and the original model, giving priority to the latter.</text>
              <doc_id>18</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Schroeder et al. (2009) presented the use of word lattices for multi-source translation, in which the multiple source input texts are compiled into a compact lattice, over which a single decoding pass is then performed.</text>
              <doc_id>19</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This lattice-based method achieved positive results across all data conditions.</text>
              <doc_id>20</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, we propose a novel method using paraphrases to facilitate translation, especially for resource-limited languages.</text>
              <doc_id>21</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Our method does not distinguish unknown words in the input sentence, but uses paraphrases of all possible words and phrases in the source input sentence to build a source-side lattice to provide a diverse and flexible list of source-side candidates to the SMT decoder so that it can search for a best path and deliver the translation with the highest probability.</text>
              <doc_id>22</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In this case, we neither need to change the phrase table, nor add new features in the log-linear model, nor add new sentences in the training data.</text>
              <doc_id>23</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The remainder of this paper is organised as follows.</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Section 2, we define the &#8220;translation difficulty&#8221; from the perspective of the source side, and then examine how well the test set is covered by the phrase table and the parallel training data .</text>
              <doc_id>25</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Section 3 describes our paraphrase lattice method and discusses how to set the weights for the edges in the lattice network.</text>
              <doc_id>26</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Section 4, we report comparative experiments conducted on small, medium and largescale English-to-Chinese data sets.</text>
              <doc_id>27</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In Section 5, we analyse the influence of our paraphrase lattice method.</text>
              <doc_id>28</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Section 6 concludes and gives avenues for future work.</text>
              <doc_id>29</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 What Makes Translation Difficult?</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Translation Difficulty</title>
            <text>We use the term &#8220;translation difficulty&#8221; to explain how difficult it is to translate the source-side sentence in three respects:
&#8226; The OOV rates of the source sentences in the test set (Callison-Burch et al., 2006).
&#8226; Translatability of a known phrase in the input
sentence. Some particular grammatical structures on the source side cannot be directly translated into the corresponding structures on the target side. Nakov (2008) presents an example showing how hard it is to translate an English construction into Spanish. Assume that an English-to-Spanish SMT system has an entry in its phrase table for &#8220;inequality of income&#8221;, but not for &#8220;income inequality&#8221;. He argues that the latter phrase is hard to translate into Spanish where noun compounds are rare: the correct translation in this case requires a suitable Spanish preposition and a reordering, which are hard for the system to realize properly in the target language (Nakov, 2008).
&#8226; Consistency between the reference and the target-side sentence in the training corpus. Nakov (2008) points out that if the target-side sentence in the parallel corpus is inconsistent with the reference of the test set, then in some cases, a test sentence might contain pieces that are equivalent, but syntactically different from the phrases learned in training, which might result in practice in a missed opportunity for a high-quality translation. In this case, if we use paraphrases for these pieces of text, then we might improve the opportunity for the translation to approach the reference, especially in the case where only one reference is available.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We use the term &#8220;translation difficulty&#8221; to explain how difficult it is to translate the source-side sentence in three respects:</text>
                  <doc_id>31</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; The OOV rates of the source sentences in the test set (Callison-Burch et al., 2006).</text>
                  <doc_id>32</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Translatability of a known phrase in the input</text>
                  <doc_id>33</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>sentence.</text>
                  <doc_id>34</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Some particular grammatical structures on the source side cannot be directly translated into the corresponding structures on the target side.</text>
                  <doc_id>35</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Nakov (2008) presents an example showing how hard it is to translate an English construction into Spanish.</text>
                  <doc_id>36</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Assume that an English-to-Spanish SMT system has an entry in its phrase table for &#8220;inequality of income&#8221;, but not for &#8220;income inequality&#8221;.</text>
                  <doc_id>37</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>He argues that the latter phrase is hard to translate into Spanish where noun compounds are rare: the correct translation in this case requires a suitable Spanish preposition and a reordering, which are hard for the system to realize properly in the target language (Nakov, 2008).</text>
                  <doc_id>38</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Consistency between the reference and the target-side sentence in the training corpus.</text>
                  <doc_id>39</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Nakov (2008) points out that if the target-side sentence in the parallel corpus is inconsistent with the reference of the test set, then in some cases, a test sentence might contain pieces that are equivalent, but syntactically different from the phrases learned in training, which might result in practice in a missed opportunity for a high-quality translation.</text>
                  <doc_id>40</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In this case, if we use paraphrases for these pieces of text, then we might improve the opportunity for the translation to approach the reference, especially in the case where only one reference is available.</text>
                  <doc_id>41</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Coverage</title>
            <text>As to the first aspect &#8211; coverage &#8211; we argue that the coverage rate of the new words or unknown words are more and more becoming a &#8220;bottleneck&#8221; for resource-limited languages. Furthermore, current SMT systems, either phrase-based (Koehn et al., 2003; Chiang, 2005) or syntax-based (Zollmann and Venugopal, 2006), use phrases as the fundamental translation unit, so how much the phrase table and training data can cover the test set is an important factor which influences the translation quality. Table 1 shows the statistics of the coverage of the test set on English-to-Chinese FBIS data, where we can see that the coverage of unigrams is very high, especially when the data is increased to the medium size (200K), where unigram coverage is greater than 90%. Based on the observations of the unknown un-
igrams, we found that most are named entities (NEs) such as person name, location name, etc. From the bigram phrases, the coverage rates begin to significantly decline. It can also be seen that phrases containing more than 5 words rarely appear either in the phrase table or in the parallel corpus, which indicates that data sparseness is severe for long phrases. Even if the size of the corpus is significantly increased (e.g. from 20K to 200K), the coverage of long phrases is still quite low.
With respect to these three aspects of the translation difficulty, especially for data-limited language pairs, we propose a more effective method to make use of the paraphrases to facilitate translation process.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>As to the first aspect &#8211; coverage &#8211; we argue that the coverage rate of the new words or unknown words are more and more becoming a &#8220;bottleneck&#8221; for resource-limited languages.</text>
                  <doc_id>42</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Furthermore, current SMT systems, either phrase-based (Koehn et al., 2003; Chiang, 2005) or syntax-based (Zollmann and Venugopal, 2006), use phrases as the fundamental translation unit, so how much the phrase table and training data can cover the test set is an important factor which influences the translation quality.</text>
                  <doc_id>43</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Table 1 shows the statistics of the coverage of the test set on English-to-Chinese FBIS data, where we can see that the coverage of unigrams is very high, especially when the data is increased to the medium size (200K), where unigram coverage is greater than 90%.</text>
                  <doc_id>44</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Based on the observations of the unknown un-</text>
                  <doc_id>45</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>igrams, we found that most are named entities (NEs) such as person name, location name, etc. From the bigram phrases, the coverage rates begin to significantly decline.</text>
                  <doc_id>46</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It can also be seen that phrases containing more than 5 words rarely appear either in the phrase table or in the parallel corpus, which indicates that data sparseness is severe for long phrases.</text>
                  <doc_id>47</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Even if the size of the corpus is significantly increased (e.g. from 20K to 200K), the coverage of long phrases is still quite low.</text>
                  <doc_id>48</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>With respect to these three aspects of the translation difficulty, especially for data-limited language pairs, we propose a more effective method to make use of the paraphrases to facilitate translation process.</text>
                  <doc_id>49</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Paraphrase Lattice for Input Sentences</title>
        <text>In this Section, we propose a novel method to employ paraphrases to reduce the translation difficulty and in so doing increase the translation quality.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this Section, we propose a novel method to employ paraphrases to reduce the translation difficulty and in so doing increase the translation quality.</text>
              <doc_id>50</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Motivation</title>
            <text>Our idea to build a paraphrase lattice for SMT is inspired by the following points:
&#8226; Handling unknown words is a challenging issue for SMT, and using paraphrases is an effective way to facilitate this problem (Callison-Burch et al., 2006);
&#8226; The method of paraphrase substitution does not show any significant improvement, especially on a large-scale data set in terms of BLEU (Papineni et al., 2002) scores (Callison-Burch et al., 2006);
&#8226; Building a paraphrase lattice might provide more translation options to the decoder so that it can flexibly search for the best path.
The major contributions of our method are:
&#8226; We consider all N-gram phrases rather than only unknown phrases in the test set, where {1 &lt;= N &lt;= 10};
&#8226; We utilise lattices rather than simple substitution to facilitate the translation process;
&#8226; We propose an empirical weight estimation method to set weights for edges in the word lattice, which is detailed in Section 3.4.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Our idea to build a paraphrase lattice for SMT is inspired by the following points:</text>
                  <doc_id>51</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Handling unknown words is a challenging issue for SMT, and using paraphrases is an effective way to facilitate this problem (Callison-Burch et al., 2006);</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; The method of paraphrase substitution does not show any significant improvement, especially on a large-scale data set in terms of BLEU (Papineni et al., 2002) scores (Callison-Burch et al., 2006);</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Building a paraphrase lattice might provide more translation options to the decoder so that it can flexibly search for the best path.</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The major contributions of our method are:</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; We consider all N-gram phrases rather than only unknown phrases in the test set, where {1 &lt;= N &lt;= 10};</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; We utilise lattices rather than simple substitution to facilitate the translation process;</text>
                  <doc_id>57</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; We propose an empirical weight estimation method to set weights for edges in the word lattice, which is detailed in Section 3.4.</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Paraphrase Acquisition</title>
            <text>Paraphrases are alternative ways to express the same or similar meaning given a certain original word, phrase or segment. The paraphrases used in our method are generated from the parallel corpora based on the algorithm in (Bannard and Callison- Burch, 2005), in which paraphrases are identified
by pivoting through phrases in another language. In this algorithm, the foreign language translations of an English phrase are identified, all occurrences of those foreign phrases are found, and all English phrases that they translate as are treated as potential paraphrases of the original English phrase (Callison- Burch et al., 2006). A paraphrase has a probability p(e 2 |e 1 ) which is defined as in (2):
p(e 2 |e 1 ) = &#8721; f p(f|e 1 )p(e 2 |f) (1)
where the probability p(f|e 1 ) is the probability that the original English phrase e 1 translates as a particular phrase f in the other language, and p(e 2 |f) is the probability that the candidate paraphrase e 2 translates as the foreign language phrase.
p(e 2 |f) and p(f|e 1 ) are defined as the translation probabilities which can be calculated straightforwardly using maximum likelihood estimation by counting how often the phrases e and f are aligned in the parallel corpus as in (2) and (3):
p(e 2 |f) &#8776; count(e 2, f) &#8721;
e 2
count(e 2 , f)
p(f|e 1 ) &#8776; count(f, e 1) &#8721;
f count(f, e 1)</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Paraphrases are alternative ways to express the same or similar meaning given a certain original word, phrase or segment.</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The paraphrases used in our method are generated from the parallel corpora based on the algorithm in (Bannard and Callison- Burch, 2005), in which paraphrases are identified</text>
                  <doc_id>60</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>by pivoting through phrases in another language.</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this algorithm, the foreign language translations of an English phrase are identified, all occurrences of those foreign phrases are found, and all English phrases that they translate as are treated as potential paraphrases of the original English phrase (Callison- Burch et al., 2006).</text>
                  <doc_id>62</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>A paraphrase has a probability p(e 2 |e 1 ) which is defined as in (2):</text>
                  <doc_id>63</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(e 2 |e 1 ) = &#8721; f p(f|e 1 )p(e 2 |f) (1)</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where the probability p(f|e 1 ) is the probability that the original English phrase e 1 translates as a particular phrase f in the other language, and p(e 2 |f) is the probability that the candidate paraphrase e 2 translates as the foreign language phrase.</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(e 2 |f) and p(f|e 1 ) are defined as the translation probabilities which can be calculated straightforwardly using maximum likelihood estimation by counting how often the phrases e and f are aligned in the parallel corpus as in (2) and (3):</text>
                  <doc_id>66</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(e 2 |f) &#8776; count(e 2, f) &#8721;</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e 2</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>count(e 2 , f)</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(f|e 1 ) &#8776; count(f, e 1) &#8721;</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f count(f, e 1)</text>
                  <doc_id>71</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Construction of Paraphrase Lattice</title>
            <text>(2)
(3)
To present paraphrase options to the PB-SMT decoder, lattices with paraphrase options are constructed to enrich the source-language sentences. The construction process takes advantage of the correspondence between detected paraphrases and positions of the original words in the input sentence, then creates extra edges in the lattices to allow the decoder to consider paths involving the paraphrase words.
An toy example is illustrated in Figure 1: given a sequence of words {w 1 , . . . , w N } as the input, two phrases &#945; = {&#945; 1 , . . . , &#945; p } and &#946; = {&#946; 1 , . . . , &#946; q } are detected as paraphrases for S 1 = {w x , . . . , w y } (1 &#8804; x &#8804; y &#8804; N) and S 2 = {w m , . . . , w n } (1 &#8804; m &#8804; n &#8804; N) respectively. The following steps are taken to transform them into word lattices:
1. Transform the original source sentence into word lattices. N + 1 nodes (&#952; k , 0 &#8804; k &#8804; N)
&#65533;
Source side 1 &#65533; 2 &#8230; &#65533; p sentence
... ... w x ... ... w y
... w n ...
Generated lattice
Paraphrase A
Paraphrase B
w m
&#65533; 1 &#65533; 2 ...
&#65533; 1
&#65533; 1 &#65533; 2 &#8230; &#65533; q
&#65533; 2
&#65533; p
w x ... w m
... w y ... w n
are created, and N edges (referred to as &#8220;ORG- E&#8221; edges) labeled with w i (1 &#8804; i &#8804; N) are generated to connect them sequentially.
2. Generate extra nodes and edges for each of the paraphrases. Taking &#945; as an example, firstly, p &#8722; 1 nodes are created, and then p edges (referred as &#8220;NEW-E&#8221; edges) labeled with &#945; j (1 &#8804; j &#8804; p) are generated to connect node &#952; x&#8722;1 , p &#8722; 1 nodes and &#952; y&#8722;1 .
Via step 2, word lattices are generated by adding new nodes and edges coming from paraphrases. Note that to build word lattices, paraphrases with multi-words are broken into word sequences, and each of the words produces one extra edge in the word lattices as shown in the bottom part in Figure 1. Figure 2 shows an example of constructing the word lattice for an input sentence which is from the test set used in our experiments. 1 The top part in Figure 2 represents nodes (double-line circles) and edges (solid lines) that are constructed by the original words from the input sentence, while the bottom part in Figure 2 indicates the final word lattice with the addition of new nodes (single-line circles) and new edges (dashed lines) which come from the paraphrases. We can see that the paraphrase lattice increases the diversity of the source phrases so that it can provide more flexible translation options during the decoding process.
1 Figure 2 contains paths that are duplicates except for the
weights. We plan to handle this in future work.
...
&#65533; q
...</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>(2)</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(3)</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To present paraphrase options to the PB-SMT decoder, lattices with paraphrase options are constructed to enrich the source-language sentences.</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The construction process takes advantage of the correspondence between detected paraphrases and positions of the original words in the input sentence, then creates extra edges in the lattices to allow the decoder to consider paths involving the paraphrase words.</text>
                  <doc_id>75</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>An toy example is illustrated in Figure 1: given a sequence of words {w 1 , .</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>77</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>78</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>, w N } as the input, two phrases &#945; = {&#945; 1 , .</text>
                  <doc_id>79</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>80</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>81</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>, &#945; p } and &#946; = {&#946; 1 , .</text>
                  <doc_id>82</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>83</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>84</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>, &#946; q } are detected as paraphrases for S 1 = {w x , .</text>
                  <doc_id>85</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>86</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>87</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
                <sentence>
                  <text>, w y } (1 &#8804; x &#8804; y &#8804; N) and S 2 = {w m , .</text>
                  <doc_id>88</doc_id>
                  <sec_id>12</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>89</doc_id>
                  <sec_id>13</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>90</doc_id>
                  <sec_id>14</sec_id>
                </sentence>
                <sentence>
                  <text>, w n } (1 &#8804; m &#8804; n &#8804; N) respectively.</text>
                  <doc_id>91</doc_id>
                  <sec_id>15</sec_id>
                </sentence>
                <sentence>
                  <text>The following steps are taken to transform them into word lattices:</text>
                  <doc_id>92</doc_id>
                  <sec_id>16</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1.</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Transform the original source sentence into word lattices.</text>
                  <doc_id>94</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>N + 1 nodes (&#952; k , 0 &#8804; k &#8804; N)</text>
                  <doc_id>95</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533;</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Source side 1 &#65533; 2 &#8230; &#65533; p sentence</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>... ... w x ... ... w y</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>... w n ...</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Generated lattice</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Paraphrase A</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Paraphrase B</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w m</text>
                  <doc_id>103</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533; 1 &#65533; 2 ...</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533; 1</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533; 1 &#65533; 2 &#8230; &#65533; q</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533; 2</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533; p</text>
                  <doc_id>108</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w x ... w m</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>... w y ... w n</text>
                  <doc_id>110</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>are created, and N edges (referred to as &#8220;ORG- E&#8221; edges) labeled with w i (1 &#8804; i &#8804; N) are generated to connect them sequentially.</text>
                  <doc_id>111</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Generate extra nodes and edges for each of the paraphrases.</text>
                  <doc_id>113</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Taking &#945; as an example, firstly, p &#8722; 1 nodes are created, and then p edges (referred as &#8220;NEW-E&#8221; edges) labeled with &#945; j (1 &#8804; j &#8804; p) are generated to connect node &#952; x&#8722;1 , p &#8722; 1 nodes and &#952; y&#8722;1 .</text>
                  <doc_id>114</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Via step 2, word lattices are generated by adding new nodes and edges coming from paraphrases.</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Note that to build word lattices, paraphrases with multi-words are broken into word sequences, and each of the words produces one extra edge in the word lattices as shown in the bottom part in Figure 1.</text>
                  <doc_id>116</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 2 shows an example of constructing the word lattice for an input sentence which is from the test set used in our experiments.</text>
                  <doc_id>117</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>1 The top part in Figure 2 represents nodes (double-line circles) and edges (solid lines) that are constructed by the original words from the input sentence, while the bottom part in Figure 2 indicates the final word lattice with the addition of new nodes (single-line circles) and new edges (dashed lines) which come from the paraphrases.</text>
                  <doc_id>118</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We can see that the paraphrase lattice increases the diversity of the source phrases so that it can provide more flexible translation options during the decoding process.</text>
                  <doc_id>119</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 Figure 2 contains paths that are duplicates except for the</text>
                  <doc_id>120</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>weights.</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We plan to handle this in future work.</text>
                  <doc_id>122</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>...</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533; q</text>
                  <doc_id>124</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>...</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>3.4 Weight Estimation</title>
            <text>Estimating and normalising the weight for each edge in the word lattice is a challenging issue when the edges come from different sources. In this section, we propose an empirical method to set the weights for the edges by distinguishing the original (&#8220;ORG- E&#8221;) and new (&#8220;NEW-E&#8221;) edges in the lattices. The aim is to utilize the original sentences as the references to weight the edges from paraphrases, so that decoding paths going through &#8220;ORG-E&#8221; edges will tend to have higher scores than those which use &#8220;NEW-E&#8221; ones. The assumption behind this is that the paraphrases are alternatives for the original sentences, so decoding paths going though them ought to be penalised.
Therefore, for all the &#8220;ORG-E&#8221; edges, their weights in the lattice are set to 1.0 as the reference. Thus, in the log-linear model, decoding paths going though these edges are not penalised because they do not come from the paraphrases.
By contrast, &#8220;NEW-E&#8221; are divided into two groups for the calculation of weights:
&#8226; For &#8220;NEW-E&#8221; edges which are outgoing edges of the lattice nodes that come from the original sentences, the probabilities p(e s |e i ) 2 of their
2 e s indicates the source phrase S, e i represents one of the
corresponding paraphrases are utilised to produce empirical weights. Supposing that a set of paraphrases X = {x 1 , . . . , x k } start at node A which comes from the original sentence, so that X are sorted descendingly based on the probabilities p(e s |e i ), their corresponding edges for node A are G = {g 1 , . . . , g k }, then the weights are calculated as in (4):
w(e i ) = 1 k + i (1 &lt;= i &lt;= k) (4)
where k is a predefined parameter to trade off between decoding speed and the number of potential paraphrases being considered. Thus, once a decoding path goes though one of these edges, it will be penalised according to its paraphrase probabilities.
&#8226; For all other &#8220;NEW-E&#8221; edges, their weights are set to 1.0, because the paraphrase penalty has been counted in their preceding &#8220;NEW-E&#8221; edges.
Figure 2 illustrates the weight estimation results. Nodes coming from the original sentences are drawn in double-line circles (e.g. nodes 0 to 7), while
paraphrases of S.
nodes created from paraphrases are shown in singleline circles (e.g. nodes 8 to 10). &#8220;ORG-E&#8221; edges are drawn in solid lines and &#8220;NEW-E&#8221; edges are shown using dashed lines. As specified previously, &#8220;ORG- E&#8221; edges are all weighted by 1.0 (e.g. edge labeled &#8220;the&#8221; from node 0 to 1). By contrast, &#8220;NEW-E&#8221; edges in the first group are weighted by equation (4) (e.g. edges in dashed lines start from node 0 to node 2 and 8), while others in the second group are weighted by 1.0 (e.g. edge labeled &#8220;training&#8221; from node 8 to 2). Note that penalties of the paths going through paraphrases are counted by equation (4), which is represented by the weights of &#8220;NEW- E&#8221; edges in the first group. For example, starting from node 2, paths going to node 9 and 10 are penalised because lattice weights are also considered in the log-linear model. However, other edges do not imply penalties since their weights are set to 1.0.
The reason to set all weights for the &#8220;ORG-E&#8221; edges to a uniform weight (e.g. 1.0) instead of a lower empirical weight is to avoid excessive penalties for the original words. For example, in Figure 2, the original edge from node 3 to 4 (continue) has a weight of 1.0, so the paths going though the original edges from node 2 to 4 (will continue) have a higher lattice score (1.0 &#215; 1.0 = 1.0) than the paths going through the edges of paraphrases (e.g. will resume (score: 0.125 &#215; 1.0 = 0.125) and will go (score: 0.11 &#215; 1.0 = 0.11)), or any other mixed paths that goes through original edges and paraphrase edges, such as will continuous (score: 1.0 &#215; 0.125 = 0.125). The point is that we should have more trust when translating the original words, but if we penalise (set weights &lt; 1.0) the &#8220;ORG- E&#8221; edges whenever there is a paraphrase for them, then when considering the context of the lattice, paraphrases will be favoured systematically. That is why we just penalise the &#8220;NEW-E&#8221; edges in the first group and set other weights to 1.0.
As to unknown words in the input sentence, even if we give them a prioritised weight, they would be severely penalised in the decoding process. So we do not need to distinguish unknown words when building and weighting the paraphrase lattice.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Estimating and normalising the weight for each edge in the word lattice is a challenging issue when the edges come from different sources.</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this section, we propose an empirical method to set the weights for the edges by distinguishing the original (&#8220;ORG- E&#8221;) and new (&#8220;NEW-E&#8221;) edges in the lattices.</text>
                  <doc_id>127</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The aim is to utilize the original sentences as the references to weight the edges from paraphrases, so that decoding paths going through &#8220;ORG-E&#8221; edges will tend to have higher scores than those which use &#8220;NEW-E&#8221; ones.</text>
                  <doc_id>128</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The assumption behind this is that the paraphrases are alternatives for the original sentences, so decoding paths going though them ought to be penalised.</text>
                  <doc_id>129</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Therefore, for all the &#8220;ORG-E&#8221; edges, their weights in the lattice are set to 1.0 as the reference.</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Thus, in the log-linear model, decoding paths going though these edges are not penalised because they do not come from the paraphrases.</text>
                  <doc_id>131</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>By contrast, &#8220;NEW-E&#8221; are divided into two groups for the calculation of weights:</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; For &#8220;NEW-E&#8221; edges which are outgoing edges of the lattice nodes that come from the original sentences, the probabilities p(e s |e i ) 2 of their</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 e s indicates the source phrase S, e i represents one of the</text>
                  <doc_id>134</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>corresponding paraphrases are utilised to produce empirical weights.</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Supposing that a set of paraphrases X = {x 1 , .</text>
                  <doc_id>136</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>137</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>138</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>, x k } start at node A which comes from the original sentence, so that X are sorted descendingly based on the probabilities p(e s |e i ), their corresponding edges for node A are G = {g 1 , .</text>
                  <doc_id>139</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>140</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>141</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>, g k }, then the weights are calculated as in (4):</text>
                  <doc_id>142</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w(e i ) = 1 k + i (1 &lt;= i &lt;= k) (4)</text>
                  <doc_id>143</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where k is a predefined parameter to trade off between decoding speed and the number of potential paraphrases being considered.</text>
                  <doc_id>144</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Thus, once a decoding path goes though one of these edges, it will be penalised according to its paraphrase probabilities.</text>
                  <doc_id>145</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; For all other &#8220;NEW-E&#8221; edges, their weights are set to 1.0, because the paraphrase penalty has been counted in their preceding &#8220;NEW-E&#8221; edges.</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 2 illustrates the weight estimation results.</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Nodes coming from the original sentences are drawn in double-line circles (e.g. nodes 0 to 7), while</text>
                  <doc_id>148</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>paraphrases of S.</text>
                  <doc_id>149</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>nodes created from paraphrases are shown in singleline circles (e.g. nodes 8 to 10).</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>&#8220;ORG-E&#8221; edges are drawn in solid lines and &#8220;NEW-E&#8221; edges are shown using dashed lines.</text>
                  <doc_id>151</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As specified previously, &#8220;ORG- E&#8221; edges are all weighted by 1.0 (e.g. edge labeled &#8220;the&#8221; from node 0 to 1).</text>
                  <doc_id>152</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>By contrast, &#8220;NEW-E&#8221; edges in the first group are weighted by equation (4) (e.g. edges in dashed lines start from node 0 to node 2 and 8), while others in the second group are weighted by 1.0 (e.g. edge labeled &#8220;training&#8221; from node 8 to 2).</text>
                  <doc_id>153</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Note that penalties of the paths going through paraphrases are counted by equation (4), which is represented by the weights of &#8220;NEW- E&#8221; edges in the first group.</text>
                  <doc_id>154</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For example, starting from node 2, paths going to node 9 and 10 are penalised because lattice weights are also considered in the log-linear model.</text>
                  <doc_id>155</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>However, other edges do not imply penalties since their weights are set to 1.0.</text>
                  <doc_id>156</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The reason to set all weights for the &#8220;ORG-E&#8221; edges to a uniform weight (e.g. 1.0) instead of a lower empirical weight is to avoid excessive penalties for the original words.</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For example, in Figure 2, the original edge from node 3 to 4 (continue) has a weight of 1.0, so the paths going though the original edges from node 2 to 4 (will continue) have a higher lattice score (1.0 &#215; 1.0 = 1.0) than the paths going through the edges of paraphrases (e.g. will resume (score: 0.125 &#215; 1.0 = 0.125) and will go (score: 0.11 &#215; 1.0 = 0.11)), or any other mixed paths that goes through original edges and paraphrase edges, such as will continuous (score: 1.0 &#215; 0.125 = 0.125).</text>
                  <doc_id>158</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The point is that we should have more trust when translating the original words, but if we penalise (set weights &lt; 1.0) the &#8220;ORG- E&#8221; edges whenever there is a paraphrase for them, then when considering the context of the lattice, paraphrases will be favoured systematically.</text>
                  <doc_id>159</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>That is why we just penalise the &#8220;NEW-E&#8221; edges in the first group and set other weights to 1.0.</text>
                  <doc_id>160</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As to unknown words in the input sentence, even if we give them a prioritised weight, they would be severely penalised in the decoding process.</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>So we do not need to distinguish unknown words when building and weighting the paraphrase lattice.</text>
                  <doc_id>162</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Experiments</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>163</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 System and Data Preparation</title>
            <text>For our experiments, we use Moses (Koehn et al., 2007) as the baseline system which can support lattice decoding. We also realise a paraphrase substitution-based system (Para-Sub) 3 based on the method in (Callison-Burch, 2006) to compare with the baseline system and our proposed paraphrase lattice-based (Lattice) system. The alignment is carried out by GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the grow-diag-final heuristic. The maximum phrase length is 10 words. Parameter tuning is performed using Minimum Error Rate Training (Och, 2003).
The experiments are conducted on English-to- Chinese translation. In order to fully compare our proposed method with the baseline and the &#8220;Para- Sub&#8221; system, we perform the experiments on three different sizes of training data: 20K, 200K and 2.1 million pairs of sentences. The former two sizes of data are derived from FBIS, 4 and the latter size of data consists of part of HK parallel corpus, 5 ISI parallel data, 6 other news data and parallel dictionaries from LDC. All the language models are 5-gram which are trained on the monolingual part of parallel data.
The development set (devset) and the test set for experiments using 20K and 200K data sets are randomly extracted from the FBIS data. Each set includes 1,200 sentences and each source sentence has one reference. For the 2.1 million data set, we use a different devset and test set in order to verify whether our proposed method can work on a language pair with sufficient resources. The devset is the NIST 2005 Chinese-English current set which has only one reference for each source sentence and the test set is the NIST 2003 English-to-Chinese current set which contains four references for each source sentence. All results are reported in BLEU and TER (Snover et al., 2006) scores.
3 We use &#8220;Para-Sub&#8221; to represent their system in the rest of
this paper. 4 This is a multilingual paragraph-aligned corpus with LDC
resource number LDC2003E14. 5 LDC number: LDC2004T08. 6 LDC number: LDC2007T09.
The paraphrase data set used in our lattice-based and the &#8220;Para-Sub&#8221; systems is same which is derived from the &#8220;Paraphrase Phrase Table&#8221; 7 of TER- Plus (Snover et al., 2009). The parameter k in equation 4 is set to 7.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>For our experiments, we use Moses (Koehn et al., 2007) as the baseline system which can support lattice decoding.</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We also realise a paraphrase substitution-based system (Para-Sub) 3 based on the method in (Callison-Burch, 2006) to compare with the baseline system and our proposed paraphrase lattice-based (Lattice) system.</text>
                  <doc_id>165</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The alignment is carried out by GIZA++ (Och and Ney, 2003) and then we symmetrized the word alignment using the grow-diag-final heuristic.</text>
                  <doc_id>166</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The maximum phrase length is 10 words.</text>
                  <doc_id>167</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Parameter tuning is performed using Minimum Error Rate Training (Och, 2003).</text>
                  <doc_id>168</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The experiments are conducted on English-to- Chinese translation.</text>
                  <doc_id>169</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In order to fully compare our proposed method with the baseline and the &#8220;Para- Sub&#8221; system, we perform the experiments on three different sizes of training data: 20K, 200K and 2.1 million pairs of sentences.</text>
                  <doc_id>170</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The former two sizes of data are derived from FBIS, 4 and the latter size of data consists of part of HK parallel corpus, 5 ISI parallel data, 6 other news data and parallel dictionaries from LDC.</text>
                  <doc_id>171</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>All the language models are 5-gram which are trained on the monolingual part of parallel data.</text>
                  <doc_id>172</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The development set (devset) and the test set for experiments using 20K and 200K data sets are randomly extracted from the FBIS data.</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Each set includes 1,200 sentences and each source sentence has one reference.</text>
                  <doc_id>174</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For the 2.1 million data set, we use a different devset and test set in order to verify whether our proposed method can work on a language pair with sufficient resources.</text>
                  <doc_id>175</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The devset is the NIST 2005 Chinese-English current set which has only one reference for each source sentence and the test set is the NIST 2003 English-to-Chinese current set which contains four references for each source sentence.</text>
                  <doc_id>176</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>All results are reported in BLEU and TER (Snover et al., 2006) scores.</text>
                  <doc_id>177</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3 We use &#8220;Para-Sub&#8221; to represent their system in the rest of</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>this paper.</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>4 This is a multilingual paragraph-aligned corpus with LDC</text>
                  <doc_id>180</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>resource number LDC2003E14.</text>
                  <doc_id>181</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>5 LDC number: LDC2004T08.</text>
                  <doc_id>182</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>6 LDC number: LDC2007T09.</text>
                  <doc_id>183</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The paraphrase data set used in our lattice-based and the &#8220;Para-Sub&#8221; systems is same which is derived from the &#8220;Paraphrase Phrase Table&#8221; 7 of TER- Plus (Snover et al., 2009).</text>
                  <doc_id>184</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The parameter k in equation 4 is set to 7.</text>
                  <doc_id>185</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Paraphrase Filtering</title>
            <text>The more edges there are in a lattice, the more complicated the decoding is in the search process. Therefore, in order to reduce the complexity of the lattice and increase decoding speed, we must filter out some potential noise in the paraphrase table. Two measures are taken to optimise the paraphrases when building a paraphrase lattice:
&#8226; Firstly, we filter out all the paraphrases whose probability is less than 0.01;
&#8226; Secondly, given a source-side input sentence, we retrieve all possible paraphrases and their probabilities for source-side phrases which appear in the paraphrase table. Then we remove the paraphrases which are not occurred in the &#8220;phrase table&#8221; of the SMT system. This measure intends to avoid adding new &#8220;unknown words&#8221; to the source-side sentence. After this measure, we can acquire the final paraphrases which can be denoted as a quadruple &lt; SEN ID, Span, P ara, P rob &gt;, where &#8220;SEN ID&#8221; indicates the ID of the input sentence, &#8220;Span&#8221; represents the span of the sourceside phrase in the original input sentence, &#8220;Para&#8221; indicates the paraphrase of the sourceside phrase, and &#8220;Prob&#8221; represents the probability between the source-side phrase and its paraphrase, which is used to set the weight of the edge in the lattice. The quadruple is used to construct the weighted lattice.
7 http://www.umiacs.umd.edu/&#732;snover/terp/
downloads/terp-pt.v1.tgz.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The more edges there are in a lattice, the more complicated the decoding is in the search process.</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, in order to reduce the complexity of the lattice and increase decoding speed, we must filter out some potential noise in the paraphrase table.</text>
                  <doc_id>187</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Two measures are taken to optimise the paraphrases when building a paraphrase lattice:</text>
                  <doc_id>188</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Firstly, we filter out all the paraphrases whose probability is less than 0.01;</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Secondly, given a source-side input sentence, we retrieve all possible paraphrases and their probabilities for source-side phrases which appear in the paraphrase table.</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Then we remove the paraphrases which are not occurred in the &#8220;phrase table&#8221; of the SMT system.</text>
                  <doc_id>191</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This measure intends to avoid adding new &#8220;unknown words&#8221; to the source-side sentence.</text>
                  <doc_id>192</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>After this measure, we can acquire the final paraphrases which can be denoted as a quadruple &lt; SEN ID, Span, P ara, P rob &gt;, where &#8220;SEN ID&#8221; indicates the ID of the input sentence, &#8220;Span&#8221; represents the span of the sourceside phrase in the original input sentence, &#8220;Para&#8221; indicates the paraphrase of the sourceside phrase, and &#8220;Prob&#8221; represents the probability between the source-side phrase and its paraphrase, which is used to set the weight of the edge in the lattice.</text>
                  <doc_id>193</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The quadruple is used to construct the weighted lattice.</text>
                  <doc_id>194</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>7 http://www.umiacs.umd.edu/&#732;snover/terp/</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>downloads/terp-pt.v1.tgz.</text>
                  <doc_id>196</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>4.3 Experimental Results</title>
            <text>The experimental results conducted on small and medium-sized data sets are shown in Table 2. The 95% confidence intervals (CI) for BLEU scores are independently computed on each of three systems, while the &#8220;pair-CI 95%&#8221; are computed relative to the baseline system only for &#8220;Para-Sub&#8221; and &#8220;Lattice&#8221; systems. All the significance tests use bootstrap and paired-bootstrap resampling normal approximation methods (Zhang and Vogel, 2004). 8 Improvements are considered to be significant if the left boundary of the confidence interval is larger than zero in terms of the &#8220;pair-CI 95%&#8221;. It can be seen that 1) our &#8220;Lattice&#8221; system outperforms the baseline by 1.02 and 1.6 absolute (7.07% and 6.78% relative) BLEU points in terms of the 20K and 200K data sets respectively, and our system also decreases the TER scores by 2.24 and 1.19 (2.97% and 1.87% relative) points than the baseline system. In terms of the &#8220;pair-CI 95%&#8221;, the left boundaries for 20K and 200K data are respectively &#8220;+0.74&#8221; and &#8220;+1.19&#8221;, which indicate that the &#8220;Lattice&#8221; system is significantly better than the baseline system on these two data sets. 2) The &#8220;Para-Sub&#8221; system performs slightly better (0.36 absolute BLEU points) than the baseline system on the 20K data set, but slightly worse (0.19 absolute BLEU points) than the baseline on the 200K data set, which indicates that the paraphrase substitution method used in (Callison-Burch et al., 2006) does not work on resource-sufficient data sets. In terms of the &#8220;pair-CI 95%&#8221;, the left boundary for 20K data is &#8220;+0.13&#8221;, which indicates that it is significantly better than the baseline system, while the left boundary is &#8220;-0.46&#8221; for 200K data, which indicates that the &#8220;Para-Sub&#8221; system is significantly worse than the baseline system. 3) comparing the &#8220;Lattice&#8221; system with the &#8220;Para-Sub&#8221;
system, the &#8220;pair-CI 95%&#8221; for 20K and 200K data are respectively [+0.41, +0.92] and [+1.40, +2.17], which indicates that the &#8220;Lattice&#8221; system is significantly better than the &#8220;Para-Sub&#8221; system on these two data sets as well. 4) In terms of the two metrics, our proposed method achieves the best performance, which shows that our method is effective and consistent on different sizes of data. In order to verify our method on large-scale data, we also perform experiments on 2.1 million sentence-pairs of English-to-Chinese data as described in Section 4.1. The results are shown in Table 3. From Table 3, it can be seen that the &#8220;Lattice&#8221; system achieves an improvement of 0.51 absolute (3.63% relative) BLEU points and a decrease of 1.6 absolute (2.14% relative) TER points compared to the baseline. In terms of the &#8220;pair-CI 95%&#8221;, the left boundary for the &#8220;Lattice&#8221; system is &#8220;+0.15&#8221; which indicates that it is significantly better than the baseline system in terms of BLEU. Interestingly, in our experiment, the &#8220;Para-Sub&#8221; system also outperforms the baseline on those three automatic metrics. However, in terms of the &#8220;pair-CI 95%&#8221;, the left boundary for the &#8220;Para-Sub&#8221; system is &#8220;-0.18&#8221; which indicates that it is not significantly better than the baseline system in terms of BLEU. The results also show that our proposed method is effective and consistent even on a large-scale data set. It also can be seen that the improvement on 2.1 million sentence-pairs is less than that of the 20K and 200K data sets. That is, as the size of the training data increases, the problems of data sparseness decrease, so that the coverage of the test set by the parallel corpus will correspondingly increase. In this case, the role of paraphrases in decoding becomes a little weaker. On the other hand, it might become a kind of noise to interfere with the exact translation of the original source-side phrases when decoding. Therefore, our proposed method may be more appropriate for language pairs with limited resources.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The experimental results conducted on small and medium-sized data sets are shown in Table 2.</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The 95% confidence intervals (CI) for BLEU scores are independently computed on each of three systems, while the &#8220;pair-CI 95%&#8221; are computed relative to the baseline system only for &#8220;Para-Sub&#8221; and &#8220;Lattice&#8221; systems.</text>
                  <doc_id>198</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>All the significance tests use bootstrap and paired-bootstrap resampling normal approximation methods (Zhang and Vogel, 2004).</text>
                  <doc_id>199</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>8 Improvements are considered to be significant if the left boundary of the confidence interval is larger than zero in terms of the &#8220;pair-CI 95%&#8221;.</text>
                  <doc_id>200</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>It can be seen that 1) our &#8220;Lattice&#8221; system outperforms the baseline by 1.02 and 1.6 absolute (7.07% and 6.78% relative) BLEU points in terms of the 20K and 200K data sets respectively, and our system also decreases the TER scores by 2.24 and 1.19 (2.97% and 1.87% relative) points than the baseline system.</text>
                  <doc_id>201</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In terms of the &#8220;pair-CI 95%&#8221;, the left boundaries for 20K and 200K data are respectively &#8220;+0.74&#8221; and &#8220;+1.19&#8221;, which indicate that the &#8220;Lattice&#8221; system is significantly better than the baseline system on these two data sets.</text>
                  <doc_id>202</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>2) The &#8220;Para-Sub&#8221; system performs slightly better (0.36 absolute BLEU points) than the baseline system on the 20K data set, but slightly worse (0.19 absolute BLEU points) than the baseline on the 200K data set, which indicates that the paraphrase substitution method used in (Callison-Burch et al., 2006) does not work on resource-sufficient data sets.</text>
                  <doc_id>203</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>In terms of the &#8220;pair-CI 95%&#8221;, the left boundary for 20K data is &#8220;+0.13&#8221;, which indicates that it is significantly better than the baseline system, while the left boundary is &#8220;-0.46&#8221; for 200K data, which indicates that the &#8220;Para-Sub&#8221; system is significantly worse than the baseline system.</text>
                  <doc_id>204</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>3) comparing the &#8220;Lattice&#8221; system with the &#8220;Para-Sub&#8221;</text>
                  <doc_id>205</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>system, the &#8220;pair-CI 95%&#8221; for 20K and 200K data are respectively [+0.41, +0.92] and [+1.40, +2.17], which indicates that the &#8220;Lattice&#8221; system is significantly better than the &#8220;Para-Sub&#8221; system on these two data sets as well.</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>4) In terms of the two metrics, our proposed method achieves the best performance, which shows that our method is effective and consistent on different sizes of data.</text>
                  <doc_id>207</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In order to verify our method on large-scale data, we also perform experiments on 2.1 million sentence-pairs of English-to-Chinese data as described in Section 4.1.</text>
                  <doc_id>208</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The results are shown in Table 3.</text>
                  <doc_id>209</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>From Table 3, it can be seen that the &#8220;Lattice&#8221; system achieves an improvement of 0.51 absolute (3.63% relative) BLEU points and a decrease of 1.6 absolute (2.14% relative) TER points compared to the baseline.</text>
                  <doc_id>210</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In terms of the &#8220;pair-CI 95%&#8221;, the left boundary for the &#8220;Lattice&#8221; system is &#8220;+0.15&#8221; which indicates that it is significantly better than the baseline system in terms of BLEU.</text>
                  <doc_id>211</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Interestingly, in our experiment, the &#8220;Para-Sub&#8221; system also outperforms the baseline on those three automatic metrics.</text>
                  <doc_id>212</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>However, in terms of the &#8220;pair-CI 95%&#8221;, the left boundary for the &#8220;Para-Sub&#8221; system is &#8220;-0.18&#8221; which indicates that it is not significantly better than the baseline system in terms of BLEU.</text>
                  <doc_id>213</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>The results also show that our proposed method is effective and consistent even on a large-scale data set.</text>
                  <doc_id>214</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>It also can be seen that the improvement on 2.1 million sentence-pairs is less than that of the 20K and 200K data sets.</text>
                  <doc_id>215</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>That is, as the size of the training data increases, the problems of data sparseness decrease, so that the coverage of the test set by the parallel corpus will correspondingly increase.</text>
                  <doc_id>216</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>In this case, the role of paraphrases in decoding becomes a little weaker.</text>
                  <doc_id>217</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
                <sentence>
                  <text>On the other hand, it might become a kind of noise to interfere with the exact translation of the original source-side phrases when decoding.</text>
                  <doc_id>218</doc_id>
                  <sec_id>12</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, our proposed method may be more appropriate for language pairs with limited resources.</text>
                  <doc_id>219</doc_id>
                  <sec_id>13</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Analysis</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>220</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Coverage of Paraphrase Test Set</title>
            <text>The coverage rate of the test set by the phrase table is an important factor that could influence the translation result, so in this section we examine the characteristics of the updated test set that adds in the paraphrases. We take the 200K data set to examine the coverage issue. Table 4 is an illustration to compare the new coverage and the old coverage (without paraphrases) on medium sized training data.
From Table 4, we can see that the coverage of unigrams, bigrams, trigrams and 4-grams goes up by about 5%, 10%, 5% and 1%, while from 5-grams there is only a slight or no increase in coverage. These results show that 1) most of the paraphrases that are added in are lower-order n-grams; 2) the paraphrases can increase the coverage of the input by handling the unknown words to some extent.
However, we observed that most untranslated words in the &#8220;Para-Sub&#8221; and &#8220;Lattice&#8221; systems are still NEs, which shows that in our paraphrase table, there are few paraphrases for the NEs. Therefore, to further improve the translation quality using paraphrases, we also need to acquire the paraphrases for NEs to increase the coverage of unknown words.
Source: whether or the albanian rebels can be genuinely disarmed completely is the main challenge to nato .
Ref: &#33021; &#21542; &#30495; &#27491; &#24443; &#24213; &#22320; &#35299; &#38500; &#38463; &#26063; &#30340; &#27494; &#35013; &#26159; &#21271; &#32422; &#38754; &#20020; &#30340; &#20027; &#35201; &#25361; &#25112; &#12290;
Baseline: &#19981; &#31649; &#38463; &#21467; &#20081; &#20998; &#23376; &#25165; &#33021; &#30495; &#27491; disarmed &#23436; &#20840; &#26159; &#21271; &#32422; &#30340; &#20027; &#35201; &#25361; &#25112; &#12290; Para-Sub: &#33021; &#21542; &#38463; &#26063; &#21467; &#20081; &#20998; &#23376; &#21487; &#20197; &#30495; &#27491; &#35009; &#20891; &#23436; &#20840; &#26159; &#21271; &#32422; &#30340; &#20027; &#35201; &#25361; &#25112; &#12290; Lattice: &#33021; &#21542; &#30495; &#27491; &#38463; &#26063; &#21467; &#20081; &#20998; &#23376; &#21487; &#20197; &#23436; &#20840; &#38750; &#20891; &#20107; &#27494; &#35013; &#26159; &#21271; &#32422; &#30340; &#20027; &#35201; &#25361; &#25112; &#12290;</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The coverage rate of the test set by the phrase table is an important factor that could influence the translation result, so in this section we examine the characteristics of the updated test set that adds in the paraphrases.</text>
                  <doc_id>221</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We take the 200K data set to examine the coverage issue.</text>
                  <doc_id>222</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Table 4 is an illustration to compare the new coverage and the old coverage (without paraphrases) on medium sized training data.</text>
                  <doc_id>223</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>From Table 4, we can see that the coverage of unigrams, bigrams, trigrams and 4-grams goes up by about 5%, 10%, 5% and 1%, while from 5-grams there is only a slight or no increase in coverage.</text>
                  <doc_id>224</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These results show that 1) most of the paraphrases that are added in are lower-order n-grams; 2) the paraphrases can increase the coverage of the input by handling the unknown words to some extent.</text>
                  <doc_id>225</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>However, we observed that most untranslated words in the &#8220;Para-Sub&#8221; and &#8220;Lattice&#8221; systems are still NEs, which shows that in our paraphrase table, there are few paraphrases for the NEs.</text>
                  <doc_id>226</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, to further improve the translation quality using paraphrases, we also need to acquire the paraphrases for NEs to increase the coverage of unknown words.</text>
                  <doc_id>227</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Source: whether or the albanian rebels can be genuinely disarmed completely is the main challenge to nato .</text>
                  <doc_id>228</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Ref: &#33021; &#21542; &#30495; &#27491; &#24443; &#24213; &#22320; &#35299; &#38500; &#38463; &#26063; &#30340; &#27494; &#35013; &#26159; &#21271; &#32422; &#38754; &#20020; &#30340; &#20027; &#35201; &#25361; &#25112; &#12290;</text>
                  <doc_id>229</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Baseline: &#19981; &#31649; &#38463; &#21467; &#20081; &#20998; &#23376; &#25165; &#33021; &#30495; &#27491; disarmed &#23436; &#20840; &#26159; &#21271; &#32422; &#30340; &#20027; &#35201; &#25361; &#25112; &#12290; Para-Sub: &#33021; &#21542; &#38463; &#26063; &#21467; &#20081; &#20998; &#23376; &#21487; &#20197; &#30495; &#27491; &#35009; &#20891; &#23436; &#20840; &#26159; &#21271; &#32422; &#30340; &#20027; &#35201; &#25361; &#25112; &#12290; Lattice: &#33021; &#21542; &#30495; &#27491; &#38463; &#26063; &#21467; &#20081; &#20998; &#23376; &#21487; &#20197; &#23436; &#20840; &#38750; &#20891; &#20107; &#27494; &#35013; &#26159; &#21271; &#32422; &#30340; &#20027; &#35201; &#25361; &#25112; &#12290;</text>
                  <doc_id>230</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Analysis on Translation Results</title>
            <text>In this section, we give an example to show the effectiveness of using paraphrase lattices to deal with unknown words. The example is evaluated according to both automatic evaluation and human evaluation at sentence level.
See Figure 3 as an illustration of how the paraphrase-based systems process unknown words. According to the word alignments between the source-side sentence and the reference, the word &#8220;disarmed&#8221; is translated into two Chinese words &#8220;d d&#8221; and &#8220;dd&#8221;. These two Chinese words are discontinuous in the reference, so it is difficult for the PB-SMT system to correctly translate the single English word into a discontinuous Chinese phrase. In fact in this example, &#8220;disarmed&#8221; is an unknown word and it is kept untranslated in the result of the baseline system. In the &#8220;Para-Sub&#8221; system, it is translated into &#8220;dd&#8221; based on a paraphrase pair P P 1 = &#8220;disarmed &#8741; disarmament &#8741; 0.087&#8221; and its translation pair T 1 = &#8220;disarmament &#8741; dd&#8221;. The number &#8220;0.087&#8221; is the probability p 1 that indicates to what extent these two words are paraphrases. It can be seen that although &#8220;dd&#8221; is quite different from the meaning of &#8220;disarmed&#8221;, it is understandable for human in some sense. In the &#8220;Lattice&#8221; system, the word &#8220;disarmed&#8221; is translated into three Chinese words &#8220;d dd dd&#8221; based on a paraphrase pair P P 2 = &#8220;disarmed &#8741; demilitarized &#8741; 0.099&#8221; and its translation pair T 2 = &#8220;demilitarized &#8741; d dd d d&#8221;. The probability p 2 is slightly greater than p 1 .
We argue that the reason that the &#8220;Lattice&#8221; system selects P P 2 and T 2 rather than P P 1 and T 1 is because of the weight estimation in the lattice. That is, P P 2 is more prioritised, while P P 1 is more penalised based on equation (4).
From the viewpoint of human evaluation, the paraphrase pair P P 2 is more appropriate than P P 1 , and the translation T 2 is more similar to the original meaning than T 1 . The sentence-level automatic evaluation scores for this example in terms of BLEU and TER metrics are shown in Table 5.
The BLEU score of the &#8220;Lattice&#8221; system is much higher than the baseline, and the TER score is quite a bit lower than the baseline. Therefore, from the viewpoint of automatic evaluation, the translation from the &#8220;Lattice&#8221; system is also better than those from the baseline and &#8220;Para-Sub&#8221; systems.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In this section, we give an example to show the effectiveness of using paraphrase lattices to deal with unknown words.</text>
                  <doc_id>231</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The example is evaluated according to both automatic evaluation and human evaluation at sentence level.</text>
                  <doc_id>232</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>See Figure 3 as an illustration of how the paraphrase-based systems process unknown words.</text>
                  <doc_id>233</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>According to the word alignments between the source-side sentence and the reference, the word &#8220;disarmed&#8221; is translated into two Chinese words &#8220;d d&#8221; and &#8220;dd&#8221;.</text>
                  <doc_id>234</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>These two Chinese words are discontinuous in the reference, so it is difficult for the PB-SMT system to correctly translate the single English word into a discontinuous Chinese phrase.</text>
                  <doc_id>235</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In fact in this example, &#8220;disarmed&#8221; is an unknown word and it is kept untranslated in the result of the baseline system.</text>
                  <doc_id>236</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In the &#8220;Para-Sub&#8221; system, it is translated into &#8220;dd&#8221; based on a paraphrase pair P P 1 = &#8220;disarmed &#8741; disarmament &#8741; 0.087&#8221; and its translation pair T 1 = &#8220;disarmament &#8741; dd&#8221;.</text>
                  <doc_id>237</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The number &#8220;0.087&#8221; is the probability p 1 that indicates to what extent these two words are paraphrases.</text>
                  <doc_id>238</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>It can be seen that although &#8220;dd&#8221; is quite different from the meaning of &#8220;disarmed&#8221;, it is understandable for human in some sense.</text>
                  <doc_id>239</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>In the &#8220;Lattice&#8221; system, the word &#8220;disarmed&#8221; is translated into three Chinese words &#8220;d dd dd&#8221; based on a paraphrase pair P P 2 = &#8220;disarmed &#8741; demilitarized &#8741; 0.099&#8221; and its translation pair T 2 = &#8220;demilitarized &#8741; d dd d d&#8221;.</text>
                  <doc_id>240</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>The probability p 2 is slightly greater than p 1 .</text>
                  <doc_id>241</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We argue that the reason that the &#8220;Lattice&#8221; system selects P P 2 and T 2 rather than P P 1 and T 1 is because of the weight estimation in the lattice.</text>
                  <doc_id>242</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>That is, P P 2 is more prioritised, while P P 1 is more penalised based on equation (4).</text>
                  <doc_id>243</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>From the viewpoint of human evaluation, the paraphrase pair P P 2 is more appropriate than P P 1 , and the translation T 2 is more similar to the original meaning than T 1 .</text>
                  <doc_id>244</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The sentence-level automatic evaluation scores for this example in terms of BLEU and TER metrics are shown in Table 5.</text>
                  <doc_id>245</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The BLEU score of the &#8220;Lattice&#8221; system is much higher than the baseline, and the TER score is quite a bit lower than the baseline.</text>
                  <doc_id>246</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, from the viewpoint of automatic evaluation, the translation from the &#8220;Lattice&#8221; system is also better than those from the baseline and &#8220;Para-Sub&#8221; systems.</text>
                  <doc_id>247</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>6</index>
        <title>6 Conclusions and Future Work</title>
        <text>In this paper, we proposed a novel method using paraphrase lattices to facilitate the translation process in SMT. Given an input sentence, our method firstly discovers all possible paraphrases from a paraphrase database for N-grams (1 &lt;= N &lt;= 10) in the test set, and then filters out the paraphrases which do not appear in the phrase table in order to avoid adding new unknown words on the input side. We then use the original words and the paraphrases to build a word lattice, and set the weights to prioritise the original edges and penalise the paraphrase edges. Finally, we import the lattice into the decoder to perform lattice decoding. The experiments are conducted on English-to-Chinese translation using the FBIS data set with small and medium-sized amounts of data, and on a large-scale corpus of 2.1 428 million sentence pairs. We also performed comparative experiments for the baseline, the &#8220;Para-Sub&#8221; system and our paraphrase lattice-based system. The experimental results show that our proposed system significantly outperforms the baseline and the &#8220;Para- Sub&#8221; system, and the effectiveness is consistent on the small, medium and large-scale data sets. As for future work, firstly we plan to propose a pruning algorithm for the duplicate paths in the lattice, which will track the edge generation with respect to the path span, and thus eliminate duplicate paths. Secondly, we plan to experiment with another feature function in the log-linear model to discount words derived from paraphrases, and use MERT to assign an appropriate weight to this feature function.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper, we proposed a novel method using paraphrase lattices to facilitate the translation process in SMT.</text>
              <doc_id>248</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Given an input sentence, our method firstly discovers all possible paraphrases from a paraphrase database for N-grams (1 &lt;= N &lt;= 10) in the test set, and then filters out the paraphrases which do not appear in the phrase table in order to avoid adding new unknown words on the input side.</text>
              <doc_id>249</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We then use the original words and the paraphrases to build a word lattice, and set the weights to prioritise the original edges and penalise the paraphrase edges.</text>
              <doc_id>250</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we import the lattice into the decoder to perform lattice decoding.</text>
              <doc_id>251</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The experiments are conducted on English-to-Chinese translation using the FBIS data set with small and medium-sized amounts of data, and on a large-scale corpus of 2.1 428 million sentence pairs.</text>
              <doc_id>252</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We also performed comparative experiments for the baseline, the &#8220;Para-Sub&#8221; system and our paraphrase lattice-based system.</text>
              <doc_id>253</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The experimental results show that our proposed system significantly outperforms the baseline and the &#8220;Para- Sub&#8221; system, and the effectiveness is consistent on the small, medium and large-scale data sets.</text>
              <doc_id>254</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>As for future work, firstly we plan to propose a pruning algorithm for the duplicate paths in the lattice, which will track the edge generation with respect to the path span, and thus eliminate duplicate paths.</text>
              <doc_id>255</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Secondly, we plan to experiment with another feature function in the log-linear model to discount words derived from paraphrases, and use MERT to assign an appropriate weight to this feature function.</text>
              <doc_id>256</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>7</index>
        <title>Acknowledgments</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>257</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: The coverage of the test set by the phrase table and the parallel corpus based on different amount of the training data. &#8220;PL&#8221; indicates the Phrase Length N, where {1 &lt;= N &lt;= 10}; &#8220;20K&#8221; and &#8220;200K&#8221; represent the sizes of the parallel data for model training and phrase extraction; &#8220;Cov.&#8221; indicates the coverage rate; &#8220;Tset&#8221; represents the number of unique phrases with the length N in the Test Set; &#8220;PT&#8221; represents the number of phrases of the Test Set occur in the Phrase Table; &#8220;Corpus&#8221; indicates the number of phrases of the Test Set appearing in the parallel corpus; &#8220;in PT&#8221; indicates the coverage of the phrases in the Test Set by the phrase table and correspondingly &#8220;in Corpus&#8221; represents the coverage of the phrases in the Test Set by the Parallel Corpus.</caption>
        <reference_text>None</reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>20K</cell>
              <cell>Cov.(%)</cell>
              <cell>200K</cell>
              <cell>20K#@#@Cov.(%)</cell>
              <cell>Cov.(%)</cell>
              <cell>Cov.(%)</cell>
              <cell>200K</cell>
              <cell>200K</cell>
              <cell>Cov.(%)</cell>
              <cell>Cov.(%)</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>PL</cell>
              <cell>Tset</cell>
              <cell>PT</cell>
              <cell>Corpus</cell>
              <cell>in PT</cell>
              <cell>in Corpus</cell>
              <cell>PT</cell>
              <cell>Corpus</cell>
              <cell>in PT</cell>
              <cell>in Corpus</cell>
            </row>
            <row>
              <cell>1</cell>
              <cell>5,369</cell>
              <cell>3,785</cell>
              <cell>4,704</cell>
              <cell>70.5</cell>
              <cell>87.61</cell>
              <cell>4,941</cell>
              <cell>5,230</cell>
              <cell>92.03</cell>
              <cell>97.41</cell>
            </row>
            <row>
              <cell>2</cell>
              <cell>24,564</cell>
              <cell>8,631</cell>
              <cell>15,109</cell>
              <cell>35.14</cell>
              <cell>61.51</cell>
              <cell>16,803</cell>
              <cell>21,071</cell>
              <cell>68.40</cell>
              <cell>85.78</cell>
            </row>
            <row>
              <cell>3</cell>
              <cell>37,402</cell>
              <cell>4,538</cell>
              <cell>12,091</cell>
              <cell>12.13</cell>
              <cell>32.33</cell>
              <cell>12,922</cell>
              <cell>22,531</cell>
              <cell>34.55</cell>
              <cell>60.24</cell>
            </row>
            <row>
              <cell>4</cell>
              <cell>41,792</cell>
              <cell>1,703</cell>
              <cell>6,150</cell>
              <cell>4.07</cell>
              <cell>14.72</cell>
              <cell>5,974</cell>
              <cell>14,698</cell>
              <cell>14.29</cell>
              <cell>35.17</cell>
            </row>
            <row>
              <cell>5</cell>
              <cell>43,008</cell>
              <cell>626</cell>
              <cell>2,933</cell>
              <cell>1.46</cell>
              <cell>6.82</cell>
              <cell>2,579</cell>
              <cell>8,425</cell>
              <cell>5.99</cell>
              <cell>19.59</cell>
            </row>
            <row>
              <cell>6</cell>
              <cell>43,054</cell>
              <cell>259</cell>
              <cell>1,459</cell>
              <cell>0.6</cell>
              <cell>3.39</cell>
              <cell>1,192</cell>
              <cell>4,856</cell>
              <cell>2.77</cell>
              <cell>11.28</cell>
            </row>
            <row>
              <cell>7</cell>
              <cell>42,601</cell>
              <cell>119</cell>
              <cell>821</cell>
              <cell>0.28</cell>
              <cell>1.93</cell>
              <cell>581</cell>
              <cell>2,936</cell>
              <cell>1.36</cell>
              <cell>6.89</cell>
            </row>
            <row>
              <cell>8</cell>
              <cell>41,865</cell>
              <cell>51</cell>
              <cell>505</cell>
              <cell>0.12</cell>
              <cell>1.21</cell>
              <cell>319</cell>
              <cell>1,890</cell>
              <cell>0.76</cell>
              <cell>4.51</cell>
            </row>
            <row>
              <cell>9</cell>
              <cell>40,984</cell>
              <cell>34</cell>
              <cell>341</cell>
              <cell>0.08</cell>
              <cell>0.83</cell>
              <cell>233</cell>
              <cell>1,294</cell>
              <cell>0.57</cell>
              <cell>3.16</cell>
            </row>
            <row>
              <cell>10</cell>
              <cell>40,002</cell>
              <cell>22</cell>
              <cell>241</cell>
              <cell>0.05</cell>
              <cell>0.6</cell>
              <cell>135</cell>
              <cell>923</cell>
              <cell>0.34</cell>
              <cell>2.31</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Comparison between the baseline, &#8220;Para-Sub&#8221; and our &#8220;Lattice&#8221; (paraphrase lattice) method.</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>20K</cell>
              <cell>200K</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>SYS</cell>
              <cell>BLEU</cell>
              <cell>CI 95%</cell>
              <cell>pair-CI 95%</cell>
              <cell>TER</cell>
              <cell>BLEU</cell>
              <cell>CI 95%</cell>
              <cell>pair-CI 95%</cell>
              <cell>TER</cell>
            </row>
            <row>
              <cell>Baseline</cell>
              <cell>14.42</cell>
              <cell>[-0.81, +0.74]</cell>
              <cell>&#8211;</cell>
              <cell>75.30</cell>
              <cell>23.60</cell>
              <cell>[-1.03, +0.97]</cell>
              <cell>&#8211;</cell>
              <cell>63.56</cell>
            </row>
            <row>
              <cell>Para-Sub</cell>
              <cell>14.78</cell>
              <cell>[-0.78, +0.82]</cell>
              <cell>[+0.13, +0.60]</cell>
              <cell>73.75</cell>
              <cell>23.41</cell>
              <cell>[-1.04, +1.00]</cell>
              <cell>[-0.46, +0.09]</cell>
              <cell>63.84</cell>
            </row>
            <row>
              <cell>Lattice</cell>
              <cell>15.44</cell>
              <cell>[-0.85, +0.84]</cell>
              <cell>[+0.74, +1.30]</cell>
              <cell>73.06</cell>
              <cell>25.20</cell>
              <cell>[-1.11, +1.15]</cell>
              <cell>[+1.19, +2.01]</cell>
              <cell>62.37</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TET</source>
        <caption>Table 3: Comparison between the baseline and our paraphrase lattice method on a large-scale data set.</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>SYS</cell>
              <cell>BLEU</cell>
              <cell>CI 95%</cell>
              <cell>pair-CI 95%</cell>
              <cell>NIST</cell>
              <cell>TER</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Baseline</cell>
              <cell>14.04</cell>
              <cell>[-0.73, +0.40]</cell>
              <cell>&#8211;</cell>
              <cell>6.50</cell>
              <cell>74.88</cell>
            </row>
            <row>
              <cell>Para-Sub</cell>
              <cell>14.13</cell>
              <cell>[-0.56, +0.56]</cell>
              <cell>[-0.18, +0.40]</cell>
              <cell>6.52</cell>
              <cell>74.43</cell>
            </row>
            <row>
              <cell>Lattice</cell>
              <cell>14.55</cell>
              <cell>[-0.75, +0.32]</cell>
              <cell>[+0.15,+0.83]</cell>
              <cell>6.55</cell>
              <cell>73.28</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: The coverage of the paraphrase-added test set by the phrase table on medium size of the training data.</caption>
        <reference_text>In PAGE 8: ... We take the 200K data set to examine the coverage issue.  Table4  is an illustration to com- pare the new coverage and the old coverage (without paraphrases) on medium sized training data....  In PAGE 8: ...Table 4: The coverage of the paraphrase-added test set by the phrase table on medium size of the training data. From  Table4 , we can see that the coverage of un- igrams, bigrams, trigrams and 4-grams goes up by about 5%, 10%, 5% and 1%, while from 5-grams there is only a slight or no increase in coverage. These results show that 1) most of the paraphrases that are added in are lower-order n-grams; 2) the paraphrases can increase the coverage of the input by handling the unknown words to some extent....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>PL</cell>
              <cell>Tset</cell>
              <cell>PT</cell>
              <cell>New Cov.(%)</cell>
              <cell>Old Cov.(%)</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>1</cell>
              <cell>9,264</cell>
              <cell>8,994</cell>
              <cell>97.09</cell>
              <cell>92.03</cell>
            </row>
            <row>
              <cell>2</cell>
              <cell>32,805</cell>
              <cell>25,796</cell>
              <cell>78.63</cell>
              <cell>68.40</cell>
            </row>
            <row>
              <cell>3</cell>
              <cell>39,918</cell>
              <cell>15,708</cell>
              <cell>39.35</cell>
              <cell>34.55</cell>
            </row>
            <row>
              <cell>4</cell>
              <cell>42,247</cell>
              <cell>6,479</cell>
              <cell>15.34</cell>
              <cell>14.29</cell>
            </row>
            <row>
              <cell>5</cell>
              <cell>43,088</cell>
              <cell>2,670</cell>
              <cell>6.20</cell>
              <cell>5.99</cell>
            </row>
            <row>
              <cell>6</cell>
              <cell>43,066</cell>
              <cell>1,204</cell>
              <cell>2.80</cell>
              <cell>2.77</cell>
            </row>
            <row>
              <cell>7</cell>
              <cell>42,602</cell>
              <cell>582</cell>
              <cell>1.37</cell>
              <cell>1.36</cell>
            </row>
            <row>
              <cell>8</cell>
              <cell>41,865</cell>
              <cell>319</cell>
              <cell>0.76</cell>
              <cell>0.76</cell>
            </row>
            <row>
              <cell>9</cell>
              <cell>40,984</cell>
              <cell>233</cell>
              <cell>0.57</cell>
              <cell>0.57</cell>
            </row>
            <row>
              <cell>10</cell>
              <cell>40,002</cell>
              <cell>135</cell>
              <cell>0.34</cell>
              <cell>0.34</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TET</source>
        <caption>Table 5: Comparison on sentence-level scores in terms of BLEU and TER metrics.</caption>
        <reference_text></reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>SYS</cell>
              <cell>BLEU</cell>
              <cell>TER</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Baseline</cell>
              <cell>20.33</cell>
              <cell>66.67</cell>
            </row>
            <row>
              <cell>Para-Sub</cell>
              <cell>21.78</cell>
              <cell>53.33</cell>
            </row>
            <row>
              <cell>Lattice</cell>
              <cell>23.51</cell>
              <cell>53.33</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Colin Bannard</author>
          <author>Chris Callison-Burch</author>
        </authors>
        <title>Paraphrasing with bilingual parallel corpora.</title>
        <publication>In 43rd Annual meeting of the Association for Computational Linguistics,</publication>
        <pages>597--604</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Chris Callison-Burch</author>
          <author>Philipp Koehn</author>
          <author>Miles Osborne</author>
        </authors>
        <title>Improved statistical machine translation using paraphrases.</title>
        <publication>In Proceedings of HLT-NAACL 2006: Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL,</publication>
        <pages>17--24</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>A hierarchical phrase-based model for statistical machine translation.</title>
        <publication>In 43rd Annual meeting of the Association for Computational Linguistics,</publication>
        <pages>263--270</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz Josef Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical phrase-based translation.</title>
        <publication>In Proceedings of HLT-NAACL 2003: conference combining Human Language Technology conference series and the North American Chapter of the Association for Computational Linguistics conference series,</publication>
        <pages>48--54</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Hieu Hoang</author>
          <author>A Birch</author>
          <author>C Callison-Burch</author>
          <author>M Federico</author>
          <author>N Bertoldi</author>
          <author>B Cowan</author>
          <author>Wade Shen</author>
          <author>C</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>R Zens Moran</author>
          <author>C Dyer</author>
          <author>O Bojar</author>
          <author>A Constantin</author>
          <author>Evan Herbst</author>
        </authors>
        <title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
        <publication>In ACL</publication>
        <pages>177--180</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Preslav Nakov</author>
        </authors>
        <title>Improving English-Spanish statistical machine translation: experiments in domain adaptation, sentence paraphrasing, tokenization, and recasing.</title>
        <publication>In Proceedings of ACL-08:HLT. Third Workshop on Statistical Machine Translation,</publication>
        <pages>147--150</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Franz Och</author>
        </authors>
        <title>Minimum Error Rate Training in Statistical Machine Translation.</title>
        <publication>In 41st Annual meeting of the Association for Computational Linguistics,</publication>
        <pages>160--167</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Franz Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A systematic comparison of various statistical alignment models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
        <publication>In 40th Annual meeting of the Association for Computational Linguistics,</publication>
        <pages>311--318</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Josh Schroeder</author>
          <author>Trevor Cohn</author>
          <author>Philipp Koehn</author>
        </authors>
        <title>Word Lattices for Multi-source Translation.</title>
        <publication>In Proceedings of the 12th Conference of the European Chapter of the ACL,</publication>
        <pages>719--727</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Matthew Snover</author>
          <author>Bonnie Dorr</author>
          <author>Richard Schwartz</author>
          <author>Linnea Micciulla</author>
          <author>John Makhoul</author>
        </authors>
        <title>A study of translation edit rate with targeted human annotation.</title>
        <publication>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas,</publication>
        <pages>223--231</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Matthew Snover</author>
          <author>Nitin Madnani</author>
          <author>Bonnie J Dorr</author>
          <author>Richard Schwartz</author>
        </authors>
        <title>Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric.</title>
        <publication>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</publication>
        <pages>259--268</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Ying Zhang</author>
          <author>Stephan Vogel</author>
        </authors>
        <title>Measuring Confidence Intervals for the Machine Translation Evaluation Metrics.</title>
        <publication>In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI),</publication>
        <pages>85--94</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Andreas Zollmann</author>
          <author>Ashish Venugopal</author>
        </authors>
        <title>Syntax augmented machine translation via chart parsing.</title>
        <publication>In Proceedings of HLT-NAACL 2006: Proceedings of the Workshop on Statistical Machine Translation,</publication>
        <pages>138--141</pages>
        <date>2006</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>1</reference_id>
        <string>Callison-Burch et al. (2006)</string>
        <sentence_id>9467</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Callison-Burch et al. (2006)</string>
        <sentence_id>9469</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>Callison-Burch et al., 2006</string>
        <sentence_id>9490</sentence_id>
        <char_offset>57</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>1</reference_id>
        <string>Callison-Burch et al., 2006</string>
        <sentence_id>9510</sentence_id>
        <char_offset>127</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>1</reference_id>
        <string>Callison-Burch et al., 2006</string>
        <sentence_id>9511</sentence_id>
        <char_offset>169</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>1</reference_id>
        <string>Callison-Burch et al., 2006</string>
        <sentence_id>9661</sentence_id>
        <char_offset>276</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>2</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>9501</sentence_id>
        <char_offset>75</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>3</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>9501</sentence_id>
        <char_offset>55</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>6</reference_id>
        <string>Nakov (2008)</string>
        <sentence_id>9472</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>6</reference_id>
        <string>Nakov (2008)</string>
        <sentence_id>9494</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>6</reference_id>
        <string>Nakov (2008)</string>
        <sentence_id>9498</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>7</reference_id>
        <string>Och, 2003</string>
        <sentence_id>9626</sentence_id>
        <char_offset>65</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>8</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>9624</sentence_id>
        <char_offset>40</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>9</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>9511</sentence_id>
        <char_offset>138</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>10</reference_id>
        <string>Schroeder et al. (2009)</string>
        <sentence_id>9478</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>11</reference_id>
        <string>Snover et al., 2006</string>
        <sentence_id>9635</sentence_id>
        <char_offset>42</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>12</reference_id>
        <string>Snover et al., 2009</string>
        <sentence_id>9642</sentence_id>
        <char_offset>153</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>13</reference_id>
        <string>Zhang and Vogel, 2004</string>
        <sentence_id>9657</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>14</reference_id>
        <string>Zollmann and Venugopal, 2006</string>
        <sentence_id>9501</sentence_id>
        <char_offset>106</char_offset>
      </citation>
    </citations>
  </content>
</document>
