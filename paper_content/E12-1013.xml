<document>
  <filename>E12-1013</filename>
  <authors/>
  <title>None</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>1 Introduction</title>
        <text>The search space of Phrase-Based Statistical Machine Translation (PBSMT) systems has the form of a very large directed acyclic graph. In several softwares, an approximation of this search space can be outputted, either as a n-best list containing the n top hypotheses found by the decoder, or as a phrase or word graph (lattice) which compactly encodes those hypotheses that have survived search space pruning. Lattices usually contain much more hypotheses than n-best lists and better approximate the search space.
Exploring the PBSMT search space is one of the few means to perform diagnostic analysis and to better understand the behavior of the system (Turchi et al., 2008; Auli et al., 2009). Useful diagnostics are, for instance, provided by looking at the best (oracle) hypotheses contained in the search space, i.e, those hypotheses that have the highest quality score with respect to one or several references. Such oracle hypotheses can be used for failure analysis and to better understand the bottlenecks of existing translation systems (Wisniewski et al., 2010). Indeed, the inability to faithfully reproduce reference translations can have many causes, such as scantiness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices, etc. Oracle decoding has several other applications: for instance, in (Liang et al., 2006; Chiang et al., 2008) it is used as a work-around to the problem of non-reachability of the reference in discriminative training of MT systems. Lattice reranking (Li and Khudanpur, 2009), a promising way to improve MT systems, also relies on oracle decoding to build the training data for a reranking algorithm.
For sentence level metrics, finding oracle hypotheses in n-best lists is a simple issue; however, solving this problem on lattices proves much more challenging, due to the number of embedded hypotheses, which prevents the use of bruteforce approaches. When using BLEU, or rather sentence-level approximations thereof, the problem is in fact known to be NP-hard (Leusch et al., 2008). This complexity stems from the fact that the contribution of a given edge to the total modified n-gram precision can not be computed without looking at all other edges on the path. Similar (or worse) complexity result are expected
for other metrics such as METEOR (Banerjee and Lavie, 2005) or TER (Snover et al., 2006). The exact computation of oracles under corpus level metrics, such as BLEU, poses supplementary combinatorial problems that will not be addressed in this work.
In this paper, we present two original methods for finding approximate oracle hypotheses on lattices. The first one is based on a linear approximation of the corpus BLEU, that was originally designed for efficient Minimum Bayesian Risk decoding on lattices (Tromble et al., 2008). The second one, based on Integer Linear Programming, is an extension to lattices of a recent work on failure analysis for phrase-based decoders (Wisniewski et al., 2010). In this framework, we study two decoding strategies: one based on a generic ILP solver, and one, based on Lagrangian relaxation. Our contribution is also experimental as we compare the quality of the BLEU approximations and the time performance of these new approaches with several existing methods, for different language pairs and using the lattice generation capacities of two publicly-available state-of-theart phrase-based decoders: Moses 1 and N-code 2 .
The rest of this paper is organized as follows. In Section 2, we formally define the oracle decoding task and recall the formalism of finite state automata on semirings. We then describe (Section 3) two existing approaches for solving this task, before detailing our new proposals in sections 4 and 5. We then report evaluations of the existing and new oracles on machine translation tasks.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The search space of Phrase-Based Statistical Machine Translation (PBSMT) systems has the form of a very large directed acyclic graph.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In several softwares, an approximation of this search space can be outputted, either as a n-best list containing the n top hypotheses found by the decoder, or as a phrase or word graph (lattice) which compactly encodes those hypotheses that have survived search space pruning.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Lattices usually contain much more hypotheses than n-best lists and better approximate the search space.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Exploring the PBSMT search space is one of the few means to perform diagnostic analysis and to better understand the behavior of the system (Turchi et al., 2008; Auli et al., 2009).</text>
              <doc_id>3</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Useful diagnostics are, for instance, provided by looking at the best (oracle) hypotheses contained in the search space, i.e, those hypotheses that have the highest quality score with respect to one or several references.</text>
              <doc_id>4</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Such oracle hypotheses can be used for failure analysis and to better understand the bottlenecks of existing translation systems (Wisniewski et al., 2010).</text>
              <doc_id>5</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Indeed, the inability to faithfully reproduce reference translations can have many causes, such as scantiness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices, etc. Oracle decoding has several other applications: for instance, in (Liang et al., 2006; Chiang et al., 2008) it is used as a work-around to the problem of non-reachability of the reference in discriminative training of MT systems.</text>
              <doc_id>6</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Lattice reranking (Li and Khudanpur, 2009), a promising way to improve MT systems, also relies on oracle decoding to build the training data for a reranking algorithm.</text>
              <doc_id>7</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For sentence level metrics, finding oracle hypotheses in n-best lists is a simple issue; however, solving this problem on lattices proves much more challenging, due to the number of embedded hypotheses, which prevents the use of bruteforce approaches.</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>When using BLEU, or rather sentence-level approximations thereof, the problem is in fact known to be NP-hard (Leusch et al., 2008).</text>
              <doc_id>9</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This complexity stems from the fact that the contribution of a given edge to the total modified n-gram precision can not be computed without looking at all other edges on the path.</text>
              <doc_id>10</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Similar (or worse) complexity result are expected</text>
              <doc_id>11</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>for other metrics such as METEOR (Banerjee and Lavie, 2005) or TER (Snover et al., 2006).</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The exact computation of oracles under corpus level metrics, such as BLEU, poses supplementary combinatorial problems that will not be addressed in this work.</text>
              <doc_id>13</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this paper, we present two original methods for finding approximate oracle hypotheses on lattices.</text>
              <doc_id>14</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The first one is based on a linear approximation of the corpus BLEU, that was originally designed for efficient Minimum Bayesian Risk decoding on lattices (Tromble et al., 2008).</text>
              <doc_id>15</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The second one, based on Integer Linear Programming, is an extension to lattices of a recent work on failure analysis for phrase-based decoders (Wisniewski et al., 2010).</text>
              <doc_id>16</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In this framework, we study two decoding strategies: one based on a generic ILP solver, and one, based on Lagrangian relaxation.</text>
              <doc_id>17</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Our contribution is also experimental as we compare the quality of the BLEU approximations and the time performance of these new approaches with several existing methods, for different language pairs and using the lattice generation capacities of two publicly-available state-of-theart phrase-based decoders: Moses 1 and N-code 2 .</text>
              <doc_id>18</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The rest of this paper is organized as follows.</text>
              <doc_id>19</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Section 2, we formally define the oracle decoding task and recall the formalism of finite state automata on semirings.</text>
              <doc_id>20</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We then describe (Section 3) two existing approaches for solving this task, before detailing our new proposals in sections 4 and 5.</text>
              <doc_id>21</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We then report evaluations of the existing and new oracles on machine translation tasks.</text>
              <doc_id>22</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>2 Preliminaries</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>23</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Oracle Decoding Task</title>
            <text>We assume that a phrase-based decoder is able to produce, for each source sentence f, a lattice L f = &#12296;Q, &#926;&#12297;, with # {Q} vertices (states) and # {&#926;} edges. Each edge carries a source phrase f i , an associated output phrase e i as well as a feature vector &#175;h i , the components of which encode various compatibility measures between f i and e i .
We further assume that L f is a word lattice, meaning that each e i carries a single word 3 and
1 http://www.statmt.org/moses/ 2 http://ncode.limsi.fr/ 3 Converting a phrase lattice to a word lattice is a simple
matter of redistributing a compound input or output over a
that it contains a unique initial state q 0 and a unique final state q F . Let &#928; f denote the set of all paths from q 0 to q F in L f . Each path &#960; &#8712; &#928; f corresponds to a possible translation e &#960; . The job of a (conventional) decoder is to find the best path(s) in L f using scores that combine the edges&#8217; feature vectors with the parameters &#175;&#955; learned during tuning. In oracle decoding, the decoder&#8217;s job is quite different, as we assume that at least a reference r f is provided to evaluate the quality of each individual hypothesis. The decoder therefore aims at finding the path &#960; &#8727; that generates the hypothesis that best matches r f . For this task, only the output labels e i will matter, the other informations can be left aside. 4 Oracle decoding assumes the definition of a measure of the similarity between a reference and a hypothesis. In this paper we will consider sentence-level approximations of the popular BLEU score (Papineni et al., 2002). BLEU is formally defined for two parallel corpora, E = {e j } J j=1 and R = {r j} J j=1 , each containing J sentences as: (
&#8719; n ) 1/n
n-BLEU(E, R) = BP &#183; p m , (1)
m=1
where BP = min(1, e 1&#8722;c 1(R)/c 1 (E) ) is the brevity penalty and p m = c m (E, R)/c m (E) are clipped or modified m-gram precisions: c m (E) is the total number of word m-grams in E; c m (E, R) accumulates over sentences the number of m- grams in e j that also belong to r j . These counts are clipped, meaning that a m-gram that appears k times in E and l times in R, with k &gt; l, is only counted l times. As it is well known, BLEU performs a compromise between precision, which is directly appears in Equation (1), and recall, which is indirectly taken into account via the brevity penalty. In most cases, Equation (1) is computed with n = 4 and we use BLEU as a synonym for
4-BLEU.
BLEU is defined for a pair of corpora, but, as an
oracle decoder is working at the sentence-level, it should rely on an approximation of BLEU that can
linear chain of arcs. 4 The algorithms described below can be straightforwardly generalized to compute oracle hypotheses under combined metrics mixing model scores and quality measures (Chiang et al., 2008), by weighting each edge with its model score and by using these weights down the pipe.
evaluate the similarity between a single hypothesis and its reference. This approximation introduces a discrepancy as gathering sentences with the highest (local) approximation may not result in the highest possible (corpus-level) BLEU score. Let BLEU &#8242; be such a sentence-level approximation of BLEU. Then lattice oracle decoding is the task of finding an optimal path &#960; &#8727; (f) among all paths &#928; f for a given f, and amounts to the following optimization problem:</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We assume that a phrase-based decoder is able to produce, for each source sentence f, a lattice L f = &#12296;Q, &#926;&#12297;, with # {Q} vertices (states) and # {&#926;} edges.</text>
                  <doc_id>24</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Each edge carries a source phrase f i , an associated output phrase e i as well as a feature vector &#175;h i , the components of which encode various compatibility measures between f i and e i .</text>
                  <doc_id>25</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We further assume that L f is a word lattice, meaning that each e i carries a single word 3 and</text>
                  <doc_id>26</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 http://www.statmt.org/moses/ 2 http://ncode.limsi.fr/ 3 Converting a phrase lattice to a word lattice is a simple</text>
                  <doc_id>27</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>matter of redistributing a compound input or output over a</text>
                  <doc_id>28</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>that it contains a unique initial state q 0 and a unique final state q F .</text>
                  <doc_id>29</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Let &#928; f denote the set of all paths from q 0 to q F in L f .</text>
                  <doc_id>30</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Each path &#960; &#8712; &#928; f corresponds to a possible translation e &#960; .</text>
                  <doc_id>31</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The job of a (conventional) decoder is to find the best path(s) in L f using scores that combine the edges&#8217; feature vectors with the parameters &#175;&#955; learned during tuning.</text>
                  <doc_id>32</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In oracle decoding, the decoder&#8217;s job is quite different, as we assume that at least a reference r f is provided to evaluate the quality of each individual hypothesis.</text>
                  <doc_id>33</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The decoder therefore aims at finding the path &#960; &#8727; that generates the hypothesis that best matches r f .</text>
                  <doc_id>34</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>For this task, only the output labels e i will matter, the other informations can be left aside.</text>
                  <doc_id>35</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>4 Oracle decoding assumes the definition of a measure of the similarity between a reference and a hypothesis.</text>
                  <doc_id>36</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>In this paper we will consider sentence-level approximations of the popular BLEU score (Papineni et al., 2002).</text>
                  <doc_id>37</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>BLEU is formally defined for two parallel corpora, E = {e j } J j=1 and R = {r j} J j=1 , each containing J sentences as: (</text>
                  <doc_id>38</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8719; n ) 1/n</text>
                  <doc_id>39</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n-BLEU(E, R) = BP &#183; p m , (1)</text>
                  <doc_id>40</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>m=1</text>
                  <doc_id>41</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where BP = min(1, e 1&#8722;c 1(R)/c 1 (E) ) is the brevity penalty and p m = c m (E, R)/c m (E) are clipped or modified m-gram precisions: c m (E) is the total number of word m-grams in E; c m (E, R) accumulates over sentences the number of m- grams in e j that also belong to r j .</text>
                  <doc_id>42</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These counts are clipped, meaning that a m-gram that appears k times in E and l times in R, with k &gt; l, is only counted l times.</text>
                  <doc_id>43</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As it is well known, BLEU performs a compromise between precision, which is directly appears in Equation (1), and recall, which is indirectly taken into account via the brevity penalty.</text>
                  <doc_id>44</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In most cases, Equation (1) is computed with n = 4 and we use BLEU as a synonym for</text>
                  <doc_id>45</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4-BLEU.</text>
                  <doc_id>46</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BLEU is defined for a pair of corpora, but, as an</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>oracle decoder is working at the sentence-level, it should rely on an approximation of BLEU that can</text>
                  <doc_id>48</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>linear chain of arcs.</text>
                  <doc_id>49</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>4 The algorithms described below can be straightforwardly generalized to compute oracle hypotheses under combined metrics mixing model scores and quality measures (Chiang et al., 2008), by weighting each edge with its model score and by using these weights down the pipe.</text>
                  <doc_id>50</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>evaluate the similarity between a single hypothesis and its reference.</text>
                  <doc_id>51</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This approximation introduces a discrepancy as gathering sentences with the highest (local) approximation may not result in the highest possible (corpus-level) BLEU score.</text>
                  <doc_id>52</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Let BLEU &#8242; be such a sentence-level approximation of BLEU.</text>
                  <doc_id>53</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Then lattice oracle decoding is the task of finding an optimal path &#960; &#8727; (f) among all paths &#928; f for a given f, and amounts to the following optimization problem:</text>
                  <doc_id>54</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Compromises of Oracle Decoding</title>
            <text>As proved by Leusch et al. (2008), even with brevity penalty dropped, the problem of deciding whether a confusion network contains a hypothesis with clipped uni- and bigram precisions all equal to 1.0 is NP-complete (and so is the associated optimization problem of oracle decoding for 2-BLEU). The case of more general word and phrase lattices and 4-BLEU score is consequently also NP-complete. This complexity stems from chaining up of local unigram decisions that, due to the clipping constraints, have non-local effect on the bigram precision scores. It is consequently necessary to keep a possibly exponential number of non-recombinable hypotheses (characterized by counts for each n-gram in the reference) until very late states in the lattice.
These complexity results imply that any oracle decoder has to waive either the form of the objective function, replacing BLEU with better-behaved scoring functions, or the exactness of the solution, relying on approximate heuristic search algorithms. In Table 1, we summarize different compromises that the existing (section 3), as well as our novel (sections 4 and 5) oracle decoders, have to make. The &#8220;target&#8221; and &#8220;target level&#8221; columns specify the targeted score. None of the decoders optimizes it directly: their objective function is rather the approximation of BLEU given in the &#8220;target replacement&#8221; column. Column &#8220;search&#8221; details the accuracy of the target replacement optimization. Finally, columns &#8220;clipping&#8221; and &#8220;brevity&#8221; indicate whether the corresponding properties of BLEU score are considered in the target substitute and in the search algorithm. 2.3 Finite State Acceptors
The implementations of the oracles described in the first part of this work (sections 3 and 4) use the common formalism of finite state acceptors (FSA) over different semirings and are implemented using the generic OpenFST toolbox (Allauzen et al., 2007).
A (&#8853;, &#8855;)-semiring K over a set K is a system &#12296;K, &#8853;, &#8855;, &#175;0, &#175;1&#12297;, where &#12296;K, &#8853;, &#175;0&#12297; is a commutative monoid with identity element &#175;0, and &#12296;K, &#8855;, &#175;1&#12297; is a monoid with identity element &#175;1. &#8855; distributes over &#8853;, so that a &#8855; (b &#8853; c) = (a &#8855; b) &#8853; (a &#8855; c) and (b &#8853; c) &#8855; a = (b &#8855; a) &#8853; (c &#8855; a) and element &#175;0 annihilates K (a &#8855; &#175;0 = &#175;0 &#8855; a = &#175;0).
Let A = (&#931;, Q, I, F, E) be a weighted finitestate acceptor with labels in &#931; and weights in K, meaning that the transitions (q, &#963;, q &#8242; ) in A carry a weight w &#8712; K. Formally, E is a mapping from (Q &#215; &#931; &#215; Q) into K; likewise, initial I and final weight F functions are mappings from Q into K. We borrow the notations of Mohri (2009): if &#958; = (q, a, q &#8242; ) is a transition in domain(E), p(&#958;) = q (resp. n(&#958;) = q &#8242; ) denotes its origin (resp. destination) state, w(&#958;) = &#963; its label and E(&#958;) its weight. These notations extend to paths: if &#960; is a path in A, p(&#960;) (resp. n(&#960;)) is its initial (resp. ending) state and w(&#960;) is the label along the path. A finite state transducer (FST) is an FSA with output alphabet, so that each transition carries a pair of input/output symbols.
As discussed in Sections 3 and 4, several oracle decoding algorithms can be expressed as shortestpath problems, provided a suitable definition of the underlying acceptor and associated semiring. In particular, quantities such as: &#8853;
E(&#960;), (3)
&#960;&#8712;&#928;(A)
where the total weight of a successful path &#960; = &#958; 1 . . . &#958; l in A is computed as:
E(&#960;) =I(p(&#958; 1 )) &#8855; [
l &#8855;
i=1
E(&#958; i ) ] &#8855; F (n(&#958; l ))
can be efficiently found by generic shortest distance algorithms over acyclic graphs (Mohri, 2002). For FSA-based implementations over semirings where &#8853; = max, the optimization problem (2) is thus reduced to Equation (3), while the oracle-specific details can be incorporated into in the definition of &#8855;.
existing this paper oracle target target level target replacement search clipping brevity</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>As proved by Leusch et al. (2008), even with brevity penalty dropped, the problem of deciding whether a confusion network contains a hypothesis with clipped uni- and bigram precisions all equal to 1.0 is NP-complete (and so is the associated optimization problem of oracle decoding for 2-BLEU).</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The case of more general word and phrase lattices and 4-BLEU score is consequently also NP-complete.</text>
                  <doc_id>56</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This complexity stems from chaining up of local unigram decisions that, due to the clipping constraints, have non-local effect on the bigram precision scores.</text>
                  <doc_id>57</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>It is consequently necessary to keep a possibly exponential number of non-recombinable hypotheses (characterized by counts for each n-gram in the reference) until very late states in the lattice.</text>
                  <doc_id>58</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>These complexity results imply that any oracle decoder has to waive either the form of the objective function, replacing BLEU with better-behaved scoring functions, or the exactness of the solution, relying on approximate heuristic search algorithms.</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In Table 1, we summarize different compromises that the existing (section 3), as well as our novel (sections 4 and 5) oracle decoders, have to make.</text>
                  <doc_id>60</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The &#8220;target&#8221; and &#8220;target level&#8221; columns specify the targeted score.</text>
                  <doc_id>61</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>None of the decoders optimizes it directly: their objective function is rather the approximation of BLEU given in the &#8220;target replacement&#8221; column.</text>
                  <doc_id>62</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Column &#8220;search&#8221; details the accuracy of the target replacement optimization.</text>
                  <doc_id>63</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, columns &#8220;clipping&#8221; and &#8220;brevity&#8221; indicate whether the corresponding properties of BLEU score are considered in the target substitute and in the search algorithm.</text>
                  <doc_id>64</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>2.3 Finite State Acceptors</text>
                  <doc_id>65</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The implementations of the oracles described in the first part of this work (sections 3 and 4) use the common formalism of finite state acceptors (FSA) over different semirings and are implemented using the generic OpenFST toolbox (Allauzen et al., 2007).</text>
                  <doc_id>66</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A (&#8853;, &#8855;)-semiring K over a set K is a system &#12296;K, &#8853;, &#8855;, &#175;0, &#175;1&#12297;, where &#12296;K, &#8853;, &#175;0&#12297; is a commutative monoid with identity element &#175;0, and &#12296;K, &#8855;, &#175;1&#12297; is a monoid with identity element &#175;1.</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>&#8855; distributes over &#8853;, so that a &#8855; (b &#8853; c) = (a &#8855; b) &#8853; (a &#8855; c) and (b &#8853; c) &#8855; a = (b &#8855; a) &#8853; (c &#8855; a) and element &#175;0 annihilates K (a &#8855; &#175;0 = &#175;0 &#8855; a = &#175;0).</text>
                  <doc_id>68</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Let A = (&#931;, Q, I, F, E) be a weighted finitestate acceptor with labels in &#931; and weights in K, meaning that the transitions (q, &#963;, q &#8242; ) in A carry a weight w &#8712; K. Formally, E is a mapping from (Q &#215; &#931; &#215; Q) into K; likewise, initial I and final weight F functions are mappings from Q into K.</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We borrow the notations of Mohri (2009): if &#958; = (q, a, q &#8242; ) is a transition in domain(E), p(&#958;) = q (resp.</text>
                  <doc_id>70</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>n(&#958;) = q &#8242; ) denotes its origin (resp.</text>
                  <doc_id>71</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>destination) state, w(&#958;) = &#963; its label and E(&#958;) its weight.</text>
                  <doc_id>72</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>These notations extend to paths: if &#960; is a path in A, p(&#960;) (resp.</text>
                  <doc_id>73</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>n(&#960;)) is its initial (resp.</text>
                  <doc_id>74</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>ending) state and w(&#960;) is the label along the path.</text>
                  <doc_id>75</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>A finite state transducer (FST) is an FSA with output alphabet, so that each transition carries a pair of input/output symbols.</text>
                  <doc_id>76</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As discussed in Sections 3 and 4, several oracle decoding algorithms can be expressed as shortestpath problems, provided a suitable definition of the underlying acceptor and associated semiring.</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In particular, quantities such as: &#8853;</text>
                  <doc_id>78</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>E(&#960;), (3)</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#960;&#8712;&#928;(A)</text>
                  <doc_id>80</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where the total weight of a successful path &#960; = &#958; 1 .</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>82</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>83</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>&#958; l in A is computed as:</text>
                  <doc_id>84</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>E(&#960;) =I(p(&#958; 1 )) &#8855; [</text>
                  <doc_id>85</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>l &#8855;</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>E(&#958; i ) ] &#8855; F (n(&#958; l ))</text>
                  <doc_id>88</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>can be efficiently found by generic shortest distance algorithms over acyclic graphs (Mohri, 2002).</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For FSA-based implementations over semirings where &#8853; = max, the optimization problem (2) is thus reduced to Equation (3), while the oracle-specific details can be incorporated into in the definition of &#8855;.</text>
                  <doc_id>90</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>existing this paper oracle target target level target replacement search clipping brevity</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>2</index>
        <title>3 Existing Algorithms</title>
        <text>In this section, we describe our reimplementation of two approximate search algorithms that have been proposed in the literature to solve the oracle decoding problem for BLEU. In addition to their approximate nature, none of them accounts for the fact that the count of each matching word has to be clipped.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this section, we describe our reimplementation of two approximate search algorithms that have been proposed in the literature to solve the oracle decoding problem for BLEU.</text>
              <doc_id>92</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In addition to their approximate nature, none of them accounts for the fact that the count of each matching word has to be clipped.</text>
              <doc_id>93</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Language Model Oracle (LM)</title>
            <text>The simplest approach we consider is introduced in (Li and Khudanpur, 2009), where oracle decoding is reduced to the problem of finding the most likely hypothesis under a n-gram language model trained with the sole reference translation.
Let us suppose we have a n-gram language model that gives a probability P (e n |e 1 . . . e n&#8722;1 ) of word e n given the n &#8722; 1 previous words. The probability of a hypothesis e is then P n (e|r) = &#8719; i=1 P (e i+n|e i . . . e i+n&#8722;1 ). The language model can conveniently be represented as a FSA A LM , with each arc carrying a negative logprobability weight and with additional &#961;-type failure transitions to accommodate for back-off arcs.
If we train, for each source sentence f, a separate language model A LM (r f ) using only the reference r f , oracle decoding amounts to finding a shortest (most probable) path in the weighted FSA resulting from the composition L &#9702; A LM (r f ) over the (min, +)-semiring:
&#960; &#8727; LM(f) = ShortestPath(L &#9702; A LM (r f )). This approach replaces the optimization of n-
BLEU with a search for the most probable path
under a simplistic n-gram language model. One may expect the most probable path to select frequent n-gram from the reference, thus augmenting n-BLEU.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The simplest approach we consider is introduced in (Li and Khudanpur, 2009), where oracle decoding is reduced to the problem of finding the most likely hypothesis under a n-gram language model trained with the sole reference translation.</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Let us suppose we have a n-gram language model that gives a probability P (e n |e 1 .</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>96</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>97</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>e n&#8722;1 ) of word e n given the n &#8722; 1 previous words.</text>
                  <doc_id>98</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The probability of a hypothesis e is then P n (e|r) = &#8719; i=1 P (e i+n|e i .</text>
                  <doc_id>99</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>100</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>101</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>e i+n&#8722;1 ).</text>
                  <doc_id>102</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>The language model can conveniently be represented as a FSA A LM , with each arc carrying a negative logprobability weight and with additional &#961;-type failure transitions to accommodate for back-off arcs.</text>
                  <doc_id>103</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>If we train, for each source sentence f, a separate language model A LM (r f ) using only the reference r f , oracle decoding amounts to finding a shortest (most probable) path in the weighted FSA resulting from the composition L &#9702; A LM (r f ) over the (min, +)-semiring:</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#960; &#8727; LM(f) = ShortestPath(L &#9702; A LM (r f )).</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This approach replaces the optimization of n-</text>
                  <doc_id>106</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BLEU with a search for the most probable path</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>under a simplistic n-gram language model.</text>
                  <doc_id>108</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>One may expect the most probable path to select frequent n-gram from the reference, thus augmenting n-BLEU.</text>
                  <doc_id>109</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Partial BLEU Oracle (PB)</title>
            <text>Another approach is put forward in (Dreyer et al., 2007) and used in (Li and Khudanpur, 2009): oracle translations are shortest paths in a lattice L, where the weight of each path &#960; is the sentence level log BLEU(&#960;) score of the corresponding complete or partial hypothesis:
log BLEU(&#960;) = 1 4 &#8721;
m=1...4
log p m . (4)
Here, the brevity penalty is ignored and n- gram precisions are offset to avoid null counts: p m = (c m (e &#960; , r) + 0.1)/(c m (e &#960; ) + 0.1). This approach has been reimplemented using the FST formalism by defining a suitable semiring. Let each weight of the semiring keep a set of tuples accumulated up to the current state of the lattice. Each tuple contains three words of recent history, a partial hypothesis as well as current values of the length of the partial hypothesis, n- gram counts (4 numbers) and the sentence-level log BLEU score defined by Equation (4). In the beginning each arc is initialized with a singleton set containing one tuple with a single word as the partial hypothesis. For the semiring operations we define one common &#8855;-operation and two versions of the &#8853;-operation: &#8226; L 1 &#8855; P B L 2 &#8211; appends a word on the edge of L 2 to L 1 &#8217;s hypotheses, shifts their recent histories and updates n-gram counts, lengths, and current score; &#8226; L 1 &#8853; P B L 2 &#8211; merges all sets from L 1 and L 2 and recombinates those having the same recent history; &#8226; L 1 &#8853; P Bl L 2 &#8211; merges all sets from L 1 and L 2 and recombinates those having the same recent history and the same hypothesis length.
If several hypotheses have the same recent history (and length in the case of &#8853; P Bl ), recombination removes all of them, but the one
(a) &#8710; 1 (b) &#8710; 2 (c) &#8710; 3
with the largest current BLEU score. Optimal path is then found by launching the generic ShortestDistance(L) algorithm over one of the semirings above.
The (&#8853; P Bl , &#8855; P B )-semiring, in which the equal length requirement also implies equal brevity penalties, is more conservative in recombining hypotheses and should achieve final BLEU that is least as good as that obtained with the (&#8853; P B , &#8855; P B )-semiring 5 .</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Another approach is put forward in (Dreyer et al., 2007) and used in (Li and Khudanpur, 2009): oracle translations are shortest paths in a lattice L, where the weight of each path &#960; is the sentence level log BLEU(&#960;) score of the corresponding complete or partial hypothesis:</text>
                  <doc_id>110</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>log BLEU(&#960;) = 1 4 &#8721;</text>
                  <doc_id>111</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>m=1...4</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>log p m .</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(4)</text>
                  <doc_id>114</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Here, the brevity penalty is ignored and n- gram precisions are offset to avoid null counts: p m = (c m (e &#960; , r) + 0.1)/(c m (e &#960; ) + 0.1).</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This approach has been reimplemented using the FST formalism by defining a suitable semiring.</text>
                  <doc_id>116</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Let each weight of the semiring keep a set of tuples accumulated up to the current state of the lattice.</text>
                  <doc_id>117</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Each tuple contains three words of recent history, a partial hypothesis as well as current values of the length of the partial hypothesis, n- gram counts (4 numbers) and the sentence-level log BLEU score defined by Equation (4).</text>
                  <doc_id>118</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In the beginning each arc is initialized with a singleton set containing one tuple with a single word as the partial hypothesis.</text>
                  <doc_id>119</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For the semiring operations we define one common &#8855;-operation and two versions of the &#8853;-operation: &#8226; L 1 &#8855; P B L 2 &#8211; appends a word on the edge of L 2 to L 1 &#8217;s hypotheses, shifts their recent histories and updates n-gram counts, lengths, and current score; &#8226; L 1 &#8853; P B L 2 &#8211; merges all sets from L 1 and L 2 and recombinates those having the same recent history; &#8226; L 1 &#8853; P Bl L 2 &#8211; merges all sets from L 1 and L 2 and recombinates those having the same recent history and the same hypothesis length.</text>
                  <doc_id>120</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>If several hypotheses have the same recent history (and length in the case of &#8853; P Bl ), recombination removes all of them, but the one</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(a) &#8710; 1 (b) &#8710; 2 (c) &#8710; 3</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>with the largest current BLEU score.</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Optimal path is then found by launching the generic ShortestDistance(L) algorithm over one of the semirings above.</text>
                  <doc_id>124</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The (&#8853; P Bl , &#8855; P B )-semiring, in which the equal length requirement also implies equal brevity penalties, is more conservative in recombining hypotheses and should achieve final BLEU that is least as good as that obtained with the (&#8853; P B , &#8855; P B )-semiring 5 .</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>4 Linear BLEU Oracle (LB)</title>
        <text>In this section, we propose a new oracle based on the linear approximation of the corpus BLEU introduced in (Tromble et al., 2008). While this approximation was earlier used for Minimum Bayes Risk decoding in lattices (Tromble et al., 2008; Blackwood et al., 2010), we show here how it can also be used to approximately compute an oracle translation.
Given five real parameters &#952; 0...4 and a word vocabulary &#931;, Tromble et al. (2008) showed that one can approximate the corpus-BLEU with its firstorder (linear) Taylor expansion:
4&#8721; &#8721; lin BLEU(&#960;) = &#952; 0 |e &#960; |+ &#952; n c u (e &#960; )&#948; u (r),
n=1 u&#8712;&#931; n (5)
where c u (e) is the number of times the n-gram u appears in e, and &#948; u (r) is an indicator variable testing the presence of u in r.
To exploit this approximation for oracle decoding, we construct four weighted FSTs &#8710; n containing a (final) state for each possible (n &#8722; 1)-</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this section, we propose a new oracle based on the linear approximation of the corpus BLEU introduced in (Tromble et al., 2008).</text>
              <doc_id>126</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>While this approximation was earlier used for Minimum Bayes Risk decoding in lattices (Tromble et al., 2008; Blackwood et al., 2010), we show here how it can also be used to approximately compute an oracle translation.</text>
              <doc_id>127</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Given five real parameters &#952; 0...4 and a word vocabulary &#931;, Tromble et al. (2008) showed that one can approximate the corpus-BLEU with its firstorder (linear) Taylor expansion:</text>
              <doc_id>128</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>4&#8721; &#8721; lin BLEU(&#960;) = &#952; 0 |e &#960; |+ &#952; n c u (e &#960; )&#948; u (r),</text>
              <doc_id>129</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>n=1 u&#8712;&#931; n (5)</text>
              <doc_id>130</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where c u (e) is the number of times the n-gram u appears in e, and &#948; u (r) is an indicator variable testing the presence of u in r.</text>
              <doc_id>131</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To exploit this approximation for oracle decoding, we construct four weighted FSTs &#8710; n containing a (final) state for each possible (n &#8722; 1)-</text>
              <doc_id>132</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>5 See, however, experiments in Section 6.</title>
        <text>gram, and all weighted transitions of the kind (&#963;1 n&#8722;1 , &#963; n : &#963;1 n/&#952; n &#215; &#948; &#963; n
(r), &#963;2 n ), where &#963;s are in &#931;, input word sequence &#963;1 n&#8722;1 and output sequence &#963;2 n , are, respectively, the maximal prefix and suffix of an n-gram &#963;1 n. In supplement, we add auxiliary states corresponding to m-grams (m &lt; n &#8722; 1), whose functional purpose is to help reach one of the main (n &#8722; 1)-gram states. There are |&#931;|n&#8722;1 &#8722;1
|&#931;|&#8722;1 , n &gt; 1,
such supplementary states and their transitions are (&#963;1 k, &#963; k+1 : &#963;1 k+1 /0, &#963;1 k+1 ), k = 1 . . . n&#8722;2. Apart from these auxiliary states, the rest of the graph (i.e., all final states) reproduces the structure of the well-known de Bruijn graph B(&#931;, n) (see Figure 1). To actually compute the best hypothesis, we first weight all arcs in the input FSA L with &#952; 0 to obtain &#8710; 0 . This makes each word&#8217;s weight equal in a hypothesis path, and the total weight of the path in &#8710; 0 is proportional to the number of words in it. Then, by sequentially composing &#8710; 0 with other &#8710; n s, we discount arcs whose output n-gram corresponds to a matching n-gram. The amount of discount is regulated by the ratio between &#952; n &#8217;s for n &gt; 0.
With all operations performed over the (min, +)-semiring, the oracle translation is then given by:
&#960; &#8727; LB = ShortestPath(&#8710; 0 &#9702;&#8710; 1 &#9702;&#8710; 2 &#9702;&#8710; 3 &#9702;&#8710; 4 ).
We set parameters &#952; n as in (Tromble et al., 2008): &#952; 0 = 1, roughly corresponding to the brevity penalty (each word in a hypothesis adds up equally to the final path length) and &#952; n = &#8722;(4p &#183; r n&#8722;1 ) &#8722;1 , which are increasing discounts
for matching n-grams. The values of p and r were found by grid search with a 0.05 step value. A typical result of the grid evaluation of the LB oracle for German to English WMT&#8217;11 task is displayed on Figure 2. The optimal values for the other pairs of languages were roughly in the same ballpark, with p &#8776; 0.3 and r &#8776; 0.2.
5 Oracles with n-gram Clipping
In this section, we describe two new oracle decoders that take n-gram clipping into account. These oracles leverage on the well-known fact that the shortest path problem, at the heart of all the oracles described so far, can be reduced straightforwardly to an Integer Linear Programming (ILP) problem (Wolsey, 1998). Once oracle decoding is formulated as an ILP problem, it is relatively easy to introduce additional constraints, for instance to enforce n-gram clipping. We will first describe the optimization problem of oracle decoding and then present several ways to efficiently solve it.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>gram, and all weighted transitions of the kind (&#963;1 n&#8722;1 , &#963; n : &#963;1 n/&#952; n &#215; &#948; &#963; n</text>
              <doc_id>133</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(r), &#963;2 n ), where &#963;s are in &#931;, input word sequence &#963;1 n&#8722;1 and output sequence &#963;2 n , are, respectively, the maximal prefix and suffix of an n-gram &#963;1 n.</text>
              <doc_id>134</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In supplement, we add auxiliary states corresponding to m-grams (m &lt; n &#8722; 1), whose functional purpose is to help reach one of the main (n &#8722; 1)-gram states.</text>
              <doc_id>135</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>There are |&#931;|n&#8722;1 &#8722;1</text>
              <doc_id>136</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>|&#931;|&#8722;1 , n &gt; 1,</text>
              <doc_id>137</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>such supplementary states and their transitions are (&#963;1 k, &#963; k+1 : &#963;1 k+1 /0, &#963;1 k+1 ), k = 1 .</text>
              <doc_id>138</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>139</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>140</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>n&#8722;2.</text>
              <doc_id>141</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Apart from these auxiliary states, the rest of the graph (i.e., all final states) reproduces the structure of the well-known de Bruijn graph B(&#931;, n) (see Figure 1).</text>
              <doc_id>142</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>To actually compute the best hypothesis, we first weight all arcs in the input FSA L with &#952; 0 to obtain &#8710; 0 .</text>
              <doc_id>143</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>This makes each word&#8217;s weight equal in a hypothesis path, and the total weight of the path in &#8710; 0 is proportional to the number of words in it.</text>
              <doc_id>144</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Then, by sequentially composing &#8710; 0 with other &#8710; n s, we discount arcs whose output n-gram corresponds to a matching n-gram.</text>
              <doc_id>145</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>The amount of discount is regulated by the ratio between &#952; n &#8217;s for n &gt; 0.</text>
              <doc_id>146</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>With all operations performed over the (min, +)-semiring, the oracle translation is then given by:</text>
              <doc_id>147</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#960; &#8727; LB = ShortestPath(&#8710; 0 &#9702;&#8710; 1 &#9702;&#8710; 2 &#9702;&#8710; 3 &#9702;&#8710; 4 ).</text>
              <doc_id>148</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We set parameters &#952; n as in (Tromble et al., 2008): &#952; 0 = 1, roughly corresponding to the brevity penalty (each word in a hypothesis adds up equally to the final path length) and &#952; n = &#8722;(4p &#183; r n&#8722;1 ) &#8722;1 , which are increasing discounts</text>
              <doc_id>149</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>for matching n-grams.</text>
              <doc_id>150</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The values of p and r were found by grid search with a 0.05 step value.</text>
              <doc_id>151</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A typical result of the grid evaluation of the LB oracle for German to English WMT&#8217;11 task is displayed on Figure 2.</text>
              <doc_id>152</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The optimal values for the other pairs of languages were roughly in the same ballpark, with p &#8776; 0.3 and r &#8776; 0.2.</text>
              <doc_id>153</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>5 Oracles with n-gram Clipping</text>
              <doc_id>154</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this section, we describe two new oracle decoders that take n-gram clipping into account.</text>
              <doc_id>155</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>These oracles leverage on the well-known fact that the shortest path problem, at the heart of all the oracles described so far, can be reduced straightforwardly to an Integer Linear Programming (ILP) problem (Wolsey, 1998).</text>
              <doc_id>156</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Once oracle decoding is formulated as an ILP problem, it is relatively easy to introduce additional constraints, for instance to enforce n-gram clipping.</text>
              <doc_id>157</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We will first describe the optimization problem of oracle decoding and then present several ways to efficiently solve it.</text>
              <doc_id>158</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Problem Description</title>
            <text>Throughout this section, abusing the notations, we will also think of an edge &#958; i as a binary variable describing whether the edge is &#8220;selected&#8221; or not. The set {0, 1} #{&#926;} of all possible edge assignments will be denoted by P. Note that &#928;, the set of all paths in the lattice is a subset of P: by enforcing some constraints on an assignment &#958; in P, it can be guaranteed that it will represent a path in the lattice. For the sake of presentation, we assume that each edge &#958; i generates a single word w(&#958; i ) and we focus first on finding the optimal hypothesis with respect to the sentence approximation of the 1-BLEU score. As 1-BLEU is decomposable, it is possible to where &#920; 1 and &#920; 2 are two positive constants chosen to maximize the corpus BLEU score 6 . Constant &#920; 1 (resp. &#920; 2 ) is a reward (resp. a penalty) for generating a word in the reference (resp. not in the reference). The score of an assignment &#958; &#8712; P is then defined as: score(&#958;) = &#8721; #{&#926;}
i=1
&#958; i &#183; &#952; i . This score can be seen as a compromise between the number of common words in the hypothesis and the reference (accounting for recall) and the number of words of the hypothesis that do not appear in the reference (accounting for precision). As explained in Section 2.3, finding the oracle hypothesis amounts to solving the shortest distance (or path) problem (3), which can be reformulated by a constrained optimization problem (Wolsey, 1998):
#{&#926;}
&#8721; arg max &#958; i &#183; &#952; i (6)
&#958;&#8712;P
i=1
s.t. &#8721; &#8721; &#958; = 1, &#958; = 1
&#958;&#8712;&#926; &#8722; (q F )
&#8721;
&#958;&#8712;&#926; + (q)
&#958; &#8722;
&#8721;
&#958;&#8712;&#926; &#8722; (q)
&#958;&#8712;&#926; + (q 0 )
&#958; = 0, q &#8712; Q\{q 0 , q F }
where q 0 (resp. q F ) is the initial (resp. final) state of the lattice and &#926; &#8722; (q) (resp. &#926; + (q)) denotes the set of incoming (resp. outgoing) edges of state q. These path constraints ensure that the solution of the problem is a valid path in the lattice.
The optimization problem in Equation (6) can be further extended to take clipping into account. Let us introduce, for each word w, a variable &#947; w that denotes the number of times w appears in the hypothesis clipped to the number of times, it appears in the reference. Formally, &#947; w is defined by: &#9127; &#9131; &#9128; &#8721; &#9132; &#947; w = min &#958;, c w (r) &#9129; &#9133;
&#958;&#8712;&#937;(w)
6 We tried several combinations of &#920; 1 and &#920; 2 and kept
the one that had the highest corpus 4-BLEU score.
where &#937; (w) is the subset of edges generating w, and &#8721; &#958;&#8712;&#937;(w) &#958; is the number of occurrences of w in the solution and c w (r) is the number of occurrences of w in the reference r. Using the &#947; variables, we define a &#8220;clipped&#8221; approximation of
1-BLEU:
&#9115;
&#920; 1 &#183; &#8721; #{&#926;} &#8721;
&#947; w &#8722; &#920; 2 &#183; &#9117; &#958; i &#8722; &#8721;
w w i=1
&#947; w
Indeed, the clipped number of words in the hypothesis that appear in the reference is given by
&#8721;w &#947; w, and &#8721; #{&#926;} i=1
&#958; i &#8722; &#8721; w &#947; w corresponds to the number of words in the hypothesis that do not appear in the reference or that are surplus to the clipped count.
Finally, the clipped lattice oracle is defined by the following optimization problem:
arg max
&#958;&#8712;P,&#947; w
(&#920; 1 + &#920; 2 ) &#183; &#8721;
w
&#9118;
&#9120;
#{&#926;}
&#8721; &#947; w &#8722; &#920; 2 &#183;
s.t. &#947; w &#8805; 0, &#947; w &#8804; c w (r), &#947; w &#8804; &#8721;
&#8721;
&#958;&#8712;&#926; &#8722; (q F )
&#8721;
&#958;&#8712;&#926; + (q)
&#958; = 1,
&#958; &#8722;
&#8721;
&#958;&#8712;&#926; + (q 0 )
&#8721;
&#958;&#8712;&#926; &#8722; (q)
&#958; = 1
&#958;&#8712;&#937;(w)
i=1
&#958;
&#958; i
(7)
&#958; = 0, q &#8712; Q \ {q 0 , q F }
where the first three sets of constraints are the linearization of the definition of &#947; w , made possible by the positivity of &#920; 1 and &#920; 2 , and the last three sets of constraints are the path constraints.
In our implementation we generalized this optimization problem to bigram lattices, in which each edge is labeled by the bigram it generates. Such bigram FSAs can be produced by composing the word lattice with &#8710; 2 from Section 4. In this case, the reward of an edge will be defined as a combination of the (clipped) number of unigram matches and bigram matches, and solving the optimization problem yields a 2-BLEU optimal hypothesis. The approach can be further generalized to higher-order BLEU or other metrics, as long as the reward of an edge can be computed locally. The constrained optimization problem (7) can be solved efficiently using off-the-shelf ILP solvers 7 .
7 In our experiments we used Gurobi (Optimization,
2010) a commercial ILP solver that offers free academic license.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Throughout this section, abusing the notations, we will also think of an edge &#958; i as a binary variable describing whether the edge is &#8220;selected&#8221; or not.</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The set {0, 1} #{&#926;} of all possible edge assignments will be denoted by P.</text>
                  <doc_id>160</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Note that &#928;, the set of all paths in the lattice is a subset of P: by enforcing some constraints on an assignment &#958; in P, it can be guaranteed that it will represent a path in the lattice.</text>
                  <doc_id>161</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For the sake of presentation, we assume that each edge &#958; i generates a single word w(&#958; i ) and we focus first on finding the optimal hypothesis with respect to the sentence approximation of the 1-BLEU score.</text>
                  <doc_id>162</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>As 1-BLEU is decomposable, it is possible to where &#920; 1 and &#920; 2 are two positive constants chosen to maximize the corpus BLEU score 6 .</text>
                  <doc_id>163</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Constant &#920; 1 (resp.</text>
                  <doc_id>164</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>&#920; 2 ) is a reward (resp.</text>
                  <doc_id>165</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>a penalty) for generating a word in the reference (resp.</text>
                  <doc_id>166</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>not in the reference).</text>
                  <doc_id>167</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>The score of an assignment &#958; &#8712; P is then defined as: score(&#958;) = &#8721; #{&#926;}</text>
                  <doc_id>168</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>169</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958; i &#183; &#952; i .</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This score can be seen as a compromise between the number of common words in the hypothesis and the reference (accounting for recall) and the number of words of the hypothesis that do not appear in the reference (accounting for precision).</text>
                  <doc_id>171</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As explained in Section 2.3, finding the oracle hypothesis amounts to solving the shortest distance (or path) problem (3), which can be reformulated by a constrained optimization problem (Wolsey, 1998):</text>
                  <doc_id>172</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>#{&#926;}</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; arg max &#958; i &#183; &#952; i (6)</text>
                  <doc_id>174</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;P</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>s.t.</text>
                  <doc_id>177</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>&#8721; &#8721; &#958; = 1, &#958; = 1</text>
                  <doc_id>178</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;&#926; &#8722; (q F )</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;&#926; + (q)</text>
                  <doc_id>181</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958; &#8722;</text>
                  <doc_id>182</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;</text>
                  <doc_id>183</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;&#926; &#8722; (q)</text>
                  <doc_id>184</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;&#926; + (q 0 )</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958; = 0, q &#8712; Q\{q 0 , q F }</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where q 0 (resp.</text>
                  <doc_id>187</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>q F ) is the initial (resp.</text>
                  <doc_id>188</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>final) state of the lattice and &#926; &#8722; (q) (resp.</text>
                  <doc_id>189</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>&#926; + (q)) denotes the set of incoming (resp.</text>
                  <doc_id>190</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>outgoing) edges of state q.</text>
                  <doc_id>191</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>These path constraints ensure that the solution of the problem is a valid path in the lattice.</text>
                  <doc_id>192</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The optimization problem in Equation (6) can be further extended to take clipping into account.</text>
                  <doc_id>193</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Let us introduce, for each word w, a variable &#947; w that denotes the number of times w appears in the hypothesis clipped to the number of times, it appears in the reference.</text>
                  <doc_id>194</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Formally, &#947; w is defined by: &#9127; &#9131; &#9128; &#8721; &#9132; &#947; w = min &#958;, c w (r) &#9129; &#9133;</text>
                  <doc_id>195</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;&#937;(w)</text>
                  <doc_id>196</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>6 We tried several combinations of &#920; 1 and &#920; 2 and kept</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the one that had the highest corpus 4-BLEU score.</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where &#937; (w) is the subset of edges generating w, and &#8721; &#958;&#8712;&#937;(w) &#958; is the number of occurrences of w in the solution and c w (r) is the number of occurrences of w in the reference r.</text>
                  <doc_id>199</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Using the &#947; variables, we define a &#8220;clipped&#8221; approximation of</text>
                  <doc_id>200</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1-BLEU:</text>
                  <doc_id>201</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9115;</text>
                  <doc_id>202</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#920; 1 &#183; &#8721; #{&#926;} &#8721;</text>
                  <doc_id>203</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#947; w &#8722; &#920; 2 &#183; &#9117; &#958; i &#8722; &#8721;</text>
                  <doc_id>204</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w w i=1</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#947; w</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Indeed, the clipped number of words in the hypothesis that appear in the reference is given by</text>
                  <doc_id>207</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;w &#947; w, and &#8721; #{&#926;} i=1</text>
                  <doc_id>208</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958; i &#8722; &#8721; w &#947; w corresponds to the number of words in the hypothesis that do not appear in the reference or that are surplus to the clipped count.</text>
                  <doc_id>209</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Finally, the clipped lattice oracle is defined by the following optimization problem:</text>
                  <doc_id>210</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>arg max</text>
                  <doc_id>211</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;P,&#947; w</text>
                  <doc_id>212</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(&#920; 1 + &#920; 2 ) &#183; &#8721;</text>
                  <doc_id>213</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w</text>
                  <doc_id>214</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9118;</text>
                  <doc_id>215</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9120;</text>
                  <doc_id>216</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>#{&#926;}</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; &#947; w &#8722; &#920; 2 &#183;</text>
                  <doc_id>218</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>s.t.</text>
                  <doc_id>219</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>&#947; w &#8805; 0, &#947; w &#8804; c w (r), &#947; w &#8804; &#8721;</text>
                  <doc_id>220</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;</text>
                  <doc_id>221</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;&#926; &#8722; (q F )</text>
                  <doc_id>222</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;</text>
                  <doc_id>223</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;&#926; + (q)</text>
                  <doc_id>224</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958; = 1,</text>
                  <doc_id>225</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958; &#8722;</text>
                  <doc_id>226</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;</text>
                  <doc_id>227</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;&#926; + (q 0 )</text>
                  <doc_id>228</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;</text>
                  <doc_id>229</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;&#926; &#8722; (q)</text>
                  <doc_id>230</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958; = 1</text>
                  <doc_id>231</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;&#937;(w)</text>
                  <doc_id>232</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>233</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;</text>
                  <doc_id>234</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958; i</text>
                  <doc_id>235</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(7)</text>
                  <doc_id>236</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958; = 0, q &#8712; Q \ {q 0 , q F }</text>
                  <doc_id>237</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where the first three sets of constraints are the linearization of the definition of &#947; w , made possible by the positivity of &#920; 1 and &#920; 2 , and the last three sets of constraints are the path constraints.</text>
                  <doc_id>238</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In our implementation we generalized this optimization problem to bigram lattices, in which each edge is labeled by the bigram it generates.</text>
                  <doc_id>239</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Such bigram FSAs can be produced by composing the word lattice with &#8710; 2 from Section 4.</text>
                  <doc_id>240</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In this case, the reward of an edge will be defined as a combination of the (clipped) number of unigram matches and bigram matches, and solving the optimization problem yields a 2-BLEU optimal hypothesis.</text>
                  <doc_id>241</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The approach can be further generalized to higher-order BLEU or other metrics, as long as the reward of an edge can be computed locally.</text>
                  <doc_id>242</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The constrained optimization problem (7) can be solved efficiently using off-the-shelf ILP solvers 7 .</text>
                  <doc_id>243</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>7 In our experiments we used Gurobi (Optimization,</text>
                  <doc_id>244</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2010) a commercial ILP solver that offers free academic license.</text>
                  <doc_id>245</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Shortest Path Oracle (SP)</title>
            <text>As a trivial special class of the above formulation, we also define a Shortest Path Oracle (SP) that solves the optimization problem in (6). As no clipping constraints apply, it can be solved efficiently using the standard Bellman algorithm.
5.3 Oracle Decoding through Lagrangian Relaxation (RLX)
In this section, we introduce another method to solve problem (7) without relying on an external ILP solver. Following (Rush et al., 2010; Chang and Collins, 2011), we propose an original method for oracle decoding based on Lagrangian relaxation. This method relies on the idea of relaxing the clipping constraints: starting from an unconstrained problem, the counts clipping is enforced by incrementally strengthening the weight of paths satisfying the constraints.
The oracle decoding problem with clipping constraints amounts to solving:
arg min
&#958;&#8712;&#928;
s.t.
#{&#926;}
&#8721; &#8722;
i=1
&#8721;
&#958;&#8712;&#937;(w)
&#958; i &#183; &#952; i (8)
&#958; &#8804; c w (r), w &#8712; r
where, by abusing the notations, r also denotes the set of words in the reference. For sake of clarity, the path constraints are incorporated into the domain (the arg min runs over &#928; and not over P). To solve this optimization problem we consider its dual form and use Lagrangian relaxation to deal with clipping constraints.
Let &#955; = {&#955; w } w&#8712;r be positive Lagrange multipliers, one for each different word of the reference, then the Lagrangian of the problem (8) is:
#{&#926;}
&#8721; L(&#955;, &#958;) = &#8722; &#958; i &#952; i + &#8721; &#955; w &#9117; &#8721;
w&#8712;r i=1
&#9115;
&#958;&#8712;&#937;(w)
&#9118;
&#958; &#8722; c w (r) &#9120;
The dual objective is L(&#955;) = min &#958; L(&#955;, &#958;) and the dual problem is: max &#955;,&#955;&#8829;0 L(&#955;). To solve the latter, we first need to work out the dual objective:
&#958; &#8727; = arg min L(&#955;, &#958;)
&#958;&#8712;&#928;
#{&#926;}
&#8721; ( ) = arg min &#958; i &#955;w(&#958;i ) &#8722; &#952; i
&#958;&#8712;&#928; i=1
where we assume that &#955; w(&#958;i ) is 0 when word w(&#958; i ) is not in the reference. In the same way as in Section 5.2, the solution of this problem can be efficiently retrieved with a shortest path algorithm.
It is possible to optimize L(&#955;) by noticing that it is a concave function. It can be shown (Chang and Collins, 2011) that, at convergence, the clipping constraints will be enforced in the optimal solution. In this work, we chose to use a simple gradient descent to solve the dual problem. A subgradient of the dual objective is:
&#8706;L(&#955;) &#8706;&#955; w = &#8721;
&#958;&#8712;&#937;(w)&#8745;&#958; &#8727; &#958; &#8722; c w (r).
Each component of the gradient corresponds to the difference between the number of times the word w appears in the hypothesis and the number of times it appears in the reference. The algorithm below sums up the optimization of task (8). In the algorithm &#945; (t) corresponds to the step size at the t th iteration. In our experiments we used a constant step size of 0.1. Compared to the usual gradient descent algorithm, there is an additional projection step of &#955; on the positive orthant, which enforces the constraint &#955; &#8829; 0.
&#8704;w, &#955; (0) w &#8592; 0 for t = 1 &#8594; T do
&#958; &#8727;(t) &#8721; = arg min &#958; i &#958; i &#183; (&#955; )
w(&#958;i ) &#8722; &#952; i
if all clipping constraints are enforced then optimal solution found else for w &#8712; r do
n w &#8592; n. of occurrences of w in &#958; &#8727;(t) &#955; (t) w &#955; (t) w
&#8592; &#955; (t) w</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>As a trivial special class of the above formulation, we also define a Shortest Path Oracle (SP) that solves the optimization problem in (6).</text>
                  <doc_id>246</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As no clipping constraints apply, it can be solved efficiently using the standard Bellman algorithm.</text>
                  <doc_id>247</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5.3 Oracle Decoding through Lagrangian Relaxation (RLX)</text>
                  <doc_id>248</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In this section, we introduce another method to solve problem (7) without relying on an external ILP solver.</text>
                  <doc_id>249</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Following (Rush et al., 2010; Chang and Collins, 2011), we propose an original method for oracle decoding based on Lagrangian relaxation.</text>
                  <doc_id>250</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This method relies on the idea of relaxing the clipping constraints: starting from an unconstrained problem, the counts clipping is enforced by incrementally strengthening the weight of paths satisfying the constraints.</text>
                  <doc_id>251</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The oracle decoding problem with clipping constraints amounts to solving:</text>
                  <doc_id>252</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>arg min</text>
                  <doc_id>253</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;&#928;</text>
                  <doc_id>254</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>s.t.</text>
                  <doc_id>255</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>#{&#926;}</text>
                  <doc_id>256</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; &#8722;</text>
                  <doc_id>257</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>258</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;</text>
                  <doc_id>259</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;&#937;(w)</text>
                  <doc_id>260</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958; i &#183; &#952; i (8)</text>
                  <doc_id>261</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958; &#8804; c w (r), w &#8712; r</text>
                  <doc_id>262</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where, by abusing the notations, r also denotes the set of words in the reference.</text>
                  <doc_id>263</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For sake of clarity, the path constraints are incorporated into the domain (the arg min runs over &#928; and not over P).</text>
                  <doc_id>264</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>To solve this optimization problem we consider its dual form and use Lagrangian relaxation to deal with clipping constraints.</text>
                  <doc_id>265</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Let &#955; = {&#955; w } w&#8712;r be positive Lagrange multipliers, one for each different word of the reference, then the Lagrangian of the problem (8) is:</text>
                  <doc_id>266</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>#{&#926;}</text>
                  <doc_id>267</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; L(&#955;, &#958;) = &#8722; &#958; i &#952; i + &#8721; &#955; w &#9117; &#8721;</text>
                  <doc_id>268</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w&#8712;r i=1</text>
                  <doc_id>269</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9115;</text>
                  <doc_id>270</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;&#937;(w)</text>
                  <doc_id>271</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9118;</text>
                  <doc_id>272</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958; &#8722; c w (r) &#9120;</text>
                  <doc_id>273</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The dual objective is L(&#955;) = min &#958; L(&#955;, &#958;) and the dual problem is: max &#955;,&#955;&#8829;0 L(&#955;).</text>
                  <doc_id>274</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To solve the latter, we first need to work out the dual objective:</text>
                  <doc_id>275</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958; &#8727; = arg min L(&#955;, &#958;)</text>
                  <doc_id>276</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;&#928;</text>
                  <doc_id>277</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>#{&#926;}</text>
                  <doc_id>278</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; ( ) = arg min &#958; i &#955;w(&#958;i ) &#8722; &#952; i</text>
                  <doc_id>279</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;&#928; i=1</text>
                  <doc_id>280</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where we assume that &#955; w(&#958;i ) is 0 when word w(&#958; i ) is not in the reference.</text>
                  <doc_id>281</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the same way as in Section 5.2, the solution of this problem can be efficiently retrieved with a shortest path algorithm.</text>
                  <doc_id>282</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It is possible to optimize L(&#955;) by noticing that it is a concave function.</text>
                  <doc_id>283</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It can be shown (Chang and Collins, 2011) that, at convergence, the clipping constraints will be enforced in the optimal solution.</text>
                  <doc_id>284</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In this work, we chose to use a simple gradient descent to solve the dual problem.</text>
                  <doc_id>285</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>A subgradient of the dual objective is:</text>
                  <doc_id>286</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8706;L(&#955;) &#8706;&#955; w = &#8721;</text>
                  <doc_id>287</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958;&#8712;&#937;(w)&#8745;&#958; &#8727; &#958; &#8722; c w (r).</text>
                  <doc_id>288</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Each component of the gradient corresponds to the difference between the number of times the word w appears in the hypothesis and the number of times it appears in the reference.</text>
                  <doc_id>289</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The algorithm below sums up the optimization of task (8).</text>
                  <doc_id>290</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In the algorithm &#945; (t) corresponds to the step size at the t th iteration.</text>
                  <doc_id>291</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In our experiments we used a constant step size of 0.1.</text>
                  <doc_id>292</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Compared to the usual gradient descent algorithm, there is an additional projection step of &#955; on the positive orthant, which enforces the constraint &#955; &#8829; 0.</text>
                  <doc_id>293</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8704;w, &#955; (0) w &#8592; 0 for t = 1 &#8594; T do</text>
                  <doc_id>294</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#958; &#8727;(t) &#8721; = arg min &#958; i &#958; i &#183; (&#955; )</text>
                  <doc_id>295</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w(&#958;i ) &#8722; &#952; i</text>
                  <doc_id>296</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>if all clipping constraints are enforced then optimal solution found else for w &#8712; r do</text>
                  <doc_id>297</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n w &#8592; n.</text>
                  <doc_id>298</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>of occurrences of w in &#958; &#8727;(t) &#955; (t) w &#955; (t) w</text>
                  <doc_id>299</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8592; &#955; (t) w</text>
                  <doc_id>300</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>6 Experiments</title>
        <text>+ &#945; (t) &#183; (n w &#8722; c w (r)) &#8592; max(0, &#955; (t) w )
For the proposed new oracles and the existing approaches, we compare the quality of oracle translations and the average time per sentence needed to compute them 8 on several datasets for 3 language pairs, using lattices generated by two opensource decoders: N-code and Moses 9 (Figures 3
8 Experiments were run in parallel on a server with 64G
of RAM and 2 Xeon CPUs with 4 cores at 2.3 GHz. 9 As the ILP (and RLX) oracle were implemented in
Python, we pruned Moses lattices to accelerate task preparation for it.
test oracle decoder fr2en de2en en2de
and 4). Systems were trained on the data provided for the WMT&#8217;11 Evaluation task 10 , tuned on the WMT&#8217;09 test data and evaluated on WMT&#8217;10 test set 11 to produce lattices. The BLEU test scores and oracle scores on 100-best lists with the approximation (4) for N-code and Moses are given in Table 2. It is not until considering 10,000-best lists that n-best oracles achieve performance comparable to the (mediocre) SP oracle.
To make a fair comparison with the ILP and RLX oracles which optimize 2-BLEU, we included 2-BLEU versions of the LB and LM oracles, identified below with the &#8220;-2g&#8221; suffix. The two versions of the PB oracle are respectively denoted as PB and PBl, by the type of the &#8853;- operation they consider (Section 3.2). Parameters p and r for the LB-4g oracle for N-code were found with grid search and reused for Moses: p = 0.25, r = 0.15 (fr2en); p = 0.175, r = 0.575 (en2de) and p = 0.35, r = 0.425 (de2en). Correspondingly, for the LB-2g oracle: p = 0.3, r = 0.15; p = 0.3, r = 0.175 and p = 0.575, r = 0.1.
The proposed LB, ILP and RLX oracles were the best performing oracles, with the ILP and RLX oracles being considerably faster, suffering only a negligible decrease in BLEU, compared to the 4-BLEU-optimized LB oracle. We stopped RLX oracle after 20 iterations, as letting it converge had a small negative effect (&#8764;1 point of the corpus BLEU), because of the sentence/corpus discrepancy ushered by the BLEU score approximation. Experiments showed consistently inferior performance of the LM-oracle resulting from the optimization of the sentence probability rather than
BLEU. The PB oracle often performed comparably to our new oracles, however, with sporadic resource-consumption bursts, that are difficult to
10 http://www.statmt.org/wmt2011 11 All BLEU scores are reported using the multi-bleu.pl
script.
avoid without more cursory hypotheses recombination strategies and the induced effect on the translations quality. The length-aware PBl oracle has unexpectedly poorer scores compared to its length-agnostic PB counterpart, while it should, at least, stay even, as it takes the brevity penalty into account. We attribute this fact to the complex effect of clipping coupled with the lack of control of the process of selecting one hypothesis among several having the same BLEU score, length and recent history. Anyhow, BLEU scores of both of PB oracles are only marginally different, so the PBl&#8217;s conservative policy of pruning and, consequently, much heavier memory consumption makes it an unwanted choice.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>+ &#945; (t) &#183; (n w &#8722; c w (r)) &#8592; max(0, &#955; (t) w )</text>
              <doc_id>301</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For the proposed new oracles and the existing approaches, we compare the quality of oracle translations and the average time per sentence needed to compute them 8 on several datasets for 3 language pairs, using lattices generated by two opensource decoders: N-code and Moses 9 (Figures 3</text>
              <doc_id>302</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>8 Experiments were run in parallel on a server with 64G</text>
              <doc_id>303</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>of RAM and 2 Xeon CPUs with 4 cores at 2.3 GHz.</text>
              <doc_id>304</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>9 As the ILP (and RLX) oracle were implemented in</text>
              <doc_id>305</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Python, we pruned Moses lattices to accelerate task preparation for it.</text>
              <doc_id>306</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>test oracle decoder fr2en de2en en2de</text>
              <doc_id>307</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and 4).</text>
              <doc_id>308</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Systems were trained on the data provided for the WMT&#8217;11 Evaluation task 10 , tuned on the WMT&#8217;09 test data and evaluated on WMT&#8217;10 test set 11 to produce lattices.</text>
              <doc_id>309</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The BLEU test scores and oracle scores on 100-best lists with the approximation (4) for N-code and Moses are given in Table 2.</text>
              <doc_id>310</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>It is not until considering 10,000-best lists that n-best oracles achieve performance comparable to the (mediocre) SP oracle.</text>
              <doc_id>311</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To make a fair comparison with the ILP and RLX oracles which optimize 2-BLEU, we included 2-BLEU versions of the LB and LM oracles, identified below with the &#8220;-2g&#8221; suffix.</text>
              <doc_id>312</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The two versions of the PB oracle are respectively denoted as PB and PBl, by the type of the &#8853;- operation they consider (Section 3.2).</text>
              <doc_id>313</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Parameters p and r for the LB-4g oracle for N-code were found with grid search and reused for Moses: p = 0.25, r = 0.15 (fr2en); p = 0.175, r = 0.575 (en2de) and p = 0.35, r = 0.425 (de2en).</text>
              <doc_id>314</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Correspondingly, for the LB-2g oracle: p = 0.3, r = 0.15; p = 0.3, r = 0.175 and p = 0.575, r = 0.1.</text>
              <doc_id>315</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The proposed LB, ILP and RLX oracles were the best performing oracles, with the ILP and RLX oracles being considerably faster, suffering only a negligible decrease in BLEU, compared to the 4-BLEU-optimized LB oracle.</text>
              <doc_id>316</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We stopped RLX oracle after 20 iterations, as letting it converge had a small negative effect (&#8764;1 point of the corpus BLEU), because of the sentence/corpus discrepancy ushered by the BLEU score approximation.</text>
              <doc_id>317</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Experiments showed consistently inferior performance of the LM-oracle resulting from the optimization of the sentence probability rather than</text>
              <doc_id>318</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>BLEU.</text>
              <doc_id>319</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The PB oracle often performed comparably to our new oracles, however, with sporadic resource-consumption bursts, that are difficult to</text>
              <doc_id>320</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>10 http://www.statmt.org/wmt2011 11 All BLEU scores are reported using the multi-bleu.pl</text>
              <doc_id>321</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>script.</text>
              <doc_id>322</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>avoid without more cursory hypotheses recombination strategies and the induced effect on the translations quality.</text>
              <doc_id>323</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The length-aware PBl oracle has unexpectedly poorer scores compared to its length-agnostic PB counterpart, while it should, at least, stay even, as it takes the brevity penalty into account.</text>
              <doc_id>324</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We attribute this fact to the complex effect of clipping coupled with the lack of control of the process of selecting one hypothesis among several having the same BLEU score, length and recent history.</text>
              <doc_id>325</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Anyhow, BLEU scores of both of PB oracles are only marginally different, so the PBl&#8217;s conservative policy of pruning and, consequently, much heavier memory consumption makes it an unwanted choice.</text>
              <doc_id>326</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>7 Conclusion</title>
        <text>We proposed two methods for finding oracle translations in lattices, based, respectively, on a linear approximation to the corpus-level BLEU and on integer linear programming techniques. We also proposed a variant of the latter approach based on Lagrangian relaxation that does not rely on a third-party ILP solver. All these oracles have superior performance to existing approaches, in terms of the quality of the found translations, resource consumption and, for the LB-2g oracles, in terms of speed. It is thus possible to use better approximations of BLEU than was previously done, taking the corpus-based nature of BLEU, or clipping constrainst into account, delivering better oracles without compromising speed.
Using 2-BLEU and 4-BLEU oracles yields comparable performance, which confirms the intuition that hypotheses sharing many 2-grams, would likely have many common 3- and 4-grams as well. Taking into consideration the exceptional speed of the LB-2g oracle, in practice one can safely optimize for 2-BLEU instead of 4-BLEU, saving large amounts of time for oracle decoding on long sentences.
Overall, these experiments accentuate the acuteness of scoring problems that plague modern decoders: very good hypotheses exist for most input sentences, but are poorly evaluated by a linear combination of standard features functions. Even though the tuning procedure can be held responsible for part of the problem, the comparison between lattice and n-best oracles shows that the beam search leaves good hypotheses out of the n- best list until very high value of n, that are never used in practice.
Acknowledgments
This work has been partially funded by OSEO under the Quaero program.
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A general and efficient weighted finite-state transducer library. In Proc. of the Int. Conf. on Implementation and Application of Automata, pages 11&#8211; 23. Michael Auli, Adam Lopez, Hieu Hoang, and Philipp
Koehn. 2009. A systematic analysis of translation model search spaces. In Proc. of WMT, pages 224&#8211; 232, Athens, Greece. Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation, pages 65&#8211;72, Ann Arbor, MI, USA. Graeme Blackwood, Adri&#224; de Gispert, and William
Byrne. 2010. Efficient path counting transducers for minimum bayes-risk decoding of statistical machine translation lattices. In Proc. of the ACL 2010 Conference Short Papers, pages 27&#8211;32, Stroudsburg, PA, USA. Yin-Wen Chang and Michael Collins. 2011. Exact decoding of phrase-based translation models through lagrangian relaxation. In Proc. of the 2011 Conf. on EMNLP, pages 26&#8211;37, Edinburgh, UK. David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proc. of the 2008 Conf. on EMNLP, pages 224&#8211;233, Honolulu, Hawaii. Markus Dreyer, Keith B. Hall, and Sanjeev P. Khudanpur. 2007. Comparing reordering constraints for SMT using efficient BLEU oracle computation. In Proc. of the Workshop on Syntax and Structure in Statistical Translation, pages 103&#8211;110, Morristown, NJ, USA. Gregor Leusch, Evgeny Matusov, and Hermann Ney.
2008. Complexity of finding the BLEU-optimal hypothesis in a confusion network. In Proc. of the 2008 Conf. on EMNLP, pages 839&#8211;847, Honolulu, Hawaii. Zhifei Li and Sanjeev Khudanpur. 2009. Efficient extraction of oracle-best translations from hypergraphs. In Proc. of Human Language Technologies: The 2009 Annual Conf. of the North American Chapter of the ACL, Companion Volume: Short Papers, pages 9&#8211;12, Morristown, NJ, USA. Percy Liang, Alexandre Bouchard-C&#244;t&#233;, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proc. of the 21st Int. Conf. on Computational Linguistics and the 44th annual meeting of the ACL, pages 761&#8211; 768, Morristown, NJ, USA.
Mehryar Mohri. 2002. Semiring frameworks and algorithms for shortest-distance problems. J. Autom. Lang. Comb., 7:321&#8211;350. Mehryar Mohri. 2009. Weighted automata algorithms. In Manfred Droste, Werner Kuich, and Heiko Vogler, editors, Handbook of Weighted Automata, chapter 6, pages 213&#8211;254. Gurobi Optimization. 2010. Gurobi optimizer, April.
Version 3.0. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of the Annual Meeting of the ACL, pages 311&#8211;318. Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proc. of the 2010 Conf. on EMNLP, pages 1&#8211;11, Stroudsburg, PA, USA. Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proc. of the Conf. of the Association for Machine Translation in the America (AMTA), pages 223&#8211;231. Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice minimum bayes-risk decoding for statistical machine translation. In Proc. of the Conf. on EMNLP, pages 620&#8211; 629, Stroudsburg, PA, USA. Marco Turchi, Tijl De Bie, and Nello Cristianini. 2008. Learning performance of a machine translation system: a statistical and computational analysis. In Proc. of WMT, pages 35&#8211;43, Columbus, Ohio. Guillaume Wisniewski, Alexandre Allauzen, and Fran&#231;ois Yvon. 2010. Assessing phrase-based translation models with oracle decoding. In Proc. of the 2010 Conf. on EMNLP, pages 933&#8211;943, Stroudsburg, PA, USA. L. Wolsey. 1998. Integer Programming. John Wiley
&amp; Sons, Inc.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We proposed two methods for finding oracle translations in lattices, based, respectively, on a linear approximation to the corpus-level BLEU and on integer linear programming techniques.</text>
              <doc_id>327</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We also proposed a variant of the latter approach based on Lagrangian relaxation that does not rely on a third-party ILP solver.</text>
              <doc_id>328</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>All these oracles have superior performance to existing approaches, in terms of the quality of the found translations, resource consumption and, for the LB-2g oracles, in terms of speed.</text>
              <doc_id>329</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>It is thus possible to use better approximations of BLEU than was previously done, taking the corpus-based nature of BLEU, or clipping constrainst into account, delivering better oracles without compromising speed.</text>
              <doc_id>330</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Using 2-BLEU and 4-BLEU oracles yields comparable performance, which confirms the intuition that hypotheses sharing many 2-grams, would likely have many common 3- and 4-grams as well.</text>
              <doc_id>331</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Taking into consideration the exceptional speed of the LB-2g oracle, in practice one can safely optimize for 2-BLEU instead of 4-BLEU, saving large amounts of time for oracle decoding on long sentences.</text>
              <doc_id>332</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Overall, these experiments accentuate the acuteness of scoring problems that plague modern decoders: very good hypotheses exist for most input sentences, but are poorly evaluated by a linear combination of standard features functions.</text>
              <doc_id>333</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Even though the tuning procedure can be held responsible for part of the problem, the comparison between lattice and n-best oracles shows that the beam search leaves good hypotheses out of the n- best list until very high value of n, that are never used in practice.</text>
              <doc_id>334</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Acknowledgments</text>
              <doc_id>335</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>This work has been partially funded by OSEO under the Quaero program.</text>
              <doc_id>336</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>References</text>
              <doc_id>337</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri.</text>
              <doc_id>338</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>339</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>OpenFst: A general and efficient weighted finite-state transducer library.</text>
              <doc_id>340</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the Int.</text>
              <doc_id>341</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Conf. on Implementation and Application of Automata, pages 11&#8211; 23.</text>
              <doc_id>342</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Michael Auli, Adam Lopez, Hieu Hoang, and Philipp</text>
              <doc_id>343</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Koehn.</text>
              <doc_id>344</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>345</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A systematic analysis of translation model search spaces.</text>
              <doc_id>346</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of WMT, pages 224&#8211; 232, Athens, Greece.</text>
              <doc_id>347</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Satanjeev Banerjee and Alon Lavie.</text>
              <doc_id>348</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>349</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>ME-</text>
              <doc_id>350</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>TEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</text>
              <doc_id>351</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation, pages 65&#8211;72, Ann Arbor, MI, USA.</text>
              <doc_id>352</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Graeme Blackwood, Adri&#224; de Gispert, and William</text>
              <doc_id>353</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Byrne.</text>
              <doc_id>354</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>355</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Efficient path counting transducers for minimum bayes-risk decoding of statistical machine translation lattices.</text>
              <doc_id>356</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the ACL 2010 Conference Short Papers, pages 27&#8211;32, Stroudsburg, PA, USA.</text>
              <doc_id>357</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Yin-Wen Chang and Michael Collins.</text>
              <doc_id>358</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2011.</text>
              <doc_id>359</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Exact decoding of phrase-based translation models through lagrangian relaxation.</text>
              <doc_id>360</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the 2011 Conf. on EMNLP, pages 26&#8211;37, Edinburgh, UK.</text>
              <doc_id>361</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>David Chiang, Yuval Marton, and Philip Resnik.</text>
              <doc_id>362</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>363</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Online large-margin training of syntactic and structural translation features.</text>
              <doc_id>364</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the 2008 Conf. on EMNLP, pages 224&#8211;233, Honolulu, Hawaii.</text>
              <doc_id>365</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Markus Dreyer, Keith B. Hall, and Sanjeev P. Khudanpur.</text>
              <doc_id>366</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>367</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>Comparing reordering constraints for SMT using efficient BLEU oracle computation.</text>
              <doc_id>368</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the Workshop on Syntax and Structure in Statistical Translation, pages 103&#8211;110, Morristown, NJ, USA.</text>
              <doc_id>369</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>Gregor Leusch, Evgeny Matusov, and Hermann Ney.</text>
              <doc_id>370</doc_id>
              <sec_id>16</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2008.</text>
              <doc_id>371</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Complexity of finding the BLEU-optimal hypothesis in a confusion network.</text>
              <doc_id>372</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the 2008 Conf. on EMNLP, pages 839&#8211;847, Honolulu, Hawaii.</text>
              <doc_id>373</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Zhifei Li and Sanjeev Khudanpur.</text>
              <doc_id>374</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>375</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Efficient extraction of oracle-best translations from hypergraphs.</text>
              <doc_id>376</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of Human Language Technologies: The 2009 Annual Conf. of the North American Chapter of the ACL, Companion Volume: Short Papers, pages 9&#8211;12, Morristown, NJ, USA.</text>
              <doc_id>377</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Percy Liang, Alexandre Bouchard-C&#244;t&#233;, Dan Klein, and Ben Taskar.</text>
              <doc_id>378</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>379</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>An end-to-end discriminative approach to machine translation.</text>
              <doc_id>380</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the 21st Int.</text>
              <doc_id>381</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>Conf. on Computational Linguistics and the 44th annual meeting of the ACL, pages 761&#8211; 768, Morristown, NJ, USA.</text>
              <doc_id>382</doc_id>
              <sec_id>11</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Mehryar Mohri.</text>
              <doc_id>383</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>384</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Semiring frameworks and algorithms for shortest-distance problems.</text>
              <doc_id>385</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>J. Autom.</text>
              <doc_id>386</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Lang.</text>
              <doc_id>387</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Comb., 7:321&#8211;350.</text>
              <doc_id>388</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Mehryar Mohri.</text>
              <doc_id>389</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>390</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Weighted automata algorithms.</text>
              <doc_id>391</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>In Manfred Droste, Werner Kuich, and Heiko Vogler, editors, Handbook of Weighted Automata, chapter 6, pages 213&#8211;254.</text>
              <doc_id>392</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Gurobi Optimization.</text>
              <doc_id>393</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>394</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Gurobi optimizer, April.</text>
              <doc_id>395</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Version 3.0.</text>
              <doc_id>396</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.</text>
              <doc_id>397</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>398</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>BLEU: a method for automatic evaluation of machine translation.</text>
              <doc_id>399</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the Annual Meeting of the ACL, pages 311&#8211;318.</text>
              <doc_id>400</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Alexander M. Rush, David Sontag, Michael Collins,</text>
              <doc_id>401</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and Tommi Jaakkola.</text>
              <doc_id>402</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>403</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>On dual decomposition and linear programming relaxations for natural language processing.</text>
              <doc_id>404</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the 2010 Conf. on EMNLP, pages 1&#8211;11, Stroudsburg, PA, USA.</text>
              <doc_id>405</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul.</text>
              <doc_id>406</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>407</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>A study of translation edit rate with targeted human annotation.</text>
              <doc_id>408</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the Conf. of the Association for Machine Translation in the America (AMTA), pages 223&#8211;231.</text>
              <doc_id>409</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Roy W. Tromble, Shankar Kumar, Franz Och, and</text>
              <doc_id>410</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Wolfgang Macherey.</text>
              <doc_id>411</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>412</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Lattice minimum bayes-risk decoding for statistical machine translation.</text>
              <doc_id>413</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the Conf. on EMNLP, pages 620&#8211; 629, Stroudsburg, PA, USA.</text>
              <doc_id>414</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Marco Turchi, Tijl De Bie, and Nello Cristianini.</text>
              <doc_id>415</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>416</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Learning performance of a machine translation system: a statistical and computational analysis.</text>
              <doc_id>417</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of WMT, pages 35&#8211;43, Columbus, Ohio.</text>
              <doc_id>418</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Guillaume Wisniewski, Alexandre Allauzen, and Fran&#231;ois Yvon.</text>
              <doc_id>419</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>2010.</text>
              <doc_id>420</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Assessing phrase-based translation models with oracle decoding.</text>
              <doc_id>421</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of the 2010 Conf. on EMNLP, pages 933&#8211;943, Stroudsburg, PA, USA.</text>
              <doc_id>422</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>L. Wolsey.</text>
              <doc_id>423</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>1998.</text>
              <doc_id>424</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>Integer Programming.</text>
              <doc_id>425</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>John Wiley</text>
              <doc_id>426</doc_id>
              <sec_id>15</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&amp; Sons, Inc.</text>
              <doc_id>427</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Recapitulative overview of oracle decoders.</caption>
        <reference_text>In PAGE 3: ... These complexity results imply that any oracle decoder has to waive either the form of the objec- tive function, replacing BLEU with better-behaved scoring functions, or the exactness of the solu- tion, relying on approximate heuristic search al- gorithms. In  Table1 , we summarize different compro- mises that the existing (section 3), as well as our novel (sections 4 and 5) oracle decoders, have to make. The  target  and  target level  columns specify the targeted score....</reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>oracle</cell>
              <cell>target</cell>
              <cell>target level</cell>
              <cell>target replacement</cell>
              <cell>search</cell>
              <cell>clipping</cell>
              <cell>brevity</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>LM-2g/4g</cell>
              <cell>2/4-BLEU</cell>
              <cell>sentence</cell>
              <cell>P2(e</cell>
              <cell>r) or P4(e</cell>
              <cell>r)</cell>
              <cell>exact</cell>
              <cell>no</cell>
              <cell>no</cell>
            </row>
            <row>
              <cell>PB</cell>
              <cell>4-BLEU</cell>
              <cell>sentence</cell>
              <cell>partial log BLEU (4)</cell>
              <cell>appr.</cell>
              <cell>no</cell>
              <cell>no</cell>
            </row>
            <row>
              <cell>xisting e PBlscript</cell>
              <cell>4-BLEU</cell>
              <cell>sentence</cell>
              <cell>partial log BLEU (4)</cell>
              <cell>appr.</cell>
              <cell>no</cell>
              <cell>yes</cell>
            </row>
            <row>
              <cell>LB-2g/4g</cell>
              <cell>2/4-BLEU</cell>
              <cell>corpus</cell>
              <cell>linear appr. lin BLEU (5)</cell>
              <cell>exact</cell>
              <cell>no</cell>
              <cell>yes</cell>
            </row>
            <row>
              <cell>SP paper</cell>
              <cell>1-BLEU</cell>
              <cell>sentence</cell>
              <cell>unigram count</cell>
              <cell>exact</cell>
              <cell>no</cell>
              <cell>yes</cell>
            </row>
            <row>
              <cell>ILP</cell>
              <cell>2-BLEU</cell>
              <cell>sentence</cell>
              <cell>uni/bi-gram counts (7)</cell>
              <cell>appr.</cell>
              <cell>yes</cell>
              <cell>yes</cell>
            </row>
            <row>
              <cell>this RLX</cell>
              <cell>2-BLEU</cell>
              <cell>sentence</cell>
              <cell>uni/bi-gram counts (8)</cell>
              <cell>exact</cell>
              <cell>yes</cell>
              <cell>yes</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Test BLEU scores and oracle scores on 100-best lists for the evaluated systems.</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>N-code</cell>
              <cell>27.88</cell>
              <cell>22.05</cell>
              <cell>15.83</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Moses</cell>
              <cell>27.68</cell>
              <cell>21.85</cell>
              <cell>15.89</cell>
            </row>
            <row>
              <cell>N-code</cell>
              <cell>36.36</cell>
              <cell>29.22</cell>
              <cell>21.18</cell>
            </row>
            <row>
              <cell>Moses</cell>
              <cell>35.25</cell>
              <cell>29.13</cell>
              <cell>22.03</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references/>
    <citations/>
  </content>
</document>
