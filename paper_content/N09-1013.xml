<document>
  <filename>N09-1013</filename>
  <authors>
    <author>Jamie Brunning</author>
    <author>Adri&#224; de_Gispert</author>
  </authors>
  <title>Context-Dependent Alignment Models for Statistical Machine Translation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>We introduce alignment models for Machine Translation that take into account the context of a source word when determining its translation. Since the use of these contexts alone causes data sparsity problems, we develop a decision tree algorithm for clustering the contexts based on optimisation of the EM auxiliary function. We show that our contextdependent models lead to an improvement in alignment quality, and an increase in translation quality when the alignments are used in Arabic-English and Chinese-English translation.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We introduce alignment models for Machine Translation that take into account the context of a source word when determining its translation.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Since the use of these contexts alone causes data sparsity problems, we develop a decision tree algorithm for clustering the contexts based on optimisation of the EM auxiliary function.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We show that our contextdependent models lead to an improvement in alignment quality, and an increase in translation quality when the alignments are used in Arabic-English and Chinese-English translation.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Alignment modelling for Statistical Machine Translation (SMT) is the task of determining translational correspondences between the words in pairs of sentences in parallel text. Given a source language word sequence f1 J and a target language word sequence e I 1 , we model the translation probability as P(e I 1|f1 J ) and introduce a hidden variable a I 1 representing a mapping from the target word positions to source word positions such that e i is aligned to f ai . Then P(e I 1|f j 1 ) = &#8721; a P(e I 1, a I 1|f j I 1 ) (Brown et al.,
1993).
Previous work on statistical alignment modelling has not taken into account the source word context when determining translations of that word. It is intuitive that a word in one context, with a particular part-of-speech and particular words surrounding it, may translate differently when in a different context. We aim to take advantage of this information to provide a better estimate of the word&#8217;s translation. The challenge of incorporating context information is maintaining computational tractability of estimation and alignment, and we develop algorithms to overcome this. The development of efficient estimation procedures
for context-dependent acoustic models revolutionised the field of Automatic Speech Recognition (ASR) (Young et al., 1994). Clustering is used extensively for improving parameter estimation of triphone (and higher order) acoustic models, enabling robust estimation of parameters and reducing the computation required for recognition. Kannan et al. (1994) introduce a binary treegrowing procedure for clustering Gaussian models for triphone contexts based on the value of a likelihood ratio. We adopt a similar approach to estimate contextdependent translation probabilities.
We focus on alignment with IBM Model 1 and HMMs. HMMs are commonly used to generate alignments from which state of the art SMT systems are built. Model 1 is used as an intermediate step in the creation of more powerful alignment models, such as HMMs and further IBM models. In addition, it is used in SMT as a feature in Minimum Error Training (Och et al., 2004) and for rescoring lattices of translation hypotheses (Blackwood et al., 2008). It is also used for lexically-weighted phrase extraction (Costa-juss&#224; and Fonollosa, 2005) and sentence segmentation of parallel text (Deng et al., 2007) prior to machine translation.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Alignment modelling for Statistical Machine Translation (SMT) is the task of determining translational correspondences between the words in pairs of sentences in parallel text.</text>
              <doc_id>3</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Given a source language word sequence f1 J and a target language word sequence e I 1 , we model the translation probability as P(e I 1|f1 J ) and introduce a hidden variable a I 1 representing a mapping from the target word positions to source word positions such that e i is aligned to f ai .</text>
              <doc_id>4</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Then P(e I 1|f j 1 ) = &#8721; a P(e I 1, a I 1|f j I 1 ) (Brown et al.,</text>
              <doc_id>5</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1993).</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Previous work on statistical alignment modelling has not taken into account the source word context when determining translations of that word.</text>
              <doc_id>7</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It is intuitive that a word in one context, with a particular part-of-speech and particular words surrounding it, may translate differently when in a different context.</text>
              <doc_id>8</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We aim to take advantage of this information to provide a better estimate of the word&#8217;s translation.</text>
              <doc_id>9</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The challenge of incorporating context information is maintaining computational tractability of estimation and alignment, and we develop algorithms to overcome this.</text>
              <doc_id>10</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The development of efficient estimation procedures</text>
              <doc_id>11</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>for context-dependent acoustic models revolutionised the field of Automatic Speech Recognition (ASR) (Young et al., 1994).</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Clustering is used extensively for improving parameter estimation of triphone (and higher order) acoustic models, enabling robust estimation of parameters and reducing the computation required for recognition.</text>
              <doc_id>13</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Kannan et al. (1994) introduce a binary treegrowing procedure for clustering Gaussian models for triphone contexts based on the value of a likelihood ratio.</text>
              <doc_id>14</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We adopt a similar approach to estimate contextdependent translation probabilities.</text>
              <doc_id>15</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We focus on alignment with IBM Model 1 and HMMs.</text>
              <doc_id>16</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>HMMs are commonly used to generate alignments from which state of the art SMT systems are built.</text>
              <doc_id>17</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Model 1 is used as an intermediate step in the creation of more powerful alignment models, such as HMMs and further IBM models.</text>
              <doc_id>18</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In addition, it is used in SMT as a feature in Minimum Error Training (Och et al., 2004) and for rescoring lattices of translation hypotheses (Blackwood et al., 2008).</text>
              <doc_id>19</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>It is also used for lexically-weighted phrase extraction (Costa-juss&#224; and Fonollosa, 2005) and sentence segmentation of parallel text (Deng et al., 2007) prior to machine translation.</text>
              <doc_id>20</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>1.1 Overview</title>
            <text>We first develop an extension to Model 1 that allows the use of arbitrary context information about a source word to estimate context-dependent word-to-word translation probabilities. Since there is insufficient training data to accurately estimate translation probabilities for less frequently occurring contexts, we develop a decision tree clustering algorithm to form context classes. We go on to develop a context-dependent HMM model for alignment.
In Section 3, we evaluate our context-dependent models on Arabic-English parallel text, comparing them to our baseline context-independent models. We perform morphological decomposition of the Arabic text using MADA, and use part-of-speech taggers on both languages. Alignment quality is measured using Alignment Error Rate (AER) measured against a manually-aligned parallel text. Section 4 uses alignments produced by
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 110&#8211;118, Boulder, Colorado, June 2009. c&#9675;2009 Association for Computational Linguistics
our improved alignment models to initialise a statistical machine translation system and evaluate the quality of translation on several data sets. We also apply part-ofspeech tagging and decision tree clustering of contexts to Chinese-English parallel text; translation results for these languages are presented in Section 4.2.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We first develop an extension to Model 1 that allows the use of arbitrary context information about a source word to estimate context-dependent word-to-word translation probabilities.</text>
                  <doc_id>21</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Since there is insufficient training data to accurately estimate translation probabilities for less frequently occurring contexts, we develop a decision tree clustering algorithm to form context classes.</text>
                  <doc_id>22</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We go on to develop a context-dependent HMM model for alignment.</text>
                  <doc_id>23</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In Section 3, we evaluate our context-dependent models on Arabic-English parallel text, comparing them to our baseline context-independent models.</text>
                  <doc_id>24</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We perform morphological decomposition of the Arabic text using MADA, and use part-of-speech taggers on both languages.</text>
                  <doc_id>25</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Alignment quality is measured using Alignment Error Rate (AER) measured against a manually-aligned parallel text.</text>
                  <doc_id>26</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Section 4 uses alignments produced by</text>
                  <doc_id>27</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 110&#8211;118, Boulder, Colorado, June 2009. c&#9675;2009 Association for Computational Linguistics</text>
                  <doc_id>28</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>our improved alignment models to initialise a statistical machine translation system and evaluate the quality of translation on several data sets.</text>
                  <doc_id>29</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We also apply part-ofspeech tagging and decision tree clustering of contexts to Chinese-English parallel text; translation results for these languages are presented in Section 4.2.</text>
                  <doc_id>30</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>1.2 Previous and related work</title>
            <text>Brown et al. (1993) introduce IBM Models 1-5 for alignment modelling; Vogel et al. (1996) propose a Hidden Markov Model (HMM) model for word-to-word alignment, where the words of the source sentence are viewed as states of an HMM and emit target sentence words; Deng and Byrne (2005a) extend this to an HMM word-tophrase model which allows many-to-one alignments and can capture dependencies within target phrases.
Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. This leads to gains in machine translation quality when systems are trained on parallel text containing the modified Arabic and processing of Arabic text is carried out prior to translation. Nie&#223;en and Ney (2001a) perform pre-processing of German and English text before translation; Nie&#223;en and Ney (2001b) use morphological information of the current word to estimate hierarchical translation probabilities.
Berger et al. (1996) introduce maximum entropy models for machine translation, and use a window either side of the target word as context information. Varea et al. (2002) test for the presence of specific words within a window of the current source word to form features for use inside a maximum entropy model of alignment. Toutanova et al. (2002) use part-of-speech information in both the source and target languages to estimate alignment probabilities, but this information is not incorporated into translation probabilities. Popovi&#263; and Ney (2004) use the base form of a word and its part-ofspeech tag during the estimation of word-to-word translation probabilities for IBM models and HMMs, but do not defined context-dependent estimates of translation probabilities. Stroppa et al. (2007) consider context-informed features of phrases as components of the log-linear model during phrase-based translation, but do not address alignment.
2 Use of source language context in alignment modelling
Consider the alignment of the target sentence e = e I 1 with the source sentence f = f1 J. Let a = aI 1 be the alignments of the target words to the source words. Let c j be the context information of f j for j = 1, . . . , J. This context information can be any information about the word, e.g. part-of-speech, previous and next words, part-ofspeech of previous and next words, or longer range context information.
We follow Brown et al. (1993), but extend their modelling framework to include information about the source word from which a target word is emitted. We model the alignment process as:
P(e I 1, a I 1, I|f1 J , c J 1 ) = I&#8719;
P(I|f1 J , c J [
1 ) P(ei |a i 1, e i&#8722;1 1 , f1 J , c J 1 , I)
i=1
&#215; P(a i |e i&#8722;1 1 , a i&#8722;1 1 , f J 1 , cJ 1 , I)] (1)
We introduce word-to-word translation tables that depend on the source language context for each word, i.e. the probability that f translates to e given f has context c is t(e|f, c). We assume that the context sequence is given for a source word sequence. This assumption can be relaxed to allow for multiple tag sequences as hidden processes, but we assume here that a tagger generates a single context sequence c J 1 for a word sequence f1 J . This corresponds to the assumption that, for a context sequence &#732;c J 1 , P(&#732;cJ 1 |f 1 J) = &#948; c J (&#732;cJ 1 1 ); hence
P(e I 1, a I 1|f J 1 ) = &#8721;&#732;c J 1 P(e I 1, a I 1, &#732;c J 1 |f J 1 ) = P(e I 1, a I 1|c J 1 , f J 1 )
For Model 1, ignoring the sentence length distribution,
P M1 (e I 1, a I 1|f J 1 , c J 1 ) =
1 (J + 1) I I&#8719;
t(e i |f ai , c ai ). (2)
i=1
Estimating translation probabilities separately for every possible context of a source word individually leads to problems with data sparsity and rapid growth of the translation table. We therefore wish to cluster source contexts which lead to similar probability distributions. Let C f denote the set of all observed contexts of source word f. A particular clustering is denoted
K f = {K f,1 , . . . , K f,Nf },
where K f is a partition of C f . We define a class membership function &#181; f such that for any context c, &#181; f (c) is the cluster containing c. We assume that all contexts in a cluster give rise to the same translation probability distribution for that source word, i.e. for a cluster K, t(e|f, c) = t(e|f, c &#8242; ) for all contexts c, c &#8242; &#8712; K and all target words e; we write this shared translation probability as t(e|f, K). The Model 1 sentence translation probability for a given alignment (Equation 2) becomes
P M1 (e I 1, a I 1|f J 1 , c J 1 ) =
1 (J + 1) I I&#8719;
t(e i |f ai , &#181; f (c ai )).
i=1
(3)
For HMM alignment, we assume that the transition probabilities a(a i |a i&#8722;1 ) are independent of the word contexts and the sentence translation probability is
I&#8719; P H (e I 1 , aI 1 |f 1 J , cJ 1 ) = a(a i |a i&#8722;1 , J)t(e i |f ai , &#181; f (c ai )).
i=1
(4) Section 2.1.1 describes how the context classes are determined by optimisation of the EM auxiliary function. Although the translation model is significantly more complex than that of context-independent models, once class membership is fixed, alignment and parameter estimation use the standard algorithms.
2.1 EM parameter estimation
We train using Expectation Maximisation (EM), optimising the log probability of the training set {e (s) , f (s) } S s=1 (Brown et al., 1993). Given model parameters &#952; &#8242; , we estimate new parameters &#952; by maximisation of the EM auxiliary function &#8721;
P &#952; &#8242;(a|f (s) , c (s) , e (s) ) log P &#952; (e (s) , a, I (s) |f (s) , c (s) ).
s,a
We assume the sentence length distribution and alignment probabilities do not depend on the contexts of the source words; hence the relevant part of the auxiliary function is &#8721; &#8721; &#8721; &#947; &#8242; (e|f, c) log t(e|f, c), (5)
e c&#8712;C f
where
f
&#947; &#8242; (e|f, c) = &#8721; s &#8721;I (s) &#8721;J (s) [ &#948; c (c (s) j
i=1 j=1
)&#948; e (e (s)
i
)&#948; f (f (s) j )
] &#215; P &#952; &#8242;(a i = j|e (s) , f (s) , c (s) )
Here &#947; &#8242; can be computed under Model 1 or the HMM, and is calculated using the forward-backward algorithm for the HMM.
2.1.1 Parameter estimation with clustered contexts
We can re-write the EM auxiliary function (Equation 5) in terms of the cluster-specific translation probabilities:
&#8721; &#8721; &#8721;
e
= &#8721; e
|K f |
&#8721; &#947; &#8242; (e|f, c) log t(e|f, c)
f l=1 c&#8712;K f,l
&#8721; &#8721; &#947; &#8242; (e|f, K) log t(e|f, K) (6)
f K&#8712;K f
where &#947; &#8242; (e|f, K) = &#8721; c&#8712;K &#947; &#8242; (e|f, c)
Following the usual derivation, the EM update for the class-specific translation probabilities becomes
&#710;t(e|f, K) = &#947;&#8242; (e|f, K)
&#8721;e &#8242; &#947;&#8242; (e &#8242; |f, K) . (7)
Standard EM training can be viewed a special case of this, with every context of a source word grouped into a single cluster. Another way to view these clustered contextdependent models is that contexts belonging to the same cluster are tied and share a common translation probability distribution, which is estimated from all training examples in which any of the contexts occur.
2.2 Decision trees for context clustering
The objective for each source word is to split the contexts into classes to maximise the likelihood of the training data. Since it is not feasible to maximise the likelihood of the observations directly, we maximise the expected log likelihood by considering the EM auxiliary function, in a similar manner to that used for modelling contextual variations of phones for ASR (Young et al., 1994; Singer and Ostendorf, 1996). We perform divisive clustering independently for each source word f, by building a binary decision tree which forms classes of contexts which maximise the EM auxiliary function. Questions for the tree are drawn from a set of questions Q = {q 1 , . . . , q |Q| } concerning the context information of f.
Let K be any set of contexts of f, and define
L(K) = &#8721; &#8721; &#947; &#8242; (e|f, c) log t(e|f, c)
e c&#8712;K
= &#8721; &#8721; &#8721;
&#947; &#8242; c&#8712;K (e|f, c) log &#947;&#8242; (e|f, c)
e &#8721;e
&#8721;c&#8712;K &#8242; &#947;&#8242; (e &#8242; |f, c) .
c&#8712;K
This is the contribution to the EM auxiliary function of source word f occurring in the contexts of K. Let q be a binary question about the context of f, and consider the effect on the partial auxiliary function (Equation 6) of splitting K into two clusters using question q. Define K q be the set of contexts in K which answer &#8216;yes&#8217; to q and K&#175;q be the contexts which answer &#8216;no&#8217;. Define the objective function
Q f,q (K) = &#8721; &#8721; &#947; &#8242; (e|f, c) log t(e|f, c)
e c&#8712;K q
+ &#8721; &#8721; &#947; &#8242; (e|f, c) log t(e|f, c)
e c&#8712;K&#175;q
= L(K q ) + L(K&#175;q )
When the node is split using question q, the increase in objective function is given by
Q f,q (K) &#8722; L(K) = L(K&#175;q ) + L(K q ) &#8722; L(K).
We choose q to maximise this.
In order to build the decision tree for f, we take the set of all contexts C f as the initial cluster at the root node. We then find the question &#710;q such that Q f,q (C f ) is maximal, i.e. &#710;q = arg max Q f,q (C f )
q&#8712;Q
This splits C f , so our decision tree now has two nodes. We iterate this process, at each iteration splitting (into two further nodes) the leaf node that leads to the greatest increase in objective function. This leads to a greedy search to optimise the log likelihood over possible state clusterings.
In order to control the growth of the tree, we put in place two thresholds:
&#8226; T imp is the minimum improvement in objective function required for a node to be split; without it, we would continue splitting nodes until each contained only one context, even though doing so would cause data sparsity problems. &#8226; T occ is the minimum occupancy of a node, based on how often the contexts at that node occur in the training data; we want to ensure that there are enough examples of a context in the training data to estimate accurately the translation probability distribution for that cluster.
For each leaf node l and set of contexts K l at that node, we find the question q l that, when used to split K l , produces the largest gain in objective function:
= arg max[L(K l,q ) + L(K l,&#175;q )]
q&#8712;Q
We then find the leaf node for which splitting gives the largest improvement:
L(K l,ql ) + L(K l, &#175;ql ) &#8722; L(K l ) &gt; T imp
&#8226; The occupancy threshold is exceeded for both child nodes:
&#8721; &#8721; &#947; &#8242; (e|f, c) &gt; T occ for x = q, &#175;q
e c&#8712;K l,x
We perform such clustering for every source word in the parallel text.
Sfqp NN byE NN 12 NN % PUNC mn IN &gt;shm NN Albnk NN
w+ CC mnAzl NN Edp JJ &gt;SHAbhA NN ybyEwn VBP Alxmwr NN fy IN Almdynp NN Figure 1: Alignment of the English selling in different contexts. In the first, it is preceded by of and links to the infinitive of the Arabic verb byE; in the second, it is preceded by were and links to an inflected form of the same Arabic verb, ybyEwn.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Brown et al. (1993) introduce IBM Models 1-5 for alignment modelling; Vogel et al. (1996) propose a Hidden Markov Model (HMM) model for word-to-word alignment, where the words of the source sentence are viewed as states of an HMM and emit target sentence words; Deng and Byrne (2005a) extend this to an HMM word-tophrase model which allows many-to-one alignments and can capture dependencies within target phrases.</text>
                  <doc_id>31</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes.</text>
                  <doc_id>32</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This leads to gains in machine translation quality when systems are trained on parallel text containing the modified Arabic and processing of Arabic text is carried out prior to translation.</text>
                  <doc_id>33</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Nie&#223;en and Ney (2001a) perform pre-processing of German and English text before translation; Nie&#223;en and Ney (2001b) use morphological information of the current word to estimate hierarchical translation probabilities.</text>
                  <doc_id>34</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Berger et al. (1996) introduce maximum entropy models for machine translation, and use a window either side of the target word as context information.</text>
                  <doc_id>35</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Varea et al. (2002) test for the presence of specific words within a window of the current source word to form features for use inside a maximum entropy model of alignment.</text>
                  <doc_id>36</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Toutanova et al. (2002) use part-of-speech information in both the source and target languages to estimate alignment probabilities, but this information is not incorporated into translation probabilities.</text>
                  <doc_id>37</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Popovi&#263; and Ney (2004) use the base form of a word and its part-ofspeech tag during the estimation of word-to-word translation probabilities for IBM models and HMMs, but do not defined context-dependent estimates of translation probabilities.</text>
                  <doc_id>38</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Stroppa et al. (2007) consider context-informed features of phrases as components of the log-linear model during phrase-based translation, but do not address alignment.</text>
                  <doc_id>39</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 Use of source language context in alignment modelling</text>
                  <doc_id>40</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Consider the alignment of the target sentence e = e I 1 with the source sentence f = f1 J. Let a = aI 1 be the alignments of the target words to the source words.</text>
                  <doc_id>41</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Let c j be the context information of f j for j = 1, .</text>
                  <doc_id>42</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>43</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>44</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>, J.</text>
                  <doc_id>45</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This context information can be any information about the word, e.g. part-of-speech, previous and next words, part-ofspeech of previous and next words, or longer range context information.</text>
                  <doc_id>46</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We follow Brown et al. (1993), but extend their modelling framework to include information about the source word from which a target word is emitted.</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We model the alignment process as:</text>
                  <doc_id>48</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P(e I 1, a I 1, I|f1 J , c J 1 ) = I&#8719;</text>
                  <doc_id>49</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P(I|f1 J , c J [</text>
                  <doc_id>50</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 ) P(ei |a i 1, e i&#8722;1 1 , f1 J , c J 1 , I)</text>
                  <doc_id>51</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#215; P(a i |e i&#8722;1 1 , a i&#8722;1 1 , f J 1 , cJ 1 , I)] (1)</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We introduce word-to-word translation tables that depend on the source language context for each word, i.e. the probability that f translates to e given f has context c is t(e|f, c).</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We assume that the context sequence is given for a source word sequence.</text>
                  <doc_id>55</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This assumption can be relaxed to allow for multiple tag sequences as hidden processes, but we assume here that a tagger generates a single context sequence c J 1 for a word sequence f1 J .</text>
                  <doc_id>56</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This corresponds to the assumption that, for a context sequence &#732;c J 1 , P(&#732;cJ 1 |f 1 J) = &#948; c J (&#732;cJ 1 1 ); hence</text>
                  <doc_id>57</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P(e I 1, a I 1|f J 1 ) = &#8721;&#732;c J 1 P(e I 1, a I 1, &#732;c J 1 |f J 1 ) = P(e I 1, a I 1|c J 1 , f J 1 )</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For Model 1, ignoring the sentence length distribution,</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P M1 (e I 1, a I 1|f J 1 , c J 1 ) =</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 (J + 1) I I&#8719;</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>t(e i |f ai , c ai ).</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(2)</text>
                  <doc_id>63</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Estimating translation probabilities separately for every possible context of a source word individually leads to problems with data sparsity and rapid growth of the translation table.</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We therefore wish to cluster source contexts which lead to similar probability distributions.</text>
                  <doc_id>66</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Let C f denote the set of all observed contexts of source word f. A particular clustering is denoted</text>
                  <doc_id>67</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>K f = {K f,1 , .</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>69</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>70</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>, K f,Nf },</text>
                  <doc_id>71</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where K f is a partition of C f .</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We define a class membership function &#181; f such that for any context c, &#181; f (c) is the cluster containing c.</text>
                  <doc_id>73</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We assume that all contexts in a cluster give rise to the same translation probability distribution for that source word, i.e. for a cluster K, t(e|f, c) = t(e|f, c &#8242; ) for all contexts c, c &#8242; &#8712; K and all target words e; we write this shared translation probability as t(e|f, K).</text>
                  <doc_id>74</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The Model 1 sentence translation probability for a given alignment (Equation 2) becomes</text>
                  <doc_id>75</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P M1 (e I 1, a I 1|f J 1 , c J 1 ) =</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 (J + 1) I I&#8719;</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>t(e i |f ai , &#181; f (c ai )).</text>
                  <doc_id>78</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(3)</text>
                  <doc_id>80</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For HMM alignment, we assume that the transition probabilities a(a i |a i&#8722;1 ) are independent of the word contexts and the sentence translation probability is</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>I&#8719; P H (e I 1 , aI 1 |f 1 J , cJ 1 ) = a(a i |a i&#8722;1 , J)t(e i |f ai , &#181; f (c ai )).</text>
                  <doc_id>82</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(4) Section 2.1.1 describes how the context classes are determined by optimisation of the EM auxiliary function.</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Although the translation model is significantly more complex than that of context-independent models, once class membership is fixed, alignment and parameter estimation use the standard algorithms.</text>
                  <doc_id>85</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.1 EM parameter estimation</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We train using Expectation Maximisation (EM), optimising the log probability of the training set {e (s) , f (s) } S s=1 (Brown et al., 1993).</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Given model parameters &#952; &#8242; , we estimate new parameters &#952; by maximisation of the EM auxiliary function &#8721;</text>
                  <doc_id>88</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P &#952; &#8242;(a|f (s) , c (s) , e (s) ) log P &#952; (e (s) , a, I (s) |f (s) , c (s) ).</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>s,a</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We assume the sentence length distribution and alignment probabilities do not depend on the contexts of the source words; hence the relevant part of the auxiliary function is &#8721; &#8721; &#8721; &#947; &#8242; (e|f, c) log t(e|f, c), (5)</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e c&#8712;C f</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#947; &#8242; (e|f, c) = &#8721; s &#8721;I (s) &#8721;J (s) [ &#948; c (c (s) j</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1 j=1</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>)&#948; e (e (s)</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>)&#948; f (f (s) j )</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>] &#215; P &#952; &#8242;(a i = j|e (s) , f (s) , c (s) )</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Here &#947; &#8242; can be computed under Model 1 or the HMM, and is calculated using the forward-backward algorithm for the HMM.</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.1.1 Parameter estimation with clustered contexts</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We can re-write the EM auxiliary function (Equation 5) in terms of the cluster-specific translation probabilities:</text>
                  <doc_id>103</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; &#8721; &#8721;</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>= &#8721; e</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>|K f |</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; &#947; &#8242; (e|f, c) log t(e|f, c)</text>
                  <doc_id>108</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f l=1 c&#8712;K f,l</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; &#8721; &#947; &#8242; (e|f, K) log t(e|f, K) (6)</text>
                  <doc_id>110</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f K&#8712;K f</text>
                  <doc_id>111</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where &#947; &#8242; (e|f, K) = &#8721; c&#8712;K &#947; &#8242; (e|f, c)</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Following the usual derivation, the EM update for the class-specific translation probabilities becomes</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#710;t(e|f, K) = &#947;&#8242; (e|f, K)</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;e &#8242; &#947;&#8242; (e &#8242; |f, K) .</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(7)</text>
                  <doc_id>116</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Standard EM training can be viewed a special case of this, with every context of a source word grouped into a single cluster.</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Another way to view these clustered contextdependent models is that contexts belonging to the same cluster are tied and share a common translation probability distribution, which is estimated from all training examples in which any of the contexts occur.</text>
                  <doc_id>118</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.2 Decision trees for context clustering</text>
                  <doc_id>119</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The objective for each source word is to split the contexts into classes to maximise the likelihood of the training data.</text>
                  <doc_id>120</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Since it is not feasible to maximise the likelihood of the observations directly, we maximise the expected log likelihood by considering the EM auxiliary function, in a similar manner to that used for modelling contextual variations of phones for ASR (Young et al., 1994; Singer and Ostendorf, 1996).</text>
                  <doc_id>121</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We perform divisive clustering independently for each source word f, by building a binary decision tree which forms classes of contexts which maximise the EM auxiliary function.</text>
                  <doc_id>122</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Questions for the tree are drawn from a set of questions Q = {q 1 , .</text>
                  <doc_id>123</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>124</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>125</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>, q |Q| } concerning the context information of f.</text>
                  <doc_id>126</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Let K be any set of contexts of f, and define</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(K) = &#8721; &#8721; &#947; &#8242; (e|f, c) log t(e|f, c)</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e c&#8712;K</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>= &#8721; &#8721; &#8721;</text>
                  <doc_id>130</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#947; &#8242; c&#8712;K (e|f, c) log &#947;&#8242; (e|f, c)</text>
                  <doc_id>131</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e &#8721;e</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721;c&#8712;K &#8242; &#947;&#8242; (e &#8242; |f, c) .</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c&#8712;K</text>
                  <doc_id>134</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>This is the contribution to the EM auxiliary function of source word f occurring in the contexts of K. Let q be a binary question about the context of f, and consider the effect on the partial auxiliary function (Equation 6) of splitting K into two clusters using question q.</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Define K q be the set of contexts in K which answer &#8216;yes&#8217; to q and K&#175;q be the contexts which answer &#8216;no&#8217;.</text>
                  <doc_id>136</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Define the objective function</text>
                  <doc_id>137</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Q f,q (K) = &#8721; &#8721; &#947; &#8242; (e|f, c) log t(e|f, c)</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e c&#8712;K q</text>
                  <doc_id>139</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>+ &#8721; &#8721; &#947; &#8242; (e|f, c) log t(e|f, c)</text>
                  <doc_id>140</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e c&#8712;K&#175;q</text>
                  <doc_id>141</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>= L(K q ) + L(K&#175;q )</text>
                  <doc_id>142</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>When the node is split using question q, the increase in objective function is given by</text>
                  <doc_id>143</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Q f,q (K) &#8722; L(K) = L(K&#175;q ) + L(K q ) &#8722; L(K).</text>
                  <doc_id>144</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We choose q to maximise this.</text>
                  <doc_id>145</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In order to build the decision tree for f, we take the set of all contexts C f as the initial cluster at the root node.</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We then find the question &#710;q such that Q f,q (C f ) is maximal, i.e. &#710;q = arg max Q f,q (C f )</text>
                  <doc_id>147</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>q&#8712;Q</text>
                  <doc_id>148</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>This splits C f , so our decision tree now has two nodes.</text>
                  <doc_id>149</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We iterate this process, at each iteration splitting (into two further nodes) the leaf node that leads to the greatest increase in objective function.</text>
                  <doc_id>150</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This leads to a greedy search to optimise the log likelihood over possible state clusterings.</text>
                  <doc_id>151</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In order to control the growth of the tree, we put in place two thresholds:</text>
                  <doc_id>152</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; T imp is the minimum improvement in objective function required for a node to be split; without it, we would continue splitting nodes until each contained only one context, even though doing so would cause data sparsity problems.</text>
                  <doc_id>153</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>&#8226; T occ is the minimum occupancy of a node, based on how often the contexts at that node occur in the training data; we want to ensure that there are enough examples of a context in the training data to estimate accurately the translation probability distribution for that cluster.</text>
                  <doc_id>154</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For each leaf node l and set of contexts K l at that node, we find the question q l that, when used to split K l , produces the largest gain in objective function:</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>= arg max[L(K l,q ) + L(K l,&#175;q )]</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>q&#8712;Q</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We then find the leaf node for which splitting gives the largest improvement:</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>L(K l,ql ) + L(K l, &#175;ql ) &#8722; L(K l ) &gt; T imp</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; The occupancy threshold is exceeded for both child nodes:</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8721; &#8721; &#947; &#8242; (e|f, c) &gt; T occ for x = q, &#175;q</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e c&#8712;K l,x</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We perform such clustering for every source word in the parallel text.</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Sfqp NN byE NN 12 NN % PUNC mn IN &gt;shm NN Albnk NN</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>w+ CC mnAzl NN Edp JJ &gt;SHAbhA NN ybyEwn VBP Alxmwr NN fy IN Almdynp NN Figure 1: Alignment of the English selling in different contexts.</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the first, it is preceded by of and links to the infinitive of the Arabic verb byE; in the second, it is preceded by were and links to an inflected form of the same Arabic verb, ybyEwn.</text>
                  <doc_id>166</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>2</index>
        <title>3 Evaluation of alignment quality</title>
        <text>Our models were built using the MTTK toolkit (Deng and Byrne, 2005b). Decision tree clustering was implemented and the process parallelised to enable thousands of decision trees to be built. Our context-dependent (CD) Model 1 models trained on context-annotated data were compared to the baseline context-independent (CI) models trained on untagged data. The models were trained using data allowed for the NIST 08 Arabic-English evaluation 1 , excluding the UN collections, comprising 300k parallel sentence pairs, a total of 8.4M words of Arabic and 9.5M words of English.
The Arabic language incorporates into its words several prefixes and suffixes which determine grammatical features such as gender, number, person and voice. The MADA toolkit (Habash and Sadat, 2006) was used to perform Arabic morphological word decomposition and part-of-speech tagging. It determines the best analysis for each word in a sentence and splits word prefixes and suffixes, based on the alternative analyses provided by BAMA (Buckwalter, 2002). We use tokenisation scheme
1 http://nist.gov/speech/tests/mt/2008
&#8216;D2&#8217;, which splits certain prefixes and has been reported to improve machine translation performance (Habash and Sadat, 2006). The alignment models are trained on this processed data, and the prefixes and suffixes are treated as words in their own right; in particular their contexts are examined and clustered. The TnT tagger (Brants, 2000), used as distributed with its model trained on the Wall Street Journal portion of the Penn treebank, was used to obtain part-of-speech tags for the English side of the parallel text. Marcus et al. (1993) gives a complete list of part-of-speech tags produced. No morphological analysis is performed for English.
Automatic word alignments were compared to a manually-aligned corpus made up of the IBM Arabic- English Word Alignment Corpus (Ittycheriah et al., 2006) and the word alignment corpora LDC2006E86 and LDC2006E93. This contains 28k parallel text sentences pairs: 724k words of Arabic and 847k words of English. The alignment links were modified to reflect the MADA tokenisation; after modification, there are 946k word-toword alignment links.
Alignment quality was evaluated by computing Alignment Error Rate (AER) (Och and Ney, 2000) relative to the manual alignments. Since the links supplied contain only &#8216;sure&#8217; links and no &#8216;possible&#8217; links, we use the following formula for computing AER given reference alignment links S and hypothesised alignment links A: AER = 1 &#8722; 2|S&#8745;A| |S|+|A| .</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Our models were built using the MTTK toolkit (Deng and Byrne, 2005b).</text>
              <doc_id>167</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Decision tree clustering was implemented and the process parallelised to enable thousands of decision trees to be built.</text>
              <doc_id>168</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Our context-dependent (CD) Model 1 models trained on context-annotated data were compared to the baseline context-independent (CI) models trained on untagged data.</text>
              <doc_id>169</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The models were trained using data allowed for the NIST 08 Arabic-English evaluation 1 , excluding the UN collections, comprising 300k parallel sentence pairs, a total of 8.4M words of Arabic and 9.5M words of English.</text>
              <doc_id>170</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The Arabic language incorporates into its words several prefixes and suffixes which determine grammatical features such as gender, number, person and voice.</text>
              <doc_id>171</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The MADA toolkit (Habash and Sadat, 2006) was used to perform Arabic morphological word decomposition and part-of-speech tagging.</text>
              <doc_id>172</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>It determines the best analysis for each word in a sentence and splits word prefixes and suffixes, based on the alternative analyses provided by BAMA (Buckwalter, 2002).</text>
              <doc_id>173</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We use tokenisation scheme</text>
              <doc_id>174</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 http://nist.gov/speech/tests/mt/2008</text>
              <doc_id>175</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8216;D2&#8217;, which splits certain prefixes and has been reported to improve machine translation performance (Habash and Sadat, 2006).</text>
              <doc_id>176</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The alignment models are trained on this processed data, and the prefixes and suffixes are treated as words in their own right; in particular their contexts are examined and clustered.</text>
              <doc_id>177</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The TnT tagger (Brants, 2000), used as distributed with its model trained on the Wall Street Journal portion of the Penn treebank, was used to obtain part-of-speech tags for the English side of the parallel text.</text>
              <doc_id>178</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Marcus et al. (1993) gives a complete list of part-of-speech tags produced.</text>
              <doc_id>179</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>No morphological analysis is performed for English.</text>
              <doc_id>180</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Automatic word alignments were compared to a manually-aligned corpus made up of the IBM Arabic- English Word Alignment Corpus (Ittycheriah et al., 2006) and the word alignment corpora LDC2006E86 and LDC2006E93.</text>
              <doc_id>181</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This contains 28k parallel text sentences pairs: 724k words of Arabic and 847k words of English.</text>
              <doc_id>182</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The alignment links were modified to reflect the MADA tokenisation; after modification, there are 946k word-toword alignment links.</text>
              <doc_id>183</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Alignment quality was evaluated by computing Alignment Error Rate (AER) (Och and Ney, 2000) relative to the manual alignments.</text>
              <doc_id>184</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Since the links supplied contain only &#8216;sure&#8217; links and no &#8216;possible&#8217; links, we use the following formula for computing AER given reference alignment links S and hypothesised alignment links A: AER = 1 &#8722; 2|S&#8745;A| |S|+|A| .</text>
              <doc_id>185</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Questions about contexts</title>
            <text>The algorithm presented in Section 2 allows for any information about the context of the source word to be considered. We could consider general questions of the form &#8216;Is the previous word x?&#8217; and &#8216;Does word y occur within n words of this one?&#8217;. To maintain computational tractability, we restrict the questions to those concerning the partof-speech tag assigned to the current, previous and next words. We do not ask questions about the identities of the words themselves. For each part-of-speech tag T , we ask the question &#8216;Does w have tag T?&#8217;. In addition, we group part-of-speech tags to ask more general questions: e.g. the set of contexts which satisfies &#8216;Is w a noun?&#8217; contains those that satisfy &#8216;Is w a proper noun?&#8217; and &#8216;Is w a singular or mass noun?&#8217;. We also ask the same questions of the previous and next words in the source sentence. In English, this gives a total of 152 distinct questions, each of which is considered when splitting a leaf node. The MADA part-of-speech tagger uses a reduced tag set, which produces a total of 68 distinct questions.
Figure 1 shows the links of the English source word selling in two different contexts where it links to different words in Arabic, which are both forms of the same verb. The part-of-speech of the previous word is useful for dis-
Log probability of training data Log probability of training data
-4.04e+07
-4.06e+07
-4.08e+07
-4.1e+07
-4.12e+07
-4.14e+07
-4.16e+07
-4.18e+07
-4.2e+07
CI Model 1 Threshold 10 Threshold 20 Threshold 60
-4.22e+07 11 12 13 14 15 16 17 18 19 20 Iteration
-2.75e+06
-2.8e+06
-2.85e+06
-2.9e+06
-2.95e+06
-3e+06
-3.05e+06
CI Model 1 Threshold 10 Threshold 20 Threshold 60
-3.1e+06 11 12 13 14 15 16 17 18 19 20
Iteration
criminating between the two cases, whereas a contextindependent model would assign the same probability to both Arabic words.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The algorithm presented in Section 2 allows for any information about the context of the source word to be considered.</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We could consider general questions of the form &#8216;Is the previous word x?&#8217; and &#8216;Does word y occur within n words of this one?&#8217;.</text>
                  <doc_id>187</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>To maintain computational tractability, we restrict the questions to those concerning the partof-speech tag assigned to the current, previous and next words.</text>
                  <doc_id>188</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We do not ask questions about the identities of the words themselves.</text>
                  <doc_id>189</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For each part-of-speech tag T , we ask the question &#8216;Does w have tag T?&#8217;.</text>
                  <doc_id>190</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, we group part-of-speech tags to ask more general questions: e.g. the set of contexts which satisfies &#8216;Is w a noun?&#8217; contains those that satisfy &#8216;Is w a proper noun?&#8217; and &#8216;Is w a singular or mass noun?&#8217;.</text>
                  <doc_id>191</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>We also ask the same questions of the previous and next words in the source sentence.</text>
                  <doc_id>192</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>In English, this gives a total of 152 distinct questions, each of which is considered when splitting a leaf node.</text>
                  <doc_id>193</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>The MADA part-of-speech tagger uses a reduced tag set, which produces a total of 68 distinct questions.</text>
                  <doc_id>194</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 1 shows the links of the English source word selling in two different contexts where it links to different words in Arabic, which are both forms of the same verb.</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The part-of-speech of the previous word is useful for dis-</text>
                  <doc_id>196</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Log probability of training data Log probability of training data</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-4.04e+07</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-4.06e+07</text>
                  <doc_id>199</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-4.08e+07</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-4.1e+07</text>
                  <doc_id>201</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-4.12e+07</text>
                  <doc_id>202</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-4.14e+07</text>
                  <doc_id>203</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-4.16e+07</text>
                  <doc_id>204</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-4.18e+07</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-4.2e+07</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>CI Model 1 Threshold 10 Threshold 20 Threshold 60</text>
                  <doc_id>207</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-4.22e+07 11 12 13 14 15 16 17 18 19 20 Iteration</text>
                  <doc_id>208</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-2.75e+06</text>
                  <doc_id>209</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-2.8e+06</text>
                  <doc_id>210</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-2.85e+06</text>
                  <doc_id>211</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-2.9e+06</text>
                  <doc_id>212</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-2.95e+06</text>
                  <doc_id>213</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-3e+06</text>
                  <doc_id>214</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-3.05e+06</text>
                  <doc_id>215</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>CI Model 1 Threshold 10 Threshold 20 Threshold 60</text>
                  <doc_id>216</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>-3.1e+06 11 12 13 14 15 16 17 18 19 20</text>
                  <doc_id>217</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Iteration</text>
                  <doc_id>218</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>criminating between the two cases, whereas a contextindependent model would assign the same probability to both Arabic words.</text>
                  <doc_id>219</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Training Model 1</title>
            <text>Training is carried out in both translation directions. For Arabic to English, the Arabic side of the parallel text is tagged and the English side remains untagged; we view the English words as being generated from the Arabic words and questions are asked about the context of the Arabic words to determine clusters for the translation table. For English to Arabic, the situation is reversed: we used tagged English text as the source language and untagged Arabic text, with morphological decomposition, as the target language. Standard CI Model 1 training, initialised with a uniform translation table so that t(e|f) is constant for all source/target word pairs (f, e), was run on untagged data for 10 iterations in each direction (Brown et al., 1993; Deng and Byrne, 2005b). A decision tree was built to cluster the contexts and a further 10 iterations of training were carried out using the tagged words-with-context to produce context-dependent models (CD Model 1). The
models were then evaluated using AER at each training iteration. A number of improvement thresholds T imp were tested, and performance compared to that of models found after further iterations of CI Model 1 training on the untagged data. In both alignment directions, the log probability of the training data increases during training (see Figure 2). As expected, the training set likelihood increases as the threshold T imp is reduced, allowing more clusters and closer fitting to the data.
3.2.1 Analysis of frequently used questions
Table 1 shows the questions used most frequently at the root node of the decision tree when clustering contexts in English and Arabic. Because they are used first, these are the questions that individually give the greatest ability to discriminate between the different contexts of a word. The list shows the importance of the left and right contexts of the word in predicting its translation: of the most common 50 questions, 25 concern the previous word, 19 concern the next, and only 6 concern the partof-speech of the current word. For Arabic, of the most frequent 50 questions, 21 concern the previous word, 20 concern the next and 9 the current word.
3.2.2 Alignment Error Rate
Since MT systems are usually built on the union of the two sets of alignments (Koehn et al., 2003), we consider the union of alignments in the two directions as well as those in each direction. Figure 3 shows the change in AER of the alignments in each direction, as well as the alignment formed by taking their union at corresponding thresholds and training iterations.
3.2.3 Variation of improvement threshold T imp
There is a trade-off between modelling the data accurately, which requires more clusters, and eliminating data sparsity problems, which requires each cluster to contain contexts that occur frequently enough in the training data to estimate the translation probabilities accurately. Use of a smaller threshold T imp leads to more clusters per word and an improved ability to fit to the data, but this can lead to reduced alignment quality if there is insufficient data to estimate the translation probability distribution accurately for each cluster. For lower thresholds, we observe over-fitting and the AER rises after the second iteration of CD training, similar to the behaviour seen in Och (2002). Setting T imp = 0 results in each context of a word having its own cluster, which leads to data sparsity problems.
Table 2 shows the percentage of words for which the contexts are split into multiple clusters for CD Model 1 with varying improvement thresholds. This occurs when there are enough training data examples and sufficient variability between the contexts of a word that splitting the contexts into more than one cluster increases the EM auxiliary function. For words where the contexts are not split, all the contexts remain in the same cluster and parameter estimation is exactly the same as for the unclustered context-independent models.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Training is carried out in both translation directions.</text>
                  <doc_id>220</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For Arabic to English, the Arabic side of the parallel text is tagged and the English side remains untagged; we view the English words as being generated from the Arabic words and questions are asked about the context of the Arabic words to determine clusters for the translation table.</text>
                  <doc_id>221</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For English to Arabic, the situation is reversed: we used tagged English text as the source language and untagged Arabic text, with morphological decomposition, as the target language.</text>
                  <doc_id>222</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Standard CI Model 1 training, initialised with a uniform translation table so that t(e|f) is constant for all source/target word pairs (f, e), was run on untagged data for 10 iterations in each direction (Brown et al., 1993; Deng and Byrne, 2005b).</text>
                  <doc_id>223</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>A decision tree was built to cluster the contexts and a further 10 iterations of training were carried out using the tagged words-with-context to produce context-dependent models (CD Model 1).</text>
                  <doc_id>224</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The</text>
                  <doc_id>225</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>models were then evaluated using AER at each training iteration.</text>
                  <doc_id>226</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A number of improvement thresholds T imp were tested, and performance compared to that of models found after further iterations of CI Model 1 training on the untagged data.</text>
                  <doc_id>227</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In both alignment directions, the log probability of the training data increases during training (see Figure 2).</text>
                  <doc_id>228</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>As expected, the training set likelihood increases as the threshold T imp is reduced, allowing more clusters and closer fitting to the data.</text>
                  <doc_id>229</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3.2.1 Analysis of frequently used questions</text>
                  <doc_id>230</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 1 shows the questions used most frequently at the root node of the decision tree when clustering contexts in English and Arabic.</text>
                  <doc_id>231</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Because they are used first, these are the questions that individually give the greatest ability to discriminate between the different contexts of a word.</text>
                  <doc_id>232</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The list shows the importance of the left and right contexts of the word in predicting its translation: of the most common 50 questions, 25 concern the previous word, 19 concern the next, and only 6 concern the partof-speech of the current word.</text>
                  <doc_id>233</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For Arabic, of the most frequent 50 questions, 21 concern the previous word, 20 concern the next and 9 the current word.</text>
                  <doc_id>234</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3.2.2 Alignment Error Rate</text>
                  <doc_id>235</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Since MT systems are usually built on the union of the two sets of alignments (Koehn et al., 2003), we consider the union of alignments in the two directions as well as those in each direction.</text>
                  <doc_id>236</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 3 shows the change in AER of the alignments in each direction, as well as the alignment formed by taking their union at corresponding thresholds and training iterations.</text>
                  <doc_id>237</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3.2.3 Variation of improvement threshold T imp</text>
                  <doc_id>238</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>There is a trade-off between modelling the data accurately, which requires more clusters, and eliminating data sparsity problems, which requires each cluster to contain contexts that occur frequently enough in the training data to estimate the translation probabilities accurately.</text>
                  <doc_id>239</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Use of a smaller threshold T imp leads to more clusters per word and an improved ability to fit to the data, but this can lead to reduced alignment quality if there is insufficient data to estimate the translation probability distribution accurately for each cluster.</text>
                  <doc_id>240</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For lower thresholds, we observe over-fitting and the AER rises after the second iteration of CD training, similar to the behaviour seen in Och (2002).</text>
                  <doc_id>241</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Setting T imp = 0 results in each context of a word having its own cluster, which leads to data sparsity problems.</text>
                  <doc_id>242</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 2 shows the percentage of words for which the contexts are split into multiple clusters for CD Model 1 with varying improvement thresholds.</text>
                  <doc_id>243</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This occurs when there are enough training data examples and sufficient variability between the contexts of a word that splitting the contexts into more than one cluster increases the EM auxiliary function.</text>
                  <doc_id>244</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For words where the contexts are not split, all the contexts remain in the same cluster and parameter estimation is exactly the same as for the unclustered context-independent models.</text>
                  <doc_id>245</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Training HMMs</title>
            <text>Adding source word context to translation has so far led to improvements in AER for Model 1, but the performance does not match that of HMMs trained on untagged data; we therefore train HMMs on tagged data.
We proceed with Model 1 and Model 2 trained in the usual way, and context-independent (CI) HMMs were trained for 5 iterations on the untagged data. Statistics were then gathered for clustering at various thresholds, after which 5 further EM iterations were performed with tagged data to produce context-dependent (CD) HMMs. The HMMs were trained in both the Arabic to English and the English to Arabic directions. The log likelihood of the training set varies with T imp in much the same way as for Model 1, increasing at each iteration, with greater likelihood at lower thresholds. Figure 4 shows how the AER of the union alignment varies with T imp during training. As with Model 1, the clustered HMM
50.8
50.6 80 p 0 =0.95
49.4
49.2 10 11 12 13 14 15 16 17 18 19 20 Iteration
51.2
72 English-Arabic CD HMM
English-Arabic CI HMM
Arabic-English CD HMM Arabic-English CI HMM p 0 =0.00
p 0 =0.00
AER 51.0
50.8
50.6
50.4
CI Model 1 Threshold 10 Threshold 20 Threshold 60 Threshold 100
50.2
50.0
49.8
models produce alignments with a lower AER than the baseline model, and there is evidence of over-fitting to the training data.
AER
49.6
51.0
50.8
50.6
50.4
50.2
50.0
49.8
49.6
49.4
49.2
49.0
10 11 12 13 14 15 16 17 18 19 20 Iteration
10 11 12 13 14 15 16 17 18 19 20 Iteration
CI Model 1 Threshold 10 Threshold 20 Threshold 60
AER 35.3
35.2
35.1
35.0
34.9
34.8
34.7
34.6
34.5
CI HMM Threshold 10 Threshold 20 Threshold 60
34.4 5 6 7 8 9 10
Iteration
3.3.1 Alignment precision and recall
The HMM models include a null transition probability, p 0 , which can be modified to adjust the number of alignments to the null token (Deng and Byrne, 2005a). Where a target word is emitted from null, it is not included in the alignment links, so this target word is viewed as not being aligned to any source word; this affects the precision and recall. The results reported above use p 0 = 0.2 for English-Arabic and p 0 = 0.4 for Arabic-English; we can tune these values to produce alignments with the lowest AER. Figure 5 shows precision-recall curves for the CD HMMs compared to the CI HMMs for both translation directions. For a given value of precision, the CD HMM has higher recall; for a given value of recall, the CD HMM has higher precision. We do not report F-score (Fraser and Marcu, 2006) since in our experiments we have not found strong correlation with translation performance, but we note that these results for precision and recall should lead to improved F-scores as well.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Adding source word context to translation has so far led to improvements in AER for Model 1, but the performance does not match that of HMMs trained on untagged data; we therefore train HMMs on tagged data.</text>
                  <doc_id>246</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We proceed with Model 1 and Model 2 trained in the usual way, and context-independent (CI) HMMs were trained for 5 iterations on the untagged data.</text>
                  <doc_id>247</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Statistics were then gathered for clustering at various thresholds, after which 5 further EM iterations were performed with tagged data to produce context-dependent (CD) HMMs.</text>
                  <doc_id>248</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The HMMs were trained in both the Arabic to English and the English to Arabic directions.</text>
                  <doc_id>249</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The log likelihood of the training set varies with T imp in much the same way as for Model 1, increasing at each iteration, with greater likelihood at lower thresholds.</text>
                  <doc_id>250</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 4 shows how the AER of the union alignment varies with T imp during training.</text>
                  <doc_id>251</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>As with Model 1, the clustered HMM</text>
                  <doc_id>252</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>50.8</text>
                  <doc_id>253</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>50.6 80 p 0 =0.95</text>
                  <doc_id>254</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>49.4</text>
                  <doc_id>255</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>49.2 10 11 12 13 14 15 16 17 18 19 20 Iteration</text>
                  <doc_id>256</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>51.2</text>
                  <doc_id>257</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>72 English-Arabic CD HMM</text>
                  <doc_id>258</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>English-Arabic CI HMM</text>
                  <doc_id>259</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Arabic-English CD HMM Arabic-English CI HMM p 0 =0.00</text>
                  <doc_id>260</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p 0 =0.00</text>
                  <doc_id>261</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>AER 51.0</text>
                  <doc_id>262</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>50.8</text>
                  <doc_id>263</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>50.6</text>
                  <doc_id>264</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>50.4</text>
                  <doc_id>265</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>CI Model 1 Threshold 10 Threshold 20 Threshold 60 Threshold 100</text>
                  <doc_id>266</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>50.2</text>
                  <doc_id>267</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>50.0</text>
                  <doc_id>268</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>49.8</text>
                  <doc_id>269</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>models produce alignments with a lower AER than the baseline model, and there is evidence of over-fitting to the training data.</text>
                  <doc_id>270</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>AER</text>
                  <doc_id>271</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>49.6</text>
                  <doc_id>272</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>51.0</text>
                  <doc_id>273</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>50.8</text>
                  <doc_id>274</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>50.6</text>
                  <doc_id>275</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>50.4</text>
                  <doc_id>276</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>50.2</text>
                  <doc_id>277</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>50.0</text>
                  <doc_id>278</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>49.8</text>
                  <doc_id>279</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>49.6</text>
                  <doc_id>280</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>49.4</text>
                  <doc_id>281</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>49.2</text>
                  <doc_id>282</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>49.0</text>
                  <doc_id>283</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>10 11 12 13 14 15 16 17 18 19 20 Iteration</text>
                  <doc_id>284</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>10 11 12 13 14 15 16 17 18 19 20 Iteration</text>
                  <doc_id>285</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>CI Model 1 Threshold 10 Threshold 20 Threshold 60</text>
                  <doc_id>286</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>AER 35.3</text>
                  <doc_id>287</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>35.2</text>
                  <doc_id>288</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>35.1</text>
                  <doc_id>289</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>35.0</text>
                  <doc_id>290</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>34.9</text>
                  <doc_id>291</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>34.8</text>
                  <doc_id>292</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>34.7</text>
                  <doc_id>293</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>34.6</text>
                  <doc_id>294</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>34.5</text>
                  <doc_id>295</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>CI HMM Threshold 10 Threshold 20 Threshold 60</text>
                  <doc_id>296</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>34.4 5 6 7 8 9 10</text>
                  <doc_id>297</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Iteration</text>
                  <doc_id>298</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3.3.1 Alignment precision and recall</text>
                  <doc_id>299</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The HMM models include a null transition probability, p 0 , which can be modified to adjust the number of alignments to the null token (Deng and Byrne, 2005a).</text>
                  <doc_id>300</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Where a target word is emitted from null, it is not included in the alignment links, so this target word is viewed as not being aligned to any source word; this affects the precision and recall.</text>
                  <doc_id>301</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The results reported above use p 0 = 0.2 for English-Arabic and p 0 = 0.4 for Arabic-English; we can tune these values to produce alignments with the lowest AER.</text>
                  <doc_id>302</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 5 shows precision-recall curves for the CD HMMs compared to the CI HMMs for both translation directions.</text>
                  <doc_id>303</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For a given value of precision, the CD HMM has higher recall; for a given value of recall, the CD HMM has higher precision.</text>
                  <doc_id>304</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We do not report F-score (Fraser and Marcu, 2006) since in our experiments we have not found strong correlation with translation performance, but we note that these results for precision and recall should lead to improved F-scores as well.</text>
                  <doc_id>305</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>4 Evaluation of translation quality</title>
        <text>We have shown that the context-dependent models produce a decrease in AER measured on manually-aligned data; we wish to show this improved model performance leads to an increase in translation quality, measured by BLEU score (Papineni et al., 2001). In addition to the Arabic systems already evaluated by AER, we also report results for a Chinese-English translation system.
Alignment models were evaluated by aligning the training data using the models in each translation direc-
tion. HiFST, a WFST-based hierarchical translation system described in (Iglesias et al., 2009), was trained on the union of these alignments. MET (Och, 2003) was carried out using a development set, and the BLEU score evaluated on two test sets. Decoding used a 4-gram language model estimated from the English side of the entire MT08 parallel text, and a 965M word subset of monolingual data from the English Gigaword Third Edition.
For both Arabic and English, the CD HMM models were evaluated as follows. Iteration 5 of the CI HMM was used to produce alignments for the parallel text training data: these were used to train the baseline system. The same data is aligned using CD HMMs after two further iterations of training and a second WFST-based translation system built from these alignments. The models are evaluated by comparing BLEU scores with those of the baseline model.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have shown that the context-dependent models produce a decrease in AER measured on manually-aligned data; we wish to show this improved model performance leads to an increase in translation quality, measured by BLEU score (Papineni et al., 2001).</text>
              <doc_id>306</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In addition to the Arabic systems already evaluated by AER, we also report results for a Chinese-English translation system.</text>
              <doc_id>307</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Alignment models were evaluated by aligning the training data using the models in each translation direc-</text>
              <doc_id>308</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tion.</text>
              <doc_id>309</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>HiFST, a WFST-based hierarchical translation system described in (Iglesias et al., 2009), was trained on the union of these alignments.</text>
              <doc_id>310</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>MET (Och, 2003) was carried out using a development set, and the BLEU score evaluated on two test sets.</text>
              <doc_id>311</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Decoding used a 4-gram language model estimated from the English side of the entire MT08 parallel text, and a 965M word subset of monolingual data from the English Gigaword Third Edition.</text>
              <doc_id>312</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For both Arabic and English, the CD HMM models were evaluated as follows.</text>
              <doc_id>313</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Iteration 5 of the CI HMM was used to produce alignments for the parallel text training data: these were used to train the baseline system.</text>
              <doc_id>314</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The same data is aligned using CD HMMs after two further iterations of training and a second WFST-based translation system built from these alignments.</text>
              <doc_id>315</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The models are evaluated by comparing BLEU scores with those of the baseline model.</text>
              <doc_id>316</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Arabic to English translation</title>
            <text>Alignment models were trained on the NIST MT08 Arabic-English parallel text, excluding the UN portion. The null alignment probability was chosen based on the AER, resulting in values of p 0 = 0.05 for Arabic to English and p 0 = 0.10 for English to Arabic. We perform experiments on the NIST Arabic-English translation task. The mt02 05 tune and mt02 05 test data sets are formed from the odd and even numbered sentences of the NIST MT02 to MT05 evaluation sets respectively; each contains 2k sentences and 60k words. We use mt02 05 tune as a development set and evaluate the system on mt02 05 test and the newswire portion of the MT08 set, MT08-nw. Table 3 shows a comparison of the system trained using CD HMMs with the baseline system, which was trained using CI HMM models on untagged data. The context-dependent models result in a gain in BLEU score of 0.3 for mt02 05 test and 0.6 for MT08-nw.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Alignment models were trained on the NIST MT08 Arabic-English parallel text, excluding the UN portion.</text>
                  <doc_id>317</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The null alignment probability was chosen based on the AER, resulting in values of p 0 = 0.05 for Arabic to English and p 0 = 0.10 for English to Arabic.</text>
                  <doc_id>318</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We perform experiments on the NIST Arabic-English translation task.</text>
                  <doc_id>319</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The mt02 05 tune and mt02 05 test data sets are formed from the odd and even numbered sentences of the NIST MT02 to MT05 evaluation sets respectively; each contains 2k sentences and 60k words.</text>
                  <doc_id>320</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We use mt02 05 tune as a development set and evaluate the system on mt02 05 test and the newswire portion of the MT08 set, MT08-nw.</text>
                  <doc_id>321</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Table 3 shows a comparison of the system trained using CD HMMs with the baseline system, which was trained using CI HMM models on untagged data.</text>
                  <doc_id>322</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The context-dependent models result in a gain in BLEU score of 0.3 for mt02 05 test and 0.6 for MT08-nw.</text>
                  <doc_id>323</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Chinese to English translation</title>
            <text>The Chinese training set was 600k random parallel text sentences of the newswire LDC collection allowed for NIST MT08, a total of 15.2M words of Chinese and 16.6M words of English. The Chinese text was tagged using the MXPOST maximum-entropy part of speech tagging tool (Ratnaparkhi, 1996) trained on the Penn Chinese Treebank 5.1; the English text was tagged using the TnT part of speech tagger (Brants, 2000) trained on the Wall Street Journal portion of the English Penn treebank.
The development set tune-nw and validation set test-nw contain a mix of the newswire portions of MT02 through MT05 and additional developments sets created by translation within the GALE program. We also report results on the newswire portion of the MT08 set. Again we see an increase in BLEU score for both test sets: 0.5 for test-
nw and 0.8 for MT08-nw.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The Chinese training set was 600k random parallel text sentences of the newswire LDC collection allowed for NIST MT08, a total of 15.2M words of Chinese and 16.6M words of English.</text>
                  <doc_id>324</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The Chinese text was tagged using the MXPOST maximum-entropy part of speech tagging tool (Ratnaparkhi, 1996) trained on the Penn Chinese Treebank 5.1; the English text was tagged using the TnT part of speech tagger (Brants, 2000) trained on the Wall Street Journal portion of the English Penn treebank.</text>
                  <doc_id>325</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The development set tune-nw and validation set test-nw contain a mix of the newswire portions of MT02 through MT05 and additional developments sets created by translation within the GALE program.</text>
                  <doc_id>326</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We also report results on the newswire portion of the MT08 set.</text>
                  <doc_id>327</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Again we see an increase in BLEU score for both test sets: 0.5 for test-</text>
                  <doc_id>328</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>nw and 0.8 for MT08-nw.</text>
                  <doc_id>329</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>5 Conclusions and future work</title>
        <text>We have introduced context-dependent Model 1 and HMM alignment models, which use context information in the source language to improve estimates of wordto-word translation probabilities. Estimation of parameters using these contexts without smoothing leads to data sparsity problems; therefore we have developed decision tree clustering algorithms to cluster source word contexts based on optimisation of the EM auxiliary function. Context information is incorporated by the use of part-ofspeech tags in both languages of the parallel text, and the EM algorithm is used for parameter estimation. We have shown that these improvements to the model lead to decreased AER compared to context-independent models. Finally, we compare machine translation systems built using our context-dependent alignments. For both Arabic- and Chinese-to-English translation, we report an increase in translation quality measured by BLEU score compared to a system built using contextindependent alignments. This paper describes an initial investigation into context-sensitive alignment models, and there are many possible directions for future research. Clustering the probability distributions of infrequently occurring may produce improvements in alignment quality, different model training schemes and extensions of the contextdependence to more sophisticated alignment models will be investigated. Further translation experiments will be carried out.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have introduced context-dependent Model 1 and HMM alignment models, which use context information in the source language to improve estimates of wordto-word translation probabilities.</text>
              <doc_id>330</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Estimation of parameters using these contexts without smoothing leads to data sparsity problems; therefore we have developed decision tree clustering algorithms to cluster source word contexts based on optimisation of the EM auxiliary function.</text>
              <doc_id>331</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Context information is incorporated by the use of part-ofspeech tags in both languages of the parallel text, and the EM algorithm is used for parameter estimation.</text>
              <doc_id>332</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We have shown that these improvements to the model lead to decreased AER compared to context-independent models.</text>
              <doc_id>333</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we compare machine translation systems built using our context-dependent alignments.</text>
              <doc_id>334</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>For both Arabic- and Chinese-to-English translation, we report an increase in translation quality measured by BLEU score compared to a system built using contextindependent alignments.</text>
              <doc_id>335</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>This paper describes an initial investigation into context-sensitive alignment models, and there are many possible directions for future research.</text>
              <doc_id>336</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Clustering the probability distributions of infrequently occurring may produce improvements in alignment quality, different model training schemes and extensions of the contextdependence to more sophisticated alignment models will be investigated.</text>
              <doc_id>337</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Further translation experiments will be carried out.</text>
              <doc_id>338</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>Acknowledgements</title>
        <text>This work was supported in part by the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-C-0022. J. Brunning is supported by a Schiff Foundation graduate studentship. Thanks to Yanjun Ma, Dublin City University, for training the Chinese part of speech tagger.
References
A. L. Berger, S. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39&#8211;71. Graeme Blackwood, Adri&#224; de Gispert, Jamie Brunning, and
William Byrne. 2008. European language translation with weighted finite state transducers: The CUED MT system for the 2008 ACL workshop on SMT. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 131&#8211;134, Columbus, Ohio, June. Association for Computational Linguistics.
Thorsten Brants. 2000. TnT &#8211; a statistical part-of-speech tagger. In Proceedings of the 6th Applied Natural Language Processing Conference: ANLP-2000, Seattle, USA.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263&#8211;311. T. Buckwalter. 2002. Buckwalter Arabic morphological analyzer. Marta Ruiz Costa-juss&#224; and Jos&#180;e A. R. Fonollosa. 2005.
Improving phrase-based statistical translation by modifying phrase extraction and including several features. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 149&#8211;154, June.
Yonggang Deng and William Byrne. 2005a. HMM word and phrase alignment for statistical machine translation. In Proc. of HLT-EMNLP.
Yonggang Deng and William Byrne. 2005b. JHU-Cambridge
statistical machine translation toolkit (MTTK) user manual. Yonggang Deng, Shankhar Kumar, and William Byrne. 2007. Segmentation and alignment of parallel text for statistical machine translation. Journal of Natural Language Engineering, 13:3:235&#8211;260. Alexander Fraser and Daniel Marcu. 2006. Measuring word
alignment quality for statistical machine translation. Technical Report ISI-TR-616, ISI/University of Southern California, May. Nizar Habash and Fatiha Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In HLT-NAACL. G. Iglesias, A. de Gispert, E. R. Banga, and W. Byrne. 2009. Hierarchical phrase-based translation with weighted finite state transducers. In Procedings of NAACL-HLT, 2009, Boulder, Colorado. Abraham Ittycheriah, Yaser Al-Onaizan, and Salim Roukos.
2006. The IBM Arabic-English word alignment corpus, August. A. Kannan, M. Ostendorf, and J. R. Rohlicek. 1994. Maximum likelihood clustering of Gaussians for speech recognition. Speech and Audio Processing, IEEE Transactions on, 2(3):453&#8211;455, July.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL &#8217;03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48&#8211;54. Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of
English: the Penn Treebank. Computational Linguistics,
19(2):313&#8211;330. Sonja Nie&#223;en and Hermann Ney. 2001a. Morpho-syntactic
analysis for reordering in statistical machine translation. In Proceedings of MT Summit VIII, pages 247&#8211;252, September. Sonja Nie&#223;en and Hermann Ney. 2001b. Toward hierarchical models for statistical machine translation of inflected languages. In Proceedings of the workshop on Data-driven methods in machine translation, pages 1&#8211;8, Morristown, NJ, USA. Association for Computational Linguistics. Franz Josef Och and Hermann Ney. 2000. A comparison of
alignment models for statistical machine translation. In Proceedings of the 18th conference on Computational Linguistics, pages 1086&#8211;1090. F. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of features for statistical machine translation. In Proceedings of NAACL. Franz Josef Och. 2002. Statistical Machine Translation: From
Single Word Models to Alignment Templates. Ph.D. thesis, Franz Josef Och. Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL &#8217;03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160&#8211;167. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311&#8211;318. Maja Popovi&#263; and Hermann Ney. 2004. Improving word alignment quality using morpho-syntactic information. In In Proceedings of COLING, page 310. Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133&#8211;142. H. Singer and M. Ostendorf. 1996. Maximum likelihood successive state splitting. Proceedings of ICASSP, 2:601&#8211;604. Nicolas Stroppa, Antal van den Bosch, and Andy Way. 2007.
Exploiting source similarity for SMT using context-informed features. In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI 2007), pages 231 &#8211; 240. Kristina Toutanova, H. Tolga Ilhan, and Christopher D. Manning. 2002. Extensions to HMM-based statistical word alignment models. In Proceedings of EMNLP, pages 87&#8211;94. Ismael Garc&#237;a Varea, Franz J. Och, Hermann Ney, and Francisco Casacuberta. 2002. Improving alignment quality in statistical machine translation using context-dependent maximum entropy models. In Proceedings of COLING, pages 1&#8211;7. Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of COLING, pages 836&#8211;841. S. J. Young, J. J. Odell, and P. C. Woodland. 1994. Tree-based
state tying for high accuracy acoustic modelling. In HLT &#8217;94: Proceedings of the workshop on Human Language Technology, pages 307&#8211;312.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This work was supported in part by the GALE program of the Defense Advanced Research Projects Agency, Contract No.</text>
              <doc_id>339</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>HR0011-06-C-0022.</text>
              <doc_id>340</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>J. Brunning is supported by a Schiff Foundation graduate studentship.</text>
              <doc_id>341</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Thanks to Yanjun Ma, Dublin City University, for training the Chinese part of speech tagger.</text>
              <doc_id>342</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>References</text>
              <doc_id>343</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A.</text>
              <doc_id>344</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>L. Berger, S. Della Pietra, and V.</text>
              <doc_id>345</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>J. Della Pietra.</text>
              <doc_id>346</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>1996.</text>
              <doc_id>347</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>A maximum entropy approach to natural language processing.</text>
              <doc_id>348</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 22(1):39&#8211;71.</text>
              <doc_id>349</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Graeme Blackwood, Adri&#224; de Gispert, Jamie Brunning, and</text>
              <doc_id>350</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>William Byrne.</text>
              <doc_id>351</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>352</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>European language translation with weighted finite state transducers: The CUED MT system for the 2008 ACL workshop on SMT.</text>
              <doc_id>353</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Third Workshop on Statistical Machine Translation, pages 131&#8211;134, Columbus, Ohio, June.</text>
              <doc_id>354</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>355</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Thorsten Brants.</text>
              <doc_id>356</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2000.</text>
              <doc_id>357</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>TnT &#8211; a statistical part-of-speech tagger.</text>
              <doc_id>358</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 6th Applied Natural Language Processing Conference: ANLP-2000, Seattle, USA.</text>
              <doc_id>359</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer.</text>
              <doc_id>360</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1993.</text>
              <doc_id>361</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The mathematics of statistical machine translation: parameter estimation.</text>
              <doc_id>362</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics, 19(2):263&#8211;311.</text>
              <doc_id>363</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>T. Buckwalter.</text>
              <doc_id>364</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>365</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Buckwalter Arabic morphological analyzer.</text>
              <doc_id>366</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Marta Ruiz Costa-juss&#224; and Jos&#180;e A. R. Fonollosa.</text>
              <doc_id>367</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>368</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Improving phrase-based statistical translation by modifying phrase extraction and including several features.</text>
              <doc_id>369</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 149&#8211;154, June.</text>
              <doc_id>370</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Yonggang Deng and William Byrne.</text>
              <doc_id>371</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2005a.</text>
              <doc_id>372</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>HMM word and phrase alignment for statistical machine translation.</text>
              <doc_id>373</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proc. of HLT-EMNLP.</text>
              <doc_id>374</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Yonggang Deng and William Byrne.</text>
              <doc_id>375</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2005b.</text>
              <doc_id>376</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>JHU-Cambridge</text>
              <doc_id>377</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>statistical machine translation toolkit (MTTK) user manual.</text>
              <doc_id>378</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Yonggang Deng, Shankhar Kumar, and William Byrne.</text>
              <doc_id>379</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>380</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Segmentation and alignment of parallel text for statistical machine translation.</text>
              <doc_id>381</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Journal of Natural Language Engineering, 13:3:235&#8211;260.</text>
              <doc_id>382</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Alexander Fraser and Daniel Marcu.</text>
              <doc_id>383</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>384</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Measuring word</text>
              <doc_id>385</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>alignment quality for statistical machine translation.</text>
              <doc_id>386</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Technical Report ISI-TR-616, ISI/University of Southern California, May.</text>
              <doc_id>387</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Nizar Habash and Fatiha Sadat.</text>
              <doc_id>388</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2006.</text>
              <doc_id>389</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Arabic preprocessing</text>
              <doc_id>390</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>schemes for statistical machine translation.</text>
              <doc_id>391</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In HLT-NAACL.</text>
              <doc_id>392</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>G. Iglesias, A. de Gispert, E. R. Banga, and W. Byrne.</text>
              <doc_id>393</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>394</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Hierarchical phrase-based translation with weighted finite state transducers.</text>
              <doc_id>395</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In Procedings of NAACL-HLT, 2009, Boulder, Colorado.</text>
              <doc_id>396</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Abraham Ittycheriah, Yaser Al-Onaizan, and Salim Roukos.</text>
              <doc_id>397</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2006.</text>
              <doc_id>398</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The IBM Arabic-English word alignment corpus, August.</text>
              <doc_id>399</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A. Kannan, M. Ostendorf, and J. R. Rohlicek.</text>
              <doc_id>400</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>1994.</text>
              <doc_id>401</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Maximum likelihood clustering of Gaussians for speech recognition.</text>
              <doc_id>402</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Speech and Audio Processing, IEEE Transactions on, 2(3):453&#8211;455, July.</text>
              <doc_id>403</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Philipp Koehn, Franz Josef Och, and Daniel Marcu.</text>
              <doc_id>404</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>405</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Statistical phrase-based translation.</text>
              <doc_id>406</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In NAACL &#8217;03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48&#8211;54.</text>
              <doc_id>407</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.</text>
              <doc_id>408</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>1993. Building a large annotated corpus of</text>
              <doc_id>409</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>English: the Penn Treebank.</text>
              <doc_id>410</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics,</text>
              <doc_id>411</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>19(2):313&#8211;330.</text>
              <doc_id>412</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Sonja Nie&#223;en and Hermann Ney.</text>
              <doc_id>413</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>2001a.</text>
              <doc_id>414</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Morpho-syntactic</text>
              <doc_id>415</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>analysis for reordering in statistical machine translation.</text>
              <doc_id>416</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of MT Summit VIII, pages 247&#8211;252, September.</text>
              <doc_id>417</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Sonja Nie&#223;en and Hermann Ney.</text>
              <doc_id>418</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2001b.</text>
              <doc_id>419</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Toward hierarchical models for statistical machine translation of inflected languages.</text>
              <doc_id>420</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the workshop on Data-driven methods in machine translation, pages 1&#8211;8, Morristown, NJ, USA.</text>
              <doc_id>421</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>422</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Franz Josef Och and Hermann Ney.</text>
              <doc_id>423</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>2000.</text>
              <doc_id>424</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>A comparison of</text>
              <doc_id>425</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>alignment models for statistical machine translation.</text>
              <doc_id>426</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 18th conference on Computational Linguistics, pages 1086&#8211;1090.</text>
              <doc_id>427</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>F. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,</text>
              <doc_id>428</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain, Z. Jin, and D. Radev.</text>
              <doc_id>429</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2004.</text>
              <doc_id>430</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A smorgasbord of features for statistical machine translation.</text>
              <doc_id>431</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of NAACL.</text>
              <doc_id>432</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Franz Josef Och.</text>
              <doc_id>433</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>434</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Statistical Machine Translation: From</text>
              <doc_id>435</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Single Word Models to Alignment Templates.</text>
              <doc_id>436</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Ph.D.</text>
              <doc_id>437</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>thesis, Franz Josef Och.</text>
              <doc_id>438</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Franz Josef Och.</text>
              <doc_id>439</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>440</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Minimum error rate training in statistical machine translation.</text>
              <doc_id>441</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In ACL &#8217;03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160&#8211;167.</text>
              <doc_id>442</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing</text>
              <doc_id>443</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Zhu.</text>
              <doc_id>444</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2001.</text>
              <doc_id>445</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Bleu: a method for automatic evaluation of machine translation.</text>
              <doc_id>446</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of ACL, pages 311&#8211;318.</text>
              <doc_id>447</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Maja Popovi&#263; and Hermann Ney.</text>
              <doc_id>448</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2004.</text>
              <doc_id>449</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Improving word alignment quality using morpho-syntactic information.</text>
              <doc_id>450</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In In Proceedings of COLING, page 310.</text>
              <doc_id>451</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Adwait Ratnaparkhi.</text>
              <doc_id>452</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>1996.</text>
              <doc_id>453</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>A maximum entropy model for part-of-speech tagging.</text>
              <doc_id>454</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>In In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133&#8211;142.</text>
              <doc_id>455</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>H. Singer and M. Ostendorf.</text>
              <doc_id>456</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>1996.</text>
              <doc_id>457</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>Maximum likelihood successive state splitting.</text>
              <doc_id>458</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>Proceedings of ICASSP, 2:601&#8211;604.</text>
              <doc_id>459</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>Nicolas Stroppa, Antal van den Bosch, and Andy Way.</text>
              <doc_id>460</doc_id>
              <sec_id>16</sec_id>
            </sentence>
            <sentence>
              <text>2007.</text>
              <doc_id>461</doc_id>
              <sec_id>17</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Exploiting source similarity for SMT using context-informed features.</text>
              <doc_id>462</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI 2007), pages 231 &#8211; 240.</text>
              <doc_id>463</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Kristina Toutanova, H. Tolga Ilhan, and Christopher D. Manning.</text>
              <doc_id>464</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>465</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Extensions to HMM-based statistical word alignment models.</text>
              <doc_id>466</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of EMNLP, pages 87&#8211;94.</text>
              <doc_id>467</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Ismael Garc&#237;a Varea, Franz J. Och, Hermann Ney, and Francisco Casacuberta.</text>
              <doc_id>468</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>2002.</text>
              <doc_id>469</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Improving alignment quality in statistical machine translation using context-dependent maximum entropy models.</text>
              <doc_id>470</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of COLING, pages 1&#8211;7.</text>
              <doc_id>471</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Stephan Vogel, Hermann Ney, and Christoph Tillmann.</text>
              <doc_id>472</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>1996.</text>
              <doc_id>473</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>HMM-based word alignment in statistical translation.</text>
              <doc_id>474</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of COLING, pages 836&#8211;841.</text>
              <doc_id>475</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>S.</text>
              <doc_id>476</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>J. Young, J.</text>
              <doc_id>477</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>J. Odell, and P.</text>
              <doc_id>478</doc_id>
              <sec_id>16</sec_id>
            </sentence>
            <sentence>
              <text>C. Woodland.</text>
              <doc_id>479</doc_id>
              <sec_id>17</sec_id>
            </sentence>
            <sentence>
              <text>1994.</text>
              <doc_id>480</doc_id>
              <sec_id>18</sec_id>
            </sentence>
            <sentence>
              <text>Tree-based</text>
              <doc_id>481</doc_id>
              <sec_id>19</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>state tying for high accuracy acoustic modelling.</text>
              <doc_id>482</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In HLT &#8217;94: Proceedings of the workshop on Human Language Technology, pages 307&#8211;312.</text>
              <doc_id>483</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Most frequent root node context questions</caption>
        <reference_text>In PAGE 6: ...2.1 Analysis of frequently used questions  Table1  shows the questions used most frequently at the root node of the decision tree when clustering con- texts in English and Arabic. Because they are used  rst, these are the questions that individually give the great- est ability to discriminate between the different contexts of a word....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>English question</cell>
              <cell>Frequency</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Is Next Preposition</cell>
              <cell>1523</cell>
            </row>
            <row>
              <cell>Is Prev Determiner</cell>
              <cell>1444</cell>
            </row>
            <row>
              <cell>Is Prev Preposition</cell>
              <cell>1209</cell>
            </row>
            <row>
              <cell>Is Prev Adjective</cell>
              <cell>864</cell>
            </row>
            <row>
              <cell>Is Next Noun Singular Mass</cell>
              <cell>772</cell>
            </row>
            <row>
              <cell>Is Prev Noun Singular Mass</cell>
              <cell>690</cell>
            </row>
            <row>
              <cell>Is Next Noun Plural</cell>
              <cell>597</cell>
            </row>
            <row>
              <cell>Is Next Noun</cell>
              <cell>549</cell>
            </row>
            <row>
              <cell>Arabic question</cell>
              <cell>Frequency</cell>
            </row>
            <row>
              <cell>Is Prev Preposition</cell>
              <cell>1110</cell>
            </row>
            <row>
              <cell>Is Next Preposition</cell>
              <cell>993</cell>
            </row>
            <row>
              <cell>Is Prev Noun</cell>
              <cell>981</cell>
            </row>
            <row>
              <cell>Is Next Noun</cell>
              <cell>912</cell>
            </row>
            <row>
              <cell>Is Prev Coordinating Conjunction</cell>
              <cell>627</cell>
            </row>
            <row>
              <cell>Is Prev Noun SingularMass</cell>
              <cell>607</cell>
            </row>
            <row>
              <cell>Is Next Punctuation</cell>
              <cell>603</cell>
            </row>
            <row>
              <cell>Is Next Adjective Adverb</cell>
              <cell>559</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Words [number (percentage)] with context-dependent translation for varying T imp</caption>
        <reference_text></reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>T imp</cell>
              <cell>Arabic-English (%)</cell>
              <cell>English-Arabic (%)</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>10</cell>
              <cell>30601 (25.33)</cell>
              <cell>26011 (39.87)</cell>
            </row>
            <row>
              <cell>20</cell>
              <cell>11193 (9.27)</cell>
              <cell>18365 (28.15)</cell>
            </row>
            <row>
              <cell>40</cell>
              <cell>1874 (1.55)</cell>
              <cell>9104 (13.96)</cell>
            </row>
            <row>
              <cell>100</cell>
              <cell>307 (0.25)</cell>
              <cell>1128 (1.73)</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Comparison, using BLEU score, of the CD HMM with the baseline CI HMM</caption>
        <reference_text>In PAGE 8: ... We use mt02 05 tune as a development set and evaluate the sys- tem on mt02 05 test and the newswire portion of the MT08 set, MT08-nw.  Table3  shows a comparison of the system trained using CD HMMs with the baseline sys- tem, which was trained using CI HMM models on un- tagged data. The context-dependent models result in a gain in BLEU score of 0....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>Arabic-English</cell>
              <cell>Arabic-English</cell>
              <cell>Arabic-English</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Alignments</cell>
              <cell>tune</cell>
              <cell>mt02 05 test</cell>
              <cell>MT08-nw</cell>
            </row>
            <row>
              <cell>CI HMM</cell>
              <cell>50.0</cell>
              <cell>49.4</cell>
              <cell>46.3</cell>
            </row>
            <row>
              <cell>CD HMM</cell>
              <cell>50.0</cell>
              <cell>49.7</cell>
              <cell>46.9</cell>
            </row>
            <row>
              <cell>Chinese-English</cell>
              <cell>None</cell>
              <cell>None</cell>
            </row>
            <row>
              <cell>Alignments</cell>
              <cell>tune</cell>
              <cell>test-nw</cell>
              <cell>MT08-nw</cell>
            </row>
            <row>
              <cell>CI HMM</cell>
              <cell>28.1</cell>
              <cell>28.5</cell>
              <cell>26.9</cell>
            </row>
            <row>
              <cell>CD HMM</cell>
              <cell>28.5</cell>
              <cell>29.0</cell>
              <cell>27.7</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>A L Berger</author>
          <author>S Della Pietra</author>
          <author>V J Della Pietra</author>
        </authors>
        <title>A maximum entropy approach to natural language processing.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1996</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Graeme Blackwood</author>
          <author>Adri&#224; de Gispert</author>
          <author>Jamie Brunning</author>
          <author>William Byrne</author>
        </authors>
        <title>European language translation with weighted finite state transducers: The CUED MT system for the</title>
        <publication>In Proceedings of the Third Workshop on Statistical Machine Translation,</publication>
        <pages>131--134</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Thorsten Brants</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>3</id>
        <authors/>
        <title>None</title>
        <publication>In Proceedings of the 6th Applied Natural Language Processing Conference: ANLP-2000,</publication>
        <pages>None</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Peter F Brown</author>
          <author>Vincent J Della Pietra</author>
          <author>Stephen A Della Pietra</author>
          <author>Robert L Mercer</author>
        </authors>
        <title>The mathematics of statistical machine translation: parameter estimation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1993</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>T Buckwalter</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Marta Ruiz Costa-juss&#224;</author>
          <author>Jos&#180;e A R Fonollosa</author>
        </authors>
        <title>Improving phrase-based statistical translation by modifying phrase extraction and including several features.</title>
        <publication>In Proceedings of the ACL Workshop on Building and Using Parallel Texts,</publication>
        <pages>149--154</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Yonggang Deng</author>
          <author>William Byrne</author>
        </authors>
        <title>HMM word and phrase alignment for statistical machine translation.</title>
        <publication>In Proc. of HLT-EMNLP.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Yonggang Deng</author>
          <author>William Byrne</author>
        </authors>
        <title>JHU-Cambridge statistical machine translation toolkit (MTTK)</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Yonggang Deng</author>
          <author>Shankhar Kumar</author>
          <author>William Byrne</author>
        </authors>
        <title>Segmentation and alignment of parallel text for statistical machine translation.</title>
        <publication>None</publication>
        <pages>13--3</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Alexander Fraser</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Measuring word alignment quality for statistical machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Nizar Habash</author>
          <author>Fatiha Sadat</author>
        </authors>
        <title>Arabic preprocessing schemes for statistical machine translation.</title>
        <publication>In HLT-NAACL.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>G Iglesias</author>
          <author>A de Gispert</author>
          <author>E R Banga</author>
          <author>W Byrne</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>13</id>
        <authors/>
        <title>Hierarchical phrase-based translation with weighted finite state transducers.</title>
        <publication>In Procedings of NAACL-HLT,</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Abraham Ittycheriah</author>
          <author>Yaser Al-Onaizan</author>
          <author>Salim Roukos</author>
        </authors>
        <title>None</title>
        <publication>The IBM Arabic-English word alignment corpus,</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>A Kannan</author>
          <author>M Ostendorf</author>
          <author>J R Rohlicek</author>
        </authors>
        <title>Maximum likelihood clustering of Gaussians for speech recognition. Speech and Audio Processing,</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1994</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz Josef Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical phrase-based translation.</title>
        <publication>In NAACL &#8217;03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</publication>
        <pages>48--54</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Mitchell P Marcus</author>
          <author>Mary Ann Marcinkiewicz</author>
          <author>Beatrice Santorini</author>
        </authors>
        <title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1993</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Sonja Nie&#223;en</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Morpho-syntactic analysis for reordering in statistical machine translation.</title>
        <publication>In Proceedings of MT Summit VIII,</publication>
        <pages>247--252</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Sonja Nie&#223;en</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Toward hierarchical models for statistical machine translation of inflected languages.</title>
        <publication>In Proceedings of the workshop on Data-driven methods in machine translation,</publication>
        <pages>1--8</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A comparison of alignment models for statistical machine translation.</title>
        <publication>In Proceedings of the 18th conference on Computational Linguistics,</publication>
        <pages>1086--1090</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>F Och</author>
          <author>D Gildea</author>
          <author>S Khudanpur</author>
          <author>A Sarkar</author>
          <author>K Yamada</author>
          <author>A Fraser</author>
          <author>S Kumar</author>
          <author>L Shen</author>
          <author>D Smith</author>
          <author>K Eng</author>
          <author>V Jain</author>
          <author>Z Jin</author>
          <author>D Radev</author>
        </authors>
        <title>A smorgasbord of features for statistical machine translation.</title>
        <publication>In Proceedings of NAACL.</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Statistical Machine Translation: From Single Word Models to Alignment Templates.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>In ACL &#8217;03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</publication>
        <pages>160--167</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>Wei-Jing Zhu</author>
        </authors>
        <title>Bleu: a method for automatic evaluation of machine translation.</title>
        <publication>In Proceedings of ACL,</publication>
        <pages>311--318</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Maja Popovi&#263;</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Improving word alignment quality using morpho-syntactic information. In</title>
        <publication>In Proceedings of COLING,</publication>
        <pages>310</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>Adwait Ratnaparkhi</author>
        </authors>
        <title>A maximum entropy model for part-of-speech tagging. In</title>
        <publication>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>133--142</pages>
        <date>1996</date>
      </reference>
      <reference>
        <id>27</id>
        <authors>
          <author>H Singer</author>
          <author>M Ostendorf</author>
        </authors>
        <title>Maximum likelihood successive state splitting.</title>
        <publication>Proceedings of ICASSP,</publication>
        <pages>2--601</pages>
        <date>1996</date>
      </reference>
      <reference>
        <id>28</id>
        <authors>
          <author>Nicolas Stroppa</author>
          <author>Antal van den Bosch</author>
          <author>Andy Way</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>29</id>
        <authors/>
        <title>Exploiting source similarity for SMT using context-informed features.</title>
        <publication>In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI</publication>
        <pages>231--240</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>30</id>
        <authors>
          <author>Kristina Toutanova</author>
          <author>H Tolga Ilhan</author>
          <author>Christopher D Manning</author>
        </authors>
        <title>Extensions to HMM-based statistical word alignment models.</title>
        <publication>In Proceedings of EMNLP,</publication>
        <pages>87--94</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>31</id>
        <authors>
          <author>Ismael Garc&#237;a Varea</author>
          <author>Franz J Och</author>
          <author>Hermann Ney</author>
          <author>Francisco Casacuberta</author>
        </authors>
        <title>Improving alignment quality in statistical machine translation using context-dependent maximum entropy models.</title>
        <publication>In Proceedings of COLING,</publication>
        <pages>1--7</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>32</id>
        <authors>
          <author>Stephan Vogel</author>
          <author>Hermann Ney</author>
          <author>Christoph Tillmann</author>
        </authors>
        <title>HMM-based word alignment in statistical translation.</title>
        <publication>In Proceedings of COLING,</publication>
        <pages>836--841</pages>
        <date>1996</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Berger et al. (1996)</string>
        <sentence_id>21748</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>2</reference_id>
        <string>Brants, 2000</string>
        <sentence_id>22029</sentence_id>
        <char_offset>16</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Brants, 2000</string>
        <sentence_id>22045</sentence_id>
        <char_offset>216</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>4</reference_id>
        <string>Brown et al., 1993</string>
        <sentence_id>21800</sentence_id>
        <char_offset>121</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>4</reference_id>
        <string>Brown et al., 1993</string>
        <sentence_id>21935</sentence_id>
        <char_offset>205</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>4</reference_id>
        <string>Brown et al. (1993)</string>
        <sentence_id>21744</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>4</reference_id>
        <string>Brown et al. (1993)</string>
        <sentence_id>21760</sentence_id>
        <char_offset>10</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>5</reference_id>
        <string>Buckwalter, 2002</string>
        <sentence_id>22024</sentence_id>
        <char_offset>151</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>6</reference_id>
        <string>Costa-juss&#224; and Fonollosa, 2005</string>
        <sentence_id>21897</sentence_id>
        <char_offset>58</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>7</reference_id>
        <string>Deng and Byrne (2005</string>
        <sentence_id>21744</sentence_id>
        <char_offset>262</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>7</reference_id>
        <string>Deng and Byrne, 2005</string>
        <sentence_id>22018</sentence_id>
        <char_offset>46</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>7</reference_id>
        <string>Deng and Byrne, 2005</string>
        <sentence_id>21935</sentence_id>
        <char_offset>225</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>7</reference_id>
        <string>Deng and Byrne, 2005</string>
        <sentence_id>22012</sentence_id>
        <char_offset>136</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>8</reference_id>
        <string>Deng and Byrne (2005</string>
        <sentence_id>21744</sentence_id>
        <char_offset>262</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>8</reference_id>
        <string>Deng and Byrne, 2005</string>
        <sentence_id>22018</sentence_id>
        <char_offset>46</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>8</reference_id>
        <string>Deng and Byrne, 2005</string>
        <sentence_id>21935</sentence_id>
        <char_offset>225</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>8</reference_id>
        <string>Deng and Byrne, 2005</string>
        <sentence_id>22012</sentence_id>
        <char_offset>136</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>9</reference_id>
        <string>Deng et al., 2007</string>
        <sentence_id>21897</sentence_id>
        <char_offset>135</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>10</reference_id>
        <string>Fraser and Marcu, 2006</string>
        <sentence_id>22017</sentence_id>
        <char_offset>26</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>11</reference_id>
        <string>Habash and Sadat (2006)</string>
        <sentence_id>21745</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>11</reference_id>
        <string>Habash and Sadat, 2006</string>
        <sentence_id>22023</sentence_id>
        <char_offset>18</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>11</reference_id>
        <string>Habash and Sadat, 2006</string>
        <sentence_id>22027</sentence_id>
        <char_offset>102</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>14</reference_id>
        <string>Ittycheriah et al., 2006</string>
        <sentence_id>22032</sentence_id>
        <char_offset>127</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>15</reference_id>
        <string>Kannan et al. (1994)</string>
        <sentence_id>21891</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>16</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>21948</sentence_id>
        <char_offset>79</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>17</reference_id>
        <string>Marcus et al. (1993)</string>
        <sentence_id>22030</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>18</reference_id>
        <string>Nie&#223;en and Ney (2001</string>
        <sentence_id>21747</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>18</reference_id>
        <string>Nie&#223;en and Ney (2001</string>
        <sentence_id>21747</sentence_id>
        <char_offset>93</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>19</reference_id>
        <string>Nie&#223;en and Ney (2001</string>
        <sentence_id>21747</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>19</reference_id>
        <string>Nie&#223;en and Ney (2001</string>
        <sentence_id>21747</sentence_id>
        <char_offset>93</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>20</reference_id>
        <string>Och and Ney, 2000</string>
        <sentence_id>22035</sentence_id>
        <char_offset>73</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>21</reference_id>
        <string>Och et al., 2004</string>
        <sentence_id>21896</sentence_id>
        <char_offset>71</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>22</reference_id>
        <string>Och (2002)</string>
        <sentence_id>21953</sentence_id>
        <char_offset>140</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>23</reference_id>
        <string>Och, 2003</string>
        <sentence_id>22055</sentence_id>
        <char_offset>5</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>24</reference_id>
        <string>Papineni et al., 2001</string>
        <sentence_id>22050</sentence_id>
        <char_offset>226</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>25</reference_id>
        <string>Popovi&#263; and Ney (2004)</string>
        <sentence_id>21751</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>26</reference_id>
        <string>Ratnaparkhi, 1996</string>
        <sentence_id>22045</sentence_id>
        <char_offset>90</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>27</reference_id>
        <string>Singer and Ostendorf, 1996</string>
        <sentence_id>21834</sentence_id>
        <char_offset>272</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>29</reference_id>
        <string>(2007)</string>
        <sentence_id>21752</sentence_id>
        <char_offset>15</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>30</reference_id>
        <string>Toutanova et al. (2002)</string>
        <sentence_id>21750</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>31</reference_id>
        <string>Varea et al. (2002)</string>
        <sentence_id>21749</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>32</reference_id>
        <string>Vogel et al. (1996)</string>
        <sentence_id>21744</sentence_id>
        <char_offset>70</char_offset>
      </citation>
    </citations>
  </content>
</document>
