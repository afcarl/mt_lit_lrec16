<document>
  <filename>W08-0306</filename>
  <authors>
    <author>Victoria Fossum</author>
    <author>Kevin Knight</author>
  </authors>
  <title>Using Syntax to Improve Word Alignment Precision for Syntax-Based Machine Translation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>Word alignments that violate syntactic correspondences interfere with the extraction of string-to-tree transducer rules for syntaxbased machine translation. We present an algorithm for identifying and deleting incorrect word alignment links, using features of the extracted rules. We obtain gains in both alignment quality and translation quality in Chinese-English and Arabic-English translation experiments relative to a GIZA++ union baseline.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Word alignments that violate syntactic correspondences interfere with the extraction of string-to-tree transducer rules for syntaxbased machine translation.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We present an algorithm for identifying and deleting incorrect word alignment links, using features of the extracted rules.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We obtain gains in both alignment quality and translation quality in Chinese-English and Arabic-English translation experiments relative to a GIZA++ union baseline.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>3</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>1.1 Motivation</title>
            <text>Word alignment typically constitutes the first stage of the statistical machine translation pipeline. GIZA++ (Och and Ney, 2003), an implementation of the IBM (Brown et al., 1993) and HMM (?) alignment models, is the most widely-used alignment system. GIZA++ union alignments have been used in the state-of-the-art syntax-based statistical MT system described in (Galley et al., 2006) and in the hierarchical phrase-based system Hiero (Chiang, 2007). GIZA++ refined alignments have been used in state-of-the-art phrase-based statistical MT systems such as (Och, 2004); variations on the refined heuristic have been used by (Koehn et al., 2003) (diag and diag-and) and by the phrase-based system Moses (grow-diag-final) (Koehn et al., 2007). GIZA++ union alignments have high recall but low precision, while intersection or refined alignments have high precision but low recall. 1 There are two natural approaches to improving upon GIZA++ alignments, then: deleting links from union alignments, or adding links to intersection or refined alignments. In this work, we delete links from GIZA++ union alignments to improve precision. The low precision of GIZA++ union alignments poses a particular problem for syntax-based rule extraction algorithms such as (Quirk et al., 2005; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006): if the incorrect links violate syntactic correspondences, they force the rule extraction algorithm to extract rules that are large in size, few in number, and poor in generalization ability. Figure 1 illustrates this problem: the dotted line represents an incorrect link in the GIZA++ union alignment. Using the rule extraction algorithm described in (Galley et al., 2004), we extract the rules shown in the leftmost column (R1&#8211;R4). Rule R1 is large and unlikely to generalize well. If we delete the incorrect link in Figure 1, we can extract the rules shown in the rightmost column (R2&#8211;R9): Rule R1, the largest rule from the initial set, disappears, and several smaller, more modular rules (R5&#8211;R9) replace it. In this work, we present a supervised algorithm that uses these two features of the extracted rules (size of largest rule and total number of rules), as well as a handful of structural and lexical features, to automatically identify and delete incorrect links from GIZA++ union alignments. We show that link
1 For a complete discussion of alignment symmetrization
heuristics, including union, intersection, and refined, refer to (Och and Ney, 2003).
its own country
&#184; &#181; &#65533;&#167; &#65533;&#65533;
FROM
x1&#65533;&#167;&#65533;&#65533;
OWN-COUNTRY NEEDS STARTS-OUT
&#8594;&#184;
Rules Extracted Using GIZA++ Union Alignments Rules Extracted After Deleting Dotted Link R1: VP &#8594; x0 R2: IN
VBZ
&#8594;&#184;
PRT PP from
starts RP x0:IN NP
out NP x1:PP
DT NNS
&#8594;&#181;
the needs
R2: IN R3: PP &#8594; x0
from
&#8594;&#181;
IN x0:NP
of
R3: PP &#8594; x0 R4: NP
IN x0:NP PRP JJ NN
of its own country
R4: NP R5: PP &#8594;x0 x1
PRP JJ NN
its own country x0:IN x1:NP
R6: NP
&#8594;&#65533;&#167;
x0:NP x1:PP
R7: NP
DT x0:NNS
the
R8: NNS
needs
R9: VP
&#8594; x1 x0
&#8594; x0
&#8594; x0&#65533;&#65533;
deletion improves alignment quality and translation quality in Chinese-English and Arabic-English MT, relative to a strong baseline. Our link deletion algorithm is easy to implement, runs quickly, and has been used by a top-scoring MT system in the Chinese newswire track of the 2008 NIST evaluation.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Word alignment typically constitutes the first stage of the statistical machine translation pipeline.</text>
                  <doc_id>4</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>GIZA++ (Och and Ney, 2003), an implementation of the IBM (Brown et al., 1993) and HMM (?</text>
                  <doc_id>5</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>) alignment models, is the most widely-used alignment system.</text>
                  <doc_id>6</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>GIZA++ union alignments have been used in the state-of-the-art syntax-based statistical MT system described in (Galley et al., 2006) and in the hierarchical phrase-based system Hiero (Chiang, 2007).</text>
                  <doc_id>7</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>GIZA++ refined alignments have been used in state-of-the-art phrase-based statistical MT systems such as (Och, 2004); variations on the refined heuristic have been used by (Koehn et al., 2003) (diag and diag-and) and by the phrase-based system Moses (grow-diag-final) (Koehn et al., 2007).</text>
                  <doc_id>8</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>GIZA++ union alignments have high recall but low precision, while intersection or refined alignments have high precision but low recall.</text>
                  <doc_id>9</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>1 There are two natural approaches to improving upon GIZA++ alignments, then: deleting links from union alignments, or adding links to intersection or refined alignments.</text>
                  <doc_id>10</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>In this work, we delete links from GIZA++ union alignments to improve precision.</text>
                  <doc_id>11</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>The low precision of GIZA++ union alignments poses a particular problem for syntax-based rule extraction algorithms such as (Quirk et al., 2005; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006): if the incorrect links violate syntactic correspondences, they force the rule extraction algorithm to extract rules that are large in size, few in number, and poor in generalization ability.</text>
                  <doc_id>12</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 1 illustrates this problem: the dotted line represents an incorrect link in the GIZA++ union alignment.</text>
                  <doc_id>13</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>Using the rule extraction algorithm described in (Galley et al., 2004), we extract the rules shown in the leftmost column (R1&#8211;R4).</text>
                  <doc_id>14</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>Rule R1 is large and unlikely to generalize well.</text>
                  <doc_id>15</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
                <sentence>
                  <text>If we delete the incorrect link in Figure 1, we can extract the rules shown in the rightmost column (R2&#8211;R9): Rule R1, the largest rule from the initial set, disappears, and several smaller, more modular rules (R5&#8211;R9) replace it.</text>
                  <doc_id>16</doc_id>
                  <sec_id>12</sec_id>
                </sentence>
                <sentence>
                  <text>In this work, we present a supervised algorithm that uses these two features of the extracted rules (size of largest rule and total number of rules), as well as a handful of structural and lexical features, to automatically identify and delete incorrect links from GIZA++ union alignments.</text>
                  <doc_id>17</doc_id>
                  <sec_id>13</sec_id>
                </sentence>
                <sentence>
                  <text>We show that link</text>
                  <doc_id>18</doc_id>
                  <sec_id>14</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 For a complete discussion of alignment symmetrization</text>
                  <doc_id>19</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>heuristics, including union, intersection, and refined, refer to (Och and Ney, 2003).</text>
                  <doc_id>20</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>its own country</text>
                  <doc_id>21</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#184; &#181; &#65533;&#167; &#65533;&#65533;</text>
                  <doc_id>22</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>FROM</text>
                  <doc_id>23</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>x1&#65533;&#167;&#65533;&#65533;</text>
                  <doc_id>24</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>OWN-COUNTRY NEEDS STARTS-OUT</text>
                  <doc_id>25</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8594;&#184;</text>
                  <doc_id>26</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Rules Extracted Using GIZA++ Union Alignments Rules Extracted After Deleting Dotted Link R1: VP &#8594; x0 R2: IN</text>
                  <doc_id>27</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>VBZ</text>
                  <doc_id>28</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8594;&#184;</text>
                  <doc_id>29</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>PRT PP from</text>
                  <doc_id>30</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>starts RP x0:IN NP</text>
                  <doc_id>31</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>out NP x1:PP</text>
                  <doc_id>32</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>DT NNS</text>
                  <doc_id>33</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8594;&#181;</text>
                  <doc_id>34</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the needs</text>
                  <doc_id>35</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R2: IN R3: PP &#8594; x0</text>
                  <doc_id>36</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>from</text>
                  <doc_id>37</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8594;&#181;</text>
                  <doc_id>38</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>IN x0:NP</text>
                  <doc_id>39</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>of</text>
                  <doc_id>40</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R3: PP &#8594; x0 R4: NP</text>
                  <doc_id>41</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>IN x0:NP PRP JJ NN</text>
                  <doc_id>42</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>of its own country</text>
                  <doc_id>43</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R4: NP R5: PP &#8594;x0 x1</text>
                  <doc_id>44</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>PRP JJ NN</text>
                  <doc_id>45</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>its own country x0:IN x1:NP</text>
                  <doc_id>46</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R6: NP</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8594;&#65533;&#167;</text>
                  <doc_id>48</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>x0:NP x1:PP</text>
                  <doc_id>49</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R7: NP</text>
                  <doc_id>50</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>DT x0:NNS</text>
                  <doc_id>51</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R8: NNS</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>needs</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>R9: VP</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8594; x1 x0</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8594; x0</text>
                  <doc_id>57</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8594; x0&#65533;&#65533;</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>deletion improves alignment quality and translation quality in Chinese-English and Arabic-English MT, relative to a strong baseline.</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Our link deletion algorithm is easy to implement, runs quickly, and has been used by a top-scoring MT system in the Chinese newswire track of the 2008 NIST evaluation.</text>
                  <doc_id>60</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>1.2 Related Work</title>
            <text>Recently, discriminative methods for alignment have rivaled the quality of IBM Model 4 alignments (Liu et al., 2005; Ittycheriah and Roukos, 2005; Taskar et al., 2005; Moore et al., 2006; Fraser and Marcu, 2007b). However, except for (Fraser and Marcu, 2007b), none of these advances in alignment quality has improved translation quality of a state-of-the-art system. We use a discriminatively trained model to identify and delete incorrect links, and demonstrate that these gains in alignment quality lead to gains in translation quality in a stateof-the-art syntax-based MT system. In contrast to the semi-supervised LEAF alignment algorithm of (Fraser and Marcu, 2007b), which requires 1,500- 2,000 CPU days per iteration to align 8.4M Chinese- English sentences (anonymous, p.c.), link deletion requires only 450 CPU hours to re-align such a corpus (after initial alignment by GIZA++, which requires 20-24 CPU days). Several recent works incorporate syntactic features into alignment. (May and Knight, 2007) use syntactic constraints to re-align a parallel corpus that has been aligned by GIZA++ as follows: they extract string-to-tree transducer rules from the corpus, the target parse trees, and the alignment; discard the initial alignment; use the extracted rules to construct a forest of possible string-to-tree derivations for each string/tree pair in the corpus; use EM to select the Viterbi derivation tree for each pair; and finally, induce a new alignment from the Viterbi derivations, using the re-aligned corpus to train a syntax-based MT system. (May and Knight, 2007) differs from our approach in two ways: first, the set of possible re-alignments they consider for each sentence pair is limited by the initial GIZA++ alignments seen over the training corpus, while we consider all alignments that can be reached by deleting links from the initial GIZA++ alignment for that sentence pair. Second, (May and Knight, 2007) use a time-intensive training algorithm to select the best re-alignment for each sentence pair, while we use a fast greedy search to determine which links to delete; in contrast to (May and Knight, 2007), who require 400 CPU hours to re-align 330k Chinese-English sentence pairs (anonymous, p.c), link deletion requires only 18 CPU hours to re-align such a corpus.
(Lopez and Resnik, 2005) and (Denero and Klein, 2007) modify the distortion model of the HMM alignment model (Vogel et al., 1996) to reflect tree distance rather than string distance; (Cherry and Lin, 2006) modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints. Similarly to these approaches, we use syntactic bracketing to constrain alignment, but our work extends beyond improving alignment quality to improve translation quality as well.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Recently, discriminative methods for alignment have rivaled the quality of IBM Model 4 alignments (Liu et al., 2005; Ittycheriah and Roukos, 2005; Taskar et al., 2005; Moore et al., 2006; Fraser and Marcu, 2007b).</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, except for (Fraser and Marcu, 2007b), none of these advances in alignment quality has improved translation quality of a state-of-the-art system.</text>
                  <doc_id>62</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We use a discriminatively trained model to identify and delete incorrect links, and demonstrate that these gains in alignment quality lead to gains in translation quality in a stateof-the-art syntax-based MT system.</text>
                  <doc_id>63</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast to the semi-supervised LEAF alignment algorithm of (Fraser and Marcu, 2007b), which requires 1,500- 2,000 CPU days per iteration to align 8.4M Chinese- English sentences (anonymous, p.c.</text>
                  <doc_id>64</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>), link deletion requires only 450 CPU hours to re-align such a corpus (after initial alignment by GIZA++, which requires 20-24 CPU days).</text>
                  <doc_id>65</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Several recent works incorporate syntactic features into alignment.</text>
                  <doc_id>66</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>(May and Knight, 2007) use syntactic constraints to re-align a parallel corpus that has been aligned by GIZA++ as follows: they extract string-to-tree transducer rules from the corpus, the target parse trees, and the alignment; discard the initial alignment; use the extracted rules to construct a forest of possible string-to-tree derivations for each string/tree pair in the corpus; use EM to select the Viterbi derivation tree for each pair; and finally, induce a new alignment from the Viterbi derivations, using the re-aligned corpus to train a syntax-based MT system.</text>
                  <doc_id>67</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>(May and Knight, 2007) differs from our approach in two ways: first, the set of possible re-alignments they consider for each sentence pair is limited by the initial GIZA++ alignments seen over the training corpus, while we consider all alignments that can be reached by deleting links from the initial GIZA++ alignment for that sentence pair.</text>
                  <doc_id>68</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>Second, (May and Knight, 2007) use a time-intensive training algorithm to select the best re-alignment for each sentence pair, while we use a fast greedy search to determine which links to delete; in contrast to (May and Knight, 2007), who require 400 CPU hours to re-align 330k Chinese-English sentence pairs (anonymous, p.c), link deletion requires only 18 CPU hours to re-align such a corpus.</text>
                  <doc_id>69</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(Lopez and Resnik, 2005) and (Denero and Klein, 2007) modify the distortion model of the HMM alignment model (Vogel et al., 1996) to reflect tree distance rather than string distance; (Cherry and Lin, 2006) modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints.</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Similarly to these approaches, we use syntactic bracketing to constrain alignment, but our work extends beyond improving alignment quality to improve translation quality as well.</text>
                  <doc_id>71</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>2</index>
        <title>2 Link Deletion</title>
        <text>We propose an algorithm to re-align a parallel bitext that has been aligned by GIZA++ (IBM Model 4), then symmetrized using the union heuristic. We then train a syntax-based translation system on the realigned bitext, and evaluate whether the re-aligned bitext yields a better translation model than a baseline system trained on the GIZA++ union aligned bitext.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We propose an algorithm to re-align a parallel bitext that has been aligned by GIZA++ (IBM Model 4), then symmetrized using the union heuristic.</text>
              <doc_id>72</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We then train a syntax-based translation system on the realigned bitext, and evaluate whether the re-aligned bitext yields a better translation model than a baseline system trained on the GIZA++ union aligned bitext.</text>
              <doc_id>73</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Link Deletion Algorithm</title>
            <text>Our algorithm for re-alignment proceeds as follows. We make a single pass over the corpus. For each sentence pair, we initialize the alignment A = A initial (the GIZA++ union alignment for that sentence pair). We represent the score of A as a weighted linear combination of features h i of the alignment A, the target parse tree parse(e) (a phrase-structure syntactic representation of e), and the source string f:
n&#8721; score(A) = &#955; i &#183; h i (A, parse(e), f)
i=0
We define a branch of links to be a contiguous 1- to-many alignment. 2 We define two alignments, A 2 In Figure 1, the 1-to-many alignment formed by {&#181;its,&#181;- own,&#181;-country} constitutes a branch, but the
1-to-many alignment formed by {&#65533;&#65533;-starts,&#65533;&#65533;-out,&#65533;&#65533;needs} does not.
and A &#8242; , to be neighbors if they differ only by the deletion of a link or branch of links. We consider all alignments A &#8242; in the neighborhood of A, greedily deleting the link l or branch of links b maximizing the score of the resulting alignment A &#8242; = A \ l or A &#8242; = A \ b. We delete links until no further increase in the score of A is possible. 3
In section 2.2 we describe the features h i , and in section 2.4 we describe how to set the weights &#955; i .</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Our algorithm for re-alignment proceeds as follows.</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We make a single pass over the corpus.</text>
                  <doc_id>75</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For each sentence pair, we initialize the alignment A = A initial (the GIZA++ union alignment for that sentence pair).</text>
                  <doc_id>76</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We represent the score of A as a weighted linear combination of features h i of the alignment A, the target parse tree parse(e) (a phrase-structure syntactic representation of e), and the source string f:</text>
                  <doc_id>77</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>n&#8721; score(A) = &#955; i &#183; h i (A, parse(e), f)</text>
                  <doc_id>78</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=0</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We define a branch of links to be a contiguous 1- to-many alignment.</text>
                  <doc_id>80</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>2 We define two alignments, A 2 In Figure 1, the 1-to-many alignment formed by {&#181;its,&#181;- own,&#181;-country} constitutes a branch, but the</text>
                  <doc_id>81</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1-to-many alignment formed by {&#65533;&#65533;-starts,&#65533;&#65533;-out,&#65533;&#65533;needs} does not.</text>
                  <doc_id>82</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and A &#8242; , to be neighbors if they differ only by the deletion of a link or branch of links.</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We consider all alignments A &#8242; in the neighborhood of A, greedily deleting the link l or branch of links b maximizing the score of the resulting alignment A &#8242; = A \ l or A &#8242; = A \ b.</text>
                  <doc_id>84</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We delete links until no further increase in the score of A is possible.</text>
                  <doc_id>85</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>3</text>
                  <doc_id>86</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In section 2.2 we describe the features h i , and in section 2.4 we describe how to set the weights &#955; i .</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Features</title>
            <text>2.2.1 Syntactic Features We use two features of the string-to-tree transducer rules extracted from A, parse(e), and f according to the rule extraction algorithm described in (Galley et al., 2004):
ruleCount: Total number of rules extracted from A, parse(e), and f. As Figure 1 illustrates, incorrect links violating syntactic brackets tend to decrease ruleCount; ruleCount increases from 4 to 8 after deleting the incorrect link.
sizeOfLargestRule: The size, measured in terms of internal nodes in the target parse tree, of the single largest rule extracted from A, parse(e), and f. In Figure 1, the largest rules in the leftmost and rightmost columns are R1 (with 9 internal nodes) and R9 (with 4 internal nodes), respectively.
2.2.2 Structural Features
1-to-many Links: Total number of links for which one word is aligned to multiple words, in either direction. In Figure 1, the links {&#65533;&#65533;-starts,&#65533;&#65533;out,&#65533;&#65533;-needs} represent a 1-to-many alignment.
1-to-many links appear more frequently in GIZA++ union alignments than in gold alignments, and are therefore good candidates for deletion. The category of 1-to-many links is further subdivided, depending on the degree of contiguity that the link exhibits with its neighbors. 4 Each link in a 1-to-many
3 While using a dynamic programming algorithm would
likely improve search efficiency and allow link deletion to find an optimal solution, in practice, the greedy search runs quickly and improves alignment quality. 4 (Deng and Byrne, 2005) observe that, in a manually aligned
Chinese-English corpus, 82% of the Chinese words that are
alignment can have 0, 1, or 2 neighbors, according to how many links are adjacent to it in the 1-to-many alignment:
zeroNeighbors: has 0 neighbors. In Figure 1, the link&#65533;&#65533;-needs
oneNeighbor: In Figure 1, the links&#65533;&#65533;-starts and&#65533;&#65533;-out each have 1 neighbor&#8211;namely, each other.
twoNeighbors: In Figure 1, in the 1-to-many alignment formed by {&#181;-its,&#181;-own,&#181;country}, the link&#181;-own has 2 neighbors,
namely&#181;-it and&#181;-country.
2.2.3 Lexical Features
highestLexProbRank: A link e i -f j is &#8220;maxprobable from e i to f j &#8221; if p(f j |e i ) &gt; p(f j &#8242;|e i ) for all alternative words f j &#8242; with which e i is aligned in A initial . In Figure 1, p(&#65533;&#167;|needs) &gt; p(&#65533; &#65533;|needs), so&#65533;&#167;-needs is max-probable for &#8220;needs&#8221;. The definition of &#8220;max-probable from f j to e i &#8221; is analogous, and a link is max-probable (nondirectionally) if it is max-probable in either direction. The value of highestLexProbRank is the total number of max-probable links. The conditional lexical probabilities p(e i |f j ) and p(f j |e i ) are estimated using frequencies of aligned word pairs in the highprecision GIZA++ intersection alignments for the training corpus.
2.2.4 History Features
In addition to the above syntactic, structural, and lexical features of A, we also incorporate two features of the link deletion history itself into Score(A):
linksDeleted: Total number of links deleted A initial thus far. At each iteration, either a link or a branch of links is deleted.
aligned to multiple English words are aligned to a contiguous block of English words; similarly, 88% of the English words that are aligned to multiple Chinese words are aligned to a contiguous block of Chinese words. Thus, if a Chinese word is correctly aligned to multiple English words, those English words are likely to be &#8220;neighbors&#8221; of each other, and if an English word is correctly aligned to multiple Chinese words, those Chinese words are likely to be &#8220;neighbors&#8221; of each other.
stepsTaken: Total number of iterations thus far in the search; at each iteration, either a link or a branch is deleted. This feature serves as a constant cost function per step taken during link deletion.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>2.2.1 Syntactic Features We use two features of the string-to-tree transducer rules extracted from A, parse(e), and f according to the rule extraction algorithm described in (Galley et al., 2004):</text>
                  <doc_id>88</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>ruleCount: Total number of rules extracted from A, parse(e), and f. As Figure 1 illustrates, incorrect links violating syntactic brackets tend to decrease ruleCount; ruleCount increases from 4 to 8 after deleting the incorrect link.</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>sizeOfLargestRule: The size, measured in terms of internal nodes in the target parse tree, of the single largest rule extracted from A, parse(e), and f. In Figure 1, the largest rules in the leftmost and rightmost columns are R1 (with 9 internal nodes) and R9 (with 4 internal nodes), respectively.</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.2.2 Structural Features</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1-to-many Links: Total number of links for which one word is aligned to multiple words, in either direction.</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In Figure 1, the links {&#65533;&#65533;-starts,&#65533;&#65533;out,&#65533;&#65533;-needs} represent a 1-to-many alignment.</text>
                  <doc_id>93</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1-to-many links appear more frequently in GIZA++ union alignments than in gold alignments, and are therefore good candidates for deletion.</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The category of 1-to-many links is further subdivided, depending on the degree of contiguity that the link exhibits with its neighbors.</text>
                  <doc_id>95</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>4 Each link in a 1-to-many</text>
                  <doc_id>96</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3 While using a dynamic programming algorithm would</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>likely improve search efficiency and allow link deletion to find an optimal solution, in practice, the greedy search runs quickly and improves alignment quality.</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>4 (Deng and Byrne, 2005) observe that, in a manually aligned</text>
                  <doc_id>99</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Chinese-English corpus, 82% of the Chinese words that are</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>alignment can have 0, 1, or 2 neighbors, according to how many links are adjacent to it in the 1-to-many alignment:</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>zeroNeighbors: has 0 neighbors.</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In Figure 1, the link&#65533;&#65533;-needs</text>
                  <doc_id>103</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>oneNeighbor: In Figure 1, the links&#65533;&#65533;-starts and&#65533;&#65533;-out each have 1 neighbor&#8211;namely, each other.</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>twoNeighbors: In Figure 1, in the 1-to-many alignment formed by {&#181;-its,&#181;-own,&#181;country}, the link&#181;-own has 2 neighbors,</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>namely&#181;-it and&#181;-country.</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.2.3 Lexical Features</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>highestLexProbRank: A link e i -f j is &#8220;maxprobable from e i to f j &#8221; if p(f j |e i ) &gt; p(f j &#8242;|e i ) for all alternative words f j &#8242; with which e i is aligned in A initial .</text>
                  <doc_id>108</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In Figure 1, p(&#65533;&#167;|needs) &gt; p(&#65533; &#65533;|needs), so&#65533;&#167;-needs is max-probable for &#8220;needs&#8221;.</text>
                  <doc_id>109</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The definition of &#8220;max-probable from f j to e i &#8221; is analogous, and a link is max-probable (nondirectionally) if it is max-probable in either direction.</text>
                  <doc_id>110</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The value of highestLexProbRank is the total number of max-probable links.</text>
                  <doc_id>111</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The conditional lexical probabilities p(e i |f j ) and p(f j |e i ) are estimated using frequencies of aligned word pairs in the highprecision GIZA++ intersection alignments for the training corpus.</text>
                  <doc_id>112</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.2.4 History Features</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In addition to the above syntactic, structural, and lexical features of A, we also incorporate two features of the link deletion history itself into Score(A):</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>linksDeleted: Total number of links deleted A initial thus far.</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>At each iteration, either a link or a branch of links is deleted.</text>
                  <doc_id>116</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>aligned to multiple English words are aligned to a contiguous block of English words; similarly, 88% of the English words that are aligned to multiple Chinese words are aligned to a contiguous block of Chinese words.</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Thus, if a Chinese word is correctly aligned to multiple English words, those English words are likely to be &#8220;neighbors&#8221; of each other, and if an English word is correctly aligned to multiple Chinese words, those Chinese words are likely to be &#8220;neighbors&#8221; of each other.</text>
                  <doc_id>118</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>stepsTaken: Total number of iterations thus far in the search; at each iteration, either a link or a branch is deleted.</text>
                  <doc_id>119</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This feature serves as a constant cost function per step taken during link deletion.</text>
                  <doc_id>120</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>2.3 Constraints</title>
            <text>Protecting Refined Links from Deletion: Since GIZA++ refined links have higher precision than union links 5 , we do not consider any GIZA++ refined links for deletion. 6
Stoplist: In our Chinese-English corpora, the 10 most common English words (excluding punctuation marks) include {a,in,to,of,and,the}, while the 10 most common Chinese words include {&#65533;,&#65533;,&#65533;,&#65533;,&#223;}. Of these, {a,the} and {&#65533;,&#223;} have no explicit translational equivalent in the other
language. These words are aligned with each other frequently (and erroneously) by GIZA++ union, but rarely in the gold standard. We delete all links in the set {a, an, the} &#215; {&#223;,&#65533;} from A initial as a preprocessing step. 7</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Protecting Refined Links from Deletion: Since GIZA++ refined links have higher precision than union links 5 , we do not consider any GIZA++ refined links for deletion.</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>6</text>
                  <doc_id>122</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Stoplist: In our Chinese-English corpora, the 10 most common English words (excluding punctuation marks) include {a,in,to,of,and,the}, while the 10 most common Chinese words include {&#65533;,&#65533;,&#65533;,&#65533;,&#223;}.</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Of these, {a,the} and {&#65533;,&#223;} have no explicit translational equivalent in the other</text>
                  <doc_id>124</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>language.</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These words are aligned with each other frequently (and erroneously) by GIZA++ union, but rarely in the gold standard.</text>
                  <doc_id>126</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We delete all links in the set {a, an, the} &#215; {&#223;,&#65533;} from A initial as a preprocessing step.</text>
                  <doc_id>127</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>7</text>
                  <doc_id>128</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>2.4 Perceptron Training</title>
            <text>We set the feature weights &#955; using a modified version of averaged perceptron learning with structured outputs (Collins, 2002). Following (Moore, 2005), we initialize the value of our expected most informative feature (ruleCount) to 1.0, and initialize all other feature weights to 0. During each pass over the discriminative training set, we &#8220;decode&#8221; each sentence pair by greedily deleting links from A initial in order to maximize the score of the resulting alignment using the current settings of &#955; (for details, refer to section 2.1).
5 On a 400-sentence-pair Chinese-English data set, GIZA++
union alignments have a precision of 77.32 while GIZA++ refined alignments have a precision of 85.26. 6 To see how GIZA++ refined alignments compare to
GIZA++ union alignments for syntax-based translation, we compare systems trained on each set of alignments for Chinese- English translation task A. Union alignments result in a test set BLEU score of 41.17, as compared to only 36.99 for refined. 7 The impact upon alignment f-measure of deleting these
stoplist links is small; on Chinese-English Data Set A, the f- measure of the baseline GIZA++ union alignments on the test set increases from 63.44 to 63.81 after deleting stoplist links, while the remaining increase in f-measure from 63.81 to 75.14 (shown in Table 3) is due to the link deletion algorithm itself.
We construct a set of candidate alignments A candidates for use in reranking as follows. Starting with A = A initial , we iteratively explore all alignments A &#8242; in the neighborhood of A, adding each neighbor to A candidates , then selecting the neighbor that maximizes Score(A &#8242; ). When it is no longer possible to increase Score(A) by deleting any links, link deletion concludes and returns the highest-scoring alignment, A 1-best.
In general, A gold /&#8712; A candidates ; following (Collins, 2000) and (Charniak and Johnson, 2005) for parse reranking and (Liang et al., 2006) for translation reranking, we define A oracle as alignment in A candidates that is most similar to A gold . 8 We update each feature weight &#955; i as follows: &#955; i = &#955; i + &#8722; h A 1-best
i . 9
h A oracle
i
Following (Moore, 2005), after each training pass, we average all the feature weight vectors seen during the pass, and decode the discriminative training set using the vector of averaged feature weights. When alignment quality stops increasing on the discriminative training set, perceptron training ends. 10 The weight vector returned by perceptron training is the average over the training set of all weight vectors seen during all iterations; averaging reduces overfitting on the training set (Collins, 2002).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We set the feature weights &#955; using a modified version of averaged perceptron learning with structured outputs (Collins, 2002).</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Following (Moore, 2005), we initialize the value of our expected most informative feature (ruleCount) to 1.0, and initialize all other feature weights to 0.</text>
                  <doc_id>130</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>During each pass over the discriminative training set, we &#8220;decode&#8221; each sentence pair by greedily deleting links from A initial in order to maximize the score of the resulting alignment using the current settings of &#955; (for details, refer to section 2.1).</text>
                  <doc_id>131</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5 On a 400-sentence-pair Chinese-English data set, GIZA++</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>union alignments have a precision of 77.32 while GIZA++ refined alignments have a precision of 85.26.</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>6 To see how GIZA++ refined alignments compare to</text>
                  <doc_id>134</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>GIZA++ union alignments for syntax-based translation, we compare systems trained on each set of alignments for Chinese- English translation task A. Union alignments result in a test set BLEU score of 41.17, as compared to only 36.99 for refined.</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>7 The impact upon alignment f-measure of deleting these</text>
                  <doc_id>136</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>stoplist links is small; on Chinese-English Data Set A, the f- measure of the baseline GIZA++ union alignments on the test set increases from 63.44 to 63.81 after deleting stoplist links, while the remaining increase in f-measure from 63.81 to 75.14 (shown in Table 3) is due to the link deletion algorithm itself.</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We construct a set of candidate alignments A candidates for use in reranking as follows.</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Starting with A = A initial , we iteratively explore all alignments A &#8242; in the neighborhood of A, adding each neighbor to A candidates , then selecting the neighbor that maximizes Score(A &#8242; ).</text>
                  <doc_id>139</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>When it is no longer possible to increase Score(A) by deleting any links, link deletion concludes and returns the highest-scoring alignment, A 1-best.</text>
                  <doc_id>140</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In general, A gold /&#8712; A candidates ; following (Collins, 2000) and (Charniak and Johnson, 2005) for parse reranking and (Liang et al., 2006) for translation reranking, we define A oracle as alignment in A candidates that is most similar to A gold .</text>
                  <doc_id>141</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>8 We update each feature weight &#955; i as follows: &#955; i = &#955; i + &#8722; h A 1-best</text>
                  <doc_id>142</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i .</text>
                  <doc_id>143</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>9</text>
                  <doc_id>144</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>h A oracle</text>
                  <doc_id>145</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Following (Moore, 2005), after each training pass, we average all the feature weight vectors seen during the pass, and decode the discriminative training set using the vector of averaged feature weights.</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>When alignment quality stops increasing on the discriminative training set, perceptron training ends.</text>
                  <doc_id>148</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>10 The weight vector returned by perceptron training is the average over the training set of all weight vectors seen during all iterations; averaging reduces overfitting on the training set (Collins, 2002).</text>
                  <doc_id>149</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Experimental Setup</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>150</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Data Sets</title>
            <text>We evaluate the effect of link deletion upon alignment quality and translation quality for two Chinese- English data sets, and one Arabic-English data set. Each data set consists of newswire, and contains a small subset of manually aligned sentence pairs. We divide the manually aligned subset into a training set (used to discriminatively set the feature weights for link deletion) and a test set (used to evaluate the impact of link deletion upon alignment quality). Table 1 lists the source and the size of the manually aligned training and test sets used for each alignment task.
8 We discuss alignment similarity metrics in detail in Section
3.2. 9 (Liang et al., 2006) report that, for translation reranking,
such local updates (towards the oracle) outperform bold updates (towards the gold standard). 10 We discuss alignment quality metrics in detail in Section
3.2.
Using the feature weights learned on the manually aligned training set, we then apply link deletion to the remainder (non-manually aligned) of each bilingual data set, and train a full syntax-based statistical MT system on these sentence pairs. After maximum BLEU tuning (Och, 2003a) on a held-out tuning set, we evaluate translation quality on a held-out test set. Table 2 lists the source and the size of the training, tuning, and test sets used for each translation task.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We evaluate the effect of link deletion upon alignment quality and translation quality for two Chinese- English data sets, and one Arabic-English data set.</text>
                  <doc_id>151</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Each data set consists of newswire, and contains a small subset of manually aligned sentence pairs.</text>
                  <doc_id>152</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We divide the manually aligned subset into a training set (used to discriminatively set the feature weights for link deletion) and a test set (used to evaluate the impact of link deletion upon alignment quality).</text>
                  <doc_id>153</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Table 1 lists the source and the size of the manually aligned training and test sets used for each alignment task.</text>
                  <doc_id>154</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>8 We discuss alignment similarity metrics in detail in Section</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3.2.</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>9 (Liang et al., 2006) report that, for translation reranking,</text>
                  <doc_id>157</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>such local updates (towards the oracle) outperform bold updates (towards the gold standard).</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>10 We discuss alignment quality metrics in detail in Section</text>
                  <doc_id>159</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3.2.</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Using the feature weights learned on the manually aligned training set, we then apply link deletion to the remainder (non-manually aligned) of each bilingual data set, and train a full syntax-based statistical MT system on these sentence pairs.</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>After maximum BLEU tuning (Och, 2003a) on a held-out tuning set, we evaluate translation quality on a held-out test set.</text>
                  <doc_id>162</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Table 2 lists the source and the size of the training, tuning, and test sets used for each translation task.</text>
                  <doc_id>163</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Evaluation Metrics</title>
            <text>AER (Alignment Error Rate) (Och and Ney, 2003) is the most widely used metric of alignment quality, but requires gold-standard alignments labelled with &#8220;sure/possible&#8221; annotations to compute; lacking such annotations, we can compute alignment f- measure instead. However, (Fraser and Marcu, 2007a) show that, in phrase-based translation, improvements in AER or f-measure do not necessarily correlate with improvements in BLEU score. They propose two modifications to f-measure: varying the precision/recall tradeoff, and fully-connecting the alignment links before computing f-measure. 11
Weighted Fully-Connected F-Measure Given a hypothesized set of alignment links H and a goldstandard set of alignment links G, we define H + = fullyConnect(H) and G + = fullyConnect(G), and then compute:
f-measure(H + ) = 1
&#945; precision(H + ) + 1&#8722;&#945; recall(H + )
For phrase-based Chinese-English and Arabic- English translation tasks, (Fraser and Marcu, 2007a) obtain the closest correlation between weighted fully-connected alignment f-measure and BLEU score using &#945;=0.5 and &#945;=0.1, respectively. We use weighted fully-connected alignment f-measure as the training criterion for link deletion, and to evaluate alignment quality on training and test sets.
Rule F-Measure To evaluate the impact of link deletion upon rule quality, we compare the rule precision, recall, and f-measure of the rule set extracted
11 In Figure 1, the fully-connected version of the alignments
shown would include the links&#65533;&#167;-starts and&#65533;&#167;- out.
from our hypothesized alignments and a Collinsstyle parser against the rule set extracted from gold alignments and gold parses.
BLEU For all translation tasks, we report caseinsensitive NIST BLEU scores (Papineni et al., 2002) using 4 references per sentence.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>AER (Alignment Error Rate) (Och and Ney, 2003) is the most widely used metric of alignment quality, but requires gold-standard alignments labelled with &#8220;sure/possible&#8221; annotations to compute; lacking such annotations, we can compute alignment f- measure instead.</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, (Fraser and Marcu, 2007a) show that, in phrase-based translation, improvements in AER or f-measure do not necessarily correlate with improvements in BLEU score.</text>
                  <doc_id>165</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>They propose two modifications to f-measure: varying the precision/recall tradeoff, and fully-connecting the alignment links before computing f-measure.</text>
                  <doc_id>166</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>11</text>
                  <doc_id>167</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Weighted Fully-Connected F-Measure Given a hypothesized set of alignment links H and a goldstandard set of alignment links G, we define H + = fullyConnect(H) and G + = fullyConnect(G), and then compute:</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>f-measure(H + ) = 1</text>
                  <doc_id>169</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#945; precision(H + ) + 1&#8722;&#945; recall(H + )</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For phrase-based Chinese-English and Arabic- English translation tasks, (Fraser and Marcu, 2007a) obtain the closest correlation between weighted fully-connected alignment f-measure and BLEU score using &#945;=0.5 and &#945;=0.1, respectively.</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We use weighted fully-connected alignment f-measure as the training criterion for link deletion, and to evaluate alignment quality on training and test sets.</text>
                  <doc_id>172</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Rule F-Measure To evaluate the impact of link deletion upon rule quality, we compare the rule precision, recall, and f-measure of the rule set extracted</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>11 In Figure 1, the fully-connected version of the alignments</text>
                  <doc_id>174</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>shown would include the links&#65533;&#167;-starts and&#65533;&#167;- out.</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>from our hypothesized alignments and a Collinsstyle parser against the rule set extracted from gold alignments and gold parses.</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BLEU For all translation tasks, we report caseinsensitive NIST BLEU scores (Papineni et al., 2002) using 4 references per sentence.</text>
                  <doc_id>177</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Experiments</title>
            <text>Starting with GIZA++ union (IBM Model 4) alignments, we use perceptron training to set the weights of each feature used in link deletion in order to optimize weighted fully-connected alignment f-measure (&#945;=0.5 for Chinese-English and &#945;=0.1 for Arabic- English) on a manually aligned discriminative training set. We report the (fully-connected) precision, recall, and weighted alignment f-measure on a heldout test set after running perceptron training, relative to the baseline GIZA++ union alignments. Using the learned feature weights, we then perform link deletion over the GIZA++ union alignments for the entire training corpus for each translation task. Using these alignments, which we refer to as &#8220;GIZA++ union + link deletion&#8221;, we train a syntax-based translation system similar to that described in (Galley et al., 2006). After extracting string-to-tree translation rules from the aligned, parsed training corpus, the system assigns weights to each rule via frequency estimation with smoothing. The rule probabilities, as well as trigram language model probabilities and a handful of additional features of each rule, are used as features during decoding. The feature weights are tuned using minimum error rate training (Och and Ney, 2003) to optimize BLEU score on a held-out development set. We then compare the BLEU score of this system against a baseline system trained using GIZA++ union alignments.
To determine which value of &#945; is most effective as a training criterion for link deletion, we set &#945;=0.4 (favoring recall), 0.5, and 0.6 (favoring precision),
and compare the effect on translation quality for Chinese-English data set A.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Starting with GIZA++ union (IBM Model 4) alignments, we use perceptron training to set the weights of each feature used in link deletion in order to optimize weighted fully-connected alignment f-measure (&#945;=0.5 for Chinese-English and &#945;=0.1 for Arabic- English) on a manually aligned discriminative training set.</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We report the (fully-connected) precision, recall, and weighted alignment f-measure on a heldout test set after running perceptron training, relative to the baseline GIZA++ union alignments.</text>
                  <doc_id>179</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Using the learned feature weights, we then perform link deletion over the GIZA++ union alignments for the entire training corpus for each translation task.</text>
                  <doc_id>180</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Using these alignments, which we refer to as &#8220;GIZA++ union + link deletion&#8221;, we train a syntax-based translation system similar to that described in (Galley et al., 2006).</text>
                  <doc_id>181</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>After extracting string-to-tree translation rules from the aligned, parsed training corpus, the system assigns weights to each rule via frequency estimation with smoothing.</text>
                  <doc_id>182</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The rule probabilities, as well as trigram language model probabilities and a handful of additional features of each rule, are used as features during decoding.</text>
                  <doc_id>183</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The feature weights are tuned using minimum error rate training (Och and Ney, 2003) to optimize BLEU score on a held-out development set.</text>
                  <doc_id>184</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>We then compare the BLEU score of this system against a baseline system trained using GIZA++ union alignments.</text>
                  <doc_id>185</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To determine which value of &#945; is most effective as a training criterion for link deletion, we set &#945;=0.4 (favoring recall), 0.5, and 0.6 (favoring precision),</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>and compare the effect on translation quality for Chinese-English data set A.</text>
                  <doc_id>187</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Results</title>
        <text>For each translation task, link deletion improves translation quality relative to a GIZA++ union baseline. For each alignment task, link deletion tends to improve fully-connected alignment precision more than it decreases fully-connected alignment recall, increasing weighted fully-connected alignment f- measure overall.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>For each translation task, link deletion improves translation quality relative to a GIZA++ union baseline.</text>
              <doc_id>188</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For each alignment task, link deletion tends to improve fully-connected alignment precision more than it decreases fully-connected alignment recall, increasing weighted fully-connected alignment f- measure overall.</text>
              <doc_id>189</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Chinese-English</title>
            <text>On Chinese-English translation task A, link deletion increases BLEU score by 1.26 points on tuning and 0.76 points on test (Table 3); on Chinese-English translation task B, link deletion increases BLEU score by 1.38 points on tuning and 0.49 points on test (Table 3).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>On Chinese-English translation task A, link deletion increases BLEU score by 1.26 points on tuning and 0.76 points on test (Table 3); on Chinese-English translation task B, link deletion increases BLEU score by 1.38 points on tuning and 0.49 points on test (Table 3).</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Arabic-English</title>
            <text>On the Arabic-English translation task, link deletion improves BLEU score by 0.84 points on tuning, 0.18 points on test1, and 0.56 points on test2 (Table 3). Note that the training criterion for Arabic- English link deletion uses &#945;=0.1; because this penalizes a loss in recall more heavily than it rewards an increase in precision, it is more difficult to increase weighted fully-connected alignment f- measure using link deletion for Arabic-English than for Chinese-English. This difference is reflected in the average number of links deleted per sentence: 4.19 for Chinese-English B (Table 3), but only 1.35 for Arabic-English (Table 3). Despite this difference, link deletion improves translation results for Arabic-English as well.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>On the Arabic-English translation task, link deletion improves BLEU score by 0.84 points on tuning, 0.18 points on test1, and 0.56 points on test2 (Table 3).</text>
                  <doc_id>191</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Note that the training criterion for Arabic- English link deletion uses &#945;=0.1; because this penalizes a loss in recall more heavily than it rewards an increase in precision, it is more difficult to increase weighted fully-connected alignment f- measure using link deletion for Arabic-English than for Chinese-English.</text>
                  <doc_id>192</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This difference is reflected in the average number of links deleted per sentence: 4.19 for Chinese-English B (Table 3), but only 1.35 for Arabic-English (Table 3).</text>
                  <doc_id>193</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Despite this difference, link deletion improves translation results for Arabic-English as well.</text>
                  <doc_id>194</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>4.3 Varying &#945;</title>
            <text>On Chinese-English data set A, we explore the effect of varying &#945; in the weighted fully-connected
Test Set Weighted Fully&#8722;Connected Alignment F&#8722;Measure 64
GIZA++ union GIZA++ union + link deletion
alignment f-measure used as the training criterion for link deletion. Using &#945;=0.5 leads to a higher gain in BLEU score on the test set relative to the baseline (+0.76 points) than either &#945;=0.4 (+0.70 points) or &#945;=0.6 (+0.67 points).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>On Chinese-English data set A, we explore the effect of varying &#945; in the weighted fully-connected</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Test Set Weighted Fully&#8722;Connected Alignment F&#8722;Measure 64</text>
                  <doc_id>196</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>GIZA++ union GIZA++ union + link deletion</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>alignment f-measure used as the training criterion for link deletion.</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Using &#945;=0.5 leads to a higher gain in BLEU score on the test set relative to the baseline (+0.76 points) than either &#945;=0.4 (+0.70 points) or &#945;=0.6 (+0.67 points).</text>
                  <doc_id>199</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>4.4 Size of Discriminative Training Set</title>
            <text>To examine how many manually aligned sentence pairs are required to set the feature weights reliably, we vary the size of the discriminative training set from 2-1500 sentence pairs while holding test set size constant at 1500 sentence pairs; run perceptron training; and record the resulting weighted fully-connected alignment f-measure on the test set. Figure 2 illustrates that using 100-200 manually aligned sentence pairs of training data is sufficient for Chinese-English; a similarly-sized training set is also sufficient for Arabic-English.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>To examine how many manually aligned sentence pairs are required to set the feature weights reliably, we vary the size of the discriminative training set from 2-1500 sentence pairs while holding test set size constant at 1500 sentence pairs; run perceptron training; and record the resulting weighted fully-connected alignment f-measure on the test set.</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 2 illustrates that using 100-200 manually aligned sentence pairs of training data is sufficient for Chinese-English; a similarly-sized training set is also sufficient for Arabic-English.</text>
                  <doc_id>201</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>4</index>
            <title>4.5 Effect of Link Deletion on Extracted Rules</title>
            <text>Link deletion increases the size of the extracted grammar. To determine how the quality of the extracted grammar changes, we compute the rule pre-
Links Del/ Grammar BLEU Language Alignment Prec Rec &#945; F-measure Sent Size Tune Test1 Test2 Chi-Eng A GIZA++ union 54.76 75.38 0.5 63.44 &#8211; 23.4M 41.80 41.17 &#8211;
Chi-Eng A GIZA++ union + 79.59 71.16 0.5 75.14 4.77 59.7M 43.06 41.93 &#8211; link deletion Chi-Eng B GIZA++ union 36.61 66.28 0.5 47.16 &#8211; 28.9M 39.59 41.39 &#8211;
Chi-Eng B GIZA++ union + 65.52 59.28 0.5 62.24 4.19 73.0M 40.97 41.88 &#8211; link deletion
cision, recall, and f-measure of the GIZA++ union alignments and various link deletion alignments on a held-out Chinese-English test set of 400 sentence pairs. Table 4 indicates the total (non-unique) number of rules extracted for each alignment/parse pairing, as well as the rule precision, recall, and f- measure of each pair. As more links are deleted, more rules are extracted&#8211;but of those, some are of good quality and others are of bad quality. Linkdeleted alignments produce rule sets with higher rule f-measure than either GIZA++ union or GIZA++ refined.
other language pairs with limited amounts (100-200 sentence pairs) of manually aligned data available.
Acknowledgments
We thank Steven DeNeefe and Wei Wang for assistance with experiments, and Alexander Fraser and Liang Huang for helpful discussions. This research was supported by DARPA (contract HR0011-06-C- 0022) and by a fellowship from AT&amp;T Labs.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Link deletion increases the size of the extracted grammar.</text>
                  <doc_id>202</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To determine how the quality of the extracted grammar changes, we compute the rule pre-</text>
                  <doc_id>203</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Links Del/ Grammar BLEU Language Alignment Prec Rec &#945; F-measure Sent Size Tune Test1 Test2 Chi-Eng A GIZA++ union 54.76 75.38 0.5 63.44 &#8211; 23.4M 41.80 41.17 &#8211;</text>
                  <doc_id>204</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Chi-Eng A GIZA++ union + 79.59 71.16 0.5 75.14 4.77 59.7M 43.06 41.93 &#8211; link deletion Chi-Eng B GIZA++ union 36.61 66.28 0.5 47.16 &#8211; 28.9M 39.59 41.39 &#8211;</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Chi-Eng B GIZA++ union + 65.52 59.28 0.5 62.24 4.19 73.0M 40.97 41.88 &#8211; link deletion</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>cision, recall, and f-measure of the GIZA++ union alignments and various link deletion alignments on a held-out Chinese-English test set of 400 sentence pairs.</text>
                  <doc_id>207</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Table 4 indicates the total (non-unique) number of rules extracted for each alignment/parse pairing, as well as the rule precision, recall, and f- measure of each pair.</text>
                  <doc_id>208</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As more links are deleted, more rules are extracted&#8211;but of those, some are of good quality and others are of bad quality.</text>
                  <doc_id>209</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Linkdeleted alignments produce rule sets with higher rule f-measure than either GIZA++ union or GIZA++ refined.</text>
                  <doc_id>210</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>other language pairs with limited amounts (100-200 sentence pairs) of manually aligned data available.</text>
                  <doc_id>211</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Acknowledgments</text>
                  <doc_id>212</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We thank Steven DeNeefe and Wei Wang for assistance with experiments, and Alexander Fraser and Liang Huang for helpful discussions.</text>
                  <doc_id>213</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This research was supported by DARPA (contract HR0011-06-C- 0022) and by a fellowship from AT&amp;T Labs.</text>
                  <doc_id>214</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Conclusion</title>
        <text>We have presented a link deletion algorithm that improves the precision of GIZA++ union alignments without notably decreasing recall. In addition to lexical and structural features, we use features of the extracted syntax-based translation rules. Our method improves alignment quality and translation quality on Chinese-English and Arabic-English translation tasks, relative to a GIZA++ union baseline. The algorithm runs quickly, and is easily applicable to</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have presented a link deletion algorithm that improves the precision of GIZA++ union alignments without notably decreasing recall.</text>
              <doc_id>215</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In addition to lexical and structural features, we use features of the extracted syntax-based translation rules.</text>
              <doc_id>216</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Our method improves alignment quality and translation quality on Chinese-English and Arabic-English translation tasks, relative to a GIZA++ union baseline.</text>
              <doc_id>217</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The algorithm runs quickly, and is easily applicable to</text>
              <doc_id>218</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Size (sentence pairs) of data sets used in alignment link deletion tasks</caption>
        <reference_text></reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>Language</cell>
              <cell>Train</cell>
              <cell>Test</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Chinese-English A</cell>
              <cell>400</cell>
              <cell>400</cell>
            </row>
            <row>
              <cell>Chinese-English B</cell>
              <cell>1500</cell>
              <cell>1500</cell>
            </row>
            <row>
              <cell>Arabic-English</cell>
              <cell>1500</cell>
              <cell>1500</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Size (English words) and source of data sets used in translation tasks</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Language</cell>
              <cell>Train</cell>
              <cell>Tune</cell>
              <cell>Test1</cell>
              <cell>Test2</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Chinese-English A</cell>
              <cell>9.8M/newswire</cell>
              <cell>25.9k/NIST02</cell>
              <cell>29.0k/NIST03</cell>
              <cell>&#8211;</cell>
            </row>
            <row>
              <cell>Chinese-English B</cell>
              <cell>12.3M/newswire</cell>
              <cell>42.9k/newswire</cell>
              <cell>42.1k/newswire</cell>
              <cell>&#8211;</cell>
            </row>
            <row>
              <cell>Arabic-English</cell>
              <cell>174.8M/newswire</cell>
              <cell>35.8k/NIST04-05</cell>
              <cell>40.3k/NIST04-05</cell>
              <cell>53.0k/newswire</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Results of link deletion. Weighted fully-connected alignment f-measure is computed on alignment test sets (Table 1); BLEU score is computed on translation test sets (Table 2).</caption>
        <reference_text>In PAGE 5: ...81 to 75.14 (shown in  Table3 ) is due to the link deletion algorithm itself. We construct a set of candidate alignments Acandidates for use in reranking as follows....  In PAGE 7: ...26 points on tuning and 0.76 points on test ( Table3 ); on Chinese-English translation task B, link deletion increases BLEU score by 1.38 points on tuning and 0....  In PAGE 7: ... This difference is reflected in the average number of links deleted per sentence: 4.19 for Chinese-English B ( Table3 ), but only 1.35 for Arabic-English (Table 3)....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>Language</cell>
              <cell>Alignment</cell>
              <cell>Prec</cell>
              <cell>Rec</cell>
              <cell>?</cell>
              <cell>F-measure</cell>
              <cell>Links Del/   Sent</cell>
              <cell>Grammar   Size</cell>
              <cell>Tune</cell>
              <cell>BLEU   Test1</cell>
              <cell>Test2</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Chi-Eng A</cell>
              <cell>GIZA++ union</cell>
              <cell>54.76</cell>
              <cell>75.38</cell>
              <cell>0.5</cell>
              <cell>63.44</cell>
              <cell>?</cell>
              <cell>23.4M</cell>
              <cell>41.80</cell>
              <cell>41.17</cell>
              <cell>?</cell>
            </row>
            <row>
              <cell>Chi-Eng A</cell>
              <cell>GIZA++ union +</cell>
              <cell>79.59</cell>
              <cell>71.16</cell>
              <cell>0.5</cell>
              <cell>75.14</cell>
              <cell>4.77</cell>
              <cell>59.7M</cell>
              <cell>43.06</cell>
              <cell>41.93</cell>
              <cell>?</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>link deletion</cell>
            </row>
            <row>
              <cell>Chi-Eng B</cell>
              <cell>GIZA++ union</cell>
              <cell>36.61</cell>
              <cell>66.28</cell>
              <cell>0.5</cell>
              <cell>47.16</cell>
              <cell>?</cell>
              <cell>28.9M</cell>
              <cell>39.59</cell>
              <cell>41.39</cell>
              <cell>?</cell>
            </row>
            <row>
              <cell>Chi-Eng B</cell>
              <cell>GIZA++ union +</cell>
              <cell>65.52</cell>
              <cell>59.28</cell>
              <cell>0.5</cell>
              <cell>62.24</cell>
              <cell>4.19</cell>
              <cell>73.0M</cell>
              <cell>40.97</cell>
              <cell>41.88</cell>
              <cell>?</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>link deletion</cell>
            </row>
            <row>
              <cell>Ara-Eng</cell>
              <cell>GIZA++ union</cell>
              <cell>35.34</cell>
              <cell>84.05</cell>
              <cell>0.1</cell>
              <cell>73.87</cell>
              <cell>?</cell>
              <cell>52.4M</cell>
              <cell>54.73</cell>
              <cell>50.9</cell>
              <cell>38.16</cell>
            </row>
            <row>
              <cell>Ara-Eng</cell>
              <cell>GIZA++ union +</cell>
              <cell>52.68</cell>
              <cell>79.75</cell>
              <cell>0.1</cell>
              <cell>75.85</cell>
              <cell>1.35</cell>
              <cell>64.9M</cell>
              <cell>55.57</cell>
              <cell>51.08</cell>
              <cell>38.72</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>link deletion</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TET</source>
        <caption>Table 4: Rule precision, recall, and f-measure of rules extracted from 400 sentence pairs of Chinese-English data</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>Alignment</cell>
              <cell>Parse</cell>
              <cell>Rule
Precision Recall F-measure Total Non-Unique</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>gold</cell>
              <cell>gold</cell>
              <cell>100.00</cell>
              <cell>100.00</cell>
              <cell>100.00</cell>
              <cell>12,809</cell>
            </row>
            <row>
              <cell>giza++ union</cell>
              <cell>collins</cell>
              <cell>50.49</cell>
              <cell>44.23</cell>
              <cell>47.15</cell>
              <cell>11,021</cell>
            </row>
            <row>
              <cell>giza++ union+link deletion, &#945;=0.5</cell>
              <cell>collins</cell>
              <cell>47.51</cell>
              <cell>53.20</cell>
              <cell>50.20</cell>
              <cell>13,987</cell>
            </row>
            <row>
              <cell>giza++ refined</cell>
              <cell>collins</cell>
              <cell>44.20</cell>
              <cell>54.06</cell>
              <cell>48.64</cell>
              <cell>15,182</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Computational Linguistics</author>
        </authors>
        <title>Eugene Charniak and Mark Johnson. Coarse-to-fine nbest parsing and MaxEnt discriminative reranking.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1993</date>
      </reference>
      <reference>
        <id>1</id>
        <authors/>
        <title>Colin Cherry and Dekang Lin. Soft Syntactic Constraints for Word Alignment through Discriminative Training.</title>
        <publication>Proceedings of ACL,</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>2</id>
        <authors/>
        <title>None</title>
        <publication>Proceedings of ACL (Poster),</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>A Hierarchical Phrase-Based Model for Statistical Machine Translation.</title>
        <publication>Proceedings of ACL,</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Computational Linguistics</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Michael Collins</author>
        </authors>
        <title>Discriminative Reranking for Natural Language Parsing.</title>
        <publication>Proceedings of ICML,</publication>
        <pages>None</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Michael Collins</author>
        </authors>
        <title>Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms.</title>
        <publication>Proceedings of EMNLP,</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>John DeNero</author>
          <author>Dan Klein</author>
        </authors>
        <title>Tailoring Word Alignments to Syntactic Machine Translation.</title>
        <publication>Proceedings of ACL, 2007. Yonggang Deng and</publication>
        <pages>None</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>8</id>
        <authors/>
        <title>Alexander Fraser and Daniel Marcu. Measuring Word Alignment Quality for Statistical Machine Translation.</title>
        <publication>Proceedings of HLT/EMNLP,</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Computational Linguistics</author>
        </authors>
        <title>None</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Alexander Fraser</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Getting the Structure Right for Word Alignment: LEAF.</title>
        <publication>Proceedings of EMNLP,</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Michel Galley</author>
          <author>Mark Hopkins</author>
          <author>Kevin Knight</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>What&#8217;s in a Translation Rule?</title>
        <publication>Proceedings of HLT/NAACL-04,</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Michel Galley</author>
          <author>Jonathan Graehl</author>
          <author>Kevin Knight</author>
          <author>Daniel Marcu</author>
          <author>Steve DeNeefe</author>
          <author>Wei Wang</author>
          <author>Ignacio Thayer</author>
        </authors>
        <title>Scalable Inference and Training of ContextRich Syntactic Translation Models.</title>
        <publication>Proceedings of ACL,</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Liang Huang</author>
          <author>Kevin Knight</author>
          <author>Aravind Joshi</author>
        </authors>
        <title>Statistical Syntax-Directed Translation with Extended Domain of Locality.</title>
        <publication>Proceedings of AMTA,</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz Josef Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>None</title>
        <publication>Proceedings of HLT/EMNLP,</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>15</id>
        <authors/>
        <title>Statistical Phrase-Based Translation.</title>
        <publication>Proceedings of HLT/NAACL,</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Hieu Hoang</author>
          <author>Alexandra Birch</author>
          <author>Chris Callison-Burch</author>
          <author>Marcello Federico</author>
          <author>Nicola Bertoldi</author>
          <author>Brooke Cowan</author>
          <author>Wade Shen</author>
        </authors>
        <title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
        <publication>Proceedings of ACL (demo),</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Percy Liang</author>
          <author>Alexandre Bouchard-Cote</author>
          <author>Dan Klein</author>
          <author>Ben Taskar</author>
        </authors>
        <title>An end-to-end discriminative approach to machine translation.</title>
        <publication>Proceedings of COLING/ACL,</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Yang Liu</author>
          <author>Qun Liu</author>
          <author>Shouxun Lin</author>
        </authors>
        <title>Log-linear Models for Word Alignment.</title>
        <publication>Proceedings of ACL,</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>19</id>
        <authors/>
        <title>Adam Lopez and Philip Resnik. Improved HMM Alignment Models for Languages with Scarce Resources.</title>
        <publication>Proceedings of ACL,</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>20</id>
        <authors/>
        <title>None</title>
        <publication>Proceedings of the ACL Workshop on Parallel Text,</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Jonathan May</author>
          <author>Kevin Knight</author>
        </authors>
        <title>Syntactic Re-Alignment Models for Machine Translation.</title>
        <publication>Proceedings of EMNLP-CoNLL,</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>Robert C Moore</author>
        </authors>
        <title>A Discriminative Framework for Bilingual Word Alignment.</title>
        <publication>Proceedings of HLT/EMNLP,</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Robert C Moore</author>
          <author>Wen-tau Yih</author>
          <author>Andreas Bode</author>
        </authors>
        <title>Improved discriminative bilingual word alignment.</title>
        <publication>Proceedings of ACL,</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum Error Rate Training in Statistical Machine Translation.</title>
        <publication>Proceedings of ACL, 2003. Franz Josef Och</publication>
        <pages>None</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>Computational Linguistics</author>
        </authors>
        <title>Franz Josef Och and Hermann Ney. The alignment template approach to statistical machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>Computational Linguistics</author>
        </authors>
        <title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>27</id>
        <authors/>
        <title>None</title>
        <publication>Proceedings of ACL,</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>28</id>
        <authors>
          <author>Chris Quirk</author>
          <author>Arul Menezes</author>
          <author>Colin Cherry</author>
        </authors>
        <title>Dependency Treelet Translation: Syntactically Informed Phrasal SMT.</title>
        <publication>Proceedings of ACL,</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>29</id>
        <authors>
          <author>Stephan Vogel</author>
          <author>Hermann Ney</author>
          <author>Christoph Tillmann</author>
        </authors>
        <title>None</title>
        <publication>Proceedings of HTL/EMNLP,</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>5</reference_id>
        <string>Collins, 2000</string>
        <sentence_id>40230</sentence_id>
        <char_offset>48</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>6</reference_id>
        <string>Collins, 2002</string>
        <sentence_id>40218</sentence_id>
        <char_offset>111</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>6</reference_id>
        <string>Collins, 2002</string>
        <sentence_id>40238</sentence_id>
        <char_offset>191</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>10</reference_id>
        <string>Fraser and Marcu, 2007</string>
        <sentence_id>40151</sentence_id>
        <char_offset>188</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>10</reference_id>
        <string>Fraser and Marcu, 2007</string>
        <sentence_id>40152</sentence_id>
        <char_offset>21</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>10</reference_id>
        <string>Fraser and Marcu, 2007</string>
        <sentence_id>40154</sentence_id>
        <char_offset>64</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>10</reference_id>
        <string>Fraser and Marcu, 2007</string>
        <sentence_id>40255</sentence_id>
        <char_offset>10</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>10</reference_id>
        <string>Fraser and Marcu, 2007</string>
        <sentence_id>40261</sentence_id>
        <char_offset>73</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>11</reference_id>
        <string>Galley et al., 2004</string>
        <sentence_id>40104</sentence_id>
        <char_offset>50</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>11</reference_id>
        <string>Galley et al., 2004</string>
        <sentence_id>40177</sentence_id>
        <char_offset>175</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>12</reference_id>
        <string>Galley et al., 2006</string>
        <sentence_id>40097</sentence_id>
        <char_offset>112</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>12</reference_id>
        <string>Galley et al., 2006</string>
        <sentence_id>40102</sentence_id>
        <char_offset>145</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>12</reference_id>
        <string>Galley et al., 2006</string>
        <sentence_id>40271</sentence_id>
        <char_offset>150</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>13</reference_id>
        <string>Huang et al., 2006</string>
        <sentence_id>40102</sentence_id>
        <char_offset>166</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>16</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>40098</sentence_id>
        <char_offset>269</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>17</reference_id>
        <string>Liang et al., 2006</string>
        <sentence_id>40230</sentence_id>
        <char_offset>121</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>17</reference_id>
        <string>Liang et al., 2006</string>
        <sentence_id>40247</sentence_id>
        <char_offset>3</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>18</reference_id>
        <string>Liu et al., 2005</string>
        <sentence_id>40151</sentence_id>
        <char_offset>99</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>21</reference_id>
        <string>May and Knight, 2007</string>
        <sentence_id>40157</sentence_id>
        <char_offset>1</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>21</reference_id>
        <string>May and Knight, 2007</string>
        <sentence_id>40158</sentence_id>
        <char_offset>1</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>21</reference_id>
        <string>May and Knight, 2007</string>
        <sentence_id>40159</sentence_id>
        <char_offset>9</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>21</reference_id>
        <string>May and Knight, 2007</string>
        <sentence_id>40159</sentence_id>
        <char_offset>213</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>22</reference_id>
        <string>Moore, 2005</string>
        <sentence_id>40219</sentence_id>
        <char_offset>11</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>22</reference_id>
        <string>Moore, 2005</string>
        <sentence_id>40236</sentence_id>
        <char_offset>11</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>23</reference_id>
        <string>Moore et al., 2006</string>
        <sentence_id>40151</sentence_id>
        <char_offset>168</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>28</reference_id>
        <string>Quirk et al., 2005</string>
        <sentence_id>40102</sentence_id>
        <char_offset>125</char_offset>
      </citation>
    </citations>
  </content>
</document>
