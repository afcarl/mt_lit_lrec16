<document>
  <filename>D09-1115</filename>
  <authors>
    <author>Yang Feng</author>
    <author>Yang Liu</author>
    <author>Haitao Mi</author>
    <author>Qun Liu</author>
  </authors>
  <title>Lattice-based System Combination for Statistical Machine Translation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>Current system combination methods usually use confusion networks to find consensus translations among different systems. Requiring one-to-one mappings between the words in candidate translations, confusion networks have difficulty in handling more general situations in which several words are connected to another several words. Instead, we propose a lattice-based system combination model that allows for such phrase alignments and uses lattices to encode all candidate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Current system combination methods usually use confusion networks to find consensus translations among different systems.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Requiring one-to-one mappings between the words in candidate translations, confusion networks have difficulty in handling more general situations in which several words are connected to another several words.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Instead, we propose a lattice-based system combination model that allows for such phrase alignments and uses lattices to encode all candidate translations.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score.
To construct a confusion network, one first need to choose one of the hypotheses (i.e., candidate translations) as the backbone (also called &#8220;skeleton&#8221; in the literature) and then decide the word alignments of other hypotheses to the backbone. Hypothesis alignment plays a crucial role in confusionnetwork-based system combination because it has a direct effect on selecting consensus translations.
However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more common that several words are connected to another several words. For example, &#8220;be capable of&#8221; and &#8220;be able to&#8221; have the same meaning. Although confusion-network-based approaches resort to inserting null words to alleviate this problem, they face the risk of producing degenerate translations such as &#8220;be capable to&#8221; and &#8220;be able of&#8221;.
In this paper, we propose a new system combination method based on lattices. As a more general form of confusion network, a lattice is capable of describing arbitrary mappings in hypothesis alignment. In a lattice, each edge is associated with a sequence of words rather than a single word. Therefore, we select phrases instead of words in each candidate set and minimize the chance to produce unexpected translations such as &#8220;be capable to&#8221;. We compared our approach with the state-of-the-art confusion-network-based system (He et al., 2008) and achieved a significant absolute improvement of 1.23 BLEU points on the NIST 2005 Chinese-to- English test set and 0.93 BLEU point on the NIST 2008 Chinese-to-English test set.
He
feels like apples
He prefer apples
He
feels like apples
He is fond of apples
(a) unidirectional alignments
He
feels like apples
He prefer apples
He
feels like apples
He is fond of apples
(b) bidirectional alignments
is fond (c) confusion network
is fond of (d) lattice</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>System combination aims to find consensus translations among different machine translation systems.</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994).</text>
              <doc_id>5</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.</text>
              <doc_id>6</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>A confusion network consists of a sequence of sets of candidate words.</text>
              <doc_id>7</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Each candidate word is associated with a score.</text>
              <doc_id>8</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score.</text>
              <doc_id>9</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>To construct a confusion network, one first need to choose one of the hypotheses (i.e., candidate translations) as the backbone (also called &#8220;skeleton&#8221; in the literature) and then decide the word alignments of other hypotheses to the backbone.</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Hypothesis alignment plays a crucial role in confusionnetwork-based system combination because it has a direct effect on selecting consensus translations.</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment.</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This is not the fact even for word alignments between the same languages.</text>
              <doc_id>13</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>It is more common that several words are connected to another several words.</text>
              <doc_id>14</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>For example, &#8220;be capable of&#8221; and &#8220;be able to&#8221; have the same meaning.</text>
              <doc_id>15</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Although confusion-network-based approaches resort to inserting null words to alleviate this problem, they face the risk of producing degenerate translations such as &#8220;be capable to&#8221; and &#8220;be able of&#8221;.</text>
              <doc_id>16</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this paper, we propose a new system combination method based on lattices.</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As a more general form of confusion network, a lattice is capable of describing arbitrary mappings in hypothesis alignment.</text>
              <doc_id>18</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In a lattice, each edge is associated with a sequence of words rather than a single word.</text>
              <doc_id>19</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, we select phrases instead of words in each candidate set and minimize the chance to produce unexpected translations such as &#8220;be capable to&#8221;.</text>
              <doc_id>20</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We compared our approach with the state-of-the-art confusion-network-based system (He et al., 2008) and achieved a significant absolute improvement of 1.23 BLEU points on the NIST 2005 Chinese-to- English test set and 0.93 BLEU point on the NIST 2008 Chinese-to-English test set.</text>
              <doc_id>21</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>He</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>feels like apples</text>
              <doc_id>23</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>He prefer apples</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>He</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>feels like apples</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>He is fond of apples</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(a) unidirectional alignments</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>He</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>feels like apples</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>He prefer apples</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>He</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>feels like apples</text>
              <doc_id>33</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>He is fond of apples</text>
              <doc_id>34</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(b) bidirectional alignments</text>
              <doc_id>35</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>is fond (c) confusion network</text>
              <doc_id>36</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>is fond of (d) lattice</text>
              <doc_id>37</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Background</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Confusion Network and Lattice</title>
            <text>We use an example shown in Figure 1 to illustrate our idea. Suppose that there are three hypotheses:
We choose the first sentence as the backbone. Then, we perform hypothesis alignment to build a confusion network, as shown in Figure 1(a). Note that although &#8220;feels like&#8221; has the same meaning with &#8220;is fond of&#8221;, a confusion network only allows for one-to-one mappings. In the confusion network shown in Figure 1(c), several null words &#949; are inserted to ensure that each hypothesis has the same length. As each edge in the confusion network only has a single word, it is possible to produce inappropriate translations such as &#8220;He is like of apples&#8221;. In contrast, we allow many-to-many mappings in the hypothesis alignment shown in Figure 2(b). For example, &#8220;like&#8221; is aligned to three words: &#8220;is&#8221;, &#8220;fond&#8221;, and &#8220;of&#8221;. Then, we use a lattice shown in Figure 1(d) to represent all possible candidate translations. Note that the phrase &#8220;is fond of&#8221; is attached to an edge. Now, it is unlikely to obtain a translation like &#8220;He is like of apples&#8221;.
A lattice G = &#12296;V, E&#12297; is a directed acyclic graph, formally a weighted finite state automation (FSA), where V is the set of nodes and E is the set of edges. The nodes in a lattice are usually labeled according to an appropriate numbering to reflect how to produce a translation. Each edge in a lattice is attached with a sequence of words as well as the associated probability.
As lattice is a more general form of confusion network (Dyer et al., 2008), we expect that replacing confusion networks with lattices will further improve system combination.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We use an example shown in Figure 1 to illustrate our idea.</text>
                  <doc_id>39</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Suppose that there are three hypotheses:</text>
                  <doc_id>40</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We choose the first sentence as the backbone.</text>
                  <doc_id>41</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Then, we perform hypothesis alignment to build a confusion network, as shown in Figure 1(a).</text>
                  <doc_id>42</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Note that although &#8220;feels like&#8221; has the same meaning with &#8220;is fond of&#8221;, a confusion network only allows for one-to-one mappings.</text>
                  <doc_id>43</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In the confusion network shown in Figure 1(c), several null words &#949; are inserted to ensure that each hypothesis has the same length.</text>
                  <doc_id>44</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>As each edge in the confusion network only has a single word, it is possible to produce inappropriate translations such as &#8220;He is like of apples&#8221;.</text>
                  <doc_id>45</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast, we allow many-to-many mappings in the hypothesis alignment shown in Figure 2(b).</text>
                  <doc_id>46</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>For example, &#8220;like&#8221; is aligned to three words: &#8220;is&#8221;, &#8220;fond&#8221;, and &#8220;of&#8221;.</text>
                  <doc_id>47</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Then, we use a lattice shown in Figure 1(d) to represent all possible candidate translations.</text>
                  <doc_id>48</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>Note that the phrase &#8220;is fond of&#8221; is attached to an edge.</text>
                  <doc_id>49</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>Now, it is unlikely to obtain a translation like &#8220;He is like of apples&#8221;.</text>
                  <doc_id>50</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A lattice G = &#12296;V, E&#12297; is a directed acyclic graph, formally a weighted finite state automation (FSA), where V is the set of nodes and E is the set of edges.</text>
                  <doc_id>51</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The nodes in a lattice are usually labeled according to an appropriate numbering to reflect how to produce a translation.</text>
                  <doc_id>52</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Each edge in a lattice is attached with a sequence of words as well as the associated probability.</text>
                  <doc_id>53</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As lattice is a more general form of confusion network (Dyer et al., 2008), we expect that replacing confusion networks with lattices will further improve system combination.</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 IHMM-based Alignment Method</title>
            <text>Since the candidate hypotheses are aligned using Indirect-HMM-based (IHMM-based) alignment method (He et al., 2008) in both direction, we briefly review the IHMM-based alignment method first. Take the direction that the hypothesis is aligned to the backbone as an example. The conditional probability that the hypothesis is generated by the backbone is given by
He feels like apples He prefer apples He is fond of apples p(e &#8242; J
1 |e I 1 ) = &#8721; aJ 1
J&#8719; [p(a j |a j&#8722;1 , I)p(e &#8242; j|e aj )]l (1)
j=1
Where e I 1 = (e 1, ..., e I ) is the backbone, e &#8242; J
1 =
(e &#8242; 1, ..., e &#8242; J) is a hypothesis aligned to e I 1 , and aJ 1 = (a 1 , .., a J ) is the alignment that specifies the position of backbone word that each hypothesis word is aligned to.
The translation probability p(e &#8242; j |e i) is a linear interpolation of semantic similarity p sem (e &#8242; j |e i) and surface similarity p sur (e &#8242; j |e i) and &#945; is the interpolation factor:
p(e &#8242; j |e i) = &#945;&#183;p sem (e &#8242; j |e i)+(1&#8722;&#945;)&#183;p sur (e &#8242; j |e i) (2)
The semantic similarity model is derived by using the source word sequence as a hidden layer, so the bilingual dictionary is necessary. The semantic sim-
ilarity model is given by
p sem (e &#8242; j |e i) =
&#8776;
K&#8721; p(f k |e i )p(e &#8242; j |f k, e i )
k=0
K&#8721; p(f k |e i )p(e &#8242; j|f k )
k=0
(3)
The surface similarity model is estimated by calculating the literal matching rate:
p sur (e &#8242; j|e i ) = exp{&#961; &#183; [s(e &#8242; j, e i ) &#8722; 1]} (4)
where s(e &#8242; j, e i ) is given by
s(e &#8242; j , e i) = M(e&#8242; j , e i) max(|e &#8242; j |, |e i|) (5)
where M(e &#8242; j , e i) is the length of the longest matched prefix (LMP) and &#961; is a smoothing factor that specifies the mapping.
The distortion probability p(a j = i|a j&#8722;1 = i &#8242; , I) is estimated by only considering the jump distance:
p(i|i &#8242; , I) =
c(i &#8722; i &#8242; ) &#8721; I
i=1 c(l &#8722; i&#8242; )
(6)
The distortion parameters c(d) are grouped into 11 buckets, c(&#8804; &#8722;4), c(&#8722;3), ..., c(0), ..., c(5), c(&#8805; 6). Since the alignments are in the same language, the distortion model favor monotonic alignments and penalize non-monotonic alignments. It is given in a intuitive way
c(d) = (1 + |d &#8722; 1|) &#8722;K , d = &#8722;4, ..., 6 (7)
where K is tuned on held-out data.
Also the probability p 0 of jumping to a null word state is tuned on held-out data. So the overall distortion model becomes {
p(i|i &#8242; p 0 if i = null state , I) = (1 &#8722; p 0 ) &#183; p(i|i &#8242; , I) otherwise</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Since the candidate hypotheses are aligned using Indirect-HMM-based (IHMM-based) alignment method (He et al., 2008) in both direction, we briefly review the IHMM-based alignment method first.</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Take the direction that the hypothesis is aligned to the backbone as an example.</text>
                  <doc_id>56</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The conditional probability that the hypothesis is generated by the backbone is given by</text>
                  <doc_id>57</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>He feels like apples He prefer apples He is fond of apples p(e &#8242; J</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 |e I 1 ) = &#8721; aJ 1</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>J&#8719; [p(a j |a j&#8722;1 , I)p(e &#8242; j|e aj )]l (1)</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=1</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Where e I 1 = (e 1, ..., e I ) is the backbone, e &#8242; J</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 =</text>
                  <doc_id>63</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(e &#8242; 1, ..., e &#8242; J) is a hypothesis aligned to e I 1 , and aJ 1 = (a 1 , .., a J ) is the alignment that specifies the position of backbone word that each hypothesis word is aligned to.</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The translation probability p(e &#8242; j |e i) is a linear interpolation of semantic similarity p sem (e &#8242; j |e i) and surface similarity p sur (e &#8242; j |e i) and &#945; is the interpolation factor:</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(e &#8242; j |e i) = &#945;&#183;p sem (e &#8242; j |e i)+(1&#8722;&#945;)&#183;p sur (e &#8242; j |e i) (2)</text>
                  <doc_id>66</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The semantic similarity model is derived by using the source word sequence as a hidden layer, so the bilingual dictionary is necessary.</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The semantic sim-</text>
                  <doc_id>68</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>ilarity model is given by</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p sem (e &#8242; j |e i) =</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8776;</text>
                  <doc_id>71</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>K&#8721; p(f k |e i )p(e &#8242; j |f k, e i )</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k=0</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>K&#8721; p(f k |e i )p(e &#8242; j|f k )</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k=0</text>
                  <doc_id>75</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(3)</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The surface similarity model is estimated by calculating the literal matching rate:</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p sur (e &#8242; j|e i ) = exp{&#961; &#183; [s(e &#8242; j, e i ) &#8722; 1]} (4)</text>
                  <doc_id>78</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where s(e &#8242; j, e i ) is given by</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>s(e &#8242; j , e i) = M(e&#8242; j , e i) max(|e &#8242; j |, |e i|) (5)</text>
                  <doc_id>80</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where M(e &#8242; j , e i) is the length of the longest matched prefix (LMP) and &#961; is a smoothing factor that specifies the mapping.</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The distortion probability p(a j = i|a j&#8722;1 = i &#8242; , I) is estimated by only considering the jump distance:</text>
                  <doc_id>82</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(i|i &#8242; , I) =</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c(i &#8722; i &#8242; ) &#8721; I</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1 c(l &#8722; i&#8242; )</text>
                  <doc_id>85</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(6)</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The distortion parameters c(d) are grouped into 11 buckets, c(&#8804; &#8722;4), c(&#8722;3), ..., c(0), ..., c(5), c(&#8805; 6).</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Since the alignments are in the same language, the distortion model favor monotonic alignments and penalize non-monotonic alignments.</text>
                  <doc_id>88</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It is given in a intuitive way</text>
                  <doc_id>89</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c(d) = (1 + |d &#8722; 1|) &#8722;K , d = &#8722;4, ..., 6 (7)</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where K is tuned on held-out data.</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Also the probability p 0 of jumping to a null word state is tuned on held-out data.</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>So the overall distortion model becomes {</text>
                  <doc_id>93</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>p(i|i &#8242; p 0 if i = null state , I) = (1 &#8722; p 0 ) &#183; p(i|i &#8242; , I) otherwise</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Lattice-based System Combination Model</title>
        <text>Lattice-based system combination involves the following steps:
(1) Collect the hypotheses from the candidate systems.
(2) Choose the backbone from the hypotheses. This is performed using a sentence-level Minimum Bayes Risk (MBR) method. The hypothesis with the minimum cost of edits against all hypotheses is selected. The backbone is significant for it influences not only the word order, but also the following alignments. The backbone is selected as follows:
&#8721; T ER(E &#8242; , E) (8)
E&#8712;E
(3) Get the alignments of the backbone and hypothesis pairs. First, each pair is aligned in both directions using the IHMM-based alignment method. In the IHMM alignment model, bilingual dictionaries in both directions are indispensable. Then, we apply a grow-diag-final algorithm which is widely used in bilingual phrase extraction (Koehn et al., 2003) to monolingual alignments. The bidirectional alignments are combined to one resorting to the grow-diag-final algorithm, allowing n-to-n mappings.
(4)Normalize the alignment pairs. The word order of the backbone determines the word order of consensus outputs, so the word order of hypotheses must be consistent with that of the backbone. All words of a hypotheses are reordered according to the alignment to the backbone. For a word aligned to null, an actual null word may be inserted to the proper position. The alignment units are extracted first and then the hypothesis words in each unit are shifted as a whole.
(5) Construct the lattice in the light of phrase pairs extracted on the normalized alignment pairs. The expression ability of the lattice depends on the phrase pairs.
(6) Decode the lattice using a model similar to the log-linear model.
The confusion-network-based system combination model goes in a similar way. The first two steps are the same as the lattice-based model. The difference is that the hypothesis pairs are aligned just in one direction due to the expression limit of the confusion network. As a result, the normalized alignments only contain 1-to-1 mappings (Actual null words are also needed in the case of null alignment). In the following, we will give more details about the steps which are different in the two models.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Lattice-based system combination involves the following steps:</text>
              <doc_id>95</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(1) Collect the hypotheses from the candidate systems.</text>
              <doc_id>96</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(2) Choose the backbone from the hypotheses.</text>
              <doc_id>97</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This is performed using a sentence-level Minimum Bayes Risk (MBR) method.</text>
              <doc_id>98</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The hypothesis with the minimum cost of edits against all hypotheses is selected.</text>
              <doc_id>99</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The backbone is significant for it influences not only the word order, but also the following alignments.</text>
              <doc_id>100</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The backbone is selected as follows:</text>
              <doc_id>101</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8721; T ER(E &#8242; , E) (8)</text>
              <doc_id>102</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>E&#8712;E</text>
              <doc_id>103</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(3) Get the alignments of the backbone and hypothesis pairs.</text>
              <doc_id>104</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>First, each pair is aligned in both directions using the IHMM-based alignment method.</text>
              <doc_id>105</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In the IHMM alignment model, bilingual dictionaries in both directions are indispensable.</text>
              <doc_id>106</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Then, we apply a grow-diag-final algorithm which is widely used in bilingual phrase extraction (Koehn et al., 2003) to monolingual alignments.</text>
              <doc_id>107</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The bidirectional alignments are combined to one resorting to the grow-diag-final algorithm, allowing n-to-n mappings.</text>
              <doc_id>108</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(4)Normalize the alignment pairs.</text>
              <doc_id>109</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The word order of the backbone determines the word order of consensus outputs, so the word order of hypotheses must be consistent with that of the backbone.</text>
              <doc_id>110</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>All words of a hypotheses are reordered according to the alignment to the backbone.</text>
              <doc_id>111</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>For a word aligned to null, an actual null word may be inserted to the proper position.</text>
              <doc_id>112</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The alignment units are extracted first and then the hypothesis words in each unit are shifted as a whole.</text>
              <doc_id>113</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(5) Construct the lattice in the light of phrase pairs extracted on the normalized alignment pairs.</text>
              <doc_id>114</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The expression ability of the lattice depends on the phrase pairs.</text>
              <doc_id>115</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(6) Decode the lattice using a model similar to the log-linear model.</text>
              <doc_id>116</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The confusion-network-based system combination model goes in a similar way.</text>
              <doc_id>117</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The first two steps are the same as the lattice-based model.</text>
              <doc_id>118</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The difference is that the hypothesis pairs are aligned just in one direction due to the expression limit of the confusion network.</text>
              <doc_id>119</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As a result, the normalized alignments only contain 1-to-1 mappings (Actual null words are also needed in the case of null alignment).</text>
              <doc_id>120</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In the following, we will give more details about the steps which are different in the two models.</text>
              <doc_id>121</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Lattice Construction</title>
        <text>Unlike a confusion network that operates words only, a lattice allows for phrase pairs. So phrase pairs must be extracted before constructing a lattice. A major difficulty in extracting phrase pairs is that the word order of hypotheses is not consistent with that of the backbone. As a result, hypothesis words belonging to a phrase pair may be discontinuous. Before phrase pairs are extracted, the hypothesis words should be normalized to make sure the words in a phrase pair is continuous. We call a phrase pair before normalization a alignment unit. The problem mentioned above is shown in Figure 2. In Figure 2 (a), although (e &#8242; 1 e&#8242; 3 , e 2) should be a phrase pair, but&#187;e &#8242; 1&#188;and&#187;e &#8242; 3&#188;are discontinuous, so the phrase pair can not be extracted. Only after the words of the hypothesis are reordered according to the corresponding words in the backbone as shown in Figure 2 (b),&#187;e&#8242; 1&#188;and&#187;e &#8242; 3&#188;become continuous and the phrase pair (e &#8242; 1 e&#8242; 3 , e 2) can be extracted. The procedure of reordering is called alignment normalization
E h : e &#8242; 1 e &#8242; 2 e &#8242; 3
E B : e 1 e 2 e 3 (a)
E h : e &#8242; 2 e &#8242; 1 e &#8242; 3</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Unlike a confusion network that operates words only, a lattice allows for phrase pairs.</text>
              <doc_id>122</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>So phrase pairs must be extracted before constructing a lattice.</text>
              <doc_id>123</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A major difficulty in extracting phrase pairs is that the word order of hypotheses is not consistent with that of the backbone.</text>
              <doc_id>124</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As a result, hypothesis words belonging to a phrase pair may be discontinuous.</text>
              <doc_id>125</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Before phrase pairs are extracted, the hypothesis words should be normalized to make sure the words in a phrase pair is continuous.</text>
              <doc_id>126</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We call a phrase pair before normalization a alignment unit.</text>
              <doc_id>127</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The problem mentioned above is shown in Figure 2.</text>
              <doc_id>128</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Figure 2 (a), although (e &#8242; 1 e&#8242; 3 , e 2) should be a phrase pair, but&#187;e &#8242; 1&#188;and&#187;e &#8242; 3&#188;are discontinuous, so the phrase pair can not be extracted.</text>
              <doc_id>129</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Only after the words of the hypothesis are reordered according to the corresponding words in the backbone as shown in Figure 2 (b),&#187;e&#8242; 1&#188;and&#187;e &#8242; 3&#188;become continuous and the phrase pair (e &#8242; 1 e&#8242; 3 , e 2) can be extracted.</text>
              <doc_id>130</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>The procedure of reordering is called alignment normalization</text>
              <doc_id>131</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>E h : e &#8242; 1 e &#8242; 2 e &#8242; 3</text>
              <doc_id>132</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>E B : e 1 e 2 e 3 (a)</text>
              <doc_id>133</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>E h : e &#8242; 2 e &#8242; 1 e &#8242; 3</text>
              <doc_id>134</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Alignment Normalization</title>
            <text>After the final alignments are generated in the growdiag-final algorithm, minimum alignment units are extracted. The hypothesis words of an alignment unit are packed as a whole in shift operations.
See the example in Figure 2 (a) first. All minimum alignment units are as follows: (e &#8242; 2 , e 1), (e &#8242; 1 e&#8242; 3 , e 2 ) and (&#949;, e 3 ). (e &#8242; 1 e&#8242; 2 e&#8242; 3 , e 1e 2 ) is an alignment unit, but not a minimum alignment unit.
Let &#257; i = (&#275; &#8242; i , &#275; i) denote a minimum alignment unit, and assume that the word string &#275; &#8242; i covers words e &#8242; i 1
,..., e &#8242; i m on the hypothesis side, and the word string &#275; i covers the consecutive words e i1 ,..., e in on the backbone side. In an alignment unit, the word string on the hypothesis side can be discontinuous. The minimum unit &#257; i = (&#275; &#8242; i , &#275; i) must observe the following rules:
E B : e 1 e 2 e 3
E h : e &#8242; 1 e &#8242; 2 (a)
E B : e 1 e 2
E h : e &#8242; 1 e &#8242; 2 e &#8242; 3
E B : e 1 e 2
E h : e &#8242; 1 e &#8242; 2 e &#8242; 3
(b)
(c)
e &#8242; 2
&#949; e &#8242; 1 e 1 e 2 e 3
e &#8242; 1 e&#8242; 2 e&#8242; 3 e 1 e 2
e &#8242; 1e &#8242; 3
e &#8242; 1 e &#8242; 2 e &#8242; 3 e 1 &#949; e 2
&#8226; &#8704; e &#8242; i k &#8712; &#275; &#8242; i , e a &#8242; i k &#8712; &#275; i
&#8226; &#8704; e ik &#8712; &#275; i , e &#8242; a ik = null or e &#8242; a ik &#8712; &#275; &#8242; i
&#8226; &#8708; &#257; j = (&#275; &#8242; j , &#275; j), &#275; j = e i1 , ..., e ik or &#275; j = e ik , ..., e in , k &#8712; [1, n]
Where a &#8242; i k denotes the position of the word in the backbone that e &#8242; i k is aligned to, and a ik denotes the position of the word in the hypothesis that e ik is aligned to.
An actual null word may be inserted to a proper position if a word, either from the hypothesis or from the backbone, is aligned to null. In this way, the minimum alignment set is extended to an alignment unit set, which includes not only minimum alignment units but also alignment units which are generated by adding null words to minimum alignment units. In general, the following three conditions should be taken into consideration:
&#8226; A backbone word is aligned to null. A null word is inserted to the hypothesis as shown in Figure 3 (a).
&#8226; A hypothesis word is aligned to null and it is between the span of a minimum alignment unit. A new alignment unit is generated by inserting the hypothesis word aligned to null to the minimum alignment unit. The new hypothesis string must remain the original word order of the hypothesis. It is illustrated in Figure 3 (b).
&#8226; A hypothesis word is aligned to null and it is not between the hypothesis span of any minimum alignment unit. In this case, a null word
&#275; &#8242; 4 &#275; &#8242; 5 &#275; &#8242; 6 e 1 e 2 &#949; e 3 &#275; &#8242; 1 &#275; &#8242; 2 &#275; &#8242; 3 e 1 &#949; e 2 e 3 &#275; &#8242; 1 &#275; &#8242; 2 &#275; &#8242; 3 e 1 &#949; e 2 e 3
(d) (e)
are inserted to the backbone. This is shown in Figure 3 (c).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>After the final alignments are generated in the growdiag-final algorithm, minimum alignment units are extracted.</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The hypothesis words of an alignment unit are packed as a whole in shift operations.</text>
                  <doc_id>136</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>See the example in Figure 2 (a) first.</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>All minimum alignment units are as follows: (e &#8242; 2 , e 1), (e &#8242; 1 e&#8242; 3 , e 2 ) and (&#949;, e 3 ).</text>
                  <doc_id>138</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>(e &#8242; 1 e&#8242; 2 e&#8242; 3 , e 1e 2 ) is an alignment unit, but not a minimum alignment unit.</text>
                  <doc_id>139</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Let &#257; i = (&#275; &#8242; i , &#275; i) denote a minimum alignment unit, and assume that the word string &#275; &#8242; i covers words e &#8242; i 1</text>
                  <doc_id>140</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>,..., e &#8242; i m on the hypothesis side, and the word string &#275; i covers the consecutive words e i1 ,..., e in on the backbone side.</text>
                  <doc_id>141</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In an alignment unit, the word string on the hypothesis side can be discontinuous.</text>
                  <doc_id>142</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The minimum unit &#257; i = (&#275; &#8242; i , &#275; i) must observe the following rules:</text>
                  <doc_id>143</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>E B : e 1 e 2 e 3</text>
                  <doc_id>144</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>E h : e &#8242; 1 e &#8242; 2 (a)</text>
                  <doc_id>145</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>E B : e 1 e 2</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>E h : e &#8242; 1 e &#8242; 2 e &#8242; 3</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>E B : e 1 e 2</text>
                  <doc_id>148</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>E h : e &#8242; 1 e &#8242; 2 e &#8242; 3</text>
                  <doc_id>149</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(b)</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(c)</text>
                  <doc_id>151</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e &#8242; 2</text>
                  <doc_id>152</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#949; e &#8242; 1 e 1 e 2 e 3</text>
                  <doc_id>153</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e &#8242; 1 e&#8242; 2 e&#8242; 3 e 1 e 2</text>
                  <doc_id>154</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e &#8242; 1e &#8242; 3</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e &#8242; 1 e &#8242; 2 e &#8242; 3 e 1 &#949; e 2</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; &#8704; e &#8242; i k &#8712; &#275; &#8242; i , e a &#8242; i k &#8712; &#275; i</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; &#8704; e ik &#8712; &#275; i , e &#8242; a ik = null or e &#8242; a ik &#8712; &#275; &#8242; i</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; &#8708; &#257; j = (&#275; &#8242; j , &#275; j), &#275; j = e i1 , ..., e ik or &#275; j = e ik , ..., e in , k &#8712; [1, n]</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Where a &#8242; i k denotes the position of the word in the backbone that e &#8242; i k is aligned to, and a ik denotes the position of the word in the hypothesis that e ik is aligned to.</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>An actual null word may be inserted to a proper position if a word, either from the hypothesis or from the backbone, is aligned to null.</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this way, the minimum alignment set is extended to an alignment unit set, which includes not only minimum alignment units but also alignment units which are generated by adding null words to minimum alignment units.</text>
                  <doc_id>162</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In general, the following three conditions should be taken into consideration:</text>
                  <doc_id>163</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; A backbone word is aligned to null.</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A null word is inserted to the hypothesis as shown in Figure 3 (a).</text>
                  <doc_id>165</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; A hypothesis word is aligned to null and it is between the span of a minimum alignment unit.</text>
                  <doc_id>166</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A new alignment unit is generated by inserting the hypothesis word aligned to null to the minimum alignment unit.</text>
                  <doc_id>167</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The new hypothesis string must remain the original word order of the hypothesis.</text>
                  <doc_id>168</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>It is illustrated in Figure 3 (b).</text>
                  <doc_id>169</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; A hypothesis word is aligned to null and it is not between the hypothesis span of any minimum alignment unit.</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this case, a null word</text>
                  <doc_id>171</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#275; &#8242; 4 &#275; &#8242; 5 &#275; &#8242; 6 e 1 e 2 &#949; e 3 &#275; &#8242; 1 &#275; &#8242; 2 &#275; &#8242; 3 e 1 &#949; e 2 e 3 &#275; &#8242; 1 &#275; &#8242; 2 &#275; &#8242; 3 e 1 &#949; e 2 e 3</text>
                  <doc_id>172</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(d) (e)</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>are inserted to the backbone.</text>
                  <doc_id>174</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This is shown in Figure 3 (c).</text>
                  <doc_id>175</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Lattice Construction Algorithm</title>
            <text>The lattice is constructed by adding the normalized alignment pairs incrementally. One backbone arc in a lattice can only span one backbone word. In contrast, all hypothesis words in an alignment unit must be packed into one hypothesis arc. First the lattice is initialized with a normalized alignment pair. Then given all other alignment pairs one by one, the lattice is modified dynamically by adding the hypothesis words of an alignment pair in a left-to-right fashion.
A toy instance is given in Figure 4 to illustrate the procedure of lattice construction. Assume the current inputs are: an alignment pair as in Figure 4 (a), and a lattice as in Figure 4 (b). The backbone words of the alignment pair are compared to the backbone words of the lattice one by one. The procedure is as follows:
&#8226; e 1 is compared with e 1 . Since they are the same, the hypothesis arc &#275; &#8242; 4 , which comes from the same node with e 1 in the alignment pair, is compared with the hypothesis arc &#275; &#8242; 1 , which comes from the same node with e 1 in the lattice. The two hypothesis arcs are not the same, so &#275; &#8242; 4 is added to the lattice as shown in Figure 4(c). Both go to the next backbone words.
&#8226; e 2 is compared with &#949;. The lattice remains the same. The lattice goes to the next backbone word e 2 .
&#8226; e 2 is compared with e 2 . There is no hypothesis arc coming from the same node with the bone arc e 2 in the alignment pair, so the lattice remains the same. Both go to the next backbone words.
&#8226; &#949; is compared with e 3 . A null backbone arc is inserted into the lattice between e 2 and e 3 . The hypothesis arc &#275; &#8242; 5 is inserted to the lattice, too. The modified lattice is shown in Figure 4(d). The alignment pair goes to the next backbone word e 3 .
&#8226; e 3 is compared with e 3 . For they are the same and there is no hypothesis arc &#275; &#8242; 6 in the lattice,
is inserted to the lattice as in Figure 4(e). &#275; &#8242; 6
&#8226; Both arrive at the end and it is the turn of the next alignment pair.
When comparing a backbone word of the given alignment pair with a backbone word of the lattice, the following three cases should be handled:
&#8226; The current backbone word of the given alignment pair is a null word while the current backbone word of the lattice is not. A null backbone word is inserted to the lattice.
&#8226; The current backbone word of the lattice is a null word while the current word of the given alignment pair is not. The current null backbone word of the lattice is skipped with nothing to do. The next backbone word of the lattice is compared with the current backbone word of the given alignment pair.
Algorithm 1 Lattice construction algorithm.
&#8226; The current backbone words of the given alignment pair and the lattice are the same. Let {harc l } denotes the set of hypothesis arcs, which come from the same node with the current backbone arc in the lattice, and harc h denotes one of the corresponding hypothesis arcs in the given alignment pair. In the {harc l }, if there is no arc which is the same with the harc h , a hypothesis arc projecting to harc h is added to the lattice.
The algorithm of constructing a lattice is illustrated in Algorithm 1. The backbone words of the alignment pair and the lattice are processed one by one in a left-to-right manner. Line 2 initializes the lattice with the first alignment pair, and Line 3 removes the hypothesis arc which contains the same words with the backbone arc. barc denotes the backbone arc, storing one backbone word only, and harc denotes the hypothesis arc, storing the hypothesis words. For there may be many alignment units span the same backbone word range, there may be more than one harc coming from one node. Line 8 &#8722; 10 consider the condition 1 and function InsertBarc in Line 9 inserts a null bone arc to the position right before the current node. Line 12 &#8722; 13 deal with condition 2 and jump to the next backbone word of the lattice. Line 15&#8722;19 handle condition 3 and function InsertHarc inserts to the lattice a harc with the same hypothesis words and the same backbone word span with the current hypothesis arc.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The lattice is constructed by adding the normalized alignment pairs incrementally.</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>One backbone arc in a lattice can only span one backbone word.</text>
                  <doc_id>177</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast, all hypothesis words in an alignment unit must be packed into one hypothesis arc.</text>
                  <doc_id>178</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>First the lattice is initialized with a normalized alignment pair.</text>
                  <doc_id>179</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Then given all other alignment pairs one by one, the lattice is modified dynamically by adding the hypothesis words of an alignment pair in a left-to-right fashion.</text>
                  <doc_id>180</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A toy instance is given in Figure 4 to illustrate the procedure of lattice construction.</text>
                  <doc_id>181</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Assume the current inputs are: an alignment pair as in Figure 4 (a), and a lattice as in Figure 4 (b).</text>
                  <doc_id>182</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The backbone words of the alignment pair are compared to the backbone words of the lattice one by one.</text>
                  <doc_id>183</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The procedure is as follows:</text>
                  <doc_id>184</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; e 1 is compared with e 1 .</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Since they are the same, the hypothesis arc &#275; &#8242; 4 , which comes from the same node with e 1 in the alignment pair, is compared with the hypothesis arc &#275; &#8242; 1 , which comes from the same node with e 1 in the lattice.</text>
                  <doc_id>186</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The two hypothesis arcs are not the same, so &#275; &#8242; 4 is added to the lattice as shown in Figure 4(c).</text>
                  <doc_id>187</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Both go to the next backbone words.</text>
                  <doc_id>188</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; e 2 is compared with &#949;.</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The lattice remains the same.</text>
                  <doc_id>190</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The lattice goes to the next backbone word e 2 .</text>
                  <doc_id>191</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; e 2 is compared with e 2 .</text>
                  <doc_id>192</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>There is no hypothesis arc coming from the same node with the bone arc e 2 in the alignment pair, so the lattice remains the same.</text>
                  <doc_id>193</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Both go to the next backbone words.</text>
                  <doc_id>194</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; &#949; is compared with e 3 .</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A null backbone arc is inserted into the lattice between e 2 and e 3 .</text>
                  <doc_id>196</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The hypothesis arc &#275; &#8242; 5 is inserted to the lattice, too.</text>
                  <doc_id>197</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The modified lattice is shown in Figure 4(d).</text>
                  <doc_id>198</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The alignment pair goes to the next backbone word e 3 .</text>
                  <doc_id>199</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; e 3 is compared with e 3 .</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For they are the same and there is no hypothesis arc &#275; &#8242; 6 in the lattice,</text>
                  <doc_id>201</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>is inserted to the lattice as in Figure 4(e).</text>
                  <doc_id>202</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>&#275; &#8242; 6</text>
                  <doc_id>203</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Both arrive at the end and it is the turn of the next alignment pair.</text>
                  <doc_id>204</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>When comparing a backbone word of the given alignment pair with a backbone word of the lattice, the following three cases should be handled:</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; The current backbone word of the given alignment pair is a null word while the current backbone word of the lattice is not.</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A null backbone word is inserted to the lattice.</text>
                  <doc_id>207</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; The current backbone word of the lattice is a null word while the current word of the given alignment pair is not.</text>
                  <doc_id>208</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The current null backbone word of the lattice is skipped with nothing to do.</text>
                  <doc_id>209</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The next backbone word of the lattice is compared with the current backbone word of the given alignment pair.</text>
                  <doc_id>210</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Algorithm 1 Lattice construction algorithm.</text>
                  <doc_id>211</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; The current backbone words of the given alignment pair and the lattice are the same.</text>
                  <doc_id>212</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Let {harc l } denotes the set of hypothesis arcs, which come from the same node with the current backbone arc in the lattice, and harc h denotes one of the corresponding hypothesis arcs in the given alignment pair.</text>
                  <doc_id>213</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In the {harc l }, if there is no arc which is the same with the harc h , a hypothesis arc projecting to harc h is added to the lattice.</text>
                  <doc_id>214</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The algorithm of constructing a lattice is illustrated in Algorithm 1.</text>
                  <doc_id>215</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The backbone words of the alignment pair and the lattice are processed one by one in a left-to-right manner.</text>
                  <doc_id>216</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Line 2 initializes the lattice with the first alignment pair, and Line 3 removes the hypothesis arc which contains the same words with the backbone arc.</text>
                  <doc_id>217</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>barc denotes the backbone arc, storing one backbone word only, and harc denotes the hypothesis arc, storing the hypothesis words.</text>
                  <doc_id>218</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For there may be many alignment units span the same backbone word range, there may be more than one harc coming from one node.</text>
                  <doc_id>219</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Line 8 &#8722; 10 consider the condition 1 and function InsertBarc in Line 9 inserts a null bone arc to the position right before the current node.</text>
                  <doc_id>220</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Line 12 &#8722; 13 deal with condition 2 and jump to the next backbone word of the lattice.</text>
                  <doc_id>221</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Line 15&#8722;19 handle condition 3 and function InsertHarc inserts to the lattice a harc with the same hypothesis words and the same backbone word span with the current hypothesis arc.</text>
                  <doc_id>222</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Decoding</title>
        <text>In confusion network decoding, a translation is generated by traveling all the nodes from left to right. So a translation path contains all the nodes. While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc.
Similar to the features in Rosti et al. (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words. The features are combined in a log-linear model with the arc posterior probabilities being processed specially as follows:
log p(e/f) =
N&#8721; arc
i=1
&#8721;N s log (
s=1
&#955; s p s (arc))
+ &#950;L(e) + &#945;N nullarc (e)
+ &#946;N longarc (e) + &#947;N word (e)
(9)
where f denotes the source sentence, e denotes a translation generated by the lattice-based system, N arc is the number of arcs the path of e covers, N s is the number of candidate systems and &#955; s is the weight of system s. &#950; is the language model weight and L(e) is the LM log-probability. N nullarcs (e) is the number of the arcs which only contain a null word, and N longarc (e) is the number of the arcs which store more than one non-null word. The above two numbers are gotten by counting both backbone arcs and hypothesis arcs. &#945; and &#946; are the corresponding weights of the numbers, respectively. N word (e) is the non-null word number and &#947; is its weight.
Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by p s (arc). p s (arc) is increased by
1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).
Cube pruning algorithm with beam search is employed to search for the consensus output (Huang and Chiang, 2005). The nodes in the lattice are searched in a topological order and each node retains a list of N best candidate partial translations.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In confusion network decoding, a translation is generated by traveling all the nodes from left to right.</text>
              <doc_id>223</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>So a translation path contains all the nodes.</text>
              <doc_id>224</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc.</text>
              <doc_id>225</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Similar to the features in Rosti et al. (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.</text>
              <doc_id>226</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The features are combined in a log-linear model with the arc posterior probabilities being processed specially as follows:</text>
              <doc_id>227</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>log p(e/f) =</text>
              <doc_id>228</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>N&#8721; arc</text>
              <doc_id>229</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1</text>
              <doc_id>230</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8721;N s log (</text>
              <doc_id>231</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>s=1</text>
              <doc_id>232</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#955; s p s (arc))</text>
              <doc_id>233</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>+ &#950;L(e) + &#945;N nullarc (e)</text>
              <doc_id>234</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>+ &#946;N longarc (e) + &#947;N word (e)</text>
              <doc_id>235</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(9)</text>
              <doc_id>236</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where f denotes the source sentence, e denotes a translation generated by the lattice-based system, N arc is the number of arcs the path of e covers, N s is the number of candidate systems and &#955; s is the weight of system s. &#950; is the language model weight and L(e) is the LM log-probability.</text>
              <doc_id>237</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>N nullarcs (e) is the number of the arcs which only contain a null word, and N longarc (e) is the number of the arcs which store more than one non-null word.</text>
              <doc_id>238</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The above two numbers are gotten by counting both backbone arcs and hypothesis arcs.</text>
              <doc_id>239</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>&#945; and &#946; are the corresponding weights of the numbers, respectively.</text>
              <doc_id>240</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>N word (e) is the non-null word number and &#947; is its weight.</text>
              <doc_id>241</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by p s (arc).</text>
              <doc_id>242</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>p s (arc) is increased by</text>
              <doc_id>243</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).</text>
              <doc_id>244</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Cube pruning algorithm with beam search is employed to search for the consensus output (Huang and Chiang, 2005).</text>
              <doc_id>245</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The nodes in the lattice are searched in a topological order and each node retains a list of N best candidate partial translations.</text>
              <doc_id>246</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>6 Experiments</title>
        <text>The candidate systems participating in the system combination are as listed in Table 1: System A is a BTG-based system using a MaxEnt-based reordering model; System B is a hierarchical phrase-based system; System C is the Moses decoder (Koehn et al., 2007); System D is a syntax-based system. 10- best hypotheses from each candidate system on the dev and test sets were collected as the input of the system combination.
In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5- gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell&#8217;s method to maximum BLEU score. The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996). In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, &#961; = 3; the controlling factor for the distortion model, K = 2.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The candidate systems participating in the system combination are as listed in Table 1: System A is a BTG-based system using a MaxEnt-based reordering model; System B is a hierarchical phrase-based system; System C is the Moses decoder (Koehn et al., 2007); System D is a syntax-based system.</text>
              <doc_id>247</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>10- best hypotheses from each candidate system on the dev and test sets were collected as the input of the system combination.</text>
              <doc_id>248</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2.</text>
              <doc_id>249</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A 5- gram language model was used which was trained on the XinHua portion of Gigaword corpus.</text>
              <doc_id>250</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The results were all reported in case sensitive BLEU score and the weights were tuned in Powell&#8217;s method to maximum BLEU score.</text>
              <doc_id>251</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996).</text>
              <doc_id>252</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, &#961; = 3; the controlling factor for the distortion model, K = 2.</text>
              <doc_id>253</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>6.1 Comparison with Confusion-network-based model</title>
            <text>In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008). In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS data set which included 289K sentence pairs. The interpolation factor of the similarity model was set to &#945; = 0.1.
The results are shown in Table 1. IHMM stands for the IHMM-based model and Lattice stands for the lattice-based model. On the dev set, the latticebased system was 3.92 BLEU points higher than the best single system and 0.36 BLEU point higher than the IHMM-based system. On the test set, the latticebased system got an absolute improvement by 3.73 BLEU points over the best single system and 1.23 BLEU points over the IHMM-based system.
The results on another test sets are reported in Table 2. The parameters were tuned on the newswire part of NIST MT06 Chinese-to-English test set, including 616 sentences, and the test set was NIST MT08 Chinese-to-English test set, including 1357 sentences. The BLEU score of the lattice-based system is 0.93 BLEU point higher than the IHMMbased system and 3.0 BLEU points higher than the best single system.
We take a real example from the output of the two systems (in Table 3) to show that higher BLEU scores correspond to better alignments and better translations. The translation of System C is selected as the backbone. From Table 3, we can see that because of 1-to-1 mappings, &#8220;Russia&#8221; is aligned to &#8220;Russian&#8221; and &#8220;&#8217;s&#8221; to &#8220;null&#8221; in the IHMM-based model, which leads to the error translation &#8220;Russian
Source:
SystemA: SystemB: SystemC: SystemD:
IHMM: Lattice:
&#162;&#65533;&#193;&#65533;&#236;&#65533;&#65533;&#65533;&#65533;&#65533;&#162;&#65533;&#193;&#65533;&#166;&#65533;&#65533;&#65533;&#65533;&#189;&#251;
Russia merger of state-owned oil company and the state-run gas company in Russia Russia &#8217;s state-owned oil company is working with Russia &#8217;s state-run gas company mergers Russian state-run oil company is combined with the Russian state-run gas company Russia &#8217;s state-owned oil companies are combined with Russia &#8217;s state-run gas company
Russian &#8217;s state-owned oil company working with Russia &#8217;s state-run gas company Russia &#8217;s state-owned oil company is combined with the Russian state-run gas company
&#8217;s&#8221;. Instead, &#8220;Russia &#8217;s&#8221; is together aligned to &#8221;Russian&#8221; in the lattice-based model. Also due to 1-to- 1 mappings, null word aligned to &#8220;is&#8221; is inserted. As a result, &#8220;is&#8221; is missed in the output of IHMMbased model. In contrast, in the lattice-based system, &#8220;is working with&#8221; are aligned to &#8220;is combined with&#8221;, forming a phrase pair.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008).</text>
                  <doc_id>254</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS data set which included 289K sentence pairs.</text>
                  <doc_id>255</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The interpolation factor of the similarity model was set to &#945; = 0.1.</text>
                  <doc_id>256</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The results are shown in Table 1.</text>
                  <doc_id>257</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>IHMM stands for the IHMM-based model and Lattice stands for the lattice-based model.</text>
                  <doc_id>258</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>On the dev set, the latticebased system was 3.92 BLEU points higher than the best single system and 0.36 BLEU point higher than the IHMM-based system.</text>
                  <doc_id>259</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>On the test set, the latticebased system got an absolute improvement by 3.73 BLEU points over the best single system and 1.23 BLEU points over the IHMM-based system.</text>
                  <doc_id>260</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The results on another test sets are reported in Table 2.</text>
                  <doc_id>261</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The parameters were tuned on the newswire part of NIST MT06 Chinese-to-English test set, including 616 sentences, and the test set was NIST MT08 Chinese-to-English test set, including 1357 sentences.</text>
                  <doc_id>262</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The BLEU score of the lattice-based system is 0.93 BLEU point higher than the IHMMbased system and 3.0 BLEU points higher than the best single system.</text>
                  <doc_id>263</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We take a real example from the output of the two systems (in Table 3) to show that higher BLEU scores correspond to better alignments and better translations.</text>
                  <doc_id>264</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The translation of System C is selected as the backbone.</text>
                  <doc_id>265</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>From Table 3, we can see that because of 1-to-1 mappings, &#8220;Russia&#8221; is aligned to &#8220;Russian&#8221; and &#8220;&#8217;s&#8221; to &#8220;null&#8221; in the IHMM-based model, which leads to the error translation &#8220;Russian</text>
                  <doc_id>266</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Source:</text>
                  <doc_id>267</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>SystemA: SystemB: SystemC: SystemD:</text>
                  <doc_id>268</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>IHMM: Lattice:</text>
                  <doc_id>269</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#162;&#65533;&#193;&#65533;&#236;&#65533;&#65533;&#65533;&#65533;&#65533;&#162;&#65533;&#193;&#65533;&#166;&#65533;&#65533;&#65533;&#65533;&#189;&#251;</text>
                  <doc_id>270</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Russia merger of state-owned oil company and the state-run gas company in Russia Russia &#8217;s state-owned oil company is working with Russia &#8217;s state-run gas company mergers Russian state-run oil company is combined with the Russian state-run gas company Russia &#8217;s state-owned oil companies are combined with Russia &#8217;s state-run gas company</text>
                  <doc_id>271</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Russian &#8217;s state-owned oil company working with Russia &#8217;s state-run gas company Russia &#8217;s state-owned oil company is combined with the Russian state-run gas company</text>
                  <doc_id>272</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8217;s&#8221;.</text>
                  <doc_id>273</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Instead, &#8220;Russia &#8217;s&#8221; is together aligned to &#8221;Russian&#8221; in the lattice-based model.</text>
                  <doc_id>274</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Also due to 1-to- 1 mappings, null word aligned to &#8220;is&#8221; is inserted.</text>
                  <doc_id>275</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>As a result, &#8220;is&#8221; is missed in the output of IHMMbased model.</text>
                  <doc_id>276</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast, in the lattice-based system, &#8220;is working with&#8221; are aligned to &#8220;is combined with&#8221;, forming a phrase pair.</text>
                  <doc_id>277</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>6.2 Effect of Dictionary Scale</title>
            <text>The dictionary is important to the semantic similarity model in IHMM-based alignment method. We evaluated the effect of the dictionary scale by using dictionaries extracted on different data sets. The dictionaries were respectively extracted on similar data sets: 30K sentence pairs, 60K sentence pairs, 289K sentence pairs (FBIS corpus) and 2500K sentence pairs. The results are illustrated in Table 4. In order to demonstrate the effect of the dictionary size clearly, the interpolation factor of similarity model was all set to &#945; = 0.1.
From Table 4, we can see that when the corpus size rise from 30k to 60k, the improvements were not obvious both on the dev set and on the test set. As the corpus was expanded to 289K, although on the dev set, the result was only 0.2 BLEU point higher, on the test set, it was 0.63 BLEU point higher. As the corpus size was up to 2500K, the BLEU scores both on the dev and test sets declined. The reason is that, on one hand, there are more noise on the 2500K sentence pairs; on the other hand, the 289K sentence pairs cover most of the words appearing on the test set. So we can conclude that in order to get better results, the dictionary scale must be up to some certain scale. If the dictionary is much smaller, the result will be impacted dramatically.
MT02 BLEU% MT05 BLEU%</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The dictionary is important to the semantic similarity model in IHMM-based alignment method.</text>
                  <doc_id>278</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We evaluated the effect of the dictionary scale by using dictionaries extracted on different data sets.</text>
                  <doc_id>279</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The dictionaries were respectively extracted on similar data sets: 30K sentence pairs, 60K sentence pairs, 289K sentence pairs (FBIS corpus) and 2500K sentence pairs.</text>
                  <doc_id>280</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The results are illustrated in Table 4.</text>
                  <doc_id>281</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In order to demonstrate the effect of the dictionary size clearly, the interpolation factor of similarity model was all set to &#945; = 0.1.</text>
                  <doc_id>282</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>From Table 4, we can see that when the corpus size rise from 30k to 60k, the improvements were not obvious both on the dev set and on the test set.</text>
                  <doc_id>283</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As the corpus was expanded to 289K, although on the dev set, the result was only 0.2 BLEU point higher, on the test set, it was 0.63 BLEU point higher.</text>
                  <doc_id>284</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As the corpus size was up to 2500K, the BLEU scores both on the dev and test sets declined.</text>
                  <doc_id>285</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The reason is that, on one hand, there are more noise on the 2500K sentence pairs; on the other hand, the 289K sentence pairs cover most of the words appearing on the test set.</text>
                  <doc_id>286</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>So we can conclude that in order to get better results, the dictionary scale must be up to some certain scale.</text>
                  <doc_id>287</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>If the dictionary is much smaller, the result will be impacted dramatically.</text>
                  <doc_id>288</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>MT02 BLEU% MT05 BLEU%</text>
                  <doc_id>289</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>6.3 Effect of Semantic Alignments</title>
            <text>For the IHMM-based alignment method, the translation probability of an English word pair is computed using a linear interpolation of the semantic similarity and the surface similarity. So the two similarity models decide the translation probability together and the proportion is controlled by the interpolation factor. We evaluated the effect of the two similarity models by varying the interpolation factor &#945;. We used the dictionaries extracted on the FBIS data set. The result is shown in Table 5. We got the best result with &#945; = 0.1. When we excluded the semantic similarity model (&#945; = 0.0) or excluded the surface similarity model (&#945; = 1.0), the performance became worse.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>For the IHMM-based alignment method, the translation probability of an English word pair is computed using a linear interpolation of the semantic similarity and the surface similarity.</text>
                  <doc_id>290</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>So the two similarity models decide the translation probability together and the proportion is controlled by the interpolation factor.</text>
                  <doc_id>291</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We evaluated the effect of the two similarity models by varying the interpolation factor &#945;.</text>
                  <doc_id>292</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We used the dictionaries extracted on the FBIS data set.</text>
                  <doc_id>293</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The result is shown in Table 5.</text>
                  <doc_id>294</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We got the best result with &#945; = 0.1.</text>
                  <doc_id>295</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>When we excluded the semantic similarity model (&#945; = 0.0) or excluded the surface similarity model (&#945; = 1.0), the performance became worse.</text>
                  <doc_id>296</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>7</index>
        <title>7 Conclusion</title>
        <text>The alignment model plays an important role in system combination. Because of the expression limitation of confusion networks, only 1-to-1 mappings are employed in the confusion-network-based model. This paper proposes a lattice-based system combination model. As a general form of confusion networks, lattices can express n-to-n mappings. So a lattice-based model processes phrase pairs while
MT02 BLEU% MT05 BLEU%
a confusion-network-based model processes words only. As a result, phrase pairs must be extracted before constructing a lattice. On NIST MT05 test set, the lattice-based system gave better results with an absolute improvement of 1.23 BLEU points over the confusionnetwork-based system (He et al., 2008) and 3.73 BLEU points over the best single system. On NIST MT08 test set, the lattice-based system outperformed the confusion-network-based system by 0.93 BLEU point and outperformed the best single system by 3.0 BLEU points.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The alignment model plays an important role in system combination.</text>
              <doc_id>297</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Because of the expression limitation of confusion networks, only 1-to-1 mappings are employed in the confusion-network-based model.</text>
              <doc_id>298</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This paper proposes a lattice-based system combination model.</text>
              <doc_id>299</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As a general form of confusion networks, lattices can express n-to-n mappings.</text>
              <doc_id>300</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>So a lattice-based model processes phrase pairs while</text>
              <doc_id>301</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>MT02 BLEU% MT05 BLEU%</text>
              <doc_id>302</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>a confusion-network-based model processes words only.</text>
              <doc_id>303</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As a result, phrase pairs must be extracted before constructing a lattice.</text>
              <doc_id>304</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>On NIST MT05 test set, the lattice-based system gave better results with an absolute improvement of 1.23 BLEU points over the confusionnetwork-based system (He et al., 2008) and 3.73 BLEU points over the best single system.</text>
              <doc_id>305</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>On NIST MT08 test set, the lattice-based system outperformed the confusion-network-based system by 0.93 BLEU point and outperformed the best single system by 3.0 BLEU points.</text>
              <doc_id>306</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>8</index>
        <title>8 Acknowledgement</title>
        <text>The authors were supported by National Natural Science</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The authors were supported by National Natural Science</text>
              <doc_id>307</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Results on the MT02 and MT05 test sets</caption>
        <reference_text>In PAGE 7: ... The nodes in the lattice are searched in a topological order and each node re- tains a list of N best candidate partial translations. 6 Experiments The candidate systems participating in the system combination are as listed in  Table1 : System A is a BTG-based system using a MaxEnt-based reorder- ing model; System B is a hierarchical phrase-based system; System C is the Moses decoder (Koehn et al., 2007); System D is a syntax-based system....  In PAGE 7: ...1. The results are shown in  Table1 . IHMM stands for the IHMM-based model and Lattice stands for the lattice-based model....</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>System</cell>
              <cell>MT02</cell>
              <cell>MT05</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>BLEU%</cell>
              <cell>BLEU%</cell>
            </row>
            <row>
              <cell>SystemA</cell>
              <cell>31.93</cell>
              <cell>30.68</cell>
            </row>
            <row>
              <cell>SystemB</cell>
              <cell>32.16</cell>
              <cell>32.07</cell>
            </row>
            <row>
              <cell>SystemC</cell>
              <cell>32.09</cell>
              <cell>31.64</cell>
            </row>
            <row>
              <cell>SystemD</cell>
              <cell>33.37</cell>
              <cell>31.26</cell>
            </row>
            <row>
              <cell>IHMM</cell>
              <cell>36.93</cell>
              <cell>34.57</cell>
            </row>
            <row>
              <cell>Lattice</cell>
              <cell>37.29</cell>
              <cell>35.80</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Results on the MT06 and MT08 test sets</caption>
        <reference_text>In PAGE 7: ...l., 2007); System D is a syntax-based system. 10- best hypotheses from each candidate system on the dev and test sets were collected as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, includ- ing 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in  Table2 . A 5- gram language model was used which was trained on the XinHua portion of Gigaword corpus....</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>best single system.</cell>
              <cell>None</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System</cell>
              <cell>MT06</cell>
              <cell>MT08</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>BLEU%</cell>
              <cell>BLEU%</cell>
            </row>
            <row>
              <cell>SystemA</cell>
              <cell>32.51</cell>
              <cell>25.63</cell>
            </row>
            <row>
              <cell>SystemB</cell>
              <cell>31.43</cell>
              <cell>26.32</cell>
            </row>
            <row>
              <cell>SystemC</cell>
              <cell>31.50</cell>
              <cell>23.43</cell>
            </row>
            <row>
              <cell>SystemD</cell>
              <cell>32.41</cell>
              <cell>26.28</cell>
            </row>
            <row>
              <cell>IHMM</cell>
              <cell>36.05</cell>
              <cell>28.39</cell>
            </row>
            <row>
              <cell>Lattice</cell>
              <cell>36.53</cell>
              <cell>29.32</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: Effect of dictionary scale</caption>
        <reference_text>In PAGE 8: ... The dic- tionaries were respectively extracted on similar data sets: 30K sentence pairs, 60K sentence pairs, 289K sentence pairs (FBIS corpus) and 2500K sentence pairs. The results are illustrated in  Table4 . In or- der to demonstrate the effect of the dictionary size clearly, the interpolation factor of similarity model was all set to ? = 0....  In PAGE 8: ...1. From  Table4 , we can see that when the cor- pus size rise from 30k to 60k, the improvements were not obvious both on the dev set and on the test set. As the corpus was expanded to 289K, al- though on the dev set, the result was only 0....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>None</cell>
              <cell>MT02   BLEU%</cell>
              <cell>MT05    BLEU%</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>30k</cell>
              <cell>36.94</cell>
              <cell>35.14</cell>
            </row>
            <row>
              <cell>60k</cell>
              <cell>37.09</cell>
              <cell>35.17</cell>
            </row>
            <row>
              <cell>289k</cell>
              <cell>37.29</cell>
              <cell>35.80</cell>
            </row>
            <row>
              <cell>2500k</cell>
              <cell>37.14</cell>
              <cell>35.62</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TableSeer</source>
        <caption>Table 5: Effect of semantic alignments</caption>
        <reference_text>In PAGE 8: ... We used the dictionaries extracted on the FBIS data set. The result is shown in  Table5 . We got the best result with ? = 0....</reference_text>
        <page_num>9</page_num>
        <head>
          <rows>
            <row>
              <cell>None</cell>
              <cell>MT02   BLEU%</cell>
              <cell>MT05    BLEU%</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>? = 1.0</cell>
              <cell>36.41</cell>
              <cell>34.92</cell>
            </row>
            <row>
              <cell>? = 0.7</cell>
              <cell>37.21</cell>
              <cell>35.65</cell>
            </row>
            <row>
              <cell>? = 0.5</cell>
              <cell>36.43</cell>
              <cell>35.02</cell>
            </row>
            <row>
              <cell>? = 0.4</cell>
              <cell>37.14</cell>
              <cell>35.55</cell>
            </row>
            <row>
              <cell>? = 0.3</cell>
              <cell>36.75</cell>
              <cell>35.66</cell>
            </row>
            <row>
              <cell>? = 0.2</cell>
              <cell>36.81</cell>
              <cell>35.55</cell>
            </row>
            <row>
              <cell>? = 0.1</cell>
              <cell>37.29</cell>
              <cell>35.80</cell>
            </row>
            <row>
              <cell>? = 0.0</cell>
              <cell>36.45</cell>
              <cell>35.14</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Srinivas Bangalore</author>
          <author>German Bordel</author>
          <author>Giuseppe Riccardi</author>
        </authors>
        <title>Computing consensus translation from multiple machine translation systems.</title>
        <publication>In Proc. of IEEE ASRU,</publication>
        <pages>351--354</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Christopher Dyer</author>
          <author>Smaranda Muresan</author>
          <author>Philip Resnik</author>
        </authors>
        <title>Generalizing word lattice translation.</title>
        <publication>In Proceedings of ACL/HLT 2008,</publication>
        <pages>1012--1020</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Robert Frederking</author>
          <author>Sergei Nirenburg</author>
        </authors>
        <title>Three heads are better than one.</title>
        <publication>In Proc. of ANLP,</publication>
        <pages>95--100</pages>
        <date>1994</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Xiaodong He</author>
          <author>Mei Yang</author>
          <author>Jangfeng Gao</author>
          <author>Patrick Nguyen</author>
          <author>Robert Moore</author>
        </authors>
        <title>Indirect-hmm-based hypothesis alignment for computing outputs from machine translation systems.</title>
        <publication>In Proc. of EMNLP,</publication>
        <pages>98--107</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Xiaodong He</author>
        </authors>
        <title>Using word-dependent translation models in hmm based word alignment for statistical machine translation.</title>
        <publication>In Proc. of COLING-ACL,</publication>
        <pages>961--968</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Liang Huang</author>
          <author>David Chiang</author>
        </authors>
        <title>Better k-best parsing.</title>
        <publication>In Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT),</publication>
        <pages>53--64</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz J Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical phrase-based translation.</title>
        <publication>In Proc. of HLTNAACL,</publication>
        <pages>127--133</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Hieu Hoang</author>
          <author>Alexandra Birch Mayne</author>
          <author>Christopher Callison-Burch</author>
          <author>Marcello Federico</author>
          <author>Nicola Bertoldi</author>
          <author>Brooke Cowan</author>
          <author>Wade Shen</author>
        </authors>
        <title>Moses: Open source toolkit for statistical machine translation.</title>
        <publication>In Proc. of the 45th ACL, Demonstration Session.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Evgeny Matusov</author>
          <author>Nicola Ueffing</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
        <publication>In Proc. of IEEE EACL,</publication>
        <pages>33--40</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Antti-Veikko I Rosti</author>
          <author>Spyros Matsoukas</author>
          <author>Richard Schwartz</author>
        </authors>
        <title>Improved word-level system combination for machine translation.</title>
        <publication>In Proc. of ACL,</publication>
        <pages>312--319</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>2007b</author>
        </authors>
        <title>Combining outputs from multiple machine translation systems.</title>
        <publication>In Proc. of NAACL-HLT,</publication>
        <pages>228--235</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Antti-Veikko I Rosti</author>
          <author>Bing Zhang</author>
          <author>Spyros Matsoukas</author>
          <author>Richard Schwartz</author>
        </authors>
        <title>Incremental hypothesis alignment for building confusion networks with application to machine translaiton system combination.</title>
        <publication>In Proc. of the Third ACL WorkShop on Statistical Machine Translation,</publication>
        <pages>183--186</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Khe Chai Sim</author>
          <author>William J Byrne</author>
          <author>Mark J F Gales</author>
          <author>Hichem Sahbi</author>
          <author>Phil C Woodland</author>
        </authors>
        <title>Consensus network decoding for statistical machine translation system combination.</title>
        <publication>In Proc. of ICASSP,</publication>
        <pages>105--108</pages>
        <date>2007</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Bangalore et al., 2001</string>
        <sentence_id>7310</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Dyer et al., 2008</string>
        <sentence_id>7357</sentence_id>
        <char_offset>56</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Frederking and Nirenburg, 1994</string>
        <sentence_id>7309</sentence_id>
        <char_offset>110</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>3</reference_id>
        <string>He et al. (2008)</string>
        <sentence_id>7599</sentence_id>
        <char_offset>61</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>3</reference_id>
        <string>He et al. (2008)</string>
        <sentence_id>7551</sentence_id>
        <char_offset>197</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>3</reference_id>
        <string>He et al., 2008</string>
        <sentence_id>7310</sentence_id>
        <char_offset>229</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>3</reference_id>
        <string>He et al., 2008</string>
        <sentence_id>7325</sentence_id>
        <char_offset>83</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>3</reference_id>
        <string>He et al., 2008</string>
        <sentence_id>7358</sentence_id>
        <char_offset>99</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>3</reference_id>
        <string>He et al., 2008</string>
        <sentence_id>7548</sentence_id>
        <char_offset>93</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>3</reference_id>
        <string>He et al., 2008</string>
        <sentence_id>7609</sentence_id>
        <char_offset>157</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>4</reference_id>
        <string>He (2007)</string>
        <sentence_id>7599</sentence_id>
        <char_offset>79</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>5</reference_id>
        <string>Huang and Chiang, 2005</string>
        <sentence_id>7549</sentence_id>
        <char_offset>88</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>6</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>7411</sentence_id>
        <char_offset>96</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>7</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>7594</sentence_id>
        <char_offset>237</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>8</reference_id>
        <string>Matusov et al., 2006</string>
        <sentence_id>7310</sentence_id>
        <char_offset>127</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>9</reference_id>
        <string>Rosti et al. (2007</string>
        <sentence_id>7530</sentence_id>
        <char_offset>27</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>9</reference_id>
        <string>Rosti et al., 2007</string>
        <sentence_id>7310</sentence_id>
        <char_offset>167</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>9</reference_id>
        <string>Rosti et al., 2007</string>
        <sentence_id>7310</sentence_id>
        <char_offset>188</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>9</reference_id>
        <string>Rosti et al., 2007</string>
        <sentence_id>7548</sentence_id>
        <char_offset>72</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>10</reference_id>
        <string>(2007)</string>
        <sentence_id>7599</sentence_id>
        <char_offset>82</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>11</reference_id>
        <string>Rosti et al., 2008</string>
        <sentence_id>7310</sentence_id>
        <char_offset>209</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>12</reference_id>
        <string>Sim et al., 2007</string>
        <sentence_id>7310</sentence_id>
        <char_offset>149</char_offset>
      </citation>
    </citations>
  </content>
</document>
