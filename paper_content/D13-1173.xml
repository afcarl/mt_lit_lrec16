<document>
  <filename>D13-1173</filename>
  <authors>
    <author>Qing Dou</author>
  </authors>
  <title>Dependency-Based Decipherment for Resource-Limited Machine Translation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>We introduce dependency relations into deciphering foreign languages and show that dependency relations help improve the state-ofthe-art deciphering accuracy by over 500%. We learn a translation lexicon from large amounts of genuinely non parallel data with decipherment to improve a phrase-based machine translation system trained with limited parallel data. In experiments, we observe BLEU gains of 1.2 to 1.8 across three different test sets.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We introduce dependency relations into deciphering foreign languages and show that dependency relations help improve the state-ofthe-art deciphering accuracy by over 500%.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We learn a translation lexicon from large amounts of genuinely non parallel data with decipherment to improve a phrase-based machine translation system trained with limited parallel data.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In experiments, we observe BLEU gains of 1.2 to 1.8 across three different test sets.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>State-of-the-art machine translation (MT) systems apply statistical techniques to learn translation rules from large amounts of parallel data. However, parallel data is limited for many language pairs and domains.
In general, it is easier to obtain non parallel data. The ability to build a machine translation system using monolingual data could alleviate problems caused by insufficient parallel data. Towards building a machine translation system without a parallel corpus, Klementiev et al. (2012) use non parallel data to estimate parameters for a large scale MT system. Other work tries to learn full MT systems using only non parallel data through decipherment (Ravi and Knight, 2011; Ravi, 2013). However, the performance of such systems is poor compared with those trained with parallel data.
Given that we often have some parallel data, it is more practical to improve a translation system trained on parallel corpora with non parallel Figure 1: Improving machine translation with decipherment (Grey boxes represent new data and process). Mono: monolingual; LM: language model; LEX: translation lexicon; TM: translation model.
data. Dou and Knight (2012) successfully apply decipherment to learn a domain specific translation lexicon from monolingual data to improve out-ofdomain machine translation. Although their approach works well for Spanish/French, they do not show whether their approach works for other language pairs. Moreover, the non parallel data used in their experiments is created from a parallel corpus. Such highly comparable data is difficult to obtain in reality.
In this work, we improve previous work by Dou and Knight (2012) using genuinely non parallel data,
and propose a framework to improve a machine translation system trained with a small amount of parallel data. As shown in Figure 1, we use a lexicon learned from decipherment to improve translations of both observed and out-of-vocabulary (OOV) words. The main contributions of this work are:
&#8226; We extract bigrams based on dependency relations for decipherment, which improves the state-of-the-art deciphering accuracy by over 500%.
&#8226; We demonstrate how to improve translations of words observed in parallel data by using a translation lexicon obtained from large amounts of non parallel data.
&#8226; We show that decipherment is able to find correct translations for OOV words.
&#8226; We use a translation lexicon learned by deciphering large amounts of non parallel data to improve a phrase-based MT system trained with limited amounts of parallel data. In experiments, we observe 1.2 to 1.8 BLEU gains across three different test sets.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>State-of-the-art machine translation (MT) systems apply statistical techniques to learn translation rules from large amounts of parallel data.</text>
              <doc_id>3</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, parallel data is limited for many language pairs and domains.</text>
              <doc_id>4</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In general, it is easier to obtain non parallel data.</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The ability to build a machine translation system using monolingual data could alleviate problems caused by insufficient parallel data.</text>
              <doc_id>6</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Towards building a machine translation system without a parallel corpus, Klementiev et al. (2012) use non parallel data to estimate parameters for a large scale MT system.</text>
              <doc_id>7</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Other work tries to learn full MT systems using only non parallel data through decipherment (Ravi and Knight, 2011; Ravi, 2013).</text>
              <doc_id>8</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>However, the performance of such systems is poor compared with those trained with parallel data.</text>
              <doc_id>9</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Given that we often have some parallel data, it is more practical to improve a translation system trained on parallel corpora with non parallel Figure 1: Improving machine translation with decipherment (Grey boxes represent new data and process).</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Mono: monolingual; LM: language model; LEX: translation lexicon; TM: translation model.</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>data.</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Dou and Knight (2012) successfully apply decipherment to learn a domain specific translation lexicon from monolingual data to improve out-ofdomain machine translation.</text>
              <doc_id>13</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Although their approach works well for Spanish/French, they do not show whether their approach works for other language pairs.</text>
              <doc_id>14</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, the non parallel data used in their experiments is created from a parallel corpus.</text>
              <doc_id>15</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Such highly comparable data is difficult to obtain in reality.</text>
              <doc_id>16</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this work, we improve previous work by Dou and Knight (2012) using genuinely non parallel data,</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and propose a framework to improve a machine translation system trained with a small amount of parallel data.</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As shown in Figure 1, we use a lexicon learned from decipherment to improve translations of both observed and out-of-vocabulary (OOV) words.</text>
              <doc_id>19</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The main contributions of this work are:</text>
              <doc_id>20</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; We extract bigrams based on dependency relations for decipherment, which improves the state-of-the-art deciphering accuracy by over 500%.</text>
              <doc_id>21</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; We demonstrate how to improve translations of words observed in parallel data by using a translation lexicon obtained from large amounts of non parallel data.</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; We show that decipherment is able to find correct translations for OOV words.</text>
              <doc_id>23</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; We use a translation lexicon learned by deciphering large amounts of non parallel data to improve a phrase-based MT system trained with limited amounts of parallel data.</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In experiments, we observe 1.2 to 1.8 BLEU gains across three different test sets.</text>
              <doc_id>25</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Previous Work</title>
        <text>Motivated by the idea that a translation lexicon induced from non parallel data can be applied to MT, a variety of prior research has tried to build a translation lexicon from non parallel or comparable data (Rapp, 1995; Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum&#233; and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a). Although previous work is able to build a translation lexicon without parallel data, little has used the lexicon to improve machine translation.
There has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012). Decipherment views one language as a cipher for another and learns a translation lexicon that produces a good decipherment.
In an effort to build a MT system without a parallel corpus, Ravi and Knight (2011) view Spanish as a cipher for English and apply Bayesian learning to directly decipher Spanish into English. Unfortunately, their approach can only work on small data with limited vocabulary. Dou and Knight (2012) propose two techniques to make Bayesian decipherment scalable. First, unlike Ravi and Knight (2011), who decipher whole sentences, Dou and Knight (2012) decipher bigrams. Reducing a ciphertext to a set of bigrams with counts significantly reduces the amount of cipher data. According to Dou and Knight (2012), a ciphertext bigram F is generated through the following generative story:
&#8226; Generate a sequence of two plaintext tokens e 1 e 2 with probability P (e 1 e 2 ) given by a language model built from large numbers of plaintext bigrams.
&#8226; Substitute e 1 with f 1 and e 2 with f 2 with probability P (f 1 |e 1 ) &#183; P (f 2 |e 2 ).
The probability of any cipher bigram F is:
P (F ) = &#8721; e 1 e 2 P (e 1 e 2 ) 2&#8719;
P (f i |e i )
i=1
Given a corpus of N cipher bigrams F 1 ...F N , the probability of the corpus is:
P (corpus) = N&#8719;
P (F j )
j=1
Given a plaintext bigram language model, the goal is to manipulate P (f |e) to maximize P (corpus). Theoretically, one can directly apply EM to solve the problem (Knight et al., 2006). However, EM has time complexity O(N &#183; Ve 2 ) and space complexity O(V f &#183; V e ), where V f , V e are the sizes of ciphertext and plaintext vocabularies respectively, and N is the number of cipher bigrams.
Ravi and Knight (2011) apply Bayesian learning to reduce the space complexity. Instead of estimating probabilities P (f|e), Bayesian learning tries to draw samples from plaintext sequences given ciphertext bigrams. During sampling, the probability of any possible plaintext sample e 1 e 2 is given as:
P sample (e 1 e 2 ) = P (e 1 e 2 ) 2&#8719;
P bayes (f i |e i )
i=1
with P bayes (f i |e i ) defined as:
P bayes (f i |e i ) = &#945;P 0(f i |e i ) + count(f i , e i ) &#945; + count(e i )
where P 0 is a base distribution, and &#945; is a parameter that controls how much we trust P 0 . count(f i , e i ) and count(e i ) record the number of times f i , e i and e i appear in previously generated samples respectively.
At the end of sampling, P (f i |e i ) is estimated by:
P (f i |e i ) = count(f i, e i ) count(e i )
However, Bayesian decipherment is still very slow with Gibbs sampling (Geman and Geman, 1987), as each sampling step requires considering V e possibilities. Dou and Knight (2012) solve the problem by introducing slice sampling (Neal, 2000) to Bayesian decipherment.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Motivated by the idea that a translation lexicon induced from non parallel data can be applied to MT, a variety of prior research has tried to build a translation lexicon from non parallel or comparable data (Rapp, 1995; Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum&#233; and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a).</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Although previous work is able to build a translation lexicon without parallel data, little has used the lexicon to improve machine translation.</text>
              <doc_id>27</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>There has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012).</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Decipherment views one language as a cipher for another and learns a translation lexicon that produces a good decipherment.</text>
              <doc_id>29</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In an effort to build a MT system without a parallel corpus, Ravi and Knight (2011) view Spanish as a cipher for English and apply Bayesian learning to directly decipher Spanish into English.</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Unfortunately, their approach can only work on small data with limited vocabulary.</text>
              <doc_id>31</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Dou and Knight (2012) propose two techniques to make Bayesian decipherment scalable.</text>
              <doc_id>32</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>First, unlike Ravi and Knight (2011), who decipher whole sentences, Dou and Knight (2012) decipher bigrams.</text>
              <doc_id>33</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Reducing a ciphertext to a set of bigrams with counts significantly reduces the amount of cipher data.</text>
              <doc_id>34</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>According to Dou and Knight (2012), a ciphertext bigram F is generated through the following generative story:</text>
              <doc_id>35</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; Generate a sequence of two plaintext tokens e 1 e 2 with probability P (e 1 e 2 ) given by a language model built from large numbers of plaintext bigrams.</text>
              <doc_id>36</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; Substitute e 1 with f 1 and e 2 with f 2 with probability P (f 1 |e 1 ) &#183; P (f 2 |e 2 ).</text>
              <doc_id>37</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The probability of any cipher bigram F is:</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (F ) = &#8721; e 1 e 2 P (e 1 e 2 ) 2&#8719;</text>
              <doc_id>39</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (f i |e i )</text>
              <doc_id>40</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1</text>
              <doc_id>41</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Given a corpus of N cipher bigrams F 1 ...F N , the probability of the corpus is:</text>
              <doc_id>42</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (corpus) = N&#8719;</text>
              <doc_id>43</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (F j )</text>
              <doc_id>44</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>j=1</text>
              <doc_id>45</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Given a plaintext bigram language model, the goal is to manipulate P (f |e) to maximize P (corpus).</text>
              <doc_id>46</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Theoretically, one can directly apply EM to solve the problem (Knight et al., 2006).</text>
              <doc_id>47</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, EM has time complexity O(N &#183; Ve 2 ) and space complexity O(V f &#183; V e ), where V f , V e are the sizes of ciphertext and plaintext vocabularies respectively, and N is the number of cipher bigrams.</text>
              <doc_id>48</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Ravi and Knight (2011) apply Bayesian learning to reduce the space complexity.</text>
              <doc_id>49</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Instead of estimating probabilities P (f|e), Bayesian learning tries to draw samples from plaintext sequences given ciphertext bigrams.</text>
              <doc_id>50</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>During sampling, the probability of any possible plaintext sample e 1 e 2 is given as:</text>
              <doc_id>51</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P sample (e 1 e 2 ) = P (e 1 e 2 ) 2&#8719;</text>
              <doc_id>52</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P bayes (f i |e i )</text>
              <doc_id>53</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1</text>
              <doc_id>54</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>with P bayes (f i |e i ) defined as:</text>
              <doc_id>55</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P bayes (f i |e i ) = &#945;P 0(f i |e i ) + count(f i , e i ) &#945; + count(e i )</text>
              <doc_id>56</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where P 0 is a base distribution, and &#945; is a parameter that controls how much we trust P 0 .</text>
              <doc_id>57</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>count(f i , e i ) and count(e i ) record the number of times f i , e i and e i appear in previously generated samples respectively.</text>
              <doc_id>58</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>At the end of sampling, P (f i |e i ) is estimated by:</text>
              <doc_id>59</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (f i |e i ) = count(f i, e i ) count(e i )</text>
              <doc_id>60</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>However, Bayesian decipherment is still very slow with Gibbs sampling (Geman and Geman, 1987), as each sampling step requires considering V e possibilities.</text>
              <doc_id>61</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Dou and Knight (2012) solve the problem by introducing slice sampling (Neal, 2000) to Bayesian decipherment.</text>
              <doc_id>62</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 From Adjacent Bigrams to Dependency Bigrams</title>
        <text>A major limitation of work by Dou and Knight (2012) is their monotonic generative story for deciphering adjacent bigrams. While the generation process works well for deciphering similar languages (e.g. Spanish and French) without considering reordering, it does not work well for languages that are more different in grammar and word order (e.g. Spanish and English). In this section, we first look at why adjacent bigrams are bad for decipherment. Then we describe how to use syntax to solve the problem.
The left column in Table 1 contains adjacent bigrams extracted from the Spanish phrase &#8220;misi&#243;n de naciones unidas en oriente medio&#8221;. The correct decipherment for the bigram &#8220;naciones unidas&#8221; should be &#8220;united nations&#8221;. Since the deciphering model described by Dou and Knight (2012) does not consider word reordering, it needs to decipher the bigram into &#8220;nations united&#8221; in order to get the right word translations &#8220;naciones&#8221;&#8594;&#8220;nations&#8221; and &#8220;unidas&#8221;&#8594;&#8220;united&#8221;. However, the English language model used for decipherment is built from English adjacent bigrams, so it strongly disprefers &#8220;nations united&#8221; and is not likely to produce a sensible decipherment for &#8220;naciones unidas&#8221;. The Spanish bigram &#8220;oriente medio&#8221; poses the same problem. Thus, without considering word reordering, the model described by Dou and Knight (2012) is not a good fit for deciphering Spanish into English.
However, if we extract bigrams based on dependency relations for both languages, the model fits better. To extract such bigrams, we first use dependency parsers to parse both languages, and extract bigrams by putting head word first, followed by the modifier. 1 We call these dependency bigrams. The right column in Table 1 lists examples of Spanish dependency bigrams extracted from the same Spanish phrase. With a language model built with English dependency bigrams, the same model used for deciphering adjacent bigrams is able to decipher Spanish dependency bigram &#8220;naciones(head) unidas(modifier)&#8221; into &#8220;nations(head) united(modifier)&#8221;. We might instead propose to consider word reordering when deciphering adjacent bigrams (e.g. add an operation to swap tokens in a bigram). However, using dependency bigrams has the following advantages:
&#8226; First, using dependency bigrams avoids complicating the model, keeping deciphering efficient and scalable.
&#8226; Second, it addresses the problem of long distance reordering, which can not be modeled by swapping tokens in bigrams.
Furthermore, using dependency bigrams allows us to use dependency types to further
1 As use of &#8220;del&#8221; and &#8220;de&#8221; in Spanish is much more frequent
than the use of &#8220;of&#8221; in English, we skip those words by using their head words as new heads if any of them serves as a head.
improve decipherment. Suppose we have a Spanish dependency bigram &#8220;accept&#243;(verb) solicitud(object)&#8221;. Then all of the following English dependency bigrams are possible decipherments: &#8220;accepted(verb) UN(subject)&#8221;, &#8220;accepted(verb) government(subject)&#8221;, &#8220;accepted(verb) request(object)&#8221;. However, if we know the type of the Spanish dependency bigram and use a language model built with the same type in English, the only possible decipherment is &#8220;accepted(verb) request(object)&#8221;. If we limit the search space, a system is more likely to find a better decipherment.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>A major limitation of work by Dou and Knight (2012) is their monotonic generative story for deciphering adjacent bigrams.</text>
              <doc_id>63</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>While the generation process works well for deciphering similar languages (e.g. Spanish and French) without considering reordering, it does not work well for languages that are more different in grammar and word order (e.g. Spanish and English).</text>
              <doc_id>64</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In this section, we first look at why adjacent bigrams are bad for decipherment.</text>
              <doc_id>65</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Then we describe how to use syntax to solve the problem.</text>
              <doc_id>66</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The left column in Table 1 contains adjacent bigrams extracted from the Spanish phrase &#8220;misi&#243;n de naciones unidas en oriente medio&#8221;.</text>
              <doc_id>67</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The correct decipherment for the bigram &#8220;naciones unidas&#8221; should be &#8220;united nations&#8221;.</text>
              <doc_id>68</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Since the deciphering model described by Dou and Knight (2012) does not consider word reordering, it needs to decipher the bigram into &#8220;nations united&#8221; in order to get the right word translations &#8220;naciones&#8221;&#8594;&#8220;nations&#8221; and &#8220;unidas&#8221;&#8594;&#8220;united&#8221;.</text>
              <doc_id>69</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>However, the English language model used for decipherment is built from English adjacent bigrams, so it strongly disprefers &#8220;nations united&#8221; and is not likely to produce a sensible decipherment for &#8220;naciones unidas&#8221;.</text>
              <doc_id>70</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The Spanish bigram &#8220;oriente medio&#8221; poses the same problem.</text>
              <doc_id>71</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Thus, without considering word reordering, the model described by Dou and Knight (2012) is not a good fit for deciphering Spanish into English.</text>
              <doc_id>72</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>However, if we extract bigrams based on dependency relations for both languages, the model fits better.</text>
              <doc_id>73</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To extract such bigrams, we first use dependency parsers to parse both languages, and extract bigrams by putting head word first, followed by the modifier.</text>
              <doc_id>74</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>1 We call these dependency bigrams.</text>
              <doc_id>75</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The right column in Table 1 lists examples of Spanish dependency bigrams extracted from the same Spanish phrase.</text>
              <doc_id>76</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>With a language model built with English dependency bigrams, the same model used for deciphering adjacent bigrams is able to decipher Spanish dependency bigram &#8220;naciones(head) unidas(modifier)&#8221; into &#8220;nations(head) united(modifier)&#8221;.</text>
              <doc_id>77</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We might instead propose to consider word reordering when deciphering adjacent bigrams (e.g. add an operation to swap tokens in a bigram).</text>
              <doc_id>78</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>However, using dependency bigrams has the following advantages:</text>
              <doc_id>79</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; First, using dependency bigrams avoids complicating the model, keeping deciphering efficient and scalable.</text>
              <doc_id>80</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; Second, it addresses the problem of long distance reordering, which can not be modeled by swapping tokens in bigrams.</text>
              <doc_id>81</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Furthermore, using dependency bigrams allows us to use dependency types to further</text>
              <doc_id>82</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 As use of &#8220;del&#8221; and &#8220;de&#8221; in Spanish is much more frequent</text>
              <doc_id>83</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>than the use of &#8220;of&#8221; in English, we skip those words by using their head words as new heads if any of them serves as a head.</text>
              <doc_id>84</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>improve decipherment.</text>
              <doc_id>85</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Suppose we have a Spanish dependency bigram &#8220;accept&#243;(verb) solicitud(object)&#8221;.</text>
              <doc_id>86</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Then all of the following English dependency bigrams are possible decipherments: &#8220;accepted(verb) UN(subject)&#8221;, &#8220;accepted(verb) government(subject)&#8221;, &#8220;accepted(verb) request(object)&#8221;.</text>
              <doc_id>87</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>However, if we know the type of the Spanish dependency bigram and use a language model built with the same type in English, the only possible decipherment is &#8220;accepted(verb) request(object)&#8221;.</text>
              <doc_id>88</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>If we limit the search space, a system is more likely to find a better decipherment.</text>
              <doc_id>89</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Deciphering Spanish Gigaword</title>
        <text>In this section, we compare dependency bigrams with adjacent bigrams for deciphering Spanish into English.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this section, we compare dependency bigrams with adjacent bigrams for deciphering Spanish into English.</text>
              <doc_id>90</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Data</title>
            <text>We use the Gigaword corpus for our decipherment experiments. The corpus contains news articles from different news agencies and is available in Spanish and English. We use only the AFP (Agence France- Presse) section of the corpus in decipherment experiments. We tokenize the corpus using tools that come with the Europarl corpus (Koehn, 2005). To shorten the time required for running different systems on large amounts of data, we keep only the top 5000 most frequent word types in both languages and replace all other word types with UNK. We also throw away lines with more than 40 tokens, as the Spanish parser (Bohnet, 2010) we use is slow when processing long sentences. After preprocessing, the corpus contains approximately 440 million tokens in Spanish and 350 million tokens in English. To obtain dependency bigrams, we use the Bohnet parsers (Bohnet, 2010) to parse both the Spanish and English version of the corpus.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We use the Gigaword corpus for our decipherment experiments.</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The corpus contains news articles from different news agencies and is available in Spanish and English.</text>
                  <doc_id>92</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We use only the AFP (Agence France- Presse) section of the corpus in decipherment experiments.</text>
                  <doc_id>93</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We tokenize the corpus using tools that come with the Europarl corpus (Koehn, 2005).</text>
                  <doc_id>94</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>To shorten the time required for running different systems on large amounts of data, we keep only the top 5000 most frequent word types in both languages and replace all other word types with UNK.</text>
                  <doc_id>95</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We also throw away lines with more than 40 tokens, as the Spanish parser (Bohnet, 2010) we use is slow when processing long sentences.</text>
                  <doc_id>96</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>After preprocessing, the corpus contains approximately 440 million tokens in Spanish and 350 million tokens in English.</text>
                  <doc_id>97</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>To obtain dependency bigrams, we use the Bohnet parsers (Bohnet, 2010) to parse both the Spanish and English version of the corpus.</text>
                  <doc_id>98</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Systems</title>
            <text>Three systems are evaluated in the experiments. We implement a baseline system, Adjacent, based on Dou and Knight (2012). The baseline system collects adjacent bigrams and their counts from Spanish and English texts. It then builds an English bigram language model using the English adjacent bigrams and uses it to decipher the Spanish adjacent bigrams. Group 1 Group 2
Group 3
Dependency Types Verb/Subject Preposition/Preposition-Object, Noun/Noun-Modifier Verb/Noun-Object
We build the second system, Dependency, using dependency bigrams for decipherment. As the two parsers do not output the same set of dependency relations, we cannot extract all types of dependency bigrams. Instead, we select a subset of dependency bigrams whose dependency relations are shared by the two parser outputs. The selected dependency relations are: Verb/Subject, Verb/Noun-Object, Preposition/Object, Noun/Modifier. Decipherment runs the same way as in the baseline system. The third system, DepType, is built using both dependent bigrams and their dependency types. We first extract dependency bigrams for both languages, then group them based on their dependency types. As both parsers treat noun phrases dependent on &#8220;del&#8221;, &#8220;de&#8221;, and &#8220;of&#8221; as prepositional phrases, we choose to divide the dependency bigrams into 3 groups and list them in Table 2. A separate language model is built for each group of English dependency bigrams and used to decipher the group of Spanish dependency bigrams with same dependency type.
For all the systems, language models are built using the SRILM toolkit (Stolcke, 2002). For the Adjacent system, we use Good-Turing smoothing. For the other systems, we use a mix of Witten-Bell and Good-Turing smoothing.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Three systems are evaluated in the experiments.</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We implement a baseline system, Adjacent, based on Dou and Knight (2012).</text>
                  <doc_id>100</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The baseline system collects adjacent bigrams and their counts from Spanish and English texts.</text>
                  <doc_id>101</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>It then builds an English bigram language model using the English adjacent bigrams and uses it to decipher the Spanish adjacent bigrams.</text>
                  <doc_id>102</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Group 1 Group 2</text>
                  <doc_id>103</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Group 3</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Dependency Types Verb/Subject Preposition/Preposition-Object, Noun/Noun-Modifier Verb/Noun-Object</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We build the second system, Dependency, using dependency bigrams for decipherment.</text>
                  <doc_id>106</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As the two parsers do not output the same set of dependency relations, we cannot extract all types of dependency bigrams.</text>
                  <doc_id>107</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Instead, we select a subset of dependency bigrams whose dependency relations are shared by the two parser outputs.</text>
                  <doc_id>108</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The selected dependency relations are: Verb/Subject, Verb/Noun-Object, Preposition/Object, Noun/Modifier.</text>
                  <doc_id>109</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Decipherment runs the same way as in the baseline system.</text>
                  <doc_id>110</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The third system, DepType, is built using both dependent bigrams and their dependency types.</text>
                  <doc_id>111</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>We first extract dependency bigrams for both languages, then group them based on their dependency types.</text>
                  <doc_id>112</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>As both parsers treat noun phrases dependent on &#8220;del&#8221;, &#8220;de&#8221;, and &#8220;of&#8221; as prepositional phrases, we choose to divide the dependency bigrams into 3 groups and list them in Table 2.</text>
                  <doc_id>113</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>A separate language model is built for each group of English dependency bigrams and used to decipher the group of Spanish dependency bigrams with same dependency type.</text>
                  <doc_id>114</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For all the systems, language models are built using the SRILM toolkit (Stolcke, 2002).</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For the Adjacent system, we use Good-Turing smoothing.</text>
                  <doc_id>116</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For the other systems, we use a mix of Witten-Bell and Good-Turing smoothing.</text>
                  <doc_id>117</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>4.3 Sampling Procedure</title>
            <text>In experiments, we find that the iterative sampling method described by Dou and Knight (2012) helps improve deciphering accuracy. We also find that combining results from different decipherments helps find more correct translations at each iteration. Thus, instead of using a single sampling process, we use 10 different sampling processes at each iteration. The details of the new sampling procedure are provided here:
&#8226; Extract dependency bigrams from parsing outputs and collect their counts.
&#8226; Keep bigrams whose counts are greater than a threshold &#945;. Then start 10 different randomly seeded and initialized sampling processes. Perform sampling.
&#8226; At the end of sampling, extract word translation pairs (f, e) from the final sample. Estimate translation probabilities P (e|f) for each pair. Then construct a translation table by keeping translation pairs (f, e) seen in more than one decipherment and use the average P (e|f) as the new translation probability.
&#8226; Lower the threshold &#945; to include more bigrams into the sampling process. Start 10 different sampling processes again and initialize the first sample using the translation pairs obtained from the previous step (for each Spanish token f, choose an English token e whose P (e|f) is the highest). Perform sampling again.
&#8226; Repeat until &#945; = 1.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In experiments, we find that the iterative sampling method described by Dou and Knight (2012) helps improve deciphering accuracy.</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We also find that combining results from different decipherments helps find more correct translations at each iteration.</text>
                  <doc_id>119</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Thus, instead of using a single sampling process, we use 10 different sampling processes at each iteration.</text>
                  <doc_id>120</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The details of the new sampling procedure are provided here:</text>
                  <doc_id>121</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Extract dependency bigrams from parsing outputs and collect their counts.</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Keep bigrams whose counts are greater than a threshold &#945;.</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Then start 10 different randomly seeded and initialized sampling processes.</text>
                  <doc_id>124</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Perform sampling.</text>
                  <doc_id>125</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; At the end of sampling, extract word translation pairs (f, e) from the final sample.</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Estimate translation probabilities P (e|f) for each pair.</text>
                  <doc_id>127</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Then construct a translation table by keeping translation pairs (f, e) seen in more than one decipherment and use the average P (e|f) as the new translation probability.</text>
                  <doc_id>128</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Lower the threshold &#945; to include more bigrams into the sampling process.</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Start 10 different sampling processes again and initialize the first sample using the translation pairs obtained from the previous step (for each Spanish token f, choose an English token e whose P (e|f) is the highest).</text>
                  <doc_id>130</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Perform sampling again.</text>
                  <doc_id>131</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Repeat until &#945; = 1.</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>4.4 Deciphering Accuracy</title>
            <text>We choose the first 1000 lines of the monolingual Spanish texts as our test data. The data contains 37,505 tokens and 6556 word types. We use type accuracy as our evaluation metric: Given a word type f in Spanish, we find a translation pair (f, e) with the highest average P (e|f) from the translation table learned through decipherment. If the translation pair (f, e) can also be found in a gold translation lexicon T gold , we treat the word type f as correctly deciphered. Let |C| be the number of word types correctly deciphered, and |V | be the total number of word types evaluated. We define type accuracy as
|C| |V | &#7786;o create T gold, we use GIZA (Och and Ney,
2003) to align a small amount of Spanish-English parallel text (1 million tokens for each language), and use the lexicon derived from the alignment as our gold translation lexicon. T gold contains a subset of 4408 types seen in the test data, among which, 2878 are also top 5000 frequent word types.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We choose the first 1000 lines of the monolingual Spanish texts as our test data.</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The data contains 37,505 tokens and 6556 word types.</text>
                  <doc_id>134</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We use type accuracy as our evaluation metric: Given a word type f in Spanish, we find a translation pair (f, e) with the highest average P (e|f) from the translation table learned through decipherment.</text>
                  <doc_id>135</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>If the translation pair (f, e) can also be found in a gold translation lexicon T gold , we treat the word type f as correctly deciphered.</text>
                  <doc_id>136</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Let |C| be the number of word types correctly deciphered, and |V | be the total number of word types evaluated.</text>
                  <doc_id>137</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We define type accuracy as</text>
                  <doc_id>138</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>|C| |V | &#7786;o create T gold, we use GIZA (Och and Ney,</text>
                  <doc_id>139</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2003) to align a small amount of Spanish-English parallel text (1 million tokens for each language), and use the lexicon derived from the alignment as our gold translation lexicon.</text>
                  <doc_id>140</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>T gold contains a subset of 4408 types seen in the test data, among which, 2878 are also top 5000 frequent word types.</text>
                  <doc_id>141</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>4</index>
            <title>4.5 Results</title>
            <text>During decipherment, we gradually increase the size of Spanish texts and compare the learning curves of three deciphering systems in Figure 2.
With 100k tokens of Spanish text, the performance of the three systems are similar. However, the learning curve of Adjacent plateaus quickly, while those of the dependency based systems soar up as more data becomes available and still rise sharply when the size of Spanish texts increases to 10 million tokens, where the DepType system improves deciphering accuracy of the Adjacent system from 4.2% to 24.6%. In the end, with 100 million tokens, the accuracy of the DepType system rises to 27.0%. The accuracy is even higher (41%), when evaluated against the top 5000 frequent word types only.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>During decipherment, we gradually increase the size of Spanish texts and compare the learning curves of three deciphering systems in Figure 2.</text>
                  <doc_id>142</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>With 100k tokens of Spanish text, the performance of the three systems are similar.</text>
                  <doc_id>143</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>However, the learning curve of Adjacent plateaus quickly, while those of the dependency based systems soar up as more data becomes available and still rise sharply when the size of Spanish texts increases to 10 million tokens, where the DepType system improves deciphering accuracy of the Adjacent system from 4.2% to 24.6%.</text>
                  <doc_id>144</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In the end, with 100 million tokens, the accuracy of the DepType system rises to 27.0%.</text>
                  <doc_id>145</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The accuracy is even higher (41%), when evaluated against the top 5000 frequent word types only.</text>
                  <doc_id>146</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Improving Machine Translation with Decipherment</title>
        <text>In this section, we demonstrate how to use a translation lexicon learned by deciphering large amounts of in-domain (news) monolingual data to improve a phrase-based machine translation system trained with limited out-of-domain (politics) parallel data.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this section, we demonstrate how to use a translation lexicon learned by deciphering large amounts of in-domain (news) monolingual data to improve a phrase-based machine translation system trained with limited out-of-domain (politics) parallel data.</text>
              <doc_id>147</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Data</title>
            <text>We use approximately one million tokens of the Europarl corpus (Koehn, 2005) as our small out-ofdomain parallel training data and Gigaword as our large in-domain monolingual training data to build language models and a new translation lexicon to improve a phrase-based MT baseline system. For tuning and testing, we use the development data
from the NAACL 2012 workshop on statistical machine translation. The data contains test data in the news domain from the 2008, 2009, 2010, and 2011 workshops. We use the 2008 test data for tuning and the rest for testing. The sizes of the training, tuning, and testing sets are listed in Table 3.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We use approximately one million tokens of the Europarl corpus (Koehn, 2005) as our small out-ofdomain parallel training data and Gigaword as our large in-domain monolingual training data to build language models and a new translation lexicon to improve a phrase-based MT baseline system.</text>
                  <doc_id>148</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For tuning and testing, we use the development data</text>
                  <doc_id>149</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>from the NAACL 2012 workshop on statistical machine translation.</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The data contains test data in the news domain from the 2008, 2009, 2010, and 2011 workshops.</text>
                  <doc_id>151</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We use the 2008 test data for tuning and the rest for testing.</text>
                  <doc_id>152</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The sizes of the training, tuning, and testing sets are listed in Table 3.</text>
                  <doc_id>153</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Systems</title>
            <text>5.2.1 Baseline Machine Translation System
We build a state-of-the-art phrase-based MT system, PBMT, using Moses (Koehn et al., 2007). PBMT has 3 models: a translation model, a distortion model, and a language model. We build a 5- gram language model using the AFP section of the English Gigaword. We train the other models using the Europarl corpus. By default, Moses uses the following 8 features to score a candidate translation:
&#8226; direct and inverse translation probabilities
&#8226; direct and inverse lexical weighting
&#8226; a language model score
&#8226; a distortion score
&#8226; phrase penalty
&#8226; word penalty
The 8 features have weights adjusted on the tuning data using minimum error rate training (MERT) (Och, 2003). PBMT has a phrase table T phrase . During decoding, Moses copies out-of-vocabulary (OOV) words, which can not be found in T phrase , directly to output. In the following sections, we describe how to use a translation lexicon learned from large amounts of non parallel data to improve translation of OOV words, as well as words observed in T phrase .
5.2.2 Decipherment for Machine Translation
To achieve better decipherment, we:
&#8226; Increase the size of Spanish ciphertext from 100 million tokens to 894 million tokens.
&#8226; Keep top 50k instead of top 5k most frequent word types of the ciphertext.
&#8226; Instead of seeding the sampling process randomly, we use a translation lexicon learned from a limited amount of parallel data as seed: For each Spanish dependency bigram f 1 , f 2 , where both f 1 and f 2 are found in the seed lexicon, we find the English sequence e 1 , e 2 that maximizes P (e 1 , e 2 )P (e 1 |f 1 )P (e 2 |f 2 ). Otherwise, for any Spanish token f that can be found in the seed lexicon, we choose English word e, where P (e|f) is the highest as the initial sample; for any f that are not seen in the seed lexicon, we do random initialization.
We perform 20 random restarts with 10k iterations on each and build a word-to-word translation lexicon T decipher by collecting translation pairs seen in at least 3 final decipherments with either P (f|e) &#8805; 0.2 or P (e|f) &#8805; 0.2.
5.2.3 Improving Translation of Observed Words with Decipherment
To improve translation of words observed in our parallel corpus, we simply use T decipher as an additional parallel corpus. First, we filter T decipher by keeping only translation pairs (f, e), where f is observed in the Spanish part and e is observed in the English part of the parallel corpus. Then we append all the Spanish and English words in the filtered T decipher to the end of Spanish part and English part of the parallel corpus respectively. The training and tuning process is the same as the baseline machine translation system PBMT. We denote this system as Decipher-OBSV.
5.2.4 Improving OOV translation with Decipherment
As T decipher is learned from large amounts of indomain monolingual data, we expect that T decipher contains a number of useful translations for words not seen in the limited amount of parallel data (OOV words). Instead of copying OOV words directly to output, which is what Moses does by default, we try to find translations from T decipher to improve translation.
During decoding, if a source word f is in T phrase , its translation options are collected from T phrase exclusively. If f is not in T phrase but in T decipher , the decoder will find translations from T decipher . If f is not in either translation table, the decoder just copies it directly to the output. We call this system Decipher-OOV. However, when an OOV&#8217;s correct translation is same as its surface form and all its possible translations in T decipher are wrong, it is better to just copy OOV words directly to output. This scenario happens frequently, as Spanish and English share many common words. To avoid over trusting T decipher , we add a new translation pair (f, f) for each source word f in T decipher if the translation pair (f, f) is not originally in T decipher . For each newly added translation pair, both of its log translation probabilities are set to 0. To distinguish the added translation pairs from the others learned through decipherment, we add a binary feature &#952; to each translation pair in T decipher . The final version of T decipher has three feature scores: P (e|f), P (f|e), and &#952;. Finally, we tune weights of the features in T decipher using MERT (Och, 2003) on the tuning set.
5.2.5 A Combined Approach
In the end, we build a system Decipher-COMB, which uses T decipher to improve translation of both observed and OOV words with methods described in sections 5.2.3 and 5.2.4.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>5.2.1 Baseline Machine Translation System</text>
                  <doc_id>154</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We build a state-of-the-art phrase-based MT system, PBMT, using Moses (Koehn et al., 2007).</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>PBMT has 3 models: a translation model, a distortion model, and a language model.</text>
                  <doc_id>156</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We build a 5- gram language model using the AFP section of the English Gigaword.</text>
                  <doc_id>157</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We train the other models using the Europarl corpus.</text>
                  <doc_id>158</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>By default, Moses uses the following 8 features to score a candidate translation:</text>
                  <doc_id>159</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; direct and inverse translation probabilities</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; direct and inverse lexical weighting</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; a language model score</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; a distortion score</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; phrase penalty</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; word penalty</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The 8 features have weights adjusted on the tuning data using minimum error rate training (MERT) (Och, 2003).</text>
                  <doc_id>166</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>PBMT has a phrase table T phrase .</text>
                  <doc_id>167</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>During decoding, Moses copies out-of-vocabulary (OOV) words, which can not be found in T phrase , directly to output.</text>
                  <doc_id>168</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In the following sections, we describe how to use a translation lexicon learned from large amounts of non parallel data to improve translation of OOV words, as well as words observed in T phrase .</text>
                  <doc_id>169</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5.2.2 Decipherment for Machine Translation</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To achieve better decipherment, we:</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Increase the size of Spanish ciphertext from 100 million tokens to 894 million tokens.</text>
                  <doc_id>172</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Keep top 50k instead of top 5k most frequent word types of the ciphertext.</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Instead of seeding the sampling process randomly, we use a translation lexicon learned from a limited amount of parallel data as seed: For each Spanish dependency bigram f 1 , f 2 , where both f 1 and f 2 are found in the seed lexicon, we find the English sequence e 1 , e 2 that maximizes P (e 1 , e 2 )P (e 1 |f 1 )P (e 2 |f 2 ).</text>
                  <doc_id>174</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Otherwise, for any Spanish token f that can be found in the seed lexicon, we choose English word e, where P (e|f) is the highest as the initial sample; for any f that are not seen in the seed lexicon, we do random initialization.</text>
                  <doc_id>175</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We perform 20 random restarts with 10k iterations on each and build a word-to-word translation lexicon T decipher by collecting translation pairs seen in at least 3 final decipherments with either P (f|e) &#8805; 0.2 or P (e|f) &#8805; 0.2.</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5.2.3 Improving Translation of Observed Words with Decipherment</text>
                  <doc_id>177</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To improve translation of words observed in our parallel corpus, we simply use T decipher as an additional parallel corpus.</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>First, we filter T decipher by keeping only translation pairs (f, e), where f is observed in the Spanish part and e is observed in the English part of the parallel corpus.</text>
                  <doc_id>179</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Then we append all the Spanish and English words in the filtered T decipher to the end of Spanish part and English part of the parallel corpus respectively.</text>
                  <doc_id>180</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The training and tuning process is the same as the baseline machine translation system PBMT.</text>
                  <doc_id>181</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We denote this system as Decipher-OBSV.</text>
                  <doc_id>182</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5.2.4 Improving OOV translation with Decipherment</text>
                  <doc_id>183</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>As T decipher is learned from large amounts of indomain monolingual data, we expect that T decipher contains a number of useful translations for words not seen in the limited amount of parallel data (OOV words).</text>
                  <doc_id>184</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Instead of copying OOV words directly to output, which is what Moses does by default, we try to find translations from T decipher to improve translation.</text>
                  <doc_id>185</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>During decoding, if a source word f is in T phrase , its translation options are collected from T phrase exclusively.</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If f is not in T phrase but in T decipher , the decoder will find translations from T decipher .</text>
                  <doc_id>187</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>If f is not in either translation table, the decoder just copies it directly to the output.</text>
                  <doc_id>188</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We call this system Decipher-OOV.</text>
                  <doc_id>189</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>However, when an OOV&#8217;s correct translation is same as its surface form and all its possible translations in T decipher are wrong, it is better to just copy OOV words directly to output.</text>
                  <doc_id>190</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This scenario happens frequently, as Spanish and English share many common words.</text>
                  <doc_id>191</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>To avoid over trusting T decipher , we add a new translation pair (f, f) for each source word f in T decipher if the translation pair (f, f) is not originally in T decipher .</text>
                  <doc_id>192</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>For each newly added translation pair, both of its log translation probabilities are set to 0.</text>
                  <doc_id>193</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>To distinguish the added translation pairs from the others learned through decipherment, we add a binary feature &#952; to each translation pair in T decipher .</text>
                  <doc_id>194</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>The final version of T decipher has three feature scores: P (e|f), P (f|e), and &#952;.</text>
                  <doc_id>195</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, we tune weights of the features in T decipher using MERT (Och, 2003) on the tuning set.</text>
                  <doc_id>196</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5.2.5 A Combined Approach</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In the end, we build a system Decipher-COMB, which uses T decipher to improve translation of both observed and OOV words with methods described in sections 5.2.3 and 5.2.4.</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>5.3 Results</title>
            <text>We tune each system three times with MERT and choose the best weights based on BLEU scores on tuning set.
Table 4 shows that the translation lexicon learned from decipherment helps achieve higher BLEU scores across tuning and testing sets. Decipher- OBSV improves BLEU scores by as much as 1.2 points. We analyze the results and find the gain mainly comes from two parts. First, adding T decipher to small amounts of parallel corpus improves word level translation probabilities, which lead to better lexical weighting; second, T decipher contains new alternative translations for words observed in the parallel corpus. Moreover, Decipher-OOV also achieves better BLEU scores compared with PBMT across all tuning and test sets. We also observe that systems using T decipher learned by deciphering dependency bigrams leads to larger gains in BLEU scores. When decipherment is used to improve translation of both observed and OOV words, we see improvement in BLEU score as high as 1.8 points on the 2010 news test set.
The consistent improvement on the tuning and different testing data suggests that decipherment is capable of learning good translations for a number of OOV words. To further demonstrate that our decipherment approach finds useful translations for OOV words, we list the top 10 most frequent OOV words from both the tuning set and testing set as well as their translations (up to three most likely translations) in Table 5. P (e|f) and P (f|e) are average scores over different decipherment runs. From the table, we can see that decipherment finds correct translations (bolded) for 7 out of the 10 most frequent OOV words. Moreover, many OOVs and their correct translations are homographs , which makes copying OOVs directly to the output a strong baseline to beat. Nonetheless, decipherment still finds enough correct translations to improve the baseline.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We tune each system three times with MERT and choose the best weights based on BLEU scores on tuning set.</text>
                  <doc_id>199</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 4 shows that the translation lexicon learned from decipherment helps achieve higher BLEU scores across tuning and testing sets.</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Decipher- OBSV improves BLEU scores by as much as 1.2 points.</text>
                  <doc_id>201</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We analyze the results and find the gain mainly comes from two parts.</text>
                  <doc_id>202</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>First, adding T decipher to small amounts of parallel corpus improves word level translation probabilities, which lead to better lexical weighting; second, T decipher contains new alternative translations for words observed in the parallel corpus.</text>
                  <doc_id>203</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Moreover, Decipher-OOV also achieves better BLEU scores compared with PBMT across all tuning and test sets.</text>
                  <doc_id>204</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We also observe that systems using T decipher learned by deciphering dependency bigrams leads to larger gains in BLEU scores.</text>
                  <doc_id>205</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>When decipherment is used to improve translation of both observed and OOV words, we see improvement in BLEU score as high as 1.8 points on the 2010 news test set.</text>
                  <doc_id>206</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The consistent improvement on the tuning and different testing data suggests that decipherment is capable of learning good translations for a number of OOV words.</text>
                  <doc_id>207</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To further demonstrate that our decipherment approach finds useful translations for OOV words, we list the top 10 most frequent OOV words from both the tuning set and testing set as well as their translations (up to three most likely translations) in Table 5.</text>
                  <doc_id>208</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>P (e|f) and P (f|e) are average scores over different decipherment runs.</text>
                  <doc_id>209</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>From the table, we can see that decipherment finds correct translations (bolded) for 7 out of the 10 most frequent OOV words.</text>
                  <doc_id>210</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Moreover, many OOVs and their correct translations are homographs , which makes copying OOVs directly to the output a strong baseline to beat.</text>
                  <doc_id>211</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Nonetheless, decipherment still finds enough correct translations to improve the baseline.</text>
                  <doc_id>212</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>6</index>
        <title>6 Conclusion</title>
        <text>We introduce syntax for deciphering Spanish into English. Experiment results show that using dependency bigrams improves decipherment accuracy by over 500% compared with the state-of-the-art approach. Moreover, we learn a domain specific translation lexicon by deciphering large amounts of monolingual data and show that the lexicon can improve a baseline machine translation system trained with limited parallel data.
Adjacent
Dependency</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We introduce syntax for deciphering Spanish into English.</text>
              <doc_id>213</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Experiment results show that using dependency bigrams improves decipherment accuracy by over 500% compared with the state-of-the-art approach.</text>
              <doc_id>214</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, we learn a domain specific translation lexicon by deciphering large amounts of monolingual data and show that the lexicon can improve a baseline machine translation system trained with limited parallel data.</text>
              <doc_id>215</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Adjacent</text>
              <doc_id>216</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Dependency</text>
              <doc_id>217</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>7</index>
        <title>7 Acknowledgments</title>
        <text>of 10 most frequent OOV word types. This work was supported by NSF Grant 0904684 and ARO grant W911NF-10-1-0533. The authors would like to thank David Chiang, Malte Nuhn, Victoria Fossum, Ashish Vaswani, Ulf Hermjakob, Yang Gao, and Hui Zhang (in no particular order) for their comments and suggestions.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>of 10 most frequent OOV word types.</text>
              <doc_id>218</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This work was supported by NSF Grant 0904684 and ARO grant W911NF-10-1-0533.</text>
              <doc_id>219</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The authors would like to thank David Chiang, Malte Nuhn, Victoria Fossum, Ashish Vaswani, Ulf Hermjakob, Yang Gao, and Hui Zhang (in no particular order) for their comments and suggestions.</text>
              <doc_id>220</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Comparison of adjacent bigrams (left) and dependency bigrams (right) extracted from the same Spanish text</caption>
        <reference_text>In PAGE 3: ... Then we describe how to use syntax to solve the problem. The left column in  Table1  contains adjacent bi- grams extracted from the Spanish phrase  misi? on de naciones unidas en oriente medio . The cor- rect decipherment for the bigram  naciones unidas  should be  united nations ....  In PAGE 3: ...1 We call these dependency bi- grams. The right column in  Table1  lists exam- ples of Spanish dependency bigrams extracted from the same Spanish phrase. With a language model built with English dependency bigrams, the same model used for deciphering adjacent bigrams is able to decipher Spanish dependency bigram  na- ciones(head) unidas(modifier)  into  nations(head) united(modifier) ....</reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>misi&#243;n de naciones unidas en oriente medio</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>misi&#243;n de</cell>
              <cell>misi&#243;n naciones</cell>
            </row>
            <row>
              <cell>de naciones</cell>
              <cell>naciones unidas</cell>
            </row>
            <row>
              <cell>naciones unidas</cell>
              <cell>misi&#243;n en</cell>
            </row>
            <row>
              <cell>unidas en</cell>
              <cell>en oriente</cell>
            </row>
            <row>
              <cell>en oriente</cell>
              <cell>oriente medio</cell>
            </row>
            <row>
              <cell>oriente medio</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Size of training, tuning, and testing data in number of tokens</caption>
        <reference_text>None</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Parallel</cell>
              <cell>Parallel   Spanish</cell>
              <cell>English</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Europarl#@#@</cell>
              <cell>1.1 million#@#@Spanish</cell>
              <cell>1.0 million#@#@English</cell>
            </row>
            <row>
              <cell>Tune-2008#@#@Europarl</cell>
              <cell>52.6k#@#@1.1 million</cell>
              <cell>49.8k#@#@1.0 million</cell>
            </row>
            <row>
              <cell>Test-2009#@#@Tune-2008</cell>
              <cell>68.1k#@#@52.6k</cell>
              <cell>65.6k#@#@49.8k</cell>
            </row>
            <row>
              <cell>Test-2010#@#@Test-2009</cell>
              <cell>65.5k#@#@68.1k</cell>
              <cell>61.9k#@#@65.6k</cell>
            </row>
            <row>
              <cell>Test-2011#@#@Test-2010</cell>
              <cell>79.4k#@#@65.5k</cell>
              <cell>74.7k#@#@61.9k</cell>
            </row>
            <row>
              <cell>Test-2011</cell>
              <cell>Non Parallel#@#@79.4k</cell>
              <cell>74.7k</cell>
            </row>
            <row>
              <cell></cell>
              <cell>Spanish#@#@Non Parallel
Spanish</cell>
              <cell>English</cell>
            </row>
            <row>
              <cell>Gigaword</cell>
              <cell>894 million</cell>
              <cell>940 million</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: Systems that use translation lexicons learned from decipherment show consistent improvement over the baseline system across tuning and testing sets. The best system, Decipher-COMB, achieves as much as 1.8 BLEU point gain on the 2010 news test set.</caption>
        <reference_text>In PAGE 7: ...3 Results We tune each system three times with MERT and choose the best weights based on BLEU scores on tuning set.  Table4  shows that the translation lexicon learned from decipherment helps achieve higher BLEU scores across tuning and testing sets. Decipher- OBSV improves BLEU scores by as much as 1....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>Decipherment</cell>
              <cell>System</cell>
              <cell>Tune2008</cell>
              <cell>Test2009</cell>
              <cell>Test2010</cell>
              <cell>Test2011</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>PBMT (Baseline)</cell>
              <cell>19.1</cell>
              <cell>19.6</cell>
              <cell>21.3</cell>
              <cell>22.1</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Decipher-OBSV</cell>
              <cell>19.5</cell>
              <cell>20.1</cell>
              <cell>22.2</cell>
              <cell>22.6</cell>
            </row>
            <row>
              <cell>Adjacent</cell>
              <cell>Decipher-OOV</cell>
              <cell>19.4</cell>
              <cell>19.9</cell>
              <cell>21.7</cell>
              <cell>22.5</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Decipher-COMB</cell>
              <cell>19.5</cell>
              <cell>20.2</cell>
              <cell>22.3</cell>
              <cell>22.5</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Decipher-OBSV</cell>
              <cell>19.7</cell>
              <cell>20.5</cell>
              <cell>22.5</cell>
              <cell>23.0</cell>
            </row>
            <row>
              <cell>Dependency</cell>
              <cell>Decipher-OOV</cell>
              <cell>19.9</cell>
              <cell>20.4</cell>
              <cell>22.4</cell>
              <cell>22.9</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>Decipher-COMB</cell>
              <cell>20.0</cell>
              <cell>20.8</cell>
              <cell>23.1</cell>
              <cell>23.4</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TableSeer</source>
        <caption>Table 5: Decipherment finds correct translations for 7 out of 10 most frequent OOV word types.</caption>
        <reference_text>In PAGE 7: ... The consistent improvement on the tuning and different testing data suggests that decipherment is capable of learning good translations for a number of OOV words. To further demonstrate that our decipherment approach finds useful translations for OOV words, we list the top 10 most frequent OOV words from both the tuning set and testing set as well as their translations (up to three most likely transla- tions) in  Table5 . P(e|f) and P(f|e) are average scores over different deciphermentruns....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>Spanish</cell>
              <cell>English</cell>
              <cell>P(e|f)#@#@P (e|f)</cell>
              <cell>P(f|e)#@#@P (f |e)</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>obama</cell>
              <cell>his</cell>
              <cell>0.33</cell>
              <cell>0.01</cell>
            </row>
            <row>
              <cell></cell>
              <cell>bush</cell>
              <cell>0.27</cell>
              <cell>0.07</cell>
            </row>
            <row>
              <cell></cell>
              <cell>clinton</cell>
              <cell>0.23</cell>
              <cell>0.11</cell>
            </row>
            <row>
              <cell>bush</cell>
              <cell>bush</cell>
              <cell>0.47</cell>
              <cell>0.45</cell>
            </row>
            <row>
              <cell></cell>
              <cell>yeltsin</cell>
              <cell>0.28</cell>
              <cell>0.81</cell>
            </row>
            <row>
              <cell></cell>
              <cell>he</cell>
              <cell>0.24</cell>
              <cell>0.05</cell>
            </row>
            <row>
              <cell>festival</cell>
              <cell>event</cell>
              <cell>0.68</cell>
              <cell>0.35</cell>
            </row>
            <row>
              <cell></cell>
              <cell>festival</cell>
              <cell>0.61</cell>
              <cell>0.72</cell>
            </row>
            <row>
              <cell>wikileaks</cell>
              <cell>zeta</cell>
              <cell>0.03</cell>
              <cell>0.33</cell>
            </row>
            <row>
              <cell>venus</cell>
              <cell>venus</cell>
              <cell>0.61</cell>
              <cell>0.74</cell>
            </row>
            <row>
              <cell></cell>
              <cell>serena</cell>
              <cell>0.47</cell>
              <cell>0.62</cell>
            </row>
            <row>
              <cell>colchones</cell>
              <cell>mattresses</cell>
              <cell>0.55</cell>
              <cell>0.73</cell>
            </row>
            <row>
              <cell></cell>
              <cell>cars</cell>
              <cell>0.31</cell>
              <cell>0.01</cell>
            </row>
            <row>
              <cell>helado</cell>
              <cell>frigid</cell>
              <cell>0.52</cell>
              <cell>0.44</cell>
            </row>
            <row>
              <cell></cell>
              <cell>chill</cell>
              <cell>0.37</cell>
              <cell>0.14</cell>
            </row>
            <row>
              <cell></cell>
              <cell>sandwich</cell>
              <cell>0.42</cell>
              <cell>0.27</cell>
            </row>
            <row>
              <cell>google</cell>
              <cell>microsoft</cell>
              <cell>0.67</cell>
              <cell>0.18</cell>
            </row>
            <row>
              <cell></cell>
              <cell>google</cell>
              <cell>0.59</cell>
              <cell>0.69</cell>
            </row>
            <row>
              <cell>cantante</cell>
              <cell>singer</cell>
              <cell>0.44</cell>
              <cell>0.92</cell>
            </row>
            <row>
              <cell></cell>
              <cell>jackson</cell>
              <cell>0.14</cell>
              <cell>0.33</cell>
            </row>
            <row>
              <cell></cell>
              <cell>artists</cell>
              <cell>0.14</cell>
              <cell>0.77</cell>
            </row>
            <row>
              <cell>mccain</cell>
              <cell>mccain</cell>
              <cell>0.66</cell>
              <cell>0.92</cell>
            </row>
            <row>
              <cell></cell>
              <cell>it</cell>
              <cell>0.22</cell>
              <cell>0.00</cell>
            </row>
            <row>
              <cell></cell>
              <cell>he</cell>
              <cell>0.21</cell>
              <cell>0.00</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Shane Bergsma</author>
          <author>Benjamin Van Durme</author>
        </authors>
        <title>Learning bilingual lexicons using the visual similarity of labeled web images.</title>
        <publication>In Proceedings of the TwentySecond international joint conference on Artificial Intelligence - Volume Volume Three.</publication>
        <pages>None</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Bernd Bohnet</author>
        </authors>
        <title>Top accuracy and fast dependency parsing is not a contradiction.</title>
        <publication>In Proceedings of the 23rd International Conference on Computational Linguistics. Coling.</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Hal Daum&#233;</author>
          <author>Jagadeesh Jagarlamudi</author>
        </authors>
        <title>Domain adaptation for machine translation by mining unseen words.</title>
        <publication>In Proceedings of the 49th Annual Meeting of the Association</publication>
        <pages>None</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Qing Dou</author>
          <author>Kevin Knight</author>
        </authors>
        <title>Large scale decipherment for out-of-domain machine translation.</title>
        <publication>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics.</publication>
        <pages>None</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Pascale Fung</author>
          <author>Lo Yuen Yee</author>
        </authors>
        <title>An IR approach for translating new words from nonparallel, comparable texts.</title>
        <publication>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational 1675 - Volume 1. Association for Computational Linguistics.</publication>
        <pages>None</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Nikesh Garera</author>
          <author>Chris Callison-Burch</author>
          <author>David Yarowsky</author>
        </authors>
        <title>Improving translation lexicon induction from monolingual corpora via dependency contexts and part-of-speech equivalences.</title>
        <publication>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Stuart Geman</author>
          <author>Donald Geman</author>
        </authors>
        <title>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. In Readings in computer vision: issues, problems, principles, and paradigms.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1987</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Aria Haghighi</author>
          <author>Percy Liang</author>
          <author>Taylor Berg-Kirkpatrick</author>
          <author>Dan Klein</author>
        </authors>
        <title>Learning bilingual lexicons from monolingual corpora.</title>
        <publication>In Proceedings of ACL08: HLT. Association for Computational Linguistics.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Ann Irvine</author>
          <author>Chris Callison-Burch</author>
        </authors>
        <title>Combining bilingual and comparable corpora for low resource machine translation.</title>
        <publication>In Proceedings of the Eighth Workshop on Statistical Machine Translation. Association for Computational Linguistics,</publication>
        <pages>None</pages>
        <date>2013</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Ann Irvine</author>
          <author>Chris Callison-Burch</author>
        </authors>
        <title>Supervised bilingual lexicon induction with multiple monolingual signals.</title>
        <publication>In Proceedings of the 2013 Conference of the North American Chapter of the Association</publication>
        <pages>None</pages>
        <date>2013</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Alexandre Klementiev</author>
          <author>Ann Irvine</author>
          <author>Chris CallisonBurch</author>
          <author>David Yarowsky</author>
        </authors>
        <title>Toward statistical machine translation without parallel corpora.</title>
        <publication>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</publication>
        <pages>None</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Kevin Knight</author>
          <author>Anish Nair</author>
          <author>Nishit Rathod</author>
          <author>Kenji Yamada</author>
        </authors>
        <title>Unsupervised analysis for decipherment problems.</title>
        <publication>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions. Association for Computational Linguistics.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Kevin Knight</author>
        </authors>
        <title>Learning a translation lexicon from monolingual corpora.</title>
        <publication>In Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition. Association for Computational Linguistics.</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Hieu Hoang</author>
          <author>Alexandra Birch</author>
          <author>Chris Callison-Burch</author>
          <author>Marcello Federico</author>
          <author>Nicola Bertoldi</author>
          <author>Brooke Cowan</author>
          <author>Wade Shen</author>
          <author>Christine Moran</author>
          <author>Richard Zens</author>
        </authors>
        <title>Chris Dyer, Ond&#345;ej Bojar,</title>
        <publication>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. Association for Computational Linguistics.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Philipp Koehn</author>
        </authors>
        <title>Europarl: a parallel corpus for statistical machine translation. In</title>
        <publication>In Proceedings of the Tenth Machine Translation Summit,</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Radford Neal</author>
        </authors>
        <title>Slice sampling.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Malte Nuhn</author>
          <author>Arne Mauser</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Deciphering foreign language by combining language models and context vectors.</title>
        <publication>In Proceedings of the 50th Annual Meeting of the Association</publication>
        <pages>None</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A systematic comparison of various statistical alignment models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics.</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Reinhard Rapp</author>
        </authors>
        <title>Identifying word translations in non-parallel texts.</title>
        <publication>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics.</publication>
        <pages>None</pages>
        <date>1995</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Sujith Ravi</author>
          <author>Kevin Knight</author>
        </authors>
        <title>Deciphering foreign language.</title>
        <publication>In Proceedings of the 49th Annual Meeting of the Association</publication>
        <pages>None</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Sujith Ravi</author>
        </authors>
        <title>Scalable decipherment for machine translation via hash sampling.</title>
        <publication>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</publication>
        <pages>None</pages>
        <date>2013</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>1</reference_id>
        <string>Bohnet, 2010</string>
        <sentence_id>17345</sentence_id>
        <char_offset>74</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Bohnet, 2010</string>
        <sentence_id>17347</sentence_id>
        <char_offset>57</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Daum&#233; and Jagarlamudi, 2011</string>
        <sentence_id>17276</sentence_id>
        <char_offset>338</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>3</reference_id>
        <string>Dou and Knight, 2012</string>
        <sentence_id>17278</sentence_id>
        <char_offset>144</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>3</reference_id>
        <string>Dou and Knight (2012)</string>
        <sentence_id>17263</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>3</reference_id>
        <string>Dou and Knight (2012)</string>
        <sentence_id>17267</sentence_id>
        <char_offset>42</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>3</reference_id>
        <string>Dou and Knight (2012)</string>
        <sentence_id>17282</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>3</reference_id>
        <string>Dou and Knight (2012)</string>
        <sentence_id>17283</sentence_id>
        <char_offset>68</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>3</reference_id>
        <string>Dou and Knight (2012)</string>
        <sentence_id>17285</sentence_id>
        <char_offset>13</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>3</reference_id>
        <string>Dou and Knight (2012)</string>
        <sentence_id>17312</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>3</reference_id>
        <string>Dou and Knight (2012)</string>
        <sentence_id>17313</sentence_id>
        <char_offset>30</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>3</reference_id>
        <string>Dou and Knight (2012)</string>
        <sentence_id>17319</sentence_id>
        <char_offset>41</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>3</reference_id>
        <string>Dou and Knight (2012)</string>
        <sentence_id>17322</sentence_id>
        <char_offset>66</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>3</reference_id>
        <string>Dou and Knight (2012)</string>
        <sentence_id>17349</sentence_id>
        <char_offset>51</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>3</reference_id>
        <string>Dou and Knight (2012)</string>
        <sentence_id>17367</sentence_id>
        <char_offset>72</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>4</reference_id>
        <string>Fung and Yee, 1998</string>
        <sentence_id>17276</sentence_id>
        <char_offset>221</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>5</reference_id>
        <string>Garera et al., 2009</string>
        <sentence_id>17276</sentence_id>
        <char_offset>288</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>6</reference_id>
        <string>Geman and Geman, 1987</string>
        <sentence_id>17311</sentence_id>
        <char_offset>71</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>7</reference_id>
        <string>Haghighi et al., 2008</string>
        <sentence_id>17276</sentence_id>
        <char_offset>265</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>8</reference_id>
        <string>Irvine and Callison-Burch, 2013</string>
        <sentence_id>17276</sentence_id>
        <char_offset>367</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>8</reference_id>
        <string>Irvine and Callison-Burch, 2013</string>
        <sentence_id>17276</sentence_id>
        <char_offset>401</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>9</reference_id>
        <string>Irvine and Callison-Burch, 2013</string>
        <sentence_id>17276</sentence_id>
        <char_offset>367</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>9</reference_id>
        <string>Irvine and Callison-Burch, 2013</string>
        <sentence_id>17276</sentence_id>
        <char_offset>401</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>10</reference_id>
        <string>Klementiev et al. (2012)</string>
        <sentence_id>17257</sentence_id>
        <char_offset>73</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>11</reference_id>
        <string>Knight et al., 2006</string>
        <sentence_id>17297</sentence_id>
        <char_offset>63</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>12</reference_id>
        <string>Koehn and Knight, 2002</string>
        <sentence_id>17276</sentence_id>
        <char_offset>241</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>13</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>17404</sentence_id>
        <char_offset>71</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>14</reference_id>
        <string>Koehn, 2005</string>
        <sentence_id>17343</sentence_id>
        <char_offset>71</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>14</reference_id>
        <string>Koehn, 2005</string>
        <sentence_id>17397</sentence_id>
        <char_offset>64</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>15</reference_id>
        <string>Neal, 2000</string>
        <sentence_id>17312</sentence_id>
        <char_offset>71</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>16</reference_id>
        <string>Nuhn et al., 2012</string>
        <sentence_id>17278</sentence_id>
        <char_offset>166</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>18</reference_id>
        <string>Och, 2003</string>
        <sentence_id>17415</sentence_id>
        <char_offset>98</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>18</reference_id>
        <string>Och, 2003</string>
        <sentence_id>17445</sentence_id>
        <char_offset>67</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>19</reference_id>
        <string>Rapp, 1995</string>
        <sentence_id>17276</sentence_id>
        <char_offset>209</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>20</reference_id>
        <string>Ravi and Knight (2011)</string>
        <sentence_id>17280</sentence_id>
        <char_offset>61</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>20</reference_id>
        <string>Ravi and Knight (2011)</string>
        <sentence_id>17283</sentence_id>
        <char_offset>14</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>20</reference_id>
        <string>Ravi and Knight (2011)</string>
        <sentence_id>17299</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>20</reference_id>
        <string>Ravi and Knight, 2011</string>
        <sentence_id>17258</sentence_id>
        <char_offset>93</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>20</reference_id>
        <string>Ravi and Knight, 2011</string>
        <sentence_id>17278</sentence_id>
        <char_offset>121</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>21</reference_id>
        <string>Ravi, 2013</string>
        <sentence_id>17258</sentence_id>
        <char_offset>116</char_offset>
      </citation>
    </citations>
  </content>
</document>
