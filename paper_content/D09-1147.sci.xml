<PAPER>
  <FILENO/>
  <TITLE>Consensus Training for Consensus Decoding in Machine Translation</TITLE>
  <AUTHORS>
    <AUTHOR>Adam Pauls</AUTHOR>
    <AUTHOR>John DeNero</AUTHOR>
  </AUTHORS>
  <ABSTRACT>
    <A-S ID="S-8113">We propose a novel objective function for discriminatively tuning log-linear machine translation models.</A-S>
    <A-S ID="S-8114">Our objective explicitly optimizes the BLEU score of expected n-gram counts, the same quantities that arise in forestbased consensus and minimum Bayes risk decoding methods.</A-S>
    <A-S ID="S-8115">Our continuous objective can be optimized using simple gradient ascent.</A-S>
    <A-S ID="S-8116">However, computing critical quantities in the gradient necessitates a novel dynamic program, which we also present here.</A-S>
    <A-S ID="S-8117">Assuming BLEU as an evaluation measure, our objective function has two principle advantages over standard max BLEU tuning.</A-S>
    <A-S ID="S-8118">First, it specifically optimizes model weights for downstream consensus decoding procedures.</A-S>
    <A-S ID="S-8119">An unexpected second benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-8120">Increasing evidence suggests that machine translation decoders should not search for a single top scoring Viterbi derivation, but should instead choose a translation that is sensitive to the model&#8217;s entire predictive distribution.</S>
        <S ID="S-8121">Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (<REF ID="R-19" RPTR="31">Tromble et al., 2008</REF>; <REF ID="R-04" RPTR="6">DeNero et al., 2009</REF>; <REF ID="R-12" RPTR="21">Li et al., 2009</REF>; <REF ID="R-10" RPTR="18">Kumar et al., 2009</REF>).</S>
        <S ID="S-8122">This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly.</S>
        <S ID="S-8123">The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams (<REF ID="R-15" RPTR="26">Och, 2003</REF>).</S>
        <S ID="S-8124">We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference translations.</S>
        <S ID="S-8125">To maintain consistency across the translation pipeline, we formulate CoBLEU to share the functional form of BLEU used for evaluation.</S>
        <S ID="S-8126">As a result, CoBLEU optimizes exactly the quantities that drive efficient consensus decoding techniques and precisely mirrors the objective used for fast consensus decoding in <REF ID="R-04" RPTR="8">DeNero et al. (2009)</REF>.</S>
        <S ID="S-8127">CoBLEU is a continuous and (mostly) differentiable function that we optimize using gradient ascent.</S>
        <S ID="S-8128">We show that this function and its gradient are efficiently computable over packed forests of translations generated by machine translation systems.</S>
        <S ID="S-8129">The gradient includes expectations of products of features and n-gram counts, a quantity that has not appeared in previous work.</S>
        <S ID="S-8130">We present a new dynamic program which allows the efficient computation of these quantities over translation forests.</S>
        <S ID="S-8131">The resulting gradient ascent procedure does not require any k-best approximations.</S>
        <S ID="S-8132">Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (<REF ID="R-13" RPTR="23">Macherey et al., 2008</REF>) and large-margin training (<REF ID="R-02" RPTR="2">Chiang et al., 2008</REF>).</S>
        <S ID="S-8133">We developed CoBLEU primarily to complement consensus decoding, which it does; it produces higher BLEU scores than coupling MERT with consensus decoding.</S>
        <S ID="S-8134">However, we found an additional empirical benefit: CoBLEU is less prone to overfitting than MERT, even when using Viterbi decoding.</S>
        <S ID="S-8135">In experiments, models trained to maximize tuning set BLEU using MERT consistently degraded in performance from tuning to test set, while CoBLEU-trained models generalized more robustly.</S>
        <S ID="S-8136">As a result, we found that optimizing CoBLEU improved test set performance reliably using consensus decoding and occasionally using Viterbi decoding.</S>
      </P>
      <P>
        <S ID="S-8137">(a) Tuning set sentence and translation</S>
      </P>
      <P>
        <S ID="S-8138">(a) Hypotheses ranked by !TM = !LM = 1</S>
      </P>
      <P>
        <S ID="S-8139">Sentence f: Reference r:</S>
      </P>
      <P>
        <S ID="S-8140">H1) Once on a rhyme</S>
      </P>
      <P>
        <S ID="S-8141">H2) Once upon a rhyme</S>
      </P>
      <P>
        <S ID="S-8142">H3) Once upon a time</S>
      </P>
      <P>
        <S ID="S-8143">Il &#233;tait une rime Once upon a rhyme</S>
      </P>
      <P>
        <S ID="S-8144">TM LM Pr</S>
      </P>
      <P>
        <S ID="S-8145">-3 -7 0.67</S>
      </P>
      <P>
        <S ID="S-8146">-5 -6 0.24</S>
      </P>
      <P>
        <S ID="S-8147">-9 -3 0.09</S>
      </P>
      <P>
        <S ID="S-8148">(b) Computing Consensus Bigram Precision</S>
      </P>
      <P>
        <S ID="S-8149">E &#952; [c(&#8220;Once upon&#8221;,d)|f] = 0.24 + 0.09 = 0.33</S>
      </P>
      <P>
        <S ID="S-8150">E &#952; [c(&#8220;upon a&#8221;,d)|f] = 0.24 + 0.09 = 0.33</S>
      </P>
      <P>
        <S ID="S-8151">E &#952; [c(&#8220;a rhyme&#8221;,d)|f] = 0.67 + 0.24 = 0.91 &#8721;</S>
      </P>
      <P>
        <S ID="S-8152">E &#952; [c(g, d)|f] = 3[0.67 + 0.24 + 0.09] &#8721;</S>
      </P>
      <P>
        <S ID="S-8153">g min{E &#952;[c(g, d)|f],c(g, r)}</S>
      </P>
      <P>
        <S ID="S-8154">&#8721;</S>
      </P>
      <P>
        <S ID="S-8155">g E &#952;[c(g, d)|f]</S>
      </P>
      <P>
        <S ID="S-8156">g</S>
      </P>
      <P>
        <S ID="S-8157">=</S>
      </P>
      <P>
        <S ID="S-8158">0.33 + 0.33 + 0.91 3</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Consensus Objective Functions</HEADER>
      <P>
        <S ID="S-8229">Our proposed objective function maximizes n- gram precision by adapting the BLEU evaluation metric as a tuning objective (<REF ID="R-16" RPTR="27">Papineni et al., 2002</REF>).</S>
        <S ID="S-8230">To simplify exposition, we begin by adapting a simpler metric: bigram precision.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>2.1 Bigram Precision Tuning</HEADER>
        <P>
          <S ID="S-8159">Let the tuning corpus consist of source sentences</S>
        </P>
        <P>
          <S ID="S-8160">sentence.</S>
          <S ID="S-8161">Let e i be a translation of f i , and let E = e 1 .</S>
          <S ID="S-8162">.</S>
          <S ID="S-8163">.</S>
          <S ID="S-8164">e m be a corpus of translations, one for each source sentence.</S>
          <S ID="S-8165">A simple evaluation score for E is its bigram precision BP(R, E):</S>
        </P>
        <P>
          <S ID="S-8166">where g 2 iterates over the set of bigrams in the target language, and c(g 2 , e) is the count of bigram g 2 in translation e. As in BLEU, we &#8220;clip&#8221; the bigram counts of e in the numerator using counts of bigrams in the reference sentence.</S>
          <S ID="S-8167">Modern machine translation systems are typically tuned to maximize the evaluation score of Viterbi derivations 1 under a log-linear model with parameters &#952;. Let d &#8727; &#952; (f i) = arg max d P &#952; (d|f i ) be the highest scoring derivation d of f i .</S>
          <S ID="S-8168">For a system employing Viterbi decoding and evaluated by bigram precision, we would want to select &#952; to maximize</S>
        </P>
        <P>
          <S ID="S-8169">end, we can evaluate an entire posterior distribution over derivations H2 by computing the same clipped precision for expected bigram counts using CoBP(R,</S>
        </P>
        <P>
          <S ID="S-8170">is the expected count of bigram g 2 in all derivations d of f i .</S>
          <S ID="S-8171">We define the precise parametric form of P &#952; (d|f i ) in Section Parameter: 3.</S>
          <S ID="S-8172">Figure 1 shows proposed translations for a single sentence along with</S>
        </P>
        <P>
          <S ID="S-8173">the bigram expectations needed to compute CoBP.</S>
          <S ID="S-8174">Equation 1 constitutes an objective function for tuning the parameters of a machine translation model.</S>
          <S ID="S-8175">Figure 2 contrasts the properties of CoBP and MaxBP as tuning objectives, using the simple example from Figure 1.</S>
          <S ID="S-8176">Consensus bigram precision is an instance of a general recipe for converting n-gram based evaluation metrics into consensus objective functions for model tuning.</S>
          <S ID="S-8177">For the remainder of this paper, we focus on consensus BLEU.</S>
          <S ID="S-8178">However, the techniques herein, including the optimization approach of Section 3, are applicable to many differentiable functions of expected n-gram counts.</S>
          <S ID="S-8179">1 By derivation, we mean a translation of a foreign sentence along with any latent structure assumed by the model.</S>
          <S ID="S-8180">Each derivation corresponds to a particular English translation, but many derivations may yield the same translation.</S>
        </P>
        <P>
          <S ID="S-8181">solid line) of model scores.</S>
          <S ID="S-8182">When varying the single parameter &#952; LM , it entirely disregards the correct translation H 2 because H 2 never attains a maximal model score.</S>
          <S ID="S-8183">(b) A plot of both objectives shows their differing characteristics.</S>
          <S ID="S-8184">The horizontal segmented line at the top of the plot indicates the range over which consensus decoding would select each hypothesis, while the segmented line at the bottom indicates the same for Viterbi decoding.</S>
          <S ID="S-8185">MaxBP is only sensitive to the single point of discontinuity between H 1 and H 3 , and disregards H 2 entirely.</S>
          <S ID="S-8186">CoBP peaks when the distribution most heavily favors H 2 while suppressing H 1 .</S>
          <S ID="S-8187">Though H 2 never has a maximal model score, if &#952; LM is in the indicated range, consensus decoding would select H 2 , the desired translation.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.2 CoBLEU</HEADER>
        <P>
          <S ID="S-8188">ln BLEU(R, E) = 1 &#8722; &#8721; m &#8721;</S>
        </P>
        <P>
          <S ID="S-8189">i=1 g 1</S>
        </P>
        <P>
          <S ID="S-8190">c(g 1 , e i )</S>
        </P>
        <P>
          <S ID="S-8191">+ 1 4 4&#8721;</S>
        </P>
        <P>
          <S ID="S-8192">ln</S>
        </P>
        <P>
          <S ID="S-8193">n=1</S>
        </P>
        <P>
          <S ID="S-8194">&#8721; m</S>
        </P>
        <P>
          <S ID="S-8195">i=1</S>
        </P>
        <P>
          <S ID="S-8196">&#8721;</S>
        </P>
        <P>
          <S ID="S-8197">g n</S>
        </P>
        <P>
          <S ID="S-8198">min{c(g n , e i ), c(g n , r i )} &#8721; m</S>
        </P>
        <P>
          <S ID="S-8199">i=1</S>
        </P>
        <P>
          <S ID="S-8200">&#8721;</S>
        </P>
        <P>
          <S ID="S-8201">g n</S>
        </P>
        <P>
          <S ID="S-8202">c(g n , e i )</S>
        </P>
        <P>
          <S ID="S-8203">Above, |R| denotes the number of words in the reference corpus.</S>
          <S ID="S-8204">The notation (&#183;) &#8722; is shorthand for min(&#183;, 0).</S>
          <S ID="S-8205">In the inner sums, g n iterates over all n-grams of order n.</S>
          <S ID="S-8206">In order to adapt BLEU to be a consensus tuning objective, we follow the recipe of Section 2.1: we replace n-gram counts from a candidate translation with expected n-gram counts under the model.</S>
          <S ID="S-8207">( ) |R| CoBLEU(R, F, &#952;)= 1&#8722; &#8721; m</S>
        </P>
        <P>
          <S ID="S-8208">i=1&#8721; g 1</S>
        </P>
        <P>
          <S ID="S-8209">E &#952; [c(g 1 , d)|f i ]</S>
        </P>
        <P>
          <S ID="S-8210">+ 1 4 4&#8721;</S>
        </P>
        <P>
          <S ID="S-8211">ln</S>
        </P>
        <P>
          <S ID="S-8212">n=1</S>
        </P>
        <P>
          <S ID="S-8213">&#8721; m</S>
        </P>
        <P>
          <S ID="S-8214">i=1</S>
        </P>
        <P>
          <S ID="S-8215">&#8721;</S>
        </P>
        <P>
          <S ID="S-8216">g n</S>
        </P>
        <P>
          <S ID="S-8217">min{E &#952; [c(g n , d)|f i ], c(g n , r i )} &#8721; m</S>
        </P>
        <P>
          <S ID="S-8218">i=1</S>
        </P>
        <P>
          <S ID="S-8219">&#8721;</S>
        </P>
        <P>
          <S ID="S-8220">g n</S>
        </P>
        <P>
          <S ID="S-8221">E &#952; [c(g n , d)|f i ]</S>
        </P>
        <P>
          <S ID="S-8222">The brevity penalty term in BLEU is calculated using the expected length of the corpus, which</S>
        </P>
        <P>
          <S ID="S-8223">2 Throughout this paper, we use only a single reference,</S>
        </P>
        <P>
          <S ID="S-8224">but our objective readily extends to multiple references.</S>
        </P>
        <P>
          <S ID="S-8225">&#8722;</S>
        </P>
        <P>
          <S ID="S-8226">&#8722;</S>
        </P>
        <P>
          <S ID="S-8227">equals the sum of all expected unigram counts.</S>
          <S ID="S-8228">We call this objective function consensus BLEU, or CoBLEU for short.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Optimizing CoBLEU</HEADER>
      <P>
        <S ID="S-8359">Unlike the more common MaxBLEU tuning objective optimized by MERT, CoBLEU is continuous.</S>
        <S ID="S-8360">For distributions P &#952; (d|f i ) that factor over synchronous grammar rules and n-grams, we show below that it is also analytically differentiable, permitting a straightforward gradient ascent optimization procedure.</S>
        <S ID="S-8361">3 In order to perform gradient ascent, we require methods for efficiently computing the gradient of the objective function for a given parameter setting &#952;.</S>
        <S ID="S-8362">Once we have the gradient, we can perform an update at iteration t of the form</S>
      </P>
      <P>
        <S ID="S-8363">&#952; (t+1) &#8592; &#952; (t) + &#951; t &#8711; &#952; CoBLEU(R, F, &#952; (t) )</S>
      </P>
      <P>
        <S ID="S-8364">where &#951; t is an adaptive step size.</S>
        <S ID="S-8365">4</S>
      </P>
      <P>
        <S ID="S-8366">3 Technically, CoBLEU is non-differentiable at some</S>
      </P>
      <P>
        <S ID="S-8367">points because of clipping.</S>
        <S ID="S-8368">At these points, we must compute a sub-gradient, and so our optimization is formally subgradient ascent.</S>
        <S ID="S-8369">See the Appendix for details.</S>
        <S ID="S-8370">4 After each successful step, we grow the step size by a</S>
      </P>
      <P>
        <S ID="S-8371">constant factor.</S>
        <S ID="S-8372">Whenever the objective does not decrease after a step, we shrink the step size by a constant factor and try again until a decrease is attained.</S>
      </P>
      <P>
        <S ID="S-8373">l 2 (h) =2</S>
      </P>
      <P>
        <S ID="S-8374">u=OnceSrhyme</S>
      </P>
      <P>
        <S ID="S-8375">tail(h)</S>
      </P>
      <P>
        <S ID="S-8376">head(h) c(&#8220;Once upon&#8221;, h) = 1 c(&#8220;upon a&#8221;, h) = 1</S>
      </P>
      <P>
        <S ID="S-8377">v1=OnceRBOnce v2=uponINupon v3=aNPrhyme</S>
      </P>
      <P>
        <S ID="S-8378">In this section, we develop an analytical expression for the gradient of CoBLEU, then discuss how to efficiently compute the value of the objective function and gradient.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>3.1 Translation Model Form</HEADER>
        <P>
          <S ID="S-8231">We first assume the general hypergraph setting of <REF ID="R-06" RPTR="13">Huang and Chiang (2007)</REF><REF ID="R-03" RPTR="3">Chiang (2007)</REF>, namely, that derivations under our translation model form a hypergraph.</S>
          <S ID="S-8232">This framework allows us to speak about both phrase-based and syntax-based translation in a unified framework.</S>
        </P>
        <P>
          <S ID="S-8233">We define a probability distribution over derivations d via &#952; as:</S>
        </P>
        <P>
          <S ID="S-8234">with</S>
        </P>
        <P>
          <S ID="S-8235">P &#952; (d|f i ) = w(d) Z(f i )</S>
        </P>
        <P>
          <S ID="S-8236">Z(f i ) = &#8721; d &#8242; w(d &#8242; )</S>
        </P>
        <P>
          <S ID="S-8237">where w(d) = exp(&#952; &#8868; &#934;(d, f i )) is the weight of a derivation and &#934;(d, f i ) is a featurized representation of the derivation d of f i .</S>
          <S ID="S-8238">We further assume that these features decompose over hyperedges in the hypergraph, like the one in Figure 3.</S>
          <S ID="S-8239">That is, &#934;(d, f i ) = &#8721; h&#8712;d &#934;(h, f i).</S>
        </P>
        <P>
          <S ID="S-8240">In this setting, we can analytically compute the gradient of CoBLEU.</S>
          <S ID="S-8241">We provide a sketch of the derivation of this gradient in the Appendix.</S>
          <S ID="S-8242">In computing this gradient, we must calculate the following expectations:</S>
        </P>
        <P>
          <S ID="S-8243">E &#952; [c(&#966; k , d)|f i ] (2)</S>
        </P>
        <P>
          <S ID="S-8244">E &#952; [l n (d)|f i ] (3)</S>
        </P>
        <P>
          <S ID="S-8245">E &#952; [c(&#966; k , d) &#183; l n (d)|f i ] (4)</S>
        </P>
        <P>
          <S ID="S-8246">where l n (d) = &#8721; g n c(g n , d) is the sum of all n- grams on derivation d (its &#8220;length&#8221;).</S>
          <S ID="S-8247">The first expectation is an expected count of the kth feature &#966; k over all derivations of f i .</S>
          <S ID="S-8248">The second is an expected length, the total expected count of all n- grams in derivations of f i .</S>
          <S ID="S-8249">We call the final expectation an expected product of counts.</S>
          <S ID="S-8250">We now present the computation of each of these expectations in turn.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.2 Computing Feature Expectations</HEADER>
        <P>
          <S ID="S-8251">The expected feature counts E &#952; [c(&#966; k , d)|f i ] can be written as</S>
        </P>
        <P>
          <S ID="S-8252">E &#952; [c(&#966; k , d)|f i ] = &#8721; d</S>
        </P>
        <P>
          <S ID="S-8253">= &#8721; h</S>
        </P>
        <P>
          <S ID="S-8254">P &#952; (d|f i )c(&#966; k , d)</S>
        </P>
        <P>
          <S ID="S-8255">P &#952; (h|f i )c(&#966; k , h)</S>
        </P>
        <P>
          <S ID="S-8256">We can justify the second step since feature counts are local to hyperedges, i.e. c(&#966; k , d) = &#8721; h&#8712;d c(&#966; k, h).</S>
          <S ID="S-8257">The posterior probability P &#952; (h|f i ) can be efficiently computed with inside-outside scores.</S>
          <S ID="S-8258">Let I(u) and O(u) be the standard inside and outside scores for a node u in the forest.</S>
          <S ID="S-8259">5</S>
        </P>
        <P>
          <S ID="S-8260">P &#952; (h|f i ) = 1 Z(f) w(h) O(head(h)) &#8719;</S>
        </P>
        <P>
          <S ID="S-8261">v&#8712;tail(h)</S>
        </P>
        <P>
          <S ID="S-8262">I(v)</S>
        </P>
        <P>
          <S ID="S-8263">where w(h) is the weight of hyperedge h, given by exp(&#952; &#8868; &#934;(h)), and Z(f) = I(root) is the inside score of the root of the forest.</S>
          <S ID="S-8264">Computing these inside-outside quantities takes time linear in the number of hyperedges in the forest.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.3 Computing n-gram Expectations</HEADER>
        <P>
          <S ID="S-8265">We can compute the expectations of any specific n-grams, or of total n-gram counts l, in the same way as feature expectations, provided that targetside n-grams are also localized to hyperedges (e.g. consider l to be a feature of a hyperedge whose value is the number of n-grams on h).</S>
          <S ID="S-8266">If the nodes in our forests are annotated with target-side</S>
        </P>
        <P>
          <S ID="S-8267">5 Appendix Figure 7 gives recursions for I(u) and O(u).</S>
        </P>
        <P>
          <S ID="S-8268">boundary words as in Figure 3, then this will be the case.</S>
          <S ID="S-8269">Note that this is the same approach used by decoders which integrate a target language model (e.g. <REF ID="R-03" RPTR="4">Chiang (2007)</REF>).</S>
          <S ID="S-8270">Other work has computed n-gram expectations in the same way (<REF ID="R-04" RPTR="7">DeNero et al., 2009</REF>; <REF ID="R-12" RPTR="22">Li et al., 2009</REF>).</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.4 Computing Expectations of Products of Counts</HEADER>
        <P>
          <S ID="S-8271">While the previous two expectations can be computed using techniques known in the literature, the expected product of counts E &#952; [c(&#966; k , d) &#183; l n (d)|f i ] is a novel quantity.</S>
          <S ID="S-8272">Fortunately, an efficient dynamic program exists for computing this expectation as well.</S>
          <S ID="S-8273">We present this dynamic program here as one of the contributions of this paper, though we omit a full derivation due to space restrictions.</S>
        </P>
        <P>
          <S ID="S-8274">To see why this expectation cannot be computed in the same way as the expected feature or n-gram counts, we expand the definition of the expectation above to get &#8721;</S>
        </P>
        <P>
          <S ID="S-8275">P &#952; (d|f i ) [c(&#966; k , d)l n (d)]</S>
        </P>
        <P>
          <S ID="S-8276">d</S>
        </P>
        <P>
          <S ID="S-8277">Unlike feature and n-gram counts, the product of counts in brackets above does not decompose over hyperedges, at least not in an obvious way.</S>
          <S ID="S-8278">We can, however, still decompose the feature counts c(&#966; k , d) over hyperedges.</S>
          <S ID="S-8279">After this decomposition and a little re-arranging, we get</S>
        </P>
        <P>
          <S ID="S-8280">= &#8721; c(&#966; k , h) &#8721; P &#952; (d|f i )l n (d)</S>
        </P>
        <P>
          <S ID="S-8281">h d:h&#8712;d</S>
        </P>
        <P>
          <S ID="S-8282">=</S>
        </P>
        <P>
          <S ID="S-8283">=</S>
        </P>
        <P>
          <S ID="S-8284">1 Z(f i )</S>
        </P>
        <P>
          <S ID="S-8285">1 Z(f i ) [ ] &#8721; &#8721;</S>
        </P>
        <P>
          <S ID="S-8286">c(&#966; k , h) w(d)l n (d)</S>
        </P>
        <P>
          <S ID="S-8287">h d:h&#8712;d</S>
        </P>
        <P>
          <S ID="S-8288">&#8721; c(&#966; k , h)&#710;D n &#952; (h|f i )</S>
        </P>
        <P>
          <S ID="S-8289">h</S>
        </P>
        <P>
          <S ID="S-8290">The quantity &#710;D n &#952; (h|f i ) = &#8721; d:h&#8712;d w(d)l n(d) is the sum of the weight-length products of all derivations d containing hyperedge h.</S>
          <S ID="S-8291">In the same way that P &#952; (h|f i ) can be efficiently computed from inside and outside probabilities, this quantity &#710;D n &#952; (h|f i ) can be efficiently computed with two new inside and outside quantities, which we call &#206; n (u) and &#212;n(u).</S>
          <S ID="S-8292">We provide recursions for these quantities in Figure 4.</S>
          <S ID="S-8293">Like the standard inside and outside computations, these recursions run in time linear in the number of hyperedges in the forest.</S>
        </P>
        <P>
          <S ID="S-8294">While a full exposition of the algorithm is not possible in the available space, we give some brief intuition behind this dynamic program.</S>
          <S ID="S-8295">We first define &#206; n (u):</S>
        </P>
        <P>
          <S ID="S-8296">&#206; n (u) = &#8721; d u w(d u )l n (d)</S>
        </P>
        <P>
          <S ID="S-8297">where d u is a derivation rooted at node u.</S>
          <S ID="S-8298">This is a sum of weight-length products similar to &#710;D.</S>
          <S ID="S-8299">To give a recurrence for &#206;, we rewrite it:</S>
        </P>
        <P>
          <S ID="S-8300">&#206; n (u) = &#8721; &#8721; [w(d u )l n (h)]</S>
        </P>
        <P>
          <S ID="S-8301">d u h&#8712;d u</S>
        </P>
        <P>
          <S ID="S-8302">Here, we have broken up the total value of l n (d) across hyperedges in d.</S>
          <S ID="S-8303">The bracketed quantity is a score of a marked derivation pair (d, h) where the edge h is some specific element of d.</S>
          <S ID="S-8304">The score of a marked derivation includes the weight of the derivation and the factor l n (h) for the marked hyperedge.</S>
        </P>
        <P>
          <S ID="S-8305">This sum over marked derivations gives the inside recurrence in Figure 4 by the following decomposition.</S>
          <S ID="S-8306">For &#206; n (u) to sum over all marked derivation pairs rooted at u, we must consider two cases.</S>
          <S ID="S-8307">First, the marked hyperedge could be at the root, in which case we must choose child derivations from regular inside scores and multiply in the local l n , giving the first summand of &#206; n (u).</S>
          <S ID="S-8308">Alternatively, the marked hyperedge is in exactly one of the children; for each possibility we recursively choose a marked derivation for one child, while the other children choose regular derivations.</S>
          <S ID="S-8309">The second summand of &#206; n (u) compactly expresses a sum over instances of this case.</S>
          <S ID="S-8310">&#212; n (u) decomposes similarly: the marked hyperedge could be local (first summand), under a sibling (second summand), or higher in the tree (third summand).</S>
        </P>
        <P>
          <S ID="S-8311">Once we have these new inside-outside quantities, we can compute &#710;D as in Figure 5.</S>
          <S ID="S-8312">This combination states that marked derivations containing h are either marked at h, below h, or above h.</S>
        </P>
        <P>
          <S ID="S-8313">As a final detail, computing the gradient &#8711;Cn clip (&#952;) (see the Appendix) involves a clipped version of the expected product of counts, for which a clipped &#710;D is required.</S>
          <S ID="S-8314">This quantity can be computed with the same dynamic program with a slight modification.</S>
          <S ID="S-8315">In Figure 4, we show the difference as a choice point when computing l n (h).</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.5 Implementation Details</HEADER>
        <P>
          <S ID="S-8316">As stated, the runtime of computing the required expectations for the objective and gradient is linear in the number of hyperedges in the forest.</S>
          <S ID="S-8317">The</S>
        </P>
        <P>
          <S ID="S-8318">&#206; n (u) = &#8721;</S>
        </P>
        <P>
          <S ID="S-8319">h&#8712;IN(u)</S>
        </P>
        <P>
          <S ID="S-8320">&#9121;</S>
        </P>
        <P>
          <S ID="S-8321">w(h) &#9123;l n (h) &#8719;</S>
        </P>
        <P>
          <S ID="S-8322">v&#8712;tail(h)</S>
        </P>
        <P>
          <S ID="S-8323">I(v) + &#8721;</S>
        </P>
        <P>
          <S ID="S-8324">v&#8712;tail(h)</S>
        </P>
        <P>
          <S ID="S-8325">&#206; n (v) &#8719; w&#8800;v</S>
        </P>
        <P>
          <S ID="S-8326">&#9124;</S>
        </P>
        <P>
          <S ID="S-8327">I(w) &#9126;</S>
        </P>
        <P>
          <S ID="S-8328">&#212; n (u) = &#8721; w(h) &#9122; &#9123; l n(h) O(head(h)) &#8719; I(v) + O(head(h)) &#8721; &#206; n (v) &#8719; I(w) + &#212;n(head(h)) &#8719; I(w) &#9125; &#9126;</S>
        </P>
        <P>
          <S ID="S-8329">h&#8712;OUT(u)</S>
        </P>
        <P>
          <S ID="S-8330">&#9121;</S>
        </P>
        <P>
          <S ID="S-8331">v&#8712;tail(h) v&#8800;u v&#8712;tail(h)</S>
        </P>
        <P>
          <S ID="S-8332">v&#8800;u w&#8712;tail(h)</S>
        </P>
        <P>
          <S ID="S-8333">w&#8800;v w&#8800;u</S>
        </P>
        <P>
          <S ID="S-8334">{ &#8721;</S>
        </P>
        <P>
          <S ID="S-8335">gn</S>
        </P>
        <P>
          <S ID="S-8336">l n (h) = c(g n, h) computing unclipped counts &#8721;</S>
        </P>
        <P>
          <S ID="S-8337">g n</S>
        </P>
        <P>
          <S ID="S-8338">c(g n , h)1 [E &#952; [c(g n , d)] &#8804; c(g n , r i )] computing clipped counts</S>
        </P>
        <P>
          <S ID="S-8339">w&#8712;tail(h) w&#8800;u</S>
        </P>
        <P>
          <S ID="S-8340">&#9124;</S>
        </P>
        <P>
          <S ID="S-8341">&#9121; w(h) &#9122; &#9123; l n(h)O(head(h)) &#8719;</S>
        </P>
        <P>
          <S ID="S-8342">v&#8712;tail(h)</S>
        </P>
        <P>
          <S ID="S-8343">&#710;D n &#952; (h|f i ) =</S>
        </P>
        <P>
          <S ID="S-8344">I(v) + O(head(h)) &#8721;</S>
        </P>
        <P>
          <S ID="S-8345">v&#8712;tail(h)</S>
        </P>
        <P>
          <S ID="S-8346">&#206; n (v) &#8719;</S>
        </P>
        <P>
          <S ID="S-8347">v&#8712;tail(h) w&#8800;v</S>
        </P>
        <P>
          <S ID="S-8348">I(w) + &#212;n(head(h)) &#8719;</S>
        </P>
        <P>
          <S ID="S-8349">w&#8712;tail(h)</S>
        </P>
        <P>
          <S ID="S-8350">&#9124; I(w) &#9125; &#9126;</S>
        </P>
        <P>
          <S ID="S-8351">number of hyperedges is very large, however, because we must track n-gram contexts in the nodes, just as we would in an integrated language model decoder.</S>
          <S ID="S-8352">These contexts are required both to correctly compute the model score of derivations and to compute clipped n-gram counts.</S>
          <S ID="S-8353">To speed our computations, we use the cube pruning method of <REF ID="R-06" RPTR="14">Huang and Chiang (2007)</REF><REF ID="R-03" RPTR="5">Chiang (2007)</REF> with a fixed beam size.</S>
        </P>
        <P>
          <S ID="S-8354">For regularization, we added an L 2 penalty on the size of &#952; to the CoBLEU objective, a simple addition for gradient ascent.</S>
          <S ID="S-8355">We did not find that our performance varied very much for moderate levels of regularization.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>3.6 Related Work</HEADER>
        <P>
          <S ID="S-8356">The calculation of expected counts can be formulated using the expectation semiring framework of <REF ID="R-05" RPTR="11">Eisner (2002)</REF>, though that work does not show how to compute expected products of counts which are needed for our gradient calculations.</S>
          <S ID="S-8357">Concurrently with this work, <REF ID="R-11" RPTR="19">Li and Eisner (2009)</REF> have generalized <REF ID="R-05" RPTR="12">Eisner (2002)</REF> to compute expected products of counts on translation forests.</S>
          <S ID="S-8358">The training algorithm of <REF ID="R-07" RPTR="15">Kakade et al. (2002)</REF> makes use of a dynamic program similar to ours, though specialized to the case of sequence models.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 Consensus Decoding</HEADER>
      <P>
        <S ID="S-8379">Once model parameters &#952; are learned, we must select an appropriate decoding objective.</S>
        <S ID="S-8380">Several new decoding approaches have been proposed recently that leverage some notion of consensus over the many weighted derivations in a translation forest.</S>
        <S ID="S-8381">In this paper, we adopt the fast consensus decoding procedure of <REF ID="R-04" RPTR="9">DeNero et al. (2009)</REF>, which directly complements CoBLEU tuning.</S>
        <S ID="S-8382">For a source sentence f, we first build a translation forest, then compute the expected count of each n-gram in the translation of f under the model.</S>
        <S ID="S-8383">We extract a k-best list from the forest, then select the translation that yields the highest BLEU score relative to the forest&#8217;s expected n-gram counts.</S>
        <S ID="S-8384">Specifically, let BLEU(e; r) compute the similarity of a sentence e to a reference r based on the n-gram counts of each.</S>
        <S ID="S-8385">When training with CoBLEU, we replace e with expected counts and maximize &#952;.</S>
        <S ID="S-8386">In consensus decoding, we replace r with expected counts and maximize e.</S>
      </P>
      <P>
        <S ID="S-8387">Several other efficient consensus decoding pro-</S>
      </P>
      <P>
        <S ID="S-8388">Fraction of Value at Convergence</S>
      </P>
      <P>
        <S ID="S-8389">0.0 0.2 0.4 0.6 0.8 1.0</S>
      </P>
      <P>
        <S ID="S-8390">cedures would similarly benefit from a tuning procedure that aggregates over derivations.</S>
        <S ID="S-8391">For instance, <REF ID="R-00" RPTR="0">Blunsom and Osborne (2008)</REF> select the translation sentence with highest posterior probability under the model, summing over derivations.</S>
        <S ID="S-8392"><REF ID="R-12" RPTR="20">Li et al. (2009)</REF> propose a variational approximation maximizing sentence probability that decomposes over n-grams.</S>
        <S ID="S-8393"><REF ID="R-19" RPTR="30">Tromble et al. (2008)</REF> minimize risk under a loss function based on the linear Taylor approximation to BLEU, which decomposes over n-gram posterior probabilities.</S>
      </P>
      <P>
        <S ID="S-8394">2 4 6 8 10</S>
      </P>
      <P>
        <S ID="S-8395">Iterations</S>
      </P>
      <P>
        <S ID="S-8396">CoBLEU MERT</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Experiments</HEADER>
      <P>
        <S ID="S-8441">We compared CoBLEU training with an implementation of minimum error rate training on two language pairs.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>5.1 Model</HEADER>
        <P>
          <S ID="S-8397">Our optimization procedure is in principle tractable for any syntactic translation system.</S>
          <S ID="S-8398">For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (<REF ID="R-01" RPTR="1">Cherry and Lin, 2007</REF>).</S>
          <S ID="S-8399">Phrasal ITG models have been shown to perform comparably to the state-ofthe art phrase-based system Moses (<REF ID="R-08" RPTR="16">Koehn et al., 2007</REF>) when using the same phrase table (<REF ID="R-17" RPTR="28">Petrov et al., 2008</REF>).</S>
        </P>
        <P>
          <S ID="S-8400">We extract a phrase table using the Moses pipeline, based on Model 4 word alignments generated from GIZA++ (<REF ID="R-14" RPTR="25">Och and Ney, 2003</REF>).</S>
          <S ID="S-8401">Our final ITG grammar includes the five standard Moses features, an n-gram language model, a length feature that counts the number of target words, a feature that counts the number of monotonic ITG rewrites, and a feature that counts the number of inverted ITG rewrites.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>5.2 Data</HEADER>
        <P>
          <S ID="S-8402">We extracted phrase tables from the Spanish- English and French-English sections of the Europarl corpus, which include approximately 8.5 million words of bitext for each of the language pairs (<REF ID="R-09" RPTR="17">Koehn, 2002</REF>).</S>
          <S ID="S-8403">We used a trigram language model trained on the entire corpus of English parliamentary proceedings provided with the Europarl distribution and generated according to the ACL 2008 SMT shared task specifications.</S>
          <S ID="S-8404">6 For tuning, we used all sentences from the 2007 SMT shared task up to length 25 (880 sentences</S>
        </P>
        <P>
          <S ID="S-8405">6 See http://www.statmt.org/wmt08 for details.</S>
        </P>
        <P>
          <S ID="S-8406">for Spanish and 923 for French), and we tested on the subset of the first 1000 development set sentences which had length at most 25 words (447 sentences for Spanish and 512 for French).</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>5.3 Tuning Optimization</HEADER>
        <P>
          <S ID="S-8407">We compared two techniques for tuning the nine log-linear model parameters of our ITG grammar.</S>
          <S ID="S-8408">We maximized CoBLEU using gradient ascent, as described above.</S>
          <S ID="S-8409">As a baseline, we maximized BLEU of the Viterbi translation derivations using minimum error rate training.</S>
          <S ID="S-8410">To improve optimization stability, MERT used a cumulative k-best list that included all translations generated during the tuning process.</S>
        </P>
        <P>
          <S ID="S-8411">One of the benefits of CoBLEU training is that we compute expectations efficiently over an entire forest of translations.</S>
          <S ID="S-8412">This has substantial stability benefits over methods based on k-best lists.</S>
          <S ID="S-8413">In Figure 6, we show the progress of CoBLEU as compared to MERT.</S>
          <S ID="S-8414">Both models are initialized from 0 and use the same features.</S>
          <S ID="S-8415">This plot exhibits a known issue with MERT training: because new k-best lists are generated at each iteration, the objective function can change drastically between iterations.</S>
          <S ID="S-8416">In contrast, CoBLEU converges</S>
        </P>
        <P>
          <S ID="S-8417">Consensus Decoding</S>
        </P>
        <P>
          <S ID="S-8418">smoothly to its final objective because the forests do not change substantially between iterations, despite the pruning needed to track n-grams.</S>
          <S ID="S-8419">Similar stability benefits have been observed for latticebased MERT (<REF ID="R-13" RPTR="24">Macherey et al., 2008</REF>).</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>5.4 Results</HEADER>
        <P>
          <S ID="S-8420">We performed experiments from both French and Spanish into English under three conditions.</S>
          <S ID="S-8421">In the first two, we initialized both MERT and CoBLEU training uniformly with zero weights and trained until convergence.</S>
          <S ID="S-8422">In the third condition, we initialized CoBLEU with the final parameters from MERT training, denoted MERT&#8594;CoBLEU in the results tables.</S>
          <S ID="S-8423">We evaluated each of these conditions on both the tuning and test sets using the consensus decoding method of <REF ID="R-04" RPTR="10">DeNero et al. (2009)</REF>.</S>
          <S ID="S-8424">The results appear in Table 1.</S>
          <S ID="S-8425">In Spanish-English, CoBLEU slightly outperformed MERT under the same initialization, while the opposite pattern appears for French-English.</S>
          <S ID="S-8426">The best test set performance in both language pairs was the third condition, in which CoBLEU training was initialized with MERT.</S>
          <S ID="S-8427">This condition also gave the highest CoBLEU objective value.</S>
          <S ID="S-8428">This pattern indicates that CoBLEU is a useful objective for translation with consensus decoding, but that the gradient ascent optimization is getting stuck in local maxima during tuning.</S>
          <S ID="S-8429">This issue can likely be addressed with annealing, as described in (<REF ID="R-18" RPTR="29">Smith and Eisner, 2006</REF>).</S>
        </P>
        <P>
          <S ID="S-8430">Interestingly, the brevity penatly results in French indicate that, even though CoBLEU did Viterbi Decoding</S>
        </P>
        <P>
          <S ID="S-8431">not outperform MERT in a statistically significant way, CoBLEU tends to find shorter sentences with higher n-gram precision than MERT.</S>
        </P>
        <P>
          <S ID="S-8432">Table 1 displays a second benefit of CoBLEU training: compared to MERT training, CoBLEU performance degrades less from tuning to test set.</S>
          <S ID="S-8433">In Spanish, initializing with MERT-trained weights and then training with CoBLEU actually decreases BLEU on the tuning set by 0.8 points.</S>
          <S ID="S-8434">However, this drop in tuning performance comes with a corresponding increase of 0.6 on the test set, relative to MERT training.</S>
          <S ID="S-8435">We see the same pattern in French, albeit to a smaller degree.</S>
        </P>
        <P>
          <S ID="S-8436">While CoBLEU ought to outperform MERT using consensus decoding, we expected that MERT would give better performance under Viterbi decoding.</S>
          <S ID="S-8437">Surprisingly, we found that CoBLEU training actually outperformed MERT in Spanish- English and performed equally well in French- English.</S>
          <S ID="S-8438">Table 2 shows the results.</S>
          <S ID="S-8439">In these experiments, we again see that CoBLEU overfit the training set to a lesser degree than MERT, as evidenced by a smaller drop in performance from tuning to test set.</S>
          <S ID="S-8440">In fact, test set performance actually improved for Spanish-English CoBLEU training while dropping by 2.3 BLEU for MERT.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>6 Conclusion</HEADER>
      <P>
        <S ID="S-8442">CoBLEU takes a fundamental quantity used in consensus decoding, expected n-grams, and trains to optimize a function of those expectations.</S>
        <S ID="S-8443">While CoBLEU can therefore be expected to increase test set BLEU under consensus decoding, it is more surprising that it seems to better regularize learning even for the Viterbi decoding condition.</S>
        <S ID="S-8444">It is also worth emphasizing that the CoBLEU approach is applicable to functions of expected n- gram counts other than BLEU.</S>
        <S ID="S-8445">1425 Appendix: The Gradient of CoBLEU We would like to compute the gradient of</S>
      </P>
      <P>
        <S ID="S-8446">of all n-grams or order n in all translations of the source corpus F , while Cn clip (&#952;) represents the sum of the same expected counts, but clipped with reference counts c(g n , r i ).</S>
        <S ID="S-8447">With this notation, we can write our objective function CoBLEU(R, F, &#952;) in three terms:</S>
      </P>
      <P>
        <S ID="S-8448">incoming and outgoing hyperedges of u, respectively.</S>
        <S ID="S-8449">We initialize with I(u) = 1 for all terminal forest nodes u and O(root) = 1 for the root node.</S>
        <S ID="S-8450">These quantities are referenced in Figure 4.</S>
        <S ID="S-8451">and the gradient &#8711;Cn</S>
      </P>
      <P>
        <S ID="S-8452">n (&#952;)E &#952; [c(&#966; k , d) + f i ] where 1 denotes an indicator function.</S>
        <S ID="S-8453">At the top level, the gradient of the first term (the brevity</S>
      </P>
      <P>
        <S ID="S-8454">Formally, we must compute a sub-gradient at these points.</S>
        <S ID="S-8455">In practice, we can choose between the gradients calculated assuming the indicator function is 0 or 1; we always choose the latter.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS/>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>Phil Blunsom</RAUTHOR>
      <REFTITLE>Probabilistic inference for machine translation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>Colin Cherry</RAUTHOR>
      <REFTITLE>Inversion transduction grammar for joint phrasal translation modeling.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>David Chiang</RAUTHOR>
      <REFTITLE>Online large-margin training of syntactic and structural translation features.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>David Chiang</RAUTHOR>
      <REFTITLE>Hierarchical phrase-based translation. Computational Linguistics.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>John DeNero</RAUTHOR>
      <REFTITLE>Fast consensus decoding over translation forests.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>Jason Eisner</RAUTHOR>
      <REFTITLE>Parameter estimation for probabilistic finite-state transducers.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>Liang Huang</RAUTHOR>
      <REFTITLE>Forest rescoring: Faster decoding with integrated language models.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>Sham Kakade</RAUTHOR>
      <REFTITLE>An alternate objective function for markovian fields.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Moses: Open source toolkit for statistical machine translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Europarl: A multilingual corpus for evaluation of machine translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>Shankar Kumar</RAUTHOR>
      <REFTITLE>Efficient minimum error rate training and minimum Bayes-risk decoding for translation hypergraphs and lattices.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>Zhifei Li</RAUTHOR>
      <REFTITLE>First- and secondorder expectation semirings with applications to minimum-risk training on translation forests.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>Zhifei Li</RAUTHOR>
      <REFTITLE>Variational decoding for statistical machine translation.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>W Macherey</RAUTHOR>
      <REFTITLE>Lattice-based minimum error rate training for statistical machine translation. In</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="14">
      <RAUTHOR>Franz Josef Och</RAUTHOR>
      <REFTITLE>A systematic comparison of various statistical alignment models.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="15">
      <RAUTHOR>Franz Josef Och</RAUTHOR>
      <REFTITLE>Minimum error rate training in statistical machine translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="16">
      <RAUTHOR>Kishore Papineni</RAUTHOR>
      <REFTITLE>BLEU: A method for automatic evaluation of machine translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="17">
      <RAUTHOR>Slav Petrov</RAUTHOR>
      <REFTITLE>Coarse-to-fine syntactic machine translation using language projections.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="18">
      <RAUTHOR>David Smith</RAUTHOR>
      <REFTITLE>Minimum risk annealing for training log-linear models. In</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="19">
      <RAUTHOR>Roy Tromble</RAUTHOR>
      <REFTITLE>Lattice minimum Bayes-risk decoding for statistical machine translation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
