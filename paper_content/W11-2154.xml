<document>
  <filename>W11-2154</filename>
  <authors/>
  <title>The LIGA (LIG/LIA) Machine Translation System for WMT 2011</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>We describe our system for the news commentary translation task of WMT 2011. The submitted run for the French-English direction is a combination of two MOSES-based systems developed at LIG and LIA laboratories. We report experiments to improve over the standard phrase-based model using statistical post-edition, information retrieval methods to subsample out-of-domain parallel corpora and
ROVER to combine n-best list of hypotheses
output by different systems.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We describe our system for the news commentary translation task of WMT 2011.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The submitted run for the French-English direction is a combination of two MOSES-based systems developed at LIG and LIA laboratories.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We report experiments to improve over the standard phrase-based model using statistical post-edition, information retrieval methods to subsample out-of-domain parallel corpora and</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>ROVER to combine n-best list of hypotheses</text>
              <doc_id>3</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>output by different systems.</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>This year, LIG and LIA have combined their efforts to produce a joint submission to WMT 2011 for the French-English translation task. Each group started by developing its own solution whilst sharing resources (corpora as provided by the organizers but also aligned data etc) and acquired knowledge (current parameters, effect of the size of n-grams, etc.) with the other. Both LIG and LIA systems are standard phrase-based translation systems based on the
MOSES toolkit with appropriate carefully-tuned setups. The final LIGA submission is a combination of the two systems. We summarize in Section 2 the resources used and the main characteristics of the systems. Sections 3 and 4 describe the specificities and report experiments of resp. the LIG and the LIA system. Section 5 presents the combination of n-best lists hypotheses generated by both systems. Finally, we conclude in Section 6.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This year, LIG and LIA have combined their efforts to produce a joint submission to WMT 2011 for the French-English translation task.</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each group started by developing its own solution whilst sharing resources (corpora as provided by the organizers but also aligned data etc) and acquired knowledge (current parameters, effect of the size of n-grams, etc.) with the other.</text>
              <doc_id>6</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Both LIG and LIA systems are standard phrase-based translation systems based on the</text>
              <doc_id>7</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>MOSES toolkit with appropriate carefully-tuned setups.</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The final LIGA submission is a combination of the two systems.</text>
              <doc_id>9</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We summarize in Section 2 the resources used and the main characteristics of the systems.</text>
              <doc_id>10</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Sections 3 and 4 describe the specificities and report experiments of resp.</text>
              <doc_id>11</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>the LIG and the LIA system.</text>
              <doc_id>12</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Section 5 presents the combination of n-best lists hypotheses generated by both systems.</text>
              <doc_id>13</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we conclude in Section 6.</text>
              <doc_id>14</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 System overview</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>15</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Used data</title>
            <text>Globally, our system 1 was built using all the French and English data supplied for the workshop&#8217;s shared translation task, apart from the Gigaword monolingual corpora released by the LDC. Table 1 sums up the used data and introduces designations that we follow in the remainder of this paper to refer to corpora. Four corpora were used to build translation models: news-c, euro, UN and giga, while three others are employed to train monolingual language models (LMs). Three bilingual corpora were devoted to model tuning: test09 was used for the development of the two seed systems (LIG and LIA), whereas test08 and testcomb08 were used to tune the weights for system combination. test10 was finally put aside to compare internally our methods.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Globally, our system 1 was built using all the French and English data supplied for the workshop&#8217;s shared translation task, apart from the Gigaword monolingual corpora released by the LDC.</text>
                  <doc_id>16</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Table 1 sums up the used data and introduces designations that we follow in the remainder of this paper to refer to corpora.</text>
                  <doc_id>17</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Four corpora were used to build translation models: news-c, euro, UN and giga, while three others are employed to train monolingual language models (LMs).</text>
                  <doc_id>18</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Three bilingual corpora were devoted to model tuning: test09 was used for the development of the two seed systems (LIG and LIA), whereas test08 and testcomb08 were used to tune the weights for system combination.</text>
                  <doc_id>19</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>test10 was finally put aside to compare internally our methods.</text>
                  <doc_id>20</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 LIG and LIA system characteristics</title>
            <text>Both LIG and LIA systems are phrase-based translation models. All the data were first tokenized with the tokenizer provided for the workshop. Kneser- Ney discounted LMs were built from monolingual corpora using the SRILM toolkit (Stolcke, 2002), while bilingual corpora were aligned at the wordlevel using GIZA++ (Och and Ney, 2003) or its multi-threaded version MGIZA++ (Gao and Vogel, 2008) for the large corpora UN and giga. Phrase table and lexicalized reordering models were built with MOSES (Koehn et al., 2007). Finally, 14 features were used in the phrase-based models:
1 When not specified otherwise &#8220;our&#8221; system refers to the
LIGA system.
CORPORA DESIGNATION SIZE (SENTENCES)
Test newstest2010 test10 2,489
&#8226; 5 translation model scores,
&#8226; 1 distance-based reordering score,
&#8226; 6 lexicalized reordering score,
&#8226; 1 LM score and
&#8226; 1 word penalty score.
The score weights were optimized on the test09 corpus according to the BLEU score with the MERT method (Och, 2003). The experiments led specifically with either LIG or LIA system are respectively described in Sections 3 and 4. Unless otherwise indicated, all the evaluations were performed using case-insensitive BLEU and were computed with the mteval-v13a.pl script provided by NIST. Table 2 summarizes the differences between the final configuration of the systems.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Both LIG and LIA systems are phrase-based translation models.</text>
                  <doc_id>21</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>All the data were first tokenized with the tokenizer provided for the workshop.</text>
                  <doc_id>22</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Kneser- Ney discounted LMs were built from monolingual corpora using the SRILM toolkit (Stolcke, 2002), while bilingual corpora were aligned at the wordlevel using GIZA++ (Och and Ney, 2003) or its multi-threaded version MGIZA++ (Gao and Vogel, 2008) for the large corpora UN and giga.</text>
                  <doc_id>23</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Phrase table and lexicalized reordering models were built with MOSES (Koehn et al., 2007).</text>
                  <doc_id>24</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, 14 features were used in the phrase-based models:</text>
                  <doc_id>25</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 When not specified otherwise &#8220;our&#8221; system refers to the</text>
                  <doc_id>26</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>LIGA system.</text>
                  <doc_id>27</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>CORPORA DESIGNATION SIZE (SENTENCES)</text>
                  <doc_id>28</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Test newstest2010 test10 2,489</text>
                  <doc_id>29</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; 5 translation model scores,</text>
                  <doc_id>30</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; 1 distance-based reordering score,</text>
                  <doc_id>31</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; 6 lexicalized reordering score,</text>
                  <doc_id>32</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; 1 LM score and</text>
                  <doc_id>33</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; 1 word penalty score.</text>
                  <doc_id>34</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The score weights were optimized on the test09 corpus according to the BLEU score with the MERT method (Och, 2003).</text>
                  <doc_id>35</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The experiments led specifically with either LIG or LIA system are respectively described in Sections 3 and 4.</text>
                  <doc_id>36</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Unless otherwise indicated, all the evaluations were performed using case-insensitive BLEU and were computed with the mteval-v13a.pl script provided by NIST.</text>
                  <doc_id>37</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Table 2 summarizes the differences between the final configuration of the systems.</text>
                  <doc_id>38</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 The LIG machine translation system</title>
        <text>LIG participated for the second time to the WMT shared news translation task for the French-English language pair.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>LIG participated for the second time to the WMT shared news translation task for the French-English language pair.</text>
              <doc_id>39</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Pre-processing</title>
            <text>Training data were first lowercased with the PERL script provided for the campaign. They were also processed in order to normalize a special French form (named euphonious &#8220;t&#8221;) as described in (Potet et al., 2010).
The baseline system was built using a 4-gram LM trained on the monolingual corpora provided last year and translation models trained on news-c and euro (Table 3, System 1). A significant improvement in terms of BLEU is obtained when taking into account a third corpus, UN, to build translation models (System 2). The next section describes the LMs that were trained using the monolingual data provided this year.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Training data were first lowercased with the PERL script provided for the campaign.</text>
                  <doc_id>40</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>They were also processed in order to normalize a special French form (named euphonious &#8220;t&#8221;) as described in (Potet et al., 2010).</text>
                  <doc_id>41</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The baseline system was built using a 4-gram LM trained on the monolingual corpora provided last year and translation models trained on news-c and euro (Table 3, System 1).</text>
                  <doc_id>42</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A significant improvement in terms of BLEU is obtained when taking into account a third corpus, UN, to build translation models (System 2).</text>
                  <doc_id>43</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The next section describes the LMs that were trained using the monolingual data provided this year.</text>
                  <doc_id>44</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Language model training</title>
            <text>Target LMs are standard 4-gram models trained on the provided monolingual corpus (mono-news-c, mono-euro and news-s). We decided to test two different n-gram cut-off settings. The fist set has low cut-offs: 1-2-3-3 (respectively for 1-gram, 2-gram, 3-gram and 4-gram counts), whereas the second one (LM 2 ) is more aggressive: 1-5-7-7. Experiment results (Table 3, Systems 3 and 4) show that resorting to LM 2 leads to an improvement of BLEU with respect to LM 1 . LM 2 was therefore used in the subsequent experiments.
FEATURES LIG SYSTEM LIA SYSTEM
Pre-processing
LM
Translation model
Text lowercased Normalization of French euphonious &#8217;t&#8217;
Training on mono-news-c, news-s and mono-euro 4-gram models
Training on news-c, euro and UN
Phrase table filtering Use of -monotone-at-punctuation option
Text truecased Reaccentuation of French words starting with a capital letter
Training on mono-news-c and news-s
5-gram models
Training on 10 M sentence pairs selected in news-c, euro, UN and giga</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Target LMs are standard 4-gram models trained on the provided monolingual corpus (mono-news-c, mono-euro and news-s).</text>
                  <doc_id>45</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We decided to test two different n-gram cut-off settings.</text>
                  <doc_id>46</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The fist set has low cut-offs: 1-2-3-3 (respectively for 1-gram, 2-gram, 3-gram and 4-gram counts), whereas the second one (LM 2 ) is more aggressive: 1-5-7-7.</text>
                  <doc_id>47</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Experiment results (Table 3, Systems 3 and 4) show that resorting to LM 2 leads to an improvement of BLEU with respect to LM 1 .</text>
                  <doc_id>48</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>LM 2 was therefore used in the subsequent experiments.</text>
                  <doc_id>49</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>FEATURES LIG SYSTEM LIA SYSTEM</text>
                  <doc_id>50</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Pre-processing</text>
                  <doc_id>51</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>LM</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Translation model</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Text lowercased Normalization of French euphonious &#8217;t&#8217;</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Training on mono-news-c, news-s and mono-euro 4-gram models</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Training on news-c, euro and UN</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Phrase table filtering Use of -monotone-at-punctuation option</text>
                  <doc_id>57</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Text truecased Reaccentuation of French words starting with a capital letter</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Training on mono-news-c and news-s</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5-gram models</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Training on 10 M sentence pairs selected in news-c, euro, UN and giga</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>3.3 Translation model training</title>
            <text>Translation models were trained from the parallel corpora news-c, euro and UN. Data were aligned at the word-level and then used to build standard phrase-based translation models. We filtered the obtained phrase table using the method described in (Johnson et al., 2007). Since this technique drastically reduces the size of the phrase table, while not degrading (and even slightly improving) the results on the development and test corpora (System 6), we decided to employ filtered phrase tables in the final configuration of the LIG system.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Translation models were trained from the parallel corpora news-c, euro and UN.</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Data were aligned at the word-level and then used to build standard phrase-based translation models.</text>
                  <doc_id>63</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We filtered the obtained phrase table using the method described in (Johnson et al., 2007).</text>
                  <doc_id>64</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Since this technique drastically reduces the size of the phrase table, while not degrading (and even slightly improving) the results on the development and test corpora (System 6), we decided to employ filtered phrase tables in the final configuration of the LIG system.</text>
                  <doc_id>65</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>3.4 Tuning</title>
            <text>For decoding, the system uses a log-linear combination of translation model scores with the LM log-probability. We prevent phrase reordering over punctuation using the MOSES option -monotone-atpunctuation. As the system can be beforehand tuned by adjusting the log-linear combination weights on a development corpus, we used the MERT method (System 5). Optimizing weights according to BLEU leads to an improvement with respect to the system with MOSES default value weights (System 5 vs System 4).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>For decoding, the system uses a log-linear combination of translation model scores with the LM log-probability.</text>
                  <doc_id>66</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We prevent phrase reordering over punctuation using the MOSES option -monotone-atpunctuation.</text>
                  <doc_id>67</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As the system can be beforehand tuned by adjusting the log-linear combination weights on a development corpus, we used the MERT method (System 5).</text>
                  <doc_id>68</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Optimizing weights according to BLEU leads to an improvement with respect to the system with MOSES default value weights (System 5 vs System 4).</text>
                  <doc_id>69</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>4</index>
            <title>3.5 Post-processing</title>
            <text>We also investigated the interest of a statistical post-editor (SPE) to improve translation hypotheses. About 9,000 sentences extracted from the news domain test corpora of the 2007&#8211;2009 WMT translation tasks were automatically translated by a system very similar to that described in (Potet et al., 2010), then manually post-edited. Manual corrections of translations were performed by means of the crowd-sourcing platform AMAZON MECHANICAL
TURK 2 ($0.15/sent.). These collected data make
a parallel corpus whose source part is MT output and target part is the human post-edited version of MT output. This are used to train a phrase-based SMT (with Moses without the tuning step) that automatically post-edit the MT output. That aims at learning how to correct translation hypotheses. System 7 obtained when post-processing MT 1-best output shows a slight improvement. However, SPE was not used in the final LIG system since we lacked time to apply SPE on the N-best hypotheses for the development and test corpora (the N-best being necessary for combination of LIG and LIA systems). Ths LIGA submission is thus a constrained one.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We also investigated the interest of a statistical post-editor (SPE) to improve translation hypotheses.</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>About 9,000 sentences extracted from the news domain test corpora of the 2007&#8211;2009 WMT translation tasks were automatically translated by a system very similar to that described in (Potet et al., 2010), then manually post-edited.</text>
                  <doc_id>71</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Manual corrections of translations were performed by means of the crowd-sourcing platform AMAZON MECHANICAL</text>
                  <doc_id>72</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>TURK 2 ($0.15/sent.).</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These collected data make</text>
                  <doc_id>74</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>a parallel corpus whose source part is MT output and target part is the human post-edited version of MT output.</text>
                  <doc_id>75</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This are used to train a phrase-based SMT (with Moses without the tuning step) that automatically post-edit the MT output.</text>
                  <doc_id>76</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>That aims at learning how to correct translation hypotheses.</text>
                  <doc_id>77</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>System 7 obtained when post-processing MT 1-best output shows a slight improvement.</text>
                  <doc_id>78</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>However, SPE was not used in the final LIG system since we lacked time to apply SPE on the N-best hypotheses for the development and test corpora (the N-best being necessary for combination of LIG and LIA systems).</text>
                  <doc_id>79</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Ths LIGA submission is thus a constrained one.</text>
                  <doc_id>80</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>5</index>
            <title>3.6 Recasing</title>
            <text>We trained a phrase-based recaser model on the news-s corpus using the provided MOSES scripts and applied it to uppercase translation outputs. A common and expected loss of around 1.5 casesensitive BLEU points was observed on the test corpus (news10) after applying this recaser (System 7) with respect to the score case-insensitive BLEU previously measured.
2 http://www.mturk.com/mturk/welcome
&#9839; SYSTEM DESCRIPTION BLEU SCORE test09 test10</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We trained a phrase-based recaser model on the news-s corpus using the provided MOSES scripts and applied it to uppercase translation outputs.</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A common and expected loss of around 1.5 casesensitive BLEU points was observed on the test corpus (news10) after applying this recaser (System 7) with respect to the score case-insensitive BLEU previously measured.</text>
                  <doc_id>82</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 http://www.mturk.com/mturk/welcome</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#9839; SYSTEM DESCRIPTION BLEU SCORE test09 test10</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 The LIA machine translation system</title>
        <text>This section describes the particularities of the MT system which was built at the LIA for its first participation to WMT.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This section describes the particularities of the MT system which was built at the LIA for its first participation to WMT.</text>
              <doc_id>85</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 System description</title>
            <text>The available corpora were pre-processed using an in-house script that normalizes quotes, dashes, spaces and ligatures. We also reaccentuated French words starting with a capital letter. We significantly cleaned up the crawled parallel giga corpus, keeping 19.3 M of the original 22.5 M sentence pairs. For example, sentence pairs with numerous numbers, nonalphanumeric characters or words starting with capital letters were removed. The whole training material is truecased, meaning that the words occuring after a strong punctuation mark were lowercased when they belonged to a dictionary of common alllowercased forms; the others were left unchanged.
The training of a 5-gram English LM was restrained to the news corpora mono-news-c and newss that we consider large enough to ignore other data. In order to reduce the size of the LM, we first limited the vocabulary of our model to a 1 M word vocabulary taking the most frequent words in the news corpora. We also resorted to cut-offs to discard infrequent n-grams (2-2-3-5 thresholds on 2- to 5-gram counts) and uses the SRILM option prune, which allowed us to train the LM on large data with 32 Gb RAM. Our translation models are phrase-based models (PBMs) built with MOSES with the following nondefault settings:
&#8226; maximum sentence length of 80 words,
&#8226; limit on the number of phrase translations loaded for each phrase fixed to 30.
Weights of LM, phrase table and lexicalized reordering model scores were optimized on the development corpus thanks to the MERT algorithm. Besides the size of used data, we experimented with two advanced features made available for
MOSES. Firstly, we filtered phrase tables using the
default setting -l a+e -n 30. This dramatically reduced phrase tables by dividing their size by a factor of 5 but did not improve our best configuration from the BLEU score perspective (Table 4, line 1); the method was therefore not kept in the LIA system. Secondly, we introduced reordering constraints in order to consider quoted material as a block. This method is particularly useful when citations included in sentences have to be translated. Two configurations were tested: zone markups inclusion around quotes and wall markups inclusion within zone markups. However, the measured gains were finally too marginal to include the method in the final system.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The available corpora were pre-processed using an in-house script that normalizes quotes, dashes, spaces and ligatures.</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We also reaccentuated French words starting with a capital letter.</text>
                  <doc_id>87</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We significantly cleaned up the crawled parallel giga corpus, keeping 19.3 M of the original 22.5 M sentence pairs.</text>
                  <doc_id>88</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For example, sentence pairs with numerous numbers, nonalphanumeric characters or words starting with capital letters were removed.</text>
                  <doc_id>89</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The whole training material is truecased, meaning that the words occuring after a strong punctuation mark were lowercased when they belonged to a dictionary of common alllowercased forms; the others were left unchanged.</text>
                  <doc_id>90</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The training of a 5-gram English LM was restrained to the news corpora mono-news-c and newss that we consider large enough to ignore other data.</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In order to reduce the size of the LM, we first limited the vocabulary of our model to a 1 M word vocabulary taking the most frequent words in the news corpora.</text>
                  <doc_id>92</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We also resorted to cut-offs to discard infrequent n-grams (2-2-3-5 thresholds on 2- to 5-gram counts) and uses the SRILM option prune, which allowed us to train the LM on large data with 32 Gb RAM.</text>
                  <doc_id>93</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Our translation models are phrase-based models (PBMs) built with MOSES with the following nondefault settings:</text>
                  <doc_id>94</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; maximum sentence length of 80 words,</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; limit on the number of phrase translations loaded for each phrase fixed to 30.</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Weights of LM, phrase table and lexicalized reordering model scores were optimized on the development corpus thanks to the MERT algorithm.</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Besides the size of used data, we experimented with two advanced features made available for</text>
                  <doc_id>98</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>MOSES.</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Firstly, we filtered phrase tables using the</text>
                  <doc_id>100</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>default setting -l a+e -n 30.</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This dramatically reduced phrase tables by dividing their size by a factor of 5 but did not improve our best configuration from the BLEU score perspective (Table 4, line 1); the method was therefore not kept in the LIA system.</text>
                  <doc_id>102</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Secondly, we introduced reordering constraints in order to consider quoted material as a block.</text>
                  <doc_id>103</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This method is particularly useful when citations included in sentences have to be translated.</text>
                  <doc_id>104</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Two configurations were tested: zone markups inclusion around quotes and wall markups inclusion within zone markups.</text>
                  <doc_id>105</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>However, the measured gains were finally too marginal to include the method in the final system.</text>
                  <doc_id>106</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Parallel corpus subsampling</title>
            <text>As the only news parallel corpus provided for the workshop contains 116 k sentence pairs, we must resort to parallel out-of-domain corpora in order to build reliable translation models. Information retrieval (IR) methods have been used in the past to subsample parallel corpora. For example, Hildebrand et al. (2005) used sentences belonging to the development and test corpora as queries to select the k most similar source sentences in an indexed parallel corpus. The retrieved sentence pairs constituted a training corpus for the translation models. The RALI submission for WMT10 proposed a similar approach that builds queries from the monolingual news corpus in order to select sentence pairs stylistically close to the news domain (Huet et al., 2010). This method has the major interest that it does not require to build a new training parallel corpus for each news data set to translate. Following the best configuration tested in (Huet et al.,
2010), we index the three out-of-domain corpora using LEMUR 3 , and build queries from English news-s sentences where stop words are removed. The 10 top sentence pairs retrieved per query are selected and added to the new training corpus if they are not redundant with a sentence pair already collected. The process is repeated until the training parallel corpus reaches a threshold over the number of retrieved pairs. Table 4 reports BLEU scores obtained with the LIA system using the in-domain corpus news-c and various amounts of out-of-domain data. MERT was re-run for each set of training data. The first four lines display results obtained with the same number of sentence pairs, which corresponds to the size of news-c appended to euro. The experiments show that using euro instead of the first sentences of UN and giga significantly improves BLEU scores, which indicates the better adequacy of euro with respect to the test10 corpus. The use of the IR method to select sentences from euro, UN and giga leads to a similar BLEU score to the one obtained with euro. The increase of the collected pairs up to 3 M pairs generates a significant improvement of 0.9 BLEU point. A further rise of the amount of collected pairs does not introduce a major gain since retrieving 10 M sentence pairs only augments BLEU from 29.1 to 29.3. This last configuration which leads to the best BLEU was used to build the final LIA system. Let us note that 2 M, 3 M and 15 M queries were required to respectively obtain 3 M, 5 M and 10 M sentence pairs because of the removal of redundant sentences in the increased corpus.
For a matter of comparison, a system was also built taking into account all the training material, i.e. 37 M sentence pairs 4 . This last system is outperformed by our best system built with IR and has finally close performance to the one obtained with news-c+euro relatively to the quantity of used data.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>As the only news parallel corpus provided for the workshop contains 116 k sentence pairs, we must resort to parallel out-of-domain corpora in order to build reliable translation models.</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Information retrieval (IR) methods have been used in the past to subsample parallel corpora.</text>
                  <doc_id>108</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For example, Hildebrand et al. (2005) used sentences belonging to the development and test corpora as queries to select the k most similar source sentences in an indexed parallel corpus.</text>
                  <doc_id>109</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The retrieved sentence pairs constituted a training corpus for the translation models.</text>
                  <doc_id>110</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The RALI submission for WMT10 proposed a similar approach that builds queries from the monolingual news corpus in order to select sentence pairs stylistically close to the news domain (Huet et al., 2010).</text>
                  <doc_id>111</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This method has the major interest that it does not require to build a new training parallel corpus for each news data set to translate.</text>
                  <doc_id>112</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Following the best configuration tested in (Huet et al.,</text>
                  <doc_id>113</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2010), we index the three out-of-domain corpora using LEMUR 3 , and build queries from English news-s sentences where stop words are removed.</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The 10 top sentence pairs retrieved per query are selected and added to the new training corpus if they are not redundant with a sentence pair already collected.</text>
                  <doc_id>115</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The process is repeated until the training parallel corpus reaches a threshold over the number of retrieved pairs.</text>
                  <doc_id>116</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Table 4 reports BLEU scores obtained with the LIA system using the in-domain corpus news-c and various amounts of out-of-domain data.</text>
                  <doc_id>117</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>MERT was re-run for each set of training data.</text>
                  <doc_id>118</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The first four lines display results obtained with the same number of sentence pairs, which corresponds to the size of news-c appended to euro.</text>
                  <doc_id>119</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The experiments show that using euro instead of the first sentences of UN and giga significantly improves BLEU scores, which indicates the better adequacy of euro with respect to the test10 corpus.</text>
                  <doc_id>120</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>The use of the IR method to select sentences from euro, UN and giga leads to a similar BLEU score to the one obtained with euro.</text>
                  <doc_id>121</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>The increase of the collected pairs up to 3 M pairs generates a significant improvement of 0.9 BLEU point.</text>
                  <doc_id>122</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>A further rise of the amount of collected pairs does not introduce a major gain since retrieving 10 M sentence pairs only augments BLEU from 29.1 to 29.3.</text>
                  <doc_id>123</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>This last configuration which leads to the best BLEU was used to build the final LIA system.</text>
                  <doc_id>124</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>Let us note that 2 M, 3 M and 15 M queries were required to respectively obtain 3 M, 5 M and 10 M sentence pairs because of the removal of redundant sentences in the increased corpus.</text>
                  <doc_id>125</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For a matter of comparison, a system was also built taking into account all the training material, i.e. 37 M sentence pairs 4 .</text>
                  <doc_id>126</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This last system is outperformed by our best system built with IR and has finally close performance to the one obtained with news-c+euro relatively to the quantity of used data.</text>
                  <doc_id>127</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 The system combination</title>
        <text>System combination is based on the 500-best outputs generated by the LIA and the LIG systems.
3 www.lemurproject.org 4 For this experiment, the data were split into three parts
to build independent alignment models: news-c+euro, UN and giga, and they were joined afterwards to build translation models.
USED PARALLEL CORPORA FILTERING
without with
They both used the MOSES option distinct, ensuring that the hypotheses produced for a given sentence are different inside an N-best list. Each N-best list is associated with a set of 14 scores and combined in several steps.
The first step takes as input lowercased 500-best lists, since preliminary experiments have shown a better behavior using only lowercased output (with cased output, combination presents some degradations). The score combination weights are optimized on the development corpus, in order to maximize the BLEU score at the sentence level when N-best lists are reordered according to the 14 available scores. To this end, we resorted to the SRILM nbest-optimize tool to do a simplex-based Amoeba search (Press et al., 1988) on the error function with multiple restarts to avoid local minima.
Once the optimized feature weights are computed independently for each system, N-best lists are turned into confusion networks (Mangu et al., 2000). The 14 features are used to compute posteriors relatively to all the hypotheses in the N-best list. Confusion networks are computed for each sentence and for each system. In Table 5 we present the ROVER (Fiscus, 1997) results for the LIA and LIG confusion networks (LIA CNC and LIG CNC). Then, both confusion networks computed for each sentence are merged into a single one. A ROVER is applied on the combined confusion network and generates a lowercased 1-best. The final step aims at producing cased hypotheses. The LIA system built from truecased corpora achieved significantly higher performance than the
LIG LIA LIG CNC LIA CNC LIG+LIA
LIG system trained on lowercased corpora (Table 5, two last lines). In order to get an improvement when combining the outputs, we had to adopt the following strategy. The 500-best truecased outputs of the LIA system are first merged in a word graph (and not a mesh lattice). Then, the lowercased 1-best previously obtained with ROVER is aligned with the graph in order to find the closest existing path, which is equivalent to matching an oracle with the graph. This method allows for several benefits. The new hypothesis is based on a &#8220;true&#8221; decoding pass generated by a truecased system and discarded marginal hypotheses. Moreover, the selected path offers a better BLEU score than the initial hypothesis with and without case. This method is better than the one which consists of applying the LIG recaser (section 3.6) on the combined (un-cased) hypothesis.
The new recased one-best hypothesis is then used as the final submission for WMT. Our combination approach improves on test11 the best single system by 0.5 case-insensitive BLEU point and by 0.4 case-sensitive BLEU (Table 5). However, it also introduces some mistakes by duplicating in particular some segments. We plan to apply rules at the segment level in order to reduce these artifacts.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>System combination is based on the 500-best outputs generated by the LIA and the LIG systems.</text>
              <doc_id>128</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3 www.lemurproject.org 4 For this experiment, the data were split into three parts</text>
              <doc_id>129</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>to build independent alignment models: news-c+euro, UN and giga, and they were joined afterwards to build translation models.</text>
              <doc_id>130</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>USED PARALLEL CORPORA FILTERING</text>
              <doc_id>131</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>without with</text>
              <doc_id>132</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>They both used the MOSES option distinct, ensuring that the hypotheses produced for a given sentence are different inside an N-best list.</text>
              <doc_id>133</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each N-best list is associated with a set of 14 scores and combined in several steps.</text>
              <doc_id>134</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The first step takes as input lowercased 500-best lists, since preliminary experiments have shown a better behavior using only lowercased output (with cased output, combination presents some degradations).</text>
              <doc_id>135</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The score combination weights are optimized on the development corpus, in order to maximize the BLEU score at the sentence level when N-best lists are reordered according to the 14 available scores.</text>
              <doc_id>136</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>To this end, we resorted to the SRILM nbest-optimize tool to do a simplex-based Amoeba search (Press et al., 1988) on the error function with multiple restarts to avoid local minima.</text>
              <doc_id>137</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Once the optimized feature weights are computed independently for each system, N-best lists are turned into confusion networks (Mangu et al., 2000).</text>
              <doc_id>138</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The 14 features are used to compute posteriors relatively to all the hypotheses in the N-best list.</text>
              <doc_id>139</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Confusion networks are computed for each sentence and for each system.</text>
              <doc_id>140</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Table 5 we present the ROVER (Fiscus, 1997) results for the LIA and LIG confusion networks (LIA CNC and LIG CNC).</text>
              <doc_id>141</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Then, both confusion networks computed for each sentence are merged into a single one.</text>
              <doc_id>142</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>A ROVER is applied on the combined confusion network and generates a lowercased 1-best.</text>
              <doc_id>143</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The final step aims at producing cased hypotheses.</text>
              <doc_id>144</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The LIA system built from truecased corpora achieved significantly higher performance than the</text>
              <doc_id>145</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>LIG LIA LIG CNC LIA CNC LIG+LIA</text>
              <doc_id>146</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>LIG system trained on lowercased corpora (Table 5, two last lines).</text>
              <doc_id>147</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In order to get an improvement when combining the outputs, we had to adopt the following strategy.</text>
              <doc_id>148</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The 500-best truecased outputs of the LIA system are first merged in a word graph (and not a mesh lattice).</text>
              <doc_id>149</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Then, the lowercased 1-best previously obtained with ROVER is aligned with the graph in order to find the closest existing path, which is equivalent to matching an oracle with the graph.</text>
              <doc_id>150</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This method allows for several benefits.</text>
              <doc_id>151</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The new hypothesis is based on a &#8220;true&#8221; decoding pass generated by a truecased system and discarded marginal hypotheses.</text>
              <doc_id>152</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, the selected path offers a better BLEU score than the initial hypothesis with and without case.</text>
              <doc_id>153</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>This method is better than the one which consists of applying the LIG recaser (section 3.6) on the combined (un-cased) hypothesis.</text>
              <doc_id>154</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The new recased one-best hypothesis is then used as the final submission for WMT.</text>
              <doc_id>155</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our combination approach improves on test11 the best single system by 0.5 case-insensitive BLEU point and by 0.4 case-sensitive BLEU (Table 5).</text>
              <doc_id>156</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, it also introduces some mistakes by duplicating in particular some segments.</text>
              <doc_id>157</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We plan to apply rules at the segment level in order to reduce these artifacts.</text>
              <doc_id>158</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>6 Conclusion</title>
        <text>This paper presented two statistical machine translation systems developed at different sites using MOSES and the combination of these systems. The LIGA submission presented this year was ranked among the best MT system for the French-English direction. This campaign was the first shot for LIA and the second for LIG. Beside following the traditional pipeline for building a phrase-based translation system, each individual system led to specific works: LIG worked on using SPE as post-treatment, LIA focused on extracting useful data from largesized corpora. And their combination implied to address the interesting issue of matching results from systems with different casing approaches. WMT is a great opportunity to chase after performance and joining our efforts has allowed to save considerable amount of time for data preparation and tuning choices (even when final decisions were different among systems), yet obtaining very competitive results. This year, our goal was to develop state-of-the-art systems so as to investigate new approaches for related topics such as translation with human-in-the-loop or multilingual interaction systems (e.g. vocal telephone information-query dialogue systems in multiple languages or language portability of such systems).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This paper presented two statistical machine translation systems developed at different sites using MOSES and the combination of these systems.</text>
              <doc_id>159</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The LIGA submission presented this year was ranked among the best MT system for the French-English direction.</text>
              <doc_id>160</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This campaign was the first shot for LIA and the second for LIG.</text>
              <doc_id>161</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Beside following the traditional pipeline for building a phrase-based translation system, each individual system led to specific works: LIG worked on using SPE as post-treatment, LIA focused on extracting useful data from largesized corpora.</text>
              <doc_id>162</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>And their combination implied to address the interesting issue of matching results from systems with different casing approaches.</text>
              <doc_id>163</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>WMT is a great opportunity to chase after performance and joining our efforts has allowed to save considerable amount of time for data preparation and tuning choices (even when final decisions were different among systems), yet obtaining very competitive results.</text>
              <doc_id>164</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>This year, our goal was to develop state-of-the-art systems so as to investigate new approaches for related topics such as translation with human-in-the-loop or multilingual interaction systems (e.g. vocal telephone information-query dialogue systems in multiple languages or language portability of such systems).</text>
              <doc_id>165</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Used corpora</caption>
        <reference_text>In PAGE 1: ...1 Used data Globally, our system1 was built using all the French and English data supplied for the workshop?s shared translation task, apart from the Gigaword monolin- gual corpora released by the LDC.  Table1  sums up the used data and introduces designations that we follow in the remainder of this paper to refer to cor- pora. Four corpora were used to build translation models: news-c, euro, UN and giga, while three others are employed to train monolingual language models (LMs)....</reference_text>
        <page_num>2</page_num>
        <head>
          <rows>
            <row>
              <cell>News Commentary v6</cell>
              <cell>news-c</cell>
              <cell>116 k</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Europarl v6</cell>
              <cell>euro</cell>
              <cell>1.8 M</cell>
            </row>
            <row>
              <cell>United Nation corpus</cell>
              <cell>UN</cell>
              <cell>12 M</cell>
            </row>
            <row>
              <cell>109 corpus</cell>
              <cell>giga</cell>
              <cell>23 M</cell>
            </row>
            <row>
              <cell>English Monolingual training</cell>
              <cell>English Monolingual training</cell>
            </row>
            <row>
              <cell>News Commentary v6</cell>
              <cell>mono-news-c</cell>
              <cell>181 k</cell>
            </row>
            <row>
              <cell>Shuffled News Crawl corpus (from 2007 to 2011)</cell>
              <cell>news-s</cell>
              <cell>25 M</cell>
            </row>
            <row>
              <cell>Europarl v6</cell>
              <cell>mono-euro</cell>
              <cell>1.8 M</cell>
            </row>
            <row>
              <cell>newstest2008</cell>
              <cell>test08</cell>
              <cell>2,051</cell>
            </row>
            <row>
              <cell>newssyscomb2009</cell>
              <cell>testcomb09</cell>
              <cell>502</cell>
            </row>
            <row>
              <cell>newstest2009</cell>
              <cell>test09</cell>
              <cell>2,525</cell>
            </row>
            <row>
              <cell>T</cell>
              <cell>e</cell>
              <cell>s</cell>
              <cell>t</cell>
            </row>
            <row>
              <cell>newstest2010</cell>
              <cell>test10</cell>
              <cell>2,489</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Incremental improvement of the LIG system in terms of case-insensitive BLEU (%), except for line 8 where case-sensitive BLEU (%) are reported</caption>
        <reference_text>In PAGE 2: ..., 2010). The baseline system was built using a 4-gram LM trained on the monolingual corpora provided last year and translation models trained on news-c and euro ( Table3 , System 1). A significant improve- ment in terms of BLEU is obtained when taking into account a third corpus, UN, to build translation mod- els (System 2)....  In PAGE 2: ... The fist set has low cut-offs: 1-2-3-3 (respectively for 1-gram, 2-gram, 3-gram and 4-gram counts), whereas the second one (LM2) is more aggressive: 1-5-7-7. Experiment re- sults ( Table3 , Systems 3 and 4) show that resorting to LM2 leads to an improvement of BLEU with re- spect to LM1. LM2 was therefore used in the sub- sequent experiments....</reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>?</cell>
              <cell>SYSTEM DESCRIPTION</cell>
              <cell>BLEU SCORE</cell>
              <cell>BLEU SCORE</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>test09</cell>
              <cell>test10</cell>
            </row>
            <row>
              <cell>1</cell>
              <cell>Training: euro+news-c</cell>
              <cell>24.89</cell>
              <cell>26.01</cell>
            </row>
            <row>
              <cell>2</cell>
              <cell>Training: euro+news-c+UN</cell>
              <cell>25.44</cell>
              <cell>26.43</cell>
            </row>
            <row>
              <cell>3</cell>
              <cell>2 + LM1</cell>
              <cell>24.81</cell>
              <cell>27.19</cell>
            </row>
            <row>
              <cell>4</cell>
              <cell>2 + LM2</cell>
              <cell>25.37</cell>
              <cell>27.25</cell>
            </row>
            <row>
              <cell>5</cell>
              <cell>4 + MERT on test09</cell>
              <cell>26.83</cell>
              <cell>27.53</cell>
            </row>
            <row>
              <cell>6</cell>
              <cell>5 + phrase-table filtering</cell>
              <cell>27.09</cell>
              <cell>27.64</cell>
            </row>
            <row>
              <cell>7</cell>
              <cell>6 + SPE</cell>
              <cell>27.53</cell>
              <cell>27.74</cell>
            </row>
            <row>
              <cell>8</cell>
              <cell>6 + recaser</cell>
              <cell>24.95</cell>
              <cell>26.07</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: BLEU (%) on test10 measured with the LIA system using different training parallel corpora</caption>
        <reference_text>In PAGE 5: ... The process is repeated until the training parallel cor- pus reaches a threshold over the number of retrieved pairs.  Table4  reports BLEU scores obtained with the LIA system using the in-domain corpus news-c and various amounts of out-of-domain data. MERT was re-run for each set of training data....</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>USED PARALLEL CORPORA</cell>
              <cell>FILTERING</cell>
              <cell>FILTERING</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>without</cell>
              <cell>with</cell>
            </row>
            <row>
              <cell>news-c + euro (1.77 M)</cell>
              <cell>28.1</cell>
              <cell>28.0</cell>
            </row>
            <row>
              <cell>news-c + 1.77 M of UN</cell>
              <cell>27.2</cell>
              <cell>-</cell>
            </row>
            <row>
              <cell>news-c + 1.77 M of giga</cell>
              <cell>27.1</cell>
              <cell>-</cell>
            </row>
            <row>
              <cell>news-c + 1.77 M with IR</cell>
              <cell>28.2</cell>
              <cell>-</cell>
            </row>
            <row>
              <cell>news-c + 3 M with IR</cell>
              <cell>29.1</cell>
              <cell>29.0</cell>
            </row>
            <row>
              <cell>news-c + 5 M with IR</cell>
              <cell>28.8</cell>
              <cell>-</cell>
            </row>
            <row>
              <cell>news-c + 10 M with IR</cell>
              <cell>29.3</cell>
              <cell>29.2</cell>
            </row>
            <row>
              <cell>All data</cell>
              <cell>28.9</cell>
              <cell>29.0</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TET</source>
        <caption>Table 5: Performance measured before and after combining systems</caption>
        <reference_text></reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>case-sensitive</cell>
              <cell>test10</cell>
              <cell>26.1</cell>
              <cell>28.4</cell>
              <cell>27.0</cell>
              <cell>28.4</cell>
              <cell>28.7</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>BLEU</cell>
              <cell>test11</cell>
              <cell>26.9</cell>
              <cell>28.4</cell>
              <cell>27.5</cell>
              <cell>28.4</cell>
              <cell>28.8</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Jonathan G Fiscus</author>
        </authors>
        <title>A post-processing system to yield reduced word error rates:recognizer output voting error reduction (ROVER).</title>
        <publication>In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding,</publication>
        <pages>347--354</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Qin Gao</author>
          <author>Stephan Vogel</author>
        </authors>
        <title>Parallel implementations of word alignment tool.</title>
        <publication>In Proceedings of the ACL Workshop: Software Engineering, Testing, and Quality Assurance for Natural Language Processing,</publication>
        <pages>49--57</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Almut Silja Hildebrand</author>
          <author>Matthias Eck</author>
          <author>Stephan Vogel</author>
          <author>Alex Waibel</author>
        </authors>
        <title>Adaptation of the translation model for statistical machine translation based on information retrieval.</title>
        <publication>In Proceedings of the 10th conference of the European Association for Machine Translation (EAMT),</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>St&#233;phane Huet</author>
          <author>Julien Bourdaillet</author>
          <author>Alexandre Patry</author>
          <author>Philippe Langlais</author>
        </authors>
        <title>The RALI machine translation system for WMT</title>
        <publication>In Proceedings of the ACL Joint 5th Workshop on Statistical Machine Translation and Metrics (WMT),</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Howard Johnson</author>
          <author>Joel Martin</author>
          <author>George Foster</author>
          <author>Roland</author>
        </authors>
        <title>Improving translation quality by discarding most of the phrasetable.</title>
        <publication>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</publication>
        <pages>967--975</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Hieu Hoang</author>
          <author>Alexandra Birch</author>
          <author>Chris Callison-Burch</author>
          <author>Marcello Federico</author>
          <author>Nicola Bertoldi</author>
          <author>Brooke Cowan</author>
          <author>Wade Shen</author>
          <author>Christine Moran</author>
          <author>Richard Zens</author>
          <author>Chris Dyer</author>
          <author>Ondrej Bojar</author>
          <author>Alexandra Constantin</author>
          <author>Evan Herbst</author>
        </authors>
        <title>Moses: Open source toolkit for statistical machine translation.</title>
        <publication>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), Companion Volume,</publication>
        <pages>177--180</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Lidia Mangu</author>
          <author>Eric Brill</author>
          <author>Andreas Stolcke</author>
        </authors>
        <title>Finding consensus in speech recognition: Word error minimization and other applications of confusion networks.</title>
        <publication>None</publication>
        <pages>400</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>A systematic comparison of various statistical alignment models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Franz Josef Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL),</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Marion Potet</author>
          <author>Laurent Besacier</author>
          <author>Herv&#233; Blanchon</author>
        </authors>
        <title>The LIG machine translation for WMT</title>
        <publication>In Proceedings of the ACL Joint 5th Workshop on Statistical Machine Translation and Metrics (WMT),</publication>
        <pages>None</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>William H Press</author>
          <author>Brian P Flannery</author>
          <author>Saul A Teukolsky</author>
          <author>William T Vetterling</author>
        </authors>
        <title>Numerical Recipes in C: The Art of Scientific Computing.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1988</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Fiscus, 1997</string>
        <sentence_id>48422</sentence_id>
        <char_offset>33</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Gao and Vogel, 2008</string>
        <sentence_id>48303</sentence_id>
        <char_offset>230</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Hildebrand et al. (2005)</string>
        <sentence_id>48389</sentence_id>
        <char_offset>13</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>3</reference_id>
        <string>Huet et al., 2010</string>
        <sentence_id>48391</sentence_id>
        <char_offset>185</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>4</reference_id>
        <string>Johnson et al., 2007</string>
        <sentence_id>48344</sentence_id>
        <char_offset>69</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>5</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>48304</sentence_id>
        <char_offset>70</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>6</reference_id>
        <string>Mangu et al., 2000</string>
        <sentence_id>48419</sentence_id>
        <char_offset>128</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>7</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>48303</sentence_id>
        <char_offset>172</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>8</reference_id>
        <string>Och, 2003</string>
        <sentence_id>48315</sentence_id>
        <char_offset>104</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>9</reference_id>
        <string>Potet et al., 2010</string>
        <sentence_id>48321</sentence_id>
        <char_offset>109</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>9</reference_id>
        <string>Potet et al., 2010</string>
        <sentence_id>48351</sentence_id>
        <char_offset>182</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>10</reference_id>
        <string>Press et al., 1988</string>
        <sentence_id>48418</sentence_id>
        <char_offset>95</char_offset>
      </citation>
    </citations>
  </content>
</document>
