<document>
  <filename>E09-1011</filename>
  <authors/>
  <title>Syntactic Phrase Reordering for English-to-Arabic Statistical Machine Translation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>Syntactic Reordering of the source language to better match the phrase structure of the target language has been shown to improve the performance of phrase-based Statistical Machine Translation. This paper applies syntactic reordering to English-to-Arabic translation. It introduces reordering rules, and motivates them linguistically. It also studies the effect of combining reordering with Arabic morphological segmentation, a preprocessing technique that has been shown to improve Arabic-English and English- Arabic translation. We report on results in the news text domain, the UN text domain and in the spoken travel domain.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Syntactic Reordering of the source language to better match the phrase structure of the target language has been shown to improve the performance of phrase-based Statistical Machine Translation.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This paper applies syntactic reordering to English-to-Arabic translation.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>It introduces reordering rules, and motivates them linguistically.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>It also studies the effect of combining reordering with Arabic morphological segmentation, a preprocessing technique that has been shown to improve Arabic-English and English- Arabic translation.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We report on results in the news text domain, the UN text domain and in the spoken travel domain.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Phrase-based Statistical Machine Translation has proven to be a robust and effective approach to machine translation, providing good performance without the need for explicit linguistic information. Phrase-based SMT systems, however, have limited capabilities in dealing with long distance phenomena, since they rely on local alignments. Automatically learned reordering models, which can be conditioned on lexical items from both the source and the target, provide some limited reordering capability when added to SMT systems. One approach that explicitly deals with long distance reordering is to reorder the source side to better match the target side, using predefined rules. The reordered source is then used as input to the phrase-based SMT system. This approach indirectly incorporates structure information since the reordering rules are applied on the parse trees of the source sentence. Obviously, the same reordering has to be applied to both training data and test data. Despite the added complexity of parsing the data, this technique has shown improvements, especially when good parses of the source side exist. It has been successfully applied to German-to- English and Chinese-to-English SMT (Collins et al., 2005; Wang et al., 2007).
In this paper, we propose the use of a similar approach for English-to-Arabic SMT. Unlike most other work on Arabic translation, our work is in the direction of the more morphologically complex language, which poses unique challenges. We propose a set of syntactic reordering rules on the English source to align it better to the Arabic target. The reordering rules exploit systematic differences between the syntax of Arabic and the syntax of English; they specifically address two syntactic constructs. The first is the Subject-Verb order in independent sentences, where the preferred order in written Arabic is Verb-Subject. The second is the noun phrase structure, where many differences exist between the two languages, among them the order of adjectives, compound nouns and genitive constructs, as well as the way definiteness is marked. The implementation of these rules is fairly straightforward since they are applied to the parse tree. It has been noted in previous work (Habash, 2007) that syntactic reordering does not improve translation if the parse quality is not good enough. Since in this paper our source language is English, the parses are more reliable, and result in more correct reorderings. We show that using the reordering rules results in gains in the translation scores and study the effect of the training data size on those gains.
This paper also investigates the effect of using morphological segmentation of the Arabic target
in combination with the reordering rules. Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al., 2008) translation, although the gains tend to decrease with increasing training data size.
Section 2 provides linguistic motivation for the paper. It describes the rich morphology of Arabic, and its implications on SMT. It also describes the syntax of the verb phrase and noun phrase in Arabic, and how they differ from their English counterparts. In Section 3, we describe some of the relevant previous work. In Section 4, we present the preprocessing techniques used in the experiments. Section 5 describes the translation system, the data used, and then presents and discusses the experimental results from three domains: news text, UN data and spoken dialogue from the travel domain. The final section provides a brief summary and conclusion.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Phrase-based Statistical Machine Translation has proven to be a robust and effective approach to machine translation, providing good performance without the need for explicit linguistic information.</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Phrase-based SMT systems, however, have limited capabilities in dealing with long distance phenomena, since they rely on local alignments.</text>
              <doc_id>6</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Automatically learned reordering models, which can be conditioned on lexical items from both the source and the target, provide some limited reordering capability when added to SMT systems.</text>
              <doc_id>7</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>One approach that explicitly deals with long distance reordering is to reorder the source side to better match the target side, using predefined rules.</text>
              <doc_id>8</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The reordered source is then used as input to the phrase-based SMT system.</text>
              <doc_id>9</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This approach indirectly incorporates structure information since the reordering rules are applied on the parse trees of the source sentence.</text>
              <doc_id>10</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Obviously, the same reordering has to be applied to both training data and test data.</text>
              <doc_id>11</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Despite the added complexity of parsing the data, this technique has shown improvements, especially when good parses of the source side exist.</text>
              <doc_id>12</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>It has been successfully applied to German-to- English and Chinese-to-English SMT (Collins et al., 2005; Wang et al., 2007).</text>
              <doc_id>13</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this paper, we propose the use of a similar approach for English-to-Arabic SMT.</text>
              <doc_id>14</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Unlike most other work on Arabic translation, our work is in the direction of the more morphologically complex language, which poses unique challenges.</text>
              <doc_id>15</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We propose a set of syntactic reordering rules on the English source to align it better to the Arabic target.</text>
              <doc_id>16</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The reordering rules exploit systematic differences between the syntax of Arabic and the syntax of English; they specifically address two syntactic constructs.</text>
              <doc_id>17</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The first is the Subject-Verb order in independent sentences, where the preferred order in written Arabic is Verb-Subject.</text>
              <doc_id>18</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The second is the noun phrase structure, where many differences exist between the two languages, among them the order of adjectives, compound nouns and genitive constructs, as well as the way definiteness is marked.</text>
              <doc_id>19</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The implementation of these rules is fairly straightforward since they are applied to the parse tree.</text>
              <doc_id>20</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>It has been noted in previous work (Habash, 2007) that syntactic reordering does not improve translation if the parse quality is not good enough.</text>
              <doc_id>21</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Since in this paper our source language is English, the parses are more reliable, and result in more correct reorderings.</text>
              <doc_id>22</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>We show that using the reordering rules results in gains in the translation scores and study the effect of the training data size on those gains.</text>
              <doc_id>23</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>This paper also investigates the effect of using morphological segmentation of the Arabic target</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>in combination with the reordering rules.</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al., 2008) translation, although the gains tend to decrease with increasing training data size.</text>
              <doc_id>26</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Section 2 provides linguistic motivation for the paper.</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It describes the rich morphology of Arabic, and its implications on SMT.</text>
              <doc_id>28</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>It also describes the syntax of the verb phrase and noun phrase in Arabic, and how they differ from their English counterparts.</text>
              <doc_id>29</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Section 3, we describe some of the relevant previous work.</text>
              <doc_id>30</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In Section 4, we present the preprocessing techniques used in the experiments.</text>
              <doc_id>31</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Section 5 describes the translation system, the data used, and then presents and discusses the experimental results from three domains: news text, UN data and spoken dialogue from the travel domain.</text>
              <doc_id>32</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The final section provides a brief summary and conclusion.</text>
              <doc_id>33</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Arabic Linguistic Issues</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>34</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Arabic Morphology</title>
            <text>Arabic has a complex morphology compared to English. The Arabic noun and adjective are inflected for gender and number; the verb is inflected in addition for tense, voice, mood and person. Various clitics can attach to words as well: Conjunctions, prepositions and possessive pronouns attach to nouns, and object pronouns attach to verbs. The example below shows the decomposition into stems and clitics of the Arabic verb phrase wsyqAblhm 1 and noun phrase wbydh, both of which are written as one word:
(1) a. w+ s+ yqAbl +hm and will meet-3SM them and he will meet them
b. w+ b+ yd +h and with hand his and with his hand
An Arabic corpus will, therefore, have more surface forms than an equivalent English corpus, and will also be sparser. In the LDC news corpora used in this paper (see Section 5.2), the average English sentence length is 33 words compared to the Arabic 25 words.
1 All examples in this paper are written in the Buckwalter Transliteration System (http://www.qamus.org/transliteration.htm)
Although the Arabic language family consists of many dialects, none of them has a standard orthography. This affects the consistency of the orthography of Modern Standard Arabic (MSA), the only written variety of Arabic. Certain characters are written inconsistently in different data sources: Final &#8217;y&#8217; is sometimes written as &#8217;Y&#8217; (Alif mqSwrp), and initial Alif hamza (The Buckwalter characters &#8217;&lt;&#8217; and &#8217;{&#8217;) are written as bare alif (A). Arabic is usually written without the diacritics that denote short vowels. This creates an ambiguity at the word level, since a word can have more than one reading. These factors adversely affect the performance of Arabic-to-English SMT, especially in the English-to-Arabic direction. Simple pattern matching is not enough to perform morphological analysis and decomposition, since a certain string of characters can, in principle, be either an affixed morpheme or part of the base word itself. Word-level linguistic information as well as context analysis are needed. For example the written form wly can mean either ruler or and for me, depending on the context. Only in the latter case should it be decomposed.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Arabic has a complex morphology compared to English.</text>
                  <doc_id>35</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The Arabic noun and adjective are inflected for gender and number; the verb is inflected in addition for tense, voice, mood and person.</text>
                  <doc_id>36</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Various clitics can attach to words as well: Conjunctions, prepositions and possessive pronouns attach to nouns, and object pronouns attach to verbs.</text>
                  <doc_id>37</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The example below shows the decomposition into stems and clitics of the Arabic verb phrase wsyqAblhm 1 and noun phrase wbydh, both of which are written as one word:</text>
                  <doc_id>38</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(1) a. w+ s+ yqAbl +hm and will meet-3SM them and he will meet them</text>
                  <doc_id>39</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>b. w+ b+ yd +h and with hand his and with his hand</text>
                  <doc_id>40</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>An Arabic corpus will, therefore, have more surface forms than an equivalent English corpus, and will also be sparser.</text>
                  <doc_id>41</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the LDC news corpora used in this paper (see Section 5.2), the average English sentence length is 33 words compared to the Arabic 25 words.</text>
                  <doc_id>42</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 All examples in this paper are written in the Buckwalter Transliteration System (http://www.qamus.org/transliteration.htm)</text>
                  <doc_id>43</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Although the Arabic language family consists of many dialects, none of them has a standard orthography.</text>
                  <doc_id>44</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This affects the consistency of the orthography of Modern Standard Arabic (MSA), the only written variety of Arabic.</text>
                  <doc_id>45</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Certain characters are written inconsistently in different data sources: Final &#8217;y&#8217; is sometimes written as &#8217;Y&#8217; (Alif mqSwrp), and initial Alif hamza (The Buckwalter characters &#8217;&lt;&#8217; and &#8217;{&#8217;) are written as bare alif (A).</text>
                  <doc_id>46</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Arabic is usually written without the diacritics that denote short vowels.</text>
                  <doc_id>47</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This creates an ambiguity at the word level, since a word can have more than one reading.</text>
                  <doc_id>48</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>These factors adversely affect the performance of Arabic-to-English SMT, especially in the English-to-Arabic direction.</text>
                  <doc_id>49</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Simple pattern matching is not enough to perform morphological analysis and decomposition, since a certain string of characters can, in principle, be either an affixed morpheme or part of the base word itself.</text>
                  <doc_id>50</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Word-level linguistic information as well as context analysis are needed.</text>
                  <doc_id>51</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>For example the written form wly can mean either ruler or and for me, depending on the context.</text>
                  <doc_id>52</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>Only in the latter case should it be decomposed.</text>
                  <doc_id>53</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Arabic Syntax</title>
            <text>In this section, we describe a number of syntactic facts about Arabic which are relevant to the reordering rules described in Section 4.2.
Clause Structure
In Arabic, the main sentence usually has the order Verb-Subject-Object (VSO). The order Subject-Verb-Object (SVO) also occurs, but is less frequent than VSO. The verb agrees with the subject in gender and number in the SVO order, but only in gender in the VSO order (Examples 2c and 2d).
(2) a. Akl Alwld AltfAHp ate-3SM the-boy the-apple the boy ate the apple
b. Alwld Akl AltfAHp the-boy ate-3SM the-apple the boy ate the apple
c. Akl AlAwlAd AltfAHAt ate-3SM the-boys the-apples the boys ate the apples
d. AlAwlAd AklwA AltfAHAt the-boys ate-3PM the-apples the boys ate the apples
In a dependent clause, the order must be SVO, as illustrated by the ungrammaticality of Example 3b below. As we discuss in more detail later, this distinction between dependent and independent clauses has to be taken into account when the syntactic reordering rules are applied.
(3) a. qAl An Alwld Akl AltfAHp said-3SM that the-boy ate the-apple he said that the boy ate the apple b. *qAl An Akl Alwld AltfAHp
said-3SM that ate the-boy the-apple he said that the boy ate the apple
Another pertinent fact is that the negation particle has to always preceed the verb:
(4) lm yAkl Alwld AltfAHp not eat-3SM the-boy the-apple the boy did not eat the apple
Noun Phrase The Arabic noun phrase can have constructs that are quite different from English. The adjective in Arabic follows the noun that it modifies, and it is marked with the definite article, if the head noun is definite:
(5) AlbAb Alkbyr the-door the-big the big door
The Arabic equivalent of the English possessive, compound nouns and the of -relationship is the Arabic idafa construct, which compounds two or more nouns. Therefore, N 1 &#8217;s N 2 and N 2 of N 1 are both translated as N 2 N 1 in Arabic. As Example 6b shows, this construct can also be chained recursively.
(6) a. bAb Albyt door the-house the house&#8217;s door b. mftAH bAb Albyt
key door the-house The key to the door of the house
Example 6 also shows that an idafa construct is made definite by adding the definite article Al- to the last noun in the noun phrase. Adjectives follow the idafa noun phrase, regardless of which noun in the chain they modify. Thus, Example 7 is ambiguous in that the adjective kbyr (big) can modify any of the preceding three nouns. The same is true for relative clauses that modify a noun.
(7) mftAH key bAb Albyt Alkbyr door the-house the-big
These and other differences between the Arabic and English syntax are likely to affect the quality of automatic alignments, since corresponding words will occupy positions in the sentence that are far apart, especially when the relevant words (e.g. the verb and its subject) are separated by subordinate clauses. In such cases, the lexicalized distortion models used in phrase-based SMT do not have the capability of performing reorderings correctly. This limitation adversely affects the translation quality.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In this section, we describe a number of syntactic facts about Arabic which are relevant to the reordering rules described in Section 4.2.</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Clause Structure</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In Arabic, the main sentence usually has the order Verb-Subject-Object (VSO).</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The order Subject-Verb-Object (SVO) also occurs, but is less frequent than VSO.</text>
                  <doc_id>57</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The verb agrees with the subject in gender and number in the SVO order, but only in gender in the VSO order (Examples 2c and 2d).</text>
                  <doc_id>58</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(2) a. Akl Alwld AltfAHp ate-3SM the-boy the-apple the boy ate the apple</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>b. Alwld Akl AltfAHp the-boy ate-3SM the-apple the boy ate the apple</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>c. Akl AlAwlAd AltfAHAt ate-3SM the-boys the-apples the boys ate the apples</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>d. AlAwlAd AklwA AltfAHAt the-boys ate-3PM the-apples the boys ate the apples</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In a dependent clause, the order must be SVO, as illustrated by the ungrammaticality of Example 3b below.</text>
                  <doc_id>63</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As we discuss in more detail later, this distinction between dependent and independent clauses has to be taken into account when the syntactic reordering rules are applied.</text>
                  <doc_id>64</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(3) a. qAl An Alwld Akl AltfAHp said-3SM that the-boy ate the-apple he said that the boy ate the apple b.</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>*qAl An Akl Alwld AltfAHp</text>
                  <doc_id>66</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>said-3SM that ate the-boy the-apple he said that the boy ate the apple</text>
                  <doc_id>67</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Another pertinent fact is that the negation particle has to always preceed the verb:</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(4) lm yAkl Alwld AltfAHp not eat-3SM the-boy the-apple the boy did not eat the apple</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Noun Phrase The Arabic noun phrase can have constructs that are quite different from English.</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The adjective in Arabic follows the noun that it modifies, and it is marked with the definite article, if the head noun is definite:</text>
                  <doc_id>71</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(5) AlbAb Alkbyr the-door the-big the big door</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The Arabic equivalent of the English possessive, compound nouns and the of -relationship is the Arabic idafa construct, which compounds two or more nouns.</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, N 1 &#8217;s N 2 and N 2 of N 1 are both translated as N 2 N 1 in Arabic.</text>
                  <doc_id>74</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As Example 6b shows, this construct can also be chained recursively.</text>
                  <doc_id>75</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(6) a. bAb Albyt door the-house the house&#8217;s door b. mftAH bAb Albyt</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>key door the-house The key to the door of the house</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Example 6 also shows that an idafa construct is made definite by adding the definite article Al- to the last noun in the noun phrase.</text>
                  <doc_id>78</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Adjectives follow the idafa noun phrase, regardless of which noun in the chain they modify.</text>
                  <doc_id>79</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Thus, Example 7 is ambiguous in that the adjective kbyr (big) can modify any of the preceding three nouns.</text>
                  <doc_id>80</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The same is true for relative clauses that modify a noun.</text>
                  <doc_id>81</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(7) mftAH key bAb Albyt Alkbyr door the-house the-big</text>
                  <doc_id>82</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>These and other differences between the Arabic and English syntax are likely to affect the quality of automatic alignments, since corresponding words will occupy positions in the sentence that are far apart, especially when the relevant words (e.g. the verb and its subject) are separated by subordinate clauses.</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In such cases, the lexicalized distortion models used in phrase-based SMT do not have the capability of performing reorderings correctly.</text>
                  <doc_id>84</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This limitation adversely affects the translation quality.</text>
                  <doc_id>85</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Previous Work</title>
        <text>Most of the work in Arabic machine translation is done in the Arabic-to-English direction. The other direction, however, is also important, since it opens the wealth of information in different domains that is available in English to the Arabic speaking world. Also, since Arabic is a morphologically richer language, translating into Arabic poses unique issues that are not present in the opposite direction. The only works on Englishto-Arabic SMT that we are aware of are Badr et al. (2008), and Sarikaya and Deng (2007). Badr et al. show that using segmentation and recombination as pre- and post- processing steps leads to significant gains especially for smaller training data corpora. Sarikaya and Deng use Joint Morphological-Lexical Language Models to rerank the output of an English-to-Arabic MT system. They use regular expression-based segmentation of the Arabic so as not to run into recombination issues on the output side. Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size. They use a trigram language model and the Arabic morphological analyzer MADA (Habash and Rambow, 2005) respectively, to segment the Arabic side of their corpora. Other work on Arabicto-English SMT tries to address the word reordering problem. Habash (2007) automatically learns syntactic reordering rules that are then applied to the Arabic side of the parallel corpora. The words are aligned in a sentence pair, then the Arabic sentence is parsed to extract reordering rules based on how the constituents in the parse tree are reordered on the English side. No significant improvement is
shown with reordering when compared to a baseline that uses a non-lexicalized distance reordering model. This is attributed in the paper to the poor quality of parsing.
Syntax-based reordering as a preprocessing step has been applied to many language pairs other than English-Arabic. Most relevant to the approach in this paper are Collins et al. (2005) and Wang et al. (2007). Both parse the source side and then reorder the sentence based on predefined, linguistically motivated rules. Significant gain is reported for German-to-English and Chinese-to-English translation. Both suggest that reordering as a preprocessing step results in better alignment, and reduces the reliance on the distortion model. Popovic and Ney (2006) use similar methods to reorder German by looking at the POS tags for German-to-English and German-to- Spanish. They show significant improvements on test set sentences that do get reordered as well as those that don&#8217;t, which is attributed to the improvement of the extracted phrases. (Xia and McCord, 2004) present a similar approach, with a notable difference: the re-ordering rules are automatically learned from aligning parse trees for both the source and target sentences. They report a 10% relative gain for English-to-French translation. Although target-side parsing is optional in this approach, it is needed to take full advantage of the approach. This is a bigger issue when no reliable parses are available for the target language, as is the case in this paper. More generally, the use of automatically-learned rules has the advantage of readily applicable to different language pairs. The use of deterministic, pre-defined rules, however, has the advantage of being linguistically motivated, since differences between the two languages are addressed explicitly. Moreover, the implementation of pre-defined transfer rules based on target-side parses is relatively easy and cheap to implement in different language pairs.
Generic approaches for translating from English to more morphologically complex languages have been proposed. Koehn and Hoang (2007) propose Factored Translation Models, which extend phrase-based statistical machine translation by allowing the integration of additional morphological features at the word level. They demonstrate improvements for English-to-German and English-to-Czech. Tighter integration of features is claimed to allow for better modeling of the morphology and hence is better than using pre-processing and post-processing techniques. Avramidis and Koehn (2008) enrich the English side by adding a feature to the Factored Model that models noun case agreement and verb person conjugation, and show that it leads to a more grammatically correct output for English-to-Greek and English-to-Czech translation. Although Factored Models are well equipped for handling languages that differ in terms of morphology, they still use the same distortion reordering model as a phrasebased MT system.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Most of the work in Arabic machine translation is done in the Arabic-to-English direction.</text>
              <doc_id>86</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The other direction, however, is also important, since it opens the wealth of information in different domains that is available in English to the Arabic speaking world.</text>
              <doc_id>87</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Also, since Arabic is a morphologically richer language, translating into Arabic poses unique issues that are not present in the opposite direction.</text>
              <doc_id>88</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The only works on Englishto-Arabic SMT that we are aware of are Badr et al. (2008), and Sarikaya and Deng (2007).</text>
              <doc_id>89</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Badr et al. show that using segmentation and recombination as pre- and post- processing steps leads to significant gains especially for smaller training data corpora.</text>
              <doc_id>90</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Sarikaya and Deng use Joint Morphological-Lexical Language Models to rerank the output of an English-to-Arabic MT system.</text>
              <doc_id>91</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>They use regular expression-based segmentation of the Arabic so as not to run into recombination issues on the output side.</text>
              <doc_id>92</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size.</text>
              <doc_id>93</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>They use a trigram language model and the Arabic morphological analyzer MADA (Habash and Rambow, 2005) respectively, to segment the Arabic side of their corpora.</text>
              <doc_id>94</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Other work on Arabicto-English SMT tries to address the word reordering problem.</text>
              <doc_id>95</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Habash (2007) automatically learns syntactic reordering rules that are then applied to the Arabic side of the parallel corpora.</text>
              <doc_id>96</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>The words are aligned in a sentence pair, then the Arabic sentence is parsed to extract reordering rules based on how the constituents in the parse tree are reordered on the English side.</text>
              <doc_id>97</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>No significant improvement is</text>
              <doc_id>98</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>shown with reordering when compared to a baseline that uses a non-lexicalized distance reordering model.</text>
              <doc_id>99</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This is attributed in the paper to the poor quality of parsing.</text>
              <doc_id>100</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Syntax-based reordering as a preprocessing step has been applied to many language pairs other than English-Arabic.</text>
              <doc_id>101</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Most relevant to the approach in this paper are Collins et al. (2005) and Wang et al. (2007).</text>
              <doc_id>102</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Both parse the source side and then reorder the sentence based on predefined, linguistically motivated rules.</text>
              <doc_id>103</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Significant gain is reported for German-to-English and Chinese-to-English translation.</text>
              <doc_id>104</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Both suggest that reordering as a preprocessing step results in better alignment, and reduces the reliance on the distortion model.</text>
              <doc_id>105</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Popovic and Ney (2006) use similar methods to reorder German by looking at the POS tags for German-to-English and German-to- Spanish.</text>
              <doc_id>106</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>They show significant improvements on test set sentences that do get reordered as well as those that don&#8217;t, which is attributed to the improvement of the extracted phrases.</text>
              <doc_id>107</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>(Xia and McCord, 2004) present a similar approach, with a notable difference: the re-ordering rules are automatically learned from aligning parse trees for both the source and target sentences.</text>
              <doc_id>108</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>They report a 10% relative gain for English-to-French translation.</text>
              <doc_id>109</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Although target-side parsing is optional in this approach, it is needed to take full advantage of the approach.</text>
              <doc_id>110</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>This is a bigger issue when no reliable parses are available for the target language, as is the case in this paper.</text>
              <doc_id>111</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>More generally, the use of automatically-learned rules has the advantage of readily applicable to different language pairs.</text>
              <doc_id>112</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>The use of deterministic, pre-defined rules, however, has the advantage of being linguistically motivated, since differences between the two languages are addressed explicitly.</text>
              <doc_id>113</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, the implementation of pre-defined transfer rules based on target-side parses is relatively easy and cheap to implement in different language pairs.</text>
              <doc_id>114</doc_id>
              <sec_id>13</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Generic approaches for translating from English to more morphologically complex languages have been proposed.</text>
              <doc_id>115</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Koehn and Hoang (2007) propose Factored Translation Models, which extend phrase-based statistical machine translation by allowing the integration of additional morphological features at the word level.</text>
              <doc_id>116</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>They demonstrate improvements for English-to-German and English-to-Czech.</text>
              <doc_id>117</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Tighter integration of features is claimed to allow for better modeling of the morphology and hence is better than using pre-processing and post-processing techniques.</text>
              <doc_id>118</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Avramidis and Koehn (2008) enrich the English side by adding a feature to the Factored Model that models noun case agreement and verb person conjugation, and show that it leads to a more grammatically correct output for English-to-Greek and English-to-Czech translation.</text>
              <doc_id>119</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Although Factored Models are well equipped for handling languages that differ in terms of morphology, they still use the same distortion reordering model as a phrasebased MT system.</text>
              <doc_id>120</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Preprocessing Techniques</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>121</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Arabic Segmentation and Recombination</title>
            <text>It has been shown previously work (Badr et al., 2008; Habash and Sadat, 2006) that morphological segmentation of Arabic improves the translation performance for both Arabic-to-English and English-to-Arabic by addressing the problem of sparsity of the Arabic side. In this paper, we use segmented and non-segmented Arabic on the target side, and study the effect of the combination of segmentation with reordering. As mentioned in Section 2.1, simple pattern matching is not enough to decompose Arabic words into stems and affixes. Lexical information and context are needed to perform the decomposition correctly. We use the Morphological Analyzer MADA (Habash and Rambow, 2005) to decompose the Arabic source. MADA uses SVMbased classifiers of features (such as POS, number, gender, etc.) to score the different analyses of a given word in context. We apply morphological decomposition before aligning the training data. We split the conjunction and preposition prefixes, as well as possessive and object pronoun suffixes. We then glue the split morphemes into one prefix and one suffix, such that any given word is split into at most three parts: prefix+ stem +suffix. Note that plural markers and subject pronouns are not split. For example, the word wlAwlAdh (&#8217;and for his children&#8217;) is segmented into wl+ AwlAd +P:3MS.
Since training is done on segmented Arabic, the output of the decoder must be recombined into its original surface form. We follow the approach of Badr et. al (2008) in combining the Arabic output, which is a non-trivial task for several reasons. First, the ending of a stem sometimes changes when a suffix is attached to it. Second, word end-
ings are normalized to remove orthographic inconsistency between different sources (Section 2.1). Finally, some words can recombine into more than one grammatically correct form. To address these issues, a lookup table is derived from the training data that maps the segmented form of the word to its original form. The table is also useful in recombining words that are erroneously segmented. If a certain word does not occur in the table, we back off to a set of manually defined recombination rules. Word ambiguity is resolved by picking the more frequent surface form.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>It has been shown previously work (Badr et al., 2008; Habash and Sadat, 2006) that morphological segmentation of Arabic improves the translation performance for both Arabic-to-English and English-to-Arabic by addressing the problem of sparsity of the Arabic side.</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this paper, we use segmented and non-segmented Arabic on the target side, and study the effect of the combination of segmentation with reordering.</text>
                  <doc_id>123</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As mentioned in Section 2.1, simple pattern matching is not enough to decompose Arabic words into stems and affixes.</text>
                  <doc_id>124</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Lexical information and context are needed to perform the decomposition correctly.</text>
                  <doc_id>125</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We use the Morphological Analyzer MADA (Habash and Rambow, 2005) to decompose the Arabic source.</text>
                  <doc_id>126</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>MADA uses SVMbased classifiers of features (such as POS, number, gender, etc.) to score the different analyses of a given word in context.</text>
                  <doc_id>127</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>We apply morphological decomposition before aligning the training data.</text>
                  <doc_id>128</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>We split the conjunction and preposition prefixes, as well as possessive and object pronoun suffixes.</text>
                  <doc_id>129</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>We then glue the split morphemes into one prefix and one suffix, such that any given word is split into at most three parts: prefix+ stem +suffix.</text>
                  <doc_id>130</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>Note that plural markers and subject pronouns are not split.</text>
                  <doc_id>131</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>For example, the word wlAwlAdh (&#8217;and for his children&#8217;) is segmented into wl+ AwlAd +P:3MS.</text>
                  <doc_id>132</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Since training is done on segmented Arabic, the output of the decoder must be recombined into its original surface form.</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We follow the approach of Badr et.</text>
                  <doc_id>134</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>al (2008) in combining the Arabic output, which is a non-trivial task for several reasons.</text>
                  <doc_id>135</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>First, the ending of a stem sometimes changes when a suffix is attached to it.</text>
                  <doc_id>136</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Second, word end-</text>
                  <doc_id>137</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>ings are normalized to remove orthographic inconsistency between different sources (Section 2.1).</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, some words can recombine into more than one grammatically correct form.</text>
                  <doc_id>139</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>To address these issues, a lookup table is derived from the training data that maps the segmented form of the word to its original form.</text>
                  <doc_id>140</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The table is also useful in recombining words that are erroneously segmented.</text>
                  <doc_id>141</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>If a certain word does not occur in the table, we back off to a set of manually defined recombination rules.</text>
                  <doc_id>142</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Word ambiguity is resolved by picking the more frequent surface form.</text>
                  <doc_id>143</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Arabic Reordering Rules</title>
            <text>This section presents the syntax-based rules used for re-ordering the English source to better match the syntax of the Arabic target. These rules are motivated by the Arabic syntactic facts described in Section 2.2.
Much like Wang et al. (2007), we parse the English side of our corpora and reorder using predefined rules. Reordering the English can be done more reliably than other source languages, such as Arabic, Chinese and German, since the stateof-the-art English parsers are considerably better than parsers of other languages. The following rules for reordering at the sentence level and the noun phrase level are applied to the English parse tree:
1. NP: All nouns, adjectives and adverbs in the noun phrase are inverted. This rule is motivated by the order of the adjective with respect to its head noun, as well as the idafa construct (see Examples 6 and 7 in Section 2.2. As a result of applying this rule, the phrase the blank computer screen becomes the screen computer blank .
2. PP: All prepositional phrases of the form N 1 ofN 2 ...ofN n are transformed to N 1 N 2 ...N n . All N i are also made indefinite, and the definite article is added to N n , the last noun in the chain. For example, the phrase the general chief of staff of the armed forces becomes general chief staff the armed forces. We also move all adjectives in the top noun phrase to the end of the construct. So the real value of the Egyptian pound becomes value the Egyptian pound real. This rule is motivated by the idafa construct and its properties (see Example 6).
3. the: The definite article the is replicated before adjectives (see Example 5 above). So the blank computer screen becomes the blank the computer the screen. This rule is applied after NP rule abote. Note that we do not replicate the before proper names.
4. VP: This rule transforms SVO sentences to VSO. All verbs are reordered on the condition that they have their own subject noun phrase and are not in the participle form, since in these cases the Arabic subject occurs before the verb participle. We also check that the verb is not in a relative clause with a that complementizer (Example 3 above). The following example illustrates all these cases: the health minister stated that 11 police officers were wounded in clashes with the demonstrators &#8594; stated the health minister that 11 police officers were wounded in clashes with the demonstrators. If the verb is negated, the negative particle is moved with the verb (Example 4. Finally, if the object of the reordered verb is a pronoun, it is reordered with the verb. Example: the authorities gave us all the necessary help becomes gave us the authorities all the necessary help.
The transformation rules 1, 2 and 3 are applied in this order, since they interact although they do not conflict. So, the real value of the Egyptian pound &#8594; value the Egyptian the pound the real The VP reordering rule is independent.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>This section presents the syntax-based rules used for re-ordering the English source to better match the syntax of the Arabic target.</text>
                  <doc_id>144</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These rules are motivated by the Arabic syntactic facts described in Section 2.2.</text>
                  <doc_id>145</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Much like Wang et al. (2007), we parse the English side of our corpora and reorder using predefined rules.</text>
                  <doc_id>146</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Reordering the English can be done more reliably than other source languages, such as Arabic, Chinese and German, since the stateof-the-art English parsers are considerably better than parsers of other languages.</text>
                  <doc_id>147</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The following rules for reordering at the sentence level and the noun phrase level are applied to the English parse tree:</text>
                  <doc_id>148</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1.</text>
                  <doc_id>149</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>NP: All nouns, adjectives and adverbs in the noun phrase are inverted.</text>
                  <doc_id>150</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This rule is motivated by the order of the adjective with respect to its head noun, as well as the idafa construct (see Examples 6 and 7 in Section 2.2.</text>
                  <doc_id>151</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>As a result of applying this rule, the phrase the blank computer screen becomes the screen computer blank .</text>
                  <doc_id>152</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.</text>
                  <doc_id>153</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>PP: All prepositional phrases of the form N 1 ofN 2 ...ofN n are transformed to N 1 N 2 ...N n .</text>
                  <doc_id>154</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>All N i are also made indefinite, and the definite article is added to N n , the last noun in the chain.</text>
                  <doc_id>155</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For example, the phrase the general chief of staff of the armed forces becomes general chief staff the armed forces.</text>
                  <doc_id>156</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We also move all adjectives in the top noun phrase to the end of the construct.</text>
                  <doc_id>157</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>So the real value of the Egyptian pound becomes value the Egyptian pound real.</text>
                  <doc_id>158</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>This rule is motivated by the idafa construct and its properties (see Example 6).</text>
                  <doc_id>159</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3. the: The definite article the is replicated before adjectives (see Example 5 above).</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>So the blank computer screen becomes the blank the computer the screen.</text>
                  <doc_id>161</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This rule is applied after NP rule abote.</text>
                  <doc_id>162</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Note that we do not replicate the before proper names.</text>
                  <doc_id>163</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4.</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>VP: This rule transforms SVO sentences to VSO.</text>
                  <doc_id>165</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>All verbs are reordered on the condition that they have their own subject noun phrase and are not in the participle form, since in these cases the Arabic subject occurs before the verb participle.</text>
                  <doc_id>166</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We also check that the verb is not in a relative clause with a that complementizer (Example 3 above).</text>
                  <doc_id>167</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The following example illustrates all these cases: the health minister stated that 11 police officers were wounded in clashes with the demonstrators &#8594; stated the health minister that 11 police officers were wounded in clashes with the demonstrators.</text>
                  <doc_id>168</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>If the verb is negated, the negative particle is moved with the verb (Example 4.</text>
                  <doc_id>169</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, if the object of the reordered verb is a pronoun, it is reordered with the verb.</text>
                  <doc_id>170</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Example: the authorities gave us all the necessary help becomes gave us the authorities all the necessary help.</text>
                  <doc_id>171</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The transformation rules 1, 2 and 3 are applied in this order, since they interact although they do not conflict.</text>
                  <doc_id>172</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>So, the real value of the Egyptian pound &#8594; value the Egyptian the pound the real The VP reordering rule is independent.</text>
                  <doc_id>173</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Experiments</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>174</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 System description</title>
            <text>For the English source, we first tokenize using the Stanford Log-linear Part-of-Speech Tagger (Toutanova et al., 2003). We then proceed to split the data into smaller sentences and tag them using Ratnaparkhi&#8217;s Maximum Entropy Tagger (Ratnaparkhi, 1996). We parse the data using the Collins Parser (Collins, 1997), and then tag person, location and organization names using the Stanford Named Entity Recognizer (Finkel et al., 2005). On the Arabic side, we normalize the data by changing final &#8217;Y&#8217; to &#8217;y&#8217;, and changing the various forms of Alif hamza to bare Alif, since these characters are written inconsistently in some Arabic sources. We then segment the data using MADA according to the scheme explained in Section 4.1.
The English source is aligned to the segmented Arabic target using the standard MOSES (MOSES, 2007) configuration of GIZA++ (Och and Ney, 2000), which is IBM Model 4, and decoding is done using the phrasebased SMT system MOSES. We use a maximum phrase length of 15 to account for the increase in length of the segmented Arabic. We also use a lexicalized bidirectional reordering model conditioned on both the source and target sides, with a distortion limit set to 6. We tune using Och&#8217;s algorithm (Och, 2003) to optimize weights for the distortion model, language model, phrase translation model and word penalty over the BLEU metric (Papineni et al., 2001). For the segmented Arabic experiments, we experiment with tuning using non-segmented Arabic as a reference. This is done by recombining the output before each tuning iteration is scored and has been shown by Badr et. al (2008) to perform better than using segmented Arabic as reference.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>For the English source, we first tokenize using the Stanford Log-linear Part-of-Speech Tagger (Toutanova et al., 2003).</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We then proceed to split the data into smaller sentences and tag them using Ratnaparkhi&#8217;s Maximum Entropy Tagger (Ratnaparkhi, 1996).</text>
                  <doc_id>176</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We parse the data using the Collins Parser (Collins, 1997), and then tag person, location and organization names using the Stanford Named Entity Recognizer (Finkel et al., 2005).</text>
                  <doc_id>177</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>On the Arabic side, we normalize the data by changing final &#8217;Y&#8217; to &#8217;y&#8217;, and changing the various forms of Alif hamza to bare Alif, since these characters are written inconsistently in some Arabic sources.</text>
                  <doc_id>178</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We then segment the data using MADA according to the scheme explained in Section 4.1.</text>
                  <doc_id>179</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The English source is aligned to the segmented Arabic target using the standard MOSES (MOSES, 2007) configuration of GIZA++ (Och and Ney, 2000), which is IBM Model 4, and decoding is done using the phrasebased SMT system MOSES.</text>
                  <doc_id>180</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We use a maximum phrase length of 15 to account for the increase in length of the segmented Arabic.</text>
                  <doc_id>181</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We also use a lexicalized bidirectional reordering model conditioned on both the source and target sides, with a distortion limit set to 6.</text>
                  <doc_id>182</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We tune using Och&#8217;s algorithm (Och, 2003) to optimize weights for the distortion model, language model, phrase translation model and word penalty over the BLEU metric (Papineni et al., 2001).</text>
                  <doc_id>183</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For the segmented Arabic experiments, we experiment with tuning using non-segmented Arabic as a reference.</text>
                  <doc_id>184</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This is done by recombining the output before each tuning iteration is scored and has been shown by Badr et.</text>
                  <doc_id>185</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>al (2008) to perform better than using segmented Arabic as reference.</text>
                  <doc_id>186</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Data Used</title>
            <text>We report results on three domains: newswire text, UN data and spoken dialogue from the travel domain. It is important to note that the sentences in the travel domain are much shorter than in the news domain, which simplifies the alignment as well as reordering during decoding. Also, since the travel domain contains spoken Arabic, it is more biased towards the Subject-Verb-Object sentence order than the Verb-Subject-Object order more common in the news domain. Also note that since most of our data was originally intended for Arabic-to-English translation, our test and tuning sets have only one reference, and therefore, the BLEU scores we report are lower than typical scores reported in the literature on Arabic-to- English.
The news training data consists of several LDC corpora 2 . We construct a test set by randomly picking 2000 sentences. We pick another 2000 sentences randomly for tuning. Our final training set consists of 3 million English words. We also test on the NIST MT 05 &#8220;test set while tuning on both the NIST MT 03 and 04 test sets. We use the first English reference of the NIST test sets as the source, and the Arabic source as our reference. For
2 LDC2003E05 LDC2003E09 LDC2003T18
LDC2004E07 LDC2004E08 LDC2004E11 LDC2004E72 LDC2004T18 LDC2004T17 LDC2005E46 LDC2005T05 LDC2007T24
the language model, we use 35 million words from the LDC Arabic Gigaword corpus, plus the Arabic side of the 3 million word training corpus. Experimentation with different language model orders shows that the optimal model orders are 4-grams for the baseline system and 6-grams for the segmented Arabic. The average sentence length is 33 for English, 25 for non-segmented Arabic and 36 for segmented Arabic. To study the effect of syntactic reordering on larger training data sizes, we use the UN English- Arabic parallel text (LDC2003T05). We experiment with two training data sizes: 30 million and 3 million words. The test and tuning sets are comprised of 1500 and 500 sentences respectively, chosen at random.
For the spoken domain, we use the BTEC 2007 Arabic-English corpus. The training set consists of 200K words, the test set has 500 sentences and the tuning set has 500 sentences. The language model consists of the Arabic side of the training data. Because of the significantly smaller data size, we use a trigram LM for the baseline, and a 4-gram for segmented Arabic. In this case, the average sentence length is 9 for English, 8 for Arabic, and 10 for segmented Arabic.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We report results on three domains: newswire text, UN data and spoken dialogue from the travel domain.</text>
                  <doc_id>187</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It is important to note that the sentences in the travel domain are much shorter than in the news domain, which simplifies the alignment as well as reordering during decoding.</text>
                  <doc_id>188</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Also, since the travel domain contains spoken Arabic, it is more biased towards the Subject-Verb-Object sentence order than the Verb-Subject-Object order more common in the news domain.</text>
                  <doc_id>189</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Also note that since most of our data was originally intended for Arabic-to-English translation, our test and tuning sets have only one reference, and therefore, the BLEU scores we report are lower than typical scores reported in the literature on Arabic-to- English.</text>
                  <doc_id>190</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The news training data consists of several LDC corpora 2 .</text>
                  <doc_id>191</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We construct a test set by randomly picking 2000 sentences.</text>
                  <doc_id>192</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We pick another 2000 sentences randomly for tuning.</text>
                  <doc_id>193</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Our final training set consists of 3 million English words.</text>
                  <doc_id>194</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We also test on the NIST MT 05 &#8220;test set while tuning on both the NIST MT 03 and 04 test sets.</text>
                  <doc_id>195</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We use the first English reference of the NIST test sets as the source, and the Arabic source as our reference.</text>
                  <doc_id>196</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>For</text>
                  <doc_id>197</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 LDC2003E05 LDC2003E09 LDC2003T18</text>
                  <doc_id>198</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>LDC2004E07 LDC2004E08 LDC2004E11 LDC2004E72 LDC2004T18 LDC2004T17 LDC2005E46 LDC2005T05 LDC2007T24</text>
                  <doc_id>199</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the language model, we use 35 million words from the LDC Arabic Gigaword corpus, plus the Arabic side of the 3 million word training corpus.</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Experimentation with different language model orders shows that the optimal model orders are 4-grams for the baseline system and 6-grams for the segmented Arabic.</text>
                  <doc_id>201</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The average sentence length is 33 for English, 25 for non-segmented Arabic and 36 for segmented Arabic.</text>
                  <doc_id>202</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>To study the effect of syntactic reordering on larger training data sizes, we use the UN English- Arabic parallel text (LDC2003T05).</text>
                  <doc_id>203</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We experiment with two training data sizes: 30 million and 3 million words.</text>
                  <doc_id>204</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The test and tuning sets are comprised of 1500 and 500 sentences respectively, chosen at random.</text>
                  <doc_id>205</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For the spoken domain, we use the BTEC 2007 Arabic-English corpus.</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The training set consists of 200K words, the test set has 500 sentences and the tuning set has 500 sentences.</text>
                  <doc_id>207</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The language model consists of the Arabic side of the training data.</text>
                  <doc_id>208</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Because of the significantly smaller data size, we use a trigram LM for the baseline, and a 4-gram for segmented Arabic.</text>
                  <doc_id>209</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In this case, the average sentence length is 9 for English, 8 for Arabic, and 10 for segmented Arabic.</text>
                  <doc_id>210</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>5.3 Translation Results</title>
            <text>The translation scores for the News domain are shown in Table 1. The notation used in the table is as follows:
&#8226; S: Segmented Arabic &#8226; NoS: Non-Segmented Arabic &#8226; RandT: Scores for test set where sentences were picked at random from NEWS data &#8226; MT 05: Scores for the NIST MT 05 test set
The reordering notation is explained in Section 4.2. All results are in terms of the BLEU met-
ric. It is important to note that the gain that we report in terms of BLEU are more significant that comparable gains on test sets that have multiple references, since our test sets have only one reference. Any amount of gain is a result of additional n-gram precision with one reference. We note that the gain achieved from the reordering of the nonsegmented and segmented systems are comparable. Replicating the before adjectives hurts the scores, possibly because it increases the sentence length noticeably, and thus deteriorates the alignments&#8217; quality. We note that the gains achieved by reordering on the NIST test set are smaller than the improvements on the random test set. This is due to the fact that the sentences in the NIST test set are longer, which adversely affects the parsing quality. The average English sentence length is 33 words in the NIST test set, while the random test set has an average sentence length of 29 words. Table 2 shows the reordering gains of the nonsegmented Arabic by sentence length. Short sentences are sentences that have less that 40 words of English, while long sentences have more than 40 words. Out of the 1055 sentence in the NIST test set 719 are short and 336 are long. We also report oracle scores in Table 3 for combining the baseline system with the reordering systems, as well as the percentage of oracle sentences produced by the reordered system. The oracle score is computed by starting with the reordered system&#8217;s candidate translations and iterating over all the sentences one by one: we replace each sentence with its corresponding baseline system translation then
compute the total BLEU score of the entire set. If the score improves, then the sentence in question is replaced with the baseline system&#8217;s translation, otherwise it remains unchanged and we move on to the next one.
In Table 4, we report results on the UN corpus for different training data sizes. It is important to note that although gains from VP reordering stay constant when scaled to larger training sets, gains from NP+PP reordering diminish. This is due to the fact that NP reordering tend to be more localized then VP reorderings. Hence with more training data the lexicalized reordering model becomes more effective in reordering NPs.
In Table 5, we report results on the BTEC corpus for different segmentation and reordering scheme combinations. We should first point out that all sentences in the BTEC corpus are short, simple and easy to align. Hence, the gain introduced by reordering might not be enough to offset the errors introduced by the parsing. We also note that spoken Arabic usually prefers the Subject- Verb-Object sentence order, rather than the Verb- Subject-Object sentence order of written Arabic. This explains the fact that no gain is observed when the verb phrase is reordered. Noun phrase reordering produces a significant gain with nonsegmented Arabic. Replicating the definite article the in the noun phrase does not create alignment problems as is the case with the newswire data, since the sentences are considerably shorter, and hence the 0.74 point gain observed on the segmented Arabic system. That gain does not translate to the non-segmented Arabic system since in that case the definite article Al remains attached to its head word.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The translation scores for the News domain are shown in Table 1.</text>
                  <doc_id>211</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The notation used in the table is as follows:</text>
                  <doc_id>212</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; S: Segmented Arabic &#8226; NoS: Non-Segmented Arabic &#8226; RandT: Scores for test set where sentences were picked at random from NEWS data &#8226; MT 05: Scores for the NIST MT 05 test set</text>
                  <doc_id>213</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The reordering notation is explained in Section 4.2.</text>
                  <doc_id>214</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>All results are in terms of the BLEU met-</text>
                  <doc_id>215</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>ric.</text>
                  <doc_id>216</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It is important to note that the gain that we report in terms of BLEU are more significant that comparable gains on test sets that have multiple references, since our test sets have only one reference.</text>
                  <doc_id>217</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Any amount of gain is a result of additional n-gram precision with one reference.</text>
                  <doc_id>218</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We note that the gain achieved from the reordering of the nonsegmented and segmented systems are comparable.</text>
                  <doc_id>219</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Replicating the before adjectives hurts the scores, possibly because it increases the sentence length noticeably, and thus deteriorates the alignments&#8217; quality.</text>
                  <doc_id>220</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We note that the gains achieved by reordering on the NIST test set are smaller than the improvements on the random test set.</text>
                  <doc_id>221</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>This is due to the fact that the sentences in the NIST test set are longer, which adversely affects the parsing quality.</text>
                  <doc_id>222</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>The average English sentence length is 33 words in the NIST test set, while the random test set has an average sentence length of 29 words.</text>
                  <doc_id>223</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>Table 2 shows the reordering gains of the nonsegmented Arabic by sentence length.</text>
                  <doc_id>224</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>Short sentences are sentences that have less that 40 words of English, while long sentences have more than 40 words.</text>
                  <doc_id>225</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>Out of the 1055 sentence in the NIST test set 719 are short and 336 are long.</text>
                  <doc_id>226</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>We also report oracle scores in Table 3 for combining the baseline system with the reordering systems, as well as the percentage of oracle sentences produced by the reordered system.</text>
                  <doc_id>227</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
                <sentence>
                  <text>The oracle score is computed by starting with the reordered system&#8217;s candidate translations and iterating over all the sentences one by one: we replace each sentence with its corresponding baseline system translation then</text>
                  <doc_id>228</doc_id>
                  <sec_id>12</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>compute the total BLEU score of the entire set.</text>
                  <doc_id>229</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If the score improves, then the sentence in question is replaced with the baseline system&#8217;s translation, otherwise it remains unchanged and we move on to the next one.</text>
                  <doc_id>230</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In Table 4, we report results on the UN corpus for different training data sizes.</text>
                  <doc_id>231</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It is important to note that although gains from VP reordering stay constant when scaled to larger training sets, gains from NP+PP reordering diminish.</text>
                  <doc_id>232</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This is due to the fact that NP reordering tend to be more localized then VP reorderings.</text>
                  <doc_id>233</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Hence with more training data the lexicalized reordering model becomes more effective in reordering NPs.</text>
                  <doc_id>234</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In Table 5, we report results on the BTEC corpus for different segmentation and reordering scheme combinations.</text>
                  <doc_id>235</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We should first point out that all sentences in the BTEC corpus are short, simple and easy to align.</text>
                  <doc_id>236</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Hence, the gain introduced by reordering might not be enough to offset the errors introduced by the parsing.</text>
                  <doc_id>237</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We also note that spoken Arabic usually prefers the Subject- Verb-Object sentence order, rather than the Verb- Subject-Object sentence order of written Arabic.</text>
                  <doc_id>238</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This explains the fact that no gain is observed when the verb phrase is reordered.</text>
                  <doc_id>239</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Noun phrase reordering produces a significant gain with nonsegmented Arabic.</text>
                  <doc_id>240</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Replicating the definite article the in the noun phrase does not create alignment problems as is the case with the newswire data, since the sentences are considerably shorter, and hence the 0.74 point gain observed on the segmented Arabic system.</text>
                  <doc_id>241</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>That gain does not translate to the non-segmented Arabic system since in that case the definite article Al remains attached to its head word.</text>
                  <doc_id>242</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>6</index>
        <title>6 Conclusion</title>
        <text>This paper presented linguistically motivated rules that reorder English to look like Arabic. We showed that these rules produce significant gains. We also studied the effect of the interaction between
Domain in the BLEU Metric. syntactic reordering on translation results, as well as how they scale to bigger training data sizes.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This paper presented linguistically motivated rules that reorder English to look like Arabic.</text>
              <doc_id>243</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We showed that these rules produce significant gains.</text>
              <doc_id>244</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We also studied the effect of the interaction between</text>
              <doc_id>245</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Domain in the BLEU Metric.</text>
              <doc_id>246</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>syntactic reordering on translation results, as well as how they scale to bigger training data sizes.</text>
              <doc_id>247</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>7</index>
        <title>Acknowledgments</title>
        <text>We would like to thank Michael Collins, Ali Mohammad and Stephanie Seneff for their valuable comments.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We would like to thank Michael Collins, Ali Mohammad and Stephanie Seneff for their valuable comments.</text>
              <doc_id>248</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Translation Results for the News Domain in terms of the BLEU Metric.</caption>
        <reference_text>In PAGE 6: ... 5.3 Translation Results The translation scores for the News domain are shown in  Table1 . The notation used in the table is as follows: ? S: Segmented Arabic ? NoS: Non-Segmented Arabic ? RandT: Scores for test set where sentences were picked at random from NEWS data ? MT 05: Scores for the NIST MT 05 test set The reordering notation is explained in Section 4....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Scheme</cell>
              <cell>RandT</cell>
              <cell>RandT</cell>
              <cell>MT 05</cell>
              <cell>MT 05</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>S</cell>
              <cell>NoS</cell>
              <cell>S</cell>
              <cell>NoS</cell>
            </row>
            <row>
              <cell>Baseline</cell>
              <cell>21.6</cell>
              <cell>21.3</cell>
              <cell>23.88</cell>
              <cell>23.44</cell>
            </row>
            <row>
              <cell>VP</cell>
              <cell>21.9</cell>
              <cell>21.5</cell>
              <cell>23.98</cell>
              <cell>23.58</cell>
            </row>
            <row>
              <cell>NP</cell>
              <cell>21.9</cell>
              <cell>21.8</cell>
            </row>
            <row>
              <cell>NP+PP</cell>
              <cell>21.8</cell>
              <cell>21.5</cell>
              <cell>23.72</cell>
              <cell>23.68</cell>
            </row>
            <row>
              <cell>NP+PP+VP</cell>
              <cell>22.2</cell>
              <cell>21.8</cell>
              <cell>23.74</cell>
              <cell>23.16</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>21.3</cell>
              <cell>21.0</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Translation Results depending on sentence length for NIST test set.</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>S</cell>
              <cell>NoS</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell></cell>
              <cell>Short</cell>
              <cell>Long</cell>
              <cell>Short</cell>
              <cell>Long</cell>
            </row>
            <row>
              <cell>Baseline</cell>
              <cell>22.57</cell>
              <cell>25.22</cell>
              <cell>22.40</cell>
              <cell>24.33</cell>
            </row>
            <row>
              <cell>VP</cell>
              <cell>22.95</cell>
              <cell>25.05</cell>
              <cell>22.95</cell>
              <cell>24.02</cell>
            </row>
            <row>
              <cell>NP+PP</cell>
              <cell>22.71</cell>
              <cell>24.76</cell>
              <cell>23.16</cell>
              <cell>24.067</cell>
            </row>
            <row>
              <cell>NP+PP+VP</cell>
              <cell>22.84</cell>
              <cell>24.62</cell>
              <cell>22.53</cell>
              <cell>24.56</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 2: Translation Results depending on sen- tence length for NIST test set.#@#@Table 3: Oracle scores for combining baseline system with other reordered systems.</caption>
        <reference_text>In PAGE 7: ... The average English sentence length is 33 words in the NIST test set, while the random test set has an average sentence length of 29 words.  Table2  shows the reordering gains of the non- segmented Arabic by sentence length. Short sen- tences are sentences that have less that 40 words of English, while long sentences have more than 40 words....</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>Scheme</cell>
              <cell>Score</cell>
              <cell>% Oracle reord</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>VP</cell>
              <cell>25.76</cell>
              <cell>59%</cell>
            </row>
            <row>
              <cell>NP+PP</cell>
              <cell>26.07</cell>
              <cell>58%</cell>
            </row>
            <row>
              <cell>NP+PP+VP</cell>
              <cell>26.17</cell>
              <cell>53%</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TET</source>
        <caption>Table 4: Translation Results on segmentd UN data in terms of the BLEU Metric.</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Scheme</cell>
              <cell>30M</cell>
              <cell>3M</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Baseline</cell>
              <cell>32.17</cell>
              <cell>28.42</cell>
            </row>
            <row>
              <cell>VP</cell>
              <cell>32.46</cell>
              <cell>28.60</cell>
            </row>
            <row>
              <cell>NP+PP</cell>
              <cell>31.73</cell>
              <cell>28.80</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TET</source>
        <caption>Table 5: Translation Results for the Spoken Language Domain in the BLEU Metric.</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>Scheme</cell>
              <cell>S</cell>
              <cell>NoS</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Baseline</cell>
              <cell>29.06</cell>
              <cell>25.4</cell>
            </row>
            <row>
              <cell>VP</cell>
              <cell>26.92</cell>
              <cell>23.49</cell>
            </row>
            <row>
              <cell>NP</cell>
              <cell>27.94</cell>
              <cell>26.83</cell>
            </row>
            <row>
              <cell>NP+PP</cell>
              <cell>28.59</cell>
              <cell>26.42</cell>
            </row>
            <row>
              <cell>The</cell>
              <cell>29.8</cell>
              <cell>25.1</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Eleftherios Avramidis</author>
          <author>Philipp Koehn</author>
        </authors>
        <title>Enriching Morphologically Poor Languages for Statistical Machine Translation.</title>
        <publication>In Proc. of ACL/HLT.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Ibrahim Badr</author>
          <author>Rabih Zbib</author>
          <author>James Glass</author>
        </authors>
        <title>Segmentation for English-to-Arabic Statistical Machine Translation.</title>
        <publication>In Proc. of ACL/HLT.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Michael Collins</author>
        </authors>
        <title>Three Generative, Lexicalized Models for Statistical Parsing.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Michael Collins</author>
          <author>Philipp Koehn</author>
          <author>Ivona Kucerova</author>
        </authors>
        <title>Clause Restructuring for Statistical Machine Translation.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>MOSES</author>
        </authors>
        <title>A Factored Phrase-based Beamsearch Decoder for Machine Translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Franz Och</author>
        </authors>
        <title>Minimum Error Rate Training in Statistical Machine Translation.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Franz Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Improved Statistical Alignment Models.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>BLUE: a Method for Automatic Evaluation of Machine Translation.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Maja Popovic</author>
          <author>Hermann Ney</author>
        </authors>
        <title>POS-based Word Reordering for Statistical Machine Translation.</title>
        <publication>In Proc. of NAACL LREC.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Adwait Ratnaparkhi</author>
        </authors>
        <title>A Maximum Entropy Model for Part-of-Speech Tagging.</title>
        <publication>In Proc. of EMNLP.</publication>
        <pages>None</pages>
        <date>1996</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Ruhi Sarikaya</author>
          <author>Yonggang Deng</author>
        </authors>
        <title>Joint Morphological-Lexical Language Modeling for Machine Translation.</title>
        <publication>In Proc. of NAACL HLT.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Kristina Toutanova</author>
          <author>Dan Klein</author>
          <author>Christopher Manning</author>
          <author>Yoram Singer</author>
        </authors>
        <title>Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network. In</title>
        <publication>Proc. of NAACL HLT.</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Chao Wang</author>
          <author>Michael Collins</author>
          <author>Philipp Koehn</author>
        </authors>
        <title>Chinese Syntactic Reordering for Statistical Machine Translation.</title>
        <publication>In Proc. of EMNLP.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Fei Xia</author>
          <author>Michael McCord</author>
        </authors>
        <title>Improving a Statistical MT System with Automatically Learned Rewrite Patterns.</title>
        <publication>In COLING.</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Jenny Rose Finkel</author>
          <author>Trond Grenager</author>
          <author>Christopher Manning</author>
        </authors>
        <title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>Nizar Habash</author>
        </authors>
        <title>Syntactic Preprocessing for Statistical Machine Translation.</title>
        <publication>In Proc. of the Machine Translation Summit (MT-Summit).</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Nizar Habash</author>
          <author>Owen Rambow</author>
        </authors>
        <title>Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Nizar Habash</author>
          <author>Fatiha Sadat</author>
        </authors>
        <title>Arabic Preprocessing Schemes for Statistical Machine Translation.</title>
        <publication>In Proc. of HLT.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Hieu Hoang</author>
        </authors>
        <title>Factored Translation Models.</title>
        <publication>In Proc. of EMNLP/CNLL.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Avramidis and Koehn (2008)</string>
        <sentence_id>18253</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Badr et al. (2008)</string>
        <sentence_id>18223</sentence_id>
        <char_offset>64</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>Badr et al., 2008</string>
        <sentence_id>18160</sentence_id>
        <char_offset>119</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>1</reference_id>
        <string>Badr et al., 2008</string>
        <sentence_id>18255</sentence_id>
        <char_offset>35</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>2</reference_id>
        <string>Collins, 1997</string>
        <sentence_id>18310</sentence_id>
        <char_offset>44</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>3</reference_id>
        <string>Collins et al. (2005)</string>
        <sentence_id>18236</sentence_id>
        <char_offset>48</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>3</reference_id>
        <string>Collins et al., 2005</string>
        <sentence_id>18147</sentence_id>
        <char_offset>83</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>4</reference_id>
        <string>MOSES, 2007</string>
        <sentence_id>18313</sentence_id>
        <char_offset>87</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>4</reference_id>
        <string>MOSES, 2007</string>
        <sentence_id>18395</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>5</reference_id>
        <string>Och, 2003</string>
        <sentence_id>18316</sentence_id>
        <char_offset>31</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>6</reference_id>
        <string>Och and Ney, 2000</string>
        <sentence_id>18313</sentence_id>
        <char_offset>125</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>7</reference_id>
        <string>Papineni et al., 2001</string>
        <sentence_id>18316</sentence_id>
        <char_offset>168</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>8</reference_id>
        <string>Popovic and Ney (2006)</string>
        <sentence_id>18240</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>9</reference_id>
        <string>Ratnaparkhi, 1996</string>
        <sentence_id>18309</sentence_id>
        <char_offset>114</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>10</reference_id>
        <string>Sarikaya and Deng (2007)</string>
        <sentence_id>18223</sentence_id>
        <char_offset>88</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>11</reference_id>
        <string>Toutanova et al., 2003</string>
        <sentence_id>18308</sentence_id>
        <char_offset>95</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>12</reference_id>
        <string>Wang et al. (2007)</string>
        <sentence_id>18236</sentence_id>
        <char_offset>74</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>12</reference_id>
        <string>Wang et al. (2007)</string>
        <sentence_id>18279</sentence_id>
        <char_offset>10</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>12</reference_id>
        <string>Wang et al., 2007</string>
        <sentence_id>18147</sentence_id>
        <char_offset>105</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>13</reference_id>
        <string>Xia and McCord, 2004</string>
        <sentence_id>18242</sentence_id>
        <char_offset>1</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>14</reference_id>
        <string>Finkel et al., 2005</string>
        <sentence_id>18310</sentence_id>
        <char_offset>157</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>15</reference_id>
        <string>Habash, 2007</string>
        <sentence_id>18155</sentence_id>
        <char_offset>36</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>15</reference_id>
        <string>Habash, 2007</string>
        <sentence_id>18429</sentence_id>
        <char_offset>6</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>15</reference_id>
        <string>Habash (2007)</string>
        <sentence_id>18230</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>16</reference_id>
        <string>Habash and Rambow, 2005</string>
        <sentence_id>18228</sentence_id>
        <char_offset>78</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>16</reference_id>
        <string>Habash and Rambow, 2005</string>
        <sentence_id>18259</sentence_id>
        <char_offset>40</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>17</reference_id>
        <string>Habash and Sadat (2006)</string>
        <sentence_id>18227</sentence_id>
        <char_offset>50</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>17</reference_id>
        <string>Habash and Sadat, 2006</string>
        <sentence_id>18160</sentence_id>
        <char_offset>72</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>17</reference_id>
        <string>Habash and Sadat, 2006</string>
        <sentence_id>18255</sentence_id>
        <char_offset>54</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>18</reference_id>
        <string>Koehn and Hoang (2007)</string>
        <sentence_id>18250</sentence_id>
        <char_offset>0</char_offset>
      </citation>
    </citations>
  </content>
</document>
