<document>
  <filename>P13-2066</filename>
  <authors/>
  <title>A Novel Translation Framework Based on Rhetorical Structure Theory</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>Rhetorical structure theory (RST) is widely used for discourse understanding, which represents a discourse as a hierarchically semantic structure. In this paper, we propose a novel translation framework with the help of RST. In our framework, the translation process mainly includes three steps: 1) Source RST-tree acquisition: a source sentence is parsed into an RST tree; 2) Rule extraction: translation rules are extracted from the source tree and the target string via bilingual word alignment; 3) RST-based translation: the source RST-tree is translated with translation rules. Experiments on Chinese-to-English show that our RST-based approach achieves improvements of 2.3/0.77/1.43 BLEU points on NIST04/NIST05/CWMT2008 respectively.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Rhetorical structure theory (RST) is widely used for discourse understanding, which represents a discourse as a hierarchically semantic structure.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, we propose a novel translation framework with the help of RST.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In our framework, the translation process mainly includes three steps: 1) Source RST-tree acquisition: a source sentence is parsed into an RST tree; 2) Rule extraction: translation rules are extracted from the source tree and the target string via bilingual word alignment; 3) RST-based translation: the source RST-tree is translated with translation rules.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Experiments on Chinese-to-English show that our RST-based approach achieves improvements of 2.3/0.77/1.43 BLEU points on NIST04/NIST05/CWMT2008 respectively.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>For statistical machine translation (SMT), a crucial issue is how to build a translation model to extract as much accurate and generative translation knowledge as possible. The existing SMT models have made much progress. However, they still suffer from the bad performance of unnatural or even unreadable translation, especially when the sentences become complicated. We think the deep reason is that those models only extract translation information on lexical or syntactic level, but fail to give an overall understanding of source sentences on semantic level of discourse. In order to solve such problem, (Gong et al., 2011; Xiao et al., 2011; Wong and Kit, 2012) build discourse-based translation models to ensure the lexical coherence or consistency. Although some lexicons can be translated better by their models, the overall structure still remains unnatural. Marcu et al. (2000) design a discourse structure transferring module, but leave much work to do, especially on how to integrate this module into SMT and how to automatically analyze the structures. Those reasons urge us to seek a new translation framework under the idea of &#8220;translation with overall understanding&#8221;.
Rhetorical structure theory (RST) (Mann and Thompson, 1988) provides us with a good perspective and inspiration to build such a framework. Generally, an RST tree can explicitly show the minimal spans with semantic functional integrity, which are called elementary discourse units (edus) (Marcu et al., 2000), and it also depicts the hierarchical relations among edus. Furthermore, since different languages&#8217; edus are usually equivalent on semantic level, it is intuitive to create a new framework based on RST by directly mapping the source edus to target ones.
Taking the Chinese-to-English translation as an example, our translation framework works as the following steps:
1) Source RST-tree acquisition: a source sentence is parsed into an RST-tree;
2) Rule extraction: translation rules are extracted from the source tree and the target string via bilingual word alignment;
3) RST-based translation: the source RSTtree is translated into target sentence with extracted translation rules.
Experiments on Chinese-to-English sentencelevel discourses demonstrate that this method achieves significant improvements.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>For statistical machine translation (SMT), a crucial issue is how to build a translation model to extract as much accurate and generative translation knowledge as possible.</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The existing SMT models have made much progress.</text>
              <doc_id>5</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, they still suffer from the bad performance of unnatural or even unreadable translation, especially when the sentences become complicated.</text>
              <doc_id>6</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We think the deep reason is that those models only extract translation information on lexical or syntactic level, but fail to give an overall understanding of source sentences on semantic level of discourse.</text>
              <doc_id>7</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In order to solve such problem, (Gong et al., 2011; Xiao et al., 2011; Wong and Kit, 2012) build discourse-based translation models to ensure the lexical coherence or consistency.</text>
              <doc_id>8</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Although some lexicons can be translated better by their models, the overall structure still remains unnatural.</text>
              <doc_id>9</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Marcu et al. (2000) design a discourse structure transferring module, but leave much work to do, especially on how to integrate this module into SMT and how to automatically analyze the structures.</text>
              <doc_id>10</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Those reasons urge us to seek a new translation framework under the idea of &#8220;translation with overall understanding&#8221;.</text>
              <doc_id>11</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Rhetorical structure theory (RST) (Mann and Thompson, 1988) provides us with a good perspective and inspiration to build such a framework.</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Generally, an RST tree can explicitly show the minimal spans with semantic functional integrity, which are called elementary discourse units (edus) (Marcu et al., 2000), and it also depicts the hierarchical relations among edus.</text>
              <doc_id>13</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Furthermore, since different languages&#8217; edus are usually equivalent on semantic level, it is intuitive to create a new framework based on RST by directly mapping the source edus to target ones.</text>
              <doc_id>14</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Taking the Chinese-to-English translation as an example, our translation framework works as the following steps:</text>
              <doc_id>15</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1) Source RST-tree acquisition: a source sentence is parsed into an RST-tree;</text>
              <doc_id>16</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2) Rule extraction: translation rules are extracted from the source tree and the target string via bilingual word alignment;</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3) RST-based translation: the source RSTtree is translated into target sentence with extracted translation rules.</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Experiments on Chinese-to-English sentencelevel discourses demonstrate that this method achieves significant improvements.</text>
              <doc_id>19</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Chinese RST Parser</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>20</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Annotation of Chinese RST Tree</title>
            <text>Similar to (Soricut and Marcu, 2003), a node of RST tree is represented as a tuple R-[s, m, e], which means the relation R controls two semantic spans U 1 and U 2 , U 1 starts from word position s and stops at word position m. U 2 starts from m+1 and ends with e. Under the guidance of definition of RST, Yue (2008) defined 12 groups 1 of
1 They are Parallel, Alternative, Condition, Reason, Elaboration, Means, Preparation, Enablement, Antithesis, Background, Evidences, Others.
Example 1: Antithesis U 1:[0,9] U 2:[10,21] Reason U 1:[10,13] U 2:[14,21]
Although the rupee's nominal rate against the dollar was held down , India's real exchange rate rose because of high inflation .
Cue-words pair matching set of cue words for span [0,9] and [10,21]:{ &#21363; &#20351; / &#30001; &#20110; , &#21363; &#20351; /NULL,NULL/ &#30001; &#20110; } Cue-words pair matching set of cue words for span [10,13] and [14,21]:{ &#30001; &#20110; /NULL} RST-based Rules: Antithesis:: &#21363; &#20351; [X]/[Y] =&gt; Although[X]/[Y] ; Reason:: &#30001; &#20110; [X]/[Y] =&gt; [Y]/because of[X]
rhetorical relations for Chinese particularly, upon which our Chinese RST parser is developed.
Figure 1 illustrates an example of Chinese RST tree and its alignment to the English string. There are two levels in this tree. The Antithesis relation controls U 1 from 0 to 9 and U 2 from 10 to 21. Thus it is written as Antithesis-[0,9,21]. Different shadow blocks denote the alignments of different edus. Links between source and target words are alignments of cue words. Cue words are viewed as the strongest clues for rhetorical relation recognition and always found at the beginning of text (Reitter, 2003), such as &#8220; &#21363; &#20351; (although), &#30001; &#20110; (because of)&#8221;. With the cue words included, the relations are much easier to be analyzed. So we focus on the explicit relations with cue words in this paper as our first try.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Similar to (Soricut and Marcu, 2003), a node of RST tree is represented as a tuple R-[s, m, e], which means the relation R controls two semantic spans U 1 and U 2 , U 1 starts from word position s and stops at word position m. U 2 starts from m+1 and ends with e. Under the guidance of definition of RST, Yue (2008) defined 12 groups 1 of</text>
                  <doc_id>21</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 They are Parallel, Alternative, Condition, Reason, Elaboration, Means, Preparation, Enablement, Antithesis, Background, Evidences, Others.</text>
                  <doc_id>22</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Example 1: Antithesis U 1:[0,9] U 2:[10,21] Reason U 1:[10,13] U 2:[14,21]</text>
                  <doc_id>23</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Although the rupee's nominal rate against the dollar was held down , India's real exchange rate rose because of high inflation .</text>
                  <doc_id>24</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Cue-words pair matching set of cue words for span [0,9] and [10,21]:{ &#21363; &#20351; / &#30001; &#20110; , &#21363; &#20351; /NULL,NULL/ &#30001; &#20110; } Cue-words pair matching set of cue words for span [10,13] and [14,21]:{ &#30001; &#20110; /NULL} RST-based Rules: Antithesis:: &#21363; &#20351; [X]/[Y] =&gt; Although[X]/[Y] ; Reason:: &#30001; &#20110; [X]/[Y] =&gt; [Y]/because of[X]</text>
                  <doc_id>25</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>rhetorical relations for Chinese particularly, upon which our Chinese RST parser is developed.</text>
                  <doc_id>26</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 1 illustrates an example of Chinese RST tree and its alignment to the English string.</text>
                  <doc_id>27</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>There are two levels in this tree.</text>
                  <doc_id>28</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The Antithesis relation controls U 1 from 0 to 9 and U 2 from 10 to 21.</text>
                  <doc_id>29</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Thus it is written as Antithesis-[0,9,21].</text>
                  <doc_id>30</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Different shadow blocks denote the alignments of different edus.</text>
                  <doc_id>31</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Links between source and target words are alignments of cue words.</text>
                  <doc_id>32</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Cue words are viewed as the strongest clues for rhetorical relation recognition and always found at the beginning of text (Reitter, 2003), such as &#8220; &#21363; &#20351; (although), &#30001; &#20110; (because of)&#8221;.</text>
                  <doc_id>33</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>With the cue words included, the relations are much easier to be analyzed.</text>
                  <doc_id>34</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>So we focus on the explicit relations with cue words in this paper as our first try.</text>
                  <doc_id>35</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Bayesian Method for Chinese RST Parser</title>
            <text>For Chinese RST parser, there are two tasks. One is the segmentation of edu and the other is the relation tagging between two semantic spans.
Feature F 1 (F 6 ) F 2 (F 5 ) F 3 (F 4 ) F 7 F 8 (F 9 )
Meaning left(right) child is a syntactic sub-tree? left(right) child ends with a punctuation? cue words of left (right) child. left and right children are sibling nodes? syntactic head symbol of left(right) child.
Inspired by the features used in English RST parser (Soricut and Marcu, 2003; Reitter, 2003; Duverle and Prendinger, 2009; Hernault et al., 2010a), we design a Bayesian model to build a joint parser for segmentation and tagging simultaneously. In this model, 9 features in Table 1 are used. In the table, punctuations include comma, semicolons, period and question mark. We view explicit connectives as cue words in this paper. Figure 2 illustrates the conditional independences of 9 features which are denoted with F 1 ~F 9.
F 1 F 2 F 8 F 3 F 4 F 5 F 6 F 7 F 9
m Rel
The segmentation and parsing conditional probabilities are computed as follows:
P (mjF 9 1 ) = P (mjF 3 1 ; F 8 ) (1)
P (ejF 9 1 ) = P (ejF 7 4 ; F 9 ) (2)
P (ReljF 9 1 ) = P (ReljF 4 3 ) (3)
where F n represents the n th feature , F l n means features from n to l. Rel is short for relation. (1) and (2) describe the conditional probabilities of m and e. When using Formula (3) to predict the relation, we search all the cue-words pair, as shown in Figure 1, to get the best match. When training, we use maximum likelihood estimation to get all the associated probabilities. For decoding, the pseudo codes are given as below.
1: Nodes={[]} 2: Parser(0,End) 3: Parser(s,e): // recursive parser function 4: if s &gt; e or e is -1: return -1; 5: m = GetMaxM(s,e) //compute m through Formula(1);if no cue words found, then m=-1; 6: e&#8217; = GetMaxE(s,m,e) //compute e&#8217; through F (2);
7: if m or e&#8217; equals to -1: return -1; 8: Rel=GetRelation(s,m,e&#8217;) //compute relation by F
(3)
9: push [Rel,s,m,e&#8217;] into Nodes 10: Parser(s,m) 11: Parser(m+1,e&#8217;) 12: Parser(e&#8217;+1,e) 13: Rel=GetRelation(s,e&#8217;,e) 14: push [Rel,s,e&#8217;,e] into Nodes 15: return e
e
For example in Figure 1, for the first iteration, s=0 and m will be chosen from {1-20}. We get m=9 through Formula (1). Then, similar with m, we get e=21 through Formula (2). Finally, the relation is figured out by Formula (3). Thus, a node is generated. A complete RST tree constructs until the end of the iterative process for this sentence. This method can run fast due to the simple greedy algorithm. It is plausible in our cases, because we only have a small scale of manually-annotated Chinese RST corpus, which prefers simple rather than complicated models.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>For Chinese RST parser, there are two tasks.</text>
                  <doc_id>36</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>One is the segmentation of edu and the other is the relation tagging between two semantic spans.</text>
                  <doc_id>37</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Feature F 1 (F 6 ) F 2 (F 5 ) F 3 (F 4 ) F 7 F 8 (F 9 )</text>
                  <doc_id>38</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Meaning left(right) child is a syntactic sub-tree?</text>
                  <doc_id>39</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>left(right) child ends with a punctuation?</text>
                  <doc_id>40</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>cue words of left (right) child.</text>
                  <doc_id>41</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>left and right children are sibling nodes?</text>
                  <doc_id>42</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>syntactic head symbol of left(right) child.</text>
                  <doc_id>43</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Inspired by the features used in English RST parser (Soricut and Marcu, 2003; Reitter, 2003; Duverle and Prendinger, 2009; Hernault et al., 2010a), we design a Bayesian model to build a joint parser for segmentation and tagging simultaneously.</text>
                  <doc_id>44</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this model, 9 features in Table 1 are used.</text>
                  <doc_id>45</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In the table, punctuations include comma, semicolons, period and question mark.</text>
                  <doc_id>46</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We view explicit connectives as cue words in this paper.</text>
                  <doc_id>47</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Figure 2 illustrates the conditional independences of 9 features which are denoted with F 1 ~F 9.</text>
                  <doc_id>48</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>F 1 F 2 F 8 F 3 F 4 F 5 F 6 F 7 F 9</text>
                  <doc_id>49</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>m Rel</text>
                  <doc_id>50</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The segmentation and parsing conditional probabilities are computed as follows:</text>
                  <doc_id>51</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (mjF 9 1 ) = P (mjF 3 1 ; F 8 ) (1)</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (ejF 9 1 ) = P (ejF 7 4 ; F 9 ) (2)</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (ReljF 9 1 ) = P (ReljF 4 3 ) (3)</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where F n represents the n th feature , F l n means features from n to l. Rel is short for relation.</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(1) and (2) describe the conditional probabilities of m and e. When using Formula (3) to predict the relation, we search all the cue-words pair, as shown in Figure 1, to get the best match.</text>
                  <doc_id>56</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>When training, we use maximum likelihood estimation to get all the associated probabilities.</text>
                  <doc_id>57</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For decoding, the pseudo codes are given as below.</text>
                  <doc_id>58</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1: Nodes={[]} 2: Parser(0,End) 3: Parser(s,e): // recursive parser function 4: if s &gt; e or e is -1: return -1; 5: m = GetMaxM(s,e) //compute m through Formula(1);if no cue words found, then m=-1; 6: e&#8217; = GetMaxE(s,m,e) //compute e&#8217; through F (2);</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>7: if m or e&#8217; equals to -1: return -1; 8: Rel=GetRelation(s,m,e&#8217;) //compute relation by F</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(3)</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>9: push [Rel,s,m,e&#8217;] into Nodes 10: Parser(s,m) 11: Parser(m+1,e&#8217;) 12: Parser(e&#8217;+1,e) 13: Rel=GetRelation(s,e&#8217;,e) 14: push [Rel,s,e&#8217;,e] into Nodes 15: return e</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e</text>
                  <doc_id>63</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For example in Figure 1, for the first iteration, s=0 and m will be chosen from {1-20}.</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We get m=9 through Formula (1).</text>
                  <doc_id>65</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Then, similar with m, we get e=21 through Formula (2).</text>
                  <doc_id>66</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Finally, the relation is figured out by Formula (3).</text>
                  <doc_id>67</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Thus, a node is generated.</text>
                  <doc_id>68</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>A complete RST tree constructs until the end of the iterative process for this sentence.</text>
                  <doc_id>69</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>This method can run fast due to the simple greedy algorithm.</text>
                  <doc_id>70</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>It is plausible in our cases, because we only have a small scale of manually-annotated Chinese RST corpus, which prefers simple rather than complicated models.</text>
                  <doc_id>71</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Translation Model</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>72</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Rule Extraction</title>
            <text>As shown in Figure 1, the RST tree-to-string alignment provides us with two types of translation rules. One is common phrase-based rules, which are just like those in phrase-based model (Koehn et al., 2003). The other is RST tree-tostring rule, and it&#8217;s defined as,
relation ::U 1 (&#174;; X)=U 2 (&#176;; Y ) ) U 1 (tr(&#174;); tr(X)) &#187; U 2 (tr(&#176;); tr(Y ))
where the terminal characters &#945; and &#947; represent the cue words which are optimum match for maximizing Formula (3). While the nonterminals X and Y represent the rest of the sequence. Function tr( &#183; ) means the translation of &#183;. The operator ~ is an operator to indicate that the order of tr(U 1 ) and tr(U 2 ) is monotone or reverse. During rules&#8217; extraction, if the mean position of all the words in tr(U 1 ) precedes that in tr(U 2 ), ~ is monotone. Otherwise, ~ is reverse.
For example in Figure 1, the Reason relation controls U 1 :[10,13] and U 2 :[14,21]. Because the mean position of tr(U 2 ) is before that of tr(U 1 ), the reverse order is selected. We list the RSTbased rules for Example 1 in Figure 1.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>As shown in Figure 1, the RST tree-to-string alignment provides us with two types of translation rules.</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>One is common phrase-based rules, which are just like those in phrase-based model (Koehn et al., 2003).</text>
                  <doc_id>74</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The other is RST tree-tostring rule, and it&#8217;s defined as,</text>
                  <doc_id>75</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>relation ::U 1 (&#174;; X)=U 2 (&#176;; Y ) ) U 1 (tr(&#174;); tr(X)) &#187; U 2 (tr(&#176;); tr(Y ))</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where the terminal characters &#945; and &#947; represent the cue words which are optimum match for maximizing Formula (3).</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>While the nonterminals X and Y represent the rest of the sequence.</text>
                  <doc_id>78</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Function tr( &#183; ) means the translation of &#183;.</text>
                  <doc_id>79</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The operator ~ is an operator to indicate that the order of tr(U 1 ) and tr(U 2 ) is monotone or reverse.</text>
                  <doc_id>80</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>During rules&#8217; extraction, if the mean position of all the words in tr(U 1 ) precedes that in tr(U 2 ), ~ is monotone.</text>
                  <doc_id>81</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Otherwise, ~ is reverse.</text>
                  <doc_id>82</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For example in Figure 1, the Reason relation controls U 1 :[10,13] and U 2 :[14,21].</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Because the mean position of tr(U 2 ) is before that of tr(U 1 ), the reverse order is selected.</text>
                  <doc_id>84</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We list the RSTbased rules for Example 1 in Figure 1.</text>
                  <doc_id>85</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Probabilities Estimation</title>
            <text>For the phrase-based translation rules, we use four common probabilities and the probabilities&#8217; estimation is the same with those in (Koehn et al., 2003). While the probabilities of RST-based translation rules are given as follows, (1) P(r e jr f ; Rel) = Count(r e;r f ;relation) : where Count(r f ;relation) r e is the target side of the rule, ignorance of the order, i.e. U 1 (tr(&#174;); tr(X)) &#187; U 2 (tr(&#176;); tr(Y )) with two directions, r f is the source side, i.e. U 1 (&#174;; X)=U 2 (&#176;; Y ), and Rel means the relation type.
(2) P(&#191;jr e ; r f ; Rel) = Count(&#191;;r e;r f ;relation)
Count(r e;r f ;relation) :
&#191; 2 fmonotone; reverseg. It is the conditional probability of re-ordering.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>For the phrase-based translation rules, we use four common probabilities and the probabilities&#8217; estimation is the same with those in (Koehn et al., 2003).</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>While the probabilities of RST-based translation rules are given as follows, (1) P(r e jr f ; Rel) = Count(r e;r f ;relation) : where Count(r f ;relation) r e is the target side of the rule, ignorance of the order, i.e. U 1 (tr(&#174;); tr(X)) &#187; U 2 (tr(&#176;); tr(Y )) with two directions, r f is the source side, i.e. U 1 (&#174;; X)=U 2 (&#176;; Y ), and Rel means the relation type.</text>
                  <doc_id>87</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(2) P(&#191;jr e ; r f ; Rel) = Count(&#191;;r e;r f ;relation)</text>
                  <doc_id>88</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Count(r e;r f ;relation) :</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#191; 2 fmonotone; reverseg.</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It is the conditional probability of re-ordering.</text>
                  <doc_id>91</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Decoding</title>
        <text>The decoding procedure of a discourse can be derived from the original decoding formula
e I 1 = argmax e I
P (e I 1jf J 1 ) . Given the rhetorical
structure of a source sentence and the corresponding rule-table, the translating process is to find an optimal path to get the highest score under structure constrains, which is,
argmax es fP(e s j; f t )g
= argmax es f Y
f n2f t
P (e u1 ; e u2 ; &#191;jf n )g
where f t is a source RST tree combined by a set of node f n . e s is the target string combined by series of e n (translations of f n ). f n consists of U 1 and U 2 . e u1 and e u2 are translations of U 1 and U 2 respectively. This global optimization problem is approximately simplified to local optimization to reduce the complexity,
Y
f n2f t
argmax en fP(e u1 ; e u2 ; &#191;jf n )g
In our paper, we have the following two ways to factorize the above formula, Decoder 1:
P (e u1 ; e u2 ; &#191;jf n ) = P (e cp ; e X ; e Y
; &#191;jf cp ; f X ; f Y
) = P (e cp jf cp )P (&#191;je cp ; f cp )P (e X jf X
)P (e Y jf Y
) = P (r e jr f ; Rel)P (&#191;jr e ; r f ; Rel)P(e X jf X
)P (e Y jf Y
)
where e X , e Y are the translation of non-terminal parts. f cp and e cp are cue-words pair of source and target sides. The first and second factors are just the probabilities introduced in Section 3.2. After approximately simplified to local optimization, the final formulae are re-written as,
argmax r fP (r e jr f ; Rel)P (&#191;jr e ; r f ; Rel)g (4) argmax eX fP (e X jf X
)g (5)
argmax eY fP (e Y jf Y
)g (6)
Taking the source sentence with its RST tree in Figure 1 for instance, we adopt a bottom-up manner to do translation recursively. Suppose the best rules selected by (4) are just those written in the figure, Then span [11,13] and [14,21] are firstly translated by (5) and (6). Their translations are then re-packaged by the rule of Reason- [10,13,21]. Iteratively, the translations of span [1,9] and [10,21] are re-packaged by the rule of Antithesis-[0,9,21] to form the final translation.
Decoder 2 : Suppose that the translating process of two spans U 1 and U 2 are independent of each other, we rewrite P(e u1 ; e u2 ; &#191;jf n ) as follows,
P (e u1 ; e u2 ; &#191;jf n )
= P (e u1 ; e u2 ; &#191;jf u1 ; f u2 ) = P (e u1 jf u1 )P (e u2 jf u2 )P (&#191;jr f ; Rel)
= P (e u1 jf u1 )P (e u2 jf u2 ) X r e P (&#191;jr e ; r f ; Rel)P (r e jr f ; Rel)
after approximately simplified to local optimization, the final formulae are re-written as below,
argmax eu1 fPr(e u1 jf u1 )g (7)
argmax eu2 fPr(e u2 jf u2 )g (8)
argmax r f X Pr(&#191;jr e ; r f ; Rel)Pr(r e jr f ; Rel)g (9)
e
We also adopt the bottom-up manner similar to Decoder 1. In Figure 1, U 1 and U 2 of Reason node are firstly translated. Their translations are then re-ordered. Then the translations of two spans of Antithesis node are re-ordered and constructed into the final translation. In Decoder 2, the minimal translation-unit is edu. While in Decoder 1, an edu is further split into cue-word part and the rest part to obtain the respective translation.
In our decoders, language model(LM) is used for translating edus in Formula(5),(6),(7),(8), but not for reordering the upper spans because with the bottom-to-up combination, the spans become longer and harder to be judged by a traditional language model. So we only use RST rules to guide the reordering. But LM will be properly considered in our future work.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The decoding procedure of a discourse can be derived from the original decoding formula</text>
              <doc_id>92</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e I 1 = argmax e I</text>
              <doc_id>93</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (e I 1jf J 1 ) .</text>
              <doc_id>94</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Given the rhetorical</text>
              <doc_id>95</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>structure of a source sentence and the corresponding rule-table, the translating process is to find an optimal path to get the highest score under structure constrains, which is,</text>
              <doc_id>96</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>argmax es fP(e s j; f t )g</text>
              <doc_id>97</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>= argmax es f Y</text>
              <doc_id>98</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>f n2f t</text>
              <doc_id>99</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (e u1 ; e u2 ; &#191;jf n )g</text>
              <doc_id>100</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where f t is a source RST tree combined by a set of node f n .</text>
              <doc_id>101</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>e s is the target string combined by series of e n (translations of f n ).</text>
              <doc_id>102</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>f n consists of U 1 and U 2 .</text>
              <doc_id>103</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>e u1 and e u2 are translations of U 1 and U 2 respectively.</text>
              <doc_id>104</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This global optimization problem is approximately simplified to local optimization to reduce the complexity,</text>
              <doc_id>105</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Y</text>
              <doc_id>106</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>f n2f t</text>
              <doc_id>107</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>argmax en fP(e u1 ; e u2 ; &#191;jf n )g</text>
              <doc_id>108</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In our paper, we have the following two ways to factorize the above formula, Decoder 1:</text>
              <doc_id>109</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (e u1 ; e u2 ; &#191;jf n ) = P (e cp ; e X ; e Y</text>
              <doc_id>110</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>; &#191;jf cp ; f X ; f Y</text>
              <doc_id>111</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>) = P (e cp jf cp )P (&#191;je cp ; f cp )P (e X jf X</text>
              <doc_id>112</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>)P (e Y jf Y</text>
              <doc_id>113</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>) = P (r e jr f ; Rel)P (&#191;jr e ; r f ; Rel)P(e X jf X</text>
              <doc_id>114</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>)P (e Y jf Y</text>
              <doc_id>115</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>)</text>
              <doc_id>116</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where e X , e Y are the translation of non-terminal parts.</text>
              <doc_id>117</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>f cp and e cp are cue-words pair of source and target sides.</text>
              <doc_id>118</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The first and second factors are just the probabilities introduced in Section 3.2.</text>
              <doc_id>119</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>After approximately simplified to local optimization, the final formulae are re-written as,</text>
              <doc_id>120</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>argmax r fP (r e jr f ; Rel)P (&#191;jr e ; r f ; Rel)g (4) argmax eX fP (e X jf X</text>
              <doc_id>121</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>)g (5)</text>
              <doc_id>122</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>argmax eY fP (e Y jf Y</text>
              <doc_id>123</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>)g (6)</text>
              <doc_id>124</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Taking the source sentence with its RST tree in Figure 1 for instance, we adopt a bottom-up manner to do translation recursively.</text>
              <doc_id>125</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Suppose the best rules selected by (4) are just those written in the figure, Then span [11,13] and [14,21] are firstly translated by (5) and (6).</text>
              <doc_id>126</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Their translations are then re-packaged by the rule of Reason- [10,13,21].</text>
              <doc_id>127</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Iteratively, the translations of span [1,9] and [10,21] are re-packaged by the rule of Antithesis-[0,9,21] to form the final translation.</text>
              <doc_id>128</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Decoder 2 : Suppose that the translating process of two spans U 1 and U 2 are independent of each other, we rewrite P(e u1 ; e u2 ; &#191;jf n ) as follows,</text>
              <doc_id>129</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (e u1 ; e u2 ; &#191;jf n )</text>
              <doc_id>130</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>= P (e u1 ; e u2 ; &#191;jf u1 ; f u2 ) = P (e u1 jf u1 )P (e u2 jf u2 )P (&#191;jr f ; Rel)</text>
              <doc_id>131</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>= P (e u1 jf u1 )P (e u2 jf u2 ) X r e P (&#191;jr e ; r f ; Rel)P (r e jr f ; Rel)</text>
              <doc_id>132</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>after approximately simplified to local optimization, the final formulae are re-written as below,</text>
              <doc_id>133</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>argmax eu1 fPr(e u1 jf u1 )g (7)</text>
              <doc_id>134</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>argmax eu2 fPr(e u2 jf u2 )g (8)</text>
              <doc_id>135</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>argmax r f X Pr(&#191;jr e ; r f ; Rel)Pr(r e jr f ; Rel)g (9)</text>
              <doc_id>136</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>e</text>
              <doc_id>137</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We also adopt the bottom-up manner similar to Decoder 1.</text>
              <doc_id>138</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Figure 1, U 1 and U 2 of Reason node are firstly translated.</text>
              <doc_id>139</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Their translations are then re-ordered.</text>
              <doc_id>140</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Then the translations of two spans of Antithesis node are re-ordered and constructed into the final translation.</text>
              <doc_id>141</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In Decoder 2, the minimal translation-unit is edu.</text>
              <doc_id>142</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>While in Decoder 1, an edu is further split into cue-word part and the rest part to obtain the respective translation.</text>
              <doc_id>143</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In our decoders, language model(LM) is used for translating edus in Formula(5),(6),(7),(8), but not for reordering the upper spans because with the bottom-to-up combination, the spans become longer and harder to be judged by a traditional language model.</text>
              <doc_id>144</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>So we only use RST rules to guide the reordering.</text>
              <doc_id>145</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>But LM will be properly considered in our future work.</text>
              <doc_id>146</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>5 Experiment</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>147</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Setup</title>
            <text>In order to do Chinese RST parser, we annotated over 1,000 complicated sentences on CTB (Xue et al., 2005), among which 1,107 sentences are used for training, and 500 sentences are used for testing. Berkeley parser 2 is used for getting the syntactic trees.
The translation experiment is conducted on Chinese-to-English direction. The bilingual training data is from the LDC corpus 3 . The training corpus contains 2.1M sentence pairs. We obtain the word alignment with the grow-diag-final-and strategy by GIZA++ 4 . A 5-gram language model is trained on the Xinhua portion of the English
2 http://code.google.com/p/berkeleyparser/ 3 LDC category number : LDC2000T50, LDC2002E18,
LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, LDC2005T10 and LDC2005T34 4 http://code.google.com/p/giza-pp/
Gigaword corpus. For tuning and testing, we use NIST03 evaluation data as the development set, and extract the relatively long and complicated sentences from NIST04, NIST05 and CWMT08 5 evaluation data as the test set. The number and average word-length of sentences are 511/36, 320/34, 590/38 respectively. We use caseinsensitive BLEU-4 with the shortest length penalty for evaluation.
To create the baseline system, we use the toolkit Moses 6 to build a phrase-based translation system. Meanwhile, considering that Xiong et al. (2009) have presented good results by dividing long and complicated sentences into subsentences only by punctuations during decoding, we re-implement their method for comparison.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In order to do Chinese RST parser, we annotated over 1,000 complicated sentences on CTB (Xue et al., 2005), among which 1,107 sentences are used for training, and 500 sentences are used for testing.</text>
                  <doc_id>148</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Berkeley parser 2 is used for getting the syntactic trees.</text>
                  <doc_id>149</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The translation experiment is conducted on Chinese-to-English direction.</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The bilingual training data is from the LDC corpus 3 .</text>
                  <doc_id>151</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The training corpus contains 2.1M sentence pairs.</text>
                  <doc_id>152</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We obtain the word alignment with the grow-diag-final-and strategy by GIZA++ 4 .</text>
                  <doc_id>153</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>A 5-gram language model is trained on the Xinhua portion of the English</text>
                  <doc_id>154</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 http://code.google.com/p/berkeleyparser/ 3 LDC category number : LDC2000T50, LDC2002E18,</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, LDC2005T10 and LDC2005T34 4 http://code.google.com/p/giza-pp/</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Gigaword corpus.</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For tuning and testing, we use NIST03 evaluation data as the development set, and extract the relatively long and complicated sentences from NIST04, NIST05 and CWMT08 5 evaluation data as the test set.</text>
                  <doc_id>158</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The number and average word-length of sentences are 511/36, 320/34, 590/38 respectively.</text>
                  <doc_id>159</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We use caseinsensitive BLEU-4 with the shortest length penalty for evaluation.</text>
                  <doc_id>160</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>To create the baseline system, we use the toolkit Moses 6 to build a phrase-based translation system.</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Meanwhile, considering that Xiong et al. (2009) have presented good results by dividing long and complicated sentences into subsentences only by punctuations during decoding, we re-implement their method for comparison.</text>
                  <doc_id>162</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Results of Chinese RST Parser</title>
            <text>Table 2 shows the results of RST parsing. On average, our RS trees are 2 layers deep. The parsing errors mostly result from the segmentation errors, which are mainly caused by syntactic parsing errors. On the other hand, the polysemous cue words, such as &#8220; &#32780; (but, and, thus)&#8221; may lead ambiguity for relation recognition, because they can be clues for different relations.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Table 2 shows the results of RST parsing.</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>On average, our RS trees are 2 layers deep.</text>
                  <doc_id>164</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The parsing errors mostly result from the segmentation errors, which are mainly caused by syntactic parsing errors.</text>
                  <doc_id>165</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>On the other hand, the polysemous cue words, such as &#8220; &#32780; (but, and, thus)&#8221; may lead ambiguity for relation recognition, because they can be clues for different relations.</text>
                  <doc_id>166</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>5.3 Results of Translation</title>
            <text>Table 3 presents the translation comparison results. In this table, XD represents the method in (Xiong et al., 2009). D1 stands for Decoder-1, and D2 for Decoder-2. Values with boldface are the highest scores in comparison. D2 performs best on the test data with 2.3/0.77/1.43/1.16 points. Compared with XD, our results also outperform by 0.52 points on the whole test data. Observing and comparing the translation results, we find that our translation results are more readable by maintaining the semantic integrality of the edus and by giving more appreciate reorganization of the translated edus.
5 China Workshop on Machine Translation 2008 6 www.statmt.org/moses/index.php?n=Main.HomePage</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Table 3 presents the translation comparison results.</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this table, XD represents the method in (Xiong et al., 2009).</text>
                  <doc_id>168</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>D1 stands for Decoder-1, and D2 for Decoder-2.</text>
                  <doc_id>169</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Values with boldface are the highest scores in comparison.</text>
                  <doc_id>170</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>D2 performs best on the test data with 2.3/0.77/1.43/1.16 points.</text>
                  <doc_id>171</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Compared with XD, our results also outperform by 0.52 points on the whole test data.</text>
                  <doc_id>172</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Observing and comparing the translation results, we find that our translation results are more readable by maintaining the semantic integrality of the edus and by giving more appreciate reorganization of the translated edus.</text>
                  <doc_id>173</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5 China Workshop on Machine Translation 2008 6 www.statmt.org/moses/index.php?n=Main.HomePage</text>
                  <doc_id>174</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>6</index>
        <title>6 Conclusion and Future Work</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>175</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>7</index>
        <title>Acknowledgments</title>
        <text>References
David A Duverle and Helmut Prendinger. 2009. A novel discourse parser based on support vector machine classification. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4 th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 665&#8211;673. Association for Computational Linguistics.
Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011. Cache-based document-level statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 909&#8211;919. Association for Computational Linguistics.
Hugo Hernault, Danushka Bollegala, and Mitsuru Ishizuka. 2010a. A sequential model for discourse segmentation. Computational Linguistics and Intelligent Text Processing, pages 315&#8211;326.
Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka, et al. 2010b. Hilda: A discourse parser using support vector machine classification. Dialogue &amp; Discourse, 1(3).
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology Volume 1, pages 48&#8211;54. Association for Computational Linguistics.
William C Mann and Sandra A Thompson. 1986. Rhetorical structure theory: Description and construction of text structures. Technical report, DTIC Document.
William C Mann and Sandra A Thompson. 1987. Rhetorical structure theory: A framework for the analysis of texts. Technical report, DTIC Document.
William C Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243&#8211;281.
Daniel Marcu, Lynn Carlson, and Maki Watanabe. 2000. The automatic translation of discourse structures. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 9&#8211;17. Morgan Kaufmann Publishers Inc.
David Reitter. 2003. Simple signals for complex rhetorics: On rhetorical analysis with rich-feature support vector models. Language, 18:52.
Radu Soricut and Daniel Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 149&#8211;156. Association for Computational Linguistics.
Billy TM Wong and Chunyu Kit. 2012. Extending machine translation evaluation metrics with lexical cohesion to document level. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, page 1060&#8211;1068. Association for Computational Linguistics.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang. 2011. Document-level consistency verification in machine translation. In Machine Translation Summit, volume 13, pages 131&#8211;138.
Hao Xiong, Wenwen Xu, Haitao Mi, Yang Liu, and Qun Liu. 2009. Sub-sentence division for treebased machine translation. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 137&#8211;140. Association for Computational Linguistics.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta Palmer. 2005. The Penn Chinese treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207.
Ming Yue. 2008. Rhetorical structure annotation of Chinese news commentaries. Journal of Chinese Information Processing, 4:002.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>References</text>
              <doc_id>176</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>David A Duverle and Helmut Prendinger.</text>
              <doc_id>177</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>178</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A novel discourse parser based on support vector machine classification.</text>
              <doc_id>179</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4 th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 665&#8211;673.</text>
              <doc_id>180</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>181</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Zhengxian Gong, Min Zhang, and Guodong Zhou.</text>
              <doc_id>182</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2011.</text>
              <doc_id>183</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Cache-based document-level statistical machine translation.</text>
              <doc_id>184</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 909&#8211;919.</text>
              <doc_id>185</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>186</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Hugo Hernault, Danushka Bollegala, and Mitsuru Ishizuka.</text>
              <doc_id>187</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2010a.</text>
              <doc_id>188</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A sequential model for discourse segmentation.</text>
              <doc_id>189</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Computational Linguistics and Intelligent Text Processing, pages 315&#8211;326.</text>
              <doc_id>190</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka, et al. 2010b.</text>
              <doc_id>191</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Hilda: A discourse parser using support vector machine classification.</text>
              <doc_id>192</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Dialogue &amp; Discourse, 1(3).</text>
              <doc_id>193</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Philipp Koehn, Franz Josef Och, and Daniel Marcu.</text>
              <doc_id>194</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>195</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Statistical phrase-based translation.</text>
              <doc_id>196</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology Volume 1, pages 48&#8211;54.</text>
              <doc_id>197</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>198</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>William C Mann and Sandra A Thompson.</text>
              <doc_id>199</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1986.</text>
              <doc_id>200</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Rhetorical structure theory: Description and construction of text structures.</text>
              <doc_id>201</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Technical report, DTIC Document.</text>
              <doc_id>202</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>William C Mann and Sandra A Thompson.</text>
              <doc_id>203</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1987.</text>
              <doc_id>204</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Rhetorical structure theory: A framework for the analysis of texts.</text>
              <doc_id>205</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Technical report, DTIC Document.</text>
              <doc_id>206</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>William C Mann and Sandra A Thompson.</text>
              <doc_id>207</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1988.</text>
              <doc_id>208</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Rhetorical structure theory: Toward a functional theory of text organization.</text>
              <doc_id>209</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Text, 8(3):243&#8211;281.</text>
              <doc_id>210</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Daniel Marcu, Lynn Carlson, and Maki Watanabe.</text>
              <doc_id>211</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2000.</text>
              <doc_id>212</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The automatic translation of discourse structures.</text>
              <doc_id>213</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 9&#8211;17.</text>
              <doc_id>214</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Morgan Kaufmann Publishers Inc.</text>
              <doc_id>215</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>David Reitter.</text>
              <doc_id>216</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>217</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Simple signals for complex rhetorics: On rhetorical analysis with rich-feature support vector models.</text>
              <doc_id>218</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Language, 18:52.</text>
              <doc_id>219</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Radu Soricut and Daniel Marcu.</text>
              <doc_id>220</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2003.</text>
              <doc_id>221</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Sentence level discourse parsing using syntactic and lexical information.</text>
              <doc_id>222</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 149&#8211;156.</text>
              <doc_id>223</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>224</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Billy TM Wong and Chunyu Kit.</text>
              <doc_id>225</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2012.</text>
              <doc_id>226</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Extending machine translation evaluation metrics with lexical cohesion to document level.</text>
              <doc_id>227</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, page 1060&#8211;1068.</text>
              <doc_id>228</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>229</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.</text>
              <doc_id>230</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2011.</text>
              <doc_id>231</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Document-level consistency verification in machine translation.</text>
              <doc_id>232</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Machine Translation Summit, volume 13, pages 131&#8211;138.</text>
              <doc_id>233</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Hao Xiong, Wenwen Xu, Haitao Mi, Yang Liu, and Qun Liu.</text>
              <doc_id>234</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2009.</text>
              <doc_id>235</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Sub-sentence division for treebased machine translation.</text>
              <doc_id>236</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 137&#8211;140.</text>
              <doc_id>237</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Association for Computational Linguistics.</text>
              <doc_id>238</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta Palmer.</text>
              <doc_id>239</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2005.</text>
              <doc_id>240</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The Penn Chinese treebank: Phrase structure annotation of a large corpus.</text>
              <doc_id>241</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Natural Language Engineering, 11(2):207.</text>
              <doc_id>242</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Ming Yue.</text>
              <doc_id>243</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2008.</text>
              <doc_id>244</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Rhetorical structure annotation of Chinese news commentaries.</text>
              <doc_id>245</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Journal of Chinese Information Processing, 4:002.</text>
              <doc_id>246</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables/>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>David A Duverle</author>
          <author>Helmut Prendinger</author>
        </authors>
        <title>A novel discourse parser based on support vector machine classification.</title>
        <publication>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4 th International Joint Conference on Natural Language Processing of the AFNLP: Volume</publication>
        <pages>665--673</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Zhengxian Gong</author>
          <author>Min Zhang</author>
          <author>Guodong Zhou</author>
        </authors>
        <title>Cache-based document-level statistical machine translation.</title>
        <publication>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>909--919</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Hugo Hernault</author>
          <author>Danushka Bollegala</author>
          <author>Mitsuru Ishizuka</author>
        </authors>
        <title>A sequential model for discourse segmentation.</title>
        <publication>Computational Linguistics and Intelligent Text Processing,</publication>
        <pages>315--326</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Hugo Hernault</author>
        </authors>
        <title>Helmut Prendinger, Mitsuru Ishizuka, et al. 2010b. Hilda: A discourse parser using support vector machine classification.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz Josef Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical phrase-based translation.</title>
        <publication>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</publication>
        <pages>48--54</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>William C Mann</author>
          <author>Sandra A Thompson</author>
        </authors>
        <title>Rhetorical structure theory: Description and construction of text structures.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1986</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>William C Mann</author>
          <author>Sandra A Thompson</author>
        </authors>
        <title>Rhetorical structure theory: A framework for the analysis of texts.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1987</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>William C Mann</author>
          <author>Sandra A Thompson</author>
        </authors>
        <title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1988</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Daniel Marcu</author>
          <author>Lynn Carlson</author>
          <author>Maki Watanabe</author>
        </authors>
        <title>The automatic translation of discourse structures.</title>
        <publication>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference,</publication>
        <pages>9--17</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>David Reitter</author>
        </authors>
        <title>Simple signals for complex rhetorics: On rhetorical analysis with rich-feature support vector models.</title>
        <publication>None</publication>
        <pages>18--52</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Radu Soricut</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Sentence level discourse parsing using syntactic and lexical information.</title>
        <publication>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1,</publication>
        <pages>149--156</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Billy TM Wong</author>
          <author>Chunyu Kit</author>
        </authors>
        <title>Extending machine translation evaluation metrics with lexical cohesion to document level.</title>
        <publication>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</publication>
        <pages>1060--1068</pages>
        <date>2012</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Tong Xiao</author>
          <author>Jingbo Zhu</author>
          <author>Shujie Yao</author>
          <author>Hao Zhang</author>
        </authors>
        <title>Document-level consistency verification in machine translation.</title>
        <publication>In Machine Translation Summit,</publication>
        <pages>131--138</pages>
        <date>2011</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Hao Xiong</author>
          <author>Wenwen Xu</author>
          <author>Haitao Mi</author>
          <author>Yang Liu</author>
          <author>Qun Liu</author>
        </authors>
        <title>Sub-sentence division for treebased machine translation.</title>
        <publication>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</publication>
        <pages>137--140</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Naiwen Xue</author>
          <author>Fei Xia</author>
          <author>Fu-Dong Chiou</author>
          <author>Marta Palmer</author>
        </authors>
        <title>The Penn Chinese treebank: Phrase structure annotation of a large corpus.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Duverle and Prendinger, 2009</string>
        <sentence_id>38642</sentence_id>
        <char_offset>93</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Gong et al., 2011</string>
        <sentence_id>38607</sentence_id>
        <char_offset>33</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Hernault et al., 2010</string>
        <sentence_id>38642</sentence_id>
        <char_offset>123</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>4</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>38672</sentence_id>
        <char_offset>83</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>4</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>38684</sentence_id>
        <char_offset>134</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>7</reference_id>
        <string>Mann and Thompson, 1988</string>
        <sentence_id>38611</sentence_id>
        <char_offset>35</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>8</reference_id>
        <string>Marcu et al. (2000)</string>
        <sentence_id>38609</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>9</reference_id>
        <string>Reitter, 2003</string>
        <sentence_id>38631</sentence_id>
        <char_offset>123</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>9</reference_id>
        <string>Reitter, 2003</string>
        <sentence_id>38642</sentence_id>
        <char_offset>78</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>10</reference_id>
        <string>Soricut and Marcu, 2003</string>
        <sentence_id>38619</sentence_id>
        <char_offset>12</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>10</reference_id>
        <string>Soricut and Marcu, 2003</string>
        <sentence_id>38642</sentence_id>
        <char_offset>53</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>11</reference_id>
        <string>Wong and Kit, 2012</string>
        <sentence_id>38607</sentence_id>
        <char_offset>71</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>12</reference_id>
        <string>Xiao et al., 2011</string>
        <sentence_id>38607</sentence_id>
        <char_offset>52</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>13</reference_id>
        <string>Xiong et al. (2009)</string>
        <sentence_id>38760</sentence_id>
        <char_offset>28</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>14</reference_id>
        <string>Xue et al., 2005</string>
        <sentence_id>38746</sentence_id>
        <char_offset>89</char_offset>
      </citation>
    </citations>
  </content>
</document>
