<document>
  <filename>C10-1070</filename>
  <authors>
    <author>Audrey Laroche</author>
  </authors>
  <title>Revisiting Context-based Projection Methods for Term-Translation Spotting in Comparable Corpora</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>Context-based projection methods for identifying the translation of terms in comparable corpora has attracted a lot of attention in the community, e.g. (Fung, 1998; Rapp, 1999). Surprisingly, none of those works have systematically investigated the impact of the many parameters controlling their approach. The present study aims at doing just this. As a testcase, we address the task of translating terms of the medical domain by exploiting pages mined from Wikipedia. One interesting outcome of this study is that significant gains can be obtained by using an association measure that is rarely used in practice.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Context-based projection methods for identifying the translation of terms in comparable corpora has attracted a lot of attention in the community, e.g. (Fung, 1998; Rapp, 1999).</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Surprisingly, none of those works have systematically investigated the impact of the many parameters controlling their approach.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The present study aims at doing just this.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As a testcase, we address the task of translating terms of the medical domain by exploiting pages mined from Wikipedia.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>One interesting outcome of this study is that significant gains can be obtained by using an association measure that is rarely used in practice.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Identifying translations of terms in comparable corpora is a challenge that has attracted many researchers. A popular idea that emerged for solving this problem is based on the assumption that the context of a term and its translation share similarities that can be used to rank translation candidates (Fung, 1998; Rapp, 1999). Many variants of this idea have been implemented. While a few studies have investigated pattern matching approaches to compare source and target contexts (Fung, 1995; Diab and Finch, 2000; Yu and Tsujii, 2009), most variants make use of a bilingual lexicon in order to translate the words of the context of a term (often called seed words). D&#233;jean et al. (2005) instead use a bilingual thesaurus for translating these.
Another distinction between approaches lies in the way the context is defined. The most common practice, the so-called window-based approach, defines the context words as those cooccuring significantly with the source term within windows centered around the term. 1 Some studies have reported gains by considering syntactically motivated co-occurrences. Yu and Tsujii (2009) propose a resource-intensive strategy which requires both source and target dependency parsers, while Otero (2007) investigates a lighter approach where a few hand coded regular expressions based on POS tags simulate source parsing. The latter approach only requires a POS tagger of the source and the target languages as well as a small parallel corpus in order to project the source regular expressions. Naturally, studies differ in the way each cooccurrence (either window or syntax-based) is weighted, and a plethora of association scores have been investigated and compared, the likelihood score (Dunning, 1993) being among the most popular. Also, different similarity measures have been proposed for ranking target context vectors, among which the popular cosine measure. The goal of the different authors who investigate context-projection approaches also varies. Some studies are tackling the problem of identifying the translation of general words (Rapp, 1999; Otero, 2007; Yu and Tsujii, 2009) while others are addressing the translation of domain specific terms. Among the latter, many are translating single-word terms (Chiao and Zweigenbaum, 2002; D&#233;jean et al., 2005; Prochasson et
1 A stoplist is typically used in order to prevent function
words from populating the context vectors.
al., 2009), while others are tackling the translation of multi-word terms (Daille and Morin, 2005). The type of discourse might as well be of concern in some of the studies dedicated to bilingual terminology mining. For instance, Morin et al. (2007) distinguish popular science versus scientific terms, while Saralegi et al. (2008) target popular science terms only. The present discussion only focuses on a few number of representative studies. Still, it is already striking that a direct comparison of them is difficult, if not impossible. Differences in resources being used (in quantities, in domains, etc.), in technical choices made (similarity measures, context vector computation, etc.) and in objectives (general versus terminological dictionary extraction) prevent one from establishing a clear landscape of the various approaches.
Indeed, many studies provide some figures that help to appreciate the influence of some parameters in a given experimental setting. Notably, Otero (2008) studies no less than 7 similarity measures for ranking context vectors while comparing window and syntax-based methods. Morin et al. (2007) consider both the log-likelihood and the mutual information association scores as well as the Jaccard and the cosine similarity measures.
Ideally, a benchmark on which researchers could run their translation finder would ease the comparison of the different approaches. However, designing such a benchmark that would satisfy the evaluation purposes of all the researchers is far too ambitious a goal for this contribution. Instead, we investigate the impact of some major factors influencing projection-based approaches on a task of translating 5,000 terms of the medical domain (the most studied domain), making use of French and English Wikipedia pages extracted monolingually thanks to an information retrieval engine. While the present work does not investigate all the parameters that could potentially impact results, we believe it constitutes the most complete and systematic comparison made so far with variants of the context-based projection approach.
In the remainder of this paper, we describe the projection-based approach to translation spotting in Section 2 and detail the parameters that directly influence its performance. The experimental protocol we followed is described in Section 3 and we analyze our results in Section 4. We discuss the main results in the light of previous work and propose some future avenues in Section 5.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Identifying translations of terms in comparable corpora is a challenge that has attracted many researchers.</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A popular idea that emerged for solving this problem is based on the assumption that the context of a term and its translation share similarities that can be used to rank translation candidates (Fung, 1998; Rapp, 1999).</text>
              <doc_id>6</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Many variants of this idea have been implemented.</text>
              <doc_id>7</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>While a few studies have investigated pattern matching approaches to compare source and target contexts (Fung, 1995; Diab and Finch, 2000; Yu and Tsujii, 2009), most variants make use of a bilingual lexicon in order to translate the words of the context of a term (often called seed words).</text>
              <doc_id>8</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>D&#233;jean et al. (2005) instead use a bilingual thesaurus for translating these.</text>
              <doc_id>9</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Another distinction between approaches lies in the way the context is defined.</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The most common practice, the so-called window-based approach, defines the context words as those cooccuring significantly with the source term within windows centered around the term.</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>1 Some studies have reported gains by considering syntactically motivated co-occurrences.</text>
              <doc_id>12</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Yu and Tsujii (2009) propose a resource-intensive strategy which requires both source and target dependency parsers, while Otero (2007) investigates a lighter approach where a few hand coded regular expressions based on POS tags simulate source parsing.</text>
              <doc_id>13</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The latter approach only requires a POS tagger of the source and the target languages as well as a small parallel corpus in order to project the source regular expressions.</text>
              <doc_id>14</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Naturally, studies differ in the way each cooccurrence (either window or syntax-based) is weighted, and a plethora of association scores have been investigated and compared, the likelihood score (Dunning, 1993) being among the most popular.</text>
              <doc_id>15</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Also, different similarity measures have been proposed for ranking target context vectors, among which the popular cosine measure.</text>
              <doc_id>16</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The goal of the different authors who investigate context-projection approaches also varies.</text>
              <doc_id>17</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Some studies are tackling the problem of identifying the translation of general words (Rapp, 1999; Otero, 2007; Yu and Tsujii, 2009) while others are addressing the translation of domain specific terms.</text>
              <doc_id>18</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Among the latter, many are translating single-word terms (Chiao and Zweigenbaum, 2002; D&#233;jean et al., 2005; Prochasson et</text>
              <doc_id>19</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 A stoplist is typically used in order to prevent function</text>
              <doc_id>20</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>words from populating the context vectors.</text>
              <doc_id>21</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>al., 2009), while others are tackling the translation of multi-word terms (Daille and Morin, 2005).</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The type of discourse might as well be of concern in some of the studies dedicated to bilingual terminology mining.</text>
              <doc_id>23</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For instance, Morin et al. (2007) distinguish popular science versus scientific terms, while Saralegi et al. (2008) target popular science terms only.</text>
              <doc_id>24</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The present discussion only focuses on a few number of representative studies.</text>
              <doc_id>25</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Still, it is already striking that a direct comparison of them is difficult, if not impossible.</text>
              <doc_id>26</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Differences in resources being used (in quantities, in domains, etc.), in technical choices made (similarity measures, context vector computation, etc.) and in objectives (general versus terminological dictionary extraction) prevent one from establishing a clear landscape of the various approaches.</text>
              <doc_id>27</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Indeed, many studies provide some figures that help to appreciate the influence of some parameters in a given experimental setting.</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Notably, Otero (2008) studies no less than 7 similarity measures for ranking context vectors while comparing window and syntax-based methods.</text>
              <doc_id>29</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Morin et al. (2007) consider both the log-likelihood and the mutual information association scores as well as the Jaccard and the cosine similarity measures.</text>
              <doc_id>30</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Ideally, a benchmark on which researchers could run their translation finder would ease the comparison of the different approaches.</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, designing such a benchmark that would satisfy the evaluation purposes of all the researchers is far too ambitious a goal for this contribution.</text>
              <doc_id>32</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Instead, we investigate the impact of some major factors influencing projection-based approaches on a task of translating 5,000 terms of the medical domain (the most studied domain), making use of French and English Wikipedia pages extracted monolingually thanks to an information retrieval engine.</text>
              <doc_id>33</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>While the present work does not investigate all the parameters that could potentially impact results, we believe it constitutes the most complete and systematic comparison made so far with variants of the context-based projection approach.</text>
              <doc_id>34</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the remainder of this paper, we describe the projection-based approach to translation spotting in Section 2 and detail the parameters that directly influence its performance.</text>
              <doc_id>35</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The experimental protocol we followed is described in Section 3 and we analyze our results in Section 4.</text>
              <doc_id>36</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We discuss the main results in the light of previous work and propose some future avenues in Section 5.</text>
              <doc_id>37</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Projection-based variants</title>
        <text>The approach we investigate for identifying term translations in comparable corpora is similar to (Rapp, 1999) and many others. We describe in the following the different steps it encompasses and the parameters we are considering in the light of typical choices made in the literature.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The approach we investigate for identifying term translations in comparable corpora is similar to (Rapp, 1999) and many others.</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We describe in the following the different steps it encompasses and the parameters we are considering in the light of typical choices made in the literature.</text>
              <doc_id>39</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Approach</title>
            <text>Step 1 A comparable corpus is constructed for each term to translate. In this study, the source and target corpora are sets of Wikipedia pages related to the source term (S) and its reference translation (T ) respectively (see Section 3.1). The degree of corpus preprocessing varies greatly from one study to another. Complex linguistic tools such as terminological extractors (Daille and Morin, 2005), parsers (Yu and Tsujii, 2009) or lemmatizers (Rapp, 1999) are sometimes used. In our case, the only preprocessing that takes place is the deletion of the Wikipedia symbols pertaining to its particular syntax (e.g. [[ ]]). 2 It is to be noted that, for the sake of simplicity and generality, our implementation does not exploit interlanguage links nor structural elements specific to Wikipedia documents, as opposed to (Yu and Tsujii, 2009).
Step 2 A context vector v s for the source term S is built (see Figure 1 for a made-up example). This vector contains the words that are in the context of the occurrences of S and are strongly correlated to S. The definition of &#8220;context&#8221; is one of the parameters whose best value we want to find. Context length can be based on a number of units, for instance 3 sentences (Daille and Morin, 2005), windows of 3 (Rapp, 1999) or 25 words (Prochasson et al., 2009), etc. It is an important parameter of the projection-based approach. Should the context length be too small, we would miss words that would be relevant in finding the translation. On the other hand, if the context is too large, it
2 We used a set of about 40 regular expressions to do this.
might contain too much noise. At this step, a stoplist made of function words is applied in order to filter out context words and reduce noise in the context vector.
Additionally, an association measure is used to score the strength of correlation between S and the words in its contexts; it serves to normalize corpus frequencies. Words that have a high association score with S are more prominent in the context vector. The association measure is the second important parameter we want to study. As already noted, most authors use the log-likelihood ratio to measure the association between collocates; some, like (Rapp, 1999), informally compare the performance of a small number of association measures, or combine the results obtained with different association measures (Daille and Morin, 2005).
Step 4 Context vectors v t are computed for each candidate term in the target language corpus (Figure 3). The dimension of the target-vector space is defined to be the one induced by the projection mechanism described in Step 3. The context vector v t of each candidate term is computed as in Step 2. Therefore, in Step 4, the parameters of context definition and association measure are important and take the same values as those in Step 2. Note that in this study, on top of all single terms, we also consider target bigrams as potential candidates (99.5 % of our reference target terms are composed of at most two words). As such, our method can handle complex terms (of up to two words), as opposed to most previous studies, without having to resort to a separate terminological extraction as in (Daille and Morin, 2005).
Step 3 Words in v s are projected into the target language with the help of the bilingual seed lexicon (Figure 2). Each word in v s which is present in the bilingual lexicon is translated, and those translations define the projected context vector v p . Words that are not found in the bilingual lexicon are simply ignored. The size of the seed lexicon and its content are therefore two important parameters of the approach. In previous studies, seed lexicons vary between 16,000 (Rapp, 1999) and 65,000 (D&#233;jean et al., 2005) entries, a typical size being around 20,000 (Fung, 1998; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005).
Step 5 Context vectors v t are ranked in decreasing order of their similarity with v p (Figure 4). The similarity measure between context vectors varies among studies: city-block measure (Rapp, 1999), cosine (Fung, 1998; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005; Prochasson et al., 2009), Dice or Jaccard indexes (Chiao and Zweigenbaum, 2002; Daille and Morin, 2005), etc. It is among the parameters whose effect we experimentally evaluate.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Step 1 A comparable corpus is constructed for each term to translate.</text>
                  <doc_id>40</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In this study, the source and target corpora are sets of Wikipedia pages related to the source term (S) and its reference translation (T ) respectively (see Section 3.1).</text>
                  <doc_id>41</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The degree of corpus preprocessing varies greatly from one study to another.</text>
                  <doc_id>42</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Complex linguistic tools such as terminological extractors (Daille and Morin, 2005), parsers (Yu and Tsujii, 2009) or lemmatizers (Rapp, 1999) are sometimes used.</text>
                  <doc_id>43</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In our case, the only preprocessing that takes place is the deletion of the Wikipedia symbols pertaining to its particular syntax (e.g. [[ ]]).</text>
                  <doc_id>44</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>2 It is to be noted that, for the sake of simplicity and generality, our implementation does not exploit interlanguage links nor structural elements specific to Wikipedia documents, as opposed to (Yu and Tsujii, 2009).</text>
                  <doc_id>45</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Step 2 A context vector v s for the source term S is built (see Figure 1 for a made-up example).</text>
                  <doc_id>46</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This vector contains the words that are in the context of the occurrences of S and are strongly correlated to S.</text>
                  <doc_id>47</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The definition of &#8220;context&#8221; is one of the parameters whose best value we want to find.</text>
                  <doc_id>48</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Context length can be based on a number of units, for instance 3 sentences (Daille and Morin, 2005), windows of 3 (Rapp, 1999) or 25 words (Prochasson et al., 2009), etc. It is an important parameter of the projection-based approach.</text>
                  <doc_id>49</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Should the context length be too small, we would miss words that would be relevant in finding the translation.</text>
                  <doc_id>50</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>On the other hand, if the context is too large, it</text>
                  <doc_id>51</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 We used a set of about 40 regular expressions to do this.</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>might contain too much noise.</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>At this step, a stoplist made of function words is applied in order to filter out context words and reduce noise in the context vector.</text>
                  <doc_id>54</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Additionally, an association measure is used to score the strength of correlation between S and the words in its contexts; it serves to normalize corpus frequencies.</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Words that have a high association score with S are more prominent in the context vector.</text>
                  <doc_id>56</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The association measure is the second important parameter we want to study.</text>
                  <doc_id>57</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>As already noted, most authors use the log-likelihood ratio to measure the association between collocates; some, like (Rapp, 1999), informally compare the performance of a small number of association measures, or combine the results obtained with different association measures (Daille and Morin, 2005).</text>
                  <doc_id>58</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Step 4 Context vectors v t are computed for each candidate term in the target language corpus (Figure 3).</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The dimension of the target-vector space is defined to be the one induced by the projection mechanism described in Step 3.</text>
                  <doc_id>60</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The context vector v t of each candidate term is computed as in Step 2.</text>
                  <doc_id>61</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, in Step 4, the parameters of context definition and association measure are important and take the same values as those in Step 2.</text>
                  <doc_id>62</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Note that in this study, on top of all single terms, we also consider target bigrams as potential candidates (99.5 % of our reference target terms are composed of at most two words).</text>
                  <doc_id>63</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>As such, our method can handle complex terms (of up to two words), as opposed to most previous studies, without having to resort to a separate terminological extraction as in (Daille and Morin, 2005).</text>
                  <doc_id>64</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Step 3 Words in v s are projected into the target language with the help of the bilingual seed lexicon (Figure 2).</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Each word in v s which is present in the bilingual lexicon is translated, and those translations define the projected context vector v p .</text>
                  <doc_id>66</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Words that are not found in the bilingual lexicon are simply ignored.</text>
                  <doc_id>67</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The size of the seed lexicon and its content are therefore two important parameters of the approach.</text>
                  <doc_id>68</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In previous studies, seed lexicons vary between 16,000 (Rapp, 1999) and 65,000 (D&#233;jean et al., 2005) entries, a typical size being around 20,000 (Fung, 1998; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005).</text>
                  <doc_id>69</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Step 5 Context vectors v t are ranked in decreasing order of their similarity with v p (Figure 4).</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The similarity measure between context vectors varies among studies: city-block measure (Rapp, 1999), cosine (Fung, 1998; Chiao and Zweigenbaum, 2002; Daille and Morin, 2005; Prochasson et al., 2009), Dice or Jaccard indexes (Chiao and Zweigenbaum, 2002; Daille and Morin, 2005), etc. It is among the parameters whose effect we experimentally evaluate.</text>
                  <doc_id>71</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Parameters studied</title>
            <text>The five steps we described involve many parameters, the values of which can influence at varying degrees the performance of a translation spotter. In the current study, we considered the following parameter values.
Context We considered contexts defined as the current sentence or the current paragraph involv-
ing S. We also considered windows of 5 and 25 words on both sides of S.
Association measure Following the aforementioned studies, we implemented these popular measures: pointwise mutual information (PMI), log-likelihood ratio (LL) and chi-square (&#967; 2 ). We also implemented the discounted log-odds (LO) described by (Evert, 2005, p. 86) in his work on collocation mining. To our knowledge, this association measure has not been used yet in translation spotting. It is computed as:
odds-ratio disc = log (O 11 + 1 2 )(O 22 + 1 2 ) (O 12 + 1 2 )(O 21 + 1 2 )
where O ij are the cells of the 2&#215;2 contingency matrix of a word token s cooccurring with the term S within a given window size. 3
Similarity measure We implemented four measures: city-block, cosine, as well as Dice and Jaccard indexes (Jurafsky and Martin, 2008, p. 666). Our implementations of Dice and Jaccard are identical to the DiceMin and JaccardMin similarity measures reported in (Otero, 2008) and which outperformed the other five metrics he tested.
Seed lexicon We investigated the impact of both the size of the lexicon and its content. We started our study with a mixed lexicon of around 5,000 word entries: roughly 2,000 of them belong to the medical domain, while the other entries belong to the general language. We also considered mixed lexicons of 7,000, 9,000 and 11,000 entries (where 2,000 entries are related to the medical domain), as well as a 5,000-entry general language only lexicon.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The five steps we described involve many parameters, the values of which can influence at varying degrees the performance of a translation spotter.</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In the current study, we considered the following parameter values.</text>
                  <doc_id>73</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Context We considered contexts defined as the current sentence or the current paragraph involv-</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>ing S.</text>
                  <doc_id>75</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We also considered windows of 5 and 25 words on both sides of S.</text>
                  <doc_id>76</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Association measure Following the aforementioned studies, we implemented these popular measures: pointwise mutual information (PMI), log-likelihood ratio (LL) and chi-square (&#967; 2 ).</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We also implemented the discounted log-odds (LO) described by (Evert, 2005, p.</text>
                  <doc_id>78</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>86) in his work on collocation mining.</text>
                  <doc_id>79</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>To our knowledge, this association measure has not been used yet in translation spotting.</text>
                  <doc_id>80</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>It is computed as:</text>
                  <doc_id>81</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>odds-ratio disc = log (O 11 + 1 2 )(O 22 + 1 2 ) (O 12 + 1 2 )(O 21 + 1 2 )</text>
                  <doc_id>82</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where O ij are the cells of the 2&#215;2 contingency matrix of a word token s cooccurring with the term S within a given window size.</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>3</text>
                  <doc_id>84</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Similarity measure We implemented four measures: city-block, cosine, as well as Dice and Jaccard indexes (Jurafsky and Martin, 2008, p.</text>
                  <doc_id>85</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>666).</text>
                  <doc_id>86</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Our implementations of Dice and Jaccard are identical to the DiceMin and JaccardMin similarity measures reported in (Otero, 2008) and which outperformed the other five metrics he tested.</text>
                  <doc_id>87</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Seed lexicon We investigated the impact of both the size of the lexicon and its content.</text>
                  <doc_id>88</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We started our study with a mixed lexicon of around 5,000 word entries: roughly 2,000 of them belong to the medical domain, while the other entries belong to the general language.</text>
                  <doc_id>89</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We also considered mixed lexicons of 7,000, 9,000 and 11,000 entries (where 2,000 entries are related to the medical domain), as well as a 5,000-entry general language only lexicon.</text>
                  <doc_id>90</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>2.3 Cognate heuristic</title>
            <text>Many authors are embedding heuristics in order to improve their approach. For instance, Chiao and Zweigenbaum (2002) propose to integrate a reverse translation spotting strategy in order to improve precision. Prochasson et al. (2009) boost the strength of context words that happen to be transliterated in the other language. A somehow
3 For instance, O 21 stands for the number of windows
containing S but not s.
generalized version of this heuristic has been described in (Shao and Ng, 2004).
In this work, we examine the performance of the best configuration of parameters we found, combined with a simple heuristic based on graphic similarity between source and target terms, similar to the orthographic features in (Haghighi et al., 2008)&#8217;s generative model. This is very specific to our task where medical terms often (but not always) share Latin or Greek roots, such as microvillosit&#233;s in French and microvilli in English. In this heuristic, translation candidates which are cognates of the source term are ranked first among the list of translation candidates. In our implementation, two words are cognates if their first four characters are identical (Simard et al., 1992). One interesting note concerns the wordorder mismatch typically observed in French and English complex terms, such as in ADN mitochondrial (French) and mitochondrial DNA (English). We do treat this case adequately.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Many authors are embedding heuristics in order to improve their approach.</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For instance, Chiao and Zweigenbaum (2002) propose to integrate a reverse translation spotting strategy in order to improve precision.</text>
                  <doc_id>92</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Prochasson et al. (2009) boost the strength of context words that happen to be transliterated in the other language.</text>
                  <doc_id>93</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>A somehow</text>
                  <doc_id>94</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3 For instance, O 21 stands for the number of windows</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>containing S but not s.</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>generalized version of this heuristic has been described in (Shao and Ng, 2004).</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In this work, we examine the performance of the best configuration of parameters we found, combined with a simple heuristic based on graphic similarity between source and target terms, similar to the orthographic features in (Haghighi et al., 2008)&#8217;s generative model.</text>
                  <doc_id>98</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This is very specific to our task where medical terms often (but not always) share Latin or Greek roots, such as microvillosit&#233;s in French and microvilli in English.</text>
                  <doc_id>99</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In this heuristic, translation candidates which are cognates of the source term are ranked first among the list of translation candidates.</text>
                  <doc_id>100</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In our implementation, two words are cognates if their first four characters are identical (Simard et al., 1992).</text>
                  <doc_id>101</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>One interesting note concerns the wordorder mismatch typically observed in French and English complex terms, such as in ADN mitochondrial (French) and mitochondrial DNA (English).</text>
                  <doc_id>102</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We do treat this case adequately.</text>
                  <doc_id>103</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Experimental protocol</title>
        <text>In order to pinpoint the best configuration of values for the parameters identified in Section 2.2, four series of experiments were carried out. In all of them, the task consists of spotting translation candidates for each source language term using the resources 4 described below. The quality of the results is evaluated with the help of the metrics described in Section 3.2.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In order to pinpoint the best configuration of values for the parameters identified in Section 2.2, four series of experiments were carried out.</text>
              <doc_id>104</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In all of them, the task consists of spotting translation candidates for each source language term using the resources 4 described below.</text>
              <doc_id>105</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The quality of the results is evaluated with the help of the metrics described in Section 3.2.</text>
              <doc_id>106</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Resources</title>
            <text>Corpora The comparable corpora are made of the (at most) 50 French and English Wikipedia documents that are the most relevant to the source term and to its reference translation respectively. These documents are retrieved with the NLGbAse Information Retrieval tool. 5 The average token count of all the 50-document corpora as well as the average frequency of the source and target terms in these corpora for our four series of experiments are listed in Table 1.
4 Our resources are available at http://olst.ling.
umontreal.ca/&#732;audrey/coling2010/. They were acquired as described in (Rubino, 2009). 5 http://nlgbase.org/
Experiment 1 2 3 4
The corpora are somewhat small (most corpora in previous studies are made of at least a million words). We believe this is more representative of a task where we try to translate domain specific terms. Some of the Wikipedia documents may contain a handful of parallel sentences (Smith et al., 2010), but this information is not used in our approach. The construction of the corpus involves a bias in that the reference translations are used to obtain the most relevant target language documents. However, since our objective is to compare the relative performance of different sets of parameters, this does not affect our results. In fact, as per (D&#233;jean et al., 2005) (whose comparable corpora are English and German abstracts), the use of such an &#8220;ideal&#8221; corpus is common (as in (Chiao and Zweigenbaum, 2002), where the corpus is built from a specific query).
Seed lexicon The mixed seed lexicon we use is taken from the Heymans Institute of Pharmacology&#8217;s Multilingual glossary of technical and popular medical terms. 6 Random general language entries from the FreeLang 7 project are also incorporated into the lexicon for some of our experiments.
Reference translations The test set is composed of 5,000 nominal single and multi-word pairs of French and English terms from the MeSH (Medical Subject Heading) thesaurus. 8</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Corpora The comparable corpora are made of the (at most) 50 French and English Wikipedia documents that are the most relevant to the source term and to its reference translation respectively.</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These documents are retrieved with the NLGbAse Information Retrieval tool.</text>
                  <doc_id>108</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>5 The average token count of all the 50-document corpora as well as the average frequency of the source and target terms in these corpora for our four series of experiments are listed in Table 1.</text>
                  <doc_id>109</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4 Our resources are available at http://olst.ling.</text>
                  <doc_id>110</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>umontreal.ca/&#732;audrey/coling2010/.</text>
                  <doc_id>111</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>They were acquired as described in (Rubino, 2009).</text>
                  <doc_id>112</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>5 http://nlgbase.org/</text>
                  <doc_id>113</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Experiment 1 2 3 4</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The corpora are somewhat small (most corpora in previous studies are made of at least a million words).</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We believe this is more representative of a task where we try to translate domain specific terms.</text>
                  <doc_id>116</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Some of the Wikipedia documents may contain a handful of parallel sentences (Smith et al., 2010), but this information is not used in our approach.</text>
                  <doc_id>117</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The construction of the corpus involves a bias in that the reference translations are used to obtain the most relevant target language documents.</text>
                  <doc_id>118</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>However, since our objective is to compare the relative performance of different sets of parameters, this does not affect our results.</text>
                  <doc_id>119</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In fact, as per (D&#233;jean et al., 2005) (whose comparable corpora are English and German abstracts), the use of such an &#8220;ideal&#8221; corpus is common (as in (Chiao and Zweigenbaum, 2002), where the corpus is built from a specific query).</text>
                  <doc_id>120</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Seed lexicon The mixed seed lexicon we use is taken from the Heymans Institute of Pharmacology&#8217;s Multilingual glossary of technical and popular medical terms.</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>6 Random general language entries from the FreeLang 7 project are also incorporated into the lexicon for some of our experiments.</text>
                  <doc_id>122</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Reference translations The test set is composed of 5,000 nominal single and multi-word pairs of French and English terms from the MeSH (Medical Subject Heading) thesaurus.</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>8</text>
                  <doc_id>124</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Evaluation metrics</title>
            <text>The performance of each set of parameters in the experiments is evaluated with Top N precision (P N ), recall (R N ) and F-measure (F N ), as well as Mean Average Precision (MAP). Precision is
6 http://users.ugent.be/&#732;rvdstich/
eugloss/welcome.html 7 http://www.freelang.net/ 8 http://www.nlm.nih.gov/mesh/
the number of correct translations (at most 1 per source term) divided by the number of terms for which our system gave at least one answer; recall is equal to the ratio of correct translations to the total number of terms. F-measure is the harmonic mean of precision and recall:
F-measure = 2 &#215; (precision &#215; recall)
(precision + recall)
The MAP represents in a single figure the quality of a system according to various recall levels (Manning et al., 2008, p. 147&#8211;148):
MAP(Q) = 1 j=1 &#8721; |Q|
|Q|
1 m j
k=1 &#8721;
m j
P recision(R jk )
where |Q| is the number of terms to be translated, m j is the number of reference translations for the j th term (always 1 in our case), and P recision(R jk ) is 0 if the reference translation is not found for the j th term or 1/r if it is (r is the rank of the reference translation in the translation candidates).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The performance of each set of parameters in the experiments is evaluated with Top N precision (P N ), recall (R N ) and F-measure (F N ), as well as Mean Average Precision (MAP).</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Precision is</text>
                  <doc_id>126</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>6 http://users.ugent.be/&#732;rvdstich/</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>eugloss/welcome.html 7 http://www.freelang.net/ 8 http://www.nlm.nih.gov/mesh/</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the number of correct translations (at most 1 per source term) divided by the number of terms for which our system gave at least one answer; recall is equal to the ratio of correct translations to the total number of terms.</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>F-measure is the harmonic mean of precision and recall:</text>
                  <doc_id>130</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>F-measure = 2 &#215; (precision &#215; recall)</text>
                  <doc_id>131</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(precision + recall)</text>
                  <doc_id>132</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The MAP represents in a single figure the quality of a system according to various recall levels (Manning et al., 2008, p.</text>
                  <doc_id>133</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>147&#8211;148):</text>
                  <doc_id>134</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>MAP(Q) = 1 j=1 &#8721; |Q|</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>|Q|</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 m j</text>
                  <doc_id>137</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>k=1 &#8721;</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>m j</text>
                  <doc_id>139</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P recision(R jk )</text>
                  <doc_id>140</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where |Q| is the number of terms to be translated, m j is the number of reference translations for the j th term (always 1 in our case), and P recision(R jk ) is 0 if the reference translation is not found for the j th term or 1/r if it is (r is the rank of the reference translation in the translation candidates).</text>
                  <doc_id>141</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Experiments</title>
        <text>In Experiment 1, 500 single and multi-word terms must be translated from French to English using each of the 64 possible configurations of these parameters: context definition, association measure and similarity measure. In Experiment 2, we submit to the 8 best variants 1,500 new terms to determine with greater confidence the best 2, which are again tested on the last 3,000 of the test terms (Experiment 3). In Experiment 4, using 1,350 frequent terms, we examine the effects of seed lexicon size and specificity and we apply a heuristic based on cognates.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In Experiment 1, 500 single and multi-word terms must be translated from French to English using each of the 64 possible configurations of these parameters: context definition, association measure and similarity measure.</text>
              <doc_id>142</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In Experiment 2, we submit to the 8 best variants 1,500 new terms to determine with greater confidence the best 2, which are again tested on the last 3,000 of the test terms (Experiment 3).</text>
              <doc_id>143</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In Experiment 4, using 1,350 frequent terms, we examine the effects of seed lexicon size and specificity and we apply a heuristic based on cognates.</text>
              <doc_id>144</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Experiment 1</title>
            <text>The results of the first series of experiments on 500 terms can be analysed from the point of view of each of the parameters whose values varied among 64 configurations (Section 2.2). The maximal MAP reached for each parametric value is given in Table 2.
The most notable result is that, of the four association measures studied, the log-odds ratio is
Param. Value Best MAP In config.
association similarity context
significantly superior to the others in every variant. There is as much as 34 % difference between LO and other measures for Top 1 recall. This is interesting since most previous works use the log-likelihood, and none use LO. Our best results for LO (with cosine sentence) and LL (with Dice sentence) are in Table 3. Note that the oracle recall is 93 % (7 % of the source and target terms were not in the corpus).
Assoc. R1 R20 P1 P20 F1 F20 MAP
Another relevant observation is that the parameters interact with each other. When the similarity measure is cosine, PMI results in higher Top 1 F-scores than LL, but the Top 20 F-scores are better with LL. PMI is better than LL when using city-block as a similarity measure, but LL is better than PMI when using Dice and Jaccard indexes. &#967; 2 gives off the worst MAP in all but 4 of the 64 parametric configurations. As for similarity measures, the Dice and Jaccard indexes have identical performances, in accordance with the fact that they are equivalent (Otero, 2008). 9 Influences among parameters are also observable in the performance of similarity measures. When the association measure is LO, the cosine measure gives slightly better Top 1 F-
9 For this reason, whenever &#8220;Dice&#8221; is mentioned from this
point on, it also applies to the Jaccard index.
scores, while the Dice index performs slightly better with regards to Top 20 F-scores. Dice is better when the association measure is LL, with a Top 1 F-score gain of about 15 % compared to the cosine. Again, in the case of context definitions, relative performances depend on the other parameters and on the number of top translation candidates considered. With LO, sentence contexts have the highest Top 1 F-measures, while Top 20 F-measures are highest with paragraphs, and 5- word contexts are the worst.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The results of the first series of experiments on 500 terms can be analysed from the point of view of each of the parameters whose values varied among 64 configurations (Section 2.2).</text>
                  <doc_id>145</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The maximal MAP reached for each parametric value is given in Table 2.</text>
                  <doc_id>146</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The most notable result is that, of the four association measures studied, the log-odds ratio is</text>
                  <doc_id>147</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Param.</text>
                  <doc_id>148</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Value Best MAP In config.</text>
                  <doc_id>149</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>association similarity context</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>significantly superior to the others in every variant.</text>
                  <doc_id>151</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>There is as much as 34 % difference between LO and other measures for Top 1 recall.</text>
                  <doc_id>152</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This is interesting since most previous works use the log-likelihood, and none use LO.</text>
                  <doc_id>153</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Our best results for LO (with cosine sentence) and LL (with Dice sentence) are in Table 3.</text>
                  <doc_id>154</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Note that the oracle recall is 93 % (7 % of the source and target terms were not in the corpus).</text>
                  <doc_id>155</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Assoc.</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>R1 R20 P1 P20 F1 F20 MAP</text>
                  <doc_id>157</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Another relevant observation is that the parameters interact with each other.</text>
                  <doc_id>158</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>When the similarity measure is cosine, PMI results in higher Top 1 F-scores than LL, but the Top 20 F-scores are better with LL.</text>
                  <doc_id>159</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>PMI is better than LL when using city-block as a similarity measure, but LL is better than PMI when using Dice and Jaccard indexes.</text>
                  <doc_id>160</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>&#967; 2 gives off the worst MAP in all but 4 of the 64 parametric configurations.</text>
                  <doc_id>161</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>As for similarity measures, the Dice and Jaccard indexes have identical performances, in accordance with the fact that they are equivalent (Otero, 2008).</text>
                  <doc_id>162</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>9 Influences among parameters are also observable in the performance of similarity measures.</text>
                  <doc_id>163</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>When the association measure is LO, the cosine measure gives slightly better Top 1 F-</text>
                  <doc_id>164</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>9 For this reason, whenever &#8220;Dice&#8221; is mentioned from this</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>point on, it also applies to the Jaccard index.</text>
                  <doc_id>166</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>scores, while the Dice index performs slightly better with regards to Top 20 F-scores.</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Dice is better when the association measure is LL, with a Top 1 F-score gain of about 15 % compared to the cosine.</text>
                  <doc_id>168</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Again, in the case of context definitions, relative performances depend on the other parameters and on the number of top translation candidates considered.</text>
                  <doc_id>169</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>With LO, sentence contexts have the highest Top 1 F-measures, while Top 20 F-measures are highest with paragraphs, and 5- word contexts are the worst.</text>
                  <doc_id>170</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Experiment 2</title>
            <text>The best parametric values found in Experiment 1 were put to the test on 1,500 different test terms for scale-up verification. Along with LO, which was the best association measure in the previous experiment, we used LL to double-check its relative inefficiency. For all of the 8 configurations evaluated, LL&#8217;s recall, precision and MAP remain worse than LO&#8217;s. In particular, LO&#8217;s MAP scores with the cosine measure are more than twice as high as LL&#8217;s (respectively 0.33 and 0.124 for sentence contexts). As in Experiment 1, the Dice index is significantly better for LL compared to the cosine, but not for LO. In the case of LO, sentence contexts have better Top 1 performances than paragraphs, and vice versa for Top 20 performances (see Table 4; oracle recall is 93.5 %). Hence, paragraph contexts would be more useful in tasks consisting of proposing candidate translations to lexicographers, while sentences would be more appropriate for automatic bilingual lexicon construction.
Ctx R1 R20 P1 P20 F1 F20 MAP
The cosine and Dice similarity measures have similar performances when LO is used. Moreover, we observe the effect of source and target term frequencies in corpus. As seen in Table 1, these frequencies are on average about half smaller in Experiment 2 as they are in Experiment 1, which results in significantly lower performances for all
8 variants. As Figure 5 shows for the variant LO cosine sentence, terms that are more frequent have a greater chance of being correctly translated at better ranks.
However, the relative performance of the different parametric configurations still holds.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The best parametric values found in Experiment 1 were put to the test on 1,500 different test terms for scale-up verification.</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Along with LO, which was the best association measure in the previous experiment, we used LL to double-check its relative inefficiency.</text>
                  <doc_id>172</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For all of the 8 configurations evaluated, LL&#8217;s recall, precision and MAP remain worse than LO&#8217;s.</text>
                  <doc_id>173</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In particular, LO&#8217;s MAP scores with the cosine measure are more than twice as high as LL&#8217;s (respectively 0.33 and 0.124 for sentence contexts).</text>
                  <doc_id>174</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>As in Experiment 1, the Dice index is significantly better for LL compared to the cosine, but not for LO.</text>
                  <doc_id>175</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In the case of LO, sentence contexts have better Top 1 performances than paragraphs, and vice versa for Top 20 performances (see Table 4; oracle recall is 93.5 %).</text>
                  <doc_id>176</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Hence, paragraph contexts would be more useful in tasks consisting of proposing candidate translations to lexicographers, while sentences would be more appropriate for automatic bilingual lexicon construction.</text>
                  <doc_id>177</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Ctx R1 R20 P1 P20 F1 F20 MAP</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The cosine and Dice similarity measures have similar performances when LO is used.</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Moreover, we observe the effect of source and target term frequencies in corpus.</text>
                  <doc_id>180</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>As seen in Table 1, these frequencies are on average about half smaller in Experiment 2 as they are in Experiment 1, which results in significantly lower performances for all</text>
                  <doc_id>181</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>8 variants.</text>
                  <doc_id>182</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As Figure 5 shows for the variant LO cosine sentence, terms that are more frequent have a greater chance of being correctly translated at better ranks.</text>
                  <doc_id>183</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>However, the relative performance of the different parametric configurations still holds.</text>
                  <doc_id>184</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>4.3 Experiment 3</title>
            <text>In Experiment 3, we evaluate the two best configurations from Experiment 2 with 3,000 new terms in order to verify the relative performance of the cosine and Dice similarity measures. As Table 5 shows, cosine has slightly better Top 1 figures, while Dice is a little better when considering the Top 20 translation candidates. Therefore, as previously mentioned, the choice of similarity measure (cosine or Dice) should depend on the goal of translation spotting. Note that the scores in Experiment 3 are much lower than those of Experiments 1 and 2 because of low term frequencies in the corpus (see Table 1 and Figure 5). Also, oracle recall is only 71.1 %.
Sim. R1 R20 P1 P20 F1 F20 MAP</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In Experiment 3, we evaluate the two best configurations from Experiment 2 with 3,000 new terms in order to verify the relative performance of the cosine and Dice similarity measures.</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>As Table 5 shows, cosine has slightly better Top 1 figures, while Dice is a little better when considering the Top 20 translation candidates.</text>
                  <doc_id>186</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, as previously mentioned, the choice of similarity measure (cosine or Dice) should depend on the goal of translation spotting.</text>
                  <doc_id>187</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Note that the scores in Experiment 3 are much lower than those of Experiments 1 and 2 because of low term frequencies in the corpus (see Table 1 and Figure 5).</text>
                  <doc_id>188</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Also, oracle recall is only 71.1 %.</text>
                  <doc_id>189</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Sim.</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>R1 R20 P1 P20 F1 F20 MAP</text>
                  <doc_id>191</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>4.4 Experiment 4</title>
            <text>In the last series of experiments, we examine the influence of the bilingual seed lexicon specificity and size, using the 1,350 terms which have source and target frequencies &#8805; 30 from the 1,500 and 3,000 sets used in Experiments 2 and 3 (oracle recall: 100 %). We tested the different lexicons (see Section 2.2) on the 4 parametric configurations made of sentence contexts, LO or LL association measures, and cosine or Dice similarity measures.
Yet again, LO is better than LL. MAP scores for LO in all variants are comprised in [0.466&#8211;0.489]; LL MAPs vary between 0.135 and 0.146 when the cosine is used and between 0.348 and 0.380 when the Dice index is used. According to our results, translation spotting is more accurate when the seed lexicon contains (5,000) entries from both the medical domain and general language instead of general language words only, but only by a very small margin. Table 6 shows the results for the configuration LO cosine sentence. The fact that the difference
Lex. R1 R20 P1 P20 F1 F20 MAP
Gen. + med. 39.3 87.0 39.6 87.6 39.4 87.3 0.473 Gen. only 38.8 88.1 39.0 88.5 38.9 88.3 0.471
is so small could be explained by our resources&#8217; properties. The reference translations from MeSH contain terms that are also used in other domains or in the general language, e.g. terms from the category &#8220;people&#8221; (N&#233;v&#233;ol and Ozdowska, 2006). Wikipedia documents retrieved by using those references may in turn not belong to the medical domain, in which case medical terms from the seed lexicon are not appropriate. Still, the relatively good performance of the general language-only lexicon supports (D&#233;jean et al., 2005, p. 119)&#8217;s claim that general language words are useful when spotting translations of domain specific terms, since the latter can appear in generic contexts.
Lexicon sizes tested are 5,000 (the mixed lexicon used in previous experiments), 7,000, 9,000 and 11,000 entries. The performance (based on MAP) is better when 7,000- and 9,000-entry lexicons are used, because more source language context words can be taken into account. However, when the lexicon reaches 11,000, Top 1 MAP scores and F-measures are slightly lower than those obtained with the 7,000-entry one. This may happen because the lexicon is increased with general language words; 9,000 of the 11,000 entries
are not from the medical domain, making it harder for the context words to be specific. It would be interesting to study the specificity of context vectors built from the source corpus. Still, the differences in scores are small, as Table 7 shows (see Table 6 for the results obtained with 5,000 entries). This is because, in our implementation, context vector size is limited to 20, as in (Daille and Morin, 2005), in order to reduce processing time. The influence of context vector sizes should be studied.
Lex. size R1 R20 P1 P20 F1 F20 MAP
The parameters related to the seed lexicon do not have as great an impact on the performance as the choice of association measure does: the biggest difference in F-measures for Experiment 4 is 2.9 %. At this point, linguistic-based heuristics such as graphic similarity should be used to significantly increase performance. We applied the cognate heuristic (Section 2.3) on the Top 20 translation candidates given by the variant LO sentence 9,000-entry lexicon using cosine and Dice similarity measures. Without the heuristic, Top 1 performances are better with cosine, while Dice is better for Top 20. Applying the cognate heuristic makes the Top 1 precision go from 41.1 % to 55.2 % in the case of cosine, and from 39.6 % to 53.9 % in the case of Dice.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In the last series of experiments, we examine the influence of the bilingual seed lexicon specificity and size, using the 1,350 terms which have source and target frequencies &#8805; 30 from the 1,500 and 3,000 sets used in Experiments 2 and 3 (oracle recall: 100 %).</text>
                  <doc_id>192</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We tested the different lexicons (see Section 2.2) on the 4 parametric configurations made of sentence contexts, LO or LL association measures, and cosine or Dice similarity measures.</text>
                  <doc_id>193</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Yet again, LO is better than LL.</text>
                  <doc_id>194</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>MAP scores for LO in all variants are comprised in [0.466&#8211;0.489]; LL MAPs vary between 0.135 and 0.146 when the cosine is used and between 0.348 and 0.380 when the Dice index is used.</text>
                  <doc_id>195</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>According to our results, translation spotting is more accurate when the seed lexicon contains (5,000) entries from both the medical domain and general language instead of general language words only, but only by a very small margin.</text>
                  <doc_id>196</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Table 6 shows the results for the configuration LO cosine sentence.</text>
                  <doc_id>197</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The fact that the difference</text>
                  <doc_id>198</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Lex.</text>
                  <doc_id>199</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>R1 R20 P1 P20 F1 F20 MAP</text>
                  <doc_id>200</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Gen.</text>
                  <doc_id>201</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>+ med.</text>
                  <doc_id>202</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>39.3 87.0 39.6 87.6 39.4 87.3 0.473 Gen.</text>
                  <doc_id>203</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>only 38.8 88.1 39.0 88.5 38.9 88.3 0.471</text>
                  <doc_id>204</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>is so small could be explained by our resources&#8217; properties.</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The reference translations from MeSH contain terms that are also used in other domains or in the general language, e.g. terms from the category &#8220;people&#8221; (N&#233;v&#233;ol and Ozdowska, 2006).</text>
                  <doc_id>206</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Wikipedia documents retrieved by using those references may in turn not belong to the medical domain, in which case medical terms from the seed lexicon are not appropriate.</text>
                  <doc_id>207</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Still, the relatively good performance of the general language-only lexicon supports (D&#233;jean et al., 2005, p.</text>
                  <doc_id>208</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>119)&#8217;s claim that general language words are useful when spotting translations of domain specific terms, since the latter can appear in generic contexts.</text>
                  <doc_id>209</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Lexicon sizes tested are 5,000 (the mixed lexicon used in previous experiments), 7,000, 9,000 and 11,000 entries.</text>
                  <doc_id>210</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The performance (based on MAP) is better when 7,000- and 9,000-entry lexicons are used, because more source language context words can be taken into account.</text>
                  <doc_id>211</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, when the lexicon reaches 11,000, Top 1 MAP scores and F-measures are slightly lower than those obtained with the 7,000-entry one.</text>
                  <doc_id>212</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This may happen because the lexicon is increased with general language words; 9,000 of the 11,000 entries</text>
                  <doc_id>213</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>are not from the medical domain, making it harder for the context words to be specific.</text>
                  <doc_id>214</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It would be interesting to study the specificity of context vectors built from the source corpus.</text>
                  <doc_id>215</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Still, the differences in scores are small, as Table 7 shows (see Table 6 for the results obtained with 5,000 entries).</text>
                  <doc_id>216</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This is because, in our implementation, context vector size is limited to 20, as in (Daille and Morin, 2005), in order to reduce processing time.</text>
                  <doc_id>217</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The influence of context vector sizes should be studied.</text>
                  <doc_id>218</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Lex.</text>
                  <doc_id>219</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>size R1 R20 P1 P20 F1 F20 MAP</text>
                  <doc_id>220</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The parameters related to the seed lexicon do not have as great an impact on the performance as the choice of association measure does: the biggest difference in F-measures for Experiment 4 is 2.9 %.</text>
                  <doc_id>221</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>At this point, linguistic-based heuristics such as graphic similarity should be used to significantly increase performance.</text>
                  <doc_id>222</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We applied the cognate heuristic (Section 2.3) on the Top 20 translation candidates given by the variant LO sentence 9,000-entry lexicon using cosine and Dice similarity measures.</text>
                  <doc_id>223</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Without the heuristic, Top 1 performances are better with cosine, while Dice is better for Top 20.</text>
                  <doc_id>224</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Applying the cognate heuristic makes the Top 1 precision go from 41.1 % to 55.2 % in the case of cosine, and from 39.6 % to 53.9 % in the case of Dice.</text>
                  <doc_id>225</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Discussion</title>
        <text>Our results show that using the log-odds ratio as the association measure allows for significantly better translation spotting than the log-likelihood. A closer look at the translation candidates obtained when using LL, the most popular association measure in projection-based approaches, shows that they are often collocates of the reference translation. Therefore, LL may fare better in an indirect approach, like the one in (Daille and Morin, 2005). Moreover, we have seen that the cosine similarity measure and sentence contexts give more correct top translation candidates, at least when LO is used. Indeed, the values of the different parameters influence one another in most cases. Parameters related to the seed lexicon (size, domain specificity) are not of great influence on the performance, but this may in part be due to our resources and the way they were built. The highest Top 1 precision, 55.2 %, was reached with the following parameters: sentence contexts, LO, cosine and a 9,000-entry mixed lexicon, with the use of a cognate heuristic. In future works, other parameters which influence the performance will be studied, among which the use of a terminological extractor to treat complex terms (Daille and Morin, 2005), more contextual window configurations, and the use of syntactic information in combination with lexical information (Yu and Tsujii, 2009). It would also be interesting to compare the projection-based approaches to (Haghighi et al., 2008)&#8217;s generative model for bilingual lexicon acquisition from monolingual corpora. One latent outcome of this work is that Wikipedia is surprisingly suitable for mining medical terms. We plan to check its adequacy for other domains and verify that LO remains a better association measure for different corpora and domains.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Our results show that using the log-odds ratio as the association measure allows for significantly better translation spotting than the log-likelihood.</text>
              <doc_id>226</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A closer look at the translation candidates obtained when using LL, the most popular association measure in projection-based approaches, shows that they are often collocates of the reference translation.</text>
              <doc_id>227</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, LL may fare better in an indirect approach, like the one in (Daille and Morin, 2005).</text>
              <doc_id>228</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, we have seen that the cosine similarity measure and sentence contexts give more correct top translation candidates, at least when LO is used.</text>
              <doc_id>229</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Indeed, the values of the different parameters influence one another in most cases.</text>
              <doc_id>230</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Parameters related to the seed lexicon (size, domain specificity) are not of great influence on the performance, but this may in part be due to our resources and the way they were built.</text>
              <doc_id>231</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The highest Top 1 precision, 55.2 %, was reached with the following parameters: sentence contexts, LO, cosine and a 9,000-entry mixed lexicon, with the use of a cognate heuristic.</text>
              <doc_id>232</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In future works, other parameters which influence the performance will be studied, among which the use of a terminological extractor to treat complex terms (Daille and Morin, 2005), more contextual window configurations, and the use of syntactic information in combination with lexical information (Yu and Tsujii, 2009).</text>
              <doc_id>233</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>It would also be interesting to compare the projection-based approaches to (Haghighi et al., 2008)&#8217;s generative model for bilingual lexicon acquisition from monolingual corpora.</text>
              <doc_id>234</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>One latent outcome of this work is that Wikipedia is surprisingly suitable for mining medical terms.</text>
              <doc_id>235</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>We plan to check its adequacy for other domains and verify that LO remains a better association measure for different corpora and domains.</text>
              <doc_id>236</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>Acknowledgments</title>
        <text>We are deeply grateful to Rapha&#235;l Rubino who provided us with the data material we have been using in this study. We thank the anonymous reviewers for their suggestions.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We are deeply grateful to Rapha&#235;l Rubino who provided us with the data material we have been using in this study.</text>
              <doc_id>237</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We thank the anonymous reviewers for their suggestions.</text>
              <doc_id>238</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: 50-document corpora averages</caption>
        <reference_text></reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>Tokens s</cell>
              <cell>89,431</cell>
              <cell>73,809</cell>
              <cell>42,762</cell>
              <cell>90,328</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Tokens t</cell>
              <cell>52,002</cell>
              <cell>27,517</cell>
              <cell>12,891</cell>
              <cell>38,929</cell>
            </row>
            <row>
              <cell>|S|</cell>
              <cell>296</cell>
              <cell>184</cell>
              <cell>66</cell>
              <cell>306</cell>
            </row>
            <row>
              <cell>|T |</cell>
              <cell>542</cell>
              <cell>255</cell>
              <cell>104</cell>
              <cell>404</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Best MAP in Experiment 1</caption>
        <reference_text>None</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Param. Value</cell>
              <cell>Best MAP</cell>
              <cell>In config.</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>LO</cell>
              <cell>0.536</cell>
              <cell>sentence cosine</cell>
            </row>
            <row>
              <cell>LL</cell>
              <cell>0.413</cell>
              <cell>sentence Dice</cell>
            </row>
            <row>
              <cell>PMI ?2</cell>
              <cell>0.299</cell>
              <cell>sentence city-block</cell>
            </row>
            <row>
              <cell>association</cell>
              <cell>0.179</cell>
              <cell>sentence Dice</cell>
            </row>
            <row>
              <cell>cosine</cell>
              <cell>0.536</cell>
              <cell>sentence LO</cell>
            </row>
            <row>
              <cell>Dice</cell>
              <cell>0.520</cell>
              <cell>sentence LO</cell>
            </row>
            <row>
              <cell>Jaccard</cell>
              <cell>0.520</cell>
              <cell>sentence LO</cell>
            </row>
            <row>
              <cell>similarity city-block</cell>
              <cell>0.415</cell>
              <cell>sentence LO</cell>
            </row>
            <row>
              <cell>sentence xt</cell>
              <cell>0.536</cell>
              <cell>cosine LO</cell>
            </row>
            <row>
              <cell>paragraph</cell>
              <cell>0.460</cell>
              <cell>cosine LO</cell>
            </row>
            <row>
              <cell>25 words conte</cell>
              <cell>0.454</cell>
              <cell>cosine LO</cell>
            </row>
            <row>
              <cell>5 words</cell>
              <cell>0.361</cell>
              <cell>Dice LO</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Best LO and LL configurations scores</caption>
        <reference_text>In PAGE 6: ... This is interesting since most previous works use the log-likelihood, and none use LO. Our best re- sults for LO (with cosine sentence) and LL (with Dice sentence) are in  Table3 . Note that the oracle recall is 93 % (7 % of the source and target terms...</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>were not in the corpus).</cell>
              <cell>were not in the corpus).</cell>
              <cell>were not in the corpus).</cell>
              <cell>were not in the corpus).</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Assoc.</cell>
              <cell>R1</cell>
              <cell>R20</cell>
              <cell>P1</cell>
              <cell>P20</cell>
              <cell>F1</cell>
              <cell>F20</cell>
              <cell>MAP</cell>
            </row>
            <row>
              <cell>LO</cell>
              <cell>39.4</cell>
              <cell>84.8</cell>
              <cell>42.3</cell>
              <cell>91.0</cell>
              <cell>40.8</cell>
              <cell>87.8</cell>
              <cell>0.536</cell>
            </row>
            <row>
              <cell>LL</cell>
              <cell>29.0</cell>
              <cell>75.2</cell>
              <cell>31.3</cell>
              <cell>81.0</cell>
              <cell>30.1</cell>
              <cell>78.0</cell>
              <cell>0.413</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: LO Dice configuration scores</caption>
        <reference_text>In PAGE 6: ... As in Experiment 1, the Dice index is significantly better for LL compared to the cosine, but not for LO. In the case of LO, sentence contexts have better Top 1 performances than paragraphs, and vice versa for Top 20 per- formances (see  Table4 ; oracle recall is 93.5 %)....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>construction.</cell>
              <cell>construction.</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Ctx</cell>
              <cell>R1</cell>
              <cell>R20</cell>
              <cell>P1</cell>
              <cell>P20</cell>
              <cell>F1</cell>
              <cell>F20</cell>
              <cell>MAP</cell>
            </row>
            <row>
              <cell>Sent.</cell>
              <cell>23.1</cell>
              <cell>63.9</cell>
              <cell>27.8</cell>
              <cell>76.6</cell>
              <cell>25.23</cell>
              <cell>69.68</cell>
              <cell>0.336</cell>
            </row>
            <row>
              <cell>Parag.</cell>
              <cell>20.1</cell>
              <cell>70.0</cell>
              <cell>22.9</cell>
              <cell>79.7</cell>
              <cell>21.41</cell>
              <cell>74.54</cell>
              <cell>0.325</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TET</source>
        <caption>Table 5: LO sentence configuration scores</caption>
        <reference_text></reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Cosine</cell>
              <cell>9.8</cell>
              <cell>28.1</cell>
              <cell>20.7</cell>
              <cell>59.4</cell>
              <cell>13.3</cell>
              <cell>38.15</cell>
              <cell>0.232</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Dice</cell>
              <cell>9.4</cell>
              <cell>28.9</cell>
              <cell>19.8</cell>
              <cell>61.2</cell>
              <cell>12.75</cell>
              <cell>39.26</cell>
              <cell>0.286</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>7</id>
        <source>TableSeer</source>
        <caption>Table 7: LO cosine sentence configuration scores</caption>
        <reference_text>In PAGE 8: ... It would be interesting to study the specificity of context vec- tors built from the source corpus. Still, the dif- ferences in scores are small, as  Table7  shows (see Table 6 for the results obtained with 5,000 entries). This is because, in our implementation, context vector size is limited to 20, as in (Daille and Morin, 2005), in order to reduce processing time....</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>be studied.</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Lex. size</cell>
              <cell>R1</cell>
              <cell>R20</cell>
              <cell>P1</cell>
              <cell>P20</cell>
              <cell>F1</cell>
              <cell>F20</cell>
              <cell>MAP</cell>
            </row>
            <row>
              <cell>7,000</cell>
              <cell>41.5</cell>
              <cell>88.8</cell>
              <cell>41.6</cell>
              <cell>89.1</cell>
              <cell>41.5</cell>
              <cell>88.9</cell>
              <cell>0.488</cell>
            </row>
            <row>
              <cell>9,000</cell>
              <cell>40.9</cell>
              <cell>89.3</cell>
              <cell>41.1</cell>
              <cell>89.7</cell>
              <cell>41.0</cell>
              <cell>89.5</cell>
              <cell>0.489</cell>
            </row>
            <row>
              <cell>11,000</cell>
              <cell>40.1</cell>
              <cell>89.8</cell>
              <cell>40.2</cell>
              <cell>90.1</cell>
              <cell>40.1</cell>
              <cell>89.9</cell>
              <cell>0.484</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Yun-Chuang Chiao</author>
          <author>Pierre Zweigenbaum</author>
        </authors>
        <title>Looking for candidate translational equivalents in specialized, comparable corpora.</title>
        <publication>In 19 th International Conference on Computational Linguistics,</publication>
        <pages>1208--1212</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>B&#233;atrice Daille</author>
          <author>Emmanuel Morin</author>
        </authors>
        <title>FrenchEnglish terminology extraction from comparable corpora.</title>
        <publication>In 2 nd International Joint Conference on Natural Language Processing,</publication>
        <pages>707--718</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Herv&#233; D&#233;jean</author>
          <author>&#201;ric Gaussier</author>
          <author>Jean-Michel Renders</author>
          <author>Fatiha Sadat</author>
        </authors>
        <title>Automatic processing of 624 medical terminology: Applications to thesaurus enrichment and cross-language information retrieval.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Mona Diab</author>
          <author>Steve Finch</author>
        </authors>
        <title>A statistical wordlevel translation model for comparable corpora.</title>
        <publication>In Proceedings of the Conference on Content-Based Multimedia Information Access.</publication>
        <pages>None</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Ted Dunning</author>
        </authors>
        <title>Accurate methods for the statistics of surprise and coincidence.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1993</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Stefan Evert</author>
        </authors>
        <title>The Statistics of Word Cooccurrences. Word Pairs and Collocations.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Pascale Fung</author>
        </authors>
        <title>A pattern matching method for finding noun and proper noun translations from noisy parallel corpora.</title>
        <publication>In 33 rd Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>236--243</pages>
        <date>1995</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Pascale Fung</author>
        </authors>
        <title>A statistical view on bilingual lexicon extraction: From parallel corpora to nonparallel corpora.</title>
        <publication>In 3 rd Conference of the Association for Machine Translation in the Americas,</publication>
        <pages>1--17</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Aria Haghighi</author>
          <author>Percy Liang</author>
          <author>Taylor Berg-Kirkpatrick</author>
          <author>Dan Klein</author>
        </authors>
        <title>Learning bilingual lexicons from monolingual corpora.</title>
        <publication>In Human Language Technology and Association for Computational Linguistics,</publication>
        <pages>771--779</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Daniel Jurafsky</author>
          <author>James H Martin</author>
        </authors>
        <title>Speech and Language Processing.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Christopher D Manning</author>
          <author>Prabhakar Raghavan</author>
          <author>Hinrich Sch&#252;tze</author>
        </authors>
        <title>Introduction to Information Retrieval.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Emmanuel Morin</author>
          <author>B&#233;atrice Daille</author>
          <author>Koichi Takeuchi</author>
          <author>Kyo Kageura</author>
        </authors>
        <title>Bilingual terminology mining &#8212; using brain, not brawn comparable corpora.</title>
        <publication>In 45 th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>664--671</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Emmanuel Prochasson</author>
          <author>Emmanuel Morin</author>
          <author>Kyo Kageura</author>
        </authors>
        <title>Anchor points for bilingual lexicon extraction from small comparable corpora.</title>
        <publication>In Machine Translation Summit XII,</publication>
        <pages>284--291</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>Reinhard Rapp</author>
        </authors>
        <title>Automatic identification of word translations from unrelated English and German corpora.</title>
        <publication>In 37 th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>66--70</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>Rapha&#235;l Rubino</author>
        </authors>
        <title>Exploring context variation and lexicon coverage in projection-based approach for term translation.</title>
        <publication>In Proceedings of the Student Research Workshop associated with RANLP&#8211; 09,</publication>
        <pages>66--70</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>X Saralegi</author>
          <author>I San Vicente</author>
          <author>A Gurrutxaga</author>
        </authors>
        <title>Automatic extraction of bilingual terms from comparable corpora in a popular science domain.</title>
        <publication>In 1 st Workshop Building and Using Comparable Corpora.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Li Shao</author>
          <author>Hwee Tou Ng</author>
        </authors>
        <title>Mining new word translations from comparable corpora.</title>
        <publication>In 20 th International Conference on Computational Linguistics,</publication>
        <pages>618--624</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>Michel Simard</author>
          <author>George Foster</author>
          <author>Pierre Isabelle</author>
        </authors>
        <title>Using cognates to align sentences in bilingual corpora.</title>
        <publication>In 4 th Conference on Theoretical and Methodological Issues in Machine Translation,</publication>
        <pages>67--81</pages>
        <date>1992</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Jason R Smith</author>
          <author>Chris Quirk</author>
          <author>Kristina Toutanova</author>
        </authors>
        <title>Extracting parallel sentences from comparable corpora using document level alignment.</title>
        <publication>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL,</publication>
        <pages>403--411</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Kun Yu</author>
          <author>Junichi Tsujii</author>
        </authors>
        <title>Bilingual dictionary extraction from Wikipedia.</title>
        <publication>In Machine Translation Summit XII.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Aur&#233;lie N&#233;v&#233;ol</author>
          <author>Sylwia Ozdowska</author>
        </authors>
        <title>Terminologie m&#233;dicale bilingue anglais/fran&#231;ais: usages cliniques et bilingues.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Pablo Gamallo Otero</author>
        </authors>
        <title>None</title>
        <publication>Learning bilingual lexicons from comparable English and Spanish corpora.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>22</id>
        <authors/>
        <title>None</title>
        <publication>In Machine Translation</publication>
        <pages>191--198</pages>
        <date>2007</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Chiao and Zweigenbaum (2002)</string>
        <sentence_id>888</sentence_id>
        <char_offset>14</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>0</reference_id>
        <string>Chiao and Zweigenbaum, 2002</string>
        <sentence_id>817</sentence_id>
        <char_offset>58</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>0</reference_id>
        <string>Chiao and Zweigenbaum, 2002</string>
        <sentence_id>865</sentence_id>
        <char_offset>158</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>0</reference_id>
        <string>Chiao and Zweigenbaum, 2002</string>
        <sentence_id>867</sentence_id>
        <char_offset>122</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>0</reference_id>
        <string>Chiao and Zweigenbaum, 2002</string>
        <sentence_id>867</sentence_id>
        <char_offset>226</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>0</reference_id>
        <string>Chiao and Zweigenbaum, 2002</string>
        <sentence_id>915</sentence_id>
        <char_offset>151</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>1</reference_id>
        <string>Daille and Morin, 2005</string>
        <sentence_id>820</sentence_id>
        <char_offset>75</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>1</reference_id>
        <string>Daille and Morin, 2005</string>
        <sentence_id>839</sentence_id>
        <char_offset>60</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>1</reference_id>
        <string>Daille and Morin, 2005</string>
        <sentence_id>845</sentence_id>
        <char_offset>76</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>1</reference_id>
        <string>Daille and Morin, 2005</string>
        <sentence_id>854</sentence_id>
        <char_offset>279</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>1</reference_id>
        <string>Daille and Morin, 2005</string>
        <sentence_id>860</sentence_id>
        <char_offset>176</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>1</reference_id>
        <string>Daille and Morin, 2005</string>
        <sentence_id>865</sentence_id>
        <char_offset>187</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>1</reference_id>
        <string>Daille and Morin, 2005</string>
        <sentence_id>867</sentence_id>
        <char_offset>151</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>1</reference_id>
        <string>Daille and Morin, 2005</string>
        <sentence_id>867</sentence_id>
        <char_offset>255</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>1</reference_id>
        <string>Daille and Morin, 2005</string>
        <sentence_id>1012</sentence_id>
        <char_offset>85</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>1</reference_id>
        <string>Daille and Morin, 2005</string>
        <sentence_id>1026</sentence_id>
        <char_offset>72</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>1</reference_id>
        <string>Daille and Morin, 2005</string>
        <sentence_id>1031</sentence_id>
        <char_offset>157</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>2</reference_id>
        <string>D&#233;jean et al. (2005)</string>
        <sentence_id>807</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>2</reference_id>
        <string>D&#233;jean et al., 2005</string>
        <sentence_id>817</sentence_id>
        <char_offset>87</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>2</reference_id>
        <string>D&#233;jean et al., 2005</string>
        <sentence_id>865</sentence_id>
        <char_offset>80</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>2</reference_id>
        <string>D&#233;jean et al., 2005</string>
        <sentence_id>915</sentence_id>
        <char_offset>17</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>2</reference_id>
        <string>D&#233;jean et al., 2005</string>
        <sentence_id>1003</sentence_id>
        <char_offset>86</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>3</reference_id>
        <string>Diab and Finch, 2000</string>
        <sentence_id>806</sentence_id>
        <char_offset>117</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>4</reference_id>
        <string>Dunning, 1993</string>
        <sentence_id>813</sentence_id>
        <char_offset>196</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>5</reference_id>
        <string>Evert, 2005</string>
        <sentence_id>874</sentence_id>
        <char_offset>63</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>6</reference_id>
        <string>Fung, 1995</string>
        <sentence_id>806</sentence_id>
        <char_offset>105</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>7</reference_id>
        <string>Fung, 1998</string>
        <sentence_id>798</sentence_id>
        <char_offset>153</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>7</reference_id>
        <string>Fung, 1998</string>
        <sentence_id>804</sentence_id>
        <char_offset>195</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>7</reference_id>
        <string>Fung, 1998</string>
        <sentence_id>865</sentence_id>
        <char_offset>146</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>7</reference_id>
        <string>Fung, 1998</string>
        <sentence_id>867</sentence_id>
        <char_offset>110</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>8</reference_id>
        <string>Haghighi et al., 2008</string>
        <sentence_id>894</sentence_id>
        <char_offset>226</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>8</reference_id>
        <string>Haghighi et al., 2008</string>
        <sentence_id>1032</sentence_id>
        <char_offset>76</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>9</reference_id>
        <string>Jurafsky and Martin, 2008</string>
        <sentence_id>881</sentence_id>
        <char_offset>106</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>10</reference_id>
        <string>Manning et al., 2008</string>
        <sentence_id>928</sentence_id>
        <char_offset>98</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>11</reference_id>
        <string>Morin et al. (2007)</string>
        <sentence_id>822</sentence_id>
        <char_offset>14</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>11</reference_id>
        <string>Morin et al. (2007)</string>
        <sentence_id>828</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>12</reference_id>
        <string>Prochasson et al., 2009</string>
        <sentence_id>845</sentence_id>
        <char_offset>140</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>12</reference_id>
        <string>Prochasson et al., 2009</string>
        <sentence_id>867</sentence_id>
        <char_offset>175</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>12</reference_id>
        <string>Prochasson et al. (2009)</string>
        <sentence_id>889</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>13</reference_id>
        <string>Rapp, 1999</string>
        <sentence_id>798</sentence_id>
        <char_offset>165</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>13</reference_id>
        <string>Rapp, 1999</string>
        <sentence_id>804</sentence_id>
        <char_offset>207</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>13</reference_id>
        <string>Rapp, 1999</string>
        <sentence_id>816</sentence_id>
        <char_offset>87</char_offset>
      </citation>
      <citation>
        <id>42</id>
        <reference_id>13</reference_id>
        <string>Rapp, 1999</string>
        <sentence_id>900</sentence_id>
        <char_offset>99</char_offset>
      </citation>
      <citation>
        <id>43</id>
        <reference_id>13</reference_id>
        <string>Rapp, 1999</string>
        <sentence_id>839</sentence_id>
        <char_offset>131</char_offset>
      </citation>
      <citation>
        <id>44</id>
        <reference_id>13</reference_id>
        <string>Rapp, 1999</string>
        <sentence_id>845</sentence_id>
        <char_offset>115</char_offset>
      </citation>
      <citation>
        <id>45</id>
        <reference_id>13</reference_id>
        <string>Rapp, 1999</string>
        <sentence_id>854</sentence_id>
        <char_offset>119</char_offset>
      </citation>
      <citation>
        <id>46</id>
        <reference_id>13</reference_id>
        <string>Rapp, 1999</string>
        <sentence_id>865</sentence_id>
        <char_offset>56</char_offset>
      </citation>
      <citation>
        <id>47</id>
        <reference_id>13</reference_id>
        <string>Rapp, 1999</string>
        <sentence_id>867</sentence_id>
        <char_offset>89</char_offset>
      </citation>
      <citation>
        <id>48</id>
        <reference_id>14</reference_id>
        <string>Rubino, 2009</string>
        <sentence_id>907</sentence_id>
        <char_offset>36</char_offset>
      </citation>
      <citation>
        <id>49</id>
        <reference_id>15</reference_id>
        <string>Saralegi et al. (2008)</string>
        <sentence_id>822</sentence_id>
        <char_offset>93</char_offset>
      </citation>
      <citation>
        <id>50</id>
        <reference_id>16</reference_id>
        <string>Shao and Ng, 2004</string>
        <sentence_id>893</sentence_id>
        <char_offset>61</char_offset>
      </citation>
      <citation>
        <id>51</id>
        <reference_id>17</reference_id>
        <string>Simard et al., 1992</string>
        <sentence_id>897</sentence_id>
        <char_offset>92</char_offset>
      </citation>
      <citation>
        <id>52</id>
        <reference_id>18</reference_id>
        <string>Smith et al., 2010</string>
        <sentence_id>912</sentence_id>
        <char_offset>77</char_offset>
      </citation>
      <citation>
        <id>53</id>
        <reference_id>19</reference_id>
        <string>Yu and Tsujii, 2009</string>
        <sentence_id>806</sentence_id>
        <char_offset>139</char_offset>
      </citation>
      <citation>
        <id>54</id>
        <reference_id>19</reference_id>
        <string>Yu and Tsujii, 2009</string>
        <sentence_id>816</sentence_id>
        <char_offset>112</char_offset>
      </citation>
      <citation>
        <id>55</id>
        <reference_id>19</reference_id>
        <string>Yu and Tsujii, 2009</string>
        <sentence_id>839</sentence_id>
        <char_offset>94</char_offset>
      </citation>
      <citation>
        <id>56</id>
        <reference_id>19</reference_id>
        <string>Yu and Tsujii, 2009</string>
        <sentence_id>841</sentence_id>
        <char_offset>197</char_offset>
      </citation>
      <citation>
        <id>57</id>
        <reference_id>19</reference_id>
        <string>Yu and Tsujii, 2009</string>
        <sentence_id>1031</sentence_id>
        <char_offset>299</char_offset>
      </citation>
      <citation>
        <id>58</id>
        <reference_id>20</reference_id>
        <string>N&#233;v&#233;ol and Ozdowska, 2006</string>
        <sentence_id>1001</sentence_id>
        <char_offset>154</char_offset>
      </citation>
      <citation>
        <id>59</id>
        <reference_id>21</reference_id>
        <string>Otero (2007)</string>
        <sentence_id>811</sentence_id>
        <char_offset>123</char_offset>
      </citation>
      <citation>
        <id>60</id>
        <reference_id>22</reference_id>
        <string>(2007)</string>
        <sentence_id>811</sentence_id>
        <char_offset>129</char_offset>
      </citation>
      <citation>
        <id>61</id>
        <reference_id>22</reference_id>
        <string>(2007)</string>
        <sentence_id>822</sentence_id>
        <char_offset>27</char_offset>
      </citation>
      <citation>
        <id>62</id>
        <reference_id>22</reference_id>
        <string>(2007)</string>
        <sentence_id>828</sentence_id>
        <char_offset>13</char_offset>
      </citation>
    </citations>
  </content>
</document>
