<document>
  <filename>N13-1074</filename>
  <authors>
    <author>Saab Mansour</author>
  </authors>
  <title>Phrase Training Based Adaptation for Statistical Machine Translation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>We present a novel approach for translation model (TM) adaptation using phrase training. The proposed adaptation procedure is initialized with a standard general-domain TM, which is then used to perform phrase training on a smaller in-domain set. This way, we bias the probabilities of the general TM towards the in-domain distribution. Experimental results on two different lectures translation tasks show significant improvements of the adapted systems over the general ones. Additionally, we compare our results to mixture modeling, where we report gains when using the suggested phrase training adaptation method.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We present a novel approach for translation model (TM) adaptation using phrase training.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The proposed adaptation procedure is initialized with a standard general-domain TM, which is then used to perform phrase training on a smaller in-domain set.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This way, we bias the probabilities of the general TM towards the in-domain distribution.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Experimental results on two different lectures translation tasks show significant improvements of the adapted systems over the general ones.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Additionally, we compare our results to mixture modeling, where we report gains when using the suggested phrase training adaptation method.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>The task of domain-adaptation attempts to exploit data mainly drawn from one domain (e.g. news, parliamentary discussion) to maximize the performance on the test domain (e.g. lectures, web forums). In this work, we focus on translation model (TM) adaptation. A prominent approach in recent work is weighting at different levels of granularity. Foster and Kuhn (2007) perform weighting at the corpus level, where different corpora receive different weights and are then combined using mixture modeling. A finer grained weighting is that of Matsoukas et al. (2009), who weight each sentence in the bitexts using features of meta-information and optimize a mapping from the feature vectors to weights using a translation quality measure.
In this work, we propose to perform TM adaptation using phrase training. We start from a generaldomain phrase table and adapt the probabilities by training on an in-domain data. Thus, we achieve direct phrase probabilities adaptation as opposed to weighting. Foster et al. (2010) perform weighting at the phrase level, assigning each phrase pair a weight according to its relevance to the test domain. They compare phrase weighting to a &#8220;flat&#8221; model, where the weight directly approximates the phrase probability. In their experiments, the weighting method performs better than the flat model, therefore, they conclude that retaining the original relative frequency probabilities of the TM is important for good performance. The &#8220;flat&#8221; model of Foster et al. (2010) is similar to our work. We differ in the following points: (i) we use the same procedure to perform the phrase training based adaptation and the search thus avoiding inconsistencies between the two; (ii) we do not directly interpolate the original statistics with the new ones, but use a training procedure to manipulate the original statistics. We perform experiments on the publicly available IWSLT TED task, on both Arabic-to-English and Germanto-English lectures translation tracks. We compare our suggested phrase training adaptation method to a variety of baselines and show its effectiveness. Finally, we experiment with mixture modeling based adaptation. We compare mixture modeling to our adaptation method, and apply our method within a mixture modeling framework.
In Section 2, we present the phrase training method and explain how it is utilized for adaptation. Experimental setup including corpora statistics and the SMT system are described in Section 3. Section 4 summarizes the phrase training adaptation results ending with a comparison to mixture modeling.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The task of domain-adaptation attempts to exploit data mainly drawn from one domain (e.g. news, parliamentary discussion) to maximize the performance on the test domain (e.g. lectures, web forums).</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this work, we focus on translation model (TM) adaptation.</text>
              <doc_id>6</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A prominent approach in recent work is weighting at different levels of granularity.</text>
              <doc_id>7</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Foster and Kuhn (2007) perform weighting at the corpus level, where different corpora receive different weights and are then combined using mixture modeling.</text>
              <doc_id>8</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>A finer grained weighting is that of Matsoukas et al. (2009), who weight each sentence in the bitexts using features of meta-information and optimize a mapping from the feature vectors to weights using a translation quality measure.</text>
              <doc_id>9</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this work, we propose to perform TM adaptation using phrase training.</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We start from a generaldomain phrase table and adapt the probabilities by training on an in-domain data.</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Thus, we achieve direct phrase probabilities adaptation as opposed to weighting.</text>
              <doc_id>12</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Foster et al. (2010) perform weighting at the phrase level, assigning each phrase pair a weight according to its relevance to the test domain.</text>
              <doc_id>13</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>They compare phrase weighting to a &#8220;flat&#8221; model, where the weight directly approximates the phrase probability.</text>
              <doc_id>14</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In their experiments, the weighting method performs better than the flat model, therefore, they conclude that retaining the original relative frequency probabilities of the TM is important for good performance.</text>
              <doc_id>15</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The &#8220;flat&#8221; model of Foster et al. (2010) is similar to our work.</text>
              <doc_id>16</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>We differ in the following points: (i) we use the same procedure to perform the phrase training based adaptation and the search thus avoiding inconsistencies between the two; (ii) we do not directly interpolate the original statistics with the new ones, but use a training procedure to manipulate the original statistics.</text>
              <doc_id>17</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>We perform experiments on the publicly available IWSLT TED task, on both Arabic-to-English and Germanto-English lectures translation tracks.</text>
              <doc_id>18</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>We compare our suggested phrase training adaptation method to a variety of baselines and show its effectiveness.</text>
              <doc_id>19</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we experiment with mixture modeling based adaptation.</text>
              <doc_id>20</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>We compare mixture modeling to our adaptation method, and apply our method within a mixture modeling framework.</text>
              <doc_id>21</doc_id>
              <sec_id>11</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In Section 2, we present the phrase training method and explain how it is utilized for adaptation.</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Experimental setup including corpora statistics and the SMT system are described in Section 3.</text>
              <doc_id>23</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Section 4 summarizes the phrase training adaptation results ending with a comparison to mixture modeling.</text>
              <doc_id>24</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Phrase Training</title>
        <text>The standard phrase extraction procedure in SMT consists of two phases: (i) word-alignment training (e.g., IBM alignment models), (ii) heuristic phrase extraction and relative frequency based phrase translation probability estimation. In this work, we utilize phrase training for the task of adaptation. We use the forced alignment (FA) method (Wuebker et al., 2010) to perform the phrase alignment training and probability estimation. We perform phrase training by running a normal SMT decoder on the training data and constrain the translation to the given target instance. Using n-best possible phrase segmentation for each training instance, the phrase probabilities are re-estimated over the output. Leaving-one-out is used during the forced alignment procedure phase to avoid over-fitting (Wuebker et al., 2010).
In the standard phrase training procedure, we are given a training set y, from which an initial heuristics-based phrase table p 0 y is generated. FA training is then done over the training set y using the phrases and probabilities in p 0 y (possibly updated by the leaving-one-out method). Finally, re-estimation of the phrase probabilities is done over the decoder output, generating the FA phrase table p 1 . We explain next how to utilize FA training for adaptation.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The standard phrase extraction procedure in SMT consists of two phases: (i) word-alignment training (e.g., IBM alignment models), (ii) heuristic phrase extraction and relative frequency based phrase translation probability estimation.</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this work, we utilize phrase training for the task of adaptation.</text>
              <doc_id>26</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We use the forced alignment (FA) method (Wuebker et al., 2010) to perform the phrase alignment training and probability estimation.</text>
              <doc_id>27</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We perform phrase training by running a normal SMT decoder on the training data and constrain the translation to the given target instance.</text>
              <doc_id>28</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Using n-best possible phrase segmentation for each training instance, the phrase probabilities are re-estimated over the output.</text>
              <doc_id>29</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Leaving-one-out is used during the forced alignment procedure phase to avoid over-fitting (Wuebker et al., 2010).</text>
              <doc_id>30</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the standard phrase training procedure, we are given a training set y, from which an initial heuristics-based phrase table p 0 y is generated.</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>FA training is then done over the training set y using the phrases and probabilities in p 0 y (possibly updated by the leaving-one-out method).</text>
              <doc_id>32</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Finally, re-estimation of the phrase probabilities is done over the decoder output, generating the FA phrase table p 1 .</text>
              <doc_id>33</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We explain next how to utilize FA training for adaptation.</text>
              <doc_id>34</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Adaptation</title>
            <text>In this work, we utilize phrase training for the task of adaptation. The main idea is to generate the initial phrase table required for FA using a general-domain training data y &#8242; , thus resulting in p 0 y &#8242;, and perform the FA training over y IN , the in-domain training data (instead of y &#8242; in the standard procedure). This way, we bias the probabilities of p 0 y &#8242; towards the indomain distribution. We denote this new procedure by Y&#8217;-FA-IN. This differs from the standard IN-FA- IN by that we have more phrase pairs to use for FA. Thus, we obtain phrase pairs relevant to IN in addition to &#8220;general&#8221; phrase pairs which were not extracted from IN, perhaps due to faulty word alignments. The probabilities of the general phrase table will be tailored towards IN. In practice, we usually have in-domain IN and other-domain OD data. We denote by ALL the concatenation of IN and OD. To adapt the ALL phrase table, we perform the FA procedure ALL-FA-IN. We also utilize leaving-one-out to avoid over-fitting.
Another procedure we experimented with is adapting the OD phrase table using FA over IN, without leaving-one-out. We denote it by OD-FA 0 - IN. In this FA scenario, we do not use leaving-oneout as IN is not contained in OD, therefore, overfitting will not occur. By this procedure, we train phrases from OD that are relevant for both OD and IN, while the probabilities will be tailored to IN. In this case, we do not expect improvements over the IN based phrase table, but, improvements over OD and reduction in the phrase table size. We compare our suggested FA based adaptation to the standard FA procedure.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In this work, we utilize phrase training for the task of adaptation.</text>
                  <doc_id>35</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The main idea is to generate the initial phrase table required for FA using a general-domain training data y &#8242; , thus resulting in p 0 y &#8242;, and perform the FA training over y IN , the in-domain training data (instead of y &#8242; in the standard procedure).</text>
                  <doc_id>36</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This way, we bias the probabilities of p 0 y &#8242; towards the indomain distribution.</text>
                  <doc_id>37</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We denote this new procedure by Y&#8217;-FA-IN.</text>
                  <doc_id>38</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This differs from the standard IN-FA- IN by that we have more phrase pairs to use for FA.</text>
                  <doc_id>39</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Thus, we obtain phrase pairs relevant to IN in addition to &#8220;general&#8221; phrase pairs which were not extracted from IN, perhaps due to faulty word alignments.</text>
                  <doc_id>40</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The probabilities of the general phrase table will be tailored towards IN.</text>
                  <doc_id>41</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>In practice, we usually have in-domain IN and other-domain OD data.</text>
                  <doc_id>42</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>We denote by ALL the concatenation of IN and OD.</text>
                  <doc_id>43</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>To adapt the ALL phrase table, we perform the FA procedure ALL-FA-IN.</text>
                  <doc_id>44</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>We also utilize leaving-one-out to avoid over-fitting.</text>
                  <doc_id>45</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Another procedure we experimented with is adapting the OD phrase table using FA over IN, without leaving-one-out.</text>
                  <doc_id>46</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We denote it by OD-FA 0 - IN.</text>
                  <doc_id>47</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In this FA scenario, we do not use leaving-oneout as IN is not contained in OD, therefore, overfitting will not occur.</text>
                  <doc_id>48</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>By this procedure, we train phrases from OD that are relevant for both OD and IN, while the probabilities will be tailored to IN.</text>
                  <doc_id>49</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In this case, we do not expect improvements over the IN based phrase table, but, improvements over OD and reduction in the phrase table size.</text>
                  <doc_id>50</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We compare our suggested FA based adaptation to the standard FA procedure.</text>
                  <doc_id>51</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Experimental Setup</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>52</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Training Corpora</title>
            <text>To evaluate the introduced methods experimentally, we use the IWSLT 2011 TED Arabic-to-English and German-to-English translation tasks. The IWSLT 2011 evaluation campaign focuses on the translation of TED talks, a collection of lectures on a variety of topics ranging from science to culture. For Arabic-to-English, the bilingual data consists of roughly 100K sentences of in-domain TED talks data and 8M sentences of &#8220;other&#8221;-domain United Nations (UN) data. For the German-to-English task, the data consists of 130K TED sentences and 2.1M sentences of &#8220;other&#8221;-domain data assembled from the news-commentary and the europarl corpora. For language model training purposes, we use an additional 1.4 billion words (supplied as part of the campaign monolingual training data).
The bilingual training and test data for the Arabicto-English and German-to-English tasks are summarized in Table 1 1 . The English data was tokenized and lowercased while the Arabic data was tokenized and segmented using MADA v3.1 (Roth et al., 2008) with the ATB scheme. The German source is decompounded (Koehn and Knight, 2003) and part-of-speech-based long-range verb reordering rules (Popovi&#263; and Ney, 2006) are applied. From Table 1, we note that using the general data considerably reduces the number of out-of-
1 For a list of the IWSLT TED 2011 training corpora, see http://www.iwslt2011.org/doku.php? id=06_evaluation
Set Sen Tok OOV/IN OOV/ALL
vocabulary (OOV) words. This comes with the price of increasing the size of the training data by a factor of more than 20. A simple concatenation of the corpora might mask the phrase probabilities obtained from the in-domain corpus, causing a deterioration in performance. One way to avoid this contamination is by filtering the general corpus, but this discards phrase translations completely from the phrase model. A more principled way is by adapting the phrase probabilities of the full system to the domain being tackled. We perform this by phrase training the full phrase table over the in-domain training set.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>To evaluate the introduced methods experimentally, we use the IWSLT 2011 TED Arabic-to-English and German-to-English translation tasks.</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The IWSLT 2011 evaluation campaign focuses on the translation of TED talks, a collection of lectures on a variety of topics ranging from science to culture.</text>
                  <doc_id>54</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For Arabic-to-English, the bilingual data consists of roughly 100K sentences of in-domain TED talks data and 8M sentences of &#8220;other&#8221;-domain United Nations (UN) data.</text>
                  <doc_id>55</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For the German-to-English task, the data consists of 130K TED sentences and 2.1M sentences of &#8220;other&#8221;-domain data assembled from the news-commentary and the europarl corpora.</text>
                  <doc_id>56</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For language model training purposes, we use an additional 1.4 billion words (supplied as part of the campaign monolingual training data).</text>
                  <doc_id>57</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The bilingual training and test data for the Arabicto-English and German-to-English tasks are summarized in Table 1 1 .</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The English data was tokenized and lowercased while the Arabic data was tokenized and segmented using MADA v3.1 (Roth et al., 2008) with the ATB scheme.</text>
                  <doc_id>59</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The German source is decompounded (Koehn and Knight, 2003) and part-of-speech-based long-range verb reordering rules (Popovi&#263; and Ney, 2006) are applied.</text>
                  <doc_id>60</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>From Table 1, we note that using the general data considerably reduces the number of out-of-</text>
                  <doc_id>61</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 For a list of the IWSLT TED 2011 training corpora, see http://www.iwslt2011.org/doku.php?</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>id=06_evaluation</text>
                  <doc_id>63</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Set Sen Tok OOV/IN OOV/ALL</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>vocabulary (OOV) words.</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This comes with the price of increasing the size of the training data by a factor of more than 20.</text>
                  <doc_id>66</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>A simple concatenation of the corpora might mask the phrase probabilities obtained from the in-domain corpus, causing a deterioration in performance.</text>
                  <doc_id>67</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>One way to avoid this contamination is by filtering the general corpus, but this discards phrase translations completely from the phrase model.</text>
                  <doc_id>68</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>A more principled way is by adapting the phrase probabilities of the full system to the domain being tackled.</text>
                  <doc_id>69</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We perform this by phrase training the full phrase table over the in-domain training set.</text>
                  <doc_id>70</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Translation System</title>
            <text>The baseline system is built using the open-source SMT toolkit Jane 2.0, which provides a state-ofthe-art phrase-based SMT system (Wuebker et al., 2012a). In addition to the phrase based decoder, Jane 2.0 implements the forced alignment procedure used in this work for the purpose of adaptation. We use the standard set of models with phrase translation probabilities for source-to-target and target-tosource directions, smoothing with lexical weights, a word and phrase penalty, distance-based reordering and an n-gram target language model. The SMT systems are tuned on the dev (dev2010) development set with minimum error rate training (Och, 2003) using BLEU (Papineni et al., 2002) accuracy measure as the optimization criterion. We test the performance of our system on the test (tst2010) and eval (tst2011) sets using the BLEU and translation edit rate (TER) (Snover et al., 2006) measures. We use
TER as an additional measure to verify the consistency of our improvements and avoid over-tuning. The Arabic-English results are case sensitive while the German-English results are case insensitive.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The baseline system is built using the open-source SMT toolkit Jane 2.0, which provides a state-ofthe-art phrase-based SMT system (Wuebker et al., 2012a).</text>
                  <doc_id>71</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In addition to the phrase based decoder, Jane 2.0 implements the forced alignment procedure used in this work for the purpose of adaptation.</text>
                  <doc_id>72</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We use the standard set of models with phrase translation probabilities for source-to-target and target-tosource directions, smoothing with lexical weights, a word and phrase penalty, distance-based reordering and an n-gram target language model.</text>
                  <doc_id>73</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The SMT systems are tuned on the dev (dev2010) development set with minimum error rate training (Och, 2003) using BLEU (Papineni et al., 2002) accuracy measure as the optimization criterion.</text>
                  <doc_id>74</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We test the performance of our system on the test (tst2010) and eval (tst2011) sets using the BLEU and translation edit rate (TER) (Snover et al., 2006) measures.</text>
                  <doc_id>75</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We use</text>
                  <doc_id>76</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>TER as an additional measure to verify the consistency of our improvements and avoid over-tuning.</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The Arabic-English results are case sensitive while the German-English results are case insensitive.</text>
                  <doc_id>78</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Results</title>
        <text>For TM training, we define three different sets: indomain (IN) which is the TED corpus, other-domain (OD) which consists of the UN corpus for Arabic- English and a concatenation of news-commentary and europarl for German-English, and ALL which consists of the concatenation of IN and OD. We experiment with the following extraction methods:
&#8226; Heuristics: standard phrase extraction using word-alignment training and heuristic phrase extraction over the word alignment. The extraction is performed for the three different training data, IN, OD and ALL.
&#8226; FA standard: standard FA phrase training where the same training set is used for initial phrase table generation as well as the FA procedure. We perform the training on the three different training sets and denote the resulting systems by IN-FA, OD-FA and ALL-FA.
&#8226; FA adaptation: FA based adaptation phrase training, where the initial table is generated from some general data and the FA training is performed on the IN data to achieve adaptation. We perform two experiments, OD-FA 0 - IN without leaving-one-out and ALL-FA-IN with leaving-one-out.
The results of the various experiments over both Arabic-English and German-English tasks are summarized in Table 2. The usefulness of the OD data differs between the Arabic-to-English and the German-to-English translation tasks. For Arabic-to- English, the OD system is 2.5%-4.3% BLEU worse than the IN system, whereas for the German-to- English task the differences between IN and OD are smaller and range from 0.9% to 1.6% BLEU. The
Phrase training System Rules dev test eval method number BLEU TER BLEU TER BLEU TER
Heuristics
FA standard
FA adaptation
Heuristics
FA standard
FA adaptation
inferior performance of the OD system can be related to noisy data or bigger discrepancy between the OD data domain distribution and the IN distribution. The ALL system performs according to the usefulness of the OD training set, where for Arabicto-English we observe deterioration in performance for all test sets and up-to -0.9% BLEU on the test set. On the other hand, for German-to-English, the ALL system is improving over IN where the biggest improvement is observed on the eval set with +0.9%
BLEU improvement.
The standard FA procedure achieves mixed results, where IN-FA deteriorates the results over the IN counterpart for Arabic-English, while improving for German-English. ALL-FA performs comparably to the ALL system on both tasks, while reducing the phrase table size considerably. The OD-FA system deteriorates the results in comparison to the OD system in most cases, which is expected as training over the OD set fits the phrase model on the OD domain, making it perform worse on IN. (Wuebker et al., 2012b) also report mixed results with FA training.
The FA adaptation results are summarized in the last block of the experiments. The OD-FA 0 -IN improves over the OD system, which means that the training procedure was able to modify the OD probabilities to perform well on the IN data. On the German-to-English task, the OD-FA 0 -IN performs comparably to the IN system, whereas for Arabicto-English OD-FA 0 -IN was able to close around half of the gap between OD and IN.
The FA adapted ALL system (ALL-FA-IN) performs best in our experiments, improving on both
BLEU and TER measures. In comparison to the
best heuristics system (IN for Arabic-English and ALL for German-English), +0.4% BLEU and -0.6%
TER improvements are observed on the eval set for
Arabic-English. For German-English, the biggest improvements are observed on TER with -0.8% on test and -0.5% on eval. The results suggest that ALL- FA-IN is able to learn more useful phrases than the IN system and adjust the ALL phrase probabilities towards the in-domain distribution.
System dev test
BLEU TER BLEU TER</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>For TM training, we define three different sets: indomain (IN) which is the TED corpus, other-domain (OD) which consists of the UN corpus for Arabic- English and a concatenation of news-commentary and europarl for German-English, and ALL which consists of the concatenation of IN and OD.</text>
              <doc_id>79</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We experiment with the following extraction methods:</text>
              <doc_id>80</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; Heuristics: standard phrase extraction using word-alignment training and heuristic phrase extraction over the word alignment.</text>
              <doc_id>81</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The extraction is performed for the three different training data, IN, OD and ALL.</text>
              <doc_id>82</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; FA standard: standard FA phrase training where the same training set is used for initial phrase table generation as well as the FA procedure.</text>
              <doc_id>83</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We perform the training on the three different training sets and denote the resulting systems by IN-FA, OD-FA and ALL-FA.</text>
              <doc_id>84</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; FA adaptation: FA based adaptation phrase training, where the initial table is generated from some general data and the FA training is performed on the IN data to achieve adaptation.</text>
              <doc_id>85</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We perform two experiments, OD-FA 0 - IN without leaving-one-out and ALL-FA-IN with leaving-one-out.</text>
              <doc_id>86</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The results of the various experiments over both Arabic-English and German-English tasks are summarized in Table 2.</text>
              <doc_id>87</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The usefulness of the OD data differs between the Arabic-to-English and the German-to-English translation tasks.</text>
              <doc_id>88</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For Arabic-to- English, the OD system is 2.5%-4.3% BLEU worse than the IN system, whereas for the German-to- English task the differences between IN and OD are smaller and range from 0.9% to 1.6% BLEU.</text>
              <doc_id>89</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The</text>
              <doc_id>90</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Phrase training System Rules dev test eval method number BLEU TER BLEU TER BLEU TER</text>
              <doc_id>91</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Heuristics</text>
              <doc_id>92</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>FA standard</text>
              <doc_id>93</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>FA adaptation</text>
              <doc_id>94</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Heuristics</text>
              <doc_id>95</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>FA standard</text>
              <doc_id>96</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>FA adaptation</text>
              <doc_id>97</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>inferior performance of the OD system can be related to noisy data or bigger discrepancy between the OD data domain distribution and the IN distribution.</text>
              <doc_id>98</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The ALL system performs according to the usefulness of the OD training set, where for Arabicto-English we observe deterioration in performance for all test sets and up-to -0.9% BLEU on the test set.</text>
              <doc_id>99</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>On the other hand, for German-to-English, the ALL system is improving over IN where the biggest improvement is observed on the eval set with +0.9%</text>
              <doc_id>100</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>BLEU improvement.</text>
              <doc_id>101</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The standard FA procedure achieves mixed results, where IN-FA deteriorates the results over the IN counterpart for Arabic-English, while improving for German-English.</text>
              <doc_id>102</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>ALL-FA performs comparably to the ALL system on both tasks, while reducing the phrase table size considerably.</text>
              <doc_id>103</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The OD-FA system deteriorates the results in comparison to the OD system in most cases, which is expected as training over the OD set fits the phrase model on the OD domain, making it perform worse on IN.</text>
              <doc_id>104</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>(Wuebker et al., 2012b) also report mixed results with FA training.</text>
              <doc_id>105</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The FA adaptation results are summarized in the last block of the experiments.</text>
              <doc_id>106</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The OD-FA 0 -IN improves over the OD system, which means that the training procedure was able to modify the OD probabilities to perform well on the IN data.</text>
              <doc_id>107</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>On the German-to-English task, the OD-FA 0 -IN performs comparably to the IN system, whereas for Arabicto-English OD-FA 0 -IN was able to close around half of the gap between OD and IN.</text>
              <doc_id>108</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The FA adapted ALL system (ALL-FA-IN) performs best in our experiments, improving on both</text>
              <doc_id>109</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>BLEU and TER measures.</text>
              <doc_id>110</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In comparison to the</text>
              <doc_id>111</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>best heuristics system (IN for Arabic-English and ALL for German-English), +0.4% BLEU and -0.6%</text>
              <doc_id>112</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>TER improvements are observed on the eval set for</text>
              <doc_id>113</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Arabic-English.</text>
              <doc_id>114</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For German-English, the biggest improvements are observed on TER with -0.8% on test and -0.5% on eval.</text>
              <doc_id>115</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The results suggest that ALL- FA-IN is able to learn more useful phrases than the IN system and adjust the ALL phrase probabilities towards the in-domain distribution.</text>
              <doc_id>116</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>System dev test</text>
              <doc_id>117</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>BLEU TER BLEU TER</text>
              <doc_id>118</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Mixture Modeling</title>
            <text>In this section, we compare our method to mixture modeling based adaptation, in addition to applying mixture modeling on top of our method. We focus on linear interpolation (Foster and Kuhn, 2007) of the in-domain (IN) and other-domain phrase tables, where we vary the latter between the heuristically extracted phrase table (OD) and the FA adapted one (OD-FA 0 -IN). The interpolation weight is uniform for the interpolated phrase tables (0.5). The results of mixture modeling are summarized in Table 3. In this table, we include the best heuristics based system (Heuristics best ) from Table 2 as a reference system. The results on the eval set are omitted as they show similar tendencies to the test set results.
Linear interpolation of IN and OD (IN,OD) is performing well in our experiments, with big improvements over the dev set, +1.0% BLEU for Arabic-to- English and +0.4% BLEU for German-to-English. On the test set, we observe smaller improvements. Interpolating IN with the phrase training adapted system OD-FA 0 -IN (IN,OD-FA 0 -IN) achieves additional gains over the IN,OD system, the biggest are observed on TER for German-to-English, with -0.4% and -0.5% improvements on the dev and test sets correspondingly.
Comparing heuristics based interpolation (IN,OD) to our best phrase training adapted system (ALL-FA-IN) shows mixed results. For Arabic-to- English, the systems are comparable, while for the German-to-English test set, IN,OD is +0.2% BLEU better and +0.8% TER worse than ALL-FA-IN. We hypothesize that for Arabic-to-English interpolation is important due to the larger size of the OD data, where it could reduce the masking of the IN training data by the much larger OD data. Nevertheless, as mentioned previously, using phrase training adapted phrase table in a mixture setup consistently improves over using heuristically extracted tables.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In this section, we compare our method to mixture modeling based adaptation, in addition to applying mixture modeling on top of our method.</text>
                  <doc_id>119</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We focus on linear interpolation (Foster and Kuhn, 2007) of the in-domain (IN) and other-domain phrase tables, where we vary the latter between the heuristically extracted phrase table (OD) and the FA adapted one (OD-FA 0 -IN).</text>
                  <doc_id>120</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The interpolation weight is uniform for the interpolated phrase tables (0.5).</text>
                  <doc_id>121</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The results of mixture modeling are summarized in Table 3.</text>
                  <doc_id>122</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In this table, we include the best heuristics based system (Heuristics best ) from Table 2 as a reference system.</text>
                  <doc_id>123</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The results on the eval set are omitted as they show similar tendencies to the test set results.</text>
                  <doc_id>124</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Linear interpolation of IN and OD (IN,OD) is performing well in our experiments, with big improvements over the dev set, +1.0% BLEU for Arabic-to- English and +0.4% BLEU for German-to-English.</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>On the test set, we observe smaller improvements.</text>
                  <doc_id>126</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Interpolating IN with the phrase training adapted system OD-FA 0 -IN (IN,OD-FA 0 -IN) achieves additional gains over the IN,OD system, the biggest are observed on TER for German-to-English, with -0.4% and -0.5% improvements on the dev and test sets correspondingly.</text>
                  <doc_id>127</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Comparing heuristics based interpolation (IN,OD) to our best phrase training adapted system (ALL-FA-IN) shows mixed results.</text>
                  <doc_id>128</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For Arabic-to- English, the systems are comparable, while for the German-to-English test set, IN,OD is +0.2% BLEU better and +0.8% TER worse than ALL-FA-IN.</text>
                  <doc_id>129</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We hypothesize that for Arabic-to-English interpolation is important due to the larger size of the OD data, where it could reduce the masking of the IN training data by the much larger OD data.</text>
                  <doc_id>130</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Nevertheless, as mentioned previously, using phrase training adapted phrase table in a mixture setup consistently improves over using heuristically extracted tables.</text>
                  <doc_id>131</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Conclusions</title>
        <text>In this work, we propose a phrase training procedure for adaptation. The phrase training is implemented using the FA method. First, we extract a standard phrase table using the whole available training data. Using this table, we initialize the FA procedure and perform training on the in-domain set. Experiments are done on the Arabic-to-English and German-to-English TED lectures translation tasks. We show that the suggested procedure is improving over unadapted baselines. On the Arabicto-English task, the FA adapted system is +0.9% BLEU better than the full unadapted counterpart on both test sets. Unlike the Arabic-to-English setup, the German-to-English OD data is helpful and produces a strong unadapted baseline in concatenation with IN. In this case, the FA adapted system achieves BLEU improvements mainly on the development set with +0.6% BLEU, on the test and eval sets, improvements of -0.8% and -0.6% TER are observed correspondingly. As a side effect of the FA training process, the size of the adapted phrase table is less than 10% of the size of the full table. Finally, we experimented with mixture modeling where improvements are observed over the unadapted baselines. The results show that using our phrase training adapted OD table yields better performance than using the heuristically extracted OD in a mixture framework.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this work, we propose a phrase training procedure for adaptation.</text>
              <doc_id>132</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The phrase training is implemented using the FA method.</text>
              <doc_id>133</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>First, we extract a standard phrase table using the whole available training data.</text>
              <doc_id>134</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Using this table, we initialize the FA procedure and perform training on the in-domain set.</text>
              <doc_id>135</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Experiments are done on the Arabic-to-English and German-to-English TED lectures translation tasks.</text>
              <doc_id>136</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We show that the suggested procedure is improving over unadapted baselines.</text>
              <doc_id>137</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>On the Arabicto-English task, the FA adapted system is +0.9% BLEU better than the full unadapted counterpart on both test sets.</text>
              <doc_id>138</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Unlike the Arabic-to-English setup, the German-to-English OD data is helpful and produces a strong unadapted baseline in concatenation with IN.</text>
              <doc_id>139</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>In this case, the FA adapted system achieves BLEU improvements mainly on the development set with +0.6% BLEU, on the test and eval sets, improvements of -0.8% and -0.6% TER are observed correspondingly.</text>
              <doc_id>140</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>As a side effect of the FA training process, the size of the adapted phrase table is less than 10% of the size of the full table.</text>
              <doc_id>141</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Finally, we experimented with mixture modeling where improvements are observed over the unadapted baselines.</text>
              <doc_id>142</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>The results show that using our phrase training adapted OD table yields better performance than using the heuristically extracted OD in a mixture framework.</text>
              <doc_id>143</doc_id>
              <sec_id>11</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>6</index>
        <title>Acknowledgments</title>
        <text>This material is based upon work supported by the DARPA BOLT project under Contract No. HR0011- 12-C-0015. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This material is based upon work supported by the DARPA BOLT project under Contract No.</text>
              <doc_id>144</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>HR0011- 12-C-0015.</text>
              <doc_id>145</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.</text>
              <doc_id>146</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: IWSLT 2011 TED bilingual corpora statistics: the number of tokens is given for the source side. OOV/X denotes the number of OOV words in relation to corpus X (the percentage is given in parentheses). IN is the TED in-domain data, OD denotes other-domain data, ALL denotes the concatenation of IN and OD.</caption>
        <reference_text>In PAGE 2: ...t al., 2008) with the ATB scheme. The German source is decompounded (Koehn and Knight, 2003) and part-of-speech-based long-range verb reorder- ing rules (Popovi? c and Ney, 2006) are applied. From  Table1 , we note that using the general data considerably reduces the number of out-of- 1For a list of the IWSLT TED 2011 training cor- pora, see http://www....</reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>Set</cell>
              <cell>Sen</cell>
              <cell>Tok OOV/IN</cell>
              <cell>OOV/ALL</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>German-to-English</cell>
            </row>
            <row>
              <cell>IN</cell>
              <cell>130K</cell>
              <cell>2.5M</cell>
            </row>
            <row>
              <cell>OD</cell>
              <cell>2.1M</cell>
              <cell>55M</cell>
            </row>
            <row>
              <cell>dev</cell>
              <cell>883</cell>
              <cell>20K 398 (2.0%)</cell>
              <cell>215 (1.1%)</cell>
            </row>
            <row>
              <cell>test</cell>
              <cell>1565</cell>
              <cell>32K 483 (1.5%)</cell>
              <cell>227 (0.7%)</cell>
            </row>
            <row>
              <cell>eval</cell>
              <cell>1436</cell>
              <cell>27K 490 (1.8%)</cell>
              <cell>271 (1.0%)</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>Arabic-to-English</cell>
            </row>
            <row>
              <cell>IN</cell>
              <cell>90K</cell>
              <cell>1.6M</cell>
            </row>
            <row>
              <cell>OD</cell>
              <cell>7.9M</cell>
              <cell>228M</cell>
            </row>
            <row>
              <cell>dev</cell>
              <cell>934</cell>
              <cell>19K 408 (2.2%)</cell>
              <cell>184 (1.0%)</cell>
            </row>
            <row>
              <cell>test</cell>
              <cell>1664</cell>
              <cell>31K 495 (1.6%)</cell>
              <cell>228 (0.8%)</cell>
            </row>
            <row>
              <cell>eval</cell>
              <cell>1450</cell>
              <cell>27K 513 (1.9%)</cell>
              <cell>163 (0.6%)</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: TED 2011 translation results. BLEU and TER are given in percentages. IN denotes the TED lectures indomain corpus, OD denotes the other-domain corpus, ALL is the concatenation of IN and OD. FA 0 denotes forced alignment training without leaving-one-out (otherwise, leaving-one-out is used).</caption>
        <reference_text>In PAGE 5: ...or the interpolated phrase tables (0.5). The results of mixture modeling are summarized in Table 3. In this table, we include the best heuristics based sys- tem (Heuristicsbest) from  Table2  as a reference sys- tem. The results on the eval set are omitted as they show similar tendencies to the test set results....</reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>Phrase training  method</cell>
              <cell>System       IN</cell>
              <cell>Rules   number   Arabic-to-English  1.1M</cell>
              <cell>BLEU   Arabic-to-English   27.2</cell>
              <cell>TER   Arabic-to-English   54.1</cell>
              <cell>BLEU     25.3</cell>
              <cell>TER     57.1</cell>
              <cell>BLEU    24.3</cell>
              <cell>eval    BLEU TER       59.9</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Heuristics</cell>
              <cell>OD</cell>
              <cell>36.3M</cell>
              <cell>24.7</cell>
              <cell>57.7</cell>
              <cell>21.2</cell>
              <cell>62.6</cell>
              <cell>21.0</cell>
              <cell>64.7</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>ALL</cell>
              <cell>36.9M</cell>
              <cell>27.1</cell>
              <cell>54.8</cell>
              <cell>24.4</cell>
              <cell>58.6</cell>
              <cell>23.8</cell>
              <cell>61.1</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>IN-FA</cell>
              <cell>1.0M</cell>
              <cell>27.0</cell>
              <cell>54.4</cell>
              <cell>25.0</cell>
              <cell>57.5</cell>
              <cell>23.8</cell>
              <cell>60.3</cell>
            </row>
            <row>
              <cell>FA standard</cell>
              <cell>OD-FA</cell>
              <cell>1.8M</cell>
              <cell>24.5</cell>
              <cell>57.7</cell>
              <cell>21.0</cell>
              <cell>62.4</cell>
              <cell>21.2</cell>
              <cell>64.3</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>ALL-FA</cell>
              <cell>2.0M</cell>
              <cell>27.2</cell>
              <cell>54.2</cell>
              <cell>24.5</cell>
              <cell>58.1</cell>
              <cell>23.8</cell>
              <cell>60.6</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>OD-FA0-IN</cell>
              <cell>0.3M</cell>
              <cell>25.8</cell>
              <cell>55.8</cell>
              <cell>23.6</cell>
              <cell>59.4</cell>
              <cell>22.7</cell>
              <cell>61.7</cell>
            </row>
            <row>
              <cell>FA adaptation</cell>
              <cell>ALL-FA-IN</cell>
              <cell>0.5M</cell>
              <cell>27.7</cell>
              <cell>53.7</cell>
              <cell>25.3</cell>
              <cell>56.9</cell>
              <cell>24.7</cell>
              <cell>59.3</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>German-to-English</cell>
              <cell>German-to-English</cell>
              <cell>German-to-English</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>IN</cell>
              <cell>1.3M</cell>
              <cell>31.0</cell>
              <cell>48.9</cell>
              <cell>29.3</cell>
              <cell>51.0</cell>
              <cell>32.7</cell>
              <cell>46.8</cell>
            </row>
            <row>
              <cell>Heuristics</cell>
              <cell>OD</cell>
              <cell>7.3M</cell>
              <cell>29.8</cell>
              <cell>49.2</cell>
              <cell>27.7</cell>
              <cell>51.5</cell>
              <cell>31.8</cell>
              <cell>47.5</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>ALL</cell>
              <cell>7.8M</cell>
              <cell>31.2</cell>
              <cell>48.3</cell>
              <cell>29.5</cell>
              <cell>50.5</cell>
              <cell>33.6</cell>
              <cell>46.1</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>IN-FA</cell>
              <cell>0.5M</cell>
              <cell>31.6</cell>
              <cell>48.2</cell>
              <cell>29.7</cell>
              <cell>50.5</cell>
              <cell>33.3</cell>
              <cell>46.4</cell>
            </row>
            <row>
              <cell>FA standard</cell>
              <cell>OD-FA</cell>
              <cell>3.0M</cell>
              <cell>29.1</cell>
              <cell>51.0</cell>
              <cell>27.6</cell>
              <cell>53.0</cell>
              <cell>30.7</cell>
              <cell>49.6</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>ALL-FA</cell>
              <cell>3.2M</cell>
              <cell>31.4</cell>
              <cell>48.3</cell>
              <cell>29.4</cell>
              <cell>50.8</cell>
              <cell>33.6</cell>
              <cell>46.2</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>OD-FA0-IN</cell>
              <cell>0.9M</cell>
              <cell>31.2</cell>
              <cell>48.7</cell>
              <cell>29.1</cell>
              <cell>50.9</cell>
              <cell>32.7</cell>
              <cell>46.9</cell>
            </row>
            <row>
              <cell>FA adaptation</cell>
              <cell>ALL-FA-IN</cell>
              <cell>0.9M</cell>
              <cell>31.8</cell>
              <cell>47.4</cell>
              <cell>29.7</cell>
              <cell>49.7</cell>
              <cell>33.6</cell>
              <cell>45.5</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: TED 2011 mixture modeling results. Heuristics best is the best heuristics based system, IN for Arabic-English and ALL for German-English. X,Y denotes linear interpolation between X and Y phrase tables.</caption>
        <reference_text>In PAGE 5: ...5). The results of mixture modeling are summarized in  Table3 . In this table, we include the best heuristics based sys- tem (Heuristicsbest) from Table 2 as a reference sys- tem....</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>System   Arabic-to-English</cell>
              <cell>BLEU   Arabic-to-English</cell>
              <cell>TER   Arabic-to-English</cell>
              <cell>BLEU</cell>
              <cell>TER</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Heuristicsbest</cell>
              <cell>27.2</cell>
              <cell>54.1</cell>
              <cell>25.3</cell>
              <cell>57.1</cell>
            </row>
            <row>
              <cell>IN,OD</cell>
              <cell>28.2</cell>
              <cell>53.1</cell>
              <cell>25.5</cell>
              <cell>56.8</cell>
            </row>
            <row>
              <cell>IN,OD-FA0-IN</cell>
              <cell>28.4</cell>
              <cell>52.9</cell>
              <cell>25.7</cell>
              <cell>56.5</cell>
            </row>
            <row>
              <cell>German-to-English</cell>
              <cell>German-to-English</cell>
              <cell>German-to-English</cell>
              <cell>German-to-English</cell>
            </row>
            <row>
              <cell>Heuristicsbest</cell>
              <cell>31.2</cell>
              <cell>48.3</cell>
              <cell>29.5</cell>
              <cell>50.5</cell>
            </row>
            <row>
              <cell>IN,OD</cell>
              <cell>31.6</cell>
              <cell>48.2</cell>
              <cell>29.9</cell>
              <cell>50.5</cell>
            </row>
            <row>
              <cell>IN,OD-FA0-IN</cell>
              <cell>31.8</cell>
              <cell>47.8</cell>
              <cell>30.0</cell>
              <cell>50.0</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>George Foster</author>
          <author>Roland Kuhn</author>
        </authors>
        <title>Mixture-model adaptation for SMT.</title>
        <publication>In Proceedings of the Second Workshop on Statistical Machine Translation,</publication>
        <pages>128--135</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>George Foster</author>
          <author>Cyril Goutte</author>
          <author>Roland Kuhn</author>
        </authors>
        <title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
        <publication>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>451--459</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Kevin Knight</author>
        </authors>
        <title>Empirical Methods for Compound Splitting. In</title>
        <publication>Proc. 10th Conf. of the Europ. Chapter of the Assoc. for Computational Linguistics (EACL),</publication>
        <pages>347--354</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Spyros Matsoukas</author>
          <author>Antti-Veikko I Rosti</author>
          <author>Bing Zhang</author>
        </authors>
        <title>Discriminative corpus weight estimation for machine translation.</title>
        <publication>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</publication>
        <pages>708--717</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Franz J Och</author>
        </authors>
        <title>Minimum Error Rate Training in Statistical Machine Translation.</title>
        <publication>In Proceedings of the 41th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>160--167</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Kishore Papineni</author>
          <author>Salim Roukos</author>
          <author>Todd Ward</author>
          <author>WeiJing Zhu</author>
        </authors>
        <title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
        <publication>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>311--318</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>M Popovi&#263;</author>
          <author>H Ney</author>
        </authors>
        <title>POS-based Word Reorderings for Statistical Machine Translation.</title>
        <publication>In International Conference on Language Resources and Evaluation,</publication>
        <pages>1278--1283</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Ryan Roth</author>
          <author>Owen Rambow</author>
          <author>Nizar Habash</author>
          <author>Mona Diab</author>
          <author>Cynthia Rudin</author>
        </authors>
        <title>Arabic morphological tagging, diacritization, and lemmatization using lexeme models and feature ranking.</title>
        <publication>In Proceedings of ACL08: HLT, Short Papers,</publication>
        <pages>117--120</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Matthew Snover</author>
          <author>Bonnie Dorr</author>
          <author>Richard Schwartz</author>
          <author>Linnea Micciulla</author>
          <author>John Makhoul</author>
        </authors>
        <title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
        <publication>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas,</publication>
        <pages>223--231</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Joern Wuebker</author>
          <author>Arne Mauser</author>
          <author>Hermann Ney</author>
        </authors>
        <title>Training phrase translation models with leaving-oneout.</title>
        <publication>In Proceedings of the 48th Annual Meeting of the Assoc. for Computational Linguistics,</publication>
        <pages>475--484</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>Joern Wuebker</author>
          <author>Matthias Huck</author>
          <author>Stephan Peitz</author>
          <author>Malte Nuhn</author>
          <author>Markus Freitag</author>
          <author>Jan-Thorsten Peter</author>
          <author>Saab Mansour</author>
          <author>Hermann Ney</author>
        </authors>
        <title>2: Open source phrase-based and hierarchical statistical machine translation.</title>
        <publication>In International Conference on Computational Linguistics,</publication>
        <pages>None</pages>
        <date>2012</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Foster and Kuhn, 2007</string>
        <sentence_id>25314</sentence_id>
        <char_offset>34</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>0</reference_id>
        <string>Foster and Kuhn (2007)</string>
        <sentence_id>25242</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>Foster et al. (2010)</string>
        <sentence_id>25247</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>1</reference_id>
        <string>Foster et al. (2010)</string>
        <sentence_id>25250</sentence_id>
        <char_offset>20</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>2</reference_id>
        <string>Koehn and Knight, 2003</string>
        <sentence_id>25293</sentence_id>
        <char_offset>35</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>3</reference_id>
        <string>Matsoukas et al. (2009)</string>
        <sentence_id>25243</sentence_id>
        <char_offset>37</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>4</reference_id>
        <string>Och, 2003</string>
        <sentence_id>25307</sentence_id>
        <char_offset>97</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>5</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>25307</sentence_id>
        <char_offset>120</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>6</reference_id>
        <string>Popovi&#263; and Ney, 2006</string>
        <sentence_id>25293</sentence_id>
        <char_offset>118</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>7</reference_id>
        <string>Roth et al., 2008</string>
        <sentence_id>25292</sentence_id>
        <char_offset>113</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>8</reference_id>
        <string>Snover et al., 2006</string>
        <sentence_id>25308</sentence_id>
        <char_offset>132</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>9</reference_id>
        <string>Wuebker et al., 2010</string>
        <sentence_id>25278</sentence_id>
        <char_offset>41</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>9</reference_id>
        <string>Wuebker et al., 2010</string>
        <sentence_id>25281</sentence_id>
        <char_offset>91</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>10</reference_id>
        <string>Wuebker et al., 2012</string>
        <sentence_id>25304</sentence_id>
        <char_offset>131</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>10</reference_id>
        <string>Wuebker et al., 2012</string>
        <sentence_id>25352</sentence_id>
        <char_offset>1</char_offset>
      </citation>
    </citations>
  </content>
</document>
