<document>
  <filename>W09-0809</filename>
  <authors>
    <author>Jakob Elming</author>
  </authors>
  <title>Syntactic Reordering for English-Arabic Phrase-Based Machine Translation</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>We investigate syntactic reordering within an English to Arabic translation task. We extend a pre-translation syntactic reordering approach developed on a close language pair (English-Danish) to the distant language pair, English-Arabic. We achieve significant improvements in translation quality over related approaches, measured by manual as well as automatic evaluations. These results prove the viability of this approach for distant languages.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We investigate syntactic reordering within an English to Arabic translation task.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We extend a pre-translation syntactic reordering approach developed on a close language pair (English-Danish) to the distant language pair, English-Arabic.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We achieve significant improvements in translation quality over related approaches, measured by manual as well as automatic evaluations.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>These results prove the viability of this approach for distant languages.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>The emergence of phrase-based statistical machine translation (PSMT) (Koehn et al., 2003a) has been one of the major developments in statistical approaches to translation. Allowing translation of word sequences (phrases) instead of single words provides PSMT with a high degree of robustness in word selection and in local-word reordering. Recent developments have shown that improvements in PSMT quality are possible using syntax. One such development is the pretranslation reordering approach, which adjusts the source sentence to resemble target-language word order prior to translation. This is typically done using rules that are either manually created or automatically learned from word-aligned parallel corpora. One particular variety of this approach, proposed by Elming (2008), uses a large set of linguistic features to automatically learn reordering rules. The rules are applied nondeterministically; however, phrase-internal wordalignments are used to ensure that the intended reordering does not come undone because of phrase internal reordering (Elming, 2008). This approach was shown to produce improved MT output on English-Danish MT, a relatively closely-related and similarly-structured language pair. In this paper, we study whether this approach can be extended to distant language pairs, specifically English-to-Arabic. We achieve significant improvement in translation quality over related approaches, measured by manual as well as automatic evaluations on this task. This proves the viability of this approach on distant languages. We also examined the effect of the alignment method on learning reordering rules. Interestingly, our experiments produced better translation using rules learned from automatic alignments than using rules learned from manual alignments.
In the next section, we discuss and contrast related work. Section 3 describes aspects of English and Arabic structure that are relevant to reordering. Section 4 describes the automatic induction of reordering rules and its integration in PSMT. In section 5, we describe the SMT system used in the experiments. In section 6, we evaluate and discuss the results of our English-Arabic MT system.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The emergence of phrase-based statistical machine translation (PSMT) (Koehn et al., 2003a) has been one of the major developments in statistical approaches to translation.</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Allowing translation of word sequences (phrases) instead of single words provides PSMT with a high degree of robustness in word selection and in local-word reordering.</text>
              <doc_id>5</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Recent developments have shown that improvements in PSMT quality are possible using syntax.</text>
              <doc_id>6</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>One such development is the pretranslation reordering approach, which adjusts the source sentence to resemble target-language word order prior to translation.</text>
              <doc_id>7</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This is typically done using rules that are either manually created or automatically learned from word-aligned parallel corpora.</text>
              <doc_id>8</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>One particular variety of this approach, proposed by Elming (2008), uses a large set of linguistic features to automatically learn reordering rules.</text>
              <doc_id>9</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The rules are applied nondeterministically; however, phrase-internal wordalignments are used to ensure that the intended reordering does not come undone because of phrase internal reordering (Elming, 2008).</text>
              <doc_id>10</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>This approach was shown to produce improved MT output on English-Danish MT, a relatively closely-related and similarly-structured language pair.</text>
              <doc_id>11</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, we study whether this approach can be extended to distant language pairs, specifically English-to-Arabic.</text>
              <doc_id>12</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>We achieve significant improvement in translation quality over related approaches, measured by manual as well as automatic evaluations on this task.</text>
              <doc_id>13</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>This proves the viability of this approach on distant languages.</text>
              <doc_id>14</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>We also examined the effect of the alignment method on learning reordering rules.</text>
              <doc_id>15</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Interestingly, our experiments produced better translation using rules learned from automatic alignments than using rules learned from manual alignments.</text>
              <doc_id>16</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the next section, we discuss and contrast related work.</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Section 3 describes aspects of English and Arabic structure that are relevant to reordering.</text>
              <doc_id>18</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Section 4 describes the automatic induction of reordering rules and its integration in PSMT.</text>
              <doc_id>19</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In section 5, we describe the SMT system used in the experiments.</text>
              <doc_id>20</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In section 6, we evaluate and discuss the results of our English-Arabic MT system.</text>
              <doc_id>21</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Related Work</title>
        <text>Much work has been done in syntactic reordering for SMT, focusing on both source and targetlanguage syntax. In this paper, we adapt an approach that utilizes source-syntax information as opposed to target-side syntax systems (Yamada and Knight, 2001; Galley et al., 2004). This is because we are translating from English to Arabic and we are discouraged by recent results indicating Arabic parsing is not at a stage that makes it usable in MT (Habash et al., 2006). While several recent authors using a pre-translation (sourceside) reordering approach have achieved positive results, it has been difficult to integrate syntactic
information while retaining the strengths of the statistical approach. In some studies, reordering decisions are done &#8220;deterministically&#8221; by supplying the decoder with a canonical word order (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Habash, 2007). These reordering rules are either manually specified or automatically learned from alignments; and they are always placed outside the actual PSMT system. By contrast, other studies (Crego and Mari&#241;o, 2007; Zhang et al., 2007; Li et al., 2007; Elming, 2008) are more in the spirit of PSMT, in that multiple reorderings are presented to the PSMT system as (possibly weighted) options that are allowed to contribute alongside other parameters. Specifically, we follow the pre-translation reordering approach of Elming (2008). This approach has been proven to remedy shortcomings of other pre-translation reordering approaches by reordering the input word sequence, but scoring the output word sequence.
Elming (2008) only examined the approach within English &#8211; Danish, a language pair that displays little reordering. By contrast, in this paper, we target the more demanding reordering task of translating between two distant languages, English and Arabic. While much work has been done on Arabic to English MT (Habash and Sadat, 2006; Lee, 2004) mostly focusing on addressing the problems caused by the rich morphology of Arabic, we handle the less described translation direction: English to Arabic. Recently, there are some new publications on English to Arabic MT. Sarikaya and Deng (2007) use joint morphological-lexical language models to re-rank the output of English dialectal-Arabic MT, and Badr et al. (2008) report results on the value of the morphological decomposition of Arabic during training and describe different techniques for re-composition of Arabic in the output. We differ from the previous efforts targeting Arabic in that (1) we do not address morphology issues through segmentation (more on this in section 3) and (2) we focus on utilizing syntactic knowledge to address the reordering challenges of this translation direction.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Much work has been done in syntactic reordering for SMT, focusing on both source and targetlanguage syntax.</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this paper, we adapt an approach that utilizes source-syntax information as opposed to target-side syntax systems (Yamada and Knight, 2001; Galley et al., 2004).</text>
              <doc_id>23</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This is because we are translating from English to Arabic and we are discouraged by recent results indicating Arabic parsing is not at a stage that makes it usable in MT (Habash et al., 2006).</text>
              <doc_id>24</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>While several recent authors using a pre-translation (sourceside) reordering approach have achieved positive results, it has been difficult to integrate syntactic</text>
              <doc_id>25</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>information while retaining the strengths of the statistical approach.</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In some studies, reordering decisions are done &#8220;deterministically&#8221; by supplying the decoder with a canonical word order (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Habash, 2007).</text>
              <doc_id>27</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>These reordering rules are either manually specified or automatically learned from alignments; and they are always placed outside the actual PSMT system.</text>
              <doc_id>28</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>By contrast, other studies (Crego and Mari&#241;o, 2007; Zhang et al., 2007; Li et al., 2007; Elming, 2008) are more in the spirit of PSMT, in that multiple reorderings are presented to the PSMT system as (possibly weighted) options that are allowed to contribute alongside other parameters.</text>
              <doc_id>29</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Specifically, we follow the pre-translation reordering approach of Elming (2008).</text>
              <doc_id>30</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This approach has been proven to remedy shortcomings of other pre-translation reordering approaches by reordering the input word sequence, but scoring the output word sequence.</text>
              <doc_id>31</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Elming (2008) only examined the approach within English &#8211; Danish, a language pair that displays little reordering.</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>By contrast, in this paper, we target the more demanding reordering task of translating between two distant languages, English and Arabic.</text>
              <doc_id>33</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>While much work has been done on Arabic to English MT (Habash and Sadat, 2006; Lee, 2004) mostly focusing on addressing the problems caused by the rich morphology of Arabic, we handle the less described translation direction: English to Arabic.</text>
              <doc_id>34</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Recently, there are some new publications on English to Arabic MT.</text>
              <doc_id>35</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Sarikaya and Deng (2007) use joint morphological-lexical language models to re-rank the output of English dialectal-Arabic MT, and Badr et al. (2008) report results on the value of the morphological decomposition of Arabic during training and describe different techniques for re-composition of Arabic in the output.</text>
              <doc_id>36</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We differ from the previous efforts targeting Arabic in that (1) we do not address morphology issues through segmentation (more on this in section 3) and (2) we focus on utilizing syntactic knowledge to address the reordering challenges of this translation direction.</text>
              <doc_id>37</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Arabic Syntactic Issues</title>
        <text>Arabic is a morphologically and syntactically complex language with many differences from English. Arabic morphology has been well studied in the context of MT. Previous results all suggest that some degree of tokenization is helpful when translating from Arabic (Habash and Sadat, 2006; Lee, 2004). However, when translating into a morphologically rich language, target tokenization means that the translation process is broken into multiple steps (Badr et al., 2008). For our experiments, Arabic was not segmented apart from simple punctuation tokenization. This low level of segmentation was maintained in order to agree with the segmentation provided in the manually aligned corpus we used to learn our rules (section 6.1). We found no simple means for transferring the manual alignments to more segmented language. We expect that better performance would be achieved by introducing more Arabic segmentation as reported by Badr et al. (2008). 1 As such, and unlike previous work in PSMT translating into Arabic, we focus here on syntax. We plan to investigate different tokenization schemes for syntactic preprocessing in future work. Next, we describe three prominent English- Arabic syntactic phenomena that have motivated some of our decisions in this paper.
First is verb-subject order. Arabic verb subjects may be: (a.) pro-dropped (verb conjugated), (b.) pre-verbal (SVO), or (c.) post-verbal (VSO). Although the English SVO order is possible in Arabic, it is not always preferred, especially when the subject is particularly long. Unfortunately, this is the harder case for PSMT to handle. For small subject noun phrases (NP), PSMT might be able to handle the reordering in the phrase table if the verb and subject were seen in training. But this becomes much less likely with very long NPs that exceed the size of the phrases in a phrase table. The example in Figure 1 illustrates this point. Bolding and italics are used to mark the verb and subordinating conjunction that surround the subject NP (19 tokens) in English and what they map to in Arabic, respectively. 2
Secondly, Arabic adjectival modifiers typically follow their nouns with the exception of some superlative adjectives. However, English adjectival modifiers can follow or precede their nouns depending on the size of the adjectival phrase: single word adjectives precede but multi-word adjectives phrases follow (or precede while hyphenated). For example, a tall man translates as &#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533; &#65533; rjl
1 Our results are not comparable to their results, since they
report on non-standard data sets. 2 All Arabic transliterations in this paper are provided in
the Habash-Soudi-Buckwalter scheme (Habash et al., 2007).
[NP-SBJ The general coordinator of the railroad project among the countries of the Gulf Cooperation Council , Hamid Khaja ,] [V announced] [SUB that ...]
[&#65533;&#65533;&#65533; &#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533; &#65533; &#65533; &#65533; &#65533; &#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; NP-SBJ] [ &#65533;&#65533;&#65533;&#65533;&#65533; V] [&#65533;&#65533;&#65533; &#65533;&#65533;&#65533; SUB]
[V A&#962;ln] [NP-SBJ Almnsq Al&#962;Am lm&#353;rw&#962; Alsk&#65533; AlHdyd byn dwl mjls Alt&#962;Awn Alxlyjy HAmd xAjh] [SUB An ...]
Twyl &#8216;man tall&#8217;; however, the English phrase a man tall of stature translates with no reordering as
&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533; &#65533; rjl Twyl AlqAm&#65533; &#8216;man tall thestature&#8217;. So does the superlative the tallest man translating into &#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533; ATwl rjl &#8216;tallest man.&#8217; Finally, Arabic has one syntactic construction, called Idafa, for indicating possession and compounding, while English has three. The Idafa construction typically consists of one or more indefinite nouns followed by a definite noun. For example, the English phrases the car keys, the car&#8217;s keys and the keys of the car all translate into the Arabic &#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533; &#65533; &#65533;&#65533; &#65533; &#65533;&#65533; mfAtyH AlsyAr&#65533; &#8216;keys thecar.&#8217; Only one of the three English constructions does not require content word reordering.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Arabic is a morphologically and syntactically complex language with many differences from English.</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Arabic morphology has been well studied in the context of MT.</text>
              <doc_id>39</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Previous results all suggest that some degree of tokenization is helpful when translating from Arabic (Habash and Sadat, 2006; Lee, 2004).</text>
              <doc_id>40</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>However, when translating into a morphologically rich language, target tokenization means that the translation process is broken into multiple steps (Badr et al., 2008).</text>
              <doc_id>41</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>For our experiments, Arabic was not segmented apart from simple punctuation tokenization.</text>
              <doc_id>42</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This low level of segmentation was maintained in order to agree with the segmentation provided in the manually aligned corpus we used to learn our rules (section 6.1).</text>
              <doc_id>43</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We found no simple means for transferring the manual alignments to more segmented language.</text>
              <doc_id>44</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>We expect that better performance would be achieved by introducing more Arabic segmentation as reported by Badr et al. (2008).</text>
              <doc_id>45</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>1 As such, and unlike previous work in PSMT translating into Arabic, we focus here on syntax.</text>
              <doc_id>46</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>We plan to investigate different tokenization schemes for syntactic preprocessing in future work.</text>
              <doc_id>47</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Next, we describe three prominent English- Arabic syntactic phenomena that have motivated some of our decisions in this paper.</text>
              <doc_id>48</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>First is verb-subject order.</text>
              <doc_id>49</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Arabic verb subjects may be: (a.</text>
              <doc_id>50</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>) pro-dropped (verb conjugated), (b.</text>
              <doc_id>51</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>) pre-verbal (SVO), or (c.</text>
              <doc_id>52</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>) post-verbal (VSO).</text>
              <doc_id>53</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Although the English SVO order is possible in Arabic, it is not always preferred, especially when the subject is particularly long.</text>
              <doc_id>54</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Unfortunately, this is the harder case for PSMT to handle.</text>
              <doc_id>55</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>For small subject noun phrases (NP), PSMT might be able to handle the reordering in the phrase table if the verb and subject were seen in training.</text>
              <doc_id>56</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>But this becomes much less likely with very long NPs that exceed the size of the phrases in a phrase table.</text>
              <doc_id>57</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>The example in Figure 1 illustrates this point.</text>
              <doc_id>58</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Bolding and italics are used to mark the verb and subordinating conjunction that surround the subject NP (19 tokens) in English and what they map to in Arabic, respectively.</text>
              <doc_id>59</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>2</text>
              <doc_id>60</doc_id>
              <sec_id>11</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Secondly, Arabic adjectival modifiers typically follow their nouns with the exception of some superlative adjectives.</text>
              <doc_id>61</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, English adjectival modifiers can follow or precede their nouns depending on the size of the adjectival phrase: single word adjectives precede but multi-word adjectives phrases follow (or precede while hyphenated).</text>
              <doc_id>62</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For example, a tall man translates as &#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533; &#65533; rjl</text>
              <doc_id>63</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 Our results are not comparable to their results, since they</text>
              <doc_id>64</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>report on non-standard data sets.</text>
              <doc_id>65</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2 All Arabic transliterations in this paper are provided in</text>
              <doc_id>66</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the Habash-Soudi-Buckwalter scheme (Habash et al., 2007).</text>
              <doc_id>67</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>[NP-SBJ The general coordinator of the railroad project among the countries of the Gulf Cooperation Council , Hamid Khaja ,] [V announced] [SUB that ...]</text>
              <doc_id>68</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>[&#65533;&#65533;&#65533; &#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533; &#65533; &#65533; &#65533; &#65533; &#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533; NP-SBJ] [ &#65533;&#65533;&#65533;&#65533;&#65533; V] [&#65533;&#65533;&#65533; &#65533;&#65533;&#65533; SUB]</text>
              <doc_id>69</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>[V A&#962;ln] [NP-SBJ Almnsq Al&#962;Am lm&#353;rw&#962; Alsk&#65533; AlHdyd byn dwl mjls Alt&#962;Awn Alxlyjy HAmd xAjh] [SUB An ...]</text>
              <doc_id>70</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Twyl &#8216;man tall&#8217;; however, the English phrase a man tall of stature translates with no reordering as</text>
              <doc_id>71</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#65533;&#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533; &#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533; &#65533; rjl Twyl AlqAm&#65533; &#8216;man tall thestature&#8217;.</text>
              <doc_id>72</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>So does the superlative the tallest man translating into &#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533; ATwl rjl &#8216;tallest man.&#8217; Finally, Arabic has one syntactic construction, called Idafa, for indicating possession and compounding, while English has three.</text>
              <doc_id>73</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The Idafa construction typically consists of one or more indefinite nouns followed by a definite noun.</text>
              <doc_id>74</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>For example, the English phrases the car keys, the car&#8217;s keys and the keys of the car all translate into the Arabic &#65533; &#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533;&#65533; &#65533;&#65533;&#65533; &#65533; &#65533;&#65533; &#65533; &#65533;&#65533; mfAtyH AlsyAr&#65533; &#8216;keys thecar.&#8217; Only one of the three English constructions does not require content word reordering.</text>
              <doc_id>75</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Reordering rules</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>76</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Definition of reordering</title>
            <text>Following Elming (2008), we define reordering as two word sequences, left sequence (LS) and right sequence (RS), exchanging positions. These two sequences are restricted by being parallel consecutive, maximal and adjacent. The sequences are not restricted in length, making both short and long distance reordering possible. Furthermore, they need not be phrases in the sense that they appear as an entry in the phrase table.
Figure 2 illustrates reordering in a word alignment matrix. The matrix contains reorderings between the light grey sequences (s 3 2 and s6 4 )3 and
3 Notation: s y x means the consecutive source sequence
the dark grey sequences (s 5 5 and s6 6 ). On the other hand, the sequences s 3 3 and s5 4 are not considered for reordering, since neither one is maximal, and s 5 4 is not consecutive on the target side.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Following Elming (2008), we define reordering as two word sequences, left sequence (LS) and right sequence (RS), exchanging positions.</text>
                  <doc_id>77</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These two sequences are restricted by being parallel consecutive, maximal and adjacent.</text>
                  <doc_id>78</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The sequences are not restricted in length, making both short and long distance reordering possible.</text>
                  <doc_id>79</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Furthermore, they need not be phrases in the sense that they appear as an entry in the phrase table.</text>
                  <doc_id>80</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 2 illustrates reordering in a word alignment matrix.</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The matrix contains reorderings between the light grey sequences (s 3 2 and s6 4 )3 and</text>
                  <doc_id>82</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3 Notation: s y x means the consecutive source sequence</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the dark grey sequences (s 5 5 and s6 6 ).</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>On the other hand, the sequences s 3 3 and s5 4 are not considered for reordering, since neither one is maximal, and s 5 4 is not consecutive on the target side.</text>
                  <doc_id>85</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Learning rules</title>
            <text>Table 1 contains an example of the features available to the algorithm learning reordering rules. We include features for the candidate reordering sequences (LS and RS) and for their possible left (LC) and right (RC) contexts. In addition to words and parts-of-speech (POS), we provide phrase structure (PS) sequences and subordination information (SUBORD). The PS sequence is made up of the highest level nodes in the syntax tree that cover the words of the current sequence and only these. Subordinate information can also be extracted from the syntax tree. A subordinate clause is defined as inside an SBAR constituent; otherwise it is a main clause. Our intuition is that all these features will allow us to learn the best rules possible to address the phenomena discussed in section 3 at the right level of generality. In order to minimize the amount of training data, word and POS sequences are annotated as too long (T/L) if they are longer than 4 words, and the same for phrase structure (PS) sequences if they are longer than 3 units. A feature vector is only used if at least one of these three levels is not T/L for both LS and RS, and T/L contexts are not included in the set. This does not constrain the possible length of a reordering, since a PS sequence of length 1 can cover an entire sentence. In the example in Table 1, LS and RS are single words, but they are not restricted in length. The span of the contexts varies from a single neighboring word to all the way to the sentence border. In the example, LS and RS should be reordered, since adjectives appear as post-modifiers in Arabic.
In order to learn rules from the annotated data, we use a rule-based classifier, Ripper (Cohen,
covering word positions x to y.
1996). The motivation for using Ripper is that it allows features to be sets of strings, which fits well with our representation of the context, and it produces easily readable rules that allow better understanding of the decisions being made. In section 6.3, extracted rules are exemplified and analyzed.
The probabilities of the rules are estimated using Maximum Likelihood Estimation based on the information supplied by Ripper on the performance of the individual rules on the training data. These logarithmic probabilities are easily integratable in the log-linear PSMT model as an additional parameter by simple addition.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Table 1 contains an example of the features available to the algorithm learning reordering rules.</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We include features for the candidate reordering sequences (LS and RS) and for their possible left (LC) and right (RC) contexts.</text>
                  <doc_id>87</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In addition to words and parts-of-speech (POS), we provide phrase structure (PS) sequences and subordination information (SUBORD).</text>
                  <doc_id>88</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The PS sequence is made up of the highest level nodes in the syntax tree that cover the words of the current sequence and only these.</text>
                  <doc_id>89</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Subordinate information can also be extracted from the syntax tree.</text>
                  <doc_id>90</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>A subordinate clause is defined as inside an SBAR constituent; otherwise it is a main clause.</text>
                  <doc_id>91</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Our intuition is that all these features will allow us to learn the best rules possible to address the phenomena discussed in section 3 at the right level of generality.</text>
                  <doc_id>92</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>In order to minimize the amount of training data, word and POS sequences are annotated as too long (T/L) if they are longer than 4 words, and the same for phrase structure (PS) sequences if they are longer than 3 units.</text>
                  <doc_id>93</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>A feature vector is only used if at least one of these three levels is not T/L for both LS and RS, and T/L contexts are not included in the set.</text>
                  <doc_id>94</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
                <sentence>
                  <text>This does not constrain the possible length of a reordering, since a PS sequence of length 1 can cover an entire sentence.</text>
                  <doc_id>95</doc_id>
                  <sec_id>9</sec_id>
                </sentence>
                <sentence>
                  <text>In the example in Table 1, LS and RS are single words, but they are not restricted in length.</text>
                  <doc_id>96</doc_id>
                  <sec_id>10</sec_id>
                </sentence>
                <sentence>
                  <text>The span of the contexts varies from a single neighboring word to all the way to the sentence border.</text>
                  <doc_id>97</doc_id>
                  <sec_id>11</sec_id>
                </sentence>
                <sentence>
                  <text>In the example, LS and RS should be reordered, since adjectives appear as post-modifiers in Arabic.</text>
                  <doc_id>98</doc_id>
                  <sec_id>12</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In order to learn rules from the annotated data, we use a rule-based classifier, Ripper (Cohen,</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>covering word positions x to y.</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1996).</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The motivation for using Ripper is that it allows features to be sets of strings, which fits well with our representation of the context, and it produces easily readable rules that allow better understanding of the decisions being made.</text>
                  <doc_id>102</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In section 6.3, extracted rules are exemplified and analyzed.</text>
                  <doc_id>103</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The probabilities of the rules are estimated using Maximum Likelihood Estimation based on the information supplied by Ripper on the performance of the individual rules on the training data.</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These logarithmic probabilities are easily integratable in the log-linear PSMT model as an additional parameter by simple addition.</text>
                  <doc_id>105</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 The PSMT system</title>
        <text>Our baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002). The decoder used for the baseline system is Pharaoh (Koehn, 2004) with its distance-penalizing reordering model. Since Pharaoh does not support word lattice input, we use our own decoder for the experiments. Except for the reordering model, it uses the same knowledge sources as Pharaoh, i.e. a bidirectional phrase translation model, a lexical weight model, phrase and word penalties, and a target language model. Its behavior is comparable to Pharaoh when doing monotone decoding.
The search algorithm of our decoder is similar to the RG graph decoder of (Zens et al., 2002). It expects a word lattice as input. Figure 3 shows the word lattice for the example in table 2. In the example used here, we choose to focus on the reordering of adjective and noun. For readability, we do not describe the possibility of reordering the subject and verb. This will also be the case in later use of the example.
Since the input format defines all possible word orders allowed by the rule set, a simple monotone search is sufficient. Using a language model of order n, for each hypothesized target string ending in the same n-1-gram, we only have to extend the highest scoring hypothesis. None of the others can possibly outperform this one later on. This is because the maximal context evaluating a phrase extending this hypothesis, is the history (n-1-gram) of the first word of that phrase. The decoder is not able to look any further back at the preceding string.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Our baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002).</text>
              <doc_id>106</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The decoder used for the baseline system is Pharaoh (Koehn, 2004) with its distance-penalizing reordering model.</text>
              <doc_id>107</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Since Pharaoh does not support word lattice input, we use our own decoder for the experiments.</text>
              <doc_id>108</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Except for the reordering model, it uses the same knowledge sources as Pharaoh, i.e. a bidirectional phrase translation model, a lexical weight model, phrase and word penalties, and a target language model.</text>
              <doc_id>109</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Its behavior is comparable to Pharaoh when doing monotone decoding.</text>
              <doc_id>110</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The search algorithm of our decoder is similar to the RG graph decoder of (Zens et al., 2002).</text>
              <doc_id>111</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It expects a word lattice as input.</text>
              <doc_id>112</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Figure 3 shows the word lattice for the example in table 2.</text>
              <doc_id>113</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In the example used here, we choose to focus on the reordering of adjective and noun.</text>
              <doc_id>114</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>For readability, we do not describe the possibility of reordering the subject and verb.</text>
              <doc_id>115</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This will also be the case in later use of the example.</text>
              <doc_id>116</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Since the input format defines all possible word orders allowed by the rule set, a simple monotone search is sufficient.</text>
              <doc_id>117</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Using a language model of order n, for each hypothesized target string ending in the same n-1-gram, we only have to extend the highest scoring hypothesis.</text>
              <doc_id>118</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>None of the others can possibly outperform this one later on.</text>
              <doc_id>119</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This is because the maximal context evaluating a phrase extending this hypothesis, is the history (n-1-gram) of the first word of that phrase.</text>
              <doc_id>120</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The decoder is not able to look any further back at the preceding string.</text>
              <doc_id>121</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 The reordering approach</title>
            <text>Similar to Elming (2008), the integration of the rule-based reordering in our PSMT system is carried out in two separate stages:
1. Reordering the source sentence to assimilate the word order of the target language.
2. Weighting of the target word order according to the rules.
Stage (1) is done in a non-deterministic fashion by generating a word lattice as input. This way, the system has both the original word order, and the reorderings predicted by the rule set. The different paths of the word lattice are merely given as equal suggestions to the decoder. They are in no way individually weighted.
Separating stage (2) from stage (1) is motivated by the fact that reordering can have two distinct origins. They can occur because of stage (1), i.e. the lattice reordering of the original English word order (phrase external reordering), and they can occur inside a single phrase (phrase internal reordering). The focus of this approach lies in doing phrase-independent word reordering. Rulepredicted reorderings should be promoted regardless of whether they owe their existence to a syntactic rule or a phrase table entry.
This is accomplished by letting the actual scoring of the reordering focus on the target string.
The decoder is informed of where a rule has predicted a reordering, how much it costs to do the reordering, and how much it costs to avoid it. This is then checked for each hypothesized target string via a word alignment.
The word alignment keeps track of which source position the word in each target position originates from. In order to access this information, each phrase table entry is annotated with its internal word alignment, which is available as an intermediate product from phrase table creation. If a phrase pair has multiple word alignments, the most frequent one is chosen.
Table 2 exemplifies the scoring approach, again with focus on the adjective-noun reordering. The source sentence is &#8216;he bought new books today&#8217;, and a rule has predicted that source word 3 and 4 should change place. Due to the pro-drop nature of Arabic, the first Arabic word is linked to the two first English words (1+2). When the decoder has covered the first four input words, two of the hypothesis target strings might be H1 and H2. At this point, it becomes apparent that H2 contains the desired reordering (namely what corresponds to source word order &#8216;4 3&#8217;), and it get assigned the reordering cost. H1 does not contain the rule-suggested reordering (instead, the words are in the original order &#8216;3 4&#8217;), and it gets the violation cost. Both these scorings are performed in a phrase-independent manner. The decoder assigns the reordering cost to H2 without knowing whether the reordering is internal (due to a phrase table entry) or external (due to a syntactic rule).
Phrase internal reorderings at other points of the sentence, i.e. points that are not covered by a rule, are not judged by the reordering model. Our rule extraction does not learn every possible reordering between the two languages, but only the most general ones. If no rule has an opinion at a certain point in a sentence, the decoder is free to choose the phrase translation it prefers without reordering cost.
Separating the scoring from the source language reordering also has the advantage that the approach in essence is compatible with other approaches such as a traditional PSMT system (Koehn et al., 2003b) or a hierarchical phrase system (Chiang, 2005). We will, however, not examine this possibility further in the present paper.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Similar to Elming (2008), the integration of the rule-based reordering in our PSMT system is carried out in two separate stages:</text>
                  <doc_id>122</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1.</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Reordering the source sentence to assimilate the word order of the target language.</text>
                  <doc_id>124</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2.</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Weighting of the target word order according to the rules.</text>
                  <doc_id>126</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Stage (1) is done in a non-deterministic fashion by generating a word lattice as input.</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This way, the system has both the original word order, and the reorderings predicted by the rule set.</text>
                  <doc_id>128</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The different paths of the word lattice are merely given as equal suggestions to the decoder.</text>
                  <doc_id>129</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>They are in no way individually weighted.</text>
                  <doc_id>130</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Separating stage (2) from stage (1) is motivated by the fact that reordering can have two distinct origins.</text>
                  <doc_id>131</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>They can occur because of stage (1), i.e. the lattice reordering of the original English word order (phrase external reordering), and they can occur inside a single phrase (phrase internal reordering).</text>
                  <doc_id>132</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The focus of this approach lies in doing phrase-independent word reordering.</text>
                  <doc_id>133</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Rulepredicted reorderings should be promoted regardless of whether they owe their existence to a syntactic rule or a phrase table entry.</text>
                  <doc_id>134</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>This is accomplished by letting the actual scoring of the reordering focus on the target string.</text>
                  <doc_id>135</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The decoder is informed of where a rule has predicted a reordering, how much it costs to do the reordering, and how much it costs to avoid it.</text>
                  <doc_id>136</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This is then checked for each hypothesized target string via a word alignment.</text>
                  <doc_id>137</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The word alignment keeps track of which source position the word in each target position originates from.</text>
                  <doc_id>138</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In order to access this information, each phrase table entry is annotated with its internal word alignment, which is available as an intermediate product from phrase table creation.</text>
                  <doc_id>139</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>If a phrase pair has multiple word alignments, the most frequent one is chosen.</text>
                  <doc_id>140</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 2 exemplifies the scoring approach, again with focus on the adjective-noun reordering.</text>
                  <doc_id>141</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The source sentence is &#8216;he bought new books today&#8217;, and a rule has predicted that source word 3 and 4 should change place.</text>
                  <doc_id>142</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Due to the pro-drop nature of Arabic, the first Arabic word is linked to the two first English words (1+2).</text>
                  <doc_id>143</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>When the decoder has covered the first four input words, two of the hypothesis target strings might be H1 and H2.</text>
                  <doc_id>144</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>At this point, it becomes apparent that H2 contains the desired reordering (namely what corresponds to source word order &#8216;4 3&#8217;), and it get assigned the reordering cost.</text>
                  <doc_id>145</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>H1 does not contain the rule-suggested reordering (instead, the words are in the original order &#8216;3 4&#8217;), and it gets the violation cost.</text>
                  <doc_id>146</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Both these scorings are performed in a phrase-independent manner.</text>
                  <doc_id>147</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>The decoder assigns the reordering cost to H2 without knowing whether the reordering is internal (due to a phrase table entry) or external (due to a syntactic rule).</text>
                  <doc_id>148</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Phrase internal reorderings at other points of the sentence, i.e. points that are not covered by a rule, are not judged by the reordering model.</text>
                  <doc_id>149</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Our rule extraction does not learn every possible reordering between the two languages, but only the most general ones.</text>
                  <doc_id>150</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>If no rule has an opinion at a certain point in a sentence, the decoder is free to choose the phrase translation it prefers without reordering cost.</text>
                  <doc_id>151</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Separating the scoring from the source language reordering also has the advantage that the approach in essence is compatible with other approaches such as a traditional PSMT system (Koehn et al., 2003b) or a hierarchical phrase system (Chiang, 2005).</text>
                  <doc_id>152</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We will, however, not examine this possibility further in the present paper.</text>
                  <doc_id>153</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>6</index>
        <title>6 Evaluation</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>154</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>6.1 Data</title>
            <text>We learn the reordering rules from the IBM Arabic-English aligned corpus (IBMAC) (Ittycheriah and Roukos, 2005). Of its total 13.9K sentence pairs, we only use 8.8K sentences because the rest of the corpus uses different normalizations for numerals that make the two sets incompatible. 6.6K of the sentences (179K English and 146K Arabic words) are used to learn rule, while the rest are used for development purposes. In addition to the manual alignment supplied with these data, we create an automatic word alignment for them using GIZA++ (Och and Ney, 2003) and the grow-diagfinal (GDF) symmetrization algorithm (Koehn et al., 2005). This was done together with the data used to train the MT system. The English side is parsed using a state-of-the-art statistical English parser (Charniak, 2000). Two rule sets are learned based on the manual alignments (MAN) and the automatic alignments (GDF).
The MT system is trained on a corpus consisting of 126K sentences with 4.2M English and 3.3M Arabic words in simple tokenization scheme. The domain is newswire (LDC- NEWS) taken from Arabic News (LDC2004T17), eTIRR (LDC2004E72), English translation of Arabic Treebank (LDC2005E46), and Ummah (LDC2004T18). Although there are additional corpora available, we restricted ourselves to this set to allow for a fast development cycle. We plan to extend the data size in the future. The Arabic language model is trained on the 5.4M sentences (133M words) of newswire text in the 1994 to 1996 part of the Arabic Gigaword corpus. We restricted ourselves to this part, since we are not able to run Pharaoh with a larger language model. 4
For test data, we used NIST MTEval test sets from 2004 (MT04) and 2005 (MT05) 5 . Since these data sets are created for Arabic-English evaluation with four English reference sentences for
4 All of the training data we use is available from the Linguistic Data Consortium (LDC): http://www.ldc.upenn.edu/. 5 http://www.nist.gov/speech/tests/mt/
each Arabic sentence, we invert the sets by concatenating all English sentences to one file. This means that the Arabic reference file contains four duplicates of each sentence. Each duplicate is the reference of a different English source sentence. Following this merger, MT04 consists of 5.4K sentences with 193K English and 144K Arabic words, and MT05 consists of 4.2K sentences with 143K English and 114K Arabic words. MT04 is a mix of domains containing speeches, editorials and newswire texts. On the other hand, MT05 is only newswire.
The NIST MTEval test set from 2002 (MT02) is split into a tuning set for optimizing decoder parameter weights and a development set for ongoing experimentation. The same merging procedure as for MT04 and MT05 is employed. This results in a tune set of 1.0K sentences with 34K English and 26K Arabic words, and a development set of 3.1K sentences with 102K English and 79K Arabic words.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We learn the reordering rules from the IBM Arabic-English aligned corpus (IBMAC) (Ittycheriah and Roukos, 2005).</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Of its total 13.9K sentence pairs, we only use 8.8K sentences because the rest of the corpus uses different normalizations for numerals that make the two sets incompatible.</text>
                  <doc_id>156</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>6.6K of the sentences (179K English and 146K Arabic words) are used to learn rule, while the rest are used for development purposes.</text>
                  <doc_id>157</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In addition to the manual alignment supplied with these data, we create an automatic word alignment for them using GIZA++ (Och and Ney, 2003) and the grow-diagfinal (GDF) symmetrization algorithm (Koehn et al., 2005).</text>
                  <doc_id>158</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This was done together with the data used to train the MT system.</text>
                  <doc_id>159</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The English side is parsed using a state-of-the-art statistical English parser (Charniak, 2000).</text>
                  <doc_id>160</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Two rule sets are learned based on the manual alignments (MAN) and the automatic alignments (GDF).</text>
                  <doc_id>161</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The MT system is trained on a corpus consisting of 126K sentences with 4.2M English and 3.3M Arabic words in simple tokenization scheme.</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The domain is newswire (LDC- NEWS) taken from Arabic News (LDC2004T17), eTIRR (LDC2004E72), English translation of Arabic Treebank (LDC2005E46), and Ummah (LDC2004T18).</text>
                  <doc_id>163</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Although there are additional corpora available, we restricted ourselves to this set to allow for a fast development cycle.</text>
                  <doc_id>164</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We plan to extend the data size in the future.</text>
                  <doc_id>165</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The Arabic language model is trained on the 5.4M sentences (133M words) of newswire text in the 1994 to 1996 part of the Arabic Gigaword corpus.</text>
                  <doc_id>166</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We restricted ourselves to this part, since we are not able to run Pharaoh with a larger language model.</text>
                  <doc_id>167</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>4</text>
                  <doc_id>168</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For test data, we used NIST MTEval test sets from 2004 (MT04) and 2005 (MT05) 5 .</text>
                  <doc_id>169</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Since these data sets are created for Arabic-English evaluation with four English reference sentences for</text>
                  <doc_id>170</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4 All of the training data we use is available from the Linguistic Data Consortium (LDC): http://www.ldc.upenn.edu/.</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>5 http://www.nist.gov/speech/tests/mt/</text>
                  <doc_id>172</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>each Arabic sentence, we invert the sets by concatenating all English sentences to one file.</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This means that the Arabic reference file contains four duplicates of each sentence.</text>
                  <doc_id>174</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Each duplicate is the reference of a different English source sentence.</text>
                  <doc_id>175</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Following this merger, MT04 consists of 5.4K sentences with 193K English and 144K Arabic words, and MT05 consists of 4.2K sentences with 143K English and 114K Arabic words.</text>
                  <doc_id>176</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>MT04 is a mix of domains containing speeches, editorials and newswire texts.</text>
                  <doc_id>177</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>On the other hand, MT05 is only newswire.</text>
                  <doc_id>178</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The NIST MTEval test set from 2002 (MT02) is split into a tuning set for optimizing decoder parameter weights and a development set for ongoing experimentation.</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The same merging procedure as for MT04 and MT05 is employed.</text>
                  <doc_id>180</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This results in a tune set of 1.0K sentences with 34K English and 26K Arabic words, and a development set of 3.1K sentences with 102K English and 79K Arabic words.</text>
                  <doc_id>181</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>6.2 Results and discussion</title>
            <text>The reordering approach is evaluated on the MT04 and MT05 test sets. Results are listed in table 3 along with results on the development set. We report on (a) Pharaoh with no restriction on reordering (Pharaoh Free), (b) Pharaoh with distortion limit 4 (Pharaoh DL4), (c) Pharaoh with monotone decoding (Pharaoh Monotone), and (d) a system provided with a rule reordered word lattice but no (NO) weighting in the spirit of (Crego and Mari&#241;o, 2007), (e) the same system but with a source order
(SO) weighting in the spirit of (Zhang et al., 2007; Li et al., 2007), and finally (f) the same system but with the target order (TO) weighting.
In addition to evaluating the reordering approaches, we also report on supplying them with different reordering rule sets: a set that was learned on manually aligned data (MAN), and a set learned on the same data but with automatic alignments (GDF).
6.2.1 Overall Results
Pharaoh Monotone performs similarly to Pharaoh Free. This shows that the question of improved reordering is not about quantity, but rather quality: what constraints are optimal to generate the best word order. The TO approach gets an increase over Pharaoh Free of 1.3 and 1.6 %BLEU on the test sets, and 0.2 and 0.5 %BLEU over Pharaoh DL4. Improvement is less noticeable over the other pre-translation reordering approaches (NO and SO). A possible explanation is that the rules do not apply very often, in combination with the fact that the approaches often behave alike. The difference in SO and TO scoring only leads to a difference in translation in &#8764;14% of the sentences. This set, the diff set, is interesting, since it provides a focus on the difference between these approaches. In table 4, we evaluate on this set.
6.2.2 Diff Set
Overall the TO approach seems to be a superior reordering method. To back this observation, 50 sentences of MT04 are manually evaluated by a native speaker of Arabic. Callison-Burch et al. (2007) show that ranking sentences gives higher inter-annotator agreement than scoring adequacy and fluency. We therefore employ this evaluation method, asking the evaluator to rank sentences from four of the systems given the input sentence. Ties are allowed. Table 4 shows the average rat-
ings of the systems. This shows the TO scoring to be significantly superior to the other methods (p &lt; 0.01 using Wilcoxon signed-rank testing).
6.2.3 MAN vs GDF
Another interesting observation is that reordering rules learned from automatic alignments lead to significantly better translation than rules learned from manual alignment. Due to the much higher quality of the manual alignment, the opposite might be expected. However, this may be just a variant on the observation that alignment improvements (measured against human references) seldom lead to MT improvements (Lopez and Resnik, 2006). The MAN alignments may in fact be better than GDF, but they are most certainly more different in nature from real alignment than the GDF alignments are. As such, the MAN alignments are not as powerful as we would have liked them to be. In our data sets, the GDF rules, seem less specific, and they therefore apply more frequently than the MAN rules. On average, this results in more than 7 times as many possible reordering paths per sentence. This means that the GDF rules supply the decoder with a larger search space, which in turn means more proposed translation hypotheses. This may play a big part in the effect of the rule sets.
6.2.4 Reordering Choices
Table 5 shows the reordering choices made by the approaches in decoding. Most noticeable is that the SO approach is strongly biased against phrase internal reorderings; TO uses more than 30 times as many phrase internal reorderings as SO. In addition, TO is less likely to reject a rule proposed reordering. The 50 sentences from the manual evaluation are also manually analyzed with regards to reordering. For each reordering in these sentences, the four systems are ranked according to how well the area affected by the reordering is translated. This indicates that the SO approach&#8217;s bias against phrase internal reorderings may hurt performance. 25% of the time, when SO chooses an external reordering, while the TO approach chooses an internal reordering, the TO approach gets a better translation. Only in 7% of the cases is it the other way around.
Another discovery from the analysis is when TO chooses an internal reordering and NO rejects the reordering. Here, TO leads to a better translation 45% of the time, while NO never outperforms TO. In these cases, either approach has used a phrase to cover the area, but via rule-based motivation, TO has forced a less likely phrase with the correct word order through. This clearly shows that local reordering is not handled sufficiently by phrase internal reordering alone. These need to be controlled too.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The reordering approach is evaluated on the MT04 and MT05 test sets.</text>
                  <doc_id>182</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Results are listed in table 3 along with results on the development set.</text>
                  <doc_id>183</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We report on (a) Pharaoh with no restriction on reordering (Pharaoh Free), (b) Pharaoh with distortion limit 4 (Pharaoh DL4), (c) Pharaoh with monotone decoding (Pharaoh Monotone), and (d) a system provided with a rule reordered word lattice but no (NO) weighting in the spirit of (Crego and Mari&#241;o, 2007), (e) the same system but with a source order</text>
                  <doc_id>184</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(SO) weighting in the spirit of (Zhang et al., 2007; Li et al., 2007), and finally (f) the same system but with the target order (TO) weighting.</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In addition to evaluating the reordering approaches, we also report on supplying them with different reordering rule sets: a set that was learned on manually aligned data (MAN), and a set learned on the same data but with automatic alignments (GDF).</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>6.2.1 Overall Results</text>
                  <doc_id>187</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Pharaoh Monotone performs similarly to Pharaoh Free.</text>
                  <doc_id>188</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This shows that the question of improved reordering is not about quantity, but rather quality: what constraints are optimal to generate the best word order.</text>
                  <doc_id>189</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The TO approach gets an increase over Pharaoh Free of 1.3 and 1.6 %BLEU on the test sets, and 0.2 and 0.5 %BLEU over Pharaoh DL4.</text>
                  <doc_id>190</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Improvement is less noticeable over the other pre-translation reordering approaches (NO and SO).</text>
                  <doc_id>191</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>A possible explanation is that the rules do not apply very often, in combination with the fact that the approaches often behave alike.</text>
                  <doc_id>192</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The difference in SO and TO scoring only leads to a difference in translation in &#8764;14% of the sentences.</text>
                  <doc_id>193</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>This set, the diff set, is interesting, since it provides a focus on the difference between these approaches.</text>
                  <doc_id>194</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>In table 4, we evaluate on this set.</text>
                  <doc_id>195</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>6.2.2 Diff Set</text>
                  <doc_id>196</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Overall the TO approach seems to be a superior reordering method.</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To back this observation, 50 sentences of MT04 are manually evaluated by a native speaker of Arabic.</text>
                  <doc_id>198</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Callison-Burch et al. (2007) show that ranking sentences gives higher inter-annotator agreement than scoring adequacy and fluency.</text>
                  <doc_id>199</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We therefore employ this evaluation method, asking the evaluator to rank sentences from four of the systems given the input sentence.</text>
                  <doc_id>200</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Ties are allowed.</text>
                  <doc_id>201</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Table 4 shows the average rat-</text>
                  <doc_id>202</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>ings of the systems.</text>
                  <doc_id>203</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This shows the TO scoring to be significantly superior to the other methods (p &lt; 0.01 using Wilcoxon signed-rank testing).</text>
                  <doc_id>204</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>6.2.3 MAN vs GDF</text>
                  <doc_id>205</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Another interesting observation is that reordering rules learned from automatic alignments lead to significantly better translation than rules learned from manual alignment.</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Due to the much higher quality of the manual alignment, the opposite might be expected.</text>
                  <doc_id>207</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>However, this may be just a variant on the observation that alignment improvements (measured against human references) seldom lead to MT improvements (Lopez and Resnik, 2006).</text>
                  <doc_id>208</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The MAN alignments may in fact be better than GDF, but they are most certainly more different in nature from real alignment than the GDF alignments are.</text>
                  <doc_id>209</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>As such, the MAN alignments are not as powerful as we would have liked them to be.</text>
                  <doc_id>210</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>In our data sets, the GDF rules, seem less specific, and they therefore apply more frequently than the MAN rules.</text>
                  <doc_id>211</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>On average, this results in more than 7 times as many possible reordering paths per sentence.</text>
                  <doc_id>212</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>This means that the GDF rules supply the decoder with a larger search space, which in turn means more proposed translation hypotheses.</text>
                  <doc_id>213</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>This may play a big part in the effect of the rule sets.</text>
                  <doc_id>214</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>6.2.4 Reordering Choices</text>
                  <doc_id>215</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Table 5 shows the reordering choices made by the approaches in decoding.</text>
                  <doc_id>216</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Most noticeable is that the SO approach is strongly biased against phrase internal reorderings; TO uses more than 30 times as many phrase internal reorderings as SO.</text>
                  <doc_id>217</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, TO is less likely to reject a rule proposed reordering.</text>
                  <doc_id>218</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The 50 sentences from the manual evaluation are also manually analyzed with regards to reordering.</text>
                  <doc_id>219</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>For each reordering in these sentences, the four systems are ranked according to how well the area affected by the reordering is translated.</text>
                  <doc_id>220</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This indicates that the SO approach&#8217;s bias against phrase internal reorderings may hurt performance.</text>
                  <doc_id>221</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>25% of the time, when SO chooses an external reordering, while the TO approach chooses an internal reordering, the TO approach gets a better translation.</text>
                  <doc_id>222</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>Only in 7% of the cases is it the other way around.</text>
                  <doc_id>223</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Another discovery from the analysis is when TO chooses an internal reordering and NO rejects the reordering.</text>
                  <doc_id>224</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Here, TO leads to a better translation 45% of the time, while NO never outperforms TO.</text>
                  <doc_id>225</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In these cases, either approach has used a phrase to cover the area, but via rule-based motivation, TO has forced a less likely phrase with the correct word order through.</text>
                  <doc_id>226</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>This clearly shows that local reordering is not handled sufficiently by phrase internal reordering alone.</text>
                  <doc_id>227</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>These need to be controlled too.</text>
                  <doc_id>228</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>6.3 Rule analysis</title>
            <text>The rule learning resulted in 61 rules based on manual alignments and 39 based on automatic alignments. Of these, the majority handled the placement of adjectives, while only a few handled the placement of the verb.
A few of the rules that were learned from the manual alignment are shown in table 6. The first two rules handle the placement of the finite verb in Arabic. Rule 16 states that if a finite verb appears in front of a subordinate clause, then it should be moved to sentence initial position with a probability of 68%. Due to the restrictions of sequence lengths, it can only swap across maximally 4 words or a sequence of words that is describable by maximally 3 syntactic phrases. The SBAR condition may help restrict the reordering to finite verbs of the main clause. This rule and its probability goes well with the description given in sections 3, since VSO order is not obligatory. The subject may be unexpressed, or it may appear in front of the verb. This is even more obvious in rule 27, which has a probability of only 43%.
Rules 11 and 1 deal with the inverse ordering of adjectives and nouns. The first is general but uncertain, the second is lexicalized and certain. The reason for the low probability of rule 11 is primarily that many proper names have been mis-tagged by the parser as either JJ or NN, and to a lesser
extent that the rule should often not apply if the right context is also an NN. Adding the latter restriction narrows the scope of the rule but would have increased the probability to 54%.
Rule 1, on the other hand, has a high probability of 90%. It is only restricted by the condition that the left context should not be an adjective. In these cases, the adjectives should often be moved together, as is the case with &#8216;the south african president&#8217; &#8594; &#65533; &#65533;
&#65533; &#65533;&#65533; &#65533;&#65533; &#65533; &#65533;&#65533; &#65533;&#65533; &#65533; &#65533; &#65533;&#65533;&#65533; &#65533; &#65533;&#65533; &#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533;
Alr&#375;ys Aljnwb Afryqy where &#8216;south african&#8217; is moved to the right of &#8216;president&#8217;.
Finally, rule 37 handles compound nouns. Here a singular noun is moved to the right of a plural noun, if the right context is a preposition, and the left context is neither an adjective nor a singular noun. This rule handles compound nouns, where the modifying function of the first noun often is hard to distinguish from that of an adjective. The left context restrictions server the same purpose as the left context in rule 1; these should often be moved together with the singular noun. The function of the right context is harder to explain, but without this restriction, the rule would have been much less successful; dropping from a probability of 71% to 51%. An overall comparison of the rules produced based on the manual and automatic alignments shows no major difference in quality. This is especially interesting in light of the better translation using the GDF rules. It is also very interesting that it seems possible to get as good rules from the GDF as from the MAN alignments. This is a new result compared to Elming (2008), where results on manual alignments only are reported.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The rule learning resulted in 61 rules based on manual alignments and 39 based on automatic alignments.</text>
                  <doc_id>229</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Of these, the majority handled the placement of adjectives, while only a few handled the placement of the verb.</text>
                  <doc_id>230</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A few of the rules that were learned from the manual alignment are shown in table 6.</text>
                  <doc_id>231</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The first two rules handle the placement of the finite verb in Arabic.</text>
                  <doc_id>232</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Rule 16 states that if a finite verb appears in front of a subordinate clause, then it should be moved to sentence initial position with a probability of 68%.</text>
                  <doc_id>233</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Due to the restrictions of sequence lengths, it can only swap across maximally 4 words or a sequence of words that is describable by maximally 3 syntactic phrases.</text>
                  <doc_id>234</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The SBAR condition may help restrict the reordering to finite verbs of the main clause.</text>
                  <doc_id>235</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>This rule and its probability goes well with the description given in sections 3, since VSO order is not obligatory.</text>
                  <doc_id>236</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The subject may be unexpressed, or it may appear in front of the verb.</text>
                  <doc_id>237</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>This is even more obvious in rule 27, which has a probability of only 43%.</text>
                  <doc_id>238</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Rules 11 and 1 deal with the inverse ordering of adjectives and nouns.</text>
                  <doc_id>239</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The first is general but uncertain, the second is lexicalized and certain.</text>
                  <doc_id>240</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The reason for the low probability of rule 11 is primarily that many proper names have been mis-tagged by the parser as either JJ or NN, and to a lesser</text>
                  <doc_id>241</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>extent that the rule should often not apply if the right context is also an NN.</text>
                  <doc_id>242</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Adding the latter restriction narrows the scope of the rule but would have increased the probability to 54%.</text>
                  <doc_id>243</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Rule 1, on the other hand, has a high probability of 90%.</text>
                  <doc_id>244</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>It is only restricted by the condition that the left context should not be an adjective.</text>
                  <doc_id>245</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In these cases, the adjectives should often be moved together, as is the case with &#8216;the south african president&#8217; &#8594; &#65533; &#65533;</text>
                  <doc_id>246</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#65533; &#65533;&#65533; &#65533;&#65533; &#65533; &#65533;&#65533; &#65533;&#65533; &#65533; &#65533; &#65533;&#65533;&#65533; &#65533; &#65533;&#65533; &#65533;&#65533;&#65533; &#65533; &#65533;&#65533;&#65533;&#65533;</text>
                  <doc_id>247</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Alr&#375;ys Aljnwb Afryqy where &#8216;south african&#8217; is moved to the right of &#8216;president&#8217;.</text>
                  <doc_id>248</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Finally, rule 37 handles compound nouns.</text>
                  <doc_id>249</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Here a singular noun is moved to the right of a plural noun, if the right context is a preposition, and the left context is neither an adjective nor a singular noun.</text>
                  <doc_id>250</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This rule handles compound nouns, where the modifying function of the first noun often is hard to distinguish from that of an adjective.</text>
                  <doc_id>251</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The left context restrictions server the same purpose as the left context in rule 1; these should often be moved together with the singular noun.</text>
                  <doc_id>252</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The function of the right context is harder to explain, but without this restriction, the rule would have been much less successful; dropping from a probability of 71% to 51%.</text>
                  <doc_id>253</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>An overall comparison of the rules produced based on the manual and automatic alignments shows no major difference in quality.</text>
                  <doc_id>254</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>This is especially interesting in light of the better translation using the GDF rules.</text>
                  <doc_id>255</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>It is also very interesting that it seems possible to get as good rules from the GDF as from the MAN alignments.</text>
                  <doc_id>256</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
                <sentence>
                  <text>This is a new result compared to Elming (2008), where results on manual alignments only are reported.</text>
                  <doc_id>257</doc_id>
                  <sec_id>8</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>7</index>
        <title>7 Conclusion and Future Plans</title>
        <text>We have explored the syntactic reordering approach previously presented in (Elming, 2008) within a more distant language pair, English- Arabic. A translation direction that is highly under-represented in MT research, compared to the opposite direction. We achieve significant improvement in translation quality over related approaches, measured by manual as well as automatic evaluations on this task. Thus proving the viability of the approach on distant languages. We also examined the effect of the alignment method on learning reordering rules. Interestingly, our experiments produced better translation using rules learned from automatic alignments than using rules learned from manual alignments. This is an aspect we want to explore further in the future. In future work, we would also like to address the morphological complexity of Arabic together with syntax. We plan to consider different segmentations for Arabic and study their interaction with translation and syntactic reordering. An important aspect of the TO approach is that it uses phrase internal alignments during translation. In the future, we wish to examine the effect their quality has on translation. We are also interested in examining the approach within a standard phrase-based decoder such as Moses (Koehn et al., 2003b) or a hierarchical phrase system (Chiang, 2005). The idea of training on reordered source language is often connected with pre-translation reordering. The present approach does not employ this strategy, since this is no trivial matter in a non-deterministic, weighted approach. Zhang et al. (2007) proposed an approach that builds on unfolding alignments. This is not an optimal solution, since this may not reflect their rules. Training on both original and reordered data may strengthen the approach, but it would not remedy the problems of the SO approach, since it would still be ignorant of the internal reorderings of a phrase. Nevertheless, it may strengthen the TO approach even further. We also wish to examine this in future work.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have explored the syntactic reordering approach previously presented in (Elming, 2008) within a more distant language pair, English- Arabic.</text>
              <doc_id>258</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A translation direction that is highly under-represented in MT research, compared to the opposite direction.</text>
              <doc_id>259</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We achieve significant improvement in translation quality over related approaches, measured by manual as well as automatic evaluations on this task.</text>
              <doc_id>260</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Thus proving the viability of the approach on distant languages.</text>
              <doc_id>261</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We also examined the effect of the alignment method on learning reordering rules.</text>
              <doc_id>262</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Interestingly, our experiments produced better translation using rules learned from automatic alignments than using rules learned from manual alignments.</text>
              <doc_id>263</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>This is an aspect we want to explore further in the future.</text>
              <doc_id>264</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In future work, we would also like to address the morphological complexity of Arabic together with syntax.</text>
              <doc_id>265</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>We plan to consider different segmentations for Arabic and study their interaction with translation and syntactic reordering.</text>
              <doc_id>266</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>An important aspect of the TO approach is that it uses phrase internal alignments during translation.</text>
              <doc_id>267</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>In the future, we wish to examine the effect their quality has on translation.</text>
              <doc_id>268</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>We are also interested in examining the approach within a standard phrase-based decoder such as Moses (Koehn et al., 2003b) or a hierarchical phrase system (Chiang, 2005).</text>
              <doc_id>269</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>The idea of training on reordered source language is often connected with pre-translation reordering.</text>
              <doc_id>270</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>The present approach does not employ this strategy, since this is no trivial matter in a non-deterministic, weighted approach.</text>
              <doc_id>271</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>Zhang et al. (2007) proposed an approach that builds on unfolding alignments.</text>
              <doc_id>272</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>This is not an optimal solution, since this may not reflect their rules.</text>
              <doc_id>273</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>Training on both original and reordered data may strengthen the approach, but it would not remedy the problems of the SO approach, since it would still be ignorant of the internal reorderings of a phrase.</text>
              <doc_id>274</doc_id>
              <sec_id>16</sec_id>
            </sentence>
            <sentence>
              <text>Nevertheless, it may strengthen the TO approach even further.</text>
              <doc_id>275</doc_id>
              <sec_id>17</sec_id>
            </sentence>
            <sentence>
              <text>We also wish to examine this in future work.</text>
              <doc_id>276</doc_id>
              <sec_id>18</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Example of features for rule-learning. Possible contexts separated by ||.</caption>
        <reference_text></reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>Level</cell>
              <cell>LC</cell>
              <cell>LS</cell>
              <cell>RS</cell>
              <cell>RC</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>WORD</cell>
              <cell>&lt;s&gt; he bought || he bought || bought</cell>
              <cell>new</cell>
              <cell>books</cell>
              <cell>today || today . || today . &lt; /s&gt;</cell>
            </row>
            <row>
              <cell>POS</cell>
              <cell>&lt;S&gt; NN VBD || NN VBD || VBD</cell>
              <cell>JJ</cell>
              <cell>NNS</cell>
              <cell>NN || NN . || NN . &lt; /S&gt;</cell>
            </row>
            <row>
              <cell>PS</cell>
              <cell>&lt;S&gt; NP VBD || NP VBD || VBD</cell>
              <cell>JJ</cell>
              <cell>NNS</cell>
              <cell>NP || NP . || NP . &lt; /S&gt;</cell>
            </row>
            <row>
              <cell>SUBORD</cell>
              <cell>MAIN</cell>
              <cell>MAIN</cell>
              <cell>MAIN</cell>
              <cell>MAIN</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Example of the scoring approach during decoding at source word 4.</caption>
        <reference_text></reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>Source:</cell>
              <cell>he 1 bought 2 new 3 books 4 today 5</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Rule:</cell>
              <cell>3 4 &#8594; 4 3</cell>
            </row>
            <row>
              <cell>Hypothesis</cell>
              <cell>Target string</cell>
              <cell>Alignment</cell>
            </row>
            <row>
              <cell>H1</cell>
              <cell>A&#353;tr&#253; jdyd&#65533; ktbA</cell>
              <cell>1+2 3 4</cell>
            </row>
            <row>
              <cell>H2</cell>
              <cell>A&#353;tr&#253; ktbA jdyd&#65533;</cell>
              <cell>1+2 4 3</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Automatic evaluation scores for different systems using rules extracted from manual alignments (MAN) and automatic alignments (GDF). The TO system using GDF rules is significantly better than the light grey cells at a 95% confidence level (Zhang et al., 2004).</caption>
        <reference_text>None</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>System</cell>
              <cell>None</cell>
              <cell>Dev</cell>
              <cell>MT04</cell>
              <cell>MT05</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Pharaoh Free</cell>
              <cell>Pharaoh Free</cell>
              <cell>28.37</cell>
              <cell>23.53</cell>
              <cell>24.79</cell>
            </row>
            <row>
              <cell>Pharaoh DL4</cell>
              <cell>Pharaoh DL4</cell>
              <cell>29.52</cell>
              <cell>24.72</cell>
              <cell>25.88</cell>
            </row>
            <row>
              <cell>Pharaoh Monotone</cell>
              <cell>Pharaoh Monotone</cell>
              <cell>27.93</cell>
              <cell>23.55</cell>
              <cell>24.72</cell>
            </row>
            <row>
              <cell>MAN</cell>
              <cell>NO weight</cell>
              <cell>29.53</cell>
              <cell>24.72</cell>
              <cell>25.82</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>SO weight</cell>
              <cell>29.43</cell>
              <cell>24.74</cell>
              <cell>25.82</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>TO weight</cell>
              <cell>29.40</cell>
              <cell>24.78</cell>
              <cell>25.93</cell>
            </row>
            <row>
              <cell>GDF</cell>
              <cell>NO weight</cell>
              <cell>29.87</cell>
              <cell>25.11</cell>
              <cell>26.04</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>SO weight</cell>
              <cell>29.84</cell>
              <cell>25.06</cell>
              <cell>26.01</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>TO weight</cell>
              <cell>29.95</cell>
              <cell>25.17</cell>
              <cell>26.09</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TET</source>
        <caption>Table 4: Evaluation on the diff set. Average human ratings are medians with means in parenthesis, lower scores are better, 1 is the best score.</caption>
        <reference_text></reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>System</cell>
              <cell>MT04</cell>
              <cell>MT05</cell>
              <cell>Avr. human</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>Pharaoh Free</cell>
              <cell>24.07</cell>
              <cell>25.15</cell>
              <cell>3.0 (2.80)</cell>
            </row>
            <row>
              <cell>Pharaoh DL4</cell>
              <cell>25.42</cell>
              <cell>26.51</cell>
              <cell>&#8212;</cell>
            </row>
            <row>
              <cell>NO scoring</cell>
              <cell>25.68</cell>
              <cell>26.29</cell>
              <cell>2.5 (2.43)</cell>
            </row>
            <row>
              <cell>SO scoring</cell>
              <cell>25.42</cell>
              <cell>26.02</cell>
              <cell>2.5 (2.64)</cell>
            </row>
            <row>
              <cell>TO scoring</cell>
              <cell>25.98</cell>
              <cell>26.49</cell>
              <cell>2.0 (2.08)</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TableSeer</source>
        <caption>Table 5: The reordering choices made based on the three pre-translation reordering approaches for the 20852 and 17195 reorderings proposed by the rules for the MT04 and MT05 test sets. Measured in %.</caption>
        <reference_text>In PAGE 7: ...2.4 Reordering Choices  Table5  shows the reordering choices made by the approaches in decoding. Most noticeable is that the SO approach is strongly biased against phrase internal reorderings; TO uses more than 30 times as many phrase internal reorderings as SO....</reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell></cell>
              <cell>Decoder choice</cell>
              <cell>NO</cell>
              <cell>SO</cell>
              <cell>TO</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>MT04</cell>
              <cell>Phrase internal</cell>
              <cell>20.7</cell>
              <cell>0.6</cell>
              <cell>21.2</cell>
            </row>
            <row>
              <cell></cell>
              <cell>Phrase external</cell>
              <cell>30.1</cell>
              <cell>43.0</cell>
              <cell>33.1</cell>
            </row>
            <row>
              <cell></cell>
              <cell>Reject</cell>
              <cell>49.2</cell>
              <cell>56.5</cell>
              <cell>45.7</cell>
            </row>
            <row>
              <cell>MT05</cell>
              <cell>Phrase internal</cell>
              <cell>21.3</cell>
              <cell>0.7</cell>
              <cell>21.6</cell>
            </row>
            <row>
              <cell></cell>
              <cell>Phrase external</cell>
              <cell>29.5</cell>
              <cell>42.9</cell>
              <cell>31.8</cell>
            </row>
            <row>
              <cell></cell>
              <cell>Reject</cell>
              <cell>49.2</cell>
              <cell>56.4</cell>
              <cell>46.5</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>6</id>
        <source>TableSeer</source>
        <caption>Table 6: Example rules. ! specifies negative conditions.</caption>
        <reference_text>None</reference_text>
        <page_num>8</page_num>
        <head>
          <rows>
            <row>
              <cell>No</cell>
              <cell>LC</cell>
              <cell>LS</cell>
              <cell>RS</cell>
              <cell>RC</cell>
              <cell>Prob.</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>16</cell>
              <cell>WORD: &lt;s&gt;</cell>
              <cell></cell>
              <cell>POS: FVF</cell>
              <cell>PS: SBAR</cell>
              <cell>68%</cell>
            </row>
            <row>
              <cell>27</cell>
              <cell>WORD: &lt;s&gt;</cell>
              <cell>PS: NP</cell>
              <cell>POS: FVF</cell>
              <cell></cell>
              <cell>43%</cell>
            </row>
            <row>
              <cell>11</cell>
              <cell>POS: IN</cell>
              <cell>POS: JJ</cell>
              <cell>POS: NN</cell>
              <cell></cell>
              <cell>46%</cell>
            </row>
            <row>
              <cell>1</cell>
              <cell>! POS: JJ</cell>
              <cell>POS: JJ</cell>
              <cell>WORD: president</cell>
              <cell></cell>
              <cell>90%</cell>
            </row>
            <row>
              <cell>37</cell>
              <cell>! POS: NN</cell>
              <cell>POS: NN</cell>
              <cell>POS: NNS</cell>
              <cell>POS: IN</cell>
              <cell>71%</cell>
            </row>
            <row>
              <cell></cell>
              <cell>! POS: JJ</cell>
              <cell></cell>
              <cell></cell>
              <cell></cell>
              <cell></cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>I Badr</author>
          <author>R Zbib</author>
          <author>J Glass</author>
        </authors>
        <title>Segmentation for English-to-Arabic statistical machine translation.</title>
        <publication>In Proceedings of ACL&#8217;08: HLT, Short Papers,</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>C Callison-Burch</author>
          <author>C Fordyce</author>
          <author>P Koehn</author>
          <author>C Monz</author>
          <author>J Schroeder</author>
        </authors>
        <title>(Meta-) evaluation of machine translation.</title>
        <publication>In Proceedings of ACL&#8217;07 Workshop on Statistical Machine Translation,</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>E Charniak</author>
        </authors>
        <title>A maximum-entropy-inspired parser.</title>
        <publication>In Proceedings of NAACL&#8217;00,</publication>
        <pages>None</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>D Chiang</author>
        </authors>
        <title>A hierarchical phrase-based model for statistical machine translation.</title>
        <publication>In Proceedings of ACL&#8217;05,</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>W Cohen</author>
        </authors>
        <title>Learning trees and rules with setvalued features.</title>
        <publication>In Proceedings of AAAI,</publication>
        <pages>None</pages>
        <date>1996</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>M Collins</author>
          <author>P Koehn</author>
          <author>I Kucerova</author>
        </authors>
        <title>Clause restructuring for statistical machine translation.</title>
        <publication>In Proceedings of ACL&#8217;05,</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>J M Crego</author>
          <author>J B Mari&#241;o</author>
        </authors>
        <title>Syntax-enhanced n-gram-based smt.</title>
        <publication>In Proceedings of the MT</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>J Elming</author>
        </authors>
        <title>Syntactic reordering integrated with phrase-based smt.</title>
        <publication>In Proceedings of the ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2),</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>M Galley</author>
          <author>M Hopkins</author>
          <author>K Knight</author>
          <author>D Marcu</author>
        </authors>
        <title>What&#8217;s in a translation rule?</title>
        <publication>In Proceedings of HLT/NAACL&#8217;04,</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>N Habash</author>
          <author>F Sadat</author>
        </authors>
        <title>Arabic preprocessing schemes for statistical machine translation.</title>
        <publication>In Proceedings of HLT-NAACL&#8217;06,</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>N Habash</author>
          <author>B Dorr</author>
          <author>C Monz</author>
        </authors>
        <title>None</title>
        <publication>Challenges in Building an Arabic-English GHMT System with SMT Components. In Proceedings of AMTA&#8217;06,</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>N Habash</author>
          <author>A Soudi</author>
          <author>T Buckwalter</author>
        </authors>
        <title>On Arabic Transliteration.</title>
        <publication>Arabic Computational Morphology: Knowledge-based and Empirical Methods.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>N Habash</author>
        </authors>
        <title>Syntactic preprocessing for statistical machine translation.</title>
        <publication>In Proceedings of the MT</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>A Ittycheriah</author>
          <author>S Roukos</author>
        </authors>
        <title>A maximum entropy word aligner for arabic-english machine translation.</title>
        <publication>In Proceedings of EMNLP,</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>P Koehn</author>
          <author>C Monz</author>
        </authors>
        <title>Manual and automatic evaluation of machine translation between European languages.</title>
        <publication>In Proceedings of the Workshop on Statistical Machine Translation at NAACL&#8217;06,</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>P Koehn</author>
          <author>F J Och</author>
          <author>D Marcu</author>
        </authors>
        <title>Statistical phrase-based translation.</title>
        <publication>In Proceedings of NAACL&#8217;03,</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>P Koehn</author>
          <author>F J Och</author>
          <author>D Marcu</author>
        </authors>
        <title>Statistical Phrase-based Translation.</title>
        <publication>In Proceedings of NAACL&#8217;03,</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>P Koehn</author>
          <author>A Axelrod</author>
          <author>A Birch Mayne</author>
          <author>C CallisonBurch</author>
          <author>M Osborne</author>
          <author>D Talbot</author>
        </authors>
        <title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
        <publication>In International Workshop on Spoken Language Translation 2005 (IWSLT&#8217;05),</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>P Koehn</author>
        </authors>
        <title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
        <publication>In Proceedings of AMTA&#8217;04,</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Y Lee</author>
        </authors>
        <title>Morphological Analysis for Statistical Machine Translation.</title>
        <publication>In Proceedings of HLTNAACL&#8217;04,</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>C Li</author>
          <author>M Li</author>
          <author>D Zhang</author>
          <author>M Li</author>
          <author>M Zhou</author>
          <author>Y Guan</author>
        </authors>
        <title>A probabilistic approach to syntax-based reordering for statistical machine translation.</title>
        <publication>In Proceedings of ACL&#8217;07,</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>A Lopez</author>
          <author>P Resnik</author>
        </authors>
        <title>Word-based alignment, phrase-based translation: what&#8217;s the link?</title>
        <publication>In Proceedings of AMTA&#8217;06,</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>F J Och</author>
          <author>H Ney</author>
        </authors>
        <title>A systematic comparison of various statistical alignment models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>R Sarikaya</author>
          <author>Y Deng</author>
        </authors>
        <title>Joint morphologicallexical language modeling for machine translation.</title>
        <publication>In Proceedings of HLT-NAACL&#8217;07, Short Papers,</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>A Stolcke</author>
        </authors>
        <title>Srilm &#8211; an extensible language modeling toolkit.</title>
        <publication>In Proceedings of the International Conference on Spoken Language Processing,</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>C Wang</author>
          <author>M Collins</author>
          <author>P Koehn</author>
        </authors>
        <title>Chinese syntactic reordering for statistical machine translation.</title>
        <publication>In Proceedings of EMNLP-CoNLL,</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>F Xia</author>
          <author>M McCord</author>
        </authors>
        <title>Improving a statistical mt system with automatically learned rewrite patterns.</title>
        <publication>In Proceedings of COLING&#8217;04,</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>27</id>
        <authors>
          <author>K Yamada</author>
          <author>K Knight</author>
        </authors>
        <title>A Syntax-Based Statistical Translation Model.</title>
        <publication>In Proceedings of ACL&#8217;01,</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>28</id>
        <authors>
          <author>R Zens</author>
          <author>F J Och</author>
          <author>H Ney</author>
        </authors>
        <title>Phrase-based statistical machine translation.</title>
        <publication>2002: Advances in Artificial Intelligence. 25. Annual German Conference on AI.</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>29</id>
        <authors>
          <author>Y Zhang</author>
          <author>S Vogel</author>
          <author>A Waibel</author>
        </authors>
        <title>Interpreting bleu/nist scores: How much improvement do we need to have a better system?</title>
        <publication>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC&#8217;04),</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Badr et al. (2008)</string>
        <sentence_id>41479</sentence_id>
        <char_offset>131</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>0</reference_id>
        <string>Badr et al. (2008)</string>
        <sentence_id>41488</sentence_id>
        <char_offset>107</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>1</reference_id>
        <string>Callison-Burch et al. (2007)</string>
        <sentence_id>41641</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>2</reference_id>
        <string>Charniak, 2000</string>
        <sentence_id>41602</sentence_id>
        <char_offset>80</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>3</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>41579</sentence_id>
        <char_offset>236</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>3</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>41712</sentence_id>
        <char_offset>157</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>5</reference_id>
        <string>Collins et al., 2005</string>
        <sentence_id>41470</sentence_id>
        <char_offset>143</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>6</reference_id>
        <string>Crego and Mari&#241;o, 2007</string>
        <sentence_id>41472</sentence_id>
        <char_offset>28</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>6</reference_id>
        <string>Crego and Mari&#241;o, 2007</string>
        <sentence_id>41626</sentence_id>
        <char_offset>282</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>7</reference_id>
        <string>Elming (2008)</string>
        <sentence_id>41452</sentence_id>
        <char_offset>53</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>7</reference_id>
        <string>Elming (2008)</string>
        <sentence_id>41473</sentence_id>
        <char_offset>67</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>7</reference_id>
        <string>Elming (2008)</string>
        <sentence_id>41475</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>7</reference_id>
        <string>Elming (2008)</string>
        <sentence_id>41519</sentence_id>
        <char_offset>10</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>7</reference_id>
        <string>Elming (2008)</string>
        <sentence_id>41549</sentence_id>
        <char_offset>11</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>7</reference_id>
        <string>Elming (2008)</string>
        <sentence_id>41699</sentence_id>
        <char_offset>33</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>7</reference_id>
        <string>Elming, 2008</string>
        <sentence_id>41453</sentence_id>
        <char_offset>192</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>7</reference_id>
        <string>Elming, 2008</string>
        <sentence_id>41472</sentence_id>
        <char_offset>89</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>7</reference_id>
        <string>Elming, 2008</string>
        <sentence_id>41701</sentence_id>
        <char_offset>76</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>8</reference_id>
        <string>Galley et al., 2004</string>
        <sentence_id>41466</sentence_id>
        <char_offset>143</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>9</reference_id>
        <string>Habash and Sadat, 2006</string>
        <sentence_id>41477</sentence_id>
        <char_offset>55</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>9</reference_id>
        <string>Habash and Sadat, 2006</string>
        <sentence_id>41483</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>10</reference_id>
        <string>Habash et al., 2006</string>
        <sentence_id>41467</sentence_id>
        <char_offset>171</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>11</reference_id>
        <string>Habash et al., 2007</string>
        <sentence_id>41510</sentence_id>
        <char_offset>36</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>12</reference_id>
        <string>Habash, 2007</string>
        <sentence_id>41470</sentence_id>
        <char_offset>184</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>13</reference_id>
        <string>Ittycheriah and Roukos, 2005</string>
        <sentence_id>41597</sentence_id>
        <char_offset>82</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>14</reference_id>
        <string>Koehn and Monz, 2006</string>
        <sentence_id>41581</sentence_id>
        <char_offset>70</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>15</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>41447</sentence_id>
        <char_offset>70</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>15</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>41579</sentence_id>
        <char_offset>182</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>15</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>41712</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>16</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>41447</sentence_id>
        <char_offset>70</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>16</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>41579</sentence_id>
        <char_offset>182</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>16</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>41712</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>17</reference_id>
        <string>Koehn et al., 2005</string>
        <sentence_id>41600</sentence_id>
        <char_offset>197</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>18</reference_id>
        <string>Koehn, 2004</string>
        <sentence_id>41582</sentence_id>
        <char_offset>53</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>19</reference_id>
        <string>Lee, 2004</string>
        <sentence_id>41477</sentence_id>
        <char_offset>79</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>19</reference_id>
        <string>Lee, 2004</string>
        <sentence_id>41483</sentence_id>
        <char_offset>127</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>20</reference_id>
        <string>Li et al., 2007</string>
        <sentence_id>41472</sentence_id>
        <char_offset>72</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>20</reference_id>
        <string>Li et al., 2007</string>
        <sentence_id>41627</sentence_id>
        <char_offset>53</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>21</reference_id>
        <string>Lopez and Resnik, 2006</string>
        <sentence_id>41650</sentence_id>
        <char_offset>151</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>22</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>41600</sentence_id>
        <char_offset>123</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>23</reference_id>
        <string>Sarikaya and Deng (2007)</string>
        <sentence_id>41479</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>24</reference_id>
        <string>Stolcke, 2002</string>
        <sentence_id>41581</sentence_id>
        <char_offset>143</char_offset>
      </citation>
      <citation>
        <id>42</id>
        <reference_id>25</reference_id>
        <string>Wang et al., 2007</string>
        <sentence_id>41470</sentence_id>
        <char_offset>165</char_offset>
      </citation>
      <citation>
        <id>43</id>
        <reference_id>26</reference_id>
        <string>Xia and McCord, 2004</string>
        <sentence_id>41470</sentence_id>
        <char_offset>121</char_offset>
      </citation>
      <citation>
        <id>44</id>
        <reference_id>27</reference_id>
        <string>Yamada and Knight, 2001</string>
        <sentence_id>41466</sentence_id>
        <char_offset>118</char_offset>
      </citation>
      <citation>
        <id>45</id>
        <reference_id>28</reference_id>
        <string>Zens et al., 2002</string>
        <sentence_id>41586</sentence_id>
        <char_offset>75</char_offset>
      </citation>
    </citations>
  </content>
</document>
