<PAPER>
  <FILENO/>
  <TITLE>Positive Diversity Tuning for Machine Translation System Combination</TITLE>
  <AUTHORS>
    <AUTHOR>Daniel Cer</AUTHOR>
    <AUTHOR>Christopher D Manning</AUTHOR>
  </AUTHORS>
  <ABSTRACT>
    <A-S ID="S-54584">We present Positive Diversity Tuning, a new method for tuning machine translation models specifically for improved performance during system combination.</A-S>
    <A-S ID="S-54585">System combination gains are often limited by the fact that the translations produced by the different component systems are too similar to each other.</A-S>
    <A-S ID="S-54586">We propose a method for reducing excess cross-system similarity by optimizing a joint objective that simultaneously rewards models for producing translations that are similar to reference translations, while also punishing them for translations that are too similar to those produced by other systems.</A-S>
    <A-S ID="S-54587">The formulation of the Positive Diversity objective is easy to implement and allows for its quick integration with most machine translation tuning pipelines.</A-S>
    <A-S ID="S-54588">We find that individual systems tuned on the same data to Positive Diversity can be even more diverse than systems built using different data sets, while still obtaining good BLEU scores.</A-S>
    <A-S ID="S-54589">When these individual systems are used together for system combination, our approach allows for significant gains of 0.8 BLEU even when the combination is performed using a small number of otherwise identical individual systems.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-54590">The best performing machine translation systems are typically not individual decoders but rather are ensembles of two or more systems whose output is then merged using system combination algorithms.</S>
        <S ID="S-54591">Since combining multiple distinct equally good translation systems reliably produces gains over any one of the systems in isolation, it is widely used in situations where high quality is essential.</S>
      </P>
      <P>
        <S ID="S-54592">Exploiting system combination brings significant cost: <REF ID="R-26" RPTR="33">Macherey and Och (2007)</REF> showed that successful system combination requires the construction of multiple systems that are simultaneously diverse and well-performing.</S>
        <S ID="S-54593">If the systems are not distinct enough, they will bring very little value during system combination.</S>
        <S ID="S-54594">However, if some of the systems produce diverse translations but achieve lower overall translation quality, their contributions risk being ignored during system combination.</S>
      </P>
      <P>
        <S ID="S-54595">Prior work has approached the need for diverse systems by using different system architectures, model components, system build parameters, decoder hyperparameters, as well as data selection and weighting (Macherey and Och, 2007; <REF ID="R-08" RPTR="9">DeNero et al., 2010</REF>; <REF ID="R-36" RPTR="48">Xiao et al., 2013</REF>).</S>
        <S ID="S-54596">However, during tuning, each individual system is still just trained to maximize its own isolated performance on a tune set, or at best an error-driven reweighting of the tune set, without explicitly taking into account the diversity of the resulting translations.</S>
        <S ID="S-54597">Such tuning does not encourage systems to rigorously explore model variations that achieve both good translation quality and diversity with respect to the other systems.</S>
        <S ID="S-54598">It is reasonable to suspect that this results in individual systems that under exploit the amount of diversity possible, given the characteristics of the individual systems.</S>
      </P>
      <P>
        <S ID="S-54599">For better system combination, we propose building individual systems to attempt to simultaneously maximize the overall quality of the individual systems and the amount of diversity across systems.</S>
        <S ID="S-54600">We operationalize this problem formulation by devising a new heuristic measure called Positive Diversity that estimates the potential usefulness of individual systems during system combination.</S>
        <S ID="S-54601">We find that optimizing systems toward Positive Diversity leads to significant performance gains during system combination even when the combination is performed using a small number of</S>
      </P>
      <P>
        <S ID="S-54602">otherwise identical individual translation systems.</S>
      </P>
      <P>
        <S ID="S-54603">The remainder of this paper is organized as follows.</S>
        <S ID="S-54604">Section 2 and 3 briefly review the tuning of individual machine translation systems and how system combination merges the output of multiple systems into an improved combined translation.</S>
        <S ID="S-54605">Section 4 introduces our Positive Diversity measure.</S>
        <S ID="S-54606">Section 5 introduces an algorithm for training a collection of translation systems toward Positive Diversity.</S>
        <S ID="S-54607">Experiments are presented in sections 6 and 7.</S>
        <S ID="S-54608">Sections 8 and 9 conclude with discussions of prior work and directions for future research.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Tuning Individual Translation Systems</HEADER>
      <P>
        <S ID="S-54609">Machine translation systems are tuned toward some measure of the correctness of the translations produced by the system according to one or more manually translated references.</S>
        <S ID="S-54610">As shown in equation (1), this can be written as finding parameter values &#920; that produce translations sys &#920; that in turn achieve a high score on some correctness measure:</S>
      </P>
      <P>
        <S ID="S-54611">arg max</S>
      </P>
      <P>
        <S ID="S-54612">&#920;</S>
      </P>
      <P>
        <S ID="S-54613">Correctness(ref[], sys &#920; ) (1)</S>
      </P>
      <P>
        <S ID="S-54614">The correctness measure that systems are typically tuned toward is BLEU (<REF ID="R-30" RPTR="39">Papineni et al., 2002</REF>), which measures the fraction of the n-grams that are both present in the reference translations and the translations produced by a system.</S>
        <S ID="S-54615">The BLEU score is computed as the geometric mean of the resulting n-gram precisions scaled by a brevity penalty.</S>
      </P>
      <P>
        <S ID="S-54616">The most widely used machine translation tuning algorithm, minimum error rate training (MERT) (<REF ID="R-29" RPTR="38">Och, 2003</REF>), attempts to maximize the correctness objective directly.</S>
        <S ID="S-54617">Popular alternatives such as pairwise ranking objective (PRO) (<REF ID="R-19" RPTR="25">Hopkins and May, 2011</REF>), MIRA (<REF ID="R-07" RPTR="8">Chiang et al., 2008</REF>), and RAMPION (<REF ID="R-12" RPTR="14">Gimpel and Smith, 2012</REF>) use surrogate optimization objectives that indirectly attempt to maximize the correctness function by using it to select targets for training discriminative classification models.</S>
        <S ID="S-54618">In practice, either optimizing correctness directly or optimizing a surrogate objective that uses correctness to choose optimization targets results in roughly equivalent translation performance (<REF ID="R-06" RPTR="7">Cherry and Foster, 2012</REF>).</S>
        <S ID="S-54619">Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maximize the potential usefulness of their contribution during system combination.</S>
        <S ID="S-54620">1 To our knowledge, no effort has been made to explicitly tune toward criteria that attempts to simultaneously maximize the translation quality of individual systems and their mutual diversity.</S>
        <S ID="S-54621">This is unfortunate since the most valuable component systems for system combination should not only obtain good translation performance, but also produce translations that are different from those produced by other systems.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 System Combination</HEADER>
      <P>
        <S ID="S-54622">Similar to speech recognition&#8217;s Recognizer Output Voting Error Reduction (ROVER) algorithm (<REF ID="R-10" RPTR="12">Fiscus, 1997</REF>), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (<REF ID="R-01" RPTR="2">Bangalore et al., 2001</REF>; <REF ID="R-27" RPTR="36">Matusov et al., 2006</REF>; <REF ID="R-32" RPTR="40">Rosti et al., 2007</REF><REF ID="R-33" RPTR="43">Rosti et al., 2007</REF>a; <REF ID="R-32" RPTR="41">Rosti et al., 2007</REF><REF ID="R-33" RPTR="44">Rosti et al., 2007</REF>b; <REF ID="R-21" RPTR="28">Karakos et al., 2008</REF>; <REF ID="R-16" RPTR="18">Heafield and Lavie, 2010</REF><REF ID="R-17" RPTR="21">Heafield and Lavie, 2010</REF>a).</S>
        <S ID="S-54623">2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (<REF ID="R-24" RPTR="31">Leusch et al., 2003</REF>; <REF ID="R-34" RPTR="46">Snover et al., 2009</REF>; <REF ID="R-09" RPTR="11">Denkowski and Lavie, 2011</REF>).</S>
        <S ID="S-54624">The piecewise selection of material from the original translations is performed using the combination model&#8217;s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (<REF ID="R-32" RPTR="42">Rosti et al., 2007</REF><REF ID="R-33" RPTR="45">Rosti et al., 2007</REF>b; Zhao and He, 2009; <REF ID="R-16" RPTR="19">Heafield and Lavie, 2010</REF><REF ID="R-17" RPTR="22">Heafield and Lavie, 2010</REF>b).</S>
      </P>
      <P>
        <S ID="S-54625">Both system confidence model features and n- gram consensus features score contributions based in part on how confident the system combination model is in each individual machine translation system.</S>
        <S ID="S-54626">This means that little or no gains will typically be seen when combining a good system with poor performing systems even if the systems col-</S>
      </P>
      <P>
        <S ID="S-54627">1 The exception being <REF ID="R-36" RPTR="49">Xiao et al. (2013)</REF>&#8217;s work using</S>
      </P>
      <P>
        <S ID="S-54628">boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; <REF ID="R-28" RPTR="37">Nomoto, 2004</REF>; <REF ID="R-22" RPTR="29">Zwarts and Dras, 2008</REF>), decoder chaining (<REF ID="R-00" RPTR="1">Aikawa and Ruopp, 2009</REF>), re-decoding informed by the decoding paths taken by other systems (<REF ID="R-20" RPTR="27">Huang and Papineni, 2007</REF>), and decoding model combination (<REF ID="R-08" RPTR="10">DeNero et al., 2010</REF>).</S>
      </P>
      <P>
        <S ID="S-54629">Input : systems [], tune(), source, refs [], &#945;, EvalMetric (), SimMetric () Output: models []</S>
      </P>
      <P>
        <S ID="S-54630">// start with an empty set of translations from prior iterations other_sys [] &#8592; []</S>
      </P>
      <P>
        <S ID="S-54631">for i &#8592; 1 to len(systems []) do // new Positive Diversity measure using prior translations PD &#945;,i () &#8592; new PD(&#945;, EvalMetric(), SimMetric(), refs [], other_sys [])</S>
      </P>
      <P>
        <S ID="S-54632">// tune a new model to fit PD &#945;,i // e.g., using MERT, PRO, MIRA, RAMPION, etc. models [i] &#8592; tune(systems [i], source, PD &#945;,i ())</S>
      </P>
      <P>
        <S ID="S-54633">// Save translations from tuned model i for use during // the diversity computation for subsequent systems push(other_sys [], translate(systems [i], models [i], source)) end</S>
      </P>
      <P>
        <S ID="S-54634">return models [] Algorithm 1: Positive Diversity Tuning (PDT)</S>
      </P>
      <P>
        <S ID="S-54635">lectively produce very diverse translations.</S>
        <S ID="S-54636">3</S>
      </P>
      <P>
        <S ID="S-54637">The requirement that the systems used for system combination be both of high quality and diverse can be and often is met by building several different systems using different system architectures, model components or tuning data.</S>
        <S ID="S-54638">However, as will be shown in the next few sections, by explicitly optimizing an objective that targets both translation quality and diversity, it is possible to obtain meaningful system combination gains even using a single system architecture with identical model components and the same tuning set.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 Positive Diversity</HEADER>
      <P>
        <S ID="S-54639">We propose Positive Diversity as a heuristic measurement of the value of potential contributions from an individual system to system combination.</S>
        <S ID="S-54640">As given in equation (2), Positive Diversity is defined as the correctness of the translations produced by a system minus a penalty term that scores how similar the systems translations are with those produced by other systems:</S>
      </P>
      <P>
        <S ID="S-54641">P D &#945; = &#945; Correctness(ref[], sys &#920; )&#8722; (1 &#8722; &#945;) Similarity(other_sys[], sys &#920; ) (2)</S>
      </P>
      <P>
        <S ID="S-54642">The hyperparameter &#945; explicitly trades-off the preference for a well performing individual sys-</S>
      </P>
      <P>
        <S ID="S-54643">3 The machine learning theory behind boosting suggests</S>
      </P>
      <P>
        <S ID="S-54644">that it should be possible to combine a very large number of poor performing systems into a single good system.</S>
        <S ID="S-54645">However, for machine translation, using a very large number of individual systems brings with it difficult computational challenges.</S>
      </P>
      <P>
        <S ID="S-54646">tem with system combination diversity.</S>
        <S ID="S-54647">Higher &#945; values result in a Positive Diversity metric that mostly favors good quality translations.</S>
        <S ID="S-54648">However, even for large &#945; values, if two translations are of approximately the same quality, the Positive Diversity metric will prefer the one that is the most diverse given the translations being produced by other systems.</S>
      </P>
      <P>
        <S ID="S-54649">The Correctness() and Similarity() measures are any function that can score translations from a single system against other translations.</S>
        <S ID="S-54650">This includes traditional machine translation evaluation metrics (e.g, BLEU, TER, METEOR) as well as any other measure of textual similarity.</S>
      </P>
      <P>
        <S ID="S-54651">For the remainder of this paper, we use BLEU to measure both correctness and the similarity of the translations produced by the individual systems.</S>
        <S ID="S-54652">When tuning individual translation systems toward Positive Diversity, our task is then to maximize equation (3) rather than equation (1):</S>
      </P>
      <P>
        <S ID="S-54653">arg max &#920;</S>
      </P>
      <P>
        <S ID="S-54654">&#945; BLEU(ref[], sys)&#8722; (1 &#8722; &#945;) BLEU(other_sys[], sys)</S>
      </P>
      <P>
        <S ID="S-54655">Since this learning objective is simply the difference between two BLEU scores, it should be easy to integrate into most existing machine translation tuning pipelines that are already designed to optimize performance on translation evaluation metrics.</S>
      </P>
      <P>
        <S ID="S-54656">(3)</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Tuning to Positive Diversity</HEADER>
      <P>
        <S ID="S-54657">To tune a collection of machine translation systems using Positive Diversity, we propose a staged process, whereby systems are tuned one-by-one to maximize equation (2) using the translations produced by previously trained systems to compute the diversity term, Similarity(other_sys[], sys &#920; ).</S>
        <S ID="S-54658">As shown in Algorithm 1, Positive Diversity Tuning (PDT) takes as input: a list of machine translation systems, systems[]; a tuning procedure for training individual systems, tune(); a tuning data set with source and reference translations, source and refs; a hyperparameter &#945; to adjust the trade-off between fitting the reference translations and diversity between the systems; and metrics to measure correctness and cross-system similarity, Correctness() and Similarity().</S>
      </P>
      <P>
        <S ID="S-54659">The list of systems can contain any translation system that can be parameterized using tune().</S>
        <S ID="S-54660">This can be a heterogeneous collection of substantially different systems (e.g., phrase-based, hierarchical, syntactic, or tunable hybrid systems) or even multiple copies of a single machine translation system.</S>
        <S ID="S-54661">In all cases, systems later in the list will be trained to produce translations that both fit the references and are encouraged to be distinct from the systems earlier in the list.</S>
        <S ID="S-54662">During each iteration, the system constructs a new Positive Diversity measure PD &#945;,i using the translations produced during prior iterations of training.</S>
        <S ID="S-54663">This PD &#945;,i measure is then given to tune() as the the training criteria for model i of system i .</S>
        <S ID="S-54664">The function tune() is any algorithm that allows a translation system&#8217;s performance to be fit to an evaluation metric.</S>
        <S ID="S-54665">This includes both minimum error rate training algorithms (MERT) that attempt to directly optimize a system&#8217;s performance on a metric, as well as other techniques such as Pairwaise Ranking Objective (PRO), MIRA, and RAMPION that optimize a surrogate loss based on the preferences of an evaluation metric.</S>
      </P>
      <P>
        <S ID="S-54666">After training a model for each system, the resulting model-system pairs can be combined using any arbitrary system combination strategy.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>6 Experiments</HEADER>
      <P>
        <S ID="S-54667">Experiments are performed using a single phrase-based Chinese-to-English translation system, built with the Stanford Phrasal machine translation toolkit (<REF ID="R-04" RPTR="5">Cer et al., 2010</REF>).</S>
        <S ID="S-54668">The system was built using all of the parallel data available for Phase 2 of the DARPA BOLT program.</S>
        <S ID="S-54669">The Chinese data was segmented to the Chinese Tree- Bank (CTB) standard using a maximum match word segmenter, trained on the output of a CRF segmenter (<REF ID="R-35" RPTR="47">Xiang et al., 2013</REF>).</S>
        <S ID="S-54670">The bitext was word aligned using the Berkeley aligner (<REF ID="R-25" RPTR="32">Liang et al., 2006</REF>).</S>
        <S ID="S-54671">Standard phrase-pair extraction heuris-</S>
      </P>
      <P>
        <S ID="S-54672">tics were used to extract a phrase-table over word alignments symmetrized using grow-diag (<REF ID="R-23" RPTR="30">Koehn et al., 2003</REF>).</S>
        <S ID="S-54673">We made use of a hierarchical reordering model (<REF ID="R-11" RPTR="13">Galley and Manning, 2008</REF>) as well as a 5-gram language model trained on the target side of the bi-text and smoothed using modified Kneeser-Ney (<REF ID="R-05" RPTR="6">Chen and Goodman, 1996</REF>).</S>
      </P>
      <P>
        <S ID="S-54674">Individual PDT systems were tuned on the GALE dev10 web tune set using online-PRO (<REF ID="R-14" RPTR="16">Green et al., 2013</REF>; <REF ID="R-19" RPTR="26">Hopkins and May, 2011</REF>) to the Positive Diversity Tuning criterion.</S>
        <S ID="S-54675">4 The Multi-Engine Machine Translation (MEMT) package was used for system combination (<REF ID="R-16" RPTR="20">Heafield and Lavie, 2010</REF><REF ID="R-17" RPTR="23">Heafield and Lavie, 2010</REF>a).</S>
        <S ID="S-54676">We used BOLT dev12 dev as a development test set to explore different &#945; parameterizations of the Positive Diversity criteria.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>7 Results</HEADER>
      <P>
        <S ID="S-54677">Table 1 illustrates the amount of diversity achieved by individual PDT systems on the BOLT dev12 dev evaluation set for &#945; values 0.95, 0.97, and 0.99.</S>
        <S ID="S-54678">5 Using different tuning sets is one of the common strategies for producing diverse component systems for system combination.</S>
        <S ID="S-54679">Thus, as a baseline, Table 2 gives the diversity of a system tuned to BLEU using a different tuning set, BOLT dev12 tune, with respect to the PDT systems available at each iteration.</S>
        <S ID="S-54680">As in Table 1, the diversity computation is performed using translations of BOLT dev12 dev.</S>
      </P>
      <P>
        <S ID="S-54681">Like the cross-system diversity term in the formulation of Positive Diversity using BLEU in</S>
      </P>
      <P>
        <S ID="S-54682">4 Preliminary experiments performed using MERT to train</S>
      </P>
      <P>
        <S ID="S-54683">the individual systems produced similar results to those seen here.</S>
        <S ID="S-54684">However, we switched to online-PRO since it dramatically reduced the amount time required to train each individual system.</S>
        <S ID="S-54685">We expect similar results when using other tuning algorithms for the individual systems, such as MIRA or RAMPION.</S>
        <S ID="S-54686">5 Due to time constraints, we were not able to try additional</S>
      </P>
      <P>
        <S ID="S-54687">&#945; values.</S>
        <S ID="S-54688">Given that our results suggest the lowest &#945; value from the ones we tried works best (i.e., &#945; = 0.95), it would be worth trying additional smaller &#945; values such as 0.90</S>
      </P>
      <P>
        <S ID="S-54689">equation (3), we measure the diversity of translations produced by an individual system as the negative BLEU score of the translations with respect to the translations from systems built during prior iterations.</S>
        <S ID="S-54690">For clarity of presentation, these diversity scores are reported as 1.0&#8722;BLEU.</S>
        <S ID="S-54691">Using 1.0&#8722;BLEU to score cross-system diversity, means that the reported numbers can be roughly interpreted as the fraction of n-grams from the individual systems built during iteration i that have not been previously produced by other systems built during any iteration &lt; i.</S>
        <S ID="S-54692">6</S>
      </P>
      <P>
        <S ID="S-54693">In our experiments, we find that for &#945; &#8804; 0.97, during the first three iterations of PDT, there is more diversity among the PDT systems tuned on a single data set (GALE dev10 web tune) than there is between systems tuned on different datasets (BOLT dev12 tune vs. GALE dev10 wb tune).</S>
        <S ID="S-54694">This is significant since using different tuning sets is a common strategy for increasing diversity during system combination.</S>
        <S ID="S-54695">These results suggest PDT is better at producing additional diversity than using different tuning sets.</S>
        <S ID="S-54696">The PDT systems also achieve good coverage of the n-grams present in the baseline system that was tuned using different data.</S>
        <S ID="S-54697">At iteration 10 and using &#945; = 0.95, the baseline systems receive a diversity score of only 7.9% when measured against the PDT systems.</S>
        <S ID="S-54698">7</S>
      </P>
      <P>
        <S ID="S-54699">As PDT progresses, it becomes more difficult to tune systems to produce high quality translations that are substantially different from those already being produced by other systems.</S>
        <S ID="S-54700">This is seen in the per iteration diversity scores, whereby during iteration 5, the individual PDT translation systems have a 1.0&#8722;BLEU diversity score with prior systems ranging from 11.9%, when using an &#945; value</S>
      </P>
      <P>
        <S ID="S-54701">6 This intuitive interpretation assumes a brevity penalty</S>
      </P>
      <P>
        <S ID="S-54702">that is approximately 1.0.</S>
        <S ID="S-54703">7 For this diversity score, the brevity penalty is 1.0, meaning the diversity score is based purely on the n-grams present in the baseline system that are not present in translations produced by one or more of the PDT systems</S>
      </P>
      <P>
        <S ID="S-54704">of 0.95, to 3.2% when using an &#945; value of 0.99.</S>
        <S ID="S-54705">A diversity score of 3.2% when using &#945; = 0.99 suggests that by iteration 5, very high &#945; values put insufficient pressure on learning to find models that produce diverse translations.</S>
        <S ID="S-54706">When using an &#945; of 0.95, a sizable amount of diversity still exists across the systems translations all the way to iteration 7.</S>
        <S ID="S-54707">By iteration 10, only a small amount of additional diversity is contributed by each additional system for all of the alpha values (&lt; 3%).</S>
        <S ID="S-54708">8</S>
      </P>
      <P>
        <S ID="S-54709">Table 3 shows the BLEU scores obtained on the BOLT dev12 dev evaluation set by the individual systems tuned during each iteration of PDT.</S>
        <S ID="S-54710">The 0 th iteration for each &#945; value has an empty set of translations for the diversity term.</S>
        <S ID="S-54711">This means the resulting systems are effectively tuned to just maximize BLEU.</S>
        <S ID="S-54712">Differences in system performance during this iteration are only due to differences in the random seeds used during training.</S>
        <S ID="S-54713">Starting at iteration 1, the individual systems are optimized to produce translations that both score well on BLEU</S>
      </P>
      <P>
        <S ID="S-54714">8 We speculate that if heterogeneous translation systems</S>
      </P>
      <P>
        <S ID="S-54715">were used with PDT, it could be possible to run with higher &#945; values and still obtain diverse translations after a large number of PDT iterations</S>
      </P>
      <P>
        <S ID="S-54716">and are diverse from the systems produced during prior iterations.</S>
        <S ID="S-54717">It is interesting to note that the systems trained during these subsequent iterations obtain BLEU scores that are usually competitive with those obtained by the iteration 0 systems.</S>
        <S ID="S-54718">Taken together with the diversity scores in Table 1, this strongly suggests that PDT is succeeding at increasing diversity while still producing high quality individual translation systems.</S>
      </P>
      <P>
        <S ID="S-54719">Figure 1 graphs the system combination BLEU score achieved by using varying numbers of Positive Diversity Tuned translation systems and different &#945; values to trade-off translation quality with translation diversity.</S>
        <S ID="S-54720">After running 4 iterations of PDT, the best configuration, &#945; = 0.95, achieves a BLEU score that is 0.8 BLEU higher than the corresponding BLEU trained iteration 0 system.</S>
        <S ID="S-54721">9 From the graph, it appears that PDT performance initially increases as additional systems are added to the system combination and then later plateaus or even drops after too many systems are included.</S>
        <S ID="S-54722">The combinations using PDT systems</S>
      </P>
      <P>
        <S ID="S-54723">9 Recall that the iteration 0 system is effectively just tuned</S>
      </P>
      <P>
        <S ID="S-54724">to maximize BLEU since we have an empty set of translations from other systems that are used to compute diversity</S>
      </P>
      <P>
        <S ID="S-54725">built with higher &#945; values reach the point of diminishing returns faster than combinations using systems built with lower alpha values.</S>
        <S ID="S-54726">For instance, &#945; = 0.99 plateaus on iteration 2, while &#945; = 0.95 peaks on iteration 4.</S>
        <S ID="S-54727">It might be possible to identify the point at which additional systems will likely not be useful by using the diversity scores in Table 1.</S>
        <S ID="S-54728">Scoring about 10% or less on the 1&#8722;BLEU diversity measure, with respect to the other systems being used within the system combination, seems to suggest the individual system will not be very helpful to add into the combination.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>8 Related Work</HEADER>
      <P>
        <S ID="S-54729">While the idea of encouraging diversity in individual systems that will be used for system combination has been proven effective in speech recognition and document summarization (<REF ID="R-18" RPTR="24">Hinton, 2002</REF>; <REF ID="R-02" RPTR="3">Breslin and Gales, 2007</REF>; <REF ID="R-03" RPTR="4">Carbonell and Goldstein, 1998</REF>; <REF ID="R-13" RPTR="15">Goldstein et al., 2000</REF>), there has only been a modest amount of prior work exploring such approaches for machine translation.</S>
        <S ID="S-54730">Prior work within machine translation has investigated adapting machine learning techniques for building ensembles of classifiers to translation system tuning, encouraging diversity by varying both the hyperparameters and the data used to build the individual systems, and chaining together individual translation systems.</S>
        <S ID="S-54731"><REF ID="R-36" RPTR="50">Xiao et al. (2013)</REF> explores using boosting to train an ensemble of machine translation systems.</S>
        <S ID="S-54732">Following the standard Adaboost algorithm, each system was trained in sequence on an error-driven reweighting of the tuning set that focuses learning on the material that is the most problematic for the current ensemble.</S>
        <S ID="S-54733">They found that using a single system to tune a large number of decoding models to different Adaboost guided weightings of the tuning data results in significant gains during system combination.</S>
      </P>
      <P>
        <S ID="S-54734"><REF ID="R-26" RPTR="34">Macherey and Och (2007)</REF> investigated system combination using automatic generation of diverse individual systems.</S>
        <S ID="S-54735">They programmatically generated variations of systems using different build and decoder hyperparameters such as choice of wordalignment algorithm, distortion limit, variations of model feature function weights, and the set of language models used.</S>
        <S ID="S-54736">Then, in a process similar to forward feature selection, they constructed a combined system by iteratively adding the individual automatically generated system that produced the largest increase in quality when used in conjunction with the systems already selected for the combined system.</S>
        <S ID="S-54737">They also explored producing variation by using different samplings of the the training data.</S>
        <S ID="S-54738">The individual and combined systems produced by sampling the training data were inferior to systems that used all of the available data.</S>
        <S ID="S-54739">However, the experiments facilitated insightful analysis on what properties an individual system must have in order to be useful during system combination.</S>
        <S ID="S-54740">They found that in order to be useful within a combination, individual systems need to produce translations of similar quality to other individual systems within the system combination while also being as uncorrelated as possible from the other systems.</S>
        <S ID="S-54741">The Positive Diversity Tuning method introduced in our work is an explicit attempt to build individual translation systems that meet this criteria, while being less computationally demanding than the diversity generating techniques explored by <REF ID="R-26" RPTR="35">Macherey and Och (2007)</REF>.</S>
        <S ID="S-54742"><REF ID="R-00" RPTR="0">Aikawa and Ruopp (2009)</REF> investigated building machine translations systems specifically for use in sequential combination with other systems.</S>
        <S ID="S-54743">They constructed chains of systems whereby the output of one decoder is feed as input to the next decoder in the pipeline.</S>
        <S ID="S-54744">The downstream systems are built and tuned to correct errors produced by the preceding system.</S>
        <S ID="S-54745">In this approach, the downstream decoder acts as a machine learning based post editing system.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>9 Conclusion</HEADER>
      <P>
        <S ID="S-54746">We have presented Positive Diversity as a new way of jointly measuring the quality and diversity of the contribution of individual machine translation systems to system combination.</S>
        <S ID="S-54747">This method heuristically assesses the value of individual translation systems by measuring their similarity to the reference translations as well as their dissimilarity from the other systems being combined.</S>
        <S ID="S-54748">We operationalize this metric by reusing existing techniques from machine translation evaluation to assess translation quality and the degree of similarity between systems.</S>
        <S ID="S-54749">We also give a straightforward algorithm for training a collection of individual systems to optimize Positive Diversity.</S>
        <S ID="S-54750">Our experimental results suggest that tuning to Positive Diversity leads to improved cross-system diversity and system combination performance even when combining otherwise identical machine translation 326 systems.</S>
        <S ID="S-54751">The Positive Diversity Tuning method explored in this work can be used to tune individual systems for any ensemble in which individual models can be fit to multiple extrinsic loss functions.</S>
        <S ID="S-54752">Since <REF ID="R-15" RPTR="17">Hall et al. (2011)</REF> demonstrated the general purpose application of multiple extrinsic loss functions to training structured prediction models, Positive Diversity Tuning could be broadly useful within natural language processing and for other machine learning tasks.</S>
        <S ID="S-54753">In future work within machine translation, it may prove fruitful to examine more sophisticated measures of dissimilarity.</S>
        <S ID="S-54754">For example, one could imagine a metric that punishes instances of similar material in proportion to some measure of the expected diversity of the material.</S>
        <S ID="S-54755">It might also be useful to explore joint rather than sequential training of the individual translation systems.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS>
    <P>
      <S ID="S-54756">We thank the reviewers and the members of the Stanford NLP group for their helpful comments and suggestions.</S>
      <S ID="S-54757">This work was supported by the Defense Advanced Research Projects Agency (DARPA) Broad Operational Language Translation (BOLT) program through IBM and a fellowship to one of the authors from the Center for Advanced Study in the Behavioral Sciences.</S>
      <S ID="S-54758">Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of DARPA or the US government.</S>
    </P>
  </ACKNOWLEDGMENTS>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>Takako Aikawa</RAUTHOR>
      <REFTITLE>Chained system: A linear combination of different types of statistical machine translation systems.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>S Bangalore</RAUTHOR>
      <REFTITLE>Computing consensus translation from multiple machine translation systems.</REFTITLE>
      <DATE>2001</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>C Breslin</RAUTHOR>
      <REFTITLE>Complementary system generation using directed decision trees.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>Jaime Carbonell</RAUTHOR>
      <REFTITLE>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</REFTITLE>
      <DATE>1998</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>Daniel Cer</RAUTHOR>
      <REFTITLE>Phrasal: A statistical machine translation toolkit for Exploring new model features.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>Stanley F Chen</RAUTHOR>
      <REFTITLE>An empirical study of smoothing techniques for language modeling.</REFTITLE>
      <DATE>1996</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>Colin Cherry</RAUTHOR>
      <REFTITLE>Batch tuning strategies for statistical machine translation.</REFTITLE>
      <DATE>2012</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>David Chiang</RAUTHOR>
      <REFTITLE>Online large-margin training of syntactic and structural translation features.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>John DeNero</RAUTHOR>
      <REFTITLE>Model combination for machine translation.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>Michael Denkowski</RAUTHOR>
      <REFTITLE>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>J G Fiscus</RAUTHOR>
      <REFTITLE>A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER).</REFTITLE>
      <DATE>1997</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>Michel Galley</RAUTHOR>
      <REFTITLE>A simple and effective hierarchical phrase reordering model.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>Kevin Gimpel</RAUTHOR>
      <REFTITLE>Structured ramp loss minimization for machine translation.</REFTITLE>
      <DATE>2012</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>J Goldstein</RAUTHOR>
      <REFTITLE>Multi-document summarization by sentence extraction.</REFTITLE>
      <DATE>2000</DATE>
    </REFERENCE>
    <REFERENCE ID="14">
      <RAUTHOR>Spence Green</RAUTHOR>
      <REFTITLE>Fast and adaptive online training of feature-rich translation models.</REFTITLE>
      <DATE>2013</DATE>
    </REFERENCE>
    <REFERENCE ID="15">
      <RAUTHOR>Keith Hall</RAUTHOR>
      <REFTITLE>Training structured prediction models with extrinsic loss functions.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="16">
      <RAUTHOR>Kenneth Heafield</RAUTHOR>
      <REFTITLE>CMU multiengine machine translation for WMT</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="17">
      <RAUTHOR>Kenneth Heafield</RAUTHOR>
      <REFTITLE>Voting on ngrams for machine translation system combination.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="18">
      <RAUTHOR>Geoffrey E Hinton</RAUTHOR>
      <REFTITLE>Training products of experts by minimizing contrastive divergence.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="19">
      <RAUTHOR>Mark Hopkins</RAUTHOR>
      <REFTITLE>Tuning as ranking.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="20">
      <RAUTHOR>Fei Huang</RAUTHOR>
      <REFTITLE>Hierarchical system combination for machine translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="21">
      <RAUTHOR>Jason Eisner Karakos</RAUTHOR>
      <REFTITLE>Machine translation system combination using ITG-based alignments.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="22">
      <RAUTHOR>Simon Zwarts</RAUTHOR>
      <REFTITLE>Choosing the right translation: A syntactically informed classification approach.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="23">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Statistical phrase-based translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="24">
      <RAUTHOR>Gregor Leusch</RAUTHOR>
      <REFTITLE>A novel string-to-string distance measure with applications to machine translation evaluation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="25">
      <RAUTHOR>Percy Liang</RAUTHOR>
      <REFTITLE>Alignment by agreement.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="26">
      <RAUTHOR>Wolfgang Macherey</RAUTHOR>
      <REFTITLE>An empirical study on computing consensus translations from multiple machine translation systems.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="27">
      <RAUTHOR>Evgeny Matusov</RAUTHOR>
      <REFTITLE>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="28">
      <RAUTHOR>Tadashi Nomoto</RAUTHOR>
      <REFTITLE>Multi-engine machine translation with voted language model.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="29">
      <RAUTHOR>Franz Josef Och</RAUTHOR>
      <REFTITLE>Minimum error rate training in statistical machine translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="30">
      <RAUTHOR>Kishore Papineni</RAUTHOR>
      <REFTITLE>Bleu: a method for automatic evaluation of machine translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="31">
      <RAUTHOR>Michael Paul</RAUTHOR>
      <REFTITLE>Takao Doi, Youngsook Hwang, Kenji Imamura, Hideo Okuma, and Eiichiro Sumita.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="32">
      <RAUTHOR>Antti-Veikko Rosti</RAUTHOR>
      <REFTITLE>Combining outputs from multiple machine translation systems.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="33">
      <RAUTHOR>Antti-Veikko Rosti</RAUTHOR>
      <REFTITLE>Improved word-level system combination for machine translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="34">
      <RAUTHOR>Matthew Snover</RAUTHOR>
      <REFTITLE>Fluency, adequacy, or HTER?: exploring different human judgments with a tunable MT metric.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="35">
      <RAUTHOR>Bing Xiang</RAUTHOR>
      <REFTITLE>Enlisting the ghost: Modeling empty categories for machine translation.</REFTITLE>
      <DATE>2013</DATE>
    </REFERENCE>
    <REFERENCE ID="36">
      <RAUTHOR>Tong Xiao</RAUTHOR>
      <REFTITLE>Bagging and boosting statistical machine translation systems.</REFTITLE>
      <DATE>2013</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
