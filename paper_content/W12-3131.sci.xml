<PAPER>
  <FILENO/>
  <TITLE>The CMU-Avenue French-English Translation System</TITLE>
  <AUTHORS/>
  <ABSTRACT>
    <A-S ID="S-50980">This paper describes the French-English translation system developed by the Avenue research group at Carnegie Mellon University for the Seventh Workshop on Statistical Machine Translation (NAACL WMT12).</A-S>
    <A-S ID="S-50981">We present a method for training data selection, a description of our hierarchical phrase-based translation system, and a discussion of the impact of data size on best practice for system building.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-50982">We describe the French-English translation system constructed by the Avenue research group at Carnegie Mellon University for the shared translation task in the Seventh Workshop on Statistical Machine Translation.</S>
        <S ID="S-50983">The core translation system uses the hierarchical phrase-based model described by <REF ID="R-03" RPTR="3">Chiang (2007)</REF> with sentence-level grammars extracted and scored using the methods described by <REF ID="R-12" RPTR="18">Lopez (2008)</REF>.</S>
        <S ID="S-50984">Improved techniques for data selection and monolingual text processing significantly improve the performance of the baseline system.</S>
      </P>
      <P>
        <S ID="S-50985">Over half of all parallel data for the French- English track is provided by the Giga-FrEn corpus (<REF ID="R-00" RPTR="0">Callison-Burch et al., 2009</REF>).</S>
        <S ID="S-50986">Assembled from crawls of bilingual websites, this corpus is known to be noisy, containing sentences that are either not parallel or not natural language.</S>
        <S ID="S-50987">Rather than simply including or excluding the resource in its entirety, we use a relatively simple technique inspired by work in machine translation quality estimation to select the best portions of the corpus for inclusion in our training data.</S>
        <S ID="S-50988">Including around 60% of the Giga-FrEn chosen by this technique yields an improvement of 0.7 BLEU.</S>
      </P>
      <P>
        <S ID="S-50989">Prior to model estimation, we process all parallel and monolingual data using in-house tokenization and normalization scripts that detect word boundaries better than the provided WMT12 scripts.</S>
        <S ID="S-50990">After translation, we apply a monolingual rule-based postprocessing step to correct obvious errors and make sentences more acceptable to human judges.</S>
        <S ID="S-50991">The post-processing step alone yields an improvement of 0.3 BLEU to the final system.</S>
      </P>
      <P>
        <S ID="S-50992">We conclude with a discussion of the impact of data size on important decisions for system building.</S>
        <S ID="S-50993">Experimental results show that &#8220;best practice&#8221; decisions for smaller data sizes do not necessarily carry over to systems built with &#8220;WMT-scale&#8221; data, and provide some explanation for why this is the case.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Training Data</HEADER>
      <P>
        <S ID="S-51042">Training data provided for the French-English translation task includes parallel corpora taken from European Parliamentary proceedings (<REF ID="R-09" RPTR="13">Koehn, 2005</REF>), news commentary, and United Nations documents.</S>
        <S ID="S-51043">Together, these sets total approximately 13 million sentences.</S>
        <S ID="S-51044">In addition, a large, web-crawled parallel corpus termed the &#8220;Giga-FrEn&#8221; (<REF ID="R-00" RPTR="1">Callison-Burch et al., 2009</REF>) is made available.</S>
        <S ID="S-51045">While this corpus contains over 22 million parallel sentences, it is inherently noisy.</S>
        <S ID="S-51046">Many parallel sentences crawled from the web are neither parallel nor sentences.</S>
        <S ID="S-51047">To make use of this large data source, we employ data selection techniques discussed in the next subsection.</S>
      </P>
      <P>
        <S ID="S-51048">Parallel data used to build our final system totals 27 million sentences.</S>
        <S ID="S-51049">Precise figures for the number of sentences in each data set, including selections from the Giga-FrEn, are found in Table 1.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>2.1 Data Selection as Quality Estimation</HEADER>
        <P>
          <S ID="S-50994">Drawing inspiration from the workshop&#8217;s featured task, we cast the problem of data selection as one of quality estimation.</S>
          <S ID="S-50995"><REF ID="R-19" RPTR="27">Specia et al. (2009)</REF> report several estimators of translation quality, the most effective of which detect difficult-to-translate source sentences, ungrammatical translations, and translations that align poorly to their source sentences.</S>
          <S ID="S-50996">We can easily adapt several of these predictive features to select good sentence pairs from noisy parallel corpora such as the Giga-FrEn.</S>
          <S ID="S-50997">We first pre-process the Giga-FrEn by removing lines with invalid Unicode characters, control characters, and insufficient concentrations of Latin characters.</S>
          <S ID="S-50998">We then score each sentence pair in the remaining set (roughly 90% of the original corpus) with the following features:</S>
        </P>
        <P>
          <S ID="S-50999">Source language model: a 4-gram modified Kneser-Ney smoothed language model trained on French Europarl, news commentary, UN doc, and news crawl corpora.</S>
          <S ID="S-51000">This model assigns high scores to grammatical source sentences and lower scores to ungrammatical sentences and non-sentences such as site maps, large lists of names, and blog comments.</S>
          <S ID="S-51001">Scores are normalized by number of n-grams scored per sentence (length + 1).</S>
          <S ID="S-51002">The model is built using the SRILM toolkit (Stolke, 2002).</S>
        </P>
        <P>
          <S ID="S-51003">Target language model: a 4-gram modified Kneser-Ney smoothed language model trained on English Europarl, news commentary, UN doc, and news crawl corpora.</S>
          <S ID="S-51004">This model scores grammaticality on the target side.</S>
        </P>
        <P>
          <S ID="S-51005">Word alignment scores: source-target and target-source MGIZA++ (<REF ID="R-06" RPTR="9">Gao and Vogel, 2008</REF>) force-alignment scores using IBM Model 4 (<REF ID="R-13" RPTR="21">Och and Ney, 2003</REF>).</S>
          <S ID="S-51006">Model parameters are estimated on 2 million words of French-English Europarl and news commentary text.</S>
          <S ID="S-51007">Scores are normalized by the number of alignment links.</S>
          <S ID="S-51008">These features measure the extent to which translations are parallel with their source sentences.</S>
          <S ID="S-51009">Fraction of aligned words: source-target and target-source ratios of aligned words to total words.</S>
          <S ID="S-51010">These features balance the link-normalized alignment scores.</S>
        </P>
        <P>
          <S ID="S-51011">To determine selection criteria, we use this feature set to score the news test sets from 2008 through 2011 (10K parallel sentences) and calculate the mean and standard deviation of each feature score distribution.</S>
          <S ID="S-51012">We then select two subsets of the Giga- FrEn, &#8220;1stdev&#8221; and &#8220;2stdev&#8221;.</S>
          <S ID="S-51013">The 1stdev set includes sentence pairs for which the score for each feature is above a threshold defined as the development set mean minus one standard deviation.</S>
          <S ID="S-51014">The 2stdev set includes sentence pairs not included in 1stdev that meet the per-feature threshold of mean minus two standard deviations.</S>
          <S ID="S-51015">Hard, per-feature thresholding is motivated by the notion that a sentence pair must meet all the criteria discussed above to constitute good translation.</S>
          <S ID="S-51016">For example, high source and target language model scores are irrelevant if the sentences are not parallel.</S>
          <S ID="S-51017">As primarily news data is used for determining thresholds and building language models, this approach has the added advantage of preferring parallel data in the domain we are interested in translating.</S>
          <S ID="S-51018">Our final translation system uses data from both 1stdev and 2stdev, corresponding to roughly 60% of the Giga-FrEn corpus.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.2 Monolingual Data</HEADER>
        <P>
          <S ID="S-51019">Monolingual English data includes European Parliamentary proceedings (<REF ID="R-09" RPTR="14">Koehn, 2005</REF>), news commentary, United Nations documents, news crawl, the English side of the Giga-FrEn, and the English Gigaword Fourth Edition (<REF ID="R-16" RPTR="25">Parker et al., 2009</REF>).</S>
          <S ID="S-51020">We use all available data subject to the following selection decisions.</S>
          <S ID="S-51021">We apply the initial filter to the Giga-FrEn to remove non-text sections, leaving approximately 90% of the corpus.</S>
          <S ID="S-51022">We exclude the known prob-</S>
        </P>
        <P>
          <S ID="S-51023">lematic New York Times section of the Gigaword.</S>
          <S ID="S-51024">As many data sets include repeated boilerplate text such as copyright information or browser compatibility notifications, we unique sentences from the UN doc, news crawl, Giga-FrEn, and Gigaword sets by source.</S>
          <S ID="S-51025">Final monolingual data totals 4.7 billion words before uniqueing and 3.9 billion after.</S>
          <S ID="S-51026">Word counts for all data sources are shown in Table 2.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.3 Text Processing</HEADER>
        <P>
          <S ID="S-51027">All monolingual and parallel system data is run through a series of pre-processing steps before construction of the language model or translation model.</S>
          <S ID="S-51028">We first run an in-house normalization script over all text in order to convert certain variably encoded characters to a canonical form.</S>
          <S ID="S-51029">For example, thin spaces and non-breaking spaces are normalized to standard ASCII space characters, various types of &#8220;curly&#8221; and &#8220;straight&#8221; quotation marks are standardized as ASCII straight quotes, and common French and English ligatures characters (e.g. &#339;, fi) are replaced with standard equivalents.</S>
        </P>
        <P>
          <S ID="S-51030">English text is tokenized with the Penn Treebankstyle tokenizer attached to the Stanford parser (<REF ID="R-08" RPTR="12">Klein and Manning, 2003</REF>), using most of the default options.</S>
          <S ID="S-51031">We set the tokenizer to Americanize variant spellings such as color vs. colour or behavior vs. behaviour.</S>
          <S ID="S-51032">Currency-symbol normalization is avoided.</S>
        </P>
        <P>
          <S ID="S-51033">For French text, we use an in-house tokenization script.</S>
          <S ID="S-51034">Aside from the standard tokenization based on punctuation marks, this step includes Frenchspecific rules for handling apostrophes (French elision), hyphens in subject-verb inversions (including the French t euphonique), and European-style numbers.</S>
          <S ID="S-51035">When compared to the default WMT12- provided tokenization script, our custom French rules more accurately identify word boundaries, particularly in the case of hyphens.</S>
          <S ID="S-51036">Figure 1 highlights the differences in sample phrases.</S>
          <S ID="S-51037">Subject-verb inversions are broken apart, while other hyphenated words are unaffected; French aujourd&#8217;hui (&#8220;today&#8221;) is retained as a single token to match English.</S>
        </P>
        <P>
          <S ID="S-51038">Parallel data is run through a further filtering step to remove sentence pairs that, by their length characteristics alone, are very unlikely to be true parallel data.</S>
          <S ID="S-51039">Sentence pairs that contain more than 95 tokens on either side are globally discarded, as are sentence pairs where either side contains a token longer than 25 characters.</S>
          <S ID="S-51040">Remaining pairs are checked for length ratio between French and English, and sentences are discarded if their English translations are either too long or too short given the French length.</S>
          <S ID="S-51041">Allowable ratios are determined from the tokenized training data and are set such that approximately the middle 95% of the data, in terms of length ratio, is kept for each French length.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Translation System</HEADER>
      <P>
        <S ID="S-51061">Our translation system uses cdec (<REF ID="R-05" RPTR="8">Dyer et al., 2010</REF>), an implementation of the hierarchical phrasebased translation model (<REF ID="R-03" RPTR="4">Chiang, 2007</REF>) that uses the KenLM library (<REF ID="R-07" RPTR="11">Heafield, 2011</REF>) for language model inference.</S>
        <S ID="S-51062">The system translates from cased French to cased English; at no point do we lowercase data.</S>
      </P>
      <P>
        <S ID="S-51063">The Parallel data is aligned in both directions using the MGIZA++ (<REF ID="R-06" RPTR="10">Gao and Vogel, 2008</REF>) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (<REF ID="R-13" RPTR="22">Och and Ney, 2003</REF>).</S>
        <S ID="S-51064">The aligned corpus is then encoded as a suffix array to facilitate sentence-level grammar extraction and scoring (<REF ID="R-12" RPTR="17">Lopez, 2008</REF>).</S>
        <S ID="S-51065">Grammars are extracted using the heuristics described by Chiang (<REF ID="R-03" RPTR="5">Chiang, 2007</REF>) and feature scores are calculated according to <REF ID="R-12" RPTR="19">Lopez (2008)</REF>.</S>
      </P>
      <P>
        <S ID="S-51066">Modified Knesser-Ney smoothed (<REF ID="R-02" RPTR="2">Chen and Goodman, 1996</REF>) n-gram language models are built from the monolingual English data using the SRI language modeling toolkit (Stolke, 2002).</S>
        <S ID="S-51067">We experiment with both 4-gram and 5-gram models.</S>
      </P>
      <P>
        <S ID="S-51068">System parameters are optimized using minimum error rate training (<REF ID="R-14" RPTR="23">Och, 2003</REF>) to maximize the corpus-level cased BLEU score (Papineni et al.,</S>
      </P>
      <P>
        <S ID="S-51069">Base: Custom: Base: Custom: Base: Custom: Base: Custom: Y a-t-il un coll&#232;gue pour prendre la parole Y a -t-il un coll&#232;gue pour prendre la parole Peut-&#234;tre , &#224; ce sujet , puis-je dire &#224; M. Ribeiro i Castro Peut-&#234;tre , &#224; ce sujet , puis -je dire &#224; M. Ribeiro i Castro le proc&#232;s-verbal de la s&#233;ance d&#8217; aujourd&#8217; hui le proc&#232;s-verbal de la s&#233;ance d&#8217; aujourd&#8217;hui s&#8217; &#233;tablit environ &#224; 1,2 % du PIB s&#8217; &#233;tablit environ &#224; 1.2 % du PIB</S>
      </P>
      <P>
        <S ID="S-51070">2002) on news-test 2008 (2051 sentences).</S>
        <S ID="S-51071">This development set is chosen for its known stability and reliability.</S>
        <S ID="S-51072">Our baseline translation system uses Viterbi decoding while our final system uses segment-level Minimum Bayes-Risk decoding (<REF ID="R-10" RPTR="15">Kumar and Byrne, 2004</REF>) over 500-best lists using 1 - BLEU as the loss function.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>3.1 Post-Processing</HEADER>
        <P>
          <S ID="S-51050">Our final system includes a monolingual rule-based post-processing step that corrects obvious translation errors.</S>
          <S ID="S-51051">Examples of correctable errors include capitalization, mismatched punctuation, malformed numbers, and incorrectly split compound words.</S>
          <S ID="S-51052">We finally employ a coarse cognate translation system to handle out-of-vocabulary words.</S>
          <S ID="S-51053">We assume that uncapitalized French source words passed through to the English output are cognates of English words and translate them by removing accents.</S>
          <S ID="S-51054">This frequently leads to (in order of desirability) fully correct translations, correct translations with foreign spellings, or correct translations with misspellings.</S>
          <S ID="S-51055">All of the above are generally preferable to untranslated foreign words.</S>
          <S ID="S-51056">Examples of cognate translations for OOV words in newstest 2011 are shown in Figure 2.</S>
          <S ID="S-51057">1</S>
        </P>
        <P>
          <S ID="S-51058">1 Some OOVs are caused by misspellings in the dev-test</S>
        </P>
        <P>
          <S ID="S-51059">source sentences.</S>
          <S ID="S-51060">In these cases we can salvage misspelled English words in place of misspelled French words</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 Experiments</HEADER>
      <P>
        <S ID="S-51088">Beginning with a baseline translation system, we incrementally evaluate the contribution of additional data and components.</S>
        <S ID="S-51089">System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (<REF ID="R-15" RPTR="24">Papineni et al., 2002</REF>), Meteor (<REF ID="R-04" RPTR="7">Denkowski and Lavie, 2011</REF>), and TER (<REF ID="R-17" RPTR="26">Snover et al., 2006</REF>).</S>
        <S ID="S-51090">For full consistency with WMT11, we use the NIST scoring script, TER-0.7.25, and Meteor-1.3 to evaluate cased, detokenized translations.</S>
        <S ID="S-51091">Results are shown in Table 3, where each evaluation point is the result of a full tune/test run that includes MERT for parameter optimization.</S>
        <S ID="S-51092">The baseline translation system is built from 14 million parallel sentences (Europarl, news commentary, and UN doc) and all monolingual data.</S>
        <S ID="S-51093">Grammars are extracted using the &#8220;tight&#8221; heuristic that requires phrase pairs to be bounded by word alignments.</S>
        <S ID="S-51094">Both 4-gram and 5-gram language models are evaluated.</S>
        <S ID="S-51095">Viterbi decoding is conducted with a cube pruning pop limit (<REF ID="R-03" RPTR="6">Chiang, 2007</REF>) of 200.</S>
        <S ID="S-51096">For this data size, the 4-gram model is shown to significantly outperform the 5-gram.</S>
      </P>
      <P>
        <S ID="S-51097">Adding the 1stdev and 2stdev sets from the Giga- FrEn increases the parallel data size to 27 million</S>
      </P>
      <P>
        <S ID="S-51098">sentences and further improves performance.</S>
        <S ID="S-51099">These runs require new grammars to be extracted, but use the same 4-gram language model and decoding method as the baseline system.</S>
        <S ID="S-51100">With large training data, moving to a 5-gram language model, increasing the cube pruning pop limit to 1000, and using Minimum Bayes-Risk decoding (<REF ID="R-10" RPTR="16">Kumar and Byrne, 2004</REF>) over 500-best lists collectively show a slight improvement.</S>
        <S ID="S-51101">Monolingual post-processing yields further improvement.</S>
        <S ID="S-51102">This decoding/processing scheme corresponds to our final translation system.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>4.1 Impact of Data Size</HEADER>
        <P>
          <S ID="S-51073">The WMT French-English track provides an opportunity to experiment in a space of data size that is generally not well explored.</S>
          <S ID="S-51074">We examine the impact of data sizes of hundreds of millions of words on two significant system building decisions: grammar extraction and language model estimation.</S>
          <S ID="S-51075">Comparative results are reported on the newstest 2011 set.</S>
        </P>
        <P>
          <S ID="S-51076">In the first case, we compare the &#8220;tight&#8221; extraction heuristic that requires phrases to be bounded by word alignments to the &#8220;loose&#8221; heuristic that allows unaligned words at phrase edges.</S>
          <S ID="S-51077"><REF ID="R-12" RPTR="20">Lopez (2008)</REF> shows that for a parallel corpus of 107 million words, using the loose heuristic produces much larger grammars and improves performance by a full BLEU point.</S>
          <S ID="S-51078">However, even our baseline system is trained on substantially more data (587 million words on the English side) and the addition of the Giga-FrEn sets increases data size to 745 million words, seven times that used in the cited work.</S>
          <S ID="S-51079">For each data size, we decode with grammars extracted using each heuristic and a 4-gram language model.</S>
          <S ID="S-51080">As shown in Table 4, the differences are much smaller and the tight heuristic actually produces the best result for the full data scenario.</S>
          <S ID="S-51081">We believe this to be directly linked to word alignment quality: smaller training data results in sparser, noisier word alignments while larger data results in denser, more accurate alignments.</S>
          <S ID="S-51082">In the first case, accumulating unaligned words can make up for shortcomings in alignment quality.</S>
          <S ID="S-51083">In the second, better rules are extracted by trusting the stronger alignment model.</S>
          <S ID="S-51084">We also compare 4-gram and 5-gram language model performance with systems using tight grammars extracted from 587 million and 745 million sentences.</S>
          <S ID="S-51085">As shown in Table 5, the 4-gram significantly outperforms the 5-gram with smaller data while the two are indistinguishable with larger data 2 .</S>
          <S ID="S-51086">With modified Kneser-Ney smoothing, a lower order model will outperform a higher order model if the higher order model constantly backs off to lower orders.</S>
          <S ID="S-51087">With stronger grammars learned from larger parallel data, the system is able to produce output that matches longer n-grams in the language model.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>5 Summary</HEADER>
      <P>
        <S ID="S-51103">We have presented the French-English translation system built for the NAACL WMT12 shared translation task, including descriptions of our data selection and text processing techniques.</S>
        <S ID="S-51104">Experimental results have shown incremental improvement for each addition to our baseline system.</S>
        <S ID="S-51105">We have finally discussed the impact of the availability of WMTscale data on system building decisions and provided comparative experimental results.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS/>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>Chris Callison-Burch</RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>of ACL WMT</RAUTHOR>
      <REFTITLE>2 We find that for the full data system, also increasing the cube pruning pop limit and using MBR decoding yields a very slight improvement with the 5-gram model over the same decoding scheme with the 4-gram.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR>F Chen</RAUTHOR>
      <REFTITLE>An Empirical Study of Smoothing Techniques for Language Modeling.</REFTITLE>
      <DATE>1996</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>David Chiang</RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>Michael Denkowski</RAUTHOR>
      <REFTITLE>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>Chris Dyer</RAUTHOR>
      <REFTITLE>cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>Qin Gao</RAUTHOR>
      <REFTITLE>Parallel Implementations of Word Alignment Tool.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>Kenneth Heafield</RAUTHOR>
      <REFTITLE>KenLM: Faster and Smaller Language Model Queries.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>Dan Klein</RAUTHOR>
      <REFTITLE>Accurate Unlexicalized Parsing.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Europarl: A Parallel Corpus for Statistical Machine Translation.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>Shankar Kumar</RAUTHOR>
      <REFTITLE>Minimum Bayes-Risk Decoding for Statistical Machine Translation.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR></RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>Adam Lopez</RAUTHOR>
      <REFTITLE>Tera-Scale Translation Models via Pattern Matching.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR>Franz Josef Och</RAUTHOR>
      <REFTITLE>A Systematic Comparison of Various Statistical Alignment Models.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="14">
      <RAUTHOR>Franz Josef Och</RAUTHOR>
      <REFTITLE>Minimum Error Rate Training for Statistical Machine Translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="15">
      <RAUTHOR>Kishore Papineni</RAUTHOR>
      <REFTITLE>BLEU: a Method for Automatic Evaluation of Machine Translation.</REFTITLE>
      <DATE>2002</DATE>
    </REFERENCE>
    <REFERENCE ID="16">
      <RAUTHOR>Robert Parker</RAUTHOR>
      <REFTITLE>English Gigaword Fourth Edition. Linguistic Data Consortium,</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="17">
      <RAUTHOR>Matthew Snover</RAUTHOR>
      <REFTITLE>A Study of Translation Edit Rate with Targeted Human Annotation.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="18">
      <RAUTHOR></RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
    <REFERENCE ID="19">
      <RAUTHOR>Lucia Specia</RAUTHOR>
      <REFTITLE>Improving the Confidence of Machine Translation Quality Estimates.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
