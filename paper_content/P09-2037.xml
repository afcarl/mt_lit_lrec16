<document>
  <filename>P09-2037</filename>
  <authors>
    <author>Zden&#283;k &#381;abokrtsk&#253;</author>
  </authors>
  <title>Hidden Markov Tree Model in Dependency-based Machine Translation &#8727;</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>We would like to draw attention to Hidden Markov Tree Models (HMTM), which are to our knowledge still unexploited in the field of Computational Linguistics, in spite of highly successful Hidden Markov (Chain) Models. In dependency trees, the independence assumptions made by HMTM correspond to the intuition of linguistic dependency. Therefore we suggest to use HMTM and tree-modified Viterbi algorithm for tasks interpretable as labeling nodes of dependency trees. In particular, we show that the transfer phase in a Machine Translation system based on tectogrammatical dependency trees can be seen as a task suitable for HMTM. When using the HMTM approach for the English-Czech translation, we reach a moderate improvement over the baseline.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We would like to draw attention to Hidden Markov Tree Models (HMTM), which are to our knowledge still unexploited in the field of Computational Linguistics, in spite of highly successful Hidden Markov (Chain) Models.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In dependency trees, the independence assumptions made by HMTM correspond to the intuition of linguistic dependency.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Therefore we suggest to use HMTM and tree-modified Viterbi algorithm for tasks interpretable as labeling nodes of dependency trees.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In particular, we show that the transfer phase in a Machine Translation system based on tectogrammatical dependency trees can be seen as a task suitable for HMTM.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>When using the HMTM approach for the English-Czech translation, we reach a moderate improvement over the baseline.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>Hidden Markov Tree Models (HMTM) were introduced in (Crouse et al., 1998) and used in applications such as image segmentation, signal classification, denoising, and image document categorization, see (Durand et al., 2004) for references.
Although Hidden Markov Models belong to the most successful techniques in Computational Linguistics (CL), the HMTM modification remains to the best of our knowledge unknown in the field.
The first novel claim made in this paper is that the independence assumptions made by Markov Tree Models can be useful for modeling syntactic trees. Especially, they fit dependency trees well, because these models assume conditional dependence (in the probabilistic sense) only along tree
&#8727; The work on this project was supported by the grants
MSM 0021620838, GAAV &#268;R 1ET101120503, and M&#352;MT &#268;R LC536. We thank Jan Haji&#269; and three anonymous reviewers for many useful comments.
edges, which corresponds to intuition behind dependency relations (in the linguistic sense) in dependency trees. Moreover, analogously to applications of HMM on sequence labeling, HMTM can be used for labeling nodes of a dependency tree, interpreted as revealing the hidden states 1 in the tree nodes, given another (observable) labeling of the nodes of the same tree.
The second novel claim is that HMTMs are suitable for modeling the transfer phase in Machine Translation systems based on deep-syntactic dependency trees. Emission probabilities represent the translation model, whereas transition (edge) probabilities represent the target-language tree model. This decomposition can be seen as a tree-shaped analogy to the popular n-gram approaches to Statistical Machine Translation (e.g. (Koehn et al., 2003)), in which translation and language models are trainable separately too. Moreover, given the input dependency tree and HMTM parameters, there is a computationally efficient HMTM-modified Viterbi algorithm for finding the globally optimal target dependency tree.
It should be noted that when using HMTM, the source-language and target-language trees are required to be isomorphic. Obviously, this is an unrealistic assumption in real translation. However, we argue that tectogrammatical deep-syntactic dependency trees (as introduced in the Functional Generative Description framework, (Sgall, 1967)) are relatively close to this requirement, which makes the HMTM approach practically testable.
As for the related work, one can found a number of experiments with dependency-based MT in the literature, e.g., (Boguslavsky et al., 2004), (Menezes and Richardson, 2001), (Bojar, 2008). However, to our knowledge none of the published systems searches for the optimal target representa-
1 HMTM looses the HMM&#8217;s time and finite automaton interpretability, as the observations are not organized linearly. However, the terms &#8220;state&#8221; and &#8220;transition&#8221; are still used.
$ &amp;#&#65533;&#65533;&amp;&#65533;%%# %#&#65533;%&#65533;&#65533;&amp;&#65533;%%#
459&#65533;2,&#65533;89,909700 70;0,&#65533;0/-&#65533;9700'&#65533;907-&#65533;
&#65533;&#65533;//0389,908 &#65533;34/097,38&#65533;,9&#65533;43 &#65533;&#65533;549&#65533;0808&#65533;
%#$#
&#65533;&#65533;;&#65533;
97,38&#65533;9&#65533;43574-8 !&#65533;&#65533;&#65533;:&#65533;&#65533;&#65533;&#65533;;&#65533;&#65533; 1742970024/0&#65533;
&#65533;&#65533;:&#65533; &#65533;&#65533;:&#65533;
02&#65533;88&#65533;43574-8 !&#65533;&#65533;&#65533;:&#65533;&#65533;&#65533;&#65533;:&#65533;&#65533; 1742-,.&#65533;&#65533;,7/ 97,38&#65533;,9&#65533;4324/0&#65533;
tion in a way similar to HMTM.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Hidden Markov Tree Models (HMTM) were introduced in (Crouse et al., 1998) and used in applications such as image segmentation, signal classification, denoising, and image document categorization, see (Durand et al., 2004) for references.</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Although Hidden Markov Models belong to the most successful techniques in Computational Linguistics (CL), the HMTM modification remains to the best of our knowledge unknown in the field.</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The first novel claim made in this paper is that the independence assumptions made by Markov Tree Models can be useful for modeling syntactic trees.</text>
              <doc_id>7</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Especially, they fit dependency trees well, because these models assume conditional dependence (in the probabilistic sense) only along tree</text>
              <doc_id>8</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8727; The work on this project was supported by the grants</text>
              <doc_id>9</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>MSM 0021620838, GAAV &#268;R 1ET101120503, and M&#352;MT &#268;R LC536.</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We thank Jan Haji&#269; and three anonymous reviewers for many useful comments.</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>edges, which corresponds to intuition behind dependency relations (in the linguistic sense) in dependency trees.</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, analogously to applications of HMM on sequence labeling, HMTM can be used for labeling nodes of a dependency tree, interpreted as revealing the hidden states 1 in the tree nodes, given another (observable) labeling of the nodes of the same tree.</text>
              <doc_id>13</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The second novel claim is that HMTMs are suitable for modeling the transfer phase in Machine Translation systems based on deep-syntactic dependency trees.</text>
              <doc_id>14</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Emission probabilities represent the translation model, whereas transition (edge) probabilities represent the target-language tree model.</text>
              <doc_id>15</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This decomposition can be seen as a tree-shaped analogy to the popular n-gram approaches to Statistical Machine Translation (e.g. (Koehn et al., 2003)), in which translation and language models are trainable separately too.</text>
              <doc_id>16</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Moreover, given the input dependency tree and HMTM parameters, there is a computationally efficient HMTM-modified Viterbi algorithm for finding the globally optimal target dependency tree.</text>
              <doc_id>17</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>It should be noted that when using HMTM, the source-language and target-language trees are required to be isomorphic.</text>
              <doc_id>18</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Obviously, this is an unrealistic assumption in real translation.</text>
              <doc_id>19</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, we argue that tectogrammatical deep-syntactic dependency trees (as introduced in the Functional Generative Description framework, (Sgall, 1967)) are relatively close to this requirement, which makes the HMTM approach practically testable.</text>
              <doc_id>20</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>As for the related work, one can found a number of experiments with dependency-based MT in the literature, e.g., (Boguslavsky et al., 2004), (Menezes and Richardson, 2001), (Bojar, 2008).</text>
              <doc_id>21</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, to our knowledge none of the published systems searches for the optimal target representa-</text>
              <doc_id>22</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 HMTM looses the HMM&#8217;s time and finite automaton interpretability, as the observations are not organized linearly.</text>
              <doc_id>23</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, the terms &#8220;state&#8221; and &#8220;transition&#8221; are still used.</text>
              <doc_id>24</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>$ &amp;#&#65533;&#65533;&amp;&#65533;%%# %#&#65533;%&#65533;&#65533;&amp;&#65533;%%#</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>459&#65533;2,&#65533;89,909700 70;0,&#65533;0/-&#65533;9700'&#65533;907-&#65533;</text>
              <doc_id>26</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#65533;&#65533;//0389,908 &#65533;34/097,38&#65533;,9&#65533;43 &#65533;&#65533;549&#65533;0808&#65533;</text>
              <doc_id>27</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>%#$#</text>
              <doc_id>28</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#65533;&#65533;;&#65533;</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>97,38&#65533;9&#65533;43574-8 !&#65533;&#65533;&#65533;:&#65533;&#65533;&#65533;&#65533;;&#65533;&#65533; 1742970024/0&#65533;</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#65533;&#65533;:&#65533; &#65533;&#65533;:&#65533;</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>02&#65533;88&#65533;43574-8 !&#65533;&#65533;&#65533;:&#65533;&#65533;&#65533;&#65533;:&#65533;&#65533; 1742-,.&#65533;&#65533;,7/ 97,38&#65533;,9&#65533;4324/0&#65533;</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tion in a way similar to HMTM.</text>
              <doc_id>33</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Hidden Markov Tree Models</title>
        <text>HMTM are described very briefly in this section. More detailed information can be found in (Durand et al., 2004) and in (Diligenti et al., 2003).
Suppose that V = {v 1 , . . . , v |V | } is the set of tree nodes, r is the root node and &#961; is a function from V \r to V storing the parent node of each non-root node. Suppose two sequences of random variables, X = (X(v 1 ), . . . , X(v |V | )) and Y = (Y (v 1 ), . . . , Y (v |V | )), which label all nodes from V . Let X(v) be understood as a hidden state of the node v, taking a value from a finite state space S = {s 1 , . . . , s K }. Let Y (v) be understood as a symbol observable on the node v, taking a value from an alphabet K = {k 1 , . . . , k 2 }. Analogously to (first-order) HMMs, (first-order) HMTMs make two independence assumptions: (1) given X(&#961;(v)), X(v) is conditionally independent of any other nodes, and (2) given X(v), Y (v) is conditionally independent of any other nodes. Given these independence assumptions, the following factorization formula holds: 2
P (Y , X) = P (Y (r)|X(r))P (X(r)) &#183; &#8719;
P (Y (v)|X(v))P (X(v)|X(&#961;(v))) (1)
v&#8712;V \r
We see that HMTM (analogously to HMM, again) is defined by the following parameters:
2 In this work we limit ourselves to fully stationary
HMTMs. This means that the transition and emission probabilities are independent of v. This &#8220;node invariance&#8221; is an analogy to HMM&#8217;s time invariance.
&#8226; P (X(v)|X(&#961;(v))) &#8211; transition probabilities between the hidden states of two treeadjacent nodes, 3
&#8226; P (Y (v)|X(v)) &#8211; emission probabilities.
Naturally the question appears how to restore the most probable hidden tree labeling given the observed tree labeling (and given the tree topology, of course). As shown in (Durand et al., 2004), a modification of the HMM Viterbi algorithm can be found for HMTM. Briefly, the algorithm starts at leaf nodes and continues upwards, storing in each node for each state and each its child the optimal downward pointer to the child&#8217;s hidden state. When the root is reached, the optimal state tree is retrieved by downward recursion along the pointers from the optimal root state.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>HMTM are described very briefly in this section.</text>
              <doc_id>34</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>More detailed information can be found in (Durand et al., 2004) and in (Diligenti et al., 2003).</text>
              <doc_id>35</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Suppose that V = {v 1 , .</text>
              <doc_id>36</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>37</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>38</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>, v |V | } is the set of tree nodes, r is the root node and &#961; is a function from V \r to V storing the parent node of each non-root node.</text>
              <doc_id>39</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Suppose two sequences of random variables, X = (X(v 1 ), .</text>
              <doc_id>40</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>41</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>42</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>, X(v |V | )) and Y = (Y (v 1 ), .</text>
              <doc_id>43</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>44</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>45</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>, Y (v |V | )), which label all nodes from V .</text>
              <doc_id>46</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>Let X(v) be understood as a hidden state of the node v, taking a value from a finite state space S = {s 1 , .</text>
              <doc_id>47</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>48</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>49</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>, s K }.</text>
              <doc_id>50</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>Let Y (v) be understood as a symbol observable on the node v, taking a value from an alphabet K = {k 1 , .</text>
              <doc_id>51</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>52</doc_id>
              <sec_id>16</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>53</doc_id>
              <sec_id>17</sec_id>
            </sentence>
            <sentence>
              <text>, k 2 }.</text>
              <doc_id>54</doc_id>
              <sec_id>18</sec_id>
            </sentence>
            <sentence>
              <text>Analogously to (first-order) HMMs, (first-order) HMTMs make two independence assumptions: (1) given X(&#961;(v)), X(v) is conditionally independent of any other nodes, and (2) given X(v), Y (v) is conditionally independent of any other nodes.</text>
              <doc_id>55</doc_id>
              <sec_id>19</sec_id>
            </sentence>
            <sentence>
              <text>Given these independence assumptions, the following factorization formula holds: 2</text>
              <doc_id>56</doc_id>
              <sec_id>20</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (Y , X) = P (Y (r)|X(r))P (X(r)) &#183; &#8719;</text>
              <doc_id>57</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (Y (v)|X(v))P (X(v)|X(&#961;(v))) (1)</text>
              <doc_id>58</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>v&#8712;V \r</text>
              <doc_id>59</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We see that HMTM (analogously to HMM, again) is defined by the following parameters:</text>
              <doc_id>60</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2 In this work we limit ourselves to fully stationary</text>
              <doc_id>61</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>HMTMs.</text>
              <doc_id>62</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This means that the transition and emission probabilities are independent of v.</text>
              <doc_id>63</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This &#8220;node invariance&#8221; is an analogy to HMM&#8217;s time invariance.</text>
              <doc_id>64</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; P (X(v)|X(&#961;(v))) &#8211; transition probabilities between the hidden states of two treeadjacent nodes, 3</text>
              <doc_id>65</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8226; P (Y (v)|X(v)) &#8211; emission probabilities.</text>
              <doc_id>66</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Naturally the question appears how to restore the most probable hidden tree labeling given the observed tree labeling (and given the tree topology, of course).</text>
              <doc_id>67</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As shown in (Durand et al., 2004), a modification of the HMM Viterbi algorithm can be found for HMTM.</text>
              <doc_id>68</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Briefly, the algorithm starts at leaf nodes and continues upwards, storing in each node for each state and each its child the optimal downward pointer to the child&#8217;s hidden state.</text>
              <doc_id>69</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>When the root is reached, the optimal state tree is retrieved by downward recursion along the pointers from the optimal root state.</text>
              <doc_id>70</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>3 Tree Transfer as a Task for HMTM</title>
        <text>HMTM Assumptions from the MT Viewpoint. We suggest to use HMTM in the conventional tree-based analysis-transfer-synthesis translation scheme: (1) First we analyze an input sentence to a certain level of abstraction on which the sentence representation is tree-shaped. (2) Then we use HMTM-modified Viterbi algorithm for creating the target-language tree from the source-language tree. Labels on the source-language nodes are treated as emitted (observable) symbols, while labels on the target-language nodes are understood as hidden states which are being searched for
3 The need for parametrizing also P (X(r)) (prior probabilites of hidden states in the root node) can be avoided by adding an artificial root whose state is fixed.
(Figure 1). (3) Finally, we synthesize the targetlanguage sentence from the target-language tree.
In the HMTM transfer step, the HMTM emission probabilities can be interpreted as probabilities from the &#8220;backward&#8221; (source given target) node-to-node translation model. HMTM transition probabilities can be interpreted as probabilities from the target-language tree model. This is an important feature from the MT viewpoint, since the decomposition into translation model and language model proved to be extremely useful in statistical MT since (Brown et al., 1993). It allows to compensate the lack of parallel resources by the relative abundance of monolingual resources. Another advantage of the HMTM approach is that it allows us to disregard the ordering of decisions made with the individual nodes (which would be otherwise nontrivial, as for a given node there might be constraints and preferences coming both from its parent and from its children). Like in HMM, it is the notion of hidden states that facilitates &#8220;summarizing&#8221; distributed information and finding the global optimum.
On the other hand, there are several limitations implied by HMTMs which we have to consider before applying it to MT: (1) There can be only one labeling function on the source-language nodes, and one labeling function on the target-language nodes. (2) The set of hidden states and the alphabet of emitted symbols must be finite. (3) The source-language tree and the target-language tree are required to be isomorphic. In other words, only node labeling can be changed in the transfer step.
The first two assumption are easy to fulfill, but the third assumption concerning the tree isomorphism is problematic. There is no known linguistic theory guaranteeing identically shaped tree representations of a sentence and its translation. However, we would like to show in the following that the tectogrammatical layer of language description is close enough to this ideal to make the HMTM approach practically applicable.
Why Tectogrammatical Trees? Tectogrammatical layer of language description was introduced within the Functional Generative Description framework, (Sgall, 1967) and has been further elaborated in the Prague Dependency Treebank project, (Haji&#269; and others, 2006).
On the tectogrammatical layer, each sentence is represented as a tectogrammatical tree (t-tree for short; abbreviations t-node and t-layer are used in the further text too). The main features of t-trees (from the viewpoint of our experiments) are following. Each sentence is represented as a dependency tree, whose nodes correspond to autosemantic (meaningful) words and whose edges correspond to syntactic-semantic relations (dependencies). The nodes are labeled with the lemmas of the autosemantic words. Functional words (such as prepositions, auxiliary verbs, and subordinating conjunctions) do not have nodes of their own. Information conveyed by word inflection or functional words in the surface sentence shape is represented by specialized semantic attributes attached to t-nodes (such as number or tense).
T-trees are still language specific (e.g. because of lemmas), but they largely abstract from language-specific means of expressing non-lexical meanings (such as inflection, agglutination, functional words). Next reason for using t-trees as the transfer medium is that they allow for a natural transfer factorization. One can separate the transfer into three relatively independent channels: 4 (1) transfer of lexicalization (stored in t-node&#8217;s lemma attribute), (2) transfer of syntactizations (stored in t-node&#8217;s formeme attribute), 5 and (3) transfer of semantically indispensable grammatical categories 6 such as number with nouns and tense with verbs (stored in specialized t-node&#8217;s attributes).
Another motivation for using t-trees is that we believe that local tree contexts in t-trees carry more information relevant for correct lexical choice, compared to linear contexts in the surface sentence shapes, mainly because of long-distance dependencies and coordination structures.
Observed Symbols, Hidden States, and HMTM Parameters. The most difficult part of the tectogrammatical transfer step lies in transfer-
4 Full independence assumption about the three channels
would be inadequate, but it can be at least used for smoothing the translation probabilities. 5 Under the term syntactization (the second channel) we
understand morphosyntactic form &#8211; how the given lemma is &#8220;shaped&#8221; on the surface. We use the t-node attribute formeme (which is not a genuine element of the semantically oriented t-layer, but rather only a technical means that facilitates modeling the transition between t-trees and surface sentence shapes) to capture syntactization of the given t-node, with values such as n:subj &#8211; semantic noun (s.n.) in subject position, n:for+X &#8211; s.n. with preposition for, n:poss &#8211; possessive form of s.n., v:because+fin &#8211; semantic verb as a subordinating finite clause introduced by because), adj:attr &#8211; semantic adjective in attributive position. 6 Categories only imposed by grammatical constraints
(e.g. grammatical number with verbs imposed by subjectverb agreement in Czech) are disregarded on the t-layer.
ring lexicalization and syntactization (attributes lemma and formeme), while the other attributes (node ordering, grammatical number, gender, tense, person, negation, degree of comparison etc.) can be transferred by much less complex methods. As there can be only one input labeling function, we treat the following ordered pair as the observed symbol: Y (v) = (L src (v), F src (v)) where L src (v) is the source-language lemma of the node v and F src (v) is its source-language formeme. Analogously, hidden state of node v is the ordered couple X(v) = (L trg (v), F trg (v)), where L trg (v) is the target-language lemma of the node v and F trg (v) is its target-language formeme. Parameters of such HMTM are then following:
P (X(v)|X(&#961;(v))) = P (L trg (v), F trg (v)|L trg (&#961;(v)), F trg (&#961;(v)))
&#8211; probability of a node labeling given its parent labeling; it can be estimated from a parsed target-language monolingual corpus, and
P (Y (v)|X(v)) = P (L src (v), F src (v)|L trg (v), F trg (v))
&#8211; backward translation probability; it can be estimated from a parsed and aligned parallel corpus. To summarize: the task of tectogrammatical transfer can be formulated as revealing the values of node labeling functions L trg and F trg given the tree topology and given the values of node labeling functions L src and F src . Given the HMTM parameters specified above, the task can be solved using HMTM-modified Viterbi algorithm by interpreting the first pair as the hidden state and the second pair as the observation.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>HMTM Assumptions from the MT Viewpoint.</text>
              <doc_id>71</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We suggest to use HMTM in the conventional tree-based analysis-transfer-synthesis translation scheme: (1) First we analyze an input sentence to a certain level of abstraction on which the sentence representation is tree-shaped.</text>
              <doc_id>72</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>(2) Then we use HMTM-modified Viterbi algorithm for creating the target-language tree from the source-language tree.</text>
              <doc_id>73</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Labels on the source-language nodes are treated as emitted (observable) symbols, while labels on the target-language nodes are understood as hidden states which are being searched for</text>
              <doc_id>74</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3 The need for parametrizing also P (X(r)) (prior probabilites of hidden states in the root node) can be avoided by adding an artificial root whose state is fixed.</text>
              <doc_id>75</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(Figure 1).</text>
              <doc_id>76</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>(3) Finally, we synthesize the targetlanguage sentence from the target-language tree.</text>
              <doc_id>77</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the HMTM transfer step, the HMTM emission probabilities can be interpreted as probabilities from the &#8220;backward&#8221; (source given target) node-to-node translation model.</text>
              <doc_id>78</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>HMTM transition probabilities can be interpreted as probabilities from the target-language tree model.</text>
              <doc_id>79</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This is an important feature from the MT viewpoint, since the decomposition into translation model and language model proved to be extremely useful in statistical MT since (Brown et al., 1993).</text>
              <doc_id>80</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>It allows to compensate the lack of parallel resources by the relative abundance of monolingual resources.</text>
              <doc_id>81</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Another advantage of the HMTM approach is that it allows us to disregard the ordering of decisions made with the individual nodes (which would be otherwise nontrivial, as for a given node there might be constraints and preferences coming both from its parent and from its children).</text>
              <doc_id>82</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Like in HMM, it is the notion of hidden states that facilitates &#8220;summarizing&#8221; distributed information and finding the global optimum.</text>
              <doc_id>83</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>On the other hand, there are several limitations implied by HMTMs which we have to consider before applying it to MT: (1) There can be only one labeling function on the source-language nodes, and one labeling function on the target-language nodes.</text>
              <doc_id>84</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>(2) The set of hidden states and the alphabet of emitted symbols must be finite.</text>
              <doc_id>85</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>(3) The source-language tree and the target-language tree are required to be isomorphic.</text>
              <doc_id>86</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In other words, only node labeling can be changed in the transfer step.</text>
              <doc_id>87</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The first two assumption are easy to fulfill, but the third assumption concerning the tree isomorphism is problematic.</text>
              <doc_id>88</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>There is no known linguistic theory guaranteeing identically shaped tree representations of a sentence and its translation.</text>
              <doc_id>89</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, we would like to show in the following that the tectogrammatical layer of language description is close enough to this ideal to make the HMTM approach practically applicable.</text>
              <doc_id>90</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Why Tectogrammatical Trees?</text>
              <doc_id>91</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Tectogrammatical layer of language description was introduced within the Functional Generative Description framework, (Sgall, 1967) and has been further elaborated in the Prague Dependency Treebank project, (Haji&#269; and others, 2006).</text>
              <doc_id>92</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>On the tectogrammatical layer, each sentence is represented as a tectogrammatical tree (t-tree for short; abbreviations t-node and t-layer are used in the further text too).</text>
              <doc_id>93</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The main features of t-trees (from the viewpoint of our experiments) are following.</text>
              <doc_id>94</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Each sentence is represented as a dependency tree, whose nodes correspond to autosemantic (meaningful) words and whose edges correspond to syntactic-semantic relations (dependencies).</text>
              <doc_id>95</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The nodes are labeled with the lemmas of the autosemantic words.</text>
              <doc_id>96</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Functional words (such as prepositions, auxiliary verbs, and subordinating conjunctions) do not have nodes of their own.</text>
              <doc_id>97</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Information conveyed by word inflection or functional words in the surface sentence shape is represented by specialized semantic attributes attached to t-nodes (such as number or tense).</text>
              <doc_id>98</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>T-trees are still language specific (e.g. because of lemmas), but they largely abstract from language-specific means of expressing non-lexical meanings (such as inflection, agglutination, functional words).</text>
              <doc_id>99</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Next reason for using t-trees as the transfer medium is that they allow for a natural transfer factorization.</text>
              <doc_id>100</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>One can separate the transfer into three relatively independent channels: 4 (1) transfer of lexicalization (stored in t-node&#8217;s lemma attribute), (2) transfer of syntactizations (stored in t-node&#8217;s formeme attribute), 5 and (3) transfer of semantically indispensable grammatical categories 6 such as number with nouns and tense with verbs (stored in specialized t-node&#8217;s attributes).</text>
              <doc_id>101</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Another motivation for using t-trees is that we believe that local tree contexts in t-trees carry more information relevant for correct lexical choice, compared to linear contexts in the surface sentence shapes, mainly because of long-distance dependencies and coordination structures.</text>
              <doc_id>102</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Observed Symbols, Hidden States, and HMTM Parameters.</text>
              <doc_id>103</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The most difficult part of the tectogrammatical transfer step lies in transfer-</text>
              <doc_id>104</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>4 Full independence assumption about the three channels</text>
              <doc_id>105</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>would be inadequate, but it can be at least used for smoothing the translation probabilities.</text>
              <doc_id>106</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>5 Under the term syntactization (the second channel) we</text>
              <doc_id>107</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>understand morphosyntactic form &#8211; how the given lemma is &#8220;shaped&#8221; on the surface.</text>
              <doc_id>108</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We use the t-node attribute formeme (which is not a genuine element of the semantically oriented t-layer, but rather only a technical means that facilitates modeling the transition between t-trees and surface sentence shapes) to capture syntactization of the given t-node, with values such as n:subj &#8211; semantic noun (s.n.</text>
              <doc_id>109</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>) in subject position, n:for+X &#8211; s.n.</text>
              <doc_id>110</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>with preposition for, n:poss &#8211; possessive form of s.n., v:because+fin &#8211; semantic verb as a subordinating finite clause introduced by because), adj:attr &#8211; semantic adjective in attributive position.</text>
              <doc_id>111</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>6 Categories only imposed by grammatical constraints</text>
              <doc_id>112</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(e.g. grammatical number with verbs imposed by subjectverb agreement in Czech) are disregarded on the t-layer.</text>
              <doc_id>113</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>ring lexicalization and syntactization (attributes lemma and formeme), while the other attributes (node ordering, grammatical number, gender, tense, person, negation, degree of comparison etc.) can be transferred by much less complex methods.</text>
              <doc_id>114</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As there can be only one input labeling function, we treat the following ordered pair as the observed symbol: Y (v) = (L src (v), F src (v)) where L src (v) is the source-language lemma of the node v and F src (v) is its source-language formeme.</text>
              <doc_id>115</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Analogously, hidden state of node v is the ordered couple X(v) = (L trg (v), F trg (v)), where L trg (v) is the target-language lemma of the node v and F trg (v) is its target-language formeme.</text>
              <doc_id>116</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Parameters of such HMTM are then following:</text>
              <doc_id>117</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (X(v)|X(&#961;(v))) = P (L trg (v), F trg (v)|L trg (&#961;(v)), F trg (&#961;(v)))</text>
              <doc_id>118</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8211; probability of a node labeling given its parent labeling; it can be estimated from a parsed target-language monolingual corpus, and</text>
              <doc_id>119</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>P (Y (v)|X(v)) = P (L src (v), F src (v)|L trg (v), F trg (v))</text>
              <doc_id>120</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8211; backward translation probability; it can be estimated from a parsed and aligned parallel corpus.</text>
              <doc_id>121</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To summarize: the task of tectogrammatical transfer can be formulated as revealing the values of node labeling functions L trg and F trg given the tree topology and given the values of node labeling functions L src and F src .</text>
              <doc_id>122</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Given the HMTM parameters specified above, the task can be solved using HMTM-modified Viterbi algorithm by interpreting the first pair as the hidden state and the second pair as the observation.</text>
              <doc_id>123</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Experiment</title>
        <text>To check the real applicability of HMTM transfer, we performed the following preliminary MT experiment. First, we used the tectogrammar-based MT system described in (&#381;abokrtsk&#253; et al., 2008) as a baseline. 7 Then we substituted its transfer phase by the HMTM variant, with parameters estimated from 800 million word Czech corpus and 60 million word parallel corpus. As shown in Table 1, the HMTM approach outperforms the baseline solution both in terms of BLEU and NIST metrics.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>To check the real applicability of HMTM transfer, we performed the following preliminary MT experiment.</text>
              <doc_id>124</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>First, we used the tectogrammar-based MT system described in (&#381;abokrtsk&#253; et al., 2008) as a baseline.</text>
              <doc_id>125</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>7 Then we substituted its transfer phase by the HMTM variant, with parameters estimated from 800 million word Czech corpus and 60 million word parallel corpus.</text>
              <doc_id>126</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As shown in Table 1, the HMTM approach outperforms the baseline solution both in terms of BLEU and NIST metrics.</text>
              <doc_id>127</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>5 Conclusion</title>
        <text>HMTM is a new approach in the field of CL. In our opinion, it has a big potential for modeling syntac- 7 For evaluation purposes we used 2700 sentences from the evaluation section of WMT 2009 Shared Translation
tic trees. To show how it can be used, we applied HMTM in an experiment on English-Czech treebased Machine Translation and reached an improvement over the solution without HMTM.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>HMTM is a new approach in the field of CL.</text>
              <doc_id>128</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In our opinion, it has a big potential for modeling syntac- 7 For evaluation purposes we used 2700 sentences from the evaluation section of WMT 2009 Shared Translation</text>
              <doc_id>129</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>tic trees.</text>
              <doc_id>130</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>To show how it can be used, we applied HMTM in an experiment on English-Czech treebased Machine Translation and reached an improvement over the solution without HMTM.</text>
              <doc_id>131</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TET</source>
        <caption>Table 1: Evaluation of English-Czech translation.</caption>
        <reference_text></reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>System</cell>
              <cell>BLEU</cell>
              <cell>NIST</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>baseline system</cell>
              <cell>0.0898</cell>
              <cell>4.5672</cell>
            </row>
            <row>
              <cell>HMTM modification 0.1043</cell>
              <cell>4.8445</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Igor Boguslavsky</author>
          <author>Leonid Iomdin</author>
          <author>Victor Sizov</author>
        </authors>
        <title>Multilinguality in ETAP-3: Reuse of Lexical Resources.</title>
        <publication>In Proceedings of Workshop Multilingual Linguistic Resources, COLING,</publication>
        <pages>7--14</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Ond&#345;ej Bojar</author>
        </authors>
        <title>Exploiting Linguistic Data in Machine Translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>Della Pietra</author>
          <author>Robert L Mercer</author>
        </authors>
        <title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1993</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Matthew Crouse</author>
          <author>Robert Nowak</author>
          <author>Richard Baraniuk</author>
        </authors>
        <title>Wavelet-based statistical signal processing using hidden markov models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>Michelangelo Diligenti</author>
          <author>Paolo Frasconi</author>
          <author>Marco Gori</author>
        </authors>
        <title>Hidden tree Markov models for document image classification.</title>
        <publication>None</publication>
        <pages>25--2003</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Jean-Baptiste Durand</author>
          <author>Paulo Goncalv&#232;s</author>
          <author>Yann Gu&#233;don</author>
        </authors>
        <title>Computational methods for hidden Markov tree models - An application to wavelet trees.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Jan Haji&#269;</author>
        </authors>
        <title>None</title>
        <publication>Prague Dependency Treebank 2.0. Linguistic Data Consortium, LDC Catalog No.: LDC2006T01,</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Philipp Koehn</author>
          <author>Franz Josef Och</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>Statistical phrase based translation.</title>
        <publication>In Proceedings of the HLT/NAACL.</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Arul Menezes</author>
          <author>Stephen D Richardson</author>
        </authors>
        <title>A best-first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora.</title>
        <publication>In Proceedings of the workshop on Data-driven methods in machine translation,</publication>
        <pages>1--8</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Petr Sgall</author>
        </authors>
        <title>Generativn&#237; popis jazyka a &#269;esk&#225; deklinace.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1967</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Boguslavsky et al., 2004</string>
        <sentence_id>28625</sentence_id>
        <char_offset>114</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Bojar, 2008</string>
        <sentence_id>28625</sentence_id>
        <char_offset>174</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>3</reference_id>
        <string>Crouse et al., 1998</string>
        <sentence_id>28609</sentence_id>
        <char_offset>53</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>4</reference_id>
        <string>Diligenti et al., 2003</string>
        <sentence_id>28639</sentence_id>
        <char_offset>72</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>5</reference_id>
        <string>Durand et al., 2004</string>
        <sentence_id>28609</sentence_id>
        <char_offset>201</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>5</reference_id>
        <string>Durand et al., 2004</string>
        <sentence_id>28639</sentence_id>
        <char_offset>43</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>5</reference_id>
        <string>Durand et al., 2004</string>
        <sentence_id>28672</sentence_id>
        <char_offset>13</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>7</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>28620</sentence_id>
        <char_offset>131</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>8</reference_id>
        <string>Menezes and Richardson, 2001</string>
        <sentence_id>28625</sentence_id>
        <char_offset>142</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>9</reference_id>
        <string>Sgall, 1967</string>
        <sentence_id>28624</sentence_id>
        <char_offset>140</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>9</reference_id>
        <string>Sgall, 1967</string>
        <sentence_id>28696</sentence_id>
        <char_offset>119</char_offset>
      </citation>
    </citations>
  </content>
</document>
