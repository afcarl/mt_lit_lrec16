<document>
  <filename>W11-2150</filename>
  <authors>
    <author>Maxim Khalilov</author>
  </authors>
  <title>ILLC-UvA translation system for EMNLP-WMT 2011</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>In this paper we describe the Institute for Logic, Language and Computation (University of Amsterdam) phrase-based statistical machine translation system for Englishto-German translation proposed within the EMNLP-WMT 2011 shared task. The main novelty of the submitted system is a syntaxdriven pre-translation reordering algorithm implemented as source string permutation via transfer of the source-side syntax tree.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this paper we describe the Institute for Logic, Language and Computation (University of Amsterdam) phrase-based statistical machine translation system for Englishto-German translation proposed within the EMNLP-WMT 2011 shared task.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The main novelty of the submitted system is a syntaxdriven pre-translation reordering algorithm implemented as source string permutation via transfer of the source-side syntax tree.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>For the WMT 2011 shared task, ILLC-UvA submitted two translations (primary and secondary) for the English-to-German translation task. This year, we directed our research toward addressing the word order problem for statistical machine translation (SMT) and discover its impact on output translation quality. We reorder the words of a sentence of the source language with respect to the word order of the target language and a given source-side parse tree. The difference from the baseline Moses-based translation system lies in the pre-translation step, in which we introduce a discriminative source string permutation model based on probabilistic parse tree transduction.
The idea here is to permute the order of the source words in such a way that the resulting permutation allows as monotone a translation process as possible is not new. This approach to enhance SMT by using a reordering step prior to translation has proved to be successful in improving translation quality for many translation tasks, see (Genzel, 2010; Costa-juss&#224; and Fonollosa, 2006; Collins et al., 2005), for example.
The general problem of source-side reordering is that the number of permutations is factorial in n, and learning a sequence of transductions for explaining a source permutation can be computationally rather challenging. We propose to address this problem by defining the source-side permutation process as the learning problem of how to transfer a given source parse tree into a parse tree that minimizes the divergence from target word order.
Our reordering system is inspired by the direction taken in (Tromble and Eisner, 2009), but differs in defining the space of permutations, using local probabilistic tree transductions, as well as in the learning objective aiming at scoring permutations based on a log-linear interpolation of a local syntax-based model with a global string-based (language) model.
The reordering (novel) and translation (standard) components are described in the following sections. The rest of this paper is structured as follows. After a brief description of the phrase-based translation system in Section 2, we present the architecture and details of our reordering system (Section 3), Section 4 reviews related work, Section 5 reports the experimental setup, details the submissions and discusses the results, while Section 6 concludes the article.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>For the WMT 2011 shared task, ILLC-UvA submitted two translations (primary and secondary) for the English-to-German translation task.</text>
              <doc_id>2</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This year, we directed our research toward addressing the word order problem for statistical machine translation (SMT) and discover its impact on output translation quality.</text>
              <doc_id>3</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We reorder the words of a sentence of the source language with respect to the word order of the target language and a given source-side parse tree.</text>
              <doc_id>4</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The difference from the baseline Moses-based translation system lies in the pre-translation step, in which we introduce a discriminative source string permutation model based on probabilistic parse tree transduction.</text>
              <doc_id>5</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The idea here is to permute the order of the source words in such a way that the resulting permutation allows as monotone a translation process as possible is not new.</text>
              <doc_id>6</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This approach to enhance SMT by using a reordering step prior to translation has proved to be successful in improving translation quality for many translation tasks, see (Genzel, 2010; Costa-juss&#224; and Fonollosa, 2006; Collins et al., 2005), for example.</text>
              <doc_id>7</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The general problem of source-side reordering is that the number of permutations is factorial in n, and learning a sequence of transductions for explaining a source permutation can be computationally rather challenging.</text>
              <doc_id>8</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We propose to address this problem by defining the source-side permutation process as the learning problem of how to transfer a given source parse tree into a parse tree that minimizes the divergence from target word order.</text>
              <doc_id>9</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Our reordering system is inspired by the direction taken in (Tromble and Eisner, 2009), but differs in defining the space of permutations, using local probabilistic tree transductions, as well as in the learning objective aiming at scoring permutations based on a log-linear interpolation of a local syntax-based model with a global string-based (language) model.</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The reordering (novel) and translation (standard) components are described in the following sections.</text>
              <doc_id>11</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The rest of this paper is structured as follows.</text>
              <doc_id>12</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>After a brief description of the phrase-based translation system in Section 2, we present the architecture and details of our reordering system (Section 3), Section 4 reviews related work, Section 5 reports the experimental setup, details the submissions and discusses the results, while Section 6 concludes the article.</text>
              <doc_id>13</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Baseline system</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>14</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Statistical machine translation</title>
            <text>In SMT the translation problem is formulated as selecting the target translation t with the highest probability from a set of target hypothesis sentences for
the source sentence s: &#710;t = arg max { p(t|s) } = arg max { p(s|t) &#183; p(t) }.
t</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In SMT the translation problem is formulated as selecting the target translation t with the highest probability from a set of target hypothesis sentences for</text>
                  <doc_id>15</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the source sentence s: &#710;t = arg max { p(t|s) } = arg max { p(s|t) &#183; p(t) }.</text>
                  <doc_id>16</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>t</text>
                  <doc_id>17</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 Phrase-based translation</title>
            <text>While first systems following this approach performed translation on the word level, modern stateof-the-art phrase-based SMT systems (Och and Ney, 2002; Koehn et al., 2003) start-out from a wordaligned parallel corpus working with (in principle) arbitrarily large phrase pairs (also called blocks) acquired from word-aligned parallel data under a simple definition of translational equivalence (Zens et al., 2002).
The conditional probabilities of one phrase given its counterpart is estimated as the relative frequency ratio of the phrases in the multiset of phrase-pairs extracted from the parallel corpus and are interpolated log-linearly together with a set of other model estimates:
&#234; I 1 = arg max
e I 1
{ &#8721; M }
&#955; m h m (e I 1, f1 J )
m=1
t
(1)
where a feature function h m refer to a system model, and the corresponding &#955; m refers to the relative weight given to this model.
A phrase-based system employs feature functions for a phrase pair translation model, a language model, a reordering model, and a model to score translation hypothesis according to length. The weights &#955; m are optimized for system performance (Och, 2003) as measured by BLEU (Papineni et al., 2002).
Apart from the novel syntax-based reordering model, we consider two reordering methods that are widely used in phrase-based systems: a simple distance-based reordering and a lexicalized blockoriented data-driven reordering model (Tillman, 2004).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>While first systems following this approach performed translation on the word level, modern stateof-the-art phrase-based SMT systems (Och and Ney, 2002; Koehn et al., 2003) start-out from a wordaligned parallel corpus working with (in principle) arbitrarily large phrase pairs (also called blocks) acquired from word-aligned parallel data under a simple definition of translational equivalence (Zens et al., 2002).</text>
                  <doc_id>18</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The conditional probabilities of one phrase given its counterpart is estimated as the relative frequency ratio of the phrases in the multiset of phrase-pairs extracted from the parallel corpus and are interpolated log-linearly together with a set of other model estimates:</text>
                  <doc_id>19</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#234; I 1 = arg max</text>
                  <doc_id>20</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>e I 1</text>
                  <doc_id>21</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>{ &#8721; M }</text>
                  <doc_id>22</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#955; m h m (e I 1, f1 J )</text>
                  <doc_id>23</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>m=1</text>
                  <doc_id>24</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>t</text>
                  <doc_id>25</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(1)</text>
                  <doc_id>26</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where a feature function h m refer to a system model, and the corresponding &#955; m refers to the relative weight given to this model.</text>
                  <doc_id>27</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A phrase-based system employs feature functions for a phrase pair translation model, a language model, a reordering model, and a model to score translation hypothesis according to length.</text>
                  <doc_id>28</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The weights &#955; m are optimized for system performance (Och, 2003) as measured by BLEU (Papineni et al., 2002).</text>
                  <doc_id>29</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Apart from the novel syntax-based reordering model, we consider two reordering methods that are widely used in phrase-based systems: a simple distance-based reordering and a lexicalized blockoriented data-driven reordering model (Tillman, 2004).</text>
                  <doc_id>30</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Architecture of the reordering system</title>
        <text>We approach the word order challenge by including syntactic information in a pre-translation reordering framework. This section details the general idea of our approach and details the reordering model that was used in English-to-German experiments.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We approach the word order challenge by including syntactic information in a pre-translation reordering framework.</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This section details the general idea of our approach and details the reordering model that was used in English-to-German experiments.</text>
              <doc_id>32</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>3.1 Pre-translation reordering framework</title>
            <text>Given a word-aligned parallel corpus, we define the source string permutation as the task of learning to unfold the crossing alignments between sentence pairs in the parallel corpus. Let be given a sourcetarget sentence pair s &#8594; t with word alignment set a between their words. Unfolding the crossing instances in a should lead to as monotone an alignment a &#8242; as possible between a permutation s &#8242; of s and the target string t. Conducting such a &#8220;monotonization&#8221; on the parallel corpus gives two parallel corpora: (1) a source-to-permutation parallel corpus (s &#8594; s &#8242; ) and (2) a source permutation-totarget parallel corpus (s &#8242; &#8594; t). The latter corpus is word-aligned automatically again and used for training a phrase-based translation system, while the former corpus is used for training our model for pretranslation source permutation via parse tree transductions. In itself, the problem of permuting the source string to unfold the crossing alignments is computationally intractable (see (Tromble and Eisner, 2009)). However, different kinds of constraints can be made on unfolding the crossing alignments in a. A common approach in hierarchical SMT is to assume that the source string has a binary parse tree, and the set of eligible permutations is defined by binary ITG transductions on this tree. This defines permutations that can be obtained only by at most inverting pairs of children under nodes of the source tree.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Given a word-aligned parallel corpus, we define the source string permutation as the task of learning to unfold the crossing alignments between sentence pairs in the parallel corpus.</text>
                  <doc_id>33</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Let be given a sourcetarget sentence pair s &#8594; t with word alignment set a between their words.</text>
                  <doc_id>34</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Unfolding the crossing instances in a should lead to as monotone an alignment a &#8242; as possible between a permutation s &#8242; of s and the target string t. Conducting such a &#8220;monotonization&#8221; on the parallel corpus gives two parallel corpora: (1) a source-to-permutation parallel corpus (s &#8594; s &#8242; ) and (2) a source permutation-totarget parallel corpus (s &#8242; &#8594; t).</text>
                  <doc_id>35</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The latter corpus is word-aligned automatically again and used for training a phrase-based translation system, while the former corpus is used for training our model for pretranslation source permutation via parse tree transductions.</text>
                  <doc_id>36</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In itself, the problem of permuting the source string to unfold the crossing alignments is computationally intractable (see (Tromble and Eisner, 2009)).</text>
                  <doc_id>37</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>However, different kinds of constraints can be made on unfolding the crossing alignments in a.</text>
                  <doc_id>38</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>A common approach in hierarchical SMT is to assume that the source string has a binary parse tree, and the set of eligible permutations is defined by binary ITG transductions on this tree.</text>
                  <doc_id>39</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>This defines permutations that can be obtained only by at most inverting pairs of children under nodes of the source tree.</text>
                  <doc_id>40</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>3.2 Conditional tree reordering model</title>
            <text>Given a parallel corpus with string pairs s &#8594; t with word alignment a, the source strings s are parsed, leading to a single parse tree &#964; s per source string. We create a source permuted parallel corpus s &#8594; s &#8242; by unfolding the crossing alignments in a without/with syntactic tree to provide constraints on the unfolding.
Our model aims at learning from the source permuted parallel corpus s &#8594; s &#8242; a probabilistic optimization arg max &#960;(s) P (&#960;(s) | s, &#964; s ). We assume that the set of permutations {&#960;(s)} is defined through a finite set of local transductions over the tree &#964; s . Hence, we view the permutations leading from s to s &#8242; as a sequence of local tree transduc-
tions &#964; s
&#8242; 0
&#8594; . . . &#8594; &#964; s
&#8242;n , where s&#8242; 0 = s and s&#8242; n = s &#8242; ,
is defined using a and each transduction &#964; &#8242; s
&#8594; &#964; &#8242;
i&#8722;1 s i
tree transduction operation that at most permutes the children of a single node in &#964; s
&#8242; i&#8722;1
as defined next.
A local transduction &#964; &#8242; s &#8594; &#964; &#8242;
i&#8722;1 s
is modelled by
i
an operation that applies to a single node with address x in &#964; s
&#8242; i&#8722;1
, labeled N x , and may permute the ordered sequence of children &#945; x dominated by node x. This constitutes a direct generalization of the ITG binary inversion transduction operation. We assign a conditional probability to each such local transduction:
P (&#964; &#8242; s | &#964; &#8242;
i s
) &#8776; P (&#960;(&#945; x ) | N x &#8594; &#945; x , C x ) (2)
i&#8722;1
where &#960;(&#945; x ) is a permutation of &#945; x (the ordered sequence of node labels under x) and C x is a local tree context of node x in tree &#964; &#8242; s
. One wrin-
i&#8722;1
kle in this definition is that the number of possible permutations of &#945; x is factorial in the length of &#945; x . Fortunately, the source permuted training data exhibits only a fraction of possible permutations even for longer &#945; x sequences. Furthermore, by conditioning the probability on local context, the general applicability of the permutation is restrained.
In principle, if we would disregard the computational cost, we could define the probability of the sequence of local tree transductions &#964; s
&#8242; 0
&#8594; . . . &#8594; &#964; s
&#8242; n
as
P (&#964; &#8242; s &#8594; . . . &#8594; &#964;
0 s &#8242; n ) = n &#8719;
i=1
P (&#964; &#8242; s | &#964; &#8242;
i s
) (3)
i&#8722;1
The problem of calculating the most likely permutation under this kind of transduction probability is intractable because every local transduction conditions on local context of an intermediate tree 1 . Hence, we disregard this formulation and in practice we take a pragmatic approach and greedily select at every intermediate point &#964; &#8242; s
&#8594; &#964; &#8242;
i&#8722;1 s
the single most
i
likely local transduction that can be conducted on any node of the current intermediate tree &#964; s
&#8242; i&#8722;1
. The
1 Note that a single transduction step on the current tree
&#964; &#8242; s leads to a forest of trees &#964; &#8242;
i&#8722;1 s
because there can be multiple alternative transduction rules. Hence, this kind of a model
i
demands optimization over many possible sequences of trees, which can be packed into a sequence of parse-forests with transduction links between them.
individual steps are made more effective by interpolating the term in Equation 2 with string probability ratios:
(
&#8242;
P (s i&#8722;1 P (&#960;(&#945; x ) | N x &#8594; &#945; x , C x ) &#215; ) ) P (s &#8242; i ) (4)
The rationale behind this interpolation is that our source permutation approach aims at finding the optimal permutation s &#8242; of s that can serve as input for a subsequent translation model. Hence, we aim at tree transductions that are syntactically motivated that also lead to improved string permutations. In this sense, the tree transduction definitions can be seen as an efficient and syntactically informed way to define the space of possible permutations.
We estimate the string probabilities P (s &#8242; i ) using 5-gram language models trained on the s &#8242; side of the source permuted parallel corpus s &#8594; s &#8242; . We estimate the conditional probability P (&#960;(&#945; x ) | N x &#8594; &#945; x , C x ) using a Maximum-Entropy framework, where feature functions are defined to capture the permutation as a class, the node label N x and its head POS tag, the child sequence &#945; x together with the corresponding sequence of head POS tags and other features corresponding to different contextual information.
We were particularly interested in those linguistic features that motivate reordering phenomena from the syntactic and linguistic perspective. The features that were used for training the permutation system are extracted for every internal node of the source tree that has more than one child:
&#8226; Local tree topology. Sub-tree instances that include parent node and the ordered sequence of child node labels.
&#8226; Dependency features. Features that determine the POS tag of the head word of the current node, together with the sequence of POS tags of the head words of its child nodes.
&#8226; Syntactic features. Two binary features from this class describe: (1) whether the parent node is a child of the node annotated with the same syntactic category, (2) whether the parent node is a descendant of a node annotated with the same syntactic category.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Given a parallel corpus with string pairs s &#8594; t with word alignment a, the source strings s are parsed, leading to a single parse tree &#964; s per source string.</text>
                  <doc_id>41</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We create a source permuted parallel corpus s &#8594; s &#8242; by unfolding the crossing alignments in a without/with syntactic tree to provide constraints on the unfolding.</text>
                  <doc_id>42</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Our model aims at learning from the source permuted parallel corpus s &#8594; s &#8242; a probabilistic optimization arg max &#960;(s) P (&#960;(s) | s, &#964; s ).</text>
                  <doc_id>43</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We assume that the set of permutations {&#960;(s)} is defined through a finite set of local transductions over the tree &#964; s .</text>
                  <doc_id>44</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Hence, we view the permutations leading from s to s &#8242; as a sequence of local tree transduc-</text>
                  <doc_id>45</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>tions &#964; s</text>
                  <doc_id>46</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8242; 0</text>
                  <doc_id>47</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8594; .</text>
                  <doc_id>48</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>49</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>50</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>&#8594; &#964; s</text>
                  <doc_id>51</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8242;n , where s&#8242; 0 = s and s&#8242; n = s &#8242; ,</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>is defined using a and each transduction &#964; &#8242; s</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8594; &#964; &#8242;</text>
                  <doc_id>54</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i&#8722;1 s i</text>
                  <doc_id>55</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>tree transduction operation that at most permutes the children of a single node in &#964; s</text>
                  <doc_id>56</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8242; i&#8722;1</text>
                  <doc_id>57</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>as defined next.</text>
                  <doc_id>58</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A local transduction &#964; &#8242; s &#8594; &#964; &#8242;</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i&#8722;1 s</text>
                  <doc_id>60</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>is modelled by</text>
                  <doc_id>61</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i</text>
                  <doc_id>62</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>an operation that applies to a single node with address x in &#964; s</text>
                  <doc_id>63</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8242; i&#8722;1</text>
                  <doc_id>64</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>, labeled N x , and may permute the ordered sequence of children &#945; x dominated by node x.</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This constitutes a direct generalization of the ITG binary inversion transduction operation.</text>
                  <doc_id>66</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We assign a conditional probability to each such local transduction:</text>
                  <doc_id>67</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (&#964; &#8242; s | &#964; &#8242;</text>
                  <doc_id>68</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i s</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>) &#8776; P (&#960;(&#945; x ) | N x &#8594; &#945; x , C x ) (2)</text>
                  <doc_id>70</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i&#8722;1</text>
                  <doc_id>71</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>where &#960;(&#945; x ) is a permutation of &#945; x (the ordered sequence of node labels under x) and C x is a local tree context of node x in tree &#964; &#8242; s</text>
                  <doc_id>72</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>.</text>
                  <doc_id>73</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>One wrin-</text>
                  <doc_id>74</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i&#8722;1</text>
                  <doc_id>75</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>kle in this definition is that the number of possible permutations of &#945; x is factorial in the length of &#945; x .</text>
                  <doc_id>76</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Fortunately, the source permuted training data exhibits only a fraction of possible permutations even for longer &#945; x sequences.</text>
                  <doc_id>77</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Furthermore, by conditioning the probability on local context, the general applicability of the permutation is restrained.</text>
                  <doc_id>78</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In principle, if we would disregard the computational cost, we could define the probability of the sequence of local tree transductions &#964; s</text>
                  <doc_id>79</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8242; 0</text>
                  <doc_id>80</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8594; .</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>82</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>83</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>&#8594; &#964; s</text>
                  <doc_id>84</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8242; n</text>
                  <doc_id>85</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>as</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (&#964; &#8242; s &#8594; .</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>88</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>.</text>
                  <doc_id>89</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>&#8594; &#964;</text>
                  <doc_id>90</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0 s &#8242; n ) = n &#8719;</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (&#964; &#8242; s | &#964; &#8242;</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i s</text>
                  <doc_id>94</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>) (3)</text>
                  <doc_id>95</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i&#8722;1</text>
                  <doc_id>96</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The problem of calculating the most likely permutation under this kind of transduction probability is intractable because every local transduction conditions on local context of an intermediate tree 1 .</text>
                  <doc_id>97</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Hence, we disregard this formulation and in practice we take a pragmatic approach and greedily select at every intermediate point &#964; &#8242; s</text>
                  <doc_id>98</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8594; &#964; &#8242;</text>
                  <doc_id>99</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i&#8722;1 s</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the single most</text>
                  <doc_id>101</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i</text>
                  <doc_id>102</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>likely local transduction that can be conducted on any node of the current intermediate tree &#964; s</text>
                  <doc_id>103</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8242; i&#8722;1</text>
                  <doc_id>104</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>.</text>
                  <doc_id>105</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The</text>
                  <doc_id>106</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>1 Note that a single transduction step on the current tree</text>
                  <doc_id>107</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#964; &#8242; s leads to a forest of trees &#964; &#8242;</text>
                  <doc_id>108</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i&#8722;1 s</text>
                  <doc_id>109</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>because there can be multiple alternative transduction rules.</text>
                  <doc_id>110</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Hence, this kind of a model</text>
                  <doc_id>111</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i</text>
                  <doc_id>112</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>demands optimization over many possible sequences of trees, which can be packed into a sequence of parse-forests with transduction links between them.</text>
                  <doc_id>113</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>individual steps are made more effective by interpolating the term in Equation 2 with string probability ratios:</text>
                  <doc_id>114</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(</text>
                  <doc_id>115</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8242;</text>
                  <doc_id>116</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P (s i&#8722;1 P (&#960;(&#945; x ) | N x &#8594; &#945; x , C x ) &#215; ) ) P (s &#8242; i ) (4)</text>
                  <doc_id>117</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The rationale behind this interpolation is that our source permutation approach aims at finding the optimal permutation s &#8242; of s that can serve as input for a subsequent translation model.</text>
                  <doc_id>118</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Hence, we aim at tree transductions that are syntactically motivated that also lead to improved string permutations.</text>
                  <doc_id>119</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In this sense, the tree transduction definitions can be seen as an efficient and syntactically informed way to define the space of possible permutations.</text>
                  <doc_id>120</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We estimate the string probabilities P (s &#8242; i ) using 5-gram language models trained on the s &#8242; side of the source permuted parallel corpus s &#8594; s &#8242; .</text>
                  <doc_id>121</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We estimate the conditional probability P (&#960;(&#945; x ) | N x &#8594; &#945; x , C x ) using a Maximum-Entropy framework, where feature functions are defined to capture the permutation as a class, the node label N x and its head POS tag, the child sequence &#945; x together with the corresponding sequence of head POS tags and other features corresponding to different contextual information.</text>
                  <doc_id>122</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We were particularly interested in those linguistic features that motivate reordering phenomena from the syntactic and linguistic perspective.</text>
                  <doc_id>123</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The features that were used for training the permutation system are extracted for every internal node of the source tree that has more than one child:</text>
                  <doc_id>124</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Local tree topology.</text>
                  <doc_id>125</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Sub-tree instances that include parent node and the ordered sequence of child node labels.</text>
                  <doc_id>126</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Dependency features.</text>
                  <doc_id>127</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Features that determine the POS tag of the head word of the current node, together with the sequence of POS tags of the head words of its child nodes.</text>
                  <doc_id>128</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Syntactic features.</text>
                  <doc_id>129</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Two binary features from this class describe: (1) whether the parent node is a child of the node annotated with the same syntactic category, (2) whether the parent node is a descendant of a node annotated with the same syntactic category.</text>
                  <doc_id>130</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>4</index>
        <title>4 Related work</title>
        <text>The integration of linguistic syntax into SMT systems offers a potential solution to reordering problem. For example, syntax is successfully integrated into hierarchical SMT (Zollmann and Venugopal, 2006). In (Yamada and Knight, 2001), a set of treestring channel operations is defined over the parse tree nodes, while reordering is modeled by permutations of children nodes. Similarly, the tree-to-string syntax-based transduction approach offers a complete translation framework (Galley et al., 2006).
The idea of augmenting SMT by a reordering step prior to translation has often been shown to improve translation quality. Clause restructuring performed with hand-crafted reordering rules for German-to- English and Chinese-to-English tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively. In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts.
In (Costa-juss&#224; and Fonollosa, 2006) source and target word order harmonization is done using wellestablished SMT techniques and without the use of syntactic knowledge. Other reordering models operate provide the decoder with multiple word orders. For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle longdistance word-block permutations. Coming up-todate, in (PVS, 2010) an effective application of data mining techniques to syntax-driven source reordering for MT is presented.
Different syntax-based reordering systems can be found in (Genzel, 2010). In this system, reordering rules capable to capture many important word order transformations are automatically learned and applied in the preprocessing step.
Recently, Tromble and Eisner (Tromble and Eisner, 2009) define source permutation as the wordordering learning problem; the model works with a preference matrix for word pairs, expressing preference for their two alternative orders, and a corresponding weight matrix that is fit to the parallel data. The huge space of permutations is then structured using a binary synchronous context-free grammar (Binary ITG) with O(n 3 ) parsing complexity, and the permutation score is calculated recursively over the tree at every node as the accumulation of the relative differences between the word-pair scores taken from the preference matrix. Application to German-to-English translation exhibits some performance improvement.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>The integration of linguistic syntax into SMT systems offers a potential solution to reordering problem.</text>
              <doc_id>131</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For example, syntax is successfully integrated into hierarchical SMT (Zollmann and Venugopal, 2006).</text>
              <doc_id>132</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In (Yamada and Knight, 2001), a set of treestring channel operations is defined over the parse tree nodes, while reordering is modeled by permutations of children nodes.</text>
              <doc_id>133</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Similarly, the tree-to-string syntax-based transduction approach offers a complete translation framework (Galley et al., 2006).</text>
              <doc_id>134</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The idea of augmenting SMT by a reordering step prior to translation has often been shown to improve translation quality.</text>
              <doc_id>135</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Clause restructuring performed with hand-crafted reordering rules for German-to- English and Chinese-to-English tasks are presented in (Collins et al., 2005) and (Wang et al., 2007), respectively.</text>
              <doc_id>136</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In (Xia and McCord, 2004; Khalilov, 2009) word reordering is addressed by exploiting syntactic representations of source and target texts.</text>
              <doc_id>137</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In (Costa-juss&#224; and Fonollosa, 2006) source and target word order harmonization is done using wellestablished SMT techniques and without the use of syntactic knowledge.</text>
              <doc_id>138</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Other reordering models operate provide the decoder with multiple word orders.</text>
              <doc_id>139</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For example, the MaxEnt reordering model described in (Xiong et al., 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder.</text>
              <doc_id>140</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle longdistance word-block permutations.</text>
              <doc_id>141</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Coming up-todate, in (PVS, 2010) an effective application of data mining techniques to syntax-driven source reordering for MT is presented.</text>
              <doc_id>142</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Different syntax-based reordering systems can be found in (Genzel, 2010).</text>
              <doc_id>143</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this system, reordering rules capable to capture many important word order transformations are automatically learned and applied in the preprocessing step.</text>
              <doc_id>144</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Recently, Tromble and Eisner (Tromble and Eisner, 2009) define source permutation as the wordordering learning problem; the model works with a preference matrix for word pairs, expressing preference for their two alternative orders, and a corresponding weight matrix that is fit to the parallel data.</text>
              <doc_id>145</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The huge space of permutations is then structured using a binary synchronous context-free grammar (Binary ITG) with O(n 3 ) parsing complexity, and the permutation score is calculated recursively over the tree at every node as the accumulation of the relative differences between the word-pair scores taken from the preference matrix.</text>
              <doc_id>146</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Application to German-to-English translation exhibits some performance improvement.</text>
              <doc_id>147</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>5 Experiments and submissions</title>
        <text>Design, architecture and configuration of the translation system that we used in experimentation coincides with the Moses-based translation system (Baseline system) described in details on the WMT 2011 web page 2 .
This section details the experiments carried out to evaluate the proposed reordering model, experimental set-up and data.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Design, architecture and configuration of the translation system that we used in experimentation coincides with the Moses-based translation system (Baseline system) described in details on the WMT 2011 web page 2 .</text>
              <doc_id>148</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>This section details the experiments carried out to evaluate the proposed reordering model, experimental set-up and data.</text>
              <doc_id>149</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Data</title>
            <text>In our experiments we used EuroParl v6.0 German- English parallel corpus provided by the organizers of the evaluation campaign.
A detailed statistics of the training, development, internal (test int.) and official (test of.) test datasets can be found in Table 1. The development corpus coincides with the 2009 test set and for internal testing we used the test data proposed to the participants of WMT 2010.
&#8221;ASL&#8220; stands for average sentence length. All the sets were provided with one reference translation.
Data Sent. Words Voc. ASL
Apart from the German portion of the EuroParl parallel corpus, two additional monolingual corpora from news domain (the News Commentary corpus (NC) and the News Crawl Corpus 2011 (NS)) were
html
2 http://www.statmt.org/wmt11/baseline.
used to train a language model for German. The characteristics of these datasets can be found in Table 2. Notice that the data were not de-duplicated.
Data Sent. Words Voc. ASL</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>In our experiments we used EuroParl v6.0 German- English parallel corpus provided by the organizers of the evaluation campaign.</text>
                  <doc_id>150</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A detailed statistics of the training, development, internal (test int.</text>
                  <doc_id>151</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>) and official (test of.</text>
                  <doc_id>152</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>) test datasets can be found in Table 1.</text>
                  <doc_id>153</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The development corpus coincides with the 2009 test set and for internal testing we used the test data proposed to the participants of WMT 2010.</text>
                  <doc_id>154</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8221;ASL&#8220; stands for average sentence length.</text>
                  <doc_id>155</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>All the sets were provided with one reference translation.</text>
                  <doc_id>156</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Data Sent.</text>
                  <doc_id>157</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Words Voc.</text>
                  <doc_id>158</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>ASL</text>
                  <doc_id>159</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Apart from the German portion of the EuroParl parallel corpus, two additional monolingual corpora from news domain (the News Commentary corpus (NC) and the News Crawl Corpus 2011 (NS)) were</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>html</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>2 http://www.statmt.org/wmt11/baseline.</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>used to train a language model for German.</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The characteristics of these datasets can be found in Table 2.</text>
                  <doc_id>164</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Notice that the data were not de-duplicated.</text>
                  <doc_id>165</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Data Sent.</text>
                  <doc_id>166</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Words Voc.</text>
                  <doc_id>167</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>ASL</text>
                  <doc_id>168</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Experimental setup</title>
            <text>Moses toolkit (Koehn et al., 2007) in its standard setting was used to build the SMT systems:
&#8226; GIZA++/mkcls (Och, 2003; Och, 1999) for word alignment.
&#8226; SRI LM (Stolcke, 2002) for language modeling. A 3-gram target language model was estimated and smoothed with modified Kneser- Ney discounting.
&#8226; MOSES (Koehn et al., 2007) to build an unfactored translation system.
&#8226; the Stanford parser (Klein and Manning, 2003) was used as a source-side parsing engine 3 .
&#8226; For maximum entropy modeling we used the maxent toolkit 4 .
The discriminative syntactic reordering model is applied to reorder training, development, and test corpora. A Moses-based translation system (corpus realignment included 5 ) is then trained using the reordered input.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Moses toolkit (Koehn et al., 2007) in its standard setting was used to build the SMT systems:</text>
                  <doc_id>169</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; GIZA++/mkcls (Och, 2003; Och, 1999) for word alignment.</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; SRI LM (Stolcke, 2002) for language modeling.</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A 3-gram target language model was estimated and smoothed with modified Kneser- Ney discounting.</text>
                  <doc_id>172</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; MOSES (Koehn et al., 2007) to build an unfactored translation system.</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; the Stanford parser (Klein and Manning, 2003) was used as a source-side parsing engine 3 .</text>
                  <doc_id>174</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; For maximum entropy modeling we used the maxent toolkit 4 .</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The discriminative syntactic reordering model is applied to reorder training, development, and test corpora.</text>
                  <doc_id>176</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>A Moses-based translation system (corpus realignment included 5 ) is then trained using the reordered input.</text>
                  <doc_id>177</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>5.3 Internal results and submissions</title>
            <text>The outputs of two translation system were submitted. First, we piled up all feature functions into a single model as described in Section 3. It was our &#8220;secondary&#8221; submission. However, our experience tells
3 The parser was trained on the English treebank set provided
with 14 syntactic categories and 48 POS tags. 4 http://homepages.inf.ed.ac.uk/lzhang10/
maxent_toolkit.html 5 Some studies show that word re-alignment of a monotonized corpus gives better results than unfolding of alignment crossings (Costa-juss&#224; and Fonollosa, 2006).
that the system performance can increase if the set of patterns is split into partial classes conditioned on the current node label (Khalilov and Sima&#8217;an, 2010). Hence, we trained three separate MaxEnt models for the categories with potentially high reordering requirements, namely NP , SENT and SBAR(Q). It was defines as our &#8220;primary&#8221; submission.
The ranking of submission was done according to the results shown on internal testing, shown in Table 3.
System BLEU dev BLEU test NIST test</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The outputs of two translation system were submitted.</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>First, we piled up all feature functions into a single model as described in Section 3.</text>
                  <doc_id>179</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It was our &#8220;secondary&#8221; submission.</text>
                  <doc_id>180</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>However, our experience tells</text>
                  <doc_id>181</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>3 The parser was trained on the English treebank set provided</text>
                  <doc_id>182</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>with 14 syntactic categories and 48 POS tags.</text>
                  <doc_id>183</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>4 http://homepages.inf.ed.ac.uk/lzhang10/</text>
                  <doc_id>184</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>maxent_toolkit.html 5 Some studies show that word re-alignment of a monotonized corpus gives better results than unfolding of alignment crossings (Costa-juss&#224; and Fonollosa, 2006).</text>
                  <doc_id>185</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>that the system performance can increase if the set of patterns is split into partial classes conditioned on the current node label (Khalilov and Sima&#8217;an, 2010).</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Hence, we trained three separate MaxEnt models for the categories with potentially high reordering requirements, namely NP , SENT and SBAR(Q).</text>
                  <doc_id>187</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>It was defines as our &#8220;primary&#8221; submission.</text>
                  <doc_id>188</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The ranking of submission was done according to the results shown on internal testing, shown in Table 3.</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>System BLEU dev BLEU test NIST test</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>5.4 Official results and discussion</title>
            <text>Unfortunately, the results of our participation this year were discouraging. The primary submission was ranked 30th (12.6 uncased BLEU-4) and the secondary 31th (11.2) out of 32 submitted systems.
It turned out that our preliminary idea to extrapolate the positive results of English-to-Dutch translation reported in (Khalilov and Sima&#8217;an, 2010) to the WMT English-to-German translation task was not right. Analyzing the reasons of negative results during the post-evaluation period, we discovered that translation into German differs from English-to-Dutch task in many cases. In contrast to English-to-Dutch translation, the difference in terms of automatic scores between the internal baseline system (without external reordering) and the system enhanced with the pre-translation reordering is minimal. It turns out that translating into German is more complex in general and discriminative reordering is more advantageous for English-to-Dutch than for Englishto-German translation.
A negative aspect influencing is the way how the rules are extracted and applied according to our approach. Syntax-driven reordering, as described in this paper, involves large contextual information applied cumulatively. Under conditions of scarce data, alignment and parsing errors, it introduces noise to the reordering system and distorts the feature prob-
ability space. At the same time, many reorderings can be performed more efficiently based on fixed (hand-crafted) rules (as it is done in (Collins et al., 2005)). A possible remedy to this problem is to combine automatically extracted features with fixed (hand-crafted) rules. Our last claims are supported by the observations described in (Visweswariah et al., 2010). During post-evaluation period we analyzed the reasons why the system performance has slightly improved when separate MaxEnt models are applied. The outline of reordered nodes for each of syntactic categories considered (SEN T , SBAR(Q) and NP ) can be found in Table 4 (the size of the corpus is 1.7 M of sentences).
It is seen that the reorderings for NP nodes is higher than for SEN T and SBAR(Q) categories. While SENT and SBAR(Q) reorderings work analogously for Dutch and German, our intuition is that German has more features that play a role in reordering of NP structures than Dutch and there is a need of more specific features to model NP permutations in an accurate way.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Unfortunately, the results of our participation this year were discouraging.</text>
                  <doc_id>191</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The primary submission was ranked 30th (12.6 uncased BLEU-4) and the secondary 31th (11.2) out of 32 submitted systems.</text>
                  <doc_id>192</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It turned out that our preliminary idea to extrapolate the positive results of English-to-Dutch translation reported in (Khalilov and Sima&#8217;an, 2010) to the WMT English-to-German translation task was not right.</text>
                  <doc_id>193</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Analyzing the reasons of negative results during the post-evaluation period, we discovered that translation into German differs from English-to-Dutch task in many cases.</text>
                  <doc_id>194</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In contrast to English-to-Dutch translation, the difference in terms of automatic scores between the internal baseline system (without external reordering) and the system enhanced with the pre-translation reordering is minimal.</text>
                  <doc_id>195</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>It turns out that translating into German is more complex in general and discriminative reordering is more advantageous for English-to-Dutch than for Englishto-German translation.</text>
                  <doc_id>196</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A negative aspect influencing is the way how the rules are extracted and applied according to our approach.</text>
                  <doc_id>197</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Syntax-driven reordering, as described in this paper, involves large contextual information applied cumulatively.</text>
                  <doc_id>198</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Under conditions of scarce data, alignment and parsing errors, it introduces noise to the reordering system and distorts the feature prob-</text>
                  <doc_id>199</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>ability space.</text>
                  <doc_id>200</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>At the same time, many reorderings can be performed more efficiently based on fixed (hand-crafted) rules (as it is done in (Collins et al., 2005)).</text>
                  <doc_id>201</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>A possible remedy to this problem is to combine automatically extracted features with fixed (hand-crafted) rules.</text>
                  <doc_id>202</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Our last claims are supported by the observations described in (Visweswariah et al., 2010).</text>
                  <doc_id>203</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>During post-evaluation period we analyzed the reasons why the system performance has slightly improved when separate MaxEnt models are applied.</text>
                  <doc_id>204</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>The outline of reordered nodes for each of syntactic categories considered (SEN T , SBAR(Q) and NP ) can be found in Table 4 (the size of the corpus is 1.7 M of sentences).</text>
                  <doc_id>205</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>It is seen that the reorderings for NP nodes is higher than for SEN T and SBAR(Q) categories.</text>
                  <doc_id>206</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>While SENT and SBAR(Q) reorderings work analogously for Dutch and German, our intuition is that German has more features that play a role in reordering of NP structures than Dutch and there is a need of more specific features to model NP permutations in an accurate way.</text>
                  <doc_id>207</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>6</index>
        <title>6 Conclusions</title>
        <text>This paper presents the ILLC-UvA translation system for English-to-German translation task proposed to the participants of the EMNLP-WMT 2011 evaluation campaign. The novel feature that we present this year is a source reordering model in which the reordering decisions are conditioned on the features from the source parse tree. Our system has not managed to outperform the majority of the participating systems, possibly due to its generic approach to reordering. We plan to investigate why our approach works well for Englishto-Dutch and less well for the English-to-German translation in order to discover more generic ways for learning discriminative reordering rules. One possible explanation of the bad results is a high sparseness of automatically extracted rules that does not allow for sufficient generalization of reordering instances.
In the future, we plan (1) to perform deeper analysis of the dissimilarity between English-to-Dutch and English-to-German translations from SMT perspective, and (2) to investigate linguisticallymotivated ideas to extend our model such that we can bring about some improvement to English-to- German translation.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>This paper presents the ILLC-UvA translation system for English-to-German translation task proposed to the participants of the EMNLP-WMT 2011 evaluation campaign.</text>
              <doc_id>208</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The novel feature that we present this year is a source reordering model in which the reordering decisions are conditioned on the features from the source parse tree.</text>
              <doc_id>209</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Our system has not managed to outperform the majority of the participating systems, possibly due to its generic approach to reordering.</text>
              <doc_id>210</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We plan to investigate why our approach works well for Englishto-Dutch and less well for the English-to-German translation in order to discover more generic ways for learning discriminative reordering rules.</text>
              <doc_id>211</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>One possible explanation of the bad results is a high sparseness of automatically extracted rules that does not allow for sufficient generalization of reordering instances.</text>
              <doc_id>212</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In the future, we plan (1) to perform deeper analysis of the dissimilarity between English-to-Dutch and English-to-German translations from SMT perspective, and (2) to investigate linguisticallymotivated ideas to extend our model such that we can bring about some improvement to English-to- German translation.</text>
              <doc_id>213</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>7</index>
        <title>7 Acknowledgements</title>
        <text></text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text></text>
              <doc_id>214</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: German-English EuroParl corpus (version 6.0).</caption>
        <reference_text>In PAGE 4: ...) and official (test of.) test datasets can be found in  Table1 . The development corpus coincides with the 2009 test set and for internal test- ing we used the test data proposed to the participants of WMT 2010....</reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>Data</cell>
              <cell>None</cell>
              <cell>Sent.</cell>
              <cell>Words</cell>
              <cell>Voc.</cell>
              <cell>ASL</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>train</cell>
              <cell>En</cell>
              <cell>1.7M</cell>
              <cell>46.0M</cell>
              <cell>121.3K</cell>
              <cell>27.0</cell>
            </row>
            <row>
              <cell>train</cell>
              <cell>Ge</cell>
              <cell>1.7M</cell>
              <cell>43.7M</cell>
              <cell>368.5K</cell>
              <cell>25.7</cell>
            </row>
            <row>
              <cell>dev</cell>
              <cell>En</cell>
              <cell>2.5K</cell>
              <cell>57.6K</cell>
              <cell>13.2K</cell>
              <cell>22.8</cell>
            </row>
            <row>
              <cell>test int.</cell>
              <cell>En</cell>
              <cell>2.5K</cell>
              <cell>53.2K</cell>
              <cell>15.9K</cell>
              <cell>21.4</cell>
            </row>
            <row>
              <cell>test of.</cell>
              <cell>En</cell>
              <cell>3.0K</cell>
              <cell>74.8K</cell>
              <cell>11.1K</cell>
              <cell>24.9</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TET</source>
        <caption>Table 2: Monolingual German corpora used for targetside language modeling.</caption>
        <reference_text></reference_text>
        <page_num>4</page_num>
        <head>
          <rows>
            <row>
              <cell>NC</cell>
              <cell>Ge</cell>
              <cell>161.8M</cell>
              <cell>3.9G</cell>
              <cell>136.7M</cell>
              <cell>23.9</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>NS</cell>
              <cell>Ge</cell>
              <cell>45.3M</cell>
              <cell>799.4M</cell>
              <cell>3.0M</cell>
              <cell>17.7</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>3</id>
        <source>TableSeer</source>
        <caption>Table 3: Internal testing results.</caption>
        <reference_text>None</reference_text>
        <page_num>5</page_num>
        <head>
          <rows>
            <row>
              <cell>ble 3.</cell>
              <cell>None</cell>
              <cell>None</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>System</cell>
              <cell>BLEU dev</cell>
              <cell>BLEU test</cell>
              <cell>NIST test</cell>
            </row>
            <row>
              <cell>Baseline</cell>
              <cell>11.03</cell>
              <cell>9.78</cell>
              <cell>3.78</cell>
            </row>
            <row>
              <cell>Primary</cell>
              <cell>11.07</cell>
              <cell>10.00</cell>
              <cell>3.79</cell>
            </row>
            <row>
              <cell>Secondary</cell>
              <cell>10.92</cell>
              <cell>9.91</cell>
              <cell>3.78</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TableSeer</source>
        <caption>Table 4: Application of reorderings for separate syntactic categories.</caption>
        <reference_text>In PAGE 6: ...l., 2010). During post-evaluation period we analyzed the reasons why the system performance has slightly improved when separate MaxEnt models are ap- plied. The outline of reordered nodes for each of syntactic categories considered (SENT , SBAR(Q) and NP ) can be found in  Table4  (the size of the corpus is 1.7 M of sentences)....</reference_text>
        <page_num>6</page_num>
        <head>
          <rows>
            <row>
              <cell>Category</cell>
              <cell># of applications</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>NP</cell>
              <cell>497,186</cell>
            </row>
            <row>
              <cell>SBAR(Q)</cell>
              <cell>106,243</cell>
            </row>
            <row>
              <cell>SENT</cell>
              <cell>221,568</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>M Collins</author>
          <author>P Koehn</author>
          <author>I Ku&#269;erov&#225;</author>
        </authors>
        <title>Clause restructuring for statistical machine translation.</title>
        <publication>In Proceedings of ACL&#8217;05,</publication>
        <pages>531--540</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>M R Costa-juss&#224;</author>
          <author>J A R Fonollosa</author>
        </authors>
        <title>Statistical machine reordering.</title>
        <publication>In Proceedings of HLT/EMNLP&#8217;06,</publication>
        <pages>70--76</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>D Manning</author>
        </authors>
        <title>A simple and effective hierarchical phrase reordering model.</title>
        <publication>In Proceedings of EMNLP&#8217;08,</publication>
        <pages>848--856</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>M Galley</author>
          <author>J Graehl</author>
          <author>K Knight</author>
          <author>D Marcu</author>
          <author>S DeNeefe</author>
          <author>W Wang</author>
          <author>I Thaye</author>
        </authors>
        <title>Scalable inference and training of context-rich syntactic translation models.</title>
        <publication>In Proc. of COLING/ACL&#8217;06,</publication>
        <pages>961--968</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>D Genzel</author>
        </authors>
        <title>Aumotatically learning source-side reordering rules for large scale machine translation.</title>
        <publication>In Proc. of COLING&#8217;10,</publication>
        <pages>376--384</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>M Khalilov</author>
          <author>K Sima&#8217;an</author>
        </authors>
        <title>A discriminative syntactic model for source permutation via tree transduction.</title>
        <publication>In Proc. of the Fourth Workshop on Syntax and Structure in Statistical Translation (SSST-4) at COLING&#8217;10,</publication>
        <pages>92--100</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>M Khalilov</author>
        </authors>
        <title>New statistical and syntactic models for machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>D Klein</author>
          <author>C Manning</author>
        </authors>
        <title>Accurate unlexicalized parsing.</title>
        <publication>In Proceedings of the 41st Annual Meeting of the ACL&#8217;03,</publication>
        <pages>423--430</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>F Och Koehn</author>
          <author>D Marcu</author>
        </authors>
        <title>Statistical phrase-based machine translation.</title>
        <publication>In Proceedings of the HLT-NAACL</publication>
        <pages>48--54</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>H Hoang Koehn</author>
          <author>A Birch</author>
          <author>C Callison-Burch</author>
          <author>M Federico</author>
          <author>N Bertoldi</author>
          <author>B Cowan</author>
          <author>W Shen</author>
        </authors>
        <title>Moses: open-source toolkit for statistical machine translation.</title>
        <publication>In Proceedings of ACL</publication>
        <pages>177--180</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>F Och</author>
          <author>H Ney</author>
        </authors>
        <title>Discriminative training and maximum entropy models for statistical machine translation.</title>
        <publication>In Proceedings of ACL&#8217;02,</publication>
        <pages>295--302</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>F Och</author>
        </authors>
        <title>An efficient method for determining bilingual word classes.</title>
        <publication>In Proceedings of ACL</publication>
        <pages>71--76</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>F Och</author>
        </authors>
        <title>Minimum error rate training in statistical machine translation.</title>
        <publication>In Proceedings of ACL&#8217;03,</publication>
        <pages>160--167</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>K Papineni</author>
          <author>S Roukos</author>
          <author>T Ward</author>
          <author>W Zhu</author>
        </authors>
        <title>Bleu: a method for automatic evaluation of machine translation.</title>
        <publication>In Proceedings of ACL&#8217;02,</publication>
        <pages>311--318</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>A PVS</author>
        </authors>
        <title>A data mining approach to learn reorder rules for SMT.</title>
        <publication>In Proceedings of NAACL/HLT&#8217;10,</publication>
        <pages>52--57</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>A Stolcke</author>
        </authors>
        <title>SRILM: an extensible language modeling toolkit.</title>
        <publication>In Proceedings of SLP&#8217;02,</publication>
        <pages>901--904</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>C Tillman</author>
        </authors>
        <title>A unigram orientation model for statistical machine translation.</title>
        <publication>In Proceedings of HLTNAACL&#8217;04,</publication>
        <pages>101--104</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>R Tromble</author>
          <author>J Eisner</author>
        </authors>
        <title>Learning linear ordering problems for better translation.</title>
        <publication>In Proceedings of EMNLP&#8217;09,</publication>
        <pages>1007--1016</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>K Visweswariah</author>
          <author>J Navratil</author>
          <author>J Sorensen</author>
          <author>V Chenthamarakshan</author>
          <author>N Kambhatla</author>
        </authors>
        <title>Syntax based reordering with automatically derived rules for improved statistical machine translation.</title>
        <publication>In Proc. of COLING&#8217;10,</publication>
        <pages>1119--1127</pages>
        <date>2010</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>Koehn</author>
        </authors>
        <title>Chinese syntactic reordering for statistical machine translation.</title>
        <publication>In Proceedings of EMNLP-CoNLL&#8217;07,</publication>
        <pages>737--745</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>F Xia</author>
          <author>M McCord</author>
        </authors>
        <title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
        <publication>In Proceedings of COLING&#8217;04,</publication>
        <pages>508--514</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>D Xiong</author>
          <author>Q Liu</author>
          <author>S Lin</author>
        </authors>
        <title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
        <publication>In Proceedings of ACL&#8217;06,</publication>
        <pages>521--528</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>K Yamada</author>
          <author>K Knight</author>
        </authors>
        <title>A syntax-based statistical translation model.</title>
        <publication>In Proceedings of ACL&#8217;01,</publication>
        <pages>523--530</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>R Zens</author>
          <author>F Och</author>
          <author>H Ney</author>
        </authors>
        <title>Phrase-based statistical machine translation.</title>
        <publication>In Proceedings of KI: Advances in Artificial Intelligence,</publication>
        <pages>18--32</pages>
        <date>2002</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Collins et al., 2005</string>
        <sentence_id>47958</sentence_id>
        <char_offset>218</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>0</reference_id>
        <string>Collins et al., 2005</string>
        <sentence_id>48087</sentence_id>
        <char_offset>136</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>0</reference_id>
        <string>Collins et al., 2005</string>
        <sentence_id>48150</sentence_id>
        <char_offset>124</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>1</reference_id>
        <string>Costa-juss&#224; and Fonollosa, 2006</string>
        <sentence_id>47958</sentence_id>
        <char_offset>185</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>1</reference_id>
        <string>Costa-juss&#224; and Fonollosa, 2006</string>
        <sentence_id>48089</sentence_id>
        <char_offset>4</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>1</reference_id>
        <string>Costa-juss&#224; and Fonollosa, 2006</string>
        <sentence_id>48134</sentence_id>
        <char_offset>147</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>2</reference_id>
        <string>Manning, 2008</string>
        <sentence_id>48092</sentence_id>
        <char_offset>15</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>3</reference_id>
        <string>Galley et al., 2006</string>
        <sentence_id>48085</sentence_id>
        <char_offset>106</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>4</reference_id>
        <string>Genzel, 2010</string>
        <sentence_id>47958</sentence_id>
        <char_offset>171</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>4</reference_id>
        <string>Genzel, 2010</string>
        <sentence_id>48094</sentence_id>
        <char_offset>59</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>6</reference_id>
        <string>Khalilov, 2009</string>
        <sentence_id>48088</sentence_id>
        <char_offset>26</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>7</reference_id>
        <string>Klein and Manning, 2003</string>
        <sentence_id>48123</sentence_id>
        <char_offset>23</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>9</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>48118</sentence_id>
        <char_offset>15</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>9</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>48122</sentence_id>
        <char_offset>9</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>10</reference_id>
        <string>Och and Ney, 2002</string>
        <sentence_id>47968</sentence_id>
        <char_offset>134</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>11</reference_id>
        <string>Och, 1999</string>
        <sentence_id>48119</sentence_id>
        <char_offset>27</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>12</reference_id>
        <string>Och, 2003</string>
        <sentence_id>47979</sentence_id>
        <char_offset>54</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>12</reference_id>
        <string>Och, 2003</string>
        <sentence_id>48119</sentence_id>
        <char_offset>16</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>13</reference_id>
        <string>Papineni et al., 2002</string>
        <sentence_id>47979</sentence_id>
        <char_offset>86</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>14</reference_id>
        <string>PVS, 2010</string>
        <sentence_id>48093</sentence_id>
        <char_offset>22</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>15</reference_id>
        <string>Stolcke, 2002</string>
        <sentence_id>48120</sentence_id>
        <char_offset>10</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>16</reference_id>
        <string>Tillman, 2004</string>
        <sentence_id>47980</sentence_id>
        <char_offset>230</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>16</reference_id>
        <string>Tillman, 2004</string>
        <sentence_id>48092</sentence_id>
        <char_offset>88</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>17</reference_id>
        <string>Tromble and Eisner, 2009</string>
        <sentence_id>47961</sentence_id>
        <char_offset>61</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>17</reference_id>
        <string>Tromble and Eisner, 2009</string>
        <sentence_id>47986</sentence_id>
        <char_offset>125</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>17</reference_id>
        <string>Tromble and Eisner, 2009</string>
        <sentence_id>48096</sentence_id>
        <char_offset>30</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>18</reference_id>
        <string>Visweswariah et al., 2010</string>
        <sentence_id>48152</sentence_id>
        <char_offset>64</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>20</reference_id>
        <string>Xia and McCord, 2004</string>
        <sentence_id>48088</sentence_id>
        <char_offset>4</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>21</reference_id>
        <string>Xiong et al., 2006</string>
        <sentence_id>48091</sentence_id>
        <char_offset>55</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>22</reference_id>
        <string>Yamada and Knight, 2001</string>
        <sentence_id>48084</sentence_id>
        <char_offset>4</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>23</reference_id>
        <string>Zens et al., 2002</string>
        <sentence_id>47968</sentence_id>
        <char_offset>395</char_offset>
      </citation>
    </citations>
  </content>
</document>
