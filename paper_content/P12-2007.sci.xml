<PAPER>
  <FILENO/>
  <TITLE>Head-Driven Hierarchical Phrase-based Translation</TITLE>
  <AUTHORS/>
  <ABSTRACT>
    <A-S ID="S-35799">This paper presents an extension of Chiang&#8217;s hierarchical phrase-based (HPB) model, called Head-Driven HPB (HD-HPB), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space.</A-S>
    <A-S ID="S-35800">Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang&#8217;s model with average gains of 1.91 points absolute in BLEU.</A-S>
  </ABSTRACT>
  <BODY>
    <DIV DEPTH="0">
      <HEADER>1 Introduction</HEADER>
      <P>
        <S ID="S-35801">Chiang&#8217;s hierarchical phrase-based (HPB) translation model utilizes synchronous context free grammar (SCFG) for translation derivation (<REF ID="R-04" RPTR="9">Chiang, 2005</REF>; <REF ID="R-05" RPTR="16">Chiang, 2007</REF>) and has been widely adopted in statistical machine translation (SMT).</S>
        <S ID="S-35802">Typically, such models define two types of translation rules: hierarchical (translation) rules which consist of both terminals and non-terminals, and glue (grammar) rules which combine translated phrases in a monotone fashion.</S>
        <S ID="S-35803">Due to lack of linguistic knowledge, Chiang&#8217;s HPB model contains only one type of nonterminal symbol X, often making it difficult to select the most appropriate translation rules.</S>
        <S ID="S-35804">1 What is more, Chiang&#8217;s HPB model suffers from limited phrase reordering combining translated phrases in a monotonic way with glue rules.</S>
        <S ID="S-35805">In addition, once a</S>
      </P>
      <P>
        <S ID="S-35806">1 Another non-terminal symbol S is used in glue rules.</S>
      </P>
      <P>
        <S ID="S-35807">glue rule is adopted, it requires all rules above it to be glue rules.</S>
      </P>
      <P>
        <S ID="S-35808">One important research question is therefore how to refine the non-terminal category X using linguistically motivated information: <REF ID="R-20" RPTR="32">Zollmann and Venugopal (2006)</REF> (SAMT) e.g. use (partial) syntactic categories derived from CFG trees while Zollmann and Vogel (2011) use word tags, generated by either POS analysis or unsupervised word class induction.</S>
        <S ID="S-35809"><REF ID="R-00" RPTR="0">Almaghout et al. (2011)</REF> employ CCGbased supertags.</S>
        <S ID="S-35810">Mylonakis and Sima&#8217;an (2011) use linguistic information of various granularities such as Phrase-Pair, Constituent, Concatenation of Constituents, and Partial Constituents, where applicable.</S>
        <S ID="S-35811">Inspired by previous work in parsing (<REF ID="R-03" RPTR="7">Charniak, 2000</REF>; <REF ID="R-06" RPTR="20">Collins, 2003</REF>), our Head-Driven HPB (HD-HPB) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment, as in HPB.</S>
        <S ID="S-35812">We identify heads using linguistically motivated dependency parsing, and use their POS to refine X.</S>
        <S ID="S-35813">In addition HD-HPB provides flexible reordering rules freely mixing translation and reordering (including swap) at any stage in a derivation.</S>
      </P>
      <P>
        <S ID="S-35814">Different from the soft constraint modeling adopted in (<REF ID="R-01" RPTR="1">Chan et al., 2007</REF>; <REF ID="R-12" RPTR="26">Marton and Resnik, 2008</REF>; <REF ID="R-19" RPTR="31">Shen et al., 2009</REF>; <REF ID="R-08" RPTR="22">He et al., 2010</REF>; <REF ID="R-09" RPTR="23">Huang et al., 2010</REF>; <REF ID="R-07" RPTR="21">Gao et al., 2011</REF>), our approach encodes syntactic information in translation rules.</S>
        <S ID="S-35815">However, the two approaches are not mutually exclusive, as we could also include a set of syntax-driven features into our translation model.</S>
        <S ID="S-35816">Our approach maintains the advantages of Chiang&#8217;s HPB model while at the same time incorporating head information and flex-</S>
      </P>
      <P>
        <S ID="S-35817">&#27431; &#27954; /NR Ouzhou &#20843; &#22269; /NN baguo &#32852; &#21517; /AD lianming root</S>
      </P>
      <P>
        <S ID="S-35818">Eight European countries jointly</S>
      </P>
      <P>
        <S ID="S-35819">&#25903; &#25345; /VV zhichi &#32654; &#22269; /NR meiguo &#31435; &#22330; /NN lichang</S>
      </P>
      <P>
        <S ID="S-35820">support America&#8217;s stand</S>
      </P>
      <P>
        <S ID="S-35821">ible reordering in a derivation in a natural way.</S>
        <S ID="S-35822">Experiments on Chinese-English translation using four NIST MT test sets show that our HD-HPB model significantly outperforms Chiang&#8217;s HPB as well as a SAMT-style refined version of HPB.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>2 Head-Driven HPB Translation Model</HEADER>
      <P>
        <S ID="S-35857">Like <REF ID="R-04" RPTR="8">Chiang (2005)</REF> and <REF ID="R-05" RPTR="11">Chiang (2007)</REF><REF ID="R-02" RPTR="2">(2007)</REF>, our HD- HPB translation model adopts a synchronous context free grammar, a rewriting system which generates source and target side string pairs simultaneously using a context-free grammar.</S>
        <S ID="S-35858">Instead of collapsing all non-terminals in the source language into a single symbol X as in <REF ID="R-05" RPTR="12">Chiang (2007)</REF><REF ID="R-02" RPTR="3">(2007)</REF>, given a word sequence f i j from position i to position j, we first find heads and then concatenate the POS tags of these heads as f i j &#8217;s non-terminal symbol.</S>
        <S ID="S-35859">Specifically, we adopt unlabeled dependency structure to derive heads, which are defined as:</S>
      </P>
      <P>
        <S ID="S-35860">Definition 1.</S>
        <S ID="S-35861">For word sequence f i j , word f k (i &#8804; k &#8804; j) is regarded as a head if it is dominated by a word outside of this sequence.</S>
      </P>
      <P>
        <S ID="S-35862">Note that this definition (i) allows for a word sequence to have one or more heads (largely due to the fact that a word sequence is not necessarily linguistically constrained) and (ii) ensures that heads are always the highest heads in the sequence from a dependency structure perspective.</S>
        <S ID="S-35863">For example, the word sequence ouzhou baguo lianming in Figure 1 has two heads (i.e., baguo and lianming, ouzhou is not a head of this sequence since its headword baguo falls within this sequence) and the non-terminal corresponding to the sequence is thus labeled as NN- AD.</S>
        <S ID="S-35864">It is worth noting that in this paper we only refine non-terminal X on the source side to headinformed ones, while still using X on the target side.</S>
      </P>
      <P>
        <S ID="S-35865">According to the occurrence of terminals in translation rules, we group rules in the HD-HPB model into two categories: head-driven hierarchical rules (HD-HRs) and non-terminal reordering rules (NRRs), where the former have at least one terminal on both source and target sides and the later have no terminals.</S>
        <S ID="S-35866">For rule extraction, we first identify initial phrase pairs on word-aligned sentence pairs by using the same criterion as most phrase-based translation models (<REF ID="R-16" RPTR="28">Och and Ney, 2004</REF>) and Chiang&#8217;s HPB model (<REF ID="R-04" RPTR="10">Chiang, 2005</REF>; <REF ID="R-05" RPTR="17">Chiang, 2007</REF>).</S>
        <S ID="S-35867">We extract HD-HRs and NRRs based on initial phrase pairs, respectively.</S>
      </P>
      <DIV DEPTH="1">
        <HEADER>2.1 HD-HRs: Head-Driven Hierarchical Rules</HEADER>
        <P>
          <S ID="S-35823">As mentioned, a HD-HR has at least one terminal on both source and target sides.</S>
          <S ID="S-35824">This is the same as the hierarchical rules defined in Chiang&#8217;s HPB model (<REF ID="R-05" RPTR="18">Chiang, 2007</REF>), except that we use head POSinformed non-terminal symbols in the source language.</S>
          <S ID="S-35825">We look for initial phrase pairs that contain other phrases and then replace sub-phrases with POS tags corresponding to their heads.</S>
          <S ID="S-35826">Given the word alignment in Figure 1, Table 1 demonstrates the difference between hierarchical rules in <REF ID="R-05" RPTR="13">Chiang (2007)</REF><REF ID="R-02" RPTR="4">(2007)</REF> and HD-HRs defined here.</S>
          <S ID="S-35827">Similar to Chiang&#8217;s HPB model, our HD-HPB model will result in a large number of rules causing problems in decoding.</S>
          <S ID="S-35828">To alleviate these problems, we filter our HD-HRs according to the same constraints as described in <REF ID="R-05" RPTR="14">Chiang (2007)</REF><REF ID="R-02" RPTR="5">(2007)</REF>.</S>
          <S ID="S-35829">Moreover, we discard rules that have non-terminals with more than four heads.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.2 NRRs: Non-terminal Reordering Rules</HEADER>
        <P>
          <S ID="S-35830">NRRs are translation rules without terminals.</S>
          <S ID="S-35831">Given an initial phrase pair on the source side, there are four possible positional relationships for their target side translations (we use Y as a variable for nonterminals on the source side while all non-terminals on the target side are labeled as X):</S>
        </P>
        <P>
          <S ID="S-35832">&#8226; Monotone &#12296;Y &#8594; Y 1 Y 2 , X &#8594; X 1 X 2 &#12297;;</S>
        </P>
        <P>
          <S ID="S-35833">&#8226; Discontinuous monotone &#12296;Y &#8594; Y 1 Y 2 , X &#8594; X 1 .</S>
          <S ID="S-35834">.</S>
          <S ID="S-35835">.</S>
          <S ID="S-35836">X 2 &#12297;;</S>
        </P>
        <P>
          <S ID="S-35837">&#8226; Swap &#12296;Y &#8594; Y 1 Y 2 , X &#8594; X 2 X 1 &#12297;;</S>
        </P>
        <P>
          <S ID="S-35838">&#8226; Discontinuous swap &#12296;Y &#8594; Y 1 Y 2 , X &#8594; X 2 .</S>
          <S ID="S-35839">.</S>
          <S ID="S-35840">.</S>
          <S ID="S-35841">X 1 &#12297;.</S>
        </P>
        <P>
          <S ID="S-35842">Merging two neighboring non-terminals into a single non-terminal, NRRs enable the translation model to explore a wider search space.</S>
          <S ID="S-35843">During training, we extract four types of NRRs and calculate probabilities for each type.</S>
          <S ID="S-35844">To speed up decoding, we currently (i) only use monotone and swap NRRs and (ii) limit the number of non-terminals in a NRR to 2.</S>
        </P>
      </DIV>
      <DIV DEPTH="1">
        <HEADER>2.3 Features and Decoding</HEADER>
        <P>
          <S ID="S-35845">Given e for the translation output in the target language, s and t for strings of terminals and nonterminals on the source and target side, respectively, we use a feature set analogous to the default feature set of <REF ID="R-05" RPTR="15">Chiang (2007)</REF><REF ID="R-02" RPTR="6">(2007)</REF>, including:</S>
        </P>
        <P>
          <S ID="S-35846">&#8226; P hd-hr (t|s) and P hd-hr (s|t), translation probabilities for HD-HRs;</S>
        </P>
        <P>
          <S ID="S-35847">&#8226; P lex (t|s) and P lex (s|t), lexical translation probabilities for HD-HRs;</S>
        </P>
        <P>
          <S ID="S-35848">&#8226; P ty hd-hr = exp (&#8722;1), rule penalty for HD-HRs;</S>
        </P>
        <P>
          <S ID="S-35849">&#8226; P nrr (t|s), translation probability for NRRs;</S>
        </P>
        <P>
          <S ID="S-35850">&#8226; P ty nrr = exp (&#8722;1), rule penalty for NRRs;</S>
        </P>
        <P>
          <S ID="S-35851">&#8226; P lm (e), language model;</S>
        </P>
        <P>
          <S ID="S-35852">&#8226; P ty word (e) = exp (&#8722;|e|), word penalty.</S>
        </P>
        <P>
          <S ID="S-35853">Our decoder is based on CKY-style chart parsing with beam search and searches for the best derivation bottom-up.</S>
          <S ID="S-35854">For a source span [i, j], it applies both types of HD-HRs and NRRs.</S>
          <S ID="S-35855">However, HD- HRs are only applied to generate derivations spanning no more than K words &#8211; the initial phrase length limit used in training to extract HD-HRs &#8211; while NRRs are applied to derivations spanning any length.</S>
          <S ID="S-35856">Unlike in Chiang&#8217;s HPB model, it is possible for a non-terminal generated by a NRR to be included afterwards by a HD-HR or another NRR.</S>
        </P>
      </DIV>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>3 Experiments</HEADER>
      <P>
        <S ID="S-35868">We evaluate the performance of our HD-HPB model and compare it with our implementation of Chiang&#8217;s HPB model (<REF ID="R-05" RPTR="19">Chiang, 2007</REF>), a source-side SAMTstyle refined version of HPB (SAMT-HPB), and the Moses implementation of HPB.</S>
        <S ID="S-35869">For fair comparison, we adopt the same parameter settings for our HD-HPB and HPB systems, including initial phrase length (as 10) in training, the maximum number of non-terminals (as 2) in translation rules, maximum number of non-terminals plus terminals (as 5) on the source, beam threshold &#946; (as 10 &#8722;5 ) (to discard derivations with a score worse than &#946; times the best score in the same chart cell), beam size b (as 200) (i.e. each chart cell contains at most b derivations).</S>
        <S ID="S-35870">For Moses HPB, we use &#8220;grow-diag-final-and&#8221; to obtain symmetric word alignments, 10 for the maximum phrase length, and the recommended default values for all other parameters.</S>
      </P>
      <P>
        <S ID="S-35871">We train our model on a dataset with &#732;1.5M sentence pairs from the LDC dataset.</S>
        <S ID="S-35872">2 We use the 2002 NIST MT evaluation test data (878 sentence pairs) as the development data, and the 2003, 2004, 2005, 2006-news NIST MT evaluation test data (919, 1788, 1082, and 616 sentence pairs, respectively) as the test data.</S>
        <S ID="S-35873">To find heads, we parse the source sentences with the Berkeley Parser 3 (<REF ID="R-18" RPTR="30">Petrov and Klein, 2007</REF>) trained on Chinese TreeBank 6.0 and use the Penn2Malt toolkit 4 to obtain (unlabeled) dependency structures.</S>
      </P>
      <P>
        <S ID="S-35874">We obtain the word alignments by running</S>
      </P>
      <P>
        <S ID="S-35875">2 This dataset includes LDC2002E18, LDC2003E07,</S>
      </P>
      <P>
        <S ID="S-35876">LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 3 http://code.google.com/p/berkeleyparser/ 4 http://w3.msi.vxu.se/&#732;nivre/research/Penn2Malt.html/</S>
      </P>
      <P>
        <S ID="S-35877">GIZA++ (<REF ID="R-15" RPTR="27">Och and Ney, 2000</REF>) on the corpus in both directions and applying &#8220;grow-diag-final-and&#8221; refinement (<REF ID="R-10" RPTR="24">Koehn et al., 2003</REF>).</S>
        <S ID="S-35878">We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (<REF ID="R-17" RPTR="29">Och, 2003</REF>) to tune the feature weights on the development data.</S>
        <S ID="S-35879">For evaluation, the NIST BLEU script (version 12) with the default settings is used to calculate the BLEU scores.</S>
        <S ID="S-35880">To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach (<REF ID="R-11" RPTR="25">Koehn, 2004</REF>).</S>
        <S ID="S-35881">In this paper, &#8216;**&#8217; and &#8216;*&#8217; denote p-values less than 0.01 and in-between [0.01, 0.05), respectively.</S>
      </P>
      <P>
        <S ID="S-35882">Table 2 lists the rule table sizes.</S>
        <S ID="S-35883">The full rule table size (including HD-HRs and NRRs) of our HD- HPB model is &#732;1.5 times that of Chiang&#8217;s, largely due to refining the non-terminal symbol X in Chiang&#8217;s model into head-informed ones in our model.</S>
        <S ID="S-35884">It is also unsurprising, that the test set-filtered rule table size of our model is only &#732;0.7 times that of Chiang&#8217;s: this is due to the fact that some of the refined translation rule patterns required by the test set are unattested in the training data.</S>
        <S ID="S-35885">Furthermore, the rule table size of NRRs is much smaller than that of HD- HRs since a NRR contains only two non-terminals.</S>
      </P>
      <P>
        <S ID="S-35886">Table 3 lists the translation performance with BLEU scores.</S>
        <S ID="S-35887">Note that our re-implementation of Chiang&#8217;s original HPB model performs on a par with Moses HPB.</S>
        <S ID="S-35888">Table 3 shows that our HD-HPB model significantly outperforms Chiang&#8217;s HPB model with an average improvement of 1.91 in BLEU (and similar improvements over Moses HPB).</S>
        <S ID="S-35889">Table 3 shows that the head-driven scheme outperforms a SAMT-style approach (for each test set p &lt; 0.01), indicating that head information is more effective than (partial) CFG categories.</S>
        <S ID="S-35890">Taking lianming zhichi in Figure 1 as an example, HD-HPB labels the span VV, as lianming is dominated by zhichi, effecively ignoring lianming in the translation rule, while the SAMT label is ADVP:AD+VV 5 which is more susceptible to data sparsity.</S>
        <S ID="S-35891">In addition, SAMT resorts to X if a text span fails to satisify pre-defined categories.</S>
        <S ID="S-35892">Examining initial phrases</S>
      </P>
      <P>
        <S ID="S-35893">5 the constituency structure for lianming zhichi is (VP (ADVP</S>
      </P>
      <P>
        <S ID="S-35894">(AD lianming)) (VP (VV zhichi) ...)).</S>
      </P>
      <P>
        <S ID="S-35895">System Total MT 03 MT 04 MT 05 MT 06 Avg.</S>
        <S ID="S-35896">HPB 39.6 2.8 4.7 3.3 3.0 3.4 HD-HPB 59.5/0.6 1.9/0.1 3.4/0.2 2.3/0.2 2.0/0.1 2.4/0.2</S>
      </P>
      <P>
        <S ID="S-35897">extracted from the SAMT training data shows that 28% of them are labeled as X.</S>
        <S ID="S-35898">In order to separate out the individual contributions of the novel HD-HRs and NRRs, we carry out an additional experiment (HD-HR+Glue) using HD- HRs with monotonic glue rules only (adjusted to refined rule labels, but effectively switching off the extra reordering power of full NRRs).</S>
        <S ID="S-35899">Table 3 shows that on average more than half of the improvement over HPB (Chiang and Moses) comes from the refined HD-HRs, the rest from NRRs.</S>
        <S ID="S-35900">Examining translation rules extracted from the training data shows that there are 72,366 types of non-terminals with respect to 33 types of POS tags.</S>
        <S ID="S-35901">On average each sentence employs 16.6/5.2 HD- HRs/NRRs in our HD-HPB model, compared to 15.9/3.6 hierarchical rules/glue rules in Chiang&#8217;s model, providing further indication of the importance of NRRs in translation.</S>
      </P>
    </DIV>
    <DIV DEPTH="0">
      <HEADER>4 Conclusion</HEADER>
      <P>
        <S ID="S-35902">We present a head-driven hierarchical phrase-based (HD-HPB) translation model, which adopts head information (derived through unlabeled dependency analysis) in the definition of non-terminals to better differentiate among translation rules.</S>
        <S ID="S-35903">In ad- 36 dition, improved and better integrated reordering rules allow better reordering between consecutive non-terminals through exploration of a larger search space in the derivation.</S>
        <S ID="S-35904">Experimental results on Chinese-English translation across four test sets demonstrate significant improvements of the HD- HPB model over both Chiang&#8217;s HPB and a sourceside SAMT-style refined version of HPB.</S>
      </P>
    </DIV>
  </BODY>
  <ACKNOWLEDGMENTS>
    <P>
      <S ID="S-35905">This work was supported by Science Foundation Ireland (Grant No.</S>
      <S ID="S-35906">07/CE/I1142) as part of the Centre for Next Generation Localisation (www.cngl.ie) at Dublin City University.</S>
      <S ID="S-35907">It was also partially supported by Project 90920004 under the National Natural Science Foundation of China and Project 2012AA011102 under the &#8220;863&#8221; National High- Tech Research and Development of China.</S>
      <S ID="S-35908">We thank the reviewers for their insightful comments.</S>
    </P>
    <P>
      <S ID="S-35909">References</S>
    </P>
    <P>
      <S ID="S-35910">Hala Almaghout, Jie Jiang, and Andy Way.</S>
      <S ID="S-35911">2011.</S>
      <S ID="S-35912">CCG contextual labels in hierarchical phrase-based SMT.</S>
      <S ID="S-35913">In Proceedings of EAMT 2011, pages 281&#8211;288.</S>
      <S ID="S-35914">Yee Seng Chan, Hwee Tou Ng, and David Chiang.</S>
      <S ID="S-35915">2007.</S>
      <S ID="S-35916">Word sense disambiguation improves statistical machine translation.</S>
      <S ID="S-35917">In Proceedings of ACL 2007, pages 33&#8211;40.</S>
      <S ID="S-35918">Eugene Charniak.</S>
      <S ID="S-35919">2000.</S>
      <S ID="S-35920">A maximum-entropy-inspired</S>
    </P>
    <P>
      <S ID="S-35921">parser.</S>
      <S ID="S-35922">In Proceedings of NAACL 2000, pages 132&#8211; 139.</S>
      <S ID="S-35923">David Chiang.</S>
      <S ID="S-35924">2005.</S>
      <S ID="S-35925">A hierarchical phrase-based model</S>
    </P>
    <P>
      <S ID="S-35926">for statistical machine translation.</S>
      <S ID="S-35927">In Proceedings of ACL 2005, pages 263&#8211;270.</S>
      <S ID="S-35928">David Chiang.</S>
      <S ID="S-35929">2007.</S>
      <S ID="S-35930">Hierarchical phrase-based translation.</S>
      <S ID="S-35931">Computational Linguistics, 33(2):201&#8211;228.</S>
      <S ID="S-35932">Michael Collins.</S>
      <S ID="S-35933">2003.</S>
      <S ID="S-35934">Head-driven statistical models</S>
    </P>
    <P>
      <S ID="S-35935">for natural language parsing.</S>
      <S ID="S-35936">Computational Linguistics, 29(4):589&#8211;637.</S>
      <S ID="S-35937">Yang Gao, Philipp Koehn, and Alexandra Birch.</S>
      <S ID="S-35938">2011.</S>
    </P>
    <P>
      <S ID="S-35939">Soft dependency constraints for reordering in hierarchical phrase-based translation.</S>
      <S ID="S-35940">In Proceedings of EMNLP 2011, pages 857&#8211;868.</S>
      <S ID="S-35941">Zhongjun He, Yao Meng, and Hao Yu.</S>
      <S ID="S-35942">2010.</S>
      <S ID="S-35943">Maximum entropy based phrase reordering for hierarchical phrase-based translation.</S>
      <S ID="S-35944">In Proceedings of EMNLP 2010, pages 555&#8211;563.</S>
      <S ID="S-35945">Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.</S>
    </P>
    <P>
      <S ID="S-35946">2010.</S>
      <S ID="S-35947">Soft syntactic constraints for hierarchical</S>
    </P>
    <P>
      <S ID="S-35948">phrase-based translation using latent syntactic distributions.</S>
      <S ID="S-35949">In Proceedings of EMNLP 2010, pages 138&#8211; 147.</S>
      <S ID="S-35950">Philipp Koehn, Franz Josef Och, and Daniel Marcu.</S>
    </P>
    <P>
      <S ID="S-35951">2003.</S>
      <S ID="S-35952">Statistical phrase-based translation.</S>
      <S ID="S-35953">In Proceedings of NAACL 2003, pages 48&#8211;54.</S>
      <S ID="S-35954">Philipp Koehn.</S>
      <S ID="S-35955">2004.</S>
      <S ID="S-35956">Statistical significance tests for machine translation evaluation.</S>
      <S ID="S-35957">In Proceedings of EMNLP 2004, pages 388&#8211;395.</S>
      <S ID="S-35958">Yuval Marton and Philip Resnik.</S>
      <S ID="S-35959">2008.</S>
      <S ID="S-35960">Soft syntactic</S>
    </P>
    <P>
      <S ID="S-35961">constraints for hierarchical phrased-based translation.</S>
      <S ID="S-35962">In Proceedings of ACL-HLT 2008, pages 1003&#8211;1011.</S>
      <S ID="S-35963">Markos Mylonakis and Khalil Sima&#8217;an.</S>
      <S ID="S-35964">2011.</S>
      <S ID="S-35965">Learning</S>
    </P>
    <P>
      <S ID="S-35966">hierarchical translation structure with linguistic annotations.</S>
      <S ID="S-35967">In Proceedings of ACL-HLT 2011, pages 642&#8211; 652.</S>
      <S ID="S-35968">Franz Josef Och and Hermann Ney.</S>
      <S ID="S-35969">2000.</S>
      <S ID="S-35970">Improved</S>
    </P>
    <P>
      <S ID="S-35971">statistical alignment models.</S>
      <S ID="S-35972">In Proceedings of ACL 2000, pages 440&#8211;447.</S>
      <S ID="S-35973">Franz Josef Och and Hermann Ney.</S>
      <S ID="S-35974">2004.</S>
      <S ID="S-35975">The alignment template approach to statistical machine translation.</S>
      <S ID="S-35976">Computational Linguistics, 30(4):417&#8211;449.</S>
      <S ID="S-35977">Franz Josef Och.</S>
      <S ID="S-35978">2003.</S>
      <S ID="S-35979">Minimum error rate training in</S>
    </P>
    <P>
      <S ID="S-35980">statistical machine translation.</S>
      <S ID="S-35981">In Proceedings of ACL 2003, pages 160&#8211;167.</S>
      <S ID="S-35982">Slav Petrov and Dan Klein.</S>
      <S ID="S-35983">2007.</S>
      <S ID="S-35984">Improved inference</S>
    </P>
    <P>
      <S ID="S-35985">for unlexicalized parsing.</S>
      <S ID="S-35986">In Proceedings of NAACL 2007, pages 404&#8211;411.</S>
      <S ID="S-35987">Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,</S>
    </P>
    <P>
      <S ID="S-35988">and Ralph Weischedel.</S>
      <S ID="S-35989">2009.</S>
      <S ID="S-35990">Effective use of linguistic and contextual information for statistical machine</S>
    </P>
    <P>
      <S ID="S-35991">translation.</S>
      <S ID="S-35992">In Proceedings of EMNLP 2009, pages</S>
    </P>
    <P>
      <S ID="S-35993">72&#8211;80.</S>
      <S ID="S-35994">Andreas Zollmann and Ashish Venugopal.</S>
      <S ID="S-35995">2006.</S>
      <S ID="S-35996">Syntax</S>
    </P>
    <P>
      <S ID="S-35997">augmented machine translation via chart parsing.</S>
      <S ID="S-35998">In Proceedings of NAACL 2006 - Workshop on Statistical Machine Translation, pages 138&#8211;141.</S>
      <S ID="S-35999">Andreas Zollmann and Stephan Vogel.</S>
      <S ID="S-36000">2011.</S>
      <S ID="S-36001">A wordclass approach to labeling PSCFG rules for machine translation.</S>
      <S ID="S-36002">In Proceedings of ACL-HLT 2011, pages 1&#8211;11.</S>
    </P>
  </ACKNOWLEDGMENTS>
  <REFERENCES>
    <REFERENCE ID="0">
      <RAUTHOR>Hala Almaghout</RAUTHOR>
      <REFTITLE>CCG contextual labels in hierarchical phrase-based SMT.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="1">
      <RAUTHOR>Yee Seng Chan</RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="2">
      <RAUTHOR></RAUTHOR>
      <REFTITLE>Word sense disambiguation improves statistical machine translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="3">
      <RAUTHOR>Eugene Charniak</RAUTHOR>
      <REFTITLE>A maximum-entropy-inspired parser.</REFTITLE>
      <DATE>2000</DATE>
    </REFERENCE>
    <REFERENCE ID="4">
      <RAUTHOR>David Chiang</RAUTHOR>
      <REFTITLE>A hierarchical phrase-based model for statistical machine translation.</REFTITLE>
      <DATE>2005</DATE>
    </REFERENCE>
    <REFERENCE ID="5">
      <RAUTHOR>David Chiang</RAUTHOR>
      <REFTITLE>Hierarchical phrase-based translation.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="6">
      <RAUTHOR>Michael Collins</RAUTHOR>
      <REFTITLE>Head-driven statistical models for natural language parsing.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="7">
      <RAUTHOR>Yang Gao</RAUTHOR>
      <REFTITLE>Soft dependency constraints for reordering in hierarchical phrase-based translation.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="8">
      <RAUTHOR>Zhongjun He</RAUTHOR>
      <REFTITLE>Maximum entropy based phrase reordering for hierarchical phrase-based translation.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="9">
      <RAUTHOR>Zhongqiang Huang</RAUTHOR>
      <REFTITLE>Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions.</REFTITLE>
      <DATE>2010</DATE>
    </REFERENCE>
    <REFERENCE ID="10">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Statistical phrase-based translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="11">
      <RAUTHOR>Philipp Koehn</RAUTHOR>
      <REFTITLE>Statistical significance tests for machine translation evaluation.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="12">
      <RAUTHOR>Yuval Marton</RAUTHOR>
      <REFTITLE>Soft syntactic constraints for hierarchical phrased-based translation.</REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="13">
      <RAUTHOR></RAUTHOR>
      <REFTITLE></REFTITLE>
      <DATE>2008</DATE>
    </REFERENCE>
    <REFERENCE ID="14">
      <RAUTHOR>Markos Mylonakis</RAUTHOR>
      <REFTITLE>Learning hierarchical translation structure with linguistic annotations.</REFTITLE>
      <DATE>2011</DATE>
    </REFERENCE>
    <REFERENCE ID="15">
      <RAUTHOR>Franz Josef Och</RAUTHOR>
      <REFTITLE>Improved statistical alignment models.</REFTITLE>
      <DATE>2000</DATE>
    </REFERENCE>
    <REFERENCE ID="16">
      <RAUTHOR>Franz Josef Och</RAUTHOR>
      <REFTITLE>The alignment template approach to statistical machine translation.</REFTITLE>
      <DATE>2004</DATE>
    </REFERENCE>
    <REFERENCE ID="17">
      <RAUTHOR>Franz Josef Och</RAUTHOR>
      <REFTITLE>Minimum error rate training in statistical machine translation.</REFTITLE>
      <DATE>2003</DATE>
    </REFERENCE>
    <REFERENCE ID="18">
      <RAUTHOR>Slav Petrov</RAUTHOR>
      <REFTITLE>Improved inference for unlexicalized parsing.</REFTITLE>
      <DATE>2007</DATE>
    </REFERENCE>
    <REFERENCE ID="19">
      <RAUTHOR>Libin Shen</RAUTHOR>
      <REFTITLE>Effective use of linguistic and contextual information for statistical machine translation.</REFTITLE>
      <DATE>2009</DATE>
    </REFERENCE>
    <REFERENCE ID="20">
      <RAUTHOR>Andreas Zollmann</RAUTHOR>
      <REFTITLE>Syntax augmented machine translation via chart parsing.</REFTITLE>
      <DATE>2006</DATE>
    </REFERENCE>
  </REFERENCES>
</PAPER>
