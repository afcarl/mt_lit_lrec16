<document>
  <filename>J09-4009</filename>
  <authors>
    <author>Liang Huang</author>
    <author>Hao Zhang</author>
    <author>Daniel Gildea</author>
  </authors>
  <title>Binarization of Synchronous Context-Free Grammars</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>1. Introduction</title>
        <text>Several recent syntax-based models for machine translation (Chiang 2005; Galley et al. 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers. In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right-hand side of a grammar rule. To alleviate this problem, we investigate bilingual binarization as a technique to factor each synchronous grammar rule into a series of binary rules. Although monolingual context-free grammars (CFGs) can always be binarized, this is not the case
We develop a technique called synchronous binarization and devise a linear-time binarization algorithm such that the resulting rule set allows efficient algorithms for both synchronous parsing and decoding with integrated n-gram language models. We examine the effect of this binarization method on end-to-end translation quality on a large-scale Chinese-to-English syntax-based system, compared to a more typical baseline method, and a state-of-the-art phrase-based system. We examine the ratio of binarizability in large, empirically derived rule sets, and show that the vast majority is binarizable. However, we also provide, for the first time, real examples of non-binarizable cases verified by native speakers. In the final, theoretical, sections of this article, we investigate the general problem of finding the most efficient synchronous parsing or decoding strategy for arbitrary synchronous context-free grammar (SCFG) rules, including non-binarizable cases. Although this problem is believed to be NP-complete, we prove two results that substantially reduce the search space over strategies. We also present an optimal algorithm that runs tractably in practice and a polynomial-time algorithm that is a good approximation of the former. Melamed (2003) discusses binarization of multi-text grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing. One way around this difficulty is to stipulate that all rules must be binary from the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binary SCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases. In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rules that apply to the parses. This approach results in rules with many nonterminals, making good binarization techniques critical. We explain how synchronous rule binarization interacts with n-gram language models and affects decoding for machine translation in Section 2. We define binarization formally in Section 3, and present an efficient algorithm for the problem in Section 4. Experiments described in Section 5 1 show that binarization improves machine translation speed and quality. Some rules cannot be binarized, and we present a decoding strategy for these rules in Section 6. Section 7 gives a solution to the general theoretical problem of finding optimal decoding and synchronous parsing strategies for arbitrary SCFGs, and presents complexity results on the nonbinarizable rules from our Chinese&#8211;English data. These final two sections are of primarily theoretical interest, as nonbinarizable rules have not been shown to benefit real-world machine translation systems. However, the algorithms presented may become relevant as machine translation systems improve.
Suppose we have the following SCFG, where superscripts indicate reorderings (formal definitions of SCFGs with a more flexible notation can be found in Section 3): (2)
Decoding can be cast as a (monolingual) parsing problem because we only need to parse the source-language side of the SCFG, as if we were constructing a CFG by projecting the SCFG onto its Chinese side:
The only extra work we need to do for decoding is to build corresponding targetlanguage (English) subtrees in parallel. In other words, we build synchronous trees when parsing the source-language input, as shown in Figure 1. For efficient parsing, we need to binarize the projected CFG either explicitly into Chomsky Normal Form as required by the CKY algorithm, or implicitly into a dotted representation as in the Earley algorithm. To simplify the presentation, we will focus on the former, but the following discussion can be easily adapted to the latter. Rules can be binarized in different ways. For example, we could binarize the first rule left to right or right to left (see Figure 2):
We call these intermediate symbols (e.g., PP-VP) virtual nonterminals and corresponding rules virtual rules, whose probabilities are all set to 1. Figure 1 A pair of synchronous parse trees in the SCFG (2). The superscript symbols ( &#8900;&#8902;&#9702;&#8226; ) indicate pairs of synchronous nonterminals (and subtrees).
Figure 2 The alignment matrix and two binarization schemes, with virtual nonterminals in gray. (a) A two-dimensional matrix representation of the first SCFG rule in grammar 2. Rows are positions in Chinese: columns are positions in English, and black cells indicate positions linked by the SCFG rule. (b) This scheme groups NP and PP into an intermediate state which contains a gap on the English side. (c) This scheme groups PP and VP into an intermediate state which is contiguous on both sides. These two binarizations are no different in the translation-model-only decoding described previously, just as in monolingual parsing. However, in the source-channel approach to machine translation, we need to combine probabilities from the translation model (TM) (an SCFG) with the language model (an n-gram), which has been shown to be very important for translation quality (Chiang 2005). To do bigram-integrated decoding (Wu 1996), we need to augment each chart item (X, i, j) with two targetlanguage boundary words u and v to produce a bigram-item which we denote ( )
where p and q are the scores of antecedent items. This situation is unpleasant because in the target language NP and PP are not contiguous so we cannot apply language model scoring when we build the NP-PP item. Instead, we have to maintain all four boundary words (rather than two) and postpone the language model scoring till the next step where NP-PP is combined with ( held &#183;&#183;&#183; meeting
to form an S item. We call this binarization method monolingual binarization because it works only on the source-language projection of the rule without respecting the constraints from the other side. This scheme generalizes to the case where we have n nonterminals in a SCFG rule, and the decoder conservatively assumes nothing can be done on language model scoring (because target-language spans are non-contiguous in general) until the real nonterminal has been recognized. In other words, target-language boundary words ) 2 An alternative to integrated decoding is rescoring, where one first computes the k-best translations according to the TM only, and then reranks the k-best list with the language model costs. This method runs very fast in practice (Huang and Chiang 2005), but often produces a considerable number of search errors because the true best translation is often outside of the k-best list, especially for longer sentences. 562 Huang et al. Binarization of Synchronous Context-Free Grammars from each child nonterminal of the rule will be cached in all virtual nonterminals derived from this rule. In the case of m-gram integrated decoding, we have to maintain 2(m &#8722; 1) boundary words for each child nonterminal, which leads to a prohibitive overall complexity of O(|w |3+2n(m&#8722;1) ), which is exponential in rule size (Huang, Zhang, and Gildea 2005). Aggressive pruning must be used to make it tractable in practice, which in general introduces many search errors and adversely affects translation quality. In the second case, however, we have:
Here, because PP and VP are contiguous (but swapped) in the target language, we can include the language model score by multiplying in Pr(with  |meeting), and the resulting item again has two boundary words. Later we multiply in Pr(held  |Powell) when the resulting item is combined with ( )
In this case m-gram integrated decoding can be done in O(|w |3+4(m&#8722;1) ) time, which is a much lower-order polynomial and no longer depends on rule size (Wu 1996), allowing the search to be much faster and more accurate, as is evidenced in the Hiero system of Chiang (2005), which restricts the hierarchical phrases to form binary-branching SCFG rules. Some recent syntax-based MT systems (Galley et al. 2004) have adopted the formalism of tree transducers (Rounds 1970), modeling translation as a set of rules for a transducer that takes a syntax tree in one language as input and transforms it into a tree (or string) in the other language. The same decoding algorithms are used for machine translation in this formalism, and the following example shows that the same issues of binarization arise. Suppose we have the following transducer rules:
where the reorderings of nonterminals are denoted by variables x i . In the treetransducer formalism of Rounds (1970), the right-hand (target) side subtree can have multiple levels, as in the first rule above. This system can model non-isomorphic transformations on English parse trees to &#8220;fit&#8221; another language, learning, for example, that the (V S O) structure in Arabic should be transformed into a (S (V O)) structure in English, by looking at two-level tree fragments (Knight and Graehl 2005). From a synchronous rewriting point of view, this is more akin to synchronous tree substitution grammar (STSG) (Eisner 2003; Shieber 2004) (see Figure 3). This larger locality captures more linguistic phenomena and leads to better parameter estimation. By creating a
Figure 3 Two equivalent representations of the first rule in Example (5): (a) tree transducer; (b) Synchronous Tree Subsitution Grammar (STSG). The &#8595; arrows denote substitution sites, which correspond to variables in tree transducers. nonterminal for each right-hand-side tree, we can convert the transducer representation to an SCFG with the same generative capacity. We can again create a projected CFG which will be exactly the same as in Example (3), and build English subtrees while parsing the Chinese input. In this sense we can neglect the tree structures when binarizing a tree-transducer rule, and consider only the alignment (or permutation) of the nonterminal variables. Again, rightmost binarization is preferable for the first rule. In SCFG-based frameworks, the problem of finding a word-level alignment between two sentences is an instance of the synchronous parsing problem: Given two strings and a synchronous grammar, find a parse tree that generates both input strings. The benefit of binary grammars also applies in this case. Wu (1997) shows that parsing a binary-branching SCFG is in O(|w |6 ), while parsing SCFG with arbitrary rules is NP-hard (Satta and Peserico 2005). For example, in Figure 2, the complexity of synchronous parsing for the original grammar (a) is O(|w |8 ), because we have to maintain four indices on either side, giving a total of eight; parsing the monolingually binarized grammar (b) involves seven indices, three on the Chinese side and four on the English side. In contrast, the synchronously binarized version (c) requires only 3 + 3 = 6 indices, which can be thought of as &#8220;CKY in two dimensions.&#8221; An efficient alignment algorithm is guaranteed if a binarization is found, and the same binarization can be used for decoding and alignment. We show how to find optimal alignment algorithms for non-binarizable rules in Section 7; in this case different grammar factorizations may be optimal for alignment and for decoding with n-gram models of various orders. Handling difficult rules may in fact be more important for alignment than for decoding, because although we may be able to find good translations during decoding within the space permitted by computationally friendly rules, during alignment we must handle the broader spectrum of phenomena found in real bitext data. In general, if we are given an arbitrary synchronous rule with many nonterminals, what are the good decompositions that lead to a binary grammar? Figure 2 suggests that a binarization is good if every virtual nonterminal has contiguous spans on both sides. We formalize this idea in the next section.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Several recent syntax-based models for machine translation (Chiang 2005; Galley et al. 2004) can be seen as instances of the general framework of synchronous grammars and tree transducers.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this framework, both alignment (synchronous parsing) and decoding can be thought of as parsing problems, whose complexity is in general exponential in the number of nonterminals on the right-hand side of a grammar rule.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>To alleviate this problem, we investigate bilingual binarization as a technique to factor each synchronous grammar rule into a series of binary rules.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Although monolingual context-free grammars (CFGs) can always be binarized, this is not the case</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We develop a technique called synchronous binarization and devise a linear-time binarization algorithm such that the resulting rule set allows efficient algorithms for both synchronous parsing and decoding with integrated n-gram language models.</text>
              <doc_id>4</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We examine the effect of this binarization method on end-to-end translation quality on a large-scale Chinese-to-English syntax-based system, compared to a more typical baseline method, and a state-of-the-art phrase-based system.</text>
              <doc_id>5</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We examine the ratio of binarizability in large, empirically derived rule sets, and show that the vast majority is binarizable.</text>
              <doc_id>6</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>However, we also provide, for the first time, real examples of non-binarizable cases verified by native speakers.</text>
              <doc_id>7</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In the final, theoretical, sections of this article, we investigate the general problem of finding the most efficient synchronous parsing or decoding strategy for arbitrary synchronous context-free grammar (SCFG) rules, including non-binarizable cases.</text>
              <doc_id>8</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Although this problem is believed to be NP-complete, we prove two results that substantially reduce the search space over strategies.</text>
              <doc_id>9</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We also present an optimal algorithm that runs tractably in practice and a polynomial-time algorithm that is a good approximation of the former.</text>
              <doc_id>10</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Melamed (2003) discusses binarization of multi-text grammars on a theoretical level, showing the importance and difficulty of binarization for efficient synchronous parsing.</text>
              <doc_id>11</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>One way around this difficulty is to stipulate that all rules must be binary from the outset, as in Inversion Transduction Grammar (ITG) (Wu 1997) and the binary SCFG employed by the Hiero system (Chiang 2005) to model the hierarchical phrases.</text>
              <doc_id>12</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>In contrast, the rule extraction method of Galley et al. (2004) aims to incorporate more syntactic information by providing parse trees for the target language and extracting tree transducer rules that apply to the parses.</text>
              <doc_id>13</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>This approach results in rules with many nonterminals, making good binarization techniques critical.</text>
              <doc_id>14</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>We explain how synchronous rule binarization interacts with n-gram language models and affects decoding for machine translation in Section 2.</text>
              <doc_id>15</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>We define binarization formally in Section 3, and present an efficient algorithm for the problem in Section 4.</text>
              <doc_id>16</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>Experiments described in Section 5 1 show that binarization improves machine translation speed and quality.</text>
              <doc_id>17</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>Some rules cannot be binarized, and we present a decoding strategy for these rules in Section 6.</text>
              <doc_id>18</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>Section 7 gives a solution to the general theoretical problem of finding optimal decoding and synchronous parsing strategies for arbitrary SCFGs, and presents complexity results on the nonbinarizable rules from our Chinese&#8211;English data.</text>
              <doc_id>19</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>These final two sections are of primarily theoretical interest, as nonbinarizable rules have not been shown to benefit real-world machine translation systems.</text>
              <doc_id>20</doc_id>
              <sec_id>16</sec_id>
            </sentence>
            <sentence>
              <text>However, the algorithms presented may become relevant as machine translation systems improve.</text>
              <doc_id>21</doc_id>
              <sec_id>17</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Suppose we have the following SCFG, where superscripts indicate reorderings (formal definitions of SCFGs with a more flexible notation can be found in Section 3): (2)</text>
              <doc_id>22</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Decoding can be cast as a (monolingual) parsing problem because we only need to parse the source-language side of the SCFG, as if we were constructing a CFG by projecting the SCFG onto its Chinese side:</text>
              <doc_id>23</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The only extra work we need to do for decoding is to build corresponding targetlanguage (English) subtrees in parallel.</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In other words, we build synchronous trees when parsing the source-language input, as shown in Figure 1.</text>
              <doc_id>25</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For efficient parsing, we need to binarize the projected CFG either explicitly into Chomsky Normal Form as required by the CKY algorithm, or implicitly into a dotted representation as in the Earley algorithm.</text>
              <doc_id>26</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>To simplify the presentation, we will focus on the former, but the following discussion can be easily adapted to the latter.</text>
              <doc_id>27</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Rules can be binarized in different ways.</text>
              <doc_id>28</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>For example, we could binarize the first rule left to right or right to left (see Figure 2):</text>
              <doc_id>29</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We call these intermediate symbols (e.g., PP-VP) virtual nonterminals and corresponding rules virtual rules, whose probabilities are all set to 1.</text>
              <doc_id>30</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Figure 1 A pair of synchronous parse trees in the SCFG (2).</text>
              <doc_id>31</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The superscript symbols ( &#8900;&#8902;&#9702;&#8226; ) indicate pairs of synchronous nonterminals (and subtrees).</text>
              <doc_id>32</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Figure 2 The alignment matrix and two binarization schemes, with virtual nonterminals in gray.</text>
              <doc_id>33</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>(a) A two-dimensional matrix representation of the first SCFG rule in grammar 2.</text>
              <doc_id>34</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Rows are positions in Chinese: columns are positions in English, and black cells indicate positions linked by the SCFG rule.</text>
              <doc_id>35</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>(b) This scheme groups NP and PP into an intermediate state which contains a gap on the English side.</text>
              <doc_id>36</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>(c) This scheme groups PP and VP into an intermediate state which is contiguous on both sides.</text>
              <doc_id>37</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>These two binarizations are no different in the translation-model-only decoding described previously, just as in monolingual parsing.</text>
              <doc_id>38</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>However, in the source-channel approach to machine translation, we need to combine probabilities from the translation model (TM) (an SCFG) with the language model (an n-gram), which has been shown to be very important for translation quality (Chiang 2005).</text>
              <doc_id>39</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>To do bigram-integrated decoding (Wu 1996), we need to augment each chart item (X, i, j) with two targetlanguage boundary words u and v to produce a bigram-item which we denote ( )</text>
              <doc_id>40</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where p and q are the scores of antecedent items.</text>
              <doc_id>41</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This situation is unpleasant because in the target language NP and PP are not contiguous so we cannot apply language model scoring when we build the NP-PP item.</text>
              <doc_id>42</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Instead, we have to maintain all four boundary words (rather than two) and postpone the language model scoring till the next step where NP-PP is combined with ( held &#183;&#183;&#183; meeting</text>
              <doc_id>43</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>to form an S item.</text>
              <doc_id>44</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We call this binarization method monolingual binarization because it works only on the source-language projection of the rule without respecting the constraints from the other side.</text>
              <doc_id>45</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This scheme generalizes to the case where we have n nonterminals in a SCFG rule, and the decoder conservatively assumes nothing can be done on language model scoring (because target-language spans are non-contiguous in general) until the real nonterminal has been recognized.</text>
              <doc_id>46</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In other words, target-language boundary words ) 2 An alternative to integrated decoding is rescoring, where one first computes the k-best translations according to the TM only, and then reranks the k-best list with the language model costs.</text>
              <doc_id>47</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This method runs very fast in practice (Huang and Chiang 2005), but often produces a considerable number of search errors because the true best translation is often outside of the k-best list, especially for longer sentences.</text>
              <doc_id>48</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>562 Huang et al. Binarization of Synchronous Context-Free Grammars from each child nonterminal of the rule will be cached in all virtual nonterminals derived from this rule.</text>
              <doc_id>49</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In the case of m-gram integrated decoding, we have to maintain 2(m &#8722; 1) boundary words for each child nonterminal, which leads to a prohibitive overall complexity of O(|w |3+2n(m&#8722;1) ), which is exponential in rule size (Huang, Zhang, and Gildea 2005).</text>
              <doc_id>50</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Aggressive pruning must be used to make it tractable in practice, which in general introduces many search errors and adversely affects translation quality.</text>
              <doc_id>51</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>In the second case, however, we have:</text>
              <doc_id>52</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Here, because PP and VP are contiguous (but swapped) in the target language, we can include the language model score by multiplying in Pr(with  |meeting), and the resulting item again has two boundary words.</text>
              <doc_id>53</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Later we multiply in Pr(held  |Powell) when the resulting item is combined with ( )</text>
              <doc_id>54</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this case m-gram integrated decoding can be done in O(|w |3+4(m&#8722;1) ) time, which is a much lower-order polynomial and no longer depends on rule size (Wu 1996), allowing the search to be much faster and more accurate, as is evidenced in the Hiero system of Chiang (2005), which restricts the hierarchical phrases to form binary-branching SCFG rules.</text>
              <doc_id>55</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Some recent syntax-based MT systems (Galley et al. 2004) have adopted the formalism of tree transducers (Rounds 1970), modeling translation as a set of rules for a transducer that takes a syntax tree in one language as input and transforms it into a tree (or string) in the other language.</text>
              <doc_id>56</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The same decoding algorithms are used for machine translation in this formalism, and the following example shows that the same issues of binarization arise.</text>
              <doc_id>57</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Suppose we have the following transducer rules:</text>
              <doc_id>58</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where the reorderings of nonterminals are denoted by variables x i .</text>
              <doc_id>59</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In the treetransducer formalism of Rounds (1970), the right-hand (target) side subtree can have multiple levels, as in the first rule above.</text>
              <doc_id>60</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This system can model non-isomorphic transformations on English parse trees to &#8220;fit&#8221; another language, learning, for example, that the (V S O) structure in Arabic should be transformed into a (S (V O)) structure in English, by looking at two-level tree fragments (Knight and Graehl 2005).</text>
              <doc_id>61</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>From a synchronous rewriting point of view, this is more akin to synchronous tree substitution grammar (STSG) (Eisner 2003; Shieber 2004) (see Figure 3).</text>
              <doc_id>62</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>This larger locality captures more linguistic phenomena and leads to better parameter estimation.</text>
              <doc_id>63</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>By creating a</text>
              <doc_id>64</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Figure 3 Two equivalent representations of the first rule in Example (5): (a) tree transducer; (b) Synchronous Tree Subsitution Grammar (STSG).</text>
              <doc_id>65</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The &#8595; arrows denote substitution sites, which correspond to variables in tree transducers.</text>
              <doc_id>66</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>nonterminal for each right-hand-side tree, we can convert the transducer representation to an SCFG with the same generative capacity.</text>
              <doc_id>67</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We can again create a projected CFG which will be exactly the same as in Example (3), and build English subtrees while parsing the Chinese input.</text>
              <doc_id>68</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In this sense we can neglect the tree structures when binarizing a tree-transducer rule, and consider only the alignment (or permutation) of the nonterminal variables.</text>
              <doc_id>69</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Again, rightmost binarization is preferable for the first rule.</text>
              <doc_id>70</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In SCFG-based frameworks, the problem of finding a word-level alignment between two sentences is an instance of the synchronous parsing problem: Given two strings and a synchronous grammar, find a parse tree that generates both input strings.</text>
              <doc_id>71</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The benefit of binary grammars also applies in this case.</text>
              <doc_id>72</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Wu (1997) shows that parsing a binary-branching SCFG is in O(|w |6 ), while parsing SCFG with arbitrary rules is NP-hard (Satta and Peserico 2005).</text>
              <doc_id>73</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>For example, in Figure 2, the complexity of synchronous parsing for the original grammar (a) is O(|w |8 ), because we have to maintain four indices on either side, giving a total of eight; parsing the monolingually binarized grammar (b) involves seven indices, three on the Chinese side and four on the English side.</text>
              <doc_id>74</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>In contrast, the synchronously binarized version (c) requires only 3 + 3 = 6 indices, which can be thought of as &#8220;CKY in two dimensions.&#8221; An efficient alignment algorithm is guaranteed if a binarization is found, and the same binarization can be used for decoding and alignment.</text>
              <doc_id>75</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>We show how to find optimal alignment algorithms for non-binarizable rules in Section 7; in this case different grammar factorizations may be optimal for alignment and for decoding with n-gram models of various orders.</text>
              <doc_id>76</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Handling difficult rules may in fact be more important for alignment than for decoding, because although we may be able to find good translations during decoding within the space permitted by computationally friendly rules, during alignment we must handle the broader spectrum of phenomena found in real bitext data.</text>
              <doc_id>77</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>In general, if we are given an arbitrary synchronous rule with many nonterminals, what are the good decompositions that lead to a binary grammar?</text>
              <doc_id>78</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>Figure 2 suggests that a binarization is good if every virtual nonterminal has contiguous spans on both sides.</text>
              <doc_id>79</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>We formalize this idea in the next section.</text>
              <doc_id>80</doc_id>
              <sec_id>15</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>3. Synchronous Binarization</title>
        <text>Definition 1 A synchronous CFG (SCFG) is a context-free rewriting system for generating string pairs. Each rule (synchronous production) A &#8594; &#945;, B &#8594; &#946; 564 Huang et al. Binarization of Synchronous Context-Free Grammars rewrites a pair of synchronous nonterminals (A, B) in two dimensions subject to the constraint that there is a one-to-one mapping between the nonterminal occurrences in &#945; and the nonterminal occurrences in &#946;. Each co-indexed child nonterminal pair is a pair of synchronous nonterminals and will be further rewritten as a unit. Note that this notation, due to Satta and Peserico (2005), is more flexible than those in the previous section, in the sense that we can allow different symbols to be synchronized, which is essential to capture the syntactic divergences between languages. For example, the following rule from Chinese to English
We define the language L(G) produced by an SCFG G as the pairs of terminal strings produced by rewriting exhaustively from the start nonterminal pair. As shown in Section 4.2, terminals do not play an important role in binarization. So we now write rules in the following notation:
where X i and Y i are variables ranging over nonterminals in the source and target projections of the synchronous grammar, respectively, and &#960; is the permutation of the rule. For example, in rule (6), we have n = 2, X = Y = VP, X 1 = VB, X 2 = NN, Y 1 = VBZ, Y 2 = NNS, and &#960; is the identity permutation. Note that this general notation includes cases where a nonterminal occurs more than once in the right-hand side, for example, when n = 2, X = Y = A, and X 1 = X 2 = Y 1 = Y 2 = B, we can have the following two rules: A &#8594; B 1 B 2 , A &#8594; B 2 B 1 ; A &#8594; B 1 B 2 , A &#8594; B 1 B 2 . We also define an SCFG rule as n-ary if its permutation is of n and call an SCFG n-ary if its longest rule is n-ary. Our goal is to produce an equivalent binary SCFG for an input n-ary SCFG. However, not every SCFG can be binarized. In fact, the binarizability of an n- ary rule is determined by the structure of its permutation, which can sometimes be resistant to factorization (Aho and Ullman 1972). We now turn to rigorously defining the binarizability of permutations. Definition 2 A permuted sequence is a permutation of consecutive integers. If a permuted sequence a can be split into the concatenation of two permuted sequences b and c, then (b; c) is called a proper split of a. We say b &amp;lt; c if each element in b is smaller than any element in c.
For example, (3, 5, 4) is a permuted sequence whereas (2, 5) is not. As special cases, single numbers are permuted sequences as well. (3; 5, 4) is a proper split of (3, 5, 4) whereas (3, 5; 4) is not. A proper split has the following property: Lemma 1 For a permuted sequence a, a = (b; c) is a proper split if and only if b &amp;lt; c or c &amp;lt; b. Proof The &#8658; direction is trivial by the definition of proper split. We prove the &#8656; direction by contradiction. If b &amp;lt; c but b is not a permuted sequence, i.e., the set of b&#8217;s elements is not consecutive, then there must be some x &#8712; c such that min b &amp;lt; x &amp;lt; max b, which contradicts the fact that b &amp;lt; c. We have a similar contradiction if c is not a permuted sequence. Now that both b and c are permuted sequences, (b; c) is a proper split. The case when b &amp;gt; c is similar. &#65533; Definition 3 A permuted sequence a is said to be binarizable if either
such split a binarizable split. This is a recursive definition, and it implies that there is a hierarchical binarization pattern associated with each binarizable sequence, which we now rigorously define. Definition 4 A binarization tree bi(a) of a binarizable sequence a is either
1. a if a = (a), or 2. [bi(b), bi(c)] if b &amp;lt; c, or &#12296;bi(b), bi(c)&#12297; otherwise, where a = (b; c) is a
binarizable split, and bi(b) is a binarization tree of b and bi(c) a binarization tree of c. Here we use [] and &#12296;&#12297; for straight (b &amp;lt; c) and inverted (b &amp;gt; c) combinations, respectively, following the ITG notation of Wu (1997). Note that a binarizable sequence might have multiple binarization trees. See Figure 4 for a binarizable sequence (1, 2, 4, 3) with its two possible binarization trees and a non-binarizable sequence (2, 4, 1, 3). We are now able to define the binarizability of SCFGs: Definition 5 An SCFG is said to be binarizable if the permutation of each synchronous production is binarizable. We denote the class of binarizable SCFGs as bSCFG. This set, bSCFG, represents an important subclass of SCFG that is easy to handle (for example, parsable in O(|w |6 )) and covers many interesting longer-than-two rules. The goal of synchronous binarization, then, is to convert a binarizable grammar G in bSCFG, which might be n-ary with n &#8805; 2, into an equivalent binary grammar G &#8242; that
Figure 5 Subclasses of SCFG. The thick arrow denotes the direction of synchronous binarization and indicates bSCFG can collapse to binary SCFG. decomposes the original permutation into a set of binary ones. All that remains is to decorate the skeleton binarization tree with nonterminal symbols and attach terminals to the skeleton appropriately (see the next section for details). We state this result as the following theorem: Theorem 1 For each grammar G in bSCFG, there exists a binary SCFG G &#8242; , such that L(G &#8242; ) = L(G).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Definition 1 A synchronous CFG (SCFG) is a context-free rewriting system for generating string pairs.</text>
              <doc_id>81</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each rule (synchronous production) A &#8594; &#945;, B &#8594; &#946; 564 Huang et al. Binarization of Synchronous Context-Free Grammars rewrites a pair of synchronous nonterminals (A, B) in two dimensions subject to the constraint that there is a one-to-one mapping between the nonterminal occurrences in &#945; and the nonterminal occurrences in &#946;.</text>
              <doc_id>82</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Each co-indexed child nonterminal pair is a pair of synchronous nonterminals and will be further rewritten as a unit.</text>
              <doc_id>83</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Note that this notation, due to Satta and Peserico (2005), is more flexible than those in the previous section, in the sense that we can allow different symbols to be synchronized, which is essential to capture the syntactic divergences between languages.</text>
              <doc_id>84</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>For example, the following rule from Chinese to English</text>
              <doc_id>85</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We define the language L(G) produced by an SCFG G as the pairs of terminal strings produced by rewriting exhaustively from the start nonterminal pair.</text>
              <doc_id>86</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As shown in Section 4.2, terminals do not play an important role in binarization.</text>
              <doc_id>87</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>So we now write rules in the following notation:</text>
              <doc_id>88</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where X i and Y i are variables ranging over nonterminals in the source and target projections of the synchronous grammar, respectively, and &#960; is the permutation of the rule.</text>
              <doc_id>89</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For example, in rule (6), we have n = 2, X = Y = VP, X 1 = VB, X 2 = NN, Y 1 = VBZ, Y 2 = NNS, and &#960; is the identity permutation.</text>
              <doc_id>90</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Note that this general notation includes cases where a nonterminal occurs more than once in the right-hand side, for example, when n = 2, X = Y = A, and X 1 = X 2 = Y 1 = Y 2 = B, we can have the following two rules: A &#8594; B 1 B 2 , A &#8594; B 2 B 1 ; A &#8594; B 1 B 2 , A &#8594; B 1 B 2 .</text>
              <doc_id>91</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We also define an SCFG rule as n-ary if its permutation is of n and call an SCFG n-ary if its longest rule is n-ary.</text>
              <doc_id>92</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Our goal is to produce an equivalent binary SCFG for an input n-ary SCFG.</text>
              <doc_id>93</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>However, not every SCFG can be binarized.</text>
              <doc_id>94</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>In fact, the binarizability of an n- ary rule is determined by the structure of its permutation, which can sometimes be resistant to factorization (Aho and Ullman 1972).</text>
              <doc_id>95</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>We now turn to rigorously defining the binarizability of permutations.</text>
              <doc_id>96</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Definition 2 A permuted sequence is a permutation of consecutive integers.</text>
              <doc_id>97</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>If a permuted sequence a can be split into the concatenation of two permuted sequences b and c, then (b; c) is called a proper split of a.</text>
              <doc_id>98</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>We say b &amp;lt; c if each element in b is smaller than any element in c.</text>
              <doc_id>99</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For example, (3, 5, 4) is a permuted sequence whereas (2, 5) is not.</text>
              <doc_id>100</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As special cases, single numbers are permuted sequences as well.</text>
              <doc_id>101</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>(3; 5, 4) is a proper split of (3, 5, 4) whereas (3, 5; 4) is not.</text>
              <doc_id>102</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>A proper split has the following property: Lemma 1 For a permuted sequence a, a = (b; c) is a proper split if and only if b &amp;lt; c or c &amp;lt; b. Proof The &#8658; direction is trivial by the definition of proper split.</text>
              <doc_id>103</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We prove the &#8656; direction by contradiction.</text>
              <doc_id>104</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>If b &amp;lt; c but b is not a permuted sequence, i.e., the set of b&#8217;s elements is not consecutive, then there must be some x &#8712; c such that min b &amp;lt; x &amp;lt; max b, which contradicts the fact that b &amp;lt; c.</text>
              <doc_id>105</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We have a similar contradiction if c is not a permuted sequence.</text>
              <doc_id>106</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Now that both b and c are permuted sequences, (b; c) is a proper split.</text>
              <doc_id>107</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>The case when b &amp;gt; c is similar.</text>
              <doc_id>108</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>&#65533; Definition 3 A permuted sequence a is said to be binarizable if either</text>
              <doc_id>109</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>such split a binarizable split.</text>
              <doc_id>110</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This is a recursive definition, and it implies that there is a hierarchical binarization pattern associated with each binarizable sequence, which we now rigorously define.</text>
              <doc_id>111</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Definition 4 A binarization tree bi(a) of a binarizable sequence a is either</text>
              <doc_id>112</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1. a if a = (a), or 2.</text>
              <doc_id>113</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>[bi(b), bi(c)] if b &amp;lt; c, or &#12296;bi(b), bi(c)&#12297; otherwise, where a = (b; c) is a</text>
              <doc_id>114</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>binarizable split, and bi(b) is a binarization tree of b and bi(c) a binarization tree of c.</text>
              <doc_id>115</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Here we use [] and &#12296;&#12297; for straight (b &amp;lt; c) and inverted (b &amp;gt; c) combinations, respectively, following the ITG notation of Wu (1997).</text>
              <doc_id>116</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Note that a binarizable sequence might have multiple binarization trees.</text>
              <doc_id>117</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>See Figure 4 for a binarizable sequence (1, 2, 4, 3) with its two possible binarization trees and a non-binarizable sequence (2, 4, 1, 3).</text>
              <doc_id>118</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We are now able to define the binarizability of SCFGs: Definition 5 An SCFG is said to be binarizable if the permutation of each synchronous production is binarizable.</text>
              <doc_id>119</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We denote the class of binarizable SCFGs as bSCFG.</text>
              <doc_id>120</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>This set, bSCFG, represents an important subclass of SCFG that is easy to handle (for example, parsable in O(|w |6 )) and covers many interesting longer-than-two rules.</text>
              <doc_id>121</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The goal of synchronous binarization, then, is to convert a binarizable grammar G in bSCFG, which might be n-ary with n &#8805; 2, into an equivalent binary grammar G &#8242; that</text>
              <doc_id>122</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Figure 5 Subclasses of SCFG.</text>
              <doc_id>123</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The thick arrow denotes the direction of synchronous binarization and indicates bSCFG can collapse to binary SCFG.</text>
              <doc_id>124</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>decomposes the original permutation into a set of binary ones.</text>
              <doc_id>125</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>All that remains is to decorate the skeleton binarization tree with nonterminal symbols and attach terminals to the skeleton appropriately (see the next section for details).</text>
              <doc_id>126</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We state this result as the following theorem: Theorem 1 For each grammar G in bSCFG, there exists a binary SCFG G &#8242; , such that L(G &#8242; ) = L(G).</text>
              <doc_id>127</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>4. Binarization Algorithms</title>
        <text>We have reduced the problem of binarizing an SCFG rule into the problem of binarizing its permutation. The simplest algorithm for this problem is to try all bracketings of a permutation and pick one that corresponds to a binarization tree. The number of all possible bracketings of a sequence of length n is known to be the Catalan Number
which grows exponentially with n. A better approach is to reduce this problem to an instance of synchronous ITG parsing (Wu 1997). Here the parallel string pair that we are parsing is the integer sequence (1...n) and its permutation (&#960;(1)...&#960;(n)). The goal of the ITG parsing is to find a synchronous tree that agrees with the alignment indicated by the permutation. Synchronous ITG parsing runs in time O(n 6 ) but can be improved to O(n 4 ) because there is no insertion or deletion in a permutation.
Another problem besides efficiency is that there are possibly multiple binarization trees for many permutations whereas we just need one. We would prefer a consistent pattern of binarization trees across different permutations so that sub-binarizations (virtual nonterminals) can be shared. For example, permutations (1, 3, 2, 5, 4) and (1, 3, 2, 4) can share the common sub-binarization tree [1, &#12296;3, 2&#12297;]. To this end, we can borrow the non-ambiguous ITG of Wu (1997, Section 7) that prefers left-heavy binarization trees so that for each permutation there is a unique synchronous derivation. 3 We now refine the definition of binarization trees accordingly. Definition 6 A binarization tree bi(a) is said to be canonical if the split at each non-leaf node of the tree is the rightmost binarizable split. For example, for sequence (1, 2, 4, 3), the binarization tree [[1, 2], &#12296;4, 3&#12297;] is canonical, whereas [1, [2, &#12296;4, 3&#12297;]] is not, because its top-level split is not at the rightmost binarizable split (1, 2; 4, 3). By definition, there is a unique canonical binarization tree for each binarizable sequence. We next present an algorithm that is both fast and consistent.
Shapiro and Stephens (1991, page 277) informally present an iterative procedure that, in each pass, scans the permuted sequence from left to right and combines two adjacent subsequences whenever possible. This procedure produces canonical binarization trees and runs in O(n 2 ) time because we need n passes in the worst case. Inspired by the Graham Scan Algorithm (Graham 1972) for computing convex hulls from computational geometry, we modify this procedure and improve it into a linear-time algorithm that only needs one pass through the sequence. The skeleton binarization algorithm is an instance of the widely used left-to-right shift-reduce algorithm. It maintains a stack for contiguous subsequences discovered so far; for example: 2&#8211;5, 1. In each iteration, it shifts the next number from the input and repeatedly tries to reduce the top two elements on the stack if they are consecutive. See Algorithm 1 for the pseudo-code and Figures 6 and 7 for example runs on binarizable and non-binarizable permutations, respectively. We need the following lemma to prove the properties of the algorithm:
We prove by induction on the length of a. Base case: |a |= 2, a (proper) subsequence of a, has length 1, so it is binarizable. For |a |&amp;gt; 2, because a has a binarization tree, there 3 We are not aiming at optimal sharing, that is, a strategy that produces the smallest binarized grammar for a given ruleset, which would require a global optimization problem over the whole set. In practice, we can only use online algorithms that binarize rules one by one. The left-heavy (or its symmetric variant, right-heavy) preference we choose here is one of the obvious candidates for consistency.
568 Huang et al. Binarization of Synchronous Context-Free Grammars Algorithm 1 The linear-time binarization algorithm. 1: function SYNCHRONOUSBINARIZER(&#960;) 2: top &#8592; 0 &#8882; stack top pointer 3: PUSH(stack, (&#960;(1), &#960;(1))) &#8882; initial shift 4: for i &#8592; 2 to |&#960; |do &#8882; for each remaining element 5: PUSH(stack, (&#960;(i), &#960;(i))) &#8882; shift 6: while top &amp;gt; 1 and CONSECUTIVE(stack[top], stack[top &#8722; 1]) do 7: &#8882; keep reducing if possible 8: (p, q) &#8592; COMBINE(stack[top], stack[top &#8722; 1]) 9: top &#8592; top &#8722; 2 10: PUSH(stack, (p, q)) 11: return (top = 1) &#8882; returns true iff. the input is reduced to a single element 12: 13: function CONSECUTIVE((a, b), (c, d)) 14: return (b = c &#8722; 1) or (d = a &#8722; 1) &#8882; either straight or inverted 15: function COMBINE((a, b), (c, d)) 16: A = {min(a, c)...max(b, d)} 17: B = {a...b} 18: C = {c...d} 19: rule[A] = A &#8594; B C 20: return (min(a, c), max(b, d))
Figure 6 Example of Algorithm 1 on the binarizable input (1, 5, 3, 4, 2). The rightmost column shows the binarization-trees generated at each reduction step. exists a (binarizable) split which is nearest to the root and splits c into two parts. Let the split be (b 1 , c 1 ; c 2 , b 2 ), where c = (c 1 ; c 2 ), and either b 1 or b 2 can be empty. By Lemma 1, we have c 1 &amp;lt; c 2 or c 1 &amp;gt; c 2 . By Lemma 1 again, we have that (c 1 ; c 2 ) is a proper split of c, i.e., both c 1 and c 2 are themselves permuted sequences. We also know both (b 1 , c 1 ) and (c 2 , b 2 ) are binarizable. By the induction hypothesis, c 1 and c 2 are both binarizable. So we conclude that c = (c 1 ; c 2 ) is binarizable (See figure 8). &#65533; We now state the central result of this work. Theorem 2 Algorithm 1 runs in time linear to the length of the input, and succeeds (i.e., it reduces the input into one single element) if and only if the input permuted sequence a is binarizable, in which case the binarization tree recovered is canonical.
Illustration of the proof of Lemma 2. The arrangement of (b 1 , c 1 ; c 2 , b 2 ) must be either all straight as in (a) or all inverted as in (b). Proof We prove the following three parts of this theorem:
1. If Algorithm 1 succeeds, then a is binarizable because we can recover a binarization tree from the algorithm. 2. If a is binarizable, then Algorithm 1 must succeed and the binarization tree
recovered must be canonical: We prove by a complete induction on n, the length of a. Base case: n = 1, trivial. Assume it holds for all n &#8242; &amp;lt; n. If a is binarizable, then let a = (b; c) be its rightmost binarizable split. By definition, both b and c are binarizable. By the induction hypothesis, the algorithm succeeds on the partial input b, reducing it to the single element stack[0] on the stack and recovering its canonical binarization tree bi(b). If c is a singleton, the algorithm will combine it with the element stack[0] and succeed. The final binarization tree is canonical because the top-level split is at the rightmost binarizable split, and both subtrees are canonical. If c is not a singleton, we want to show by contradiction that the algorithm will never combine b with a proper prefix of c. Because a = (b; c) is a proper split, we know that either b &amp;lt; c or c &amp;lt; b. Now if the algorithm makes a combination of (b; c 1 ) for some proper prefix c 1 where c = (c 1 ; c 2 ), we have either c 1 &amp;lt; c 2 or c 1 &amp;gt; c 2 . By Lemma 1, (c 1 ; c 2 ) is a proper split. Because c is binarizable, by Lemma 2, c 2 is also binarizable. So (b, c 1 ; c 2 ) is a binarizable split to the right of (b; c), which contradicts the assumption that the latter is the rightmost binarizable split (see Figure 9). 570 Huang et al. Binarization of Synchronous Context-Free Grammars Figure 9 Illustration of the proof of Theorem 2. The combination of (b; c 1 ) (in dashed squares) contradicts the assumption that (b; c) is the rightmost binarizable split of a. &#65533; Therefore, the algorithm will scan through the whole c as if from the empty stack. By the induction hypothesis again, it will reduce c into stack[1] on the stack and recover its canonical binarization tree bi(c). Because b and c are combinable, the algorithm reduces stack[0] and stack[1] in the last step, forming the canonical binarization tree for a, which is either [bi(b), bi(c)] or &#12296;bi(b), bi(c)&#12297;.
By amortized analysis (Cormen, Leiserson, and Rivest 1990), there are exactly n shifts and at most n &#8722; 1 reductions, and each shift or reduction takes O(1) time. So the total time complexity is O(n).
Thus far we have discussed how to binarize synchronous productions involving only nonterminals through binarizing the corresponding skeleton permutations. We now turn to technical details for the implementation of a synchronous binarizer in real MT systems. We will first show how to deal with the terminal symbols, and then describe how to adapt it to tree transducers. Consider the following SCFG rule: (7) ADJP &#8594; RB 1 # / f&#249;z&#233; PP 2 &#8222; / de NN 3 , ADJP &#8594; RB 1 responsible for the NN 3 PP 2 whose permutation is (1, 3, 2). We run the skeleton binarization algorithm and get the (canonical) binarization tree [1, &#12296;3, 2&#12297;], which corresponds to [RB, &#12296;NN, PP&#12297;] (see Figure 10(a)). The alignment matrix is shown in Figure 11. We will then do a post-order traversal of the skeleton tree, and attach the terminals from both languages when appropriate. It turns out we can do this quite freely as long as we can uniquely reconstruct the original rule from its binary parse tree. We use the following rules for this step: 1. Attach source-language terminals to the leaf nodes of the skeleton tree. Consecutive terminals are attached to the nonterminal on their left
Figure 10 Attaching terminals in SCFG binarization. (a) The skeleton binarization tree, (b) attaching Chinese words at leaf nodes, (c) attaching English words at internal nodes. Figure 11 Alignment matrix of the SCFG rule (7). Areas shaded in gray and light gray denote virtual nonterminals (see rules in Example (8)). (except for the initial ones which are attached to the nonterminal on their right). 2. Attach target-language terminals to the internal nodes (virtual nonterminals) of the skeleton tree. These terminals are attached greedily: When combining two nonterminals, all target-side terminal strings neighboring either nonterminal will be included. This greedy merging is motivated by the idea that the language model score helps to guide the decoder and should be computed as early as possible. For example, at the leaf nodes, the Chinese word # / f&#249;z&#233; is attached to RB 1 , and &#8222; / de to PP 1 (Figure 10(b)). Next, when combining NN 3 and the virtual nonterminal PP-&#8222; / de, we also include the English-side string responsible for the (Figure 10(c)). In order to do this rigorously we need to keep track of sub-alignments including both aligned nonterminals and incorporated terminals. A pre-order traversal of the fully decorated binarization tree gives us the following binary SCFG rules:
Analogous to the &#8220;dotted rules&#8221; in Earley parsing for monolingual CFGs, the names we create for the virtual nonterminals reflect the underlying sub-alignments, ensuring intermediate states can be shared across different string-to-tree rules without causing ambiguity. The whole binarization algorithm still runs in time linear in the number of symbols in the rule (including both terminals and nonterminals). We now turn to tree transducer rules. We view each left-hand side subtree as a monolithic nonterminal symbol and factor each transducer rule into two SCFG rules: one from the root nonterminal to the subtree, and the other from the subtree to the leaves. In this way we can uniquely reconstruct the transducer derivation using the two-step SCFG derivation. For example, consider the following tree transducer rule: We create a specific nonterminal, say, T 859 , which uniquely identifies the left-hand side subtree. This gives the following two SCFG rules:
The newly created nonterminals ensure that the newly created rules can only combine with one another to reconstruct the original rule, leaving the output of the transducer, as well as the probabilities it assigns to transductions, unchanged. The problem of binarizing tree transducers is now reduced to the binarization of SCFG rules, which we solved previously.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have reduced the problem of binarizing an SCFG rule into the problem of binarizing its permutation.</text>
              <doc_id>128</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The simplest algorithm for this problem is to try all bracketings of a permutation and pick one that corresponds to a binarization tree.</text>
              <doc_id>129</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The number of all possible bracketings of a sequence of length n is known to be the Catalan Number</text>
              <doc_id>130</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>which grows exponentially with n.</text>
              <doc_id>131</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>A better approach is to reduce this problem to an instance of synchronous ITG parsing (Wu 1997).</text>
              <doc_id>132</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Here the parallel string pair that we are parsing is the integer sequence (1...n) and its permutation (&#960;(1)...&#960;(n)).</text>
              <doc_id>133</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The goal of the ITG parsing is to find a synchronous tree that agrees with the alignment indicated by the permutation.</text>
              <doc_id>134</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Synchronous ITG parsing runs in time O(n 6 ) but can be improved to O(n 4 ) because there is no insertion or deletion in a permutation.</text>
              <doc_id>135</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Another problem besides efficiency is that there are possibly multiple binarization trees for many permutations whereas we just need one.</text>
              <doc_id>136</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We would prefer a consistent pattern of binarization trees across different permutations so that sub-binarizations (virtual nonterminals) can be shared.</text>
              <doc_id>137</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For example, permutations (1, 3, 2, 5, 4) and (1, 3, 2, 4) can share the common sub-binarization tree [1, &#12296;3, 2&#12297;].</text>
              <doc_id>138</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>To this end, we can borrow the non-ambiguous ITG of Wu (1997, Section 7) that prefers left-heavy binarization trees so that for each permutation there is a unique synchronous derivation.</text>
              <doc_id>139</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>3 We now refine the definition of binarization trees accordingly.</text>
              <doc_id>140</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Definition 6 A binarization tree bi(a) is said to be canonical if the split at each non-leaf node of the tree is the rightmost binarizable split.</text>
              <doc_id>141</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>For example, for sequence (1, 2, 4, 3), the binarization tree [[1, 2], &#12296;4, 3&#12297;] is canonical, whereas [1, [2, &#12296;4, 3&#12297;]] is not, because its top-level split is not at the rightmost binarizable split (1, 2; 4, 3).</text>
              <doc_id>142</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>By definition, there is a unique canonical binarization tree for each binarizable sequence.</text>
              <doc_id>143</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>We next present an algorithm that is both fast and consistent.</text>
              <doc_id>144</doc_id>
              <sec_id>8</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Shapiro and Stephens (1991, page 277) informally present an iterative procedure that, in each pass, scans the permuted sequence from left to right and combines two adjacent subsequences whenever possible.</text>
              <doc_id>145</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This procedure produces canonical binarization trees and runs in O(n 2 ) time because we need n passes in the worst case.</text>
              <doc_id>146</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Inspired by the Graham Scan Algorithm (Graham 1972) for computing convex hulls from computational geometry, we modify this procedure and improve it into a linear-time algorithm that only needs one pass through the sequence.</text>
              <doc_id>147</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The skeleton binarization algorithm is an instance of the widely used left-to-right shift-reduce algorithm.</text>
              <doc_id>148</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>It maintains a stack for contiguous subsequences discovered so far; for example: 2&#8211;5, 1.</text>
              <doc_id>149</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In each iteration, it shifts the next number from the input and repeatedly tries to reduce the top two elements on the stack if they are consecutive.</text>
              <doc_id>150</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>See Algorithm 1 for the pseudo-code and Figures 6 and 7 for example runs on binarizable and non-binarizable permutations, respectively.</text>
              <doc_id>151</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>We need the following lemma to prove the properties of the algorithm:</text>
              <doc_id>152</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We prove by induction on the length of a.</text>
              <doc_id>153</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Base case: |a |= 2, a (proper) subsequence of a, has length 1, so it is binarizable.</text>
              <doc_id>154</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For |a |&amp;gt; 2, because a has a binarization tree, there 3 We are not aiming at optimal sharing, that is, a strategy that produces the smallest binarized grammar for a given ruleset, which would require a global optimization problem over the whole set.</text>
              <doc_id>155</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In practice, we can only use online algorithms that binarize rules one by one.</text>
              <doc_id>156</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The left-heavy (or its symmetric variant, right-heavy) preference we choose here is one of the obvious candidates for consistency.</text>
              <doc_id>157</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>568 Huang et al. Binarization of Synchronous Context-Free Grammars Algorithm 1 The linear-time binarization algorithm.</text>
              <doc_id>158</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1: function SYNCHRONOUSBINARIZER(&#960;) 2: top &#8592; 0 &#8882; stack top pointer 3: PUSH(stack, (&#960;(1), &#960;(1))) &#8882; initial shift 4: for i &#8592; 2 to |&#960; |do &#8882; for each remaining element 5: PUSH(stack, (&#960;(i), &#960;(i))) &#8882; shift 6: while top &amp;gt; 1 and CONSECUTIVE(stack[top], stack[top &#8722; 1]) do 7: &#8882; keep reducing if possible 8: (p, q) &#8592; COMBINE(stack[top], stack[top &#8722; 1]) 9: top &#8592; top &#8722; 2 10: PUSH(stack, (p, q)) 11: return (top = 1) &#8882; returns true iff.</text>
              <doc_id>159</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>the input is reduced to a single element 12: 13: function CONSECUTIVE((a, b), (c, d)) 14: return (b = c &#8722; 1) or (d = a &#8722; 1) &#8882; either straight or inverted 15: function COMBINE((a, b), (c, d)) 16: A = {min(a, c)...max(b, d)} 17: B = {a...b} 18: C = {c...d} 19: rule[A] = A &#8594; B C 20: return (min(a, c), max(b, d))</text>
              <doc_id>160</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Figure 6 Example of Algorithm 1 on the binarizable input (1, 5, 3, 4, 2).</text>
              <doc_id>161</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The rightmost column shows the binarization-trees generated at each reduction step.</text>
              <doc_id>162</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>exists a (binarizable) split which is nearest to the root and splits c into two parts.</text>
              <doc_id>163</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Let the split be (b 1 , c 1 ; c 2 , b 2 ), where c = (c 1 ; c 2 ), and either b 1 or b 2 can be empty.</text>
              <doc_id>164</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>By Lemma 1, we have c 1 &amp;lt; c 2 or c 1 &amp;gt; c 2 .</text>
              <doc_id>165</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>By Lemma 1 again, we have that (c 1 ; c 2 ) is a proper split of c, i.e., both c 1 and c 2 are themselves permuted sequences.</text>
              <doc_id>166</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We also know both (b 1 , c 1 ) and (c 2 , b 2 ) are binarizable.</text>
              <doc_id>167</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>By the induction hypothesis, c 1 and c 2 are both binarizable.</text>
              <doc_id>168</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>So we conclude that c = (c 1 ; c 2 ) is binarizable (See figure 8).</text>
              <doc_id>169</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>&#65533; We now state the central result of this work.</text>
              <doc_id>170</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Theorem 2 Algorithm 1 runs in time linear to the length of the input, and succeeds (i.e., it reduces the input into one single element) if and only if the input permuted sequence a is binarizable, in which case the binarization tree recovered is canonical.</text>
              <doc_id>171</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Illustration of the proof of Lemma 2.</text>
              <doc_id>172</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The arrangement of (b 1 , c 1 ; c 2 , b 2 ) must be either all straight as in (a) or all inverted as in (b).</text>
              <doc_id>173</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Proof We prove the following three parts of this theorem:</text>
              <doc_id>174</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1.</text>
              <doc_id>175</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>If Algorithm 1 succeeds, then a is binarizable because we can recover a binarization tree from the algorithm.</text>
              <doc_id>176</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>2.</text>
              <doc_id>177</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>If a is binarizable, then Algorithm 1 must succeed and the binarization tree</text>
              <doc_id>178</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>recovered must be canonical: We prove by a complete induction on n, the length of a.</text>
              <doc_id>179</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Base case: n = 1, trivial.</text>
              <doc_id>180</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Assume it holds for all n &#8242; &amp;lt; n.</text>
              <doc_id>181</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>If a is binarizable, then let a = (b; c) be its rightmost binarizable split.</text>
              <doc_id>182</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>By definition, both b and c are binarizable.</text>
              <doc_id>183</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>By the induction hypothesis, the algorithm succeeds on the partial input b, reducing it to the single element stack[0] on the stack and recovering its canonical binarization tree bi(b).</text>
              <doc_id>184</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>If c is a singleton, the algorithm will combine it with the element stack[0] and succeed.</text>
              <doc_id>185</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The final binarization tree is canonical because the top-level split is at the rightmost binarizable split, and both subtrees are canonical.</text>
              <doc_id>186</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>If c is not a singleton, we want to show by contradiction that the algorithm will never combine b with a proper prefix of c.</text>
              <doc_id>187</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Because a = (b; c) is a proper split, we know that either b &amp;lt; c or c &amp;lt; b.</text>
              <doc_id>188</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Now if the algorithm makes a combination of (b; c 1 ) for some proper prefix c 1 where c = (c 1 ; c 2 ), we have either c 1 &amp;lt; c 2 or c 1 &amp;gt; c 2 .</text>
              <doc_id>189</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>By Lemma 1, (c 1 ; c 2 ) is a proper split.</text>
              <doc_id>190</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Because c is binarizable, by Lemma 2, c 2 is also binarizable.</text>
              <doc_id>191</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>So (b, c 1 ; c 2 ) is a binarizable split to the right of (b; c), which contradicts the assumption that the latter is the rightmost binarizable split (see Figure 9).</text>
              <doc_id>192</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>570 Huang et al. Binarization of Synchronous Context-Free Grammars Figure 9 Illustration of the proof of Theorem 2.</text>
              <doc_id>193</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>The combination of (b; c 1 ) (in dashed squares) contradicts the assumption that (b; c) is the rightmost binarizable split of a.</text>
              <doc_id>194</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>&#65533; Therefore, the algorithm will scan through the whole c as if from the empty stack.</text>
              <doc_id>195</doc_id>
              <sec_id>16</sec_id>
            </sentence>
            <sentence>
              <text>By the induction hypothesis again, it will reduce c into stack[1] on the stack and recover its canonical binarization tree bi(c).</text>
              <doc_id>196</doc_id>
              <sec_id>17</sec_id>
            </sentence>
            <sentence>
              <text>Because b and c are combinable, the algorithm reduces stack[0] and stack[1] in the last step, forming the canonical binarization tree for a, which is either [bi(b), bi(c)] or &#12296;bi(b), bi(c)&#12297;.</text>
              <doc_id>197</doc_id>
              <sec_id>18</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>By amortized analysis (Cormen, Leiserson, and Rivest 1990), there are exactly n shifts and at most n &#8722; 1 reductions, and each shift or reduction takes O(1) time.</text>
              <doc_id>198</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>So the total time complexity is O(n).</text>
              <doc_id>199</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Thus far we have discussed how to binarize synchronous productions involving only nonterminals through binarizing the corresponding skeleton permutations.</text>
              <doc_id>200</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We now turn to technical details for the implementation of a synchronous binarizer in real MT systems.</text>
              <doc_id>201</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We will first show how to deal with the terminal symbols, and then describe how to adapt it to tree transducers.</text>
              <doc_id>202</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Consider the following SCFG rule: (7) ADJP &#8594; RB 1 # / f&#249;z&#233; PP 2 &#8222; / de NN 3 , ADJP &#8594; RB 1 responsible for the NN 3 PP 2 whose permutation is (1, 3, 2).</text>
              <doc_id>203</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We run the skeleton binarization algorithm and get the (canonical) binarization tree [1, &#12296;3, 2&#12297;], which corresponds to [RB, &#12296;NN, PP&#12297;] (see Figure 10(a)).</text>
              <doc_id>204</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The alignment matrix is shown in Figure 11.</text>
              <doc_id>205</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We will then do a post-order traversal of the skeleton tree, and attach the terminals from both languages when appropriate.</text>
              <doc_id>206</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>It turns out we can do this quite freely as long as we can uniquely reconstruct the original rule from its binary parse tree.</text>
              <doc_id>207</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>We use the following rules for this step: 1.</text>
              <doc_id>208</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Attach source-language terminals to the leaf nodes of the skeleton tree.</text>
              <doc_id>209</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Consecutive terminals are attached to the nonterminal on their left</text>
              <doc_id>210</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Figure 10 Attaching terminals in SCFG binarization.</text>
              <doc_id>211</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>(a) The skeleton binarization tree, (b) attaching Chinese words at leaf nodes, (c) attaching English words at internal nodes.</text>
              <doc_id>212</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Figure 11 Alignment matrix of the SCFG rule (7).</text>
              <doc_id>213</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Areas shaded in gray and light gray denote virtual nonterminals (see rules in Example (8)).</text>
              <doc_id>214</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>(except for the initial ones which are attached to the nonterminal on their right).</text>
              <doc_id>215</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>2.</text>
              <doc_id>216</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Attach target-language terminals to the internal nodes (virtual nonterminals) of the skeleton tree.</text>
              <doc_id>217</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>These terminals are attached greedily: When combining two nonterminals, all target-side terminal strings neighboring either nonterminal will be included.</text>
              <doc_id>218</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>This greedy merging is motivated by the idea that the language model score helps to guide the decoder and should be computed as early as possible.</text>
              <doc_id>219</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>For example, at the leaf nodes, the Chinese word # / f&#249;z&#233; is attached to RB 1 , and &#8222; / de to PP 1 (Figure 10(b)).</text>
              <doc_id>220</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Next, when combining NN 3 and the virtual nonterminal PP-&#8222; / de, we also include the English-side string responsible for the (Figure 10(c)).</text>
              <doc_id>221</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>In order to do this rigorously we need to keep track of sub-alignments including both aligned nonterminals and incorporated terminals.</text>
              <doc_id>222</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>A pre-order traversal of the fully decorated binarization tree gives us the following binary SCFG rules:</text>
              <doc_id>223</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Analogous to the &#8220;dotted rules&#8221; in Earley parsing for monolingual CFGs, the names we create for the virtual nonterminals reflect the underlying sub-alignments, ensuring intermediate states can be shared across different string-to-tree rules without causing ambiguity.</text>
              <doc_id>224</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The whole binarization algorithm still runs in time linear in the number of symbols in the rule (including both terminals and nonterminals).</text>
              <doc_id>225</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We now turn to tree transducer rules.</text>
              <doc_id>226</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We view each left-hand side subtree as a monolithic nonterminal symbol and factor each transducer rule into two SCFG rules: one from the root nonterminal to the subtree, and the other from the subtree to the leaves.</text>
              <doc_id>227</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>In this way we can uniquely reconstruct the transducer derivation using the two-step SCFG derivation.</text>
              <doc_id>228</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>For example, consider the following tree transducer rule: We create a specific nonterminal, say, T 859 , which uniquely identifies the left-hand side subtree.</text>
              <doc_id>229</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>This gives the following two SCFG rules:</text>
              <doc_id>230</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The newly created nonterminals ensure that the newly created rules can only combine with one another to reconstruct the original rule, leaving the output of the transducer, as well as the probabilities it assigns to transductions, unchanged.</text>
              <doc_id>231</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The problem of binarizing tree transducers is now reduced to the binarization of SCFG rules, which we solved previously.</text>
              <doc_id>232</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>3</index>
        <title>5. Experiments</title>
        <text>In this section, we investigate two empirical questions with regard to synchronous binarization.
Shapiro and Stephens (1991) and Wu (1997, Section 4) show that the percentage of binarizable cases over all permutations of length n quickly approaches 0 as n grows
(see Figure 12). However, for machine translation, the percentage of synchronous rules that are binarizable is what we care about. We answer this question in both large-scale automatically aligned data and small-scale hand-aligned data. Automatically Aligned Data. Our rule set here is obtained by first doing word alignment using GIZA++ on a Chinese&#8211;English parallel corpus containing 50 million words in English, then parsing the English sentences using a variant of the Collins parser, and finally extracting rules using the graph-theoretic algorithm of Galley et al. (2004). We did a spectrum analysis on the resulting rule set with 50,879,242 rules. Figure 12 shows how the rules are distributed against their lengths (number of nonterminals). We can see that the percentage of non-binarizable rules in each bucket of the same length does not exceed 25%. Overall, 99.7% of the rules are binarizable. Even for the 0.3% of rules that are not binarizable, human evaluations show that the majority are due to alignment errors. Because the rule extraction process looks for rules that are consistent with both the automatic parses of the English sentences, and automatic word level alignments from GIZA++, errors in either parsing or word-level alignment can lead to noisy rules being input to the binarizer. It is also interesting to know that 86.8% of the rules have monotonic permutations, i.e., either taking identical or totally inverted order.
One might wonder whether automatic alignments computed by GIZA++ are systematically biased toward or against binarizability. If syntactic constraints not taken into account by GIZA++ enforce binarizability, automatic alignments could tend to contain spurious non-binarizable cases. On the other hand, simply by preferring monotonic alignments, GIZA++ might tend to miss complex non-binarizable patterns. To test this, we carried out experiments on hand-aligned sentence pairs with three language pairs: Chinese&#8211;English, French&#8211;English, and German&#8211;English. Chinese&#8211;English Data. For Chinese&#8211;English, we used the data of Liu, Liu, and Lin (2005) which contains 935 pairs of parallel sentences. Of the 13,713 rules extracted using the same method described herein, 0.3% (44) are non-binarizable, which is exactly the Figure 12 The solid-line curve represents the distribution of all rules against permutation lengths. The dashed-line stairs indicate the percentage of non-binarizable rules in our initial rule set, and the dotted line denotes that percentage among all permutations. 574 Huang et al. Binarization of Synchronous Context-Free Grammars same ratio as the GIZA-aligned data. The following is an interesting example of nonbinarizable rules: where ... in with ... Mishira is the long phrase in shadow modifying Mishira. Here the non-binarizable permutation is (3, 2, 5, 1, 4), which is reducible to (2, 4, 1, 3). The SCFG version of the tree-transducer rule is as follows: where we indicate dependency links in solid arcs and permutation in dashed lines. It is interesting to examine dependency structures, as some authors have argued that they are more likely to generalize across languages than phrase structures. The Chinese ADVP 1 S) / d&#257;ngti&#257;n (lit., that day) is translated into an English PP 1 (on the same day), but the dependency structures on both sides are isomorphic (i.e., this is an extremely literal translation). A simpler but slightly non-literal example is the following:
where the SCFG version of the tree-transducer rule (in the same format as the previous example) is: Note that the Chinese ADVP 1 &#219; e / j&#236;ny&#299;b&#249; modifying the verb VB 3 becomes a JJ 1 ( further) in the English translation modifying the object of the verb, NNS 4 , and this change also happens to PP 2 . This is an example of syntactic divergence, where the dependency structures are not isomorphic between the two languages (Eisner 2003). Wu (1997, page 158) has &#8220;been unable to find real examples&#8221; of non-binarizable cases, at least in &#8220;fixed-word-order languages that are lightly inflected, such as English
and Chinese.&#8221; Our empirical results not only confirm that this is largely correct (99.7% in our data sets), but also provide, for the first time, &#8220;real examples&#8221; between English and Chinese, verified by native speakers. It is interesting to note that our non-binarizable examples include both cases of isomorphic and non-isomorphic dependency structures, indicating that it is difficult to find any general linguistic explanation that covers all such examples. Wellington, Waxmonsky, and Melamed (2006) used a different measure of non-binarizability, which is on the sentence-level permutations, as opposed to rulelevel permutation as in our case, and reported 5% non-binarizable cases for a different hand-aligned English&#8211;Chinese data set, but they did not provide real examples. French&#8211;English Data. We analyzed 447 hand-aligned French&#8211;English sentences from the NAACL 2003 alignment workshop (Mihalcea and Pederson 2003). We found only 2 out of 2,659 rules to be non-binarizable, or 0.1%. One of these two is an instance of topicalization: The second instance is due to movement of an adverbial: German&#8211;English Data. We analyzed 220 sentences from the Europarl corpus, handaligned by a native German speaker (Callison-Burch, personal communication). Of 2,328 rules extracted, 13 were non-binarizable, or 0.6%. Some cases are due to separable German verb prefixes: Here the German prefix auf is separated from the verb auffordern (request). Another cause of non-binarizability is verb-final word order in German in embedded clauses: Although fewer than 1% of the rules were non-binarizable in each language pair we studied, German&#8211;English had the highest percentage with 0.6%. The fact that the German&#8211;English examples are due to syntactic phenomena such as separable prefixes and verb-final word order may indicate that an MT system would have less freedom to choose an equivalent binarizable reordering than in the case of the examples due to adverbial placement, heavy NP shift, and topicalization that we see in the Chinese&#8211; 576 Huang et al. Binarization of Synchronous Context-Free Grammars English and French&#8211;English data. The results on binarizability of hand-aligned data for the three language pairs are summarized in Table 1. It is worth noting that for most of these non-binarizable examples, there do exist alternative translations that only involve binarizable permutations. For example, in Chinese&#8211;English Example (9), we can move the English PP on the same day to the first position (before will), which results in a binarizable permutation (1, 3, 2, 5, 4). Similarly, we can avoid non-binarizability in French&#8211;English Example (12) by moving the English adverbial still under private ownership to the third position. German&#8211;English Example (13) would also become binarizable by replacing call on with a single word request on the English side. However, the point of this experiment is to test the ITG hypothesis by attempting to explain existing real data (the hand-aligned parallel text), rather than to generate fresh translations for a given source sentence, which is the topic of the subsequent decoding experiment. This subsection not only provides the first solid confirmation of the existence of linguistically-motivated non-binarizable reorderings, but also motivates further theoretical studies on parsing and decoding with these nonbinarizable synchronous grammars, which is the topic of Sections 6 and 7.
We did experiments on our CKY-based decoder with two binarization methods. It is the responsibility of the binarizer to instruct the decoder how to compute the language model scores from children nonterminals in each rule. The baseline method is monolingual left-to-right binarization. As shown in Section 2, decoding complexity with this method is exponential in the size of the longest rule, and because we postpone all the language model scorings, pruning in this case is also biased. To move on to synchronous binarization, we first did an experiment using this baseline system without the 0.3% of rules that are non-binarizable and did not observe any difference in BLEU scores. This indicates that we can safely focus on the binarizable rules, discarding the rest. The decoder now works on the binary translation rules supplied by an external synchronous binarizer. As shown in Section 1, this results in a simplified decoder with a polynomial time complexity, allowing less aggressive and more effective pruning based on both translation model and language model scores. We compare the two binarization schemes in terms of translation quality with various pruning thresholds. The rule set is that of the previous section. The test set has 116 Chinese sentences of no longer than 15 words, taken from the NIST 2002 test set. Both systems use trigram as the integrated language model. Figure 13 demonstrates that decoding accuracy is significantly improved after synchronous binarization. The number of edges (or items, in the deductive parsing terminology) proposed during
decoding is used as a measure of the size of search space, or time efficiency. Our system is consistently faster and more accurate than the baseline system. We also compare the top result of our synchronous binarization system with the state-of-the-art alignment-template system (ATS) (Och and Ney 2004). The results are shown in Table 2. Our system has a promising improvement over the ATS system, which is trained on a larger data set but tuned independently. A larger-scale system based on our best result performs very well in the 2006 NIST MT Evaluation (ISI Machine Translation Team 2006), achieving the best overall BLEU scores in the Chineseto-English track among all participants. 4 The readers are referred to Galley et al. (2004) for details of the decoder and the overall system.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this section, we investigate two empirical questions with regard to synchronous binarization.</text>
              <doc_id>233</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Shapiro and Stephens (1991) and Wu (1997, Section 4) show that the percentage of binarizable cases over all permutations of length n quickly approaches 0 as n grows</text>
              <doc_id>234</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(see Figure 12).</text>
              <doc_id>235</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>However, for machine translation, the percentage of synchronous rules that are binarizable is what we care about.</text>
              <doc_id>236</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We answer this question in both large-scale automatically aligned data and small-scale hand-aligned data.</text>
              <doc_id>237</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Automatically Aligned Data.</text>
              <doc_id>238</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Our rule set here is obtained by first doing word alignment using GIZA++ on a Chinese&#8211;English parallel corpus containing 50 million words in English, then parsing the English sentences using a variant of the Collins parser, and finally extracting rules using the graph-theoretic algorithm of Galley et al. (2004).</text>
              <doc_id>239</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We did a spectrum analysis on the resulting rule set with 50,879,242 rules.</text>
              <doc_id>240</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Figure 12 shows how the rules are distributed against their lengths (number of nonterminals).</text>
              <doc_id>241</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>We can see that the percentage of non-binarizable rules in each bucket of the same length does not exceed 25%.</text>
              <doc_id>242</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Overall, 99.7% of the rules are binarizable.</text>
              <doc_id>243</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Even for the 0.3% of rules that are not binarizable, human evaluations show that the majority are due to alignment errors.</text>
              <doc_id>244</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Because the rule extraction process looks for rules that are consistent with both the automatic parses of the English sentences, and automatic word level alignments from GIZA++, errors in either parsing or word-level alignment can lead to noisy rules being input to the binarizer.</text>
              <doc_id>245</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>It is also interesting to know that 86.8% of the rules have monotonic permutations, i.e., either taking identical or totally inverted order.</text>
              <doc_id>246</doc_id>
              <sec_id>11</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>One might wonder whether automatic alignments computed by GIZA++ are systematically biased toward or against binarizability.</text>
              <doc_id>247</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>If syntactic constraints not taken into account by GIZA++ enforce binarizability, automatic alignments could tend to contain spurious non-binarizable cases.</text>
              <doc_id>248</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>On the other hand, simply by preferring monotonic alignments, GIZA++ might tend to miss complex non-binarizable patterns.</text>
              <doc_id>249</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>To test this, we carried out experiments on hand-aligned sentence pairs with three language pairs: Chinese&#8211;English, French&#8211;English, and German&#8211;English.</text>
              <doc_id>250</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Chinese&#8211;English Data.</text>
              <doc_id>251</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>For Chinese&#8211;English, we used the data of Liu, Liu, and Lin (2005) which contains 935 pairs of parallel sentences.</text>
              <doc_id>252</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Of the 13,713 rules extracted using the same method described herein, 0.3% (44) are non-binarizable, which is exactly the Figure 12 The solid-line curve represents the distribution of all rules against permutation lengths.</text>
              <doc_id>253</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>The dashed-line stairs indicate the percentage of non-binarizable rules in our initial rule set, and the dotted line denotes that percentage among all permutations.</text>
              <doc_id>254</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>574 Huang et al. Binarization of Synchronous Context-Free Grammars same ratio as the GIZA-aligned data.</text>
              <doc_id>255</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>The following is an interesting example of nonbinarizable rules: where ... in with ... Mishira is the long phrase in shadow modifying Mishira.</text>
              <doc_id>256</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Here the non-binarizable permutation is (3, 2, 5, 1, 4), which is reducible to (2, 4, 1, 3).</text>
              <doc_id>257</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>The SCFG version of the tree-transducer rule is as follows: where we indicate dependency links in solid arcs and permutation in dashed lines.</text>
              <doc_id>258</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>It is interesting to examine dependency structures, as some authors have argued that they are more likely to generalize across languages than phrase structures.</text>
              <doc_id>259</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>The Chinese ADVP 1 S) / d&#257;ngti&#257;n (lit., that day) is translated into an English PP 1 (on the same day), but the dependency structures on both sides are isomorphic (i.e., this is an extremely literal translation).</text>
              <doc_id>260</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>A simpler but slightly non-literal example is the following:</text>
              <doc_id>261</doc_id>
              <sec_id>14</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where the SCFG version of the tree-transducer rule (in the same format as the previous example) is: Note that the Chinese ADVP 1 &#219; e / j&#236;ny&#299;b&#249; modifying the verb VB 3 becomes a JJ 1 ( further) in the English translation modifying the object of the verb, NNS 4 , and this change also happens to PP 2 .</text>
              <doc_id>262</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This is an example of syntactic divergence, where the dependency structures are not isomorphic between the two languages (Eisner 2003).</text>
              <doc_id>263</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Wu (1997, page 158) has &#8220;been unable to find real examples&#8221; of non-binarizable cases, at least in &#8220;fixed-word-order languages that are lightly inflected, such as English</text>
              <doc_id>264</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and Chinese.&#8221; Our empirical results not only confirm that this is largely correct (99.7% in our data sets), but also provide, for the first time, &#8220;real examples&#8221; between English and Chinese, verified by native speakers.</text>
              <doc_id>265</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It is interesting to note that our non-binarizable examples include both cases of isomorphic and non-isomorphic dependency structures, indicating that it is difficult to find any general linguistic explanation that covers all such examples.</text>
              <doc_id>266</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Wellington, Waxmonsky, and Melamed (2006) used a different measure of non-binarizability, which is on the sentence-level permutations, as opposed to rulelevel permutation as in our case, and reported 5% non-binarizable cases for a different hand-aligned English&#8211;Chinese data set, but they did not provide real examples.</text>
              <doc_id>267</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>French&#8211;English Data.</text>
              <doc_id>268</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We analyzed 447 hand-aligned French&#8211;English sentences from the NAACL 2003 alignment workshop (Mihalcea and Pederson 2003).</text>
              <doc_id>269</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We found only 2 out of 2,659 rules to be non-binarizable, or 0.1%.</text>
              <doc_id>270</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>One of these two is an instance of topicalization: The second instance is due to movement of an adverbial: German&#8211;English Data.</text>
              <doc_id>271</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>We analyzed 220 sentences from the Europarl corpus, handaligned by a native German speaker (Callison-Burch, personal communication).</text>
              <doc_id>272</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Of 2,328 rules extracted, 13 were non-binarizable, or 0.6%.</text>
              <doc_id>273</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Some cases are due to separable German verb prefixes: Here the German prefix auf is separated from the verb auffordern (request).</text>
              <doc_id>274</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Another cause of non-binarizability is verb-final word order in German in embedded clauses: Although fewer than 1% of the rules were non-binarizable in each language pair we studied, German&#8211;English had the highest percentage with 0.6%.</text>
              <doc_id>275</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>The fact that the German&#8211;English examples are due to syntactic phenomena such as separable prefixes and verb-final word order may indicate that an MT system would have less freedom to choose an equivalent binarizable reordering than in the case of the examples due to adverbial placement, heavy NP shift, and topicalization that we see in the Chinese&#8211; 576 Huang et al. Binarization of Synchronous Context-Free Grammars English and French&#8211;English data.</text>
              <doc_id>276</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>The results on binarizability of hand-aligned data for the three language pairs are summarized in Table 1.</text>
              <doc_id>277</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>It is worth noting that for most of these non-binarizable examples, there do exist alternative translations that only involve binarizable permutations.</text>
              <doc_id>278</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>For example, in Chinese&#8211;English Example (9), we can move the English PP on the same day to the first position (before will), which results in a binarizable permutation (1, 3, 2, 5, 4).</text>
              <doc_id>279</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>Similarly, we can avoid non-binarizability in French&#8211;English Example (12) by moving the English adverbial still under private ownership to the third position.</text>
              <doc_id>280</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>German&#8211;English Example (13) would also become binarizable by replacing call on with a single word request on the English side.</text>
              <doc_id>281</doc_id>
              <sec_id>16</sec_id>
            </sentence>
            <sentence>
              <text>However, the point of this experiment is to test the ITG hypothesis by attempting to explain existing real data (the hand-aligned parallel text), rather than to generate fresh translations for a given source sentence, which is the topic of the subsequent decoding experiment.</text>
              <doc_id>282</doc_id>
              <sec_id>17</sec_id>
            </sentence>
            <sentence>
              <text>This subsection not only provides the first solid confirmation of the existence of linguistically-motivated non-binarizable reorderings, but also motivates further theoretical studies on parsing and decoding with these nonbinarizable synchronous grammars, which is the topic of Sections 6 and 7.</text>
              <doc_id>283</doc_id>
              <sec_id>18</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>We did experiments on our CKY-based decoder with two binarization methods.</text>
              <doc_id>284</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It is the responsibility of the binarizer to instruct the decoder how to compute the language model scores from children nonterminals in each rule.</text>
              <doc_id>285</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The baseline method is monolingual left-to-right binarization.</text>
              <doc_id>286</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As shown in Section 2, decoding complexity with this method is exponential in the size of the longest rule, and because we postpone all the language model scorings, pruning in this case is also biased.</text>
              <doc_id>287</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>To move on to synchronous binarization, we first did an experiment using this baseline system without the 0.3% of rules that are non-binarizable and did not observe any difference in BLEU scores.</text>
              <doc_id>288</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This indicates that we can safely focus on the binarizable rules, discarding the rest.</text>
              <doc_id>289</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The decoder now works on the binary translation rules supplied by an external synchronous binarizer.</text>
              <doc_id>290</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>As shown in Section 1, this results in a simplified decoder with a polynomial time complexity, allowing less aggressive and more effective pruning based on both translation model and language model scores.</text>
              <doc_id>291</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>We compare the two binarization schemes in terms of translation quality with various pruning thresholds.</text>
              <doc_id>292</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>The rule set is that of the previous section.</text>
              <doc_id>293</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>The test set has 116 Chinese sentences of no longer than 15 words, taken from the NIST 2002 test set.</text>
              <doc_id>294</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>Both systems use trigram as the integrated language model.</text>
              <doc_id>295</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Figure 13 demonstrates that decoding accuracy is significantly improved after synchronous binarization.</text>
              <doc_id>296</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>The number of edges (or items, in the deductive parsing terminology) proposed during</text>
              <doc_id>297</doc_id>
              <sec_id>13</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>decoding is used as a measure of the size of search space, or time efficiency.</text>
              <doc_id>298</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our system is consistently faster and more accurate than the baseline system.</text>
              <doc_id>299</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We also compare the top result of our synchronous binarization system with the state-of-the-art alignment-template system (ATS) (Och and Ney 2004).</text>
              <doc_id>300</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The results are shown in Table 2.</text>
              <doc_id>301</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Our system has a promising improvement over the ATS system, which is trained on a larger data set but tuned independently.</text>
              <doc_id>302</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>A larger-scale system based on our best result performs very well in the 2006 NIST MT Evaluation (ISI Machine Translation Team 2006), achieving the best overall BLEU scores in the Chineseto-English track among all participants.</text>
              <doc_id>303</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>4 The readers are referred to Galley et al. (2004) for details of the decoder and the overall system.</text>
              <doc_id>304</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>6. One-Sided Binarization</title>
        <text>In this section and the following section, we discuss techniques for handling rules that are not binarizable. This is primarily of theoretical interest, as we found that they constitute a small fraction of all rules, and removing these did not affect our Chinese-to- English translation results. However, non-binarizable rules are shown to be important in explaining existing hand-aligned data, especially for other language pairs such as German&#8211;English (see Section 5.2, as well as Wellington, Waxmonsky, and Melamed [2006]). Non-binarizable rules may also become more important as machine translation
A summary of the four factorization algorithms, and the &#8220;incremental relaxation&#8221; theme of the whole paper. Algorithms 2&#8211;4 are for non-binarizable SCFGs, and are mainly of theoretical interest. Algorithms 1&#8211;3 make fewer and fewer assumptions on the strategy space, and produce parsing strategies closer and closer to the optimal. Algorithm 4 further improves Algorithm 3.
systems improve. Synchronous grammars that go beyond the power of SCFG (and therefore binary SCFG) have been defined by Shieber and Schabes (1990) and Rambow and Satta (1999), and motivated for machine translation by Melamed (2003), although previous work has not given algorithms for finding efficient and optimal parsing strategies for general SCFGs, which we believe is an important problem. In the remainder of this section and the next section, we will present a series of algorithms that produce increasingly faster parsing strategies, by gradually relaxing the strong &#8220;continuity&#8221; constraint made by the synchronous binarization technique. As that technique requires continuity on both languages, we will first study a relaxation where binarized rules are always continuous in one of the two languages, but may be discontinuous in the other. We will present a CKY-style algorithm (Section 6.2) for finding the best parsing strategy under this new constraint, which we call one-sided binarization. In practice, this factorization has the advantage that we need to maintain only one set of language model boundary words for each partial hypothesis. We will see, however, that it is not always possible to achieve the best asymptotic complexity within this constraint. But most importantly, as the synchronous binarization algorithm covers most of the SCFG rules in real data, the one-sided binarization we discuss in this section is able to achieve optimal parsing complexity for most of the non-binarizable rules in real data. So this section can be viewed as a middle step between the synchronous binarization we focus on in the previous sections and the optimal factorization coming in Section 7, and also a trade-off point between simplicity and asymptotic complexity for parsing strategies of SCFGs. Table 3 summarizes this incremental structure of the whole paper.
The complexity for decoding given a grammar factorization can be expressed in terms of the number of spans of the items being combined at each step. As an example, Figure 14 shows the three combination steps for one factorization of the non-binarizable rule:
the rule&#8217;s child nonterminals. Each step combines two dynamic programming items covering disjoint spans of the Chinese input, and creates a new item covering the union of the spans. For example, in the first combination step shown in Figure 14, where nonterminals A and B are combined, A has one span in Chinese, from position y 1 to y 2 in the string, and B has one span from y 3 to y 4 . The chart entry for the nonterminal
The tree at the top of the figure defines a three-step decoding strategy for rule (15), building an English output string on the horizontal axis as we process the Chinese input on the vertical axis. In each step, the two subsets of nonterminals in the inner marked spans are combined into a new chart item with the outer spans. The intersection of the outer spans, shaded, has now been processed. pair {A, B} must record a total of four string indices: positions y 1 , y 2 , y 3 , and y 4 in the Chinese string. Any combination of two subsets of the rule&#8217;s nonterminals involves the indices for the spans of each subset. However, some of the indices are tied together: If we are joining two spans into one span in the new item, one of the original spans&#8217; end-points must be equal to another span&#8217;s beginning point. For example, the index y 2 is the end-point of A in Chinese, as well as the beginning position of D. In general, if we are combining a subset B of nonterminals having b spans with a subset C having c spans, to produce a spans for a combined subset A = B &#8746; C, the number of linked indices is b + c &#8722; a. In the example of the first step of Figure 14, subset {A} has two spans (one in each language) so b = 1, and {B} also has two spans, so c = 1. The combined subset {A, B} has two spans, so a = 2. The total number of indices involved in a combination of two subsets is
number of shared indices. In the first step of Figure 14, a + b + c = 1 + 1 + 2 = 4 total indices, and therefore the complexity of this step is O(|w |4 ) where |w |is the length of the input Chinese strings, and we ignore the language model for the moment. Applying this formula to the second and third step, we see that the second is O(|w |5 ), and the third is again O(|w |4 ). In order to find a good decoding strategy for a given grammar rule, we need to search over possible orders in which partial translation hypotheses can be built by successively combining nonterminals. Any strategy we find can be used for synchronous parsing as well as decoding. For example, the strategy shown in Figure 14 can be used to parse an input Chinese/English string pair. The complexity of each step is determined by the total number of indices into both the Chinese and English strings. Each step in
1: function BESTCONTINUOUSPARSER(&#960;) 2: n = |&#960;| 3: for span &#8592; 1 to n &#8722; 1 do 4: for i &#8592; 1 to n &#8722; span do 5: A = {i...i + span} 6: best[A] = &#8734; 7: for j &#8592; i + 1 to i + span do 8: B = {i...j &#8722; 1} 9: C = {j...i + span} 10: Let a &#960; , b &#960; , and c &#960; denote the number of &#960;(A), &#960;(B), and &#960;(C)&#8217;s spans 11: compl[A &#8594; B C] = max {a &#960; + b &#960; + c &#960; , best[B], best[C]} 12: if compl[A &#8594; B C] &amp;lt; best[A] then 13: best[A] = compl[A &#8594; B C] 14: rule[A] = A &#8594; B C 15: return best[{1...n}] the diagram has three indices into the English string, so the complexity of the first step is O(|w |4+3 ) = O(|w |7 ), the second step is O(|w |8 ), and the third is again O(|w |7 ).
The O(n 3 ) algorithm we present in this section can find good factorizations for most non-binarizable rules; we discuss optimal factorization in the next section. This algorithm, shown in Algorithm 2, considers only factorizations that have only one span in one of the two languages, and efficiently searches over all such factorizations by combining adjacent spans with CKY-style parsing. 5 The input is an SCFG grammar rule in its abstract form, which is a permutation, and best is a dynamic programming table used to store the lowest complexity with which we can parse a given subset of the input rule&#8217;s child nonterminals. Although this CKY-style algorithm finds the best grammar factorization maintaining continuous spans in one of the two dimensions, in general the best factorization may require discontinuous spans in both dimensions. As an example, the following pattern causes problems for the algorithm regardless of which of the dimensions it parses across:
Left, a general pattern of non-binarizable permutations. Center, a partially completed chart item with two spans in each dimension; the intersection of the completed spans is shaded. Right, the combination of the item from the center panel with a singleton item. The two subsets of nonterminals in the inner marked spans are combined into a new chart item with the outer spans. beginning at position 1 and one beginning at position n 2 + 1, and adding one nonterminal at a time to the partially completed item, as shown in Figure 15 (right). However, our CKY factorization algorithm will give a factorization with n/2 discontinuous spans in one dimension. Thus in the worst case, the number of spans found by the cubic-time algorithm grows with n, even when a constant number of spans is possible, implying that there is no approximation ratio on how close the algorithm will get to the optimal solution.
The method presented in the previous section is not optimal for all permutations, because in some cases it is better to maintain multiple spans in the output language (despite the extra language model state that is needed) in order to maintain continuous spans in the input language. In this section we give a method for finding decoding strategies that are guaranteed to be optimal in their asymptotic complexity. This method can also be used to find the optimal strategy for synchronous parsing (alignment) using complex rules. This answers a question left open by earlier work in synchronous grammars: Although Satta and Peserico (2005) show that tabular parsing of a worst-case SCFG can be NP-hard, they do not give a procedure for finding the complexity of an arbitrary input grammar. Similarly, Melamed (2003) defines the cardinality of a grammar, and discusses the interaction of this property with parsing complexity, but does not show how to find a normal form for a grammar with the lowest possible cardinality. We show below how to analyze parsing and decoding strategies for a given SCFG rule in Section 7.1, and then present an exponential-time dynamic programming algorithm for finding the best strategy in Section 7.2. We prove that factorizing an SCFG rule into smaller SCFG rules is a safe preprocessing step for finding the best strategy in Section 7.4, which leads to much faster computation in many cases. First, however, we take a brief detour to discuss modifying our a + b + c formula from the previous section in order to take the state from an m-gram language model into account during
First, how do we analyze algorithms that create discontinuous spans in both the source and target language? It turns out that the analysis in Section 6.1 for counting string indices in terms of spans in fact applies in the same way to both of our languages. For synchronous parsing, if we are combining item B with b e target spans and b f source spans with item C having c e target spans and c f source spans to form new item A having a e target spans and a f source spans, the complexity of the operation is O(|w |a e+b e +c e +a f +b f +c f ). The a + b + c formula can also be generalized for decoding with an integrated m-gram language model. At first glance, because we need to maintain (m &#8722; 1) boundary words at both the left and right edges of each target span, the total number of interacting variables is: 2(m &#8722; 1)(b e + c e ) + a f + b f + c f However, by using the &#8220;hook trick&#8221; suggested by Huang, Zhang, and Gildea (2005), we can optimize the decoding algorithm by factorizing the dynamic programming combination rule into two steps. One step incorporates the language model probability, and the other step combines adjacent spans in the input language and incorporates the SCFG rule probability. The hook trick for a bigram language model and binary SCFG is shown in Figure 16. In the equations of Figure 16, i,j,k range over 1 to |w|, the length of the input foreign sentence, and u,v,v 1 ,u 2 (or u,v,v 2 ,u 1 ) range over possible English words, which we assume to take O(|w|) possible values. There are seven free variables related to input size for doing the maximization computation, hence the algorithmic complexity is O(|w |7 ). The two terms in Figure 16 (top) within the first level of the max operator correspond to straight and inverted ITG rules. Figure 16 (bottom) shows how to decompose the first term; the same method applies to the second term. Counting the free variables enclosed in the innermost max operator, we get five: i, k, u, v 1 , and u 2 . The where
The hook trick for machine translation decoding with a binary SCFG (equivalent to Inversion Transduction Grammar). The fundamental dynamic programming equation is shown at the top, with an efficient factorization shown below.
decomposition eliminates one free variable, v 1 . In the outermost level, there are six free variables left. The maximum number of interacting variables is six overall. So, we have reduced the complexity of ITG decoding using the bigram language model from O(|w |7 ) to O(|w |6 ). When we generalize the hook trick for any m-gram language model and more complex SCFGs, each left boundary for a substring of an output language hypothesis contains m &#8722; 1 words of language model state, and each right boundary contains a &#8220;hook&#8221; specifying what the next m &#8722; 1 words must be. This yields a complexity analysis similar to that for synchronous parsing, based on the total number of boundaries, but now multiplied by a factor of m &#8722; 1: for translation from source to target. (m &#8722; 1)(a e + b e + c e ) + a f + b f + c f (17)
The number of m-gram weighted spans of a constituent, denoted a m , is defined as the number of source spans plus the number target spans weighted by the language model factor (m &#8722; 1): a m = a f + (m &#8722; 1)a e (18) Using this notation, we can rewrite the expression for the complexity of decoding in Equation 17 as a simple sum of the numbers of weighted spans of constituent subsets A, B, and C: a m + b m + c m (19) and more generally when k &#8805; 2 constituents are combined together:
It can be seen that, as m grows, the parsing/decoding strategies that favor contiguity on the output side will prevail. This effect is demonstrated by the experimental results in Section 7.5. This analysis applies to one combination of two subsets of a rule&#8217;s children during parsing or decoding. A strategy for parsing (or decoding) the entire rule must build up the complete set of the rule&#8217;s children through a sequence of such combinations. Thus a parsing strategy corresponds to a recursive partitioning of the rule&#8217;s children, that is, an unordered rooted tree having the child nonterminals as leaves. Each node in the partition tree represents a subset of nonterminals used as a partial result in the chart for parsing, built by combining the subsets corresponding to the node&#8217;s children. This combination step at each node has complexity determined by the number of spans, and the overall complexity of a parsing strategy is the complexity of the strategy&#8217;s worst combination step. We wish to find the recursive partition with the lowest overall complexity. Unfortunately, the number of recursive partitions of n items grows super- i=1 584 Huang et al. Binarization of Synchronous Context-Free Grammars exponentially, as 0.175n!n &#8722;3/2 2.59 n = &#920;(&#915;(n + 1)2.59 n ) (Schr&#246;der 1870, Problem IV). 6 More formally, the optimization over the space of all recursive partitions is expressed as:
where compl(A &#8594; B 1 ...B k ) is given by Equation (20). This recursive equation implies that we can solve the optimization problem using dynamic programming techniques.
In this section we show that a branching factor of more than two is not necessary in our recursive partitions, by showing that any ternary combination can be factored into two binary combinations with no increase in complexity. This fact leads to a more efficient, but still exponential, algorithm for finding the best parsing strategy for a given SCFG rule. Theorem 3 For any SCFG rule, if there exists a recursive partition of child nonterminals which enables tabular parsing of an input sentence w in time O(|w |k ), then there exists a recursive binary partition whose corresponding parser is also O(|w |k ). Proof We use the notion of number of weighted spans (Equation (20)) to concisely analyze the complexity of synchronous parsing/decoding. For any fixed m, we count a constituent&#8217;s number of spans using the weighted span value from Equation (18), and we drop both the adjective &#8220;weighted&#8221; and the m subscript from this point forward. If we are combining k subsets B i (i = 1, ..., k, k &#8805; 2) together to produce a new subset A = &#8899; i B i, the generalized formula for counting the total number of indices is
where b i is the number of spans for B i and a is the number of spans for the resulting item A. Consider a ternary rule X &#8594; A B C where X has x spans, A has a spans, B has b spans, and C has c spans. In the example shown in Figure 17, x = 2, a = 2, b = 1, and c =
and refer to the number of spans in partial constituent Y as y. Parsing Y &#8594; A B takes time O(|w |y+a+b ), so we need to show that y + a + b &#8804; a + b + c + x to show that we can parse this new rule in no more time than the original ternary rule. Subtracting a + b from both sides, we need to prove that y &#8804; c + x Each of the y spans in Y corresponds to a left edge. (In the case of decoding, each edge has a multiplicity of (m &#8722; 1) on the output language side.) The left edge in each span of Y corresponds to the left edge of a span in X or to the right edge of a span in C. Therefore, Y has at most one span for each span in C &#8746; X , so y &#8804; c + x. Returning to the first rule in our factorization, the time to parse X &#8594; Y C is O(|w |x+y+c ). We know that y &#8804; a + b since Y was formed from A and B. Therefore x + y + c &#8804; x + (a + b) + c so parsing X &#8594; Y C also takes no more time than the original rule X &#8594; A B C. By induction over the number of subsets, a rule having any number of subsets on the righthand side can be converted into a series of binary rules. &#65533; Our finding that combining no more than two subsets of children at a time is optimal implies that we need consider only binary recursive partitions, which correspond to unordered binary rooted trees having the SCFG rule&#8217;s child nonterminals as leaves. The (2n&#8722;3)! total number of binary recursive partitions of n nodes is 2 n&#8722;2 (n&#8722;2)! = &#920;(&#915;(n &#8722; 2 1 )2n&#8722;1 ) (Schr&#246;der 1870, Problem III). Note that this number grows much faster than the Catalan Number, which characterizes the number of bracketings representing the search space of synchronous binarization (Section 4). Although the total number of binary recursive partitions is still superexponential, the binary branching property also enables a straightforward dynamic programming algorithm, shown in Algorithm 3. The same algorithm can be used to find optimal strategies for synchronous parsing or for m-gram decoding: for parsing, the variables a, b, and c in Line 9 refer to the total number of spans of A, B, and C (Equation (16)), Figure 17 Left, example spans for a ternary rule decomposition X &#8594; A B C. Each symbol represents a subset of nonterminals from the original SCFG rule, and the subsets may cover discontinuous spans in either language. Line segments represent the projection of each set of child nonterminals into a single language, as in Figure 15. Right, factorization into X &#8594; Y C and Y &#8594; A B.
2: n = |&#960;| 3: for i &#8592; 2 to n do 4: for A &#8834; {1...n} s.t. |A |= i do 5: best[A] &#8592; &#8734; 6: for B, C s.t. A = B &#8746; C &#8743; B &#8745; C = &#8709; do 7: Let a, b, and c denote the number of 8: (A, &#960;(A)), (B, &#960;(B)), and (C, &#960;(C))&#8217;s spans 9: compl[A &#8594; B C] = max {a + b + c, best[B], best[C]} 10: if compl[A &#8594; B C] &amp;lt; best[A] then 11: best[A] &#8592; compl[A &#8594; B C] 12: rule[A] &#8592; A &#8594; B C 13: return best[{1...n}]
while for decoding, a, b, and c refer to weighted spans (Equation (19)). The dynamic programming states correspond to subsets of the input rule&#8217;s children, for which an optimal strategy has already been computed. In each iteration of the algorithm&#8217;s inner loop, each of the child nonterminals is identified as belonging to B, C, or neither B nor C, making the total running time of the algorithm O(3 n ). Although this is exponential in n, it is a significant improvement over considering all recursive partitions. The algorithm can be improved by adopting a best-first exploration strategy (Knuth 1977), in which dynamic programming items are placed on a priority queue sorted according to their complexity, and only used to build further items after all items of lower complexity have been exhausted. This technique, shown in Algorithm 4, guarantees polynomial-time processing on input permutations of bounded complexity. To see why this is, observe that each rule of the form A &#8594; B C that has complexity no greater than k can be written using a string of k e &amp;lt; k indices into the target nonterminal string to represent the spans&#8217; boundaries. For each index we must specify whether the corresponding nonterminal either starts a span of subset B, starts a span of subset C, or ends a span of B &#8746; C. Therefore there are O((3n) k ) rules of complexity no greater than k. If there exists a parsing strategy for the entire rule with complexity k, the best-first algorithm will find it after, in the worst case, popping all O((3n) k ) rules of complexity less than or equal to k off of the heap in the outer loop, and combining each one with all other O((3n) k ) such rules in the inner loop, for a total running time of O(9 k n 2k ). Although the algorithm is still exponential in the rule length n in the worst case (when k is linearly correlated to n), the best-first behavior makes it much more practical for our empirically observed rules.
One might wonder whether it is necessary to consider all combinations of all subsets of nonterminals, or whether an optimal parsing strategy can be found by adding one nonterminal at a time to an existing subset of nonterminals until the entire permutation has been covered. Were such an assumption warranted, this would enable an O(n2 n ) dynamic programming algorithm. It turns out that one-at-a-time parsing strategies are sometimes not optimal. For example, the permutation (4, 7, 3, 8, 1, 6, 2, 5), shown in Figure 18, can be parsed in time O(|w |8 ) using unconstrained subsets, but only in
The permutation (4, 7, 3, 8, 1, 6, 2, 5) cannot be efficiently parsed by adding one nonterminal at a time. The optimal grouping of nonterminals is shown on the right. time O(|w |10 ) by adding one nonterminal at a time. All permutations of less than eight elements can be optimally parsed by adding one element at a time.
In this subsection, we show that an optimal parsing strategy can be found by first factoring an SCFG rule into a sequence of shorter SCFG rules, if possible, and then considering each of the new rules independently. The first step can be done efficiently using the algorithms of Zhang and Gildea (2007). The second step can be done in time O(9 k c &#183; n 2k c) using Algorithm 4, where k c is the complexity of the longest SCFG rule after factorizations, implying that k c &#8804; (n + 4). We show that this two-step process is optimal, by proving that the optimal parsing strategy for the initial rule will not need to build subsets of children that cross the boundaries of the factorization into shorter SCFG rules. Figure 19 shows a permutation that contains permutations of fewer numbers within itself so that the entire permutation can be decomposed hierarchically. We prove that if there is a contiguous block of numbers that are permuted within a permutation, the Algorithm 4 Best-first search for the optimal parsing strategy.
1: function BESTDISCONTINUOUSPARSER(&#960;) 2: n = |&#960;| 3: for A &#8834; {1...n} do 4: chart[A] = &#8734; 5: for i &#8592; 1...n do 6: push[heap, 0, {i}] &#8882; Priority queue of good subsets 7: while chart[{1 . . . n}] = &#8734; do 8: B &#8592; pop(heap) 9: chart[B] &#8592; best[B] &#8882; guaranteed to have found optimal analysis for subset B 10: for C s.t. B &#8745; C = &#8709; &#8743; chart[C] &amp;lt; &#8734; do 11: A &#8592; B &#8746; C 12: Let a, b, and c denote the number of 13: (A, &#960;(A)), (B, &#960;(B)), and (C, &#960;(C))&#8217;s spans 14: compl[A &#8594; B C] = max {a + b + c, best[B], best[C]} 15: if compl[A &#8594; B C] &amp;lt; best[A] then 16: best[A] &#8592; compl[A &#8594; B C] 17: rule[A] &#8592; A &#8594; B C 18: push(heap, best[A], A) 19: return best[{1...n}] 588 Huang et al.
A permutation that can be decomposed into smaller permutations hierarchically. We prove that this decomposition corresponds to the optimal parsing strategy for an SCFG rule with this permutation. optimal parsing strategy for the entire permutation does not have to involve interactions between subsets of numbers inside and outside the block. We call filled entries in the permutation matrix pebbles; the contiguous blocks are shaded in Figure 19, and form submatrices with a pebble in each row and column. We can first decompose a given permutation into a hierarchy of smaller permutations as the tree shown in Figure 19 and then apply the discontinuous strategy to the non-decomposable permutations in the tree. So, in this example, we just need to focus on the optimal parsing strategy for (2, 4, 1, 3), which is applied to permute (4, 5, 6, 7) into (5, 7, 4, 6). By doing this kind of minimization, we can effectively reduce the search space without losing optimality of the parsing strategy for the original permutation. Theorem 4 For any SCFG rule, if there exists a recursive partition of child nonterminals which enables tabular parsing of an input sentence w in time O(|w |k ), and if S is a subset of child nonterminals forming a single continuous span in each language, then there exists a recursive partition containing S as a member whose corresponding parser is also O(|w |k ). See the Appendix for the proof. See Table 3 for a summary of the four factorization algorithms presented in this article (Algorithms 3 and 4 can be improved by first factorizing the permutation into smaller permutations [Section 7.4]).
The combination of minimizing SCFG rule length as a preprocessing step and then applying the best-first version of Algorithm 3 makes it possible to find optimal parsing strategies for all of the rules in the large Chinese&#8211;English rule set used for our decoding experiments. For the 157,212 non-binarizable rules (0.3% of the total), the complexity of the optimal parsing strategies is shown in Table 4. Although the worst parsing complexity is O(|w |12 ), this is only achieved by a single rule. The best-first analyzer takes approximately five minutes of CPU time to analyze this single rule, but processes all others in less than one second. We tested the CKY-based factorization algorithm on our set of non-binarizable rules extracted from the Chinese&#8211;English data. The CKY-on-English method found an optimal parsing strategy for 98% of the rules, and its worst-case complexity over the entire ruleset was O(|w |15 ), rather than the optimal O(|w |12 ). If we run CKY factorization from two directions (one for the permutation &#960; and the other for the permutation &#960; &#8722;1 ) and take the minimum of both, we can get an even better approximation. In Table 4, we compare the approximate strategy which takes the minimum of CKY runs for 589 Computational Linguistics Volume 35, Number 4 Table 4 The distribution of parsing complexities of non-binarizable rules extracted from the GIZA-aligned Chinese&#8211;English data in Section 5. The first column denotes the exponent of the time complexity&#8212;for example, 10 means O(|w |10 ). opt denotes the optimal parsing strategy and cky-min denotes the approximation strategy that takes the better of the CKY results on both sides.
two languages, which we call CKY-min, with the optimal strategy. For synchronous parsing, for 99.77% of the rules, the CKY-min method found an optimal strategy. When generalized for m-gram integrated decoding, CKY maintains continuous spans on the output language and allows for discontinuous parsing on the input sentence. The difference between CKY-on-output and the optimal decoding strategy was negligible in the situation of trigram-integrated decoding for the given rules. The worst-case complexity for decoding into English by CKY-on-English was O(|w |18 ), versus O(|w |17 ) from the optimal strategy. The CKY-on-English approach found an optimal decoding strategy for 99.97% of the non-binarizable rules. The CKY-min strategy was even better, only finding sub-optimal results for six rules out of all rules, which translates to 99.996%. In Table 4, we have also included the comparison for translating into Chinese, in which case the inverted permutations are used and the language model weight is put on the Chinese side. A similar approximation accuracy was achieved. 7.6 Bounds on Complexity of Factorization Given that our algorithms for optimal factorization are exponential, it is natural to ask whether the problem is provably NP-complete. Gildea and &#352;tefankovi&#269; (2007) relate the problem of finding the optimal parsing strategy for a rule to computing the treewidth of a graph derived from the rule&#8217;s permutation. Computing treewidth of arbitrary graphs is NP-complete (Arnborg, Corneil, and Proskurowski 1987), but the graphs derived from SCFG permutations have a restricted structure that it might be possible to exploit. In particular, the graphs have degree no greater than six. While computing treewidth for graphs of bounded degree nine was shown to be NP-complete by Bodlaender and Thilikos (1997), whether the treewidth problem for graphs of degree between three and eight is NP-complete is not known. Thus, whether computing the optimal parsing strategy for an SCFG rule is NP-complete remains an interesting open problem.
This work develops a theory of binarization for synchronous context-free grammars. We present a technique called synchronous binarization along with an efficient binarization algorithm. Empirical study shows that the vast majority of syntactic reorderings, at least between languages like English and Chinese, can be efficiently decomposed into hierarchical binary reorderings. As a result, decoding with n-gram models can be fast and accurate, making it possible for our syntax-based system to overtake a comparable phrase-based system in BLEU score. There are, however, some interesting rules that are not binarizable, and we provide, for the first time, real examples verified by native speakers. For these remaining rules, we have shown an exponential time algorithm for finding optimal parsing strategies, which runs quite fast with the help of two optimality-maintaining operations and the A* search strategy. We also provide an efficient approximation, which usually finds optimal parsing strategies in practice. As non-binarizable rules did not improve our translation system, these parsing strategies are primarily of theoretical interest, though they may become more important in future systems.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>In this section and the following section, we discuss techniques for handling rules that are not binarizable.</text>
              <doc_id>305</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This is primarily of theoretical interest, as we found that they constitute a small fraction of all rules, and removing these did not affect our Chinese-to- English translation results.</text>
              <doc_id>306</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>However, non-binarizable rules are shown to be important in explaining existing hand-aligned data, especially for other language pairs such as German&#8211;English (see Section 5.2, as well as Wellington, Waxmonsky, and Melamed [2006]).</text>
              <doc_id>307</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Non-binarizable rules may also become more important as machine translation</text>
              <doc_id>308</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A summary of the four factorization algorithms, and the &#8220;incremental relaxation&#8221; theme of the whole paper.</text>
              <doc_id>309</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Algorithms 2&#8211;4 are for non-binarizable SCFGs, and are mainly of theoretical interest.</text>
              <doc_id>310</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Algorithms 1&#8211;3 make fewer and fewer assumptions on the strategy space, and produce parsing strategies closer and closer to the optimal.</text>
              <doc_id>311</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Algorithm 4 further improves Algorithm 3.</text>
              <doc_id>312</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>systems improve.</text>
              <doc_id>313</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Synchronous grammars that go beyond the power of SCFG (and therefore binary SCFG) have been defined by Shieber and Schabes (1990) and Rambow and Satta (1999), and motivated for machine translation by Melamed (2003), although previous work has not given algorithms for finding efficient and optimal parsing strategies for general SCFGs, which we believe is an important problem.</text>
              <doc_id>314</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In the remainder of this section and the next section, we will present a series of algorithms that produce increasingly faster parsing strategies, by gradually relaxing the strong &#8220;continuity&#8221; constraint made by the synchronous binarization technique.</text>
              <doc_id>315</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As that technique requires continuity on both languages, we will first study a relaxation where binarized rules are always continuous in one of the two languages, but may be discontinuous in the other.</text>
              <doc_id>316</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We will present a CKY-style algorithm (Section 6.2) for finding the best parsing strategy under this new constraint, which we call one-sided binarization.</text>
              <doc_id>317</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In practice, this factorization has the advantage that we need to maintain only one set of language model boundary words for each partial hypothesis.</text>
              <doc_id>318</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We will see, however, that it is not always possible to achieve the best asymptotic complexity within this constraint.</text>
              <doc_id>319</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>But most importantly, as the synchronous binarization algorithm covers most of the SCFG rules in real data, the one-sided binarization we discuss in this section is able to achieve optimal parsing complexity for most of the non-binarizable rules in real data.</text>
              <doc_id>320</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>So this section can be viewed as a middle step between the synchronous binarization we focus on in the previous sections and the optimal factorization coming in Section 7, and also a trade-off point between simplicity and asymptotic complexity for parsing strategies of SCFGs.</text>
              <doc_id>321</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Table 3 summarizes this incremental structure of the whole paper.</text>
              <doc_id>322</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The complexity for decoding given a grammar factorization can be expressed in terms of the number of spans of the items being combined at each step.</text>
              <doc_id>323</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>As an example, Figure 14 shows the three combination steps for one factorization of the non-binarizable rule:</text>
              <doc_id>324</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the rule&#8217;s child nonterminals.</text>
              <doc_id>325</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Each step combines two dynamic programming items covering disjoint spans of the Chinese input, and creates a new item covering the union of the spans.</text>
              <doc_id>326</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For example, in the first combination step shown in Figure 14, where nonterminals A and B are combined, A has one span in Chinese, from position y 1 to y 2 in the string, and B has one span from y 3 to y 4 .</text>
              <doc_id>327</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The chart entry for the nonterminal</text>
              <doc_id>328</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The tree at the top of the figure defines a three-step decoding strategy for rule (15), building an English output string on the horizontal axis as we process the Chinese input on the vertical axis.</text>
              <doc_id>329</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In each step, the two subsets of nonterminals in the inner marked spans are combined into a new chart item with the outer spans.</text>
              <doc_id>330</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The intersection of the outer spans, shaded, has now been processed.</text>
              <doc_id>331</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>pair {A, B} must record a total of four string indices: positions y 1 , y 2 , y 3 , and y 4 in the Chinese string.</text>
              <doc_id>332</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Any combination of two subsets of the rule&#8217;s nonterminals involves the indices for the spans of each subset.</text>
              <doc_id>333</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>However, some of the indices are tied together: If we are joining two spans into one span in the new item, one of the original spans&#8217; end-points must be equal to another span&#8217;s beginning point.</text>
              <doc_id>334</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>For example, the index y 2 is the end-point of A in Chinese, as well as the beginning position of D.</text>
              <doc_id>335</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In general, if we are combining a subset B of nonterminals having b spans with a subset C having c spans, to produce a spans for a combined subset A = B &#8746; C, the number of linked indices is b + c &#8722; a.</text>
              <doc_id>336</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>In the example of the first step of Figure 14, subset {A} has two spans (one in each language) so b = 1, and {B} also has two spans, so c = 1.</text>
              <doc_id>337</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>The combined subset {A, B} has two spans, so a = 2.</text>
              <doc_id>338</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>The total number of indices involved in a combination of two subsets is</text>
              <doc_id>339</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>number of shared indices.</text>
              <doc_id>340</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In the first step of Figure 14, a + b + c = 1 + 1 + 2 = 4 total indices, and therefore the complexity of this step is O(|w |4 ) where |w |is the length of the input Chinese strings, and we ignore the language model for the moment.</text>
              <doc_id>341</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Applying this formula to the second and third step, we see that the second is O(|w |5 ), and the third is again O(|w |4 ).</text>
              <doc_id>342</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>In order to find a good decoding strategy for a given grammar rule, we need to search over possible orders in which partial translation hypotheses can be built by successively combining nonterminals.</text>
              <doc_id>343</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Any strategy we find can be used for synchronous parsing as well as decoding.</text>
              <doc_id>344</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>For example, the strategy shown in Figure 14 can be used to parse an input Chinese/English string pair.</text>
              <doc_id>345</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The complexity of each step is determined by the total number of indices into both the Chinese and English strings.</text>
              <doc_id>346</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Each step in</text>
              <doc_id>347</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1: function BESTCONTINUOUSPARSER(&#960;) 2: n = |&#960;| 3: for span &#8592; 1 to n &#8722; 1 do 4: for i &#8592; 1 to n &#8722; span do 5: A = {i...i + span} 6: best[A] = &#8734; 7: for j &#8592; i + 1 to i + span do 8: B = {i...j &#8722; 1} 9: C = {j...i + span} 10: Let a &#960; , b &#960; , and c &#960; denote the number of &#960;(A), &#960;(B), and &#960;(C)&#8217;s spans 11: compl[A &#8594; B C] = max {a &#960; + b &#960; + c &#960; , best[B], best[C]} 12: if compl[A &#8594; B C] &amp;lt; best[A] then 13: best[A] = compl[A &#8594; B C] 14: rule[A] = A &#8594; B C 15: return best[{1...n}] the diagram has three indices into the English string, so the complexity of the first step is O(|w |4+3 ) = O(|w |7 ), the second step is O(|w |8 ), and the third is again O(|w |7 ).</text>
              <doc_id>348</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The O(n 3 ) algorithm we present in this section can find good factorizations for most non-binarizable rules; we discuss optimal factorization in the next section.</text>
              <doc_id>349</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This algorithm, shown in Algorithm 2, considers only factorizations that have only one span in one of the two languages, and efficiently searches over all such factorizations by combining adjacent spans with CKY-style parsing.</text>
              <doc_id>350</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>5 The input is an SCFG grammar rule in its abstract form, which is a permutation, and best is a dynamic programming table used to store the lowest complexity with which we can parse a given subset of the input rule&#8217;s child nonterminals.</text>
              <doc_id>351</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Although this CKY-style algorithm finds the best grammar factorization maintaining continuous spans in one of the two dimensions, in general the best factorization may require discontinuous spans in both dimensions.</text>
              <doc_id>352</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>As an example, the following pattern causes problems for the algorithm regardless of which of the dimensions it parses across:</text>
              <doc_id>353</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Left, a general pattern of non-binarizable permutations.</text>
              <doc_id>354</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Center, a partially completed chart item with two spans in each dimension; the intersection of the completed spans is shaded.</text>
              <doc_id>355</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Right, the combination of the item from the center panel with a singleton item.</text>
              <doc_id>356</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The two subsets of nonterminals in the inner marked spans are combined into a new chart item with the outer spans.</text>
              <doc_id>357</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>beginning at position 1 and one beginning at position n 2 + 1, and adding one nonterminal at a time to the partially completed item, as shown in Figure 15 (right).</text>
              <doc_id>358</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>However, our CKY factorization algorithm will give a factorization with n/2 discontinuous spans in one dimension.</text>
              <doc_id>359</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Thus in the worst case, the number of spans found by the cubic-time algorithm grows with n, even when a constant number of spans is possible, implying that there is no approximation ratio on how close the algorithm will get to the optimal solution.</text>
              <doc_id>360</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The method presented in the previous section is not optimal for all permutations, because in some cases it is better to maintain multiple spans in the output language (despite the extra language model state that is needed) in order to maintain continuous spans in the input language.</text>
              <doc_id>361</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In this section we give a method for finding decoding strategies that are guaranteed to be optimal in their asymptotic complexity.</text>
              <doc_id>362</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This method can also be used to find the optimal strategy for synchronous parsing (alignment) using complex rules.</text>
              <doc_id>363</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This answers a question left open by earlier work in synchronous grammars: Although Satta and Peserico (2005) show that tabular parsing of a worst-case SCFG can be NP-hard, they do not give a procedure for finding the complexity of an arbitrary input grammar.</text>
              <doc_id>364</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Similarly, Melamed (2003) defines the cardinality of a grammar, and discusses the interaction of this property with parsing complexity, but does not show how to find a normal form for a grammar with the lowest possible cardinality.</text>
              <doc_id>365</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We show below how to analyze parsing and decoding strategies for a given SCFG rule in Section 7.1, and then present an exponential-time dynamic programming algorithm for finding the best strategy in Section 7.2.</text>
              <doc_id>366</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We prove that factorizing an SCFG rule into smaller SCFG rules is a safe preprocessing step for finding the best strategy in Section 7.4, which leads to much faster computation in many cases.</text>
              <doc_id>367</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>First, however, we take a brief detour to discuss modifying our a + b + c formula from the previous section in order to take the state from an m-gram language model into account during</text>
              <doc_id>368</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>First, how do we analyze algorithms that create discontinuous spans in both the source and target language?</text>
              <doc_id>369</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It turns out that the analysis in Section 6.1 for counting string indices in terms of spans in fact applies in the same way to both of our languages.</text>
              <doc_id>370</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>For synchronous parsing, if we are combining item B with b e target spans and b f source spans with item C having c e target spans and c f source spans to form new item A having a e target spans and a f source spans, the complexity of the operation is O(|w |a e+b e +c e +a f +b f +c f ).</text>
              <doc_id>371</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The a + b + c formula can also be generalized for decoding with an integrated m-gram language model.</text>
              <doc_id>372</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>At first glance, because we need to maintain (m &#8722; 1) boundary words at both the left and right edges of each target span, the total number of interacting variables is: 2(m &#8722; 1)(b e + c e ) + a f + b f + c f However, by using the &#8220;hook trick&#8221; suggested by Huang, Zhang, and Gildea (2005), we can optimize the decoding algorithm by factorizing the dynamic programming combination rule into two steps.</text>
              <doc_id>373</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>One step incorporates the language model probability, and the other step combines adjacent spans in the input language and incorporates the SCFG rule probability.</text>
              <doc_id>374</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The hook trick for a bigram language model and binary SCFG is shown in Figure 16.</text>
              <doc_id>375</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In the equations of Figure 16, i,j,k range over 1 to |w|, the length of the input foreign sentence, and u,v,v 1 ,u 2 (or u,v,v 2 ,u 1 ) range over possible English words, which we assume to take O(|w|) possible values.</text>
              <doc_id>376</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>There are seven free variables related to input size for doing the maximization computation, hence the algorithmic complexity is O(|w |7 ).</text>
              <doc_id>377</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>The two terms in Figure 16 (top) within the first level of the max operator correspond to straight and inverted ITG rules.</text>
              <doc_id>378</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Figure 16 (bottom) shows how to decompose the first term; the same method applies to the second term.</text>
              <doc_id>379</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>Counting the free variables enclosed in the innermost max operator, we get five: i, k, u, v 1 , and u 2 .</text>
              <doc_id>380</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>The where</text>
              <doc_id>381</doc_id>
              <sec_id>12</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The hook trick for machine translation decoding with a binary SCFG (equivalent to Inversion Transduction Grammar).</text>
              <doc_id>382</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The fundamental dynamic programming equation is shown at the top, with an efficient factorization shown below.</text>
              <doc_id>383</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>decomposition eliminates one free variable, v 1 .</text>
              <doc_id>384</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In the outermost level, there are six free variables left.</text>
              <doc_id>385</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The maximum number of interacting variables is six overall.</text>
              <doc_id>386</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>So, we have reduced the complexity of ITG decoding using the bigram language model from O(|w |7 ) to O(|w |6 ).</text>
              <doc_id>387</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>When we generalize the hook trick for any m-gram language model and more complex SCFGs, each left boundary for a substring of an output language hypothesis contains m &#8722; 1 words of language model state, and each right boundary contains a &#8220;hook&#8221; specifying what the next m &#8722; 1 words must be.</text>
              <doc_id>388</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This yields a complexity analysis similar to that for synchronous parsing, based on the total number of boundaries, but now multiplied by a factor of m &#8722; 1: for translation from source to target.</text>
              <doc_id>389</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>(m &#8722; 1)(a e + b e + c e ) + a f + b f + c f (17)</text>
              <doc_id>390</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The number of m-gram weighted spans of a constituent, denoted a m , is defined as the number of source spans plus the number target spans weighted by the language model factor (m &#8722; 1): a m = a f + (m &#8722; 1)a e (18) Using this notation, we can rewrite the expression for the complexity of decoding in Equation 17 as a simple sum of the numbers of weighted spans of constituent subsets A, B, and C: a m + b m + c m (19) and more generally when k &#8805; 2 constituents are combined together:</text>
              <doc_id>391</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>It can be seen that, as m grows, the parsing/decoding strategies that favor contiguity on the output side will prevail.</text>
              <doc_id>392</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This effect is demonstrated by the experimental results in Section 7.5.</text>
              <doc_id>393</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>This analysis applies to one combination of two subsets of a rule&#8217;s children during parsing or decoding.</text>
              <doc_id>394</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>A strategy for parsing (or decoding) the entire rule must build up the complete set of the rule&#8217;s children through a sequence of such combinations.</text>
              <doc_id>395</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Thus a parsing strategy corresponds to a recursive partitioning of the rule&#8217;s children, that is, an unordered rooted tree having the child nonterminals as leaves.</text>
              <doc_id>396</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Each node in the partition tree represents a subset of nonterminals used as a partial result in the chart for parsing, built by combining the subsets corresponding to the node&#8217;s children.</text>
              <doc_id>397</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>This combination step at each node has complexity determined by the number of spans, and the overall complexity of a parsing strategy is the complexity of the strategy&#8217;s worst combination step.</text>
              <doc_id>398</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>We wish to find the recursive partition with the lowest overall complexity.</text>
              <doc_id>399</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Unfortunately, the number of recursive partitions of n items grows super- i=1 584 Huang et al. Binarization of Synchronous Context-Free Grammars exponentially, as 0.175n!n &#8722;3/2 2.59 n = &#920;(&#915;(n + 1)2.59 n ) (Schr&#246;der 1870, Problem IV).</text>
              <doc_id>400</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>6 More formally, the optimization over the space of all recursive partitions is expressed as:</text>
              <doc_id>401</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where compl(A &#8594; B 1 ...B k ) is given by Equation (20).</text>
              <doc_id>402</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This recursive equation implies that we can solve the optimization problem using dynamic programming techniques.</text>
              <doc_id>403</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this section we show that a branching factor of more than two is not necessary in our recursive partitions, by showing that any ternary combination can be factored into two binary combinations with no increase in complexity.</text>
              <doc_id>404</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This fact leads to a more efficient, but still exponential, algorithm for finding the best parsing strategy for a given SCFG rule.</text>
              <doc_id>405</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Theorem 3 For any SCFG rule, if there exists a recursive partition of child nonterminals which enables tabular parsing of an input sentence w in time O(|w |k ), then there exists a recursive binary partition whose corresponding parser is also O(|w |k ).</text>
              <doc_id>406</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Proof We use the notion of number of weighted spans (Equation (20)) to concisely analyze the complexity of synchronous parsing/decoding.</text>
              <doc_id>407</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>For any fixed m, we count a constituent&#8217;s number of spans using the weighted span value from Equation (18), and we drop both the adjective &#8220;weighted&#8221; and the m subscript from this point forward.</text>
              <doc_id>408</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>If we are combining k subsets B i (i = 1, ..., k, k &#8805; 2) together to produce a new subset A = &#8899; i B i, the generalized formula for counting the total number of indices is</text>
              <doc_id>409</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where b i is the number of spans for B i and a is the number of spans for the resulting item A.</text>
              <doc_id>410</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Consider a ternary rule X &#8594; A B C where X has x spans, A has a spans, B has b spans, and C has c spans.</text>
              <doc_id>411</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In the example shown in Figure 17, x = 2, a = 2, b = 1, and c =</text>
              <doc_id>412</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and refer to the number of spans in partial constituent Y as y.</text>
              <doc_id>413</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Parsing Y &#8594; A B takes time O(|w |y+a+b ), so we need to show that y + a + b &#8804; a + b + c + x to show that we can parse this new rule in no more time than the original ternary rule.</text>
              <doc_id>414</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Subtracting a + b from both sides, we need to prove that y &#8804; c + x Each of the y spans in Y corresponds to a left edge.</text>
              <doc_id>415</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>(In the case of decoding, each edge has a multiplicity of (m &#8722; 1) on the output language side.</text>
              <doc_id>416</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>) The left edge in each span of Y corresponds to the left edge of a span in X or to the right edge of a span in C.</text>
              <doc_id>417</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, Y has at most one span for each span in C &#8746; X , so y &#8804; c + x. Returning to the first rule in our factorization, the time to parse X &#8594; Y C is O(|w |x+y+c ).</text>
              <doc_id>418</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We know that y &#8804; a + b since Y was formed from A and B.</text>
              <doc_id>419</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Therefore x + y + c &#8804; x + (a + b) + c so parsing X &#8594; Y C also takes no more time than the original rule X &#8594; A B C.</text>
              <doc_id>420</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>By induction over the number of subsets, a rule having any number of subsets on the righthand side can be converted into a series of binary rules.</text>
              <doc_id>421</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>&#65533; Our finding that combining no more than two subsets of children at a time is optimal implies that we need consider only binary recursive partitions, which correspond to unordered binary rooted trees having the SCFG rule&#8217;s child nonterminals as leaves.</text>
              <doc_id>422</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>The (2n&#8722;3)!</text>
              <doc_id>423</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>total number of binary recursive partitions of n nodes is 2 n&#8722;2 (n&#8722;2)!</text>
              <doc_id>424</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>= &#920;(&#915;(n &#8722; 2 1 )2n&#8722;1 ) (Schr&#246;der 1870, Problem III).</text>
              <doc_id>425</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>Note that this number grows much faster than the Catalan Number, which characterizes the number of bracketings representing the search space of synchronous binarization (Section 4).</text>
              <doc_id>426</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>Although the total number of binary recursive partitions is still superexponential, the binary branching property also enables a straightforward dynamic programming algorithm, shown in Algorithm 3.</text>
              <doc_id>427</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>The same algorithm can be used to find optimal strategies for synchronous parsing or for m-gram decoding: for parsing, the variables a, b, and c in Line 9 refer to the total number of spans of A, B, and C (Equation (16)), Figure 17 Left, example spans for a ternary rule decomposition X &#8594; A B C.</text>
              <doc_id>428</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>Each symbol represents a subset of nonterminals from the original SCFG rule, and the subsets may cover discontinuous spans in either language.</text>
              <doc_id>429</doc_id>
              <sec_id>16</sec_id>
            </sentence>
            <sentence>
              <text>Line segments represent the projection of each set of child nonterminals into a single language, as in Figure 15.</text>
              <doc_id>430</doc_id>
              <sec_id>17</sec_id>
            </sentence>
            <sentence>
              <text>Right, factorization into X &#8594; Y C and Y &#8594; A B.</text>
              <doc_id>431</doc_id>
              <sec_id>18</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2: n = |&#960;| 3: for i &#8592; 2 to n do 4: for A &#8834; {1...n} s.t.</text>
              <doc_id>432</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>|A |= i do 5: best[A] &#8592; &#8734; 6: for B, C s.t.</text>
              <doc_id>433</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>A = B &#8746; C &#8743; B &#8745; C = &#8709; do 7: Let a, b, and c denote the number of 8: (A, &#960;(A)), (B, &#960;(B)), and (C, &#960;(C))&#8217;s spans 9: compl[A &#8594; B C] = max {a + b + c, best[B], best[C]} 10: if compl[A &#8594; B C] &amp;lt; best[A] then 11: best[A] &#8592; compl[A &#8594; B C] 12: rule[A] &#8592; A &#8594; B C 13: return best[{1...n}]</text>
              <doc_id>434</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>while for decoding, a, b, and c refer to weighted spans (Equation (19)).</text>
              <doc_id>435</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The dynamic programming states correspond to subsets of the input rule&#8217;s children, for which an optimal strategy has already been computed.</text>
              <doc_id>436</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In each iteration of the algorithm&#8217;s inner loop, each of the child nonterminals is identified as belonging to B, C, or neither B nor C, making the total running time of the algorithm O(3 n ).</text>
              <doc_id>437</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Although this is exponential in n, it is a significant improvement over considering all recursive partitions.</text>
              <doc_id>438</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The algorithm can be improved by adopting a best-first exploration strategy (Knuth 1977), in which dynamic programming items are placed on a priority queue sorted according to their complexity, and only used to build further items after all items of lower complexity have been exhausted.</text>
              <doc_id>439</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>This technique, shown in Algorithm 4, guarantees polynomial-time processing on input permutations of bounded complexity.</text>
              <doc_id>440</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>To see why this is, observe that each rule of the form A &#8594; B C that has complexity no greater than k can be written using a string of k e &amp;lt; k indices into the target nonterminal string to represent the spans&#8217; boundaries.</text>
              <doc_id>441</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>For each index we must specify whether the corresponding nonterminal either starts a span of subset B, starts a span of subset C, or ends a span of B &#8746; C.</text>
              <doc_id>442</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>Therefore there are O((3n) k ) rules of complexity no greater than k.</text>
              <doc_id>443</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>If there exists a parsing strategy for the entire rule with complexity k, the best-first algorithm will find it after, in the worst case, popping all O((3n) k ) rules of complexity less than or equal to k off of the heap in the outer loop, and combining each one with all other O((3n) k ) such rules in the inner loop, for a total running time of O(9 k n 2k ).</text>
              <doc_id>444</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Although the algorithm is still exponential in the rule length n in the worst case (when k is linearly correlated to n), the best-first behavior makes it much more practical for our empirically observed rules.</text>
              <doc_id>445</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>One might wonder whether it is necessary to consider all combinations of all subsets of nonterminals, or whether an optimal parsing strategy can be found by adding one nonterminal at a time to an existing subset of nonterminals until the entire permutation has been covered.</text>
              <doc_id>446</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Were such an assumption warranted, this would enable an O(n2 n ) dynamic programming algorithm.</text>
              <doc_id>447</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>It turns out that one-at-a-time parsing strategies are sometimes not optimal.</text>
              <doc_id>448</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>For example, the permutation (4, 7, 3, 8, 1, 6, 2, 5), shown in Figure 18, can be parsed in time O(|w |8 ) using unconstrained subsets, but only in</text>
              <doc_id>449</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The permutation (4, 7, 3, 8, 1, 6, 2, 5) cannot be efficiently parsed by adding one nonterminal at a time.</text>
              <doc_id>450</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The optimal grouping of nonterminals is shown on the right.</text>
              <doc_id>451</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>time O(|w |10 ) by adding one nonterminal at a time.</text>
              <doc_id>452</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>All permutations of less than eight elements can be optimally parsed by adding one element at a time.</text>
              <doc_id>453</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In this subsection, we show that an optimal parsing strategy can be found by first factoring an SCFG rule into a sequence of shorter SCFG rules, if possible, and then considering each of the new rules independently.</text>
              <doc_id>454</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The first step can be done efficiently using the algorithms of Zhang and Gildea (2007).</text>
              <doc_id>455</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The second step can be done in time O(9 k c &#183; n 2k c) using Algorithm 4, where k c is the complexity of the longest SCFG rule after factorizations, implying that k c &#8804; (n + 4).</text>
              <doc_id>456</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We show that this two-step process is optimal, by proving that the optimal parsing strategy for the initial rule will not need to build subsets of children that cross the boundaries of the factorization into shorter SCFG rules.</text>
              <doc_id>457</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Figure 19 shows a permutation that contains permutations of fewer numbers within itself so that the entire permutation can be decomposed hierarchically.</text>
              <doc_id>458</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We prove that if there is a contiguous block of numbers that are permuted within a permutation, the Algorithm 4 Best-first search for the optimal parsing strategy.</text>
              <doc_id>459</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1: function BESTDISCONTINUOUSPARSER(&#960;) 2: n = |&#960;| 3: for A &#8834; {1...n} do 4: chart[A] = &#8734; 5: for i &#8592; 1...n do 6: push[heap, 0, {i}] &#8882; Priority queue of good subsets 7: while chart[{1 .</text>
              <doc_id>460</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>461</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>462</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>n}] = &#8734; do 8: B &#8592; pop(heap) 9: chart[B] &#8592; best[B] &#8882; guaranteed to have found optimal analysis for subset B 10: for C s.t.</text>
              <doc_id>463</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>B &#8745; C = &#8709; &#8743; chart[C] &amp;lt; &#8734; do 11: A &#8592; B &#8746; C 12: Let a, b, and c denote the number of 13: (A, &#960;(A)), (B, &#960;(B)), and (C, &#960;(C))&#8217;s spans 14: compl[A &#8594; B C] = max {a + b + c, best[B], best[C]} 15: if compl[A &#8594; B C] &amp;lt; best[A] then 16: best[A] &#8592; compl[A &#8594; B C] 17: rule[A] &#8592; A &#8594; B C 18: push(heap, best[A], A) 19: return best[{1...n}] 588 Huang et al.</text>
              <doc_id>464</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>A permutation that can be decomposed into smaller permutations hierarchically.</text>
              <doc_id>465</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We prove that this decomposition corresponds to the optimal parsing strategy for an SCFG rule with this permutation.</text>
              <doc_id>466</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>optimal parsing strategy for the entire permutation does not have to involve interactions between subsets of numbers inside and outside the block.</text>
              <doc_id>467</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We call filled entries in the permutation matrix pebbles; the contiguous blocks are shaded in Figure 19, and form submatrices with a pebble in each row and column.</text>
              <doc_id>468</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We can first decompose a given permutation into a hierarchy of smaller permutations as the tree shown in Figure 19 and then apply the discontinuous strategy to the non-decomposable permutations in the tree.</text>
              <doc_id>469</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>So, in this example, we just need to focus on the optimal parsing strategy for (2, 4, 1, 3), which is applied to permute (4, 5, 6, 7) into (5, 7, 4, 6).</text>
              <doc_id>470</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>By doing this kind of minimization, we can effectively reduce the search space without losing optimality of the parsing strategy for the original permutation.</text>
              <doc_id>471</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Theorem 4 For any SCFG rule, if there exists a recursive partition of child nonterminals which enables tabular parsing of an input sentence w in time O(|w |k ), and if S is a subset of child nonterminals forming a single continuous span in each language, then there exists a recursive partition containing S as a member whose corresponding parser is also O(|w |k ).</text>
              <doc_id>472</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>See the Appendix for the proof.</text>
              <doc_id>473</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>See Table 3 for a summary of the four factorization algorithms presented in this article (Algorithms 3 and 4 can be improved by first factorizing the permutation into smaller permutations [Section 7.4]).</text>
              <doc_id>474</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>The combination of minimizing SCFG rule length as a preprocessing step and then applying the best-first version of Algorithm 3 makes it possible to find optimal parsing strategies for all of the rules in the large Chinese&#8211;English rule set used for our decoding experiments.</text>
              <doc_id>475</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For the 157,212 non-binarizable rules (0.3% of the total), the complexity of the optimal parsing strategies is shown in Table 4.</text>
              <doc_id>476</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Although the worst parsing complexity is O(|w |12 ), this is only achieved by a single rule.</text>
              <doc_id>477</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The best-first analyzer takes approximately five minutes of CPU time to analyze this single rule, but processes all others in less than one second.</text>
              <doc_id>478</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We tested the CKY-based factorization algorithm on our set of non-binarizable rules extracted from the Chinese&#8211;English data.</text>
              <doc_id>479</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The CKY-on-English method found an optimal parsing strategy for 98% of the rules, and its worst-case complexity over the entire ruleset was O(|w |15 ), rather than the optimal O(|w |12 ).</text>
              <doc_id>480</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>If we run CKY factorization from two directions (one for the permutation &#960; and the other for the permutation &#960; &#8722;1 ) and take the minimum of both, we can get an even better approximation.</text>
              <doc_id>481</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Table 4, we compare the approximate strategy which takes the minimum of CKY runs for 589 Computational Linguistics Volume 35, Number 4 Table 4 The distribution of parsing complexities of non-binarizable rules extracted from the GIZA-aligned Chinese&#8211;English data in Section 5.</text>
              <doc_id>482</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>The first column denotes the exponent of the time complexity&#8212;for example, 10 means O(|w |10 ).</text>
              <doc_id>483</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>opt denotes the optimal parsing strategy and cky-min denotes the approximation strategy that takes the better of the CKY results on both sides.</text>
              <doc_id>484</doc_id>
              <sec_id>9</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>two languages, which we call CKY-min, with the optimal strategy.</text>
              <doc_id>485</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>For synchronous parsing, for 99.77% of the rules, the CKY-min method found an optimal strategy.</text>
              <doc_id>486</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>When generalized for m-gram integrated decoding, CKY maintains continuous spans on the output language and allows for discontinuous parsing on the input sentence.</text>
              <doc_id>487</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The difference between CKY-on-output and the optimal decoding strategy was negligible in the situation of trigram-integrated decoding for the given rules.</text>
              <doc_id>488</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The worst-case complexity for decoding into English by CKY-on-English was O(|w |18 ), versus O(|w |17 ) from the optimal strategy.</text>
              <doc_id>489</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The CKY-on-English approach found an optimal decoding strategy for 99.97% of the non-binarizable rules.</text>
              <doc_id>490</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The CKY-min strategy was even better, only finding sub-optimal results for six rules out of all rules, which translates to 99.996%.</text>
              <doc_id>491</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>In Table 4, we have also included the comparison for translating into Chinese, in which case the inverted permutations are used and the language model weight is put on the Chinese side.</text>
              <doc_id>492</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>A similar approximation accuracy was achieved.</text>
              <doc_id>493</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>7.6 Bounds on Complexity of Factorization Given that our algorithms for optimal factorization are exponential, it is natural to ask whether the problem is provably NP-complete.</text>
              <doc_id>494</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Gildea and &#352;tefankovi&#269; (2007) relate the problem of finding the optimal parsing strategy for a rule to computing the treewidth of a graph derived from the rule&#8217;s permutation.</text>
              <doc_id>495</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>Computing treewidth of arbitrary graphs is NP-complete (Arnborg, Corneil, and Proskurowski 1987), but the graphs derived from SCFG permutations have a restricted structure that it might be possible to exploit.</text>
              <doc_id>496</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>In particular, the graphs have degree no greater than six.</text>
              <doc_id>497</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>While computing treewidth for graphs of bounded degree nine was shown to be NP-complete by Bodlaender and Thilikos (1997), whether the treewidth problem for graphs of degree between three and eight is NP-complete is not known.</text>
              <doc_id>498</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>Thus, whether computing the optimal parsing strategy for an SCFG rule is NP-complete remains an interesting open problem.</text>
              <doc_id>499</doc_id>
              <sec_id>14</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>This work develops a theory of binarization for synchronous context-free grammars.</text>
              <doc_id>500</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We present a technique called synchronous binarization along with an efficient binarization algorithm.</text>
              <doc_id>501</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Empirical study shows that the vast majority of syntactic reorderings, at least between languages like English and Chinese, can be efficiently decomposed into hierarchical binary reorderings.</text>
              <doc_id>502</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>As a result, decoding with n-gram models can be fast and accurate, making it possible for our syntax-based system to overtake a comparable phrase-based system in BLEU score.</text>
              <doc_id>503</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>There are, however, some interesting rules that are not binarizable, and we provide, for the first time, real examples verified by native speakers.</text>
              <doc_id>504</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>For these remaining rules, we have shown an exponential time algorithm for finding optimal parsing strategies, which runs quite fast with the help of two optimality-maintaining operations and the A* search strategy.</text>
              <doc_id>505</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>We also provide an efficient approximation, which usually finds optimal parsing strategies in practice.</text>
              <doc_id>506</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>As non-binarizable rules did not improve our translation system, these parsing strategies are primarily of theoretical interest, though they may become more important in future systems.</text>
              <doc_id>507</doc_id>
              <sec_id>7</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>5</index>
        <title>Acknowledgments</title>
        <text>Much of this work was done while the first two authors were visiting USC/ISI. This work was partially funded by NSF grants IIS-0428020 and EIA-0205456. Appendix A. Proof of Theorem 4 We prove by contradiction. Let us suppose that the optimal parsing strategy for a permutation P splits a contiguous block S of P into two subtrees T L and T R , as shown at the top of Figure 20, in either or both of which there are some pebbles from outside S. As in Section 7, we count spans in this section using the weighted span value of Equation (18) to account for m-gram language model state. We use d LO to denote the number of spans of the pebbles outside of S in T L . d LI is the number of spans of the pebbles inside S for T L . We use r L to denote the reduction in the number of spans achieved by merging the pebbles inside and outside of S for T L . So, the number of spans of the root of T L is d LO + d LI &#8722; r L . We have symmetric notions for T R . We use d to denote the number of spans of the root of the subtree T after merging T L and T R . The number of spans is annotated for each node in Figure 20. Notice that (r R + r L ) &#8804; 2m because there are at most two boundaries shared by inside pebbles and outside pebbles in each language. Each boundary in the source corresponds to the reduction of one span. Each boundary in the target corresponds to the reduction of one weighted span of (m &#8722; 1). In total, we can reduce the number of (weighted) spans by no more than 2(m &#8722; 1) + 2 = 2m. This inequality implies either
Reorganization of a parsing strategy to build a continuous span S first. Figure 21 gives a concrete example. The permutation is (5, 7, 4, 6, 1, 2, 3). The block S we focus on is (5, 7, 4, 6). The original strategy at the top of the figure splits the block into T L and T R . The improved strategy on the bottom merges the pebbles inside S together before making combinations with pebbles outside S. Figure 21 An actual example of reorganization of a parsing strategy to build a continuous span S first. Before, the overall strategy cost is (7m &#8722; 3). After, the cost is (5m &#8722; 2). Note that (m &#8805; 2). We use black to represent pebbles in the right branch of the root node and white for the left branch. Gray areas are continuous blocks within the permutation. The reorganized strategy can be further improved by making another such transformation to allow for the lower right corner pebbles to group before interacting with the upper left corner. 592 Huang et al. Binarization of Synchronous Context-Free Grammars In general, we argue that we can have an equally good or better strategy by separating each of T L and T R into two trees involving pebbles purely inside or outside of S, as shown at the bottom of Figure 20. The separation works by simply ignoring the pebbles that are not inside when creating the inside half of the tree or outside when doing the outside half throughout T L and T R . Then we have four elementary subtrees T LO , T LI , T RI , and T RO . In our new strategy, we recombine the four elementary trees by merging T LI and T RI to create a pebble first and merging the resulting pebble back into T LO to make a T &#8242; LO , and finally merging T&#8242; LO with T RO. The elementary trees yield better strategies because the number of spans of each node in these trees is reduced or not changed as compared to that before separation. Using the a + b + c formula with reduced a, b, and c will produce lower complexity. Roughly speaking, the reason is the inside pebbles and outside pebbles are positioned side by side instead of mixed together. Mathematically, the reduction of spans by combining both sides is upper-bounded by 2m, considering there are two boundaries in each language. At the same time, the number of spans of either the inside pebbles or the outside pebbles is lower-bounded by 2m because both T L and T R only partially cover S. Hence, we have the following set of inequalities:
Now we consider what happens when the pebble of S joins T LO . Because T LO is created from T L by pruning away the pebbles that are inside S, the pebble of S can join T LO by taking the place of any trace of the pruned leaves and making the number of spans from the bottom up to the root no greater than in the counterpart nodes in T L . So the fragment of the new left subtree T LO &#8242; with S being its leaf has a better yield than the original T L :
where we use T LO &#8242; /S to denote the tree fragment excluding the nodes under S. The number of spans for each node in the reorganized tree is shown in Figure 20 (bottom), where r (&#8804; 2m) is the reduction in spans after combining the new pebble S with T LO . r sums up the reductions achievable on the four boundaries of S with T LO , while r L sums up the reductions on some of the four boundaries. Thus, r L &#8804; r The final yield of the updated strategy is
This simplifies to m &#8722; r &#8804; d LI + d RI &#8722; r R &#8722; r L This inequality is true because m &#8804; d LI , since there is at least one inside pebble in T L , and d RI &#8805; r R because d RI &#8805; m &#8805; r R , referring to Equation (A.1), and finally r &#8805; r L , as shown in Equation (A.7). Figure 20 also demonstrates the re-distribution of numbers of spans after the reorganization. In the example, the updated parsing/decoding complexity is O(|w |5m&#8722;2 ), better than before (O(|w |7m&#8722;3 )). Therefore, any synchronous parsing/decoding strategy that crosses decomposition boundaries cannot be better than an optimized strategy that respects such boundaries. &#65533;</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Much of this work was done while the first two authors were visiting USC/ISI.</text>
              <doc_id>508</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This work was partially funded by NSF grants IIS-0428020 and EIA-0205456.</text>
              <doc_id>509</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Appendix A. Proof of Theorem 4 We prove by contradiction.</text>
              <doc_id>510</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Let us suppose that the optimal parsing strategy for a permutation P splits a contiguous block S of P into two subtrees T L and T R , as shown at the top of Figure 20, in either or both of which there are some pebbles from outside S.</text>
              <doc_id>511</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>As in Section 7, we count spans in this section using the weighted span value of Equation (18) to account for m-gram language model state.</text>
              <doc_id>512</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We use d LO to denote the number of spans of the pebbles outside of S in T L .</text>
              <doc_id>513</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>d LI is the number of spans of the pebbles inside S for T L .</text>
              <doc_id>514</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>We use r L to denote the reduction in the number of spans achieved by merging the pebbles inside and outside of S for T L .</text>
              <doc_id>515</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>So, the number of spans of the root of T L is d LO + d LI &#8722; r L .</text>
              <doc_id>516</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>We have symmetric notions for T R .</text>
              <doc_id>517</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>We use d to denote the number of spans of the root of the subtree T after merging T L and T R .</text>
              <doc_id>518</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>The number of spans is annotated for each node in Figure 20.</text>
              <doc_id>519</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Notice that (r R + r L ) &#8804; 2m because there are at most two boundaries shared by inside pebbles and outside pebbles in each language.</text>
              <doc_id>520</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>Each boundary in the source corresponds to the reduction of one span.</text>
              <doc_id>521</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>Each boundary in the target corresponds to the reduction of one weighted span of (m &#8722; 1).</text>
              <doc_id>522</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>In total, we can reduce the number of (weighted) spans by no more than 2(m &#8722; 1) + 2 = 2m.</text>
              <doc_id>523</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>This inequality implies either</text>
              <doc_id>524</doc_id>
              <sec_id>16</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Reorganization of a parsing strategy to build a continuous span S first.</text>
              <doc_id>525</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Figure 21 gives a concrete example.</text>
              <doc_id>526</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>The permutation is (5, 7, 4, 6, 1, 2, 3).</text>
              <doc_id>527</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The block S we focus on is (5, 7, 4, 6).</text>
              <doc_id>528</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The original strategy at the top of the figure splits the block into T L and T R .</text>
              <doc_id>529</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>The improved strategy on the bottom merges the pebbles inside S together before making combinations with pebbles outside S.</text>
              <doc_id>530</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Figure 21 An actual example of reorganization of a parsing strategy to build a continuous span S first.</text>
              <doc_id>531</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>Before, the overall strategy cost is (7m &#8722; 3).</text>
              <doc_id>532</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>After, the cost is (5m &#8722; 2).</text>
              <doc_id>533</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>Note that (m &#8805; 2).</text>
              <doc_id>534</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>We use black to represent pebbles in the right branch of the root node and white for the left branch.</text>
              <doc_id>535</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>Gray areas are continuous blocks within the permutation.</text>
              <doc_id>536</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>The reorganized strategy can be further improved by making another such transformation to allow for the lower right corner pebbles to group before interacting with the upper left corner.</text>
              <doc_id>537</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>592 Huang et al. Binarization of Synchronous Context-Free Grammars In general, we argue that we can have an equally good or better strategy by separating each of T L and T R into two trees involving pebbles purely inside or outside of S, as shown at the bottom of Figure 20.</text>
              <doc_id>538</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>The separation works by simply ignoring the pebbles that are not inside when creating the inside half of the tree or outside when doing the outside half throughout T L and T R .</text>
              <doc_id>539</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>Then we have four elementary subtrees T LO , T LI , T RI , and T RO .</text>
              <doc_id>540</doc_id>
              <sec_id>15</sec_id>
            </sentence>
            <sentence>
              <text>In our new strategy, we recombine the four elementary trees by merging T LI and T RI to create a pebble first and merging the resulting pebble back into T LO to make a T &#8242; LO , and finally merging T&#8242; LO with T RO.</text>
              <doc_id>541</doc_id>
              <sec_id>16</sec_id>
            </sentence>
            <sentence>
              <text>The elementary trees yield better strategies because the number of spans of each node in these trees is reduced or not changed as compared to that before separation.</text>
              <doc_id>542</doc_id>
              <sec_id>17</sec_id>
            </sentence>
            <sentence>
              <text>Using the a + b + c formula with reduced a, b, and c will produce lower complexity.</text>
              <doc_id>543</doc_id>
              <sec_id>18</sec_id>
            </sentence>
            <sentence>
              <text>Roughly speaking, the reason is the inside pebbles and outside pebbles are positioned side by side instead of mixed together.</text>
              <doc_id>544</doc_id>
              <sec_id>19</sec_id>
            </sentence>
            <sentence>
              <text>Mathematically, the reduction of spans by combining both sides is upper-bounded by 2m, considering there are two boundaries in each language.</text>
              <doc_id>545</doc_id>
              <sec_id>20</sec_id>
            </sentence>
            <sentence>
              <text>At the same time, the number of spans of either the inside pebbles or the outside pebbles is lower-bounded by 2m because both T L and T R only partially cover S.</text>
              <doc_id>546</doc_id>
              <sec_id>21</sec_id>
            </sentence>
            <sentence>
              <text>Hence, we have the following set of inequalities:</text>
              <doc_id>547</doc_id>
              <sec_id>22</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Now we consider what happens when the pebble of S joins T LO .</text>
              <doc_id>548</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Because T LO is created from T L by pruning away the pebbles that are inside S, the pebble of S can join T LO by taking the place of any trace of the pruned leaves and making the number of spans from the bottom up to the root no greater than in the counterpart nodes in T L .</text>
              <doc_id>549</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>So the fragment of the new left subtree T LO &#8242; with S being its leaf has a better yield than the original T L :</text>
              <doc_id>550</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where we use T LO &#8242; /S to denote the tree fragment excluding the nodes under S.</text>
              <doc_id>551</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The number of spans for each node in the reorganized tree is shown in Figure 20 (bottom), where r (&#8804; 2m) is the reduction in spans after combining the new pebble S with T LO .</text>
              <doc_id>552</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>r sums up the reductions achievable on the four boundaries of S with T LO , while r L sums up the reductions on some of the four boundaries.</text>
              <doc_id>553</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Thus, r L &#8804; r The final yield of the updated strategy is</text>
              <doc_id>554</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>This simplifies to m &#8722; r &#8804; d LI + d RI &#8722; r R &#8722; r L This inequality is true because m &#8804; d LI , since there is at least one inside pebble in T L , and d RI &#8805; r R because d RI &#8805; m &#8805; r R , referring to Equation (A.1), and finally r &#8805; r L , as shown in Equation (A.7).</text>
              <doc_id>555</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Figure 20 also demonstrates the re-distribution of numbers of spans after the reorganization.</text>
              <doc_id>556</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In the example, the updated parsing/decoding complexity is O(|w |5m&#8722;2 ), better than before (O(|w |7m&#8722;3 )).</text>
              <doc_id>557</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Therefore, any synchronous parsing/decoding strategy that crosses decomposition boundaries cannot be better than an optimized strategy that respects such boundaries.</text>
              <doc_id>558</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>&#65533;</text>
              <doc_id>559</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables/>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>Albert V Aho</author>
          <author>Jeffery D Ullman</author>
        </authors>
        <title>None</title>
        <publication>The Theory of Parsing, Translation, and Compiling,</publication>
        <pages>None</pages>
        <date>1972</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>Stefen Arnborg</author>
          <author>Derek G Corneil</author>
          <author>Andrzej Proskurowski</author>
        </authors>
        <title>Complexity of finding embeddings in a k-tree.</title>
        <publication>None</publication>
        <pages>8--277</pages>
        <date>1987</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>H L Bodlaender</author>
          <author>D M Thilikos</author>
        </authors>
        <title>Treewidth for graphs with small chordality.</title>
        <publication>None</publication>
        <pages>79--45</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>3</id>
        <authors/>
        <title>Note extraite d&#8217;une lettre adress&#233;e.</title>
        <publication>Journal f&#252;rdie reine und angewandte Mathematik,</publication>
        <pages>27--192</pages>
        <date>None</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>David Chiang</author>
        </authors>
        <title>A hierarchical phrase-based model for statistical machine translation.</title>
        <publication>In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL-05),</publication>
        <pages>263--270</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>Thomas H Cormen</author>
          <author>Charles E Leiserson</author>
          <author>Ronald L Rivest</author>
        </authors>
        <title>Introduction to Algorithms.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1990</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>Jason Eisner</author>
        </authors>
        <title>Learning non-isomorphic tree mappings for machine translation.</title>
        <publication>In Proceedings of the 41st Meeting of the Association for Computational Linguistics,</publication>
        <pages>205--208</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>Michel Galley</author>
          <author>Mark Hopkins</author>
          <author>Kevin Knight</author>
          <author>Daniel Marcu</author>
        </authors>
        <title>What&#8217;s in a translation rule?</title>
        <publication>In Proceedings of the 2004 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-04),</publication>
        <pages>273--280</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>Daniel Gildea</author>
          <author>Daniel &#352;tefankovi&#269;</author>
        </authors>
        <title>Worst-case synchronous grammar rules.</title>
        <publication>In Proceedings of the 2007 Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-07),</publication>
        <pages>147--154</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>Ronald Graham</author>
        </authors>
        <title>An efficient algorithm for determining the convex hull of a finite planar set. Information Processing Letters, 1:132&#8211;133. 594 et al.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1972</date>
      </reference>
      <reference>
        <id>10</id>
        <authors/>
        <title>Binarization of Synchronous Context-Free Grammars Huang, Liang.</title>
        <publication>In Proceedings of the NAACL/AMTA Workshop on Syntax and Structure in Statistical Translation (SSST),</publication>
        <pages>33--40</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>Liang Huang</author>
          <author>David Chiang</author>
        </authors>
        <title>Better k-best parsing.</title>
        <publication>In International Workshop on Parsing Technologies (IWPT05),</publication>
        <pages>53--64</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Liang Huang</author>
          <author>Hao Zhang</author>
          <author>Daniel Gildea</author>
        </authors>
        <title>Machine translation as lexicalized parsing with hooks.</title>
        <publication>In International Workshop on Parsing Technologies (IWPT05),</publication>
        <pages>65--73</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>ISI Machine</author>
        </authors>
        <title>Translation Team.</title>
        <publication>ISI at NIST-06. Working Notes of the NIST MT Evaluation Workshop,</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>D C Knight Washington</author>
          <author>Kevin</author>
          <author>Jonathan Graehl</author>
        </authors>
        <title>An overview of probabilistic tree transducers for natural language processing.</title>
        <publication>In Conference on Intelligent Text Processing and Computational Linguistics (CICLing),</publication>
        <pages>1--24</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>D Knuth</author>
        </authors>
        <title>A generalization of Dijkstra&#8217;s algorithm.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1977</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>Yang Liu</author>
          <author>Qun Liu</author>
          <author>Shouxun Lin</author>
        </authors>
        <title>Log-linear models for word alignment.</title>
        <publication>In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL-05),</publication>
        <pages>459--466</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>I Dan Melamed</author>
        </authors>
        <title>Multitext grammars and synchronous parsers.</title>
        <publication>In Proceedings of the 2003 Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-03),</publication>
        <pages>158--165</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>Rada Mihalcea</author>
          <author>Ted Pederson</author>
        </authors>
        <title>An evaluation exercise for word alignment.</title>
        <publication>In HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,</publication>
        <pages>1--10</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>M-J Nederhof</author>
        </authors>
        <title>Weighted deductive parsing and knuth&#8217;s algorithm.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>Franz Josef Och</author>
          <author>Hermann Ney</author>
        </authors>
        <title>The alignment template approach to statistical machine translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>Owen Rambow</author>
          <author>Giorgio Satta</author>
        </authors>
        <title>Independent parallelism in finite copying parallel rewriting systems.</title>
        <publication>None</publication>
        <pages>223--1</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>William C Rounds</author>
        </authors>
        <title>Mappings and grammars on trees.</title>
        <publication>Mathematical Systems Theory,</publication>
        <pages>4--3</pages>
        <date>1970</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>Giorgio Satta</author>
          <author>Enoch Peserico</author>
        </authors>
        <title>Some computational complexity results for synchronous context-free grammars.</title>
        <publication>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP),</publication>
        <pages>803--810</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>E Schr&#246;der</author>
        </authors>
        <title>None</title>
        <publication>Vier combinatorische Probleme. Zeitschrift f&#252;r Mathematik und Physik,</publication>
        <pages>15--361</pages>
        <date>1870</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>L Shapiro</author>
          <author>A B Stephens</author>
        </authors>
        <title>Bootstrap percolation, the Schr&#246;der numbers, and the n-kings problem.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1991</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>Stuart M Shieber</author>
        </authors>
        <title>Synchronous grammars as tree transducers.</title>
        <publication>In Proceedings of the Seventh International Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+ 7),</publication>
        <pages>88--95</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>27</id>
        <authors>
          <author>Stuart Shieber</author>
          <author>Yves Schabes</author>
        </authors>
        <title>Synchronous tree-adjoining grammars.</title>
        <publication>In Proceedings of the 13th International Conference on Computational Linguistics (COLING-90), volume III,</publication>
        <pages>253--258</pages>
        <date>1990</date>
      </reference>
      <reference>
        <id>28</id>
        <authors>
          <author>Benjamin Wellington</author>
          <author>Sonjia Waxmonsky</author>
          <author>I Dan Melamed</author>
        </authors>
        <title>Empirical lower bounds on the complexity of translational equivalence.</title>
        <publication>In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06),</publication>
        <pages>977--984</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>29</id>
        <authors>
          <author>Dekai Wu</author>
        </authors>
        <title>A polynomial&#8211;time algorithm for statistical machine translation.</title>
        <publication>In 34th Annual Meeting of the Association for Computational Linguistics,</publication>
        <pages>152--158</pages>
        <date>1996</date>
      </reference>
      <reference>
        <id>30</id>
        <authors>
          <author>Dekai Wu</author>
        </authors>
        <title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>31</id>
        <authors>
          <author>Hao Zhang</author>
          <author>Daniel Gildea</author>
        </authors>
        <title>Factorization of synchronous context-free grammars in linear time.</title>
        <publication>In NAACL Workshop on Syntax and Structure in Statistical Translation (SSST),</publication>
        <pages>25--32</pages>
        <date>2007</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Aho and Ullman 1972</string>
        <sentence_id>21111</sentence_id>
        <char_offset>148</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>2</reference_id>
        <string>Bodlaender and Thilikos (1997)</string>
        <sentence_id>21514</sentence_id>
        <char_offset>91</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>4</reference_id>
        <string>Chiang 2005</string>
        <sentence_id>21016</sentence_id>
        <char_offset>60</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>4</reference_id>
        <string>Chiang 2005</string>
        <sentence_id>21028</sentence_id>
        <char_offset>197</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>4</reference_id>
        <string>Chiang 2005</string>
        <sentence_id>21055</sentence_id>
        <char_offset>243</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>4</reference_id>
        <string>Chiang 2005</string>
        <sentence_id>21064</sentence_id>
        <char_offset>50</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>4</reference_id>
        <string>Chiang (2005)</string>
        <sentence_id>21071</sentence_id>
        <char_offset>259</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>6</reference_id>
        <string>Eisner 2003</string>
        <sentence_id>21078</sentence_id>
        <char_offset>111</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>6</reference_id>
        <string>Eisner 2003</string>
        <sentence_id>21279</sentence_id>
        <char_offset>122</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>7</reference_id>
        <string>Galley et al. (2004)</string>
        <sentence_id>21029</sentence_id>
        <char_offset>43</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>7</reference_id>
        <string>Galley et al. (2004)</string>
        <sentence_id>21255</sentence_id>
        <char_offset>292</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>7</reference_id>
        <string>Galley et al. (2004)</string>
        <sentence_id>21320</sentence_id>
        <char_offset>30</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>7</reference_id>
        <string>Galley et al. 2004</string>
        <sentence_id>21016</sentence_id>
        <char_offset>73</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>7</reference_id>
        <string>Galley et al. 2004</string>
        <sentence_id>21072</sentence_id>
        <char_offset>37</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>8</reference_id>
        <string>Gildea and &#352;tefankovi&#269; (2007)</string>
        <sentence_id>21511</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>9</reference_id>
        <string>Graham 1972</string>
        <sentence_id>21163</sentence_id>
        <char_offset>39</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>10</reference_id>
        <string>(2007)</string>
        <sentence_id>21471</sentence_id>
        <char_offset>80</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>10</reference_id>
        <string>(2007)</string>
        <sentence_id>21511</sentence_id>
        <char_offset>23</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>11</reference_id>
        <string>Huang and Chiang 2005</string>
        <sentence_id>21064</sentence_id>
        <char_offset>40</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>15</reference_id>
        <string>Knuth 1977</string>
        <sentence_id>21455</sentence_id>
        <char_offset>77</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>17</reference_id>
        <string>Melamed (2003)</string>
        <sentence_id>21027</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>17</reference_id>
        <string>Melamed (2003)</string>
        <sentence_id>21330</sentence_id>
        <char_offset>200</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>17</reference_id>
        <string>Melamed (2003)</string>
        <sentence_id>21381</sentence_id>
        <char_offset>11</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>18</reference_id>
        <string>Mihalcea and Pederson 2003</string>
        <sentence_id>21285</sentence_id>
        <char_offset>94</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>20</reference_id>
        <string>Och and Ney 2004</string>
        <sentence_id>21316</sentence_id>
        <char_offset>129</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>21</reference_id>
        <string>Rambow and Satta (1999)</string>
        <sentence_id>21330</sentence_id>
        <char_offset>134</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>22</reference_id>
        <string>Rounds 1970</string>
        <sentence_id>21072</sentence_id>
        <char_offset>105</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>23</reference_id>
        <string>Satta and Peserico 2005</string>
        <sentence_id>21089</sentence_id>
        <char_offset>122</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>23</reference_id>
        <string>Satta and Peserico (2005)</string>
        <sentence_id>21100</sentence_id>
        <char_offset>32</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>23</reference_id>
        <string>Satta and Peserico (2005)</string>
        <sentence_id>21380</sentence_id>
        <char_offset>84</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>24</reference_id>
        <string>Schr&#246;der 1870</string>
        <sentence_id>21416</sentence_id>
        <char_offset>206</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>24</reference_id>
        <string>Schr&#246;der 1870</string>
        <sentence_id>21441</sentence_id>
        <char_offset>23</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>25</reference_id>
        <string>Shapiro and Stephens (1991)</string>
        <sentence_id>21250</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>25</reference_id>
        <string>Shapiro and Stephens (1991</string>
        <sentence_id>21161</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>25</reference_id>
        <string>Shapiro and Stephens (1991</string>
        <sentence_id>21250</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>26</reference_id>
        <string>Shieber 2004</string>
        <sentence_id>21078</sentence_id>
        <char_offset>124</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>27</reference_id>
        <string>Shieber and Schabes (1990)</string>
        <sentence_id>21330</sentence_id>
        <char_offset>103</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>29</reference_id>
        <string>Wu 1996</string>
        <sentence_id>21056</sentence_id>
        <char_offset>34</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>29</reference_id>
        <string>Wu 1996</string>
        <sentence_id>21071</sentence_id>
        <char_offset>153</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>30</reference_id>
        <string>Wu (1997</string>
        <sentence_id>21089</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>30</reference_id>
        <string>Wu (1997</string>
        <sentence_id>21132</sentence_id>
        <char_offset>128</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>30</reference_id>
        <string>Wu (1997</string>
        <sentence_id>21155</sentence_id>
        <char_offset>52</char_offset>
      </citation>
      <citation>
        <id>42</id>
        <reference_id>30</reference_id>
        <string>Wu (1997</string>
        <sentence_id>21250</sentence_id>
        <char_offset>32</char_offset>
      </citation>
      <citation>
        <id>43</id>
        <reference_id>30</reference_id>
        <string>Wu (1997</string>
        <sentence_id>21280</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>44</id>
        <reference_id>30</reference_id>
        <string>Wu 1997</string>
        <sentence_id>21028</sentence_id>
        <char_offset>138</char_offset>
      </citation>
      <citation>
        <id>45</id>
        <reference_id>30</reference_id>
        <string>Wu 1997</string>
        <sentence_id>21148</sentence_id>
        <char_offset>87</char_offset>
      </citation>
      <citation>
        <id>46</id>
        <reference_id>30</reference_id>
        <string>Wu (1997)</string>
        <sentence_id>21089</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>47</id>
        <reference_id>30</reference_id>
        <string>Wu (1997)</string>
        <sentence_id>21132</sentence_id>
        <char_offset>128</char_offset>
      </citation>
      <citation>
        <id>48</id>
        <reference_id>31</reference_id>
        <string>Zhang and Gildea (2007)</string>
        <sentence_id>21471</sentence_id>
        <char_offset>63</char_offset>
      </citation>
    </citations>
  </content>
</document>
