<document>
  <filename>D09-1023</filename>
  <authors>
    <author>Kevin Gimpel</author>
  </authors>
  <title>Feature-Rich Translation by Quasi-Synchronous Lattice Parsing</title>
  <content>
    <sections>
      <section>
        <index>0</index>
        <title>Abstract</title>
        <text>We present a machine translation framework that can incorporate arbitrary features of both input and output sentences. The core of the approach is a novel decoder based on lattice parsing with quasisynchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic. Using generic approximate dynamic programming techniques, this decoder can handle &#8220;non-local&#8221; features. Similar approximate inference techniques support efficient parameter estimation with hidden variables. We use the decoder to conduct controlled experiments on a German-to-English translation task, to compare lexical phrase, syntax, and combined models, and to measure effects of various restrictions on nonisomorphism.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.</text>
              <doc_id>0</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The core of the approach is a novel decoder based on lattice parsing with quasisynchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.</text>
              <doc_id>1</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Using generic approximate dynamic programming techniques, this decoder can handle &#8220;non-local&#8221; features.</text>
              <doc_id>2</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Similar approximate inference techniques support efficient parameter estimation with hidden variables.</text>
              <doc_id>3</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We use the decoder to conduct controlled experiments on a German-to-English translation task, to compare lexical phrase, syntax, and combined models, and to measure effects of various restrictions on nonisomorphism.</text>
              <doc_id>4</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>1</index>
        <title>1 Introduction</title>
        <text>We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms. 1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with &#8220;non-local&#8221; features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen-
1 Informally, features are &#8220;parts&#8221; of a parallel sentence pair
and/or their mutual derivation structure (trees, alignments, etc.). Features are often implied by a choice of formalism.
dence assumptions they imply) from inference algorithms in MT; this separation is widely appreciated in machine learning. Here we take first steps toward such a &#8220;universal&#8221; decoder, making the following contributions:
Arbitrary feature model (&#167;2): We define a single, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments. The trees are optional and can be easily removed, allowing simulation of &#8220;string-to-tree,&#8221; &#8220;tree-to-string,&#8221; &#8220;treeto-tree,&#8221; and &#8220;phrase-based&#8221; models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model.
Decoding as QG parsing (&#167;3&#8211;4): We present a novel decoder based on lattice parsing with quasisynchronous grammar (QG; Smith and Eisner, 2006). 2 Further, we exploit generic approximate inference techniques to incorporate arbitrary &#8220;nonlocal&#8221; features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009). Parameter estimation (&#167;5): We exploit similar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model. Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (&#167;6): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel com-
2 To date, QG has been used for word alignment (Smith
and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT.
bination of the two. We quantify the effects of our approximate inference. We explore the effects of various ways of restricting syntactic non-isomorphism between source and target trees through the QG. We do not report state-of-the-art performance, but these experiments reveal interesting trends that will inform continued research.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.</text>
              <doc_id>5</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001).</text>
              <doc_id>6</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Hence a tension is visible in the many recent research efforts aiming to decode with &#8220;non-local&#8221; features (Chiang, 2007; Huang and Chiang, 2007).</text>
              <doc_id>7</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Lopez (2009) recently argued for a separation between features/formalisms (and the indepen-</text>
              <doc_id>8</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>1 Informally, features are &#8220;parts&#8221; of a parallel sentence pair</text>
              <doc_id>9</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and/or their mutual derivation structure (trees, alignments, etc.).</text>
              <doc_id>10</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Features are often implied by a choice of formalism.</text>
              <doc_id>11</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>dence assumptions they imply) from inference algorithms in MT; this separation is widely appreciated in machine learning.</text>
              <doc_id>12</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Here we take first steps toward such a &#8220;universal&#8221; decoder, making the following contributions:</text>
              <doc_id>13</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Arbitrary feature model (&#167;2): We define a single, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments.</text>
              <doc_id>14</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>The trees are optional and can be easily removed, allowing simulation of &#8220;string-to-tree,&#8221; &#8220;tree-to-string,&#8221; &#8220;treeto-tree,&#8221; and &#8220;phrase-based&#8221; models, among many others.</text>
              <doc_id>15</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model.</text>
              <doc_id>16</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Decoding as QG parsing (&#167;3&#8211;4): We present a novel decoder based on lattice parsing with quasisynchronous grammar (QG; Smith and Eisner, 2006).</text>
              <doc_id>17</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2 Further, we exploit generic approximate inference techniques to incorporate arbitrary &#8220;nonlocal&#8221; features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).</text>
              <doc_id>18</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Parameter estimation (&#167;5): We exploit similar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.</text>
              <doc_id>19</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Because we start with inference (the key subroutine in training), many other learning algorithms are possible.</text>
              <doc_id>20</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Experimental platform (&#167;6): The flexibility of our model/decoder permits carefully controlled experiments.</text>
              <doc_id>21</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We compare lexical phrase and dependency syntax features, as well as a novel com-</text>
              <doc_id>22</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>2 To date, QG has been used for word alignment (Smith</text>
              <doc_id>23</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT.</text>
              <doc_id>24</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>bination of the two.</text>
              <doc_id>25</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We quantify the effects of our approximate inference.</text>
              <doc_id>26</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We explore the effects of various ways of restricting syntactic non-isomorphism between source and target trees through the QG.</text>
              <doc_id>27</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We do not report state-of-the-art performance, but these experiments reveal interesting trends that will inform continued research.</text>
              <doc_id>28</doc_id>
              <sec_id>3</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>2</index>
        <title>2 Model</title>
        <text>(Table 1 explains notation.) Given a sentence s and its parse tree &#964; s , we formulate the translation problem as finding the target sentence t &#8727; (along with its parse tree &#964;t &#8727; and alignment a &#8727; to the source tree) such that 3
&#12296;t &#8727; , &#964; &#8727; t , a &#8727; &#12297; = argmax
&#12296;t,&#964; t ,a&#12297;
p(t, &#964; t , a | s, &#964; s ) (1)
In order to include overlapping features and permit hidden variables during training, we use a single globally-normalized conditional log-linear model. That is, p(t, &#964; t , a | s, &#964; s ) =
exp{&#952; &#8868; g(s, &#964; s , a, t, &#964; t )}
&#8721;a &#8242; ,t &#8242; ,&#964; &#8242;t exp{&#952;&#8868; g(s, &#964; s , a &#8242; , t &#8242; , &#964; &#8242; t )} (2)
where the g are arbitrary feature functions and the &#952; are feature weights. If one or both parse trees or the word alignments are unavailable, they can be ignored or marginalized out as hidden variables. In a log-linear model over structured objects, the choice of feature functions g has a huge effect
3 We assume in this work that s is parsed. In principle, we
might include source-side parsing as part of decoding.
on the feasibility of inference, including decoding. Typically these feature functions are chosen to factor into local parts of the overall structure. We next define some key features used in current MT systems, explaining how they factor. We will use subscripts on g to denote different groups of features, which may depend on subsets of the structures t, &#964; t , a, s, and &#964; s . When these features factor into parts, we will use f to denote the factored vectors, so that if x is an object that breaks into parts {x i } i , then g(x) = &#8721; i f(x i). 4</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>(Table 1 explains notation.</text>
              <doc_id>29</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>) Given a sentence s and its parse tree &#964; s , we formulate the translation problem as finding the target sentence t &#8727; (along with its parse tree &#964;t &#8727; and alignment a &#8727; to the source tree) such that 3</text>
              <doc_id>30</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#12296;t &#8727; , &#964; &#8727; t , a &#8727; &#12297; = argmax</text>
              <doc_id>31</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#12296;t,&#964; t ,a&#12297;</text>
              <doc_id>32</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>p(t, &#964; t , a | s, &#964; s ) (1)</text>
              <doc_id>33</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>In order to include overlapping features and permit hidden variables during training, we use a single globally-normalized conditional log-linear model.</text>
              <doc_id>34</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>That is, p(t, &#964; t , a | s, &#964; s ) =</text>
              <doc_id>35</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>exp{&#952; &#8868; g(s, &#964; s , a, t, &#964; t )}</text>
              <doc_id>36</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8721;a &#8242; ,t &#8242; ,&#964; &#8242;t exp{&#952;&#8868; g(s, &#964; s , a &#8242; , t &#8242; , &#964; &#8242; t )} (2)</text>
              <doc_id>37</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>where the g are arbitrary feature functions and the &#952; are feature weights.</text>
              <doc_id>38</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>If one or both parse trees or the word alignments are unavailable, they can be ignored or marginalized out as hidden variables.</text>
              <doc_id>39</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>In a log-linear model over structured objects, the choice of feature functions g has a huge effect</text>
              <doc_id>40</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>3 We assume in this work that s is parsed.</text>
              <doc_id>41</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>In principle, we</text>
              <doc_id>42</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>might include source-side parsing as part of decoding.</text>
              <doc_id>43</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>on the feasibility of inference, including decoding.</text>
              <doc_id>44</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Typically these feature functions are chosen to factor into local parts of the overall structure.</text>
              <doc_id>45</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We next define some key features used in current MT systems, explaining how they factor.</text>
              <doc_id>46</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>We will use subscripts on g to denote different groups of features, which may depend on subsets of the structures t, &#964; t , a, s, and &#964; s .</text>
              <doc_id>47</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>When these features factor into parts, we will use f to denote the factored vectors, so that if x is an object that breaks into parts {x i } i , then g(x) = &#8721; i f(x i).</text>
              <doc_id>48</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>4</text>
              <doc_id>49</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>2.1 Lexical Translations</title>
            <text>Classical lexical translation features depend on s and t and the alignment a between them. The simplest are word-to-word features, estimated as the conditional probabilities p(t | s) and p(s | t) for s &#8712; &#931; and t &#8712; T. Phrase-to-phrase features generalize these, estimated as p(t &#8242; | s &#8242; ) and p(s &#8242; | t &#8242; ) where s &#8242; (respectively, t &#8242; ) is a substring of s (t).
A major difference between the phrase features used in this work and those used elsewhere is that we do not assume that phrases segment into disjoint parts of the source and target sentences
4 There are two conventional definitions of feature functions. One is to let the range of these functions be conditional probability estimates (Och and Ney, 2002). These estimates are usually heuristic and inconsistent (Koehn et al., 2003). An alternative is to instantiate features for different structural patterns (Liang et al., 2006; Blunsom et al., 2008). This offers more expressive power but may require much more training data to avoid overfitting. For this reason, and to keep training fast, we opt for the former convention, though our decoder can handle both, and the factorings we describe are agnostic about this choice.
(Koehn et al., 2003); they can overlap. 5 Additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005; Ittycheriah and Roukos, 2007). Lexical translation features factor as in Eq. 3 (Tab. 2). We score all phrase pairs in a sentence pair that pair a target phrase with the smallest source phrase that contains all of the alignments in the target phrase; if &#8899; k:i&#8804;k&#8804;j a(k) = &#8709;, no phrase feature fires for t j i .</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Classical lexical translation features depend on s and t and the alignment a between them.</text>
                  <doc_id>50</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The simplest are word-to-word features, estimated as the conditional probabilities p(t | s) and p(s | t) for s &#8712; &#931; and t &#8712; T. Phrase-to-phrase features generalize these, estimated as p(t &#8242; | s &#8242; ) and p(s &#8242; | t &#8242; ) where s &#8242; (respectively, t &#8242; ) is a substring of s (t).</text>
                  <doc_id>51</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>A major difference between the phrase features used in this work and those used elsewhere is that we do not assume that phrases segment into disjoint parts of the source and target sentences</text>
                  <doc_id>52</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>4 There are two conventional definitions of feature functions.</text>
                  <doc_id>53</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>One is to let the range of these functions be conditional probability estimates (Och and Ney, 2002).</text>
                  <doc_id>54</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>These estimates are usually heuristic and inconsistent (Koehn et al., 2003).</text>
                  <doc_id>55</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>An alternative is to instantiate features for different structural patterns (Liang et al., 2006; Blunsom et al., 2008).</text>
                  <doc_id>56</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>This offers more expressive power but may require much more training data to avoid overfitting.</text>
                  <doc_id>57</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For this reason, and to keep training fast, we opt for the former convention, though our decoder can handle both, and the factorings we describe are agnostic about this choice.</text>
                  <doc_id>58</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>(Koehn et al., 2003); they can overlap.</text>
                  <doc_id>59</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>5 Additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005; Ittycheriah and Roukos, 2007).</text>
                  <doc_id>60</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Lexical translation features factor as in Eq.</text>
                  <doc_id>61</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>3 (Tab.</text>
                  <doc_id>62</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>2).</text>
                  <doc_id>63</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>We score all phrase pairs in a sentence pair that pair a target phrase with the smallest source phrase that contains all of the alignments in the target phrase; if &#8899; k:i&#8804;k&#8804;j a(k) = &#8709;, no phrase feature fires for t j i .</text>
                  <doc_id>64</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>2.2 N-gram Language Model</title>
            <text>N-gram language models have become standard in machine translation systems. For bigrams and trigrams (used in this paper), the factoring is in Eq. 4 (Tab. 2).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>N-gram language models have become standard in machine translation systems.</text>
                  <doc_id>65</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For bigrams and trigrams (used in this paper), the factoring is in Eq.</text>
                  <doc_id>66</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>4 (Tab.</text>
                  <doc_id>67</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>2).</text>
                  <doc_id>68</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>2.3 Target Syntax</title>
            <text>There have been many features proposed that consider source- and target-language syntax during translation. Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008).
In this work, we focus on syntactic features of target-side dependency trees, &#964; t , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2). Features that consider only target-side syntax and words without considering s can be seen as &#8220;syntactic language model&#8221; features (Shen et al., 2008).
5 Segmentation might be modeled as a hidden variable in
future work.
g trans (s, a, t) = P m P
j=1 i&#8712;a(j) f lex (s i, t j) (3)
+ P i,j:1&#8804;i&lt;j&#8804;m f phr (s last(i,j) first(i,j) , tj i ) g lm (t) = P P m+1
N&#8712;{2,3} j=1 f N (t j j&#8722;N+1 ) (4)
g syn (t, &#964; t) = P m
j=1 f att(t j, j, t &#964;t (j), &#964; t(j))
+f val (t j, j, &#964; &#8722;1 t (j)) (5) g reor (s, &#964; s, a, t, &#964; t) = P m
P
j=1 i&#8712;a(j) f dist(i, j) (6)
mX g tree 2(&#964; s, a, &#964; t) = f qg (a(j), a(&#964; t(j)), j, &#964; t(j)) (7)
j=1</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>There have been many features proposed that consider source- and target-language syntax during translation.</text>
                  <doc_id>69</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible.</text>
                  <doc_id>70</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence.</text>
                  <doc_id>71</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008).</text>
                  <doc_id>72</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008).</text>
                  <doc_id>73</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>In this work, we focus on syntactic features of target-side dependency trees, &#964; t , along with the words t.</text>
                  <doc_id>74</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These include attachment features that relate a word to its syntactic parent, and valence features.</text>
                  <doc_id>75</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>They factor as in Eq.</text>
                  <doc_id>76</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>5 (Tab.</text>
                  <doc_id>77</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>2).</text>
                  <doc_id>78</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Features that consider only target-side syntax and words without considering s can be seen as &#8220;syntactic language model&#8221; features (Shen et al., 2008).</text>
                  <doc_id>79</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>5 Segmentation might be modeled as a hidden variable in</text>
                  <doc_id>80</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>future work.</text>
                  <doc_id>81</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>g trans (s, a, t) = P m P</text>
                  <doc_id>82</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=1 i&#8712;a(j) f lex (s i, t j) (3)</text>
                  <doc_id>83</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>+ P i,j:1&#8804;i&lt;j&#8804;m f phr (s last(i,j) first(i,j) , tj i ) g lm (t) = P P m+1</text>
                  <doc_id>84</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>N&#8712;{2,3} j=1 f N (t j j&#8722;N+1 ) (4)</text>
                  <doc_id>85</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>g syn (t, &#964; t) = P m</text>
                  <doc_id>86</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=1 f att(t j, j, t &#964;t (j), &#964; t(j))</text>
                  <doc_id>87</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>+f val (t j, j, &#964; &#8722;1 t (j)) (5) g reor (s, &#964; s, a, t, &#964; t) = P m</text>
                  <doc_id>88</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>P</text>
                  <doc_id>89</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=1 i&#8712;a(j) f dist(i, j) (6)</text>
                  <doc_id>90</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>mX g tree 2(&#964; s, a, &#964; t) = f qg (a(j), a(&#964; t(j)), j, &#964; t(j)) (7)</text>
                  <doc_id>91</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>j=1</text>
                  <doc_id>92</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>2.4 Reordering</title>
            <text>Reordering features take many forms in MT. In phrase-based systems, reordering is accomplished both within phrase pairs (local reordering) as well as through distance-based distortion models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2007). In syntax-based systems, reordering is typically parameterized by grammar rules. For generality we permit these features to &#8220;see&#8221; all structures and denote them g reor (s, &#964; s , a, t, &#964; t ). Eq. 6 (Tab. 2) shows a factoring of reordering features based on absolute positions of aligned words.
We turn next to the &#8220;backbone&#8221; model for our decoder; the formalism and the properties of its decoding algorithm will inspire two additional sets of features.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Reordering features take many forms in MT.</text>
                  <doc_id>93</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In phrase-based systems, reordering is accomplished both within phrase pairs (local reordering) as well as through distance-based distortion models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2007).</text>
                  <doc_id>94</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>In syntax-based systems, reordering is typically parameterized by grammar rules.</text>
                  <doc_id>95</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For generality we permit these features to &#8220;see&#8221; all structures and denote them g reor (s, &#964; s , a, t, &#964; t ).</text>
                  <doc_id>96</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Eq.</text>
                  <doc_id>97</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>6 (Tab.</text>
                  <doc_id>98</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>2) shows a factoring of reordering features based on absolute positions of aligned words.</text>
                  <doc_id>99</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>We turn next to the &#8220;backbone&#8221; model for our decoder; the formalism and the properties of its decoding algorithm will inspire two additional sets of features.</text>
                  <doc_id>100</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>3</index>
        <title>3 Quasi-Synchronous Grammars</title>
        <text>A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, &#964; t , a | s, &#964; s ). Given a source sentence s and its parse &#964; s , a QDG induces a probabilistic monolingual dependency grammar over sentences &#8220;inspired&#8221; by the source sentence and tree. We denote this grammar by G s,&#964;s ; its (weighted) language is the set of translations of s. Each word generated by G s,&#964;s is annotated with a &#8220;sense,&#8221; which consists of zero or more words from s. The senses imply an alignment (a) between words in t and words in s, or equivalently, between nodes in &#964; t and nodes in &#964; s . In principle, any portion of &#964; t may align to any portion of &#964; s , but in practice we often make restrictions on the alignments to simplify computation. Smith and Eisner, for example, restricted |a(j)| for all words
t j to be at most one, so that each target word aligned to at most one source word, which we also do here. 6 Which translations are possible depends heavily on the configurations that the QDG permits. Formally, for a parent-child pair &#12296;t &#964;t (j), t j &#12297; in &#964; t , we consider the relationship between a(&#964; t (j)) and a(j), the source-side words to which t &#964;t (j) and t j align. If, for example, we require that, for all j, a(&#964; t (j)) = &#964; s (a(j)) or a(j) = 0, and that the root of &#964; t must align to the root of &#964; s or to NULL, then strict isomorphism must hold between &#964; s and &#964; t , and we have implemented a synchronous CF dependency grammar (Alshawi et al., 2000; Ding and Palmer, 2005). Smith and Eisner (2006) grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment. (&#8220;a(&#964; t (j)) = &#964; s (a(j))&#8221; corresponds to their &#8220;parent-child&#8221; configuration; see Fig. 3 in Smith and Eisner (2006) for illustrations of the rest.) More generally, we can define features on tree pairs that factor into these local configurations, as shown in Eq. 7 (Tab. 2). Note that the QDG instantiates the model in Eq. 2. Of the features discussed in &#167;2, f lex , f att , f val , and f dist can be easily incorporated into the QDG as described while respecting the independence assumptions implied by the configuration features. The others (f phr , f 2 , and f 3 ) are nonlocal, or involve parts of the structure that, from the QDG&#8217;s perspective, are conditionally independent given intervening material. Note that &#8220;nonlocality&#8221; is relative to a choice of formalism; in &#167;2 we did not commit to any formalism, so it is only now that we can describe phrase and N-gram features as non-local. Non-local features will present a challenge for decoding and training (&#167;4.3).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, &#964; t , a | s, &#964; s ).</text>
              <doc_id>101</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Given a source sentence s and its parse &#964; s , a QDG induces a probabilistic monolingual dependency grammar over sentences &#8220;inspired&#8221; by the source sentence and tree.</text>
              <doc_id>102</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We denote this grammar by G s,&#964;s ; its (weighted) language is the set of translations of s.</text>
              <doc_id>103</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>Each word generated by G s,&#964;s is annotated with a &#8220;sense,&#8221; which consists of zero or more words from s.</text>
              <doc_id>104</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The senses imply an alignment (a) between words in t and words in s, or equivalently, between nodes in &#964; t and nodes in &#964; s .</text>
              <doc_id>105</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>In principle, any portion of &#964; t may align to any portion of &#964; s , but in practice we often make restrictions on the alignments to simplify computation.</text>
              <doc_id>106</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Smith and Eisner, for example, restricted |a(j)| for all words</text>
              <doc_id>107</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>t j to be at most one, so that each target word aligned to at most one source word, which we also do here.</text>
              <doc_id>108</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>6 Which translations are possible depends heavily on the configurations that the QDG permits.</text>
              <doc_id>109</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Formally, for a parent-child pair &#12296;t &#964;t (j), t j &#12297; in &#964; t , we consider the relationship between a(&#964; t (j)) and a(j), the source-side words to which t &#964;t (j) and t j align.</text>
              <doc_id>110</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>If, for example, we require that, for all j, a(&#964; t (j)) = &#964; s (a(j)) or a(j) = 0, and that the root of &#964; t must align to the root of &#964; s or to NULL, then strict isomorphism must hold between &#964; s and &#964; t , and we have implemented a synchronous CF dependency grammar (Alshawi et al., 2000; Ding and Palmer, 2005).</text>
              <doc_id>111</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>Smith and Eisner (2006) grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment.</text>
              <doc_id>112</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>(&#8220;a(&#964; t (j)) = &#964; s (a(j))&#8221; corresponds to their &#8220;parent-child&#8221; configuration; see Fig.</text>
              <doc_id>113</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>3 in Smith and Eisner (2006) for illustrations of the rest.</text>
              <doc_id>114</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>) More generally, we can define features on tree pairs that factor into these local configurations, as shown in Eq.</text>
              <doc_id>115</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>7 (Tab.</text>
              <doc_id>116</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>2).</text>
              <doc_id>117</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>Note that the QDG instantiates the model in Eq.</text>
              <doc_id>118</doc_id>
              <sec_id>10</sec_id>
            </sentence>
            <sentence>
              <text>2.</text>
              <doc_id>119</doc_id>
              <sec_id>11</sec_id>
            </sentence>
            <sentence>
              <text>Of the features discussed in &#167;2, f lex , f att , f val , and f dist can be easily incorporated into the QDG as described while respecting the independence assumptions implied by the configuration features.</text>
              <doc_id>120</doc_id>
              <sec_id>12</sec_id>
            </sentence>
            <sentence>
              <text>The others (f phr , f 2 , and f 3 ) are nonlocal, or involve parts of the structure that, from the QDG&#8217;s perspective, are conditionally independent given intervening material.</text>
              <doc_id>121</doc_id>
              <sec_id>13</sec_id>
            </sentence>
            <sentence>
              <text>Note that &#8220;nonlocality&#8221; is relative to a choice of formalism; in &#167;2 we did not commit to any formalism, so it is only now that we can describe phrase and N-gram features as non-local.</text>
              <doc_id>122</doc_id>
              <sec_id>14</sec_id>
            </sentence>
            <sentence>
              <text>Non-local features will present a challenge for decoding and training (&#167;4.3).</text>
              <doc_id>123</doc_id>
              <sec_id>15</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>4</index>
        <title>4 Decoding</title>
        <text>Given a sentence s and its parse &#964; s , at decoding time we seek the target sentence t &#8727; , the target tree &#964; &#8727; t , and the alignments a&#8727; that are most probable, as defined in Eq. 1. 7 (In &#167;5 we will consider k- best and all-translations variations on this prob-
6 I.e., from here on, a : {1, . . . , m} &#8594; {0, . . . , n} where
0 denotes alignment to NULL. 7 Arguably, we seek argmax t p(t | s), marginalizing out
everything else. Approximate solutions have been proposed for that problem in several settings (Blunsom and Osborne, 2008; Sun and Tsujii, 2009); we leave their combination with our approach to future work.
lem.) As usual, the normalization constant is not required for decoding; it suffices to solve:
&#12296;t &#8727; , &#964;t &#8727; , a &#8727; &#12297; = argmax &#952; &#8868; g(s, &#964; s , a, t, &#964; t ) (8)
&#12296;t,&#964; t ,a&#12297;
For a QDG model, the decoding problem has not been addressed before. It equates to finding the most probable derivation under the s/&#964; s -specific grammar G s,&#964;s . We solve this by lattice parsing, assuming that an upper bound on m (the length of t) is known. The advantage offered by this approach (like most other grammar-based translation approaches) is that decoding becomes dynamic programming (DP), a technique that is both widely understood in NLP and for which practical, efficient, generic techniques exist. A major advantage of DP is that, with small modifications, summing over structures is also possible with &#8220;inside&#8221; DP algorithms. We will exploit this in training (&#167;5). Efficient summing opens up many possibilities for training &#952;, such as likelihood and pseudolikelihood, and provides principled ways to handle hidden variables during learning.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Given a sentence s and its parse &#964; s , at decoding time we seek the target sentence t &#8727; , the target tree &#964; &#8727; t , and the alignments a&#8727; that are most probable, as defined in Eq.</text>
              <doc_id>124</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>1.</text>
              <doc_id>125</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>7 (In &#167;5 we will consider k- best and all-translations variations on this prob-</text>
              <doc_id>126</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>6 I.e., from here on, a : {1, .</text>
              <doc_id>127</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>128</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>129</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>, m} &#8594; {0, .</text>
              <doc_id>130</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>131</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>.</text>
              <doc_id>132</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>, n} where</text>
              <doc_id>133</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>0 denotes alignment to NULL.</text>
              <doc_id>134</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>7 Arguably, we seek argmax t p(t | s), marginalizing out</text>
              <doc_id>135</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>everything else.</text>
              <doc_id>136</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Approximate solutions have been proposed for that problem in several settings (Blunsom and Osborne, 2008; Sun and Tsujii, 2009); we leave their combination with our approach to future work.</text>
              <doc_id>137</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>lem.</text>
              <doc_id>138</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>) As usual, the normalization constant is not required for decoding; it suffices to solve:</text>
              <doc_id>139</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#12296;t &#8727; , &#964;t &#8727; , a &#8727; &#12297; = argmax &#952; &#8868; g(s, &#964; s , a, t, &#964; t ) (8)</text>
              <doc_id>140</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#12296;t,&#964; t ,a&#12297;</text>
              <doc_id>141</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>For a QDG model, the decoding problem has not been addressed before.</text>
              <doc_id>142</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>It equates to finding the most probable derivation under the s/&#964; s -specific grammar G s,&#964;s .</text>
              <doc_id>143</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>We solve this by lattice parsing, assuming that an upper bound on m (the length of t) is known.</text>
              <doc_id>144</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>The advantage offered by this approach (like most other grammar-based translation approaches) is that decoding becomes dynamic programming (DP), a technique that is both widely understood in NLP and for which practical, efficient, generic techniques exist.</text>
              <doc_id>145</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>A major advantage of DP is that, with small modifications, summing over structures is also possible with &#8220;inside&#8221; DP algorithms.</text>
              <doc_id>146</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>We will exploit this in training (&#167;5).</text>
              <doc_id>147</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>Efficient summing opens up many possibilities for training &#952;, such as likelihood and pseudolikelihood, and provides principled ways to handle hidden variables during learning.</text>
              <doc_id>148</doc_id>
              <sec_id>6</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>4.1 Translation as Monolingual Parsing</title>
            <text>We decode by performing lattice parsing on a lattice encoding the set of possible translations. The lattice is a weighted &#8220;sausage&#8221; lattice that permits sentences up to some maximum length l; l is derived from the source sentence length. Let the states be numbered 0 to l; states from &#8970;&#961;l&#8971; to l are final states (for some &#961; &#8712; (0, 1)). For every position between consecutive states j &#8722; 1 and j (0 &lt; j &#8804; l), and for every word s i in s, and for every word t &#8712; Trans(s i ), we instantiate an arc annotated with t and i. The weight of such an arc is exp{&#952; &#8868; f}, where f is the sum of feature functions that fire when s i translates as t in target position j (e.g., f lex (s i , t) and f dist (i, j)).
Given the lattice and G s,&#964;s , lattice parsing is a straightforward generalization of standard context-free dependency parsing DP algorithms (Eisner, 1997). This decoder accounts for f lex , f att , f val , f dist , and f qg as local features.
Figure 1 gives an example, showing a German sentence and dependency tree from an automatic parser, an English reference, and a lattice representing possible translations. In each bundle, the arcs are listed in decreasing order according to weight and for clarity only the first five are shown. The output of the decoder consists of lattice arcs
Source: $ konnten sie es &#252;bersetzen ?
Reference: could you translate it ?
Decoder output:
$
konnten:could
sie:you
konnten:couldn konnten:might
es:it
...
konnten:could
sie:you
es:it
es:it
konnten:could
sie:you
&#252;bersetzen: translate &#252;bersetzen: translated &#252;bersetzen: translate &#252;bersetzen: translated
?:? ?:?
selected at each position and a dependency tree over them.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We decode by performing lattice parsing on a lattice encoding the set of possible translations.</text>
                  <doc_id>149</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The lattice is a weighted &#8220;sausage&#8221; lattice that permits sentences up to some maximum length l; l is derived from the source sentence length.</text>
                  <doc_id>150</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Let the states be numbered 0 to l; states from &#8970;&#961;l&#8971; to l are final states (for some &#961; &#8712; (0, 1)).</text>
                  <doc_id>151</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>For every position between consecutive states j &#8722; 1 and j (0 &lt; j &#8804; l), and for every word s i in s, and for every word t &#8712; Trans(s i ), we instantiate an arc annotated with t and i.</text>
                  <doc_id>152</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The weight of such an arc is exp{&#952; &#8868; f}, where f is the sum of feature functions that fire when s i translates as t in target position j (e.g., f lex (s i , t) and f dist (i, j)).</text>
                  <doc_id>153</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Given the lattice and G s,&#964;s , lattice parsing is a straightforward generalization of standard context-free dependency parsing DP algorithms (Eisner, 1997).</text>
                  <doc_id>154</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This decoder accounts for f lex , f att , f val , f dist , and f qg as local features.</text>
                  <doc_id>155</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Figure 1 gives an example, showing a German sentence and dependency tree from an automatic parser, an English reference, and a lattice representing possible translations.</text>
                  <doc_id>156</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In each bundle, the arcs are listed in decreasing order according to weight and for clarity only the first five are shown.</text>
                  <doc_id>157</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The output of the decoder consists of lattice arcs</text>
                  <doc_id>158</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Source: $ konnten sie es &#252;bersetzen ?</text>
                  <doc_id>159</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Reference: could you translate it ?</text>
                  <doc_id>160</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Decoder output:</text>
                  <doc_id>161</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>$</text>
                  <doc_id>162</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>konnten:could</text>
                  <doc_id>163</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>sie:you</text>
                  <doc_id>164</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>konnten:couldn konnten:might</text>
                  <doc_id>165</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>es:it</text>
                  <doc_id>166</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>...</text>
                  <doc_id>167</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>konnten:could</text>
                  <doc_id>168</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>sie:you</text>
                  <doc_id>169</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>es:it</text>
                  <doc_id>170</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>es:it</text>
                  <doc_id>171</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>konnten:could</text>
                  <doc_id>172</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>sie:you</text>
                  <doc_id>173</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#252;bersetzen: translate &#252;bersetzen: translated &#252;bersetzen: translate &#252;bersetzen: translated</text>
                  <doc_id>174</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>?:?</text>
                  <doc_id>175</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>?</text>
                  <doc_id>176</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>:?</text>
                  <doc_id>177</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>selected at each position and a dependency tree over them.</text>
                  <doc_id>178</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>4.2 Source-Side Coverage Features</title>
            <text>Most MT decoders enforce a notion of &#8220;coverage&#8221; of the source sentence during translation: all parts of s should be aligned to some part of t (alignment to NULL incurs an explicit cost). Phrase-based systems such as Moses (Koehn et al., 2007) explicitly search for the highest-scoring string in which all source words are translated. Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in &#964; t (or a deliberate choice is made by the decoder to translate it to NULL). In such systems, we do not need to use features to implement source-side coverage, as it is assumed as a hard constraint always respected by the decoder.
Our QDG decoder has no way to enforce coverage; it does not track any kind of state in &#964; s apart from a single recently aligned word. This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model (Brown et al., 1993). This sacrifice is the result of our choice to use a conditional model (&#167;2).
The solution is to introduce a set of coverage features g cov (a). Here, these include:
&#8226; A counter for the number of times each source word is covered: f scov (a) = &#8721; n
i=1 |a&#8722;1 (i)|.
&#8226; Features that fire once when a source word is
covered the zth time (z &#8712; {2, 3, 4}) and fire again all subsequent times it is covered; these are denoted f 2nd , f 3rd , and f 4th .
&#8226; A counter of uncovered source words: f sunc (a) = &#8721; n
i=1 &#948;(|a&#8722;1 (i)|, 0).
Of these, only f scov is local.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Most MT decoders enforce a notion of &#8220;coverage&#8221; of the source sentence during translation: all parts of s should be aligned to some part of t (alignment to NULL incurs an explicit cost).</text>
                  <doc_id>179</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Phrase-based systems such as Moses (Koehn et al., 2007) explicitly search for the highest-scoring string in which all source words are translated.</text>
                  <doc_id>180</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in &#964; t (or a deliberate choice is made by the decoder to translate it to NULL).</text>
                  <doc_id>181</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In such systems, we do not need to use features to implement source-side coverage, as it is assumed as a hard constraint always respected by the decoder.</text>
                  <doc_id>182</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Our QDG decoder has no way to enforce coverage; it does not track any kind of state in &#964; s apart from a single recently aligned word.</text>
                  <doc_id>183</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model (Brown et al., 1993).</text>
                  <doc_id>184</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This sacrifice is the result of our choice to use a conditional model (&#167;2).</text>
                  <doc_id>185</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The solution is to introduce a set of coverage features g cov (a).</text>
                  <doc_id>186</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Here, these include:</text>
                  <doc_id>187</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; A counter for the number of times each source word is covered: f scov (a) = &#8721; n</text>
                  <doc_id>188</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1 |a&#8722;1 (i)|.</text>
                  <doc_id>189</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; Features that fire once when a source word is</text>
                  <doc_id>190</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>covered the zth time (z &#8712; {2, 3, 4}) and fire again all subsequent times it is covered; these are denoted f 2nd , f 3rd , and f 4th .</text>
                  <doc_id>191</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8226; A counter of uncovered source words: f sunc (a) = &#8721; n</text>
                  <doc_id>192</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1 &#948;(|a&#8722;1 (i)|, 0).</text>
                  <doc_id>193</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Of these, only f scov is local.</text>
                  <doc_id>194</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>4.3 Non-Local Features</title>
            <text>The lattice QDG parsing decoder incorporates many of the features we have discussed, but not all of them. Phrase lexicon features f phr , language model features f N for N &gt; 1, and most coverage features are non-local with respect to our QDG. Recently Chiang (2007) introduced &#8220;cube pruning&#8221; as an approximate decoding method that extends a DP decoder with the ability to incorporate features that break the Markovian independence assumptions DP exploits. Techniques like cube pruning can be used to include the non-local features in our decoder. 8</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The lattice QDG parsing decoder incorporates many of the features we have discussed, but not all of them.</text>
                  <doc_id>195</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Phrase lexicon features f phr , language model features f N for N &gt; 1, and most coverage features are non-local with respect to our QDG.</text>
                  <doc_id>196</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Recently Chiang (2007) introduced &#8220;cube pruning&#8221; as an approximate decoding method that extends a DP decoder with the ability to incorporate features that break the Markovian independence assumptions DP exploits.</text>
                  <doc_id>197</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Techniques like cube pruning can be used to include the non-local features in our decoder.</text>
                  <doc_id>198</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>8</text>
                  <doc_id>199</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>5</index>
        <title>5 Training</title>
        <text>Training requires us to learn values for the parameters &#952; in Eq. 2. Given T training examples of the form &#12296;t (i) , &#964; (i) t , s (i) , &#964; s (i) &#12297;, for i = 1, ..., T , maximum likelihood estimation for this model consists of solving Eq. 9 (Tab. 3). 9 Note that the
8 A full discussion is omitted for space, but in fact we use
&#8220;cube decoding,&#8221; a slightly less approximate, slightly more expensive method that is more closely related to the approximate inference methods we use for training, discussed in &#167;5. 9 In practice, we regularize by including a term &#8722;c&#8214;&#952;&#8214; 2 2.
LL(&#952;) = TX
i=1
&#8220;denominator&#8221; of term 1 in Eq. 10 log p(t (i) , &#964; (i) t
=
S(j, i, t) =
PL(&#952;) =
nX
| s (i) , &#964; (i) s ) =
i=1
i=0 t &#8242; &#8712;Trans(s i )
Y
k&#8712;&#964; &#8722;1 t
X TX
Pa
log exp{&#952;&#8868; g(s (i) , &#964; s (i)
a
, a, t (i) , &#964; (i) t )}
i=1 Pt,&#964; t ,a exp{&#952;&#8868; g(s (i) , &#964; s (i) , a, t, &#964; = t)}
TX &#8222; X &#171; TX &#8222; X
log p(t (i) , a | &#964; (i) t , s (i) , &#964; s (i) ) + log
nX
i=1 a
TX log &#8220;numerator&#8221; &#8220;denominator&#8221;
i=1
p(&#964; (i) t , a | t (i) , s (i) , &#964; s (i) )
S(&#964; &#8722;1 t (0), i, t &#8242; ) &#215; exp n&#952; &#8868; `f lex (s i, t &#8242; ) + f att ($, 0, t &#8242; , k) + f qg (0, i, 0, k)&#180;o
X
(j) i &#8242; =0 t &#8242; &#8712;Trans(s i &#8242; )
j S(k, i &#8242; , t &#8242; ) &#215; exp &#952; &#8868; &#8222; f lex (s i &#8242;, t &#8242; ) + f att (t, j, t &#8242; , k)+ f val (t, j, &#964; &#8722;1 t (j)) + f qg (i, i &#8242; , j, k)
n S(j, i, t) = exp &#952; &#8868; `f val (t, j, &#964; &#8722;1 t (j))&#180;o &#171;
&#171;ff
(9)
(10)
(11)
(12)
if &#964; &#8722;1 t (j) = &#8709; (13)
alignments are treated as a hidden variable to be marginalized out. 10 Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al., 2008). Such problems are typically solved using variations of gradient ascent; in our experiments, we will use an online method called stochastic gradient ascent (SGA). This requires us to calculate the function&#8217;s gradient (vector of first derivatives) with respect to &#952;. 11
Computing the numerator in Eq. 9 involves summing over all possible alignments; with QDG and a hard bound of 1 on |a(j)| for all j, a fast &#8220;inside&#8221; DP solution is known (Smith and Eisner, 2006; Wang et al., 2007). It runs in O(mn 2 ) time and O(mn) space.
Computing the denominator in Eq. 9 requires summing over all word sequences and dependency trees for the target language sentence and all word alignments between the sentences. With a maximum length imposed, this is tractable using the &#8220;inside&#8221; version of the maximizing DP algorithm of Sec. 4, but it is prohibitively expensive. We therefore optimize pseudo-likelihood instead, making the following approximation (Be-
10 Alignments could be supplied by automatic word alignment algorithms. We chose to leave them hidden so that we could make the best use of our parsed training data when configuration constraints are imposed, since it is not always possible to reconcile automatic word alignments with automatic parses. 11 When the function&#8217;s value is computed by &#8220;inside&#8221; DP,
the corresponding &#8220;outside&#8221; algorithm can be used to obtain the gradient. Because outside algorithms can be automatically derived from inside ones, we discuss only inside algorithms in this paper; see Eisner et al. (2005).
sag, 1975):
p(t, &#964; t | s, &#964; s ) &#8776; p(t | &#964; t , s, &#964; s ) &#215; p(&#964; t | t, s, &#964; s )
Plugging this into Eq. 9, we arrive at Eq. 10 (Tab. 3). The two parenthesized terms in Eq. 10 each have their own numerators and denominators (not shown). The numerators are identical to each other and to that in Eq. 9. The denominators are much more manageable than in Eq. 9, never requiring summation over more than two structures at a time. We must sum over target word sequences and word alignments (with fixed &#964; t ), and separately over target trees and word alignments (with fixed t).</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Training requires us to learn values for the parameters &#952; in Eq.</text>
              <doc_id>200</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>2.</text>
              <doc_id>201</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Given T training examples of the form &#12296;t (i) , &#964; (i) t , s (i) , &#964; s (i) &#12297;, for i = 1, ..., T , maximum likelihood estimation for this model consists of solving Eq.</text>
              <doc_id>202</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>9 (Tab.</text>
              <doc_id>203</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>3).</text>
              <doc_id>204</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>9 Note that the</text>
              <doc_id>205</doc_id>
              <sec_id>5</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>8 A full discussion is omitted for space, but in fact we use</text>
              <doc_id>206</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8220;cube decoding,&#8221; a slightly less approximate, slightly more expensive method that is more closely related to the approximate inference methods we use for training, discussed in &#167;5.</text>
              <doc_id>207</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>9 In practice, we regularize by including a term &#8722;c&#8214;&#952;&#8214; 2 2.</text>
              <doc_id>208</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>LL(&#952;) = TX</text>
              <doc_id>209</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1</text>
              <doc_id>210</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#8220;denominator&#8221; of term 1 in Eq.</text>
              <doc_id>211</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>10 log p(t (i) , &#964; (i) t</text>
              <doc_id>212</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>=</text>
              <doc_id>213</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S(j, i, t) =</text>
              <doc_id>214</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>PL(&#952;) =</text>
              <doc_id>215</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>nX</text>
              <doc_id>216</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>| s (i) , &#964; (i) s ) =</text>
              <doc_id>217</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1</text>
              <doc_id>218</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=0 t &#8242; &#8712;Trans(s i )</text>
              <doc_id>219</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Y</text>
              <doc_id>220</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>k&#8712;&#964; &#8722;1 t</text>
              <doc_id>221</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>X TX</text>
              <doc_id>222</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Pa</text>
              <doc_id>223</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>log exp{&#952;&#8868; g(s (i) , &#964; s (i)</text>
              <doc_id>224</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>a</text>
              <doc_id>225</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>, a, t (i) , &#964; (i) t )}</text>
              <doc_id>226</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1 Pt,&#964; t ,a exp{&#952;&#8868; g(s (i) , &#964; s (i) , a, t, &#964; = t)}</text>
              <doc_id>227</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>TX &#8222; X &#171; TX &#8222; X</text>
              <doc_id>228</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>log p(t (i) , a | &#964; (i) t , s (i) , &#964; s (i) ) + log</text>
              <doc_id>229</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>nX</text>
              <doc_id>230</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1 a</text>
              <doc_id>231</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>TX log &#8220;numerator&#8221; &#8220;denominator&#8221;</text>
              <doc_id>232</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>i=1</text>
              <doc_id>233</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>p(&#964; (i) t , a | t (i) , s (i) , &#964; s (i) )</text>
              <doc_id>234</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>S(&#964; &#8722;1 t (0), i, t &#8242; ) &#215; exp n&#952; &#8868; `f lex (s i, t &#8242; ) + f att ($, 0, t &#8242; , k) + f qg (0, i, 0, k)&#180;o</text>
              <doc_id>235</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>X</text>
              <doc_id>236</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(j) i &#8242; =0 t &#8242; &#8712;Trans(s i &#8242; )</text>
              <doc_id>237</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>j S(k, i &#8242; , t &#8242; ) &#215; exp &#952; &#8868; &#8222; f lex (s i &#8242;, t &#8242; ) + f att (t, j, t &#8242; , k)+ f val (t, j, &#964; &#8722;1 t (j)) + f qg (i, i &#8242; , j, k)</text>
              <doc_id>238</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>n S(j, i, t) = exp &#952; &#8868; `f val (t, j, &#964; &#8722;1 t (j))&#180;o &#171;</text>
              <doc_id>239</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>&#171;ff</text>
              <doc_id>240</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(9)</text>
              <doc_id>241</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(10)</text>
              <doc_id>242</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(11)</text>
              <doc_id>243</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>(12)</text>
              <doc_id>244</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>if &#964; &#8722;1 t (j) = &#8709; (13)</text>
              <doc_id>245</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>alignments are treated as a hidden variable to be marginalized out.</text>
              <doc_id>246</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>10 Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al., 2008).</text>
              <doc_id>247</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Such problems are typically solved using variations of gradient ascent; in our experiments, we will use an online method called stochastic gradient ascent (SGA).</text>
              <doc_id>248</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>This requires us to calculate the function&#8217;s gradient (vector of first derivatives) with respect to &#952;.</text>
              <doc_id>249</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>11</text>
              <doc_id>250</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Computing the numerator in Eq.</text>
              <doc_id>251</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>9 involves summing over all possible alignments; with QDG and a hard bound of 1 on |a(j)| for all j, a fast &#8220;inside&#8221; DP solution is known (Smith and Eisner, 2006; Wang et al., 2007).</text>
              <doc_id>252</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>It runs in O(mn 2 ) time and O(mn) space.</text>
              <doc_id>253</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Computing the denominator in Eq.</text>
              <doc_id>254</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>9 requires summing over all word sequences and dependency trees for the target language sentence and all word alignments between the sentences.</text>
              <doc_id>255</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>With a maximum length imposed, this is tractable using the &#8220;inside&#8221; version of the maximizing DP algorithm of Sec.</text>
              <doc_id>256</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>4, but it is prohibitively expensive.</text>
              <doc_id>257</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>We therefore optimize pseudo-likelihood instead, making the following approximation (Be-</text>
              <doc_id>258</doc_id>
              <sec_id>4</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>10 Alignments could be supplied by automatic word alignment algorithms.</text>
              <doc_id>259</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>We chose to leave them hidden so that we could make the best use of our parsed training data when configuration constraints are imposed, since it is not always possible to reconcile automatic word alignments with automatic parses.</text>
              <doc_id>260</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>11 When the function&#8217;s value is computed by &#8220;inside&#8221; DP,</text>
              <doc_id>261</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>the corresponding &#8220;outside&#8221; algorithm can be used to obtain the gradient.</text>
              <doc_id>262</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Because outside algorithms can be automatically derived from inside ones, we discuss only inside algorithms in this paper; see Eisner et al. (2005).</text>
              <doc_id>263</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>sag, 1975):</text>
              <doc_id>264</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>p(t, &#964; t | s, &#964; s ) &#8776; p(t | &#964; t , s, &#964; s ) &#215; p(&#964; t | t, s, &#964; s )</text>
              <doc_id>265</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
          <paragraph>
            <sentence>
              <text>Plugging this into Eq.</text>
              <doc_id>266</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>9, we arrive at Eq.</text>
              <doc_id>267</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>10 (Tab.</text>
              <doc_id>268</doc_id>
              <sec_id>2</sec_id>
            </sentence>
            <sentence>
              <text>3).</text>
              <doc_id>269</doc_id>
              <sec_id>3</sec_id>
            </sentence>
            <sentence>
              <text>The two parenthesized terms in Eq.</text>
              <doc_id>270</doc_id>
              <sec_id>4</sec_id>
            </sentence>
            <sentence>
              <text>10 each have their own numerators and denominators (not shown).</text>
              <doc_id>271</doc_id>
              <sec_id>5</sec_id>
            </sentence>
            <sentence>
              <text>The numerators are identical to each other and to that in Eq.</text>
              <doc_id>272</doc_id>
              <sec_id>6</sec_id>
            </sentence>
            <sentence>
              <text>9.</text>
              <doc_id>273</doc_id>
              <sec_id>7</sec_id>
            </sentence>
            <sentence>
              <text>The denominators are much more manageable than in Eq.</text>
              <doc_id>274</doc_id>
              <sec_id>8</sec_id>
            </sentence>
            <sentence>
              <text>9, never requiring summation over more than two structures at a time.</text>
              <doc_id>275</doc_id>
              <sec_id>9</sec_id>
            </sentence>
            <sentence>
              <text>We must sum over target word sequences and word alignments (with fixed &#964; t ), and separately over target trees and word alignments (with fixed t).</text>
              <doc_id>276</doc_id>
              <sec_id>10</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>5.1 Summing over t and a</title>
            <text>The summation over target word sequences and alignments given fixed &#964; t bears a resemblance to the inside algorithm, except that the tree structure is fixed (Pereira and Schabes, 1992). Let S(j, i, t) denote the sum of all translations rooted at position j in &#964; t such that a(j) = i and t j = t.
Tab. 3 gives the equations for this DP: Eq. 11 is the quantity of interest, Eq. 12 is the recursion, and Eq. 13 shows the base cases for leaves of &#964; t .
Letting q = max 0&#8804;i&#8804;n |Trans(s i )|, this algorithm runs in O(mn 2 q 2 ) time and O(mnq) space. For efficiency we place a hard upper bound on q during training (details in &#167;6).</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>The summation over target word sequences and alignments given fixed &#964; t bears a resemblance to the inside algorithm, except that the tree structure is fixed (Pereira and Schabes, 1992).</text>
                  <doc_id>277</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Let S(j, i, t) denote the sum of all translations rooted at position j in &#964; t such that a(j) = i and t j = t.</text>
                  <doc_id>278</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Tab.</text>
                  <doc_id>279</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>3 gives the equations for this DP: Eq.</text>
                  <doc_id>280</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>11 is the quantity of interest, Eq.</text>
                  <doc_id>281</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>12 is the recursion, and Eq.</text>
                  <doc_id>282</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>13 shows the base cases for leaves of &#964; t .</text>
                  <doc_id>283</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Letting q = max 0&#8804;i&#8804;n |Trans(s i )|, this algorithm runs in O(mn 2 q 2 ) time and O(mnq) space.</text>
                  <doc_id>284</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>For efficiency we place a hard upper bound on q during training (details in &#167;6).</text>
                  <doc_id>285</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>5.2 Summing over &#964; t and a</title>
            <text>For the summation over dependency trees and alignments given fixed t, required for p(&#964; t | t, s, &#964; s ), we perform &#8220;inside&#8221; lattice parsing with G s,&#964;s . The technique is the summing variant of the decoding method in &#167;4, except for each state j,
the sausage lattice only includes arcs from j &#8722; 1 to j that are labeled with the known target word t j . If a is the number of arcs in the lattice, which is O(mn), this algorithm runs in O(a 3 ) time and requires O(a 2 ) space. Because we use a hard upper bound on |Trans(s)| for all s &#8712; &#931;, this summation is much faster in practice than the one over words and alignments.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>For the summation over dependency trees and alignments given fixed t, required for p(&#964; t | t, s, &#964; s ), we perform &#8220;inside&#8221; lattice parsing with G s,&#964;s .</text>
                  <doc_id>286</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The technique is the summing variant of the decoding method in &#167;4, except for each state j,</text>
                  <doc_id>287</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>the sausage lattice only includes arcs from j &#8722; 1 to j that are labeled with the known target word t j .</text>
                  <doc_id>288</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>If a is the number of arcs in the lattice, which is O(mn), this algorithm runs in O(a 3 ) time and requires O(a 2 ) space.</text>
                  <doc_id>289</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Because we use a hard upper bound on |Trans(s)| for all s &#8712; &#931;, this summation is much faster in practice than the one over words and alignments.</text>
                  <doc_id>290</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>5.3 Handling Non-Local Features</title>
            <text>So far, all of our algorithms have exploited DP, disallowing any non-local features (e.g., f phr , f N for N &gt; 1, f zth , f sunc ). We recently proposed &#8220;cube summing,&#8221; an approximate technique that permits the use of non-local features for inside DP algorithms (Gimpel and Smith, 2009). Cube summing is based on a slightly less greedy variation of cube pruning (Chiang, 2007) that maintains k-best lists of derivations for each DP chart item. Cube summing augments the k-best list with a residual term that sums over remaining structures not in the k-best list, albeit without their non-local features. Using the machinery of cube summing, it is straightforward to include the desired non-local features in the summations required for pseudolikelihood, as well as to compute their approximate gradients. Our approach permits an alternative to minimum error-rate training (MERT; Och, 2003); it is discriminative but handles latent structure and regularization in more principled ways. The pseudolikelihood calculations for a sentence pair, taken together, are faster than (k-best) decoding, making SGA&#8217;s inner loop faster than MERT&#8217;s inner loop.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>So far, all of our algorithms have exploited DP, disallowing any non-local features (e.g., f phr , f N for N &gt; 1, f zth , f sunc ).</text>
                  <doc_id>291</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We recently proposed &#8220;cube summing,&#8221; an approximate technique that permits the use of non-local features for inside DP algorithms (Gimpel and Smith, 2009).</text>
                  <doc_id>292</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Cube summing is based on a slightly less greedy variation of cube pruning (Chiang, 2007) that maintains k-best lists of derivations for each DP chart item.</text>
                  <doc_id>293</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Cube summing augments the k-best list with a residual term that sums over remaining structures not in the k-best list, albeit without their non-local features.</text>
                  <doc_id>294</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>Using the machinery of cube summing, it is straightforward to include the desired non-local features in the summations required for pseudolikelihood, as well as to compute their approximate gradients.</text>
                  <doc_id>295</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Our approach permits an alternative to minimum error-rate training (MERT; Och, 2003); it is discriminative but handles latent structure and regularization in more principled ways.</text>
                  <doc_id>296</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>The pseudolikelihood calculations for a sentence pair, taken together, are faster than (k-best) decoding, making SGA&#8217;s inner loop faster than MERT&#8217;s inner loop.</text>
                  <doc_id>297</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>6</index>
        <title>6 Experiments</title>
        <text>Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output.</text>
              <doc_id>298</doc_id>
              <sec_id>0</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections>
          <subsection>
            <index>0</index>
            <title>6.1 Data and Evaluation</title>
            <text>We use the German-English portion of the Basic Travel Expression Corpus (BTEC). The corpus has approximately 100K sentence pairs. We filter sentences of length more than 15 words, which only removes 6% of the data. We end up with a training set of 82,299 sentences, a development set of 934 sentences, and a test set of 500 sentences. We evaluate translation output using case-insensitive BLEU (Papineni et al., 2001), as provided by NIST, and METEOR (Banerjee and Lavie, 2005), version 0.6, with Porter stemming and WordNet synonym matching.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We use the German-English portion of the Basic Travel Expression Corpus (BTEC).</text>
                  <doc_id>299</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The corpus has approximately 100K sentence pairs.</text>
                  <doc_id>300</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We filter sentences of length more than 15 words, which only removes 6% of the data.</text>
                  <doc_id>301</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We end up with a training set of 82,299 sentences, a development set of 934 sentences, and a test set of 500 sentences.</text>
                  <doc_id>302</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We evaluate translation output using case-insensitive BLEU (Papineni et al., 2001), as provided by NIST, and METEOR (Banerjee and Lavie, 2005), version 0.6, with Porter stemming and WordNet synonym matching.</text>
                  <doc_id>303</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>1</index>
            <title>6.2 Features</title>
            <text>Our base system uses features as discussed in &#167;2. To obtain lexical translation features g trans (s, a, t), we use the Moses pipeline (Koehn et al., 2007). We perform word alignment using GIZA++ (Och and Ney, 2003), symmetrize the alignments using the &#8220;grow-diag-final-and&#8221; heuristic, and extract phrases up to length 3. We define f lex by the lexical probabilities p(t | s) and p(s | t) estimated from the symmetrized alignments. After discarding phrase pairs with only one target-side word (since we only allow a target word to align to at most one source word), we define f phr by 8 features: {2, 3} target words &#215; phrase conditional and &#8220;lexical smoothing&#8221; probabilities &#215; two conditional directions.
Bigram and trigam language model features, f 2 and f 3 , are estimated using the SRI toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998).
For our target-language syntactic features g syn , we use features similar to lexicalized CFG events (Collins, 1999), specifically following the dependency model of Klein and Manning (2004). These include probabilities associated with individual attachments (f att ) and child-generation valence probabilities (f val ). These probabilities are estimated on the training corpus parsed using the Stanford factored parser (Klein and Manning, 2003). The same probabilities are also included using 50 hard word classes derived from the parallel corpus using the GIZA++ mkcls utility (Och and Ney, 2003). In total, there are 7 lexical and 7 word-class syntax features.
For reordering, we use a single absolute distortion feature f dist (i, j) that returns |i&#8722;j| whenever a(j) = i and i, j &gt; 0. (Unlike the other feature functions, which returned probabilities, this feature function returns a nonnegative integer.)
The tree-to-tree syntactic features g tree 2 in our model are binary features f qg that fire for particular QG configurations. We use one feature for each of the configurations in (Smith and Eisner, 2006), adding 7 additional features that score configura-
tions involving root words and NULL-alignments more finely. There are 14 features in this category.
Coverage features g cov are as described in &#167;4.2. In all, 46 feature weights are learned.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Our base system uses features as discussed in &#167;2.</text>
                  <doc_id>304</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To obtain lexical translation features g trans (s, a, t), we use the Moses pipeline (Koehn et al., 2007).</text>
                  <doc_id>305</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We perform word alignment using GIZA++ (Och and Ney, 2003), symmetrize the alignments using the &#8220;grow-diag-final-and&#8221; heuristic, and extract phrases up to length 3.</text>
                  <doc_id>306</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>We define f lex by the lexical probabilities p(t | s) and p(s | t) estimated from the symmetrized alignments.</text>
                  <doc_id>307</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>After discarding phrase pairs with only one target-side word (since we only allow a target word to align to at most one source word), we define f phr by 8 features: {2, 3} target words &#215; phrase conditional and &#8220;lexical smoothing&#8221; probabilities &#215; two conditional directions.</text>
                  <doc_id>308</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Bigram and trigam language model features, f 2 and f 3 , are estimated using the SRI toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998).</text>
                  <doc_id>309</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For our target-language syntactic features g syn , we use features similar to lexicalized CFG events (Collins, 1999), specifically following the dependency model of Klein and Manning (2004).</text>
                  <doc_id>310</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>These include probabilities associated with individual attachments (f att ) and child-generation valence probabilities (f val ).</text>
                  <doc_id>311</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>These probabilities are estimated on the training corpus parsed using the Stanford factored parser (Klein and Manning, 2003).</text>
                  <doc_id>312</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The same probabilities are also included using 50 hard word classes derived from the parallel corpus using the GIZA++ mkcls utility (Och and Ney, 2003).</text>
                  <doc_id>313</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>In total, there are 7 lexical and 7 word-class syntax features.</text>
                  <doc_id>314</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>For reordering, we use a single absolute distortion feature f dist (i, j) that returns |i&#8722;j| whenever a(j) = i and i, j &gt; 0.</text>
                  <doc_id>315</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>(Unlike the other feature functions, which returned probabilities, this feature function returns a nonnegative integer.</text>
                  <doc_id>316</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>)</text>
                  <doc_id>317</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>The tree-to-tree syntactic features g tree 2 in our model are binary features f qg that fire for particular QG configurations.</text>
                  <doc_id>318</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We use one feature for each of the configurations in (Smith and Eisner, 2006), adding 7 additional features that score configura-</text>
                  <doc_id>319</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>tions involving root words and NULL-alignments more finely.</text>
                  <doc_id>320</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>There are 14 features in this category.</text>
                  <doc_id>321</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Coverage features g cov are as described in &#167;4.2.</text>
                  <doc_id>322</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In all, 46 feature weights are learned.</text>
                  <doc_id>323</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>2</index>
            <title>6.3 Experimental Procedure</title>
            <text>Our model permits training the system on the full set of parallel data, but we instead use the parallel data to estimate feature functions and learn &#952; on the development set. 12 We trained using three iterations of SGA over the development data with a batch size of 1 and a fixed step size of 0.01. We used l 2 regularization with a fixed, untuned coefficient of 0.1. Cube summing used a 10-best list for training and a 7-best list for decoding unless otherwise specified. To obtain the translation lexicon (Trans) we first included the top three target words t for each s using p(s | t) &#215; p(t | s) to score target words. For any training sentence &#12296;s, t&#12297; and t j for which t j
&#8713; &#8899; n
i=1 Trans(s i), we added t j to Trans(s i )
for i = argmax i &#8242; &#8712;I p(s i &#8242;|t j ) &#215; p(t j |s i &#8242;), where I = {i : 0 &#8804; i &#8804; n &#8743; |Trans(s i )| &lt; q i }. We used q 0 = 10 and q &gt;0 = 5, restricting |Trans(NULL)| &#8804; 10 and |Trans(s)| &#8804; 5 for any s &#8712; &#931;. This made 191 of the development sentences unreachable by the model, leaving 743 sentences for learning &#952;.
During decoding, we generated lattices with all t &#8712; Trans(s i ) for 0 &#8804; i &#8804; n, for every position. We used &#961; = 0.9, causing states within 90% of the source sentence length to be final states. Between each pair of consecutive states, we pruned edges that fell outside a beam of 70% of the sum of edge weights (see &#167;4.1; edge weights use f lex , f dist , and f scov ) of all edges between those two states.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Our model permits training the system on the full set of parallel data, but we instead use the parallel data to estimate feature functions and learn &#952; on the development set.</text>
                  <doc_id>324</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>12 We trained using three iterations of SGA over the development data with a batch size of 1 and a fixed step size of 0.01.</text>
                  <doc_id>325</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We used l 2 regularization with a fixed, untuned coefficient of 0.1.</text>
                  <doc_id>326</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Cube summing used a 10-best list for training and a 7-best list for decoding unless otherwise specified.</text>
                  <doc_id>327</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>To obtain the translation lexicon (Trans) we first included the top three target words t for each s using p(s | t) &#215; p(t | s) to score target words.</text>
                  <doc_id>328</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>For any training sentence &#12296;s, t&#12297; and t j for which t j</text>
                  <doc_id>329</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>&#8713; &#8899; n</text>
                  <doc_id>330</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>i=1 Trans(s i), we added t j to Trans(s i )</text>
                  <doc_id>331</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>for i = argmax i &#8242; &#8712;I p(s i &#8242;|t j ) &#215; p(t j |s i &#8242;), where I = {i : 0 &#8804; i &#8804; n &#8743; |Trans(s i )| &lt; q i }.</text>
                  <doc_id>332</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We used q 0 = 10 and q &gt;0 = 5, restricting |Trans(NULL)| &#8804; 10 and |Trans(s)| &#8804; 5 for any s &#8712; &#931;.</text>
                  <doc_id>333</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>This made 191 of the development sentences unreachable by the model, leaving 743 sentences for learning &#952;.</text>
                  <doc_id>334</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>During decoding, we generated lattices with all t &#8712; Trans(s i ) for 0 &#8804; i &#8804; n, for every position.</text>
                  <doc_id>335</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>We used &#961; = 0.9, causing states within 90% of the source sentence length to be final states.</text>
                  <doc_id>336</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Between each pair of consecutive states, we pruned edges that fell outside a beam of 70% of the sum of edge weights (see &#167;4.1; edge weights use f lex , f dist , and f scov ) of all edges between those two states.</text>
                  <doc_id>337</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>3</index>
            <title>6.4 Feature Set Comparison</title>
            <text>Our first set of experiments compares feature sets commonly used in phrase- and syntax-based translation. In particular, we compare the effects of combining phrase features and syntactic features. The base model contains f lex , g lm , g reor , and
12 We made this choice both for similarity to standard MT
systems and a more rapid experiment cycle.
g cov . The results are shown in Table 4. The second row contains scores when adding in the eight f phr features. The second column shows scores when adding the 14 target syntax features (f att and f val ), and the third column adds to them the 14 additional tree-to-tree features (f qg ). We find large gains in BLEU by adding more features, and find that gains obtained through phrase features and syntactic features are partially additive, suggesting that these feature sets are making complementary contributions to translation quality.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>Our first set of experiments compares feature sets commonly used in phrase- and syntax-based translation.</text>
                  <doc_id>338</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>In particular, we compare the effects of combining phrase features and syntactic features.</text>
                  <doc_id>339</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The base model contains f lex , g lm , g reor , and</text>
                  <doc_id>340</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>12 We made this choice both for similarity to standard MT</text>
                  <doc_id>341</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>systems and a more rapid experiment cycle.</text>
                  <doc_id>342</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>g cov .</text>
                  <doc_id>343</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The results are shown in Table 4.</text>
                  <doc_id>344</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The second row contains scores when adding in the eight f phr features.</text>
                  <doc_id>345</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>The second column shows scores when adding the 14 target syntax features (f att and f val ), and the third column adds to them the 14 additional tree-to-tree features (f qg ).</text>
                  <doc_id>346</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>We find large gains in BLEU by adding more features, and find that gains obtained through phrase features and syntactic features are partially additive, suggesting that these feature sets are making complementary contributions to translation quality.</text>
                  <doc_id>347</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>4</index>
            <title>6.5 Varying k During Decoding</title>
            <text>For models without syntactic features, we constrained the decoder to produce dependency trees in which every word&#8217;s parent is immediately to its right and ignored syntactic features while scoring structures. This causes decoding to proceed leftto-right in the lattice, the way phrase-based decoders operate. Since these models do not search over trees, they are substantially faster during decoding than those that use syntactic features and do not require any pruning of the lattice. Therefore, we explored varying the value of k used during k-best cube decoding; results are shown in Fig. 2. Scores improve when we increase k up to 10, but not much beyond, and there is still a substantial gap (2.5 BLEU) between using phrase features with k = 20 and using all features with k = 5. Models without syntax perform poorly when using a very small k, due to their reliance on non-local language model and phrase features. By contrast, models with syntactic features, which are local in our decoder, perform relatively well even with k = 1.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>For models without syntactic features, we constrained the decoder to produce dependency trees in which every word&#8217;s parent is immediately to its right and ignored syntactic features while scoring structures.</text>
                  <doc_id>348</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>This causes decoding to proceed leftto-right in the lattice, the way phrase-based decoders operate.</text>
                  <doc_id>349</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Since these models do not search over trees, they are substantially faster during decoding than those that use syntactic features and do not require any pruning of the lattice.</text>
                  <doc_id>350</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>Therefore, we explored varying the value of k used during k-best cube decoding; results are shown in Fig.</text>
                  <doc_id>351</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>2.</text>
                  <doc_id>352</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>Scores improve when we increase k up to 10, but not much beyond, and there is still a substantial gap (2.5 BLEU) between using phrase features with k = 20 and using all features with k = 5.</text>
                  <doc_id>353</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
                <sentence>
                  <text>Models without syntax perform poorly when using a very small k, due to their reliance on non-local language model and phrase features.</text>
                  <doc_id>354</doc_id>
                  <sec_id>6</sec_id>
                </sentence>
                <sentence>
                  <text>By contrast, models with syntactic features, which are local in our decoder, perform relatively well even with k = 1.</text>
                  <doc_id>355</doc_id>
                  <sec_id>7</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>5</index>
            <title>6.6 QG Configuration Comparison</title>
            <text>We next compare different constraints on isomorphism between the source and target dependency
BLEU 0.55
0.50
0.45
0.40
0.35
0.30
0.25
0.20
Phrase + Syntactic Phrase Syntactic Neither
trees. To do this, we impose harsh penalties on some QDG configurations (&#167;3) by fixing their feature weights to &#8722;1000. Hence they are permitted only when absolutely necessary in training and rarely in decoding. 13 Each model uses all phrase and syntactic features; they differ only in the sets of configurations which have fixed negative weights.
Tab. 5 shows experimental results. The base &#8220;synchronous&#8221; model permits parent-child (a(&#964; t (j)) = &#964; s (a(j))), any configuration where a(j) = 0, including both words being linked to
NULL, and requires the root word in &#964; t to be linked
to the root word in &#964; s or to NULL(5 of our 14 configurations). The second row allows any configuration involving NULL, including those where t j aligns to a non-NULL word in s and its parent aligns to NULL, and allows the root in &#964; t to be linked to any word in &#964; s . Each subsequent row adds additional configurations (i.e., trains its &#952; rather than fixing it to &#8722;1000). In general, we see large improvements as we permit more configurations, and the largest jump occurs when we add the &#8220;sibling&#8221; configuration (&#964; s (a(&#964; t (j))) = &#964; s (a(j))). The BLEU score does not increase, however, when we permit all configurations in the final row of the table, and the METEOR score increases only slightly. While allowing certain categories of non-isomorphism clearly seems helpful, permitting arbitrary violations does not appear to be necessary for this dataset.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We next compare different constraints on isomorphism between the source and target dependency</text>
                  <doc_id>356</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>BLEU 0.55</text>
                  <doc_id>357</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.50</text>
                  <doc_id>358</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.45</text>
                  <doc_id>359</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.40</text>
                  <doc_id>360</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.35</text>
                  <doc_id>361</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.30</text>
                  <doc_id>362</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.25</text>
                  <doc_id>363</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>0.20</text>
                  <doc_id>364</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Phrase + Syntactic Phrase Syntactic Neither</text>
                  <doc_id>365</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>trees.</text>
                  <doc_id>366</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>To do this, we impose harsh penalties on some QDG configurations (&#167;3) by fixing their feature weights to &#8722;1000.</text>
                  <doc_id>367</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Hence they are permitted only when absolutely necessary in training and rarely in decoding.</text>
                  <doc_id>368</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>13 Each model uses all phrase and syntactic features; they differ only in the sets of configurations which have fixed negative weights.</text>
                  <doc_id>369</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Tab.</text>
                  <doc_id>370</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>5 shows experimental results.</text>
                  <doc_id>371</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>The base &#8220;synchronous&#8221; model permits parent-child (a(&#964; t (j)) = &#964; s (a(j))), any configuration where a(j) = 0, including both words being linked to</text>
                  <doc_id>372</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>NULL, and requires the root word in &#964; t to be linked</text>
                  <doc_id>373</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>to the root word in &#964; s or to NULL(5 of our 14 configurations).</text>
                  <doc_id>374</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The second row allows any configuration involving NULL, including those where t j aligns to a non-NULL word in s and its parent aligns to NULL, and allows the root in &#964; t to be linked to any word in &#964; s .</text>
                  <doc_id>375</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>Each subsequent row adds additional configurations (i.e., trains its &#952; rather than fixing it to &#8722;1000).</text>
                  <doc_id>376</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
                <sentence>
                  <text>In general, we see large improvements as we permit more configurations, and the largest jump occurs when we add the &#8220;sibling&#8221; configuration (&#964; s (a(&#964; t (j))) = &#964; s (a(j))).</text>
                  <doc_id>377</doc_id>
                  <sec_id>3</sec_id>
                </sentence>
                <sentence>
                  <text>The BLEU score does not increase, however, when we permit all configurations in the final row of the table, and the METEOR score increases only slightly.</text>
                  <doc_id>378</doc_id>
                  <sec_id>4</sec_id>
                </sentence>
                <sentence>
                  <text>While allowing certain categories of non-isomorphism clearly seems helpful, permitting arbitrary violations does not appear to be necessary for this dataset.</text>
                  <doc_id>379</doc_id>
                  <sec_id>5</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
          <subsection>
            <index>6</index>
            <title>6.7 Discussion</title>
            <text>We note that these results are not state-of-theart on this dataset (on this task, Moses/MERT achieves 0.6838 BLEU and 0.8523 METEOR with maximum phrase length 3). 14 Our aim has been to
13 In fact, the strictest &#8220;synchronous&#8221; model used the
almost-forbidden configurations in 2% of test sentences; this behavior disappears as configurations are legalized. 14 We believe one cause for this performance gap is the generation of the lattice and plan to address this in future work by allowing the phrase table to inform lattice generation.
illustrate how a single model can provide a controlled experimental framework for comparisons of features, of inference methods, and of constraints. Our findings show that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-totree configurations (a new feature in MT) are helpful for translation, and that substantial gains can be obtained by permitting certain types of nonisomorphism. We have validated cube summing and decoding as practical methods for approximate inference.
Our framework permits exploration of alternative objectives, alternative approximate inference techniques, additional hidden variables (e.g., Moses&#8217; phrase segmentation variable), and, of course, additional feature representations. The system is publicly available at www.ark.cs. cmu.edu/Quipu.</text>
            <paragraphs>
              <paragraph>
                <sentence>
                  <text>We note that these results are not state-of-theart on this dataset (on this task, Moses/MERT achieves 0.6838 BLEU and 0.8523 METEOR with maximum phrase length 3).</text>
                  <doc_id>380</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>14 Our aim has been to</text>
                  <doc_id>381</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>13 In fact, the strictest &#8220;synchronous&#8221; model used the</text>
                  <doc_id>382</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>almost-forbidden configurations in 2% of test sentences; this behavior disappears as configurations are legalized.</text>
                  <doc_id>383</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>14 We believe one cause for this performance gap is the generation of the lattice and plan to address this in future work by allowing the phrase table to inform lattice generation.</text>
                  <doc_id>384</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>illustrate how a single model can provide a controlled experimental framework for comparisons of features, of inference methods, and of constraints.</text>
                  <doc_id>385</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>Our findings show that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-totree configurations (a new feature in MT) are helpful for translation, and that substantial gains can be obtained by permitting certain types of nonisomorphism.</text>
                  <doc_id>386</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>We have validated cube summing and decoding as practical methods for approximate inference.</text>
                  <doc_id>387</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
              <paragraph>
                <sentence>
                  <text>Our framework permits exploration of alternative objectives, alternative approximate inference techniques, additional hidden variables (e.g., Moses&#8217; phrase segmentation variable), and, of course, additional feature representations.</text>
                  <doc_id>388</doc_id>
                  <sec_id>0</sec_id>
                </sentence>
                <sentence>
                  <text>The system is publicly available at www.ark.cs.</text>
                  <doc_id>389</doc_id>
                  <sec_id>1</sec_id>
                </sentence>
                <sentence>
                  <text>cmu.edu/Quipu.</text>
                  <doc_id>390</doc_id>
                  <sec_id>2</sec_id>
                </sentence>
              </paragraph>
            </paragraphs>
          </subsection>
        </subsections>
      </section>
      <section>
        <index>7</index>
        <title>7 Conclusion</title>
        <text>We presented feature-rich MT using a principled probabilistic framework that separates features from inference. Our novel decoder is based on efficient DP-based QG lattice parsing extended to handle &#8220;non-local&#8221; features using generic techniques that also support efficient parameter estimation. Controlled experiments permitted with this system show interesting trends in the use of syntactic features and constraints.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We presented feature-rich MT using a principled probabilistic framework that separates features from inference.</text>
              <doc_id>391</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>Our novel decoder is based on efficient DP-based QG lattice parsing extended to handle &#8220;non-local&#8221; features using generic techniques that also support efficient parameter estimation.</text>
              <doc_id>392</doc_id>
              <sec_id>1</sec_id>
            </sentence>
            <sentence>
              <text>Controlled experiments permitted with this system show interesting trends in the use of syntactic features and constraints.</text>
              <doc_id>393</doc_id>
              <sec_id>2</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
      <section>
        <index>8</index>
        <title>Acknowledgments</title>
        <text>We thank three anonymous EMNLP reviewers, David Smith, and Stephan Vogel for helpful comments and feedback that improved this paper. This research was supported by NSF IIS-0836431 and IIS-0844507, a grant from Google, and computational resources provided by Yahoo.</text>
        <paragraphs>
          <paragraph>
            <sentence>
              <text>We thank three anonymous EMNLP reviewers, David Smith, and Stephan Vogel for helpful comments and feedback that improved this paper.</text>
              <doc_id>394</doc_id>
              <sec_id>0</sec_id>
            </sentence>
            <sentence>
              <text>This research was supported by NSF IIS-0836431 and IIS-0844507, a grant from Google, and computational resources provided by Yahoo.</text>
              <doc_id>395</doc_id>
              <sec_id>1</sec_id>
            </sentence>
          </paragraph>
        </paragraphs>
        <subsections/>
      </section>
    </sections>
    <tables>
      <table>
        <id>1</id>
        <source>TableSeer</source>
        <caption>Table 1: Key notation. Feature factorings are elaborated in Tab. 2.</caption>
        <reference_text>In PAGE 2: ... We do not report state-of-the-art performance, but these experiments reveal inter- esting trends that will inform continued research. 2 Model ( Table1  explains notation.) Given a sentence s and its parse tree ?s, we formulate the translation problem as finding the target sentence t? (along with its parse tree ?? t and alignment a? to the source tree) such that3 ?t?, ?? t , a?? = argmax ?t,?t,a? p(t, ?t, a | s, ?s) (1) In order to include overlapping features and permit hidden variables during training, we use a single globally-normalized conditional log-linear model....</reference_text>
        <page_num>2</page_num>
        <head>
          <rows>
            <row>
              <cell>bination of the two.</cell>
              <cell>We quantify the effects</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>of our approximate inference.</cell>
              <cell>We explore the</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>2</id>
        <source>TableSeer</source>
        <caption>Table 2: Factoring of global feature collections g into f. xj i denotes ?xi, . . . xj? in sequence x = ?x1, . . .?. first(i, j) = mink:i?k?j (min(a(k))) and last(i, j) = maxk:i?k?j (max(a(k))).</caption>
        <reference_text>None</reference_text>
        <page_num>3</page_num>
        <head>
          <rows>
            <row>
              <cell>gtrans(s, a, t)</cell>
              <cell>=</cell>
              <cell>Pm</cell>
              <cell>P i?a(j) flex(si, tj) (3)</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>j=1</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>+ P</cell>
              <cell>i,j:1?i lt</cell>
              <cell>j?m fphr(slast(i,j) first(i,j), tj</cell>
            </row>
            <row>
              <cell>glm(t)</cell>
              <cell>=</cell>
              <cell>P</cell>
              <cell>Pm+1 j=1 fN(tj j?N+1) (4)</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>N?{2,3}</cell>
              <cell>N?{2,3}</cell>
            </row>
            <row>
              <cell>gsyn(t, ?t)</cell>
              <cell>=</cell>
              <cell>Pm</cell>
              <cell>j=1 fatt(tj, j, t?t(j), ?t(j))</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>+fval(tj, j, ??1</cell>
              <cell>+fval(tj, j, ??1 (j)) (5) t</cell>
            </row>
            <row>
              <cell>greor(s, ?s, a, t, ?t)</cell>
              <cell>=</cell>
              <cell>Pm</cell>
              <cell>P i?a(j) fdist(i, j) (6)</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>j=1</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>m X</cell>
            </row>
            <row>
              <cell>gtree2(?s, a, ?t)</cell>
              <cell>=</cell>
              <cell>fqg(a(j), a(?t(j)), j, ?t(j)) (7)</cell>
              <cell>fqg(a(j), a(?t(j)), j, ?t(j)) (7)</cell>
            </row>
            <row>
              <cell>None</cell>
              <cell>None</cell>
              <cell>j=1</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>4</id>
        <source>TET</source>
        <caption>Table 4: Feature set comparison (BLEU).</caption>
        <reference_text></reference_text>
        <page_num>7</page_num>
        <head>
          <rows>
            <row>
              <cell>Phrase</cell>
              <cell>Syntactic Features:</cell>
              <cell></cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>features:</cell>
              <cell>+f att &#8746; f val</cell>
              <cell>+f qg</cell>
            </row>
            <row>
              <cell></cell>
              <cell>(base)</cell>
              <cell>(target)</cell>
              <cell>(tree-to-tree)</cell>
            </row>
            <row>
              <cell>(base)</cell>
              <cell>0.3727</cell>
              <cell>0.4458</cell>
              <cell>0.4424</cell>
            </row>
            <row>
              <cell>+f phr</cell>
              <cell>0.4682</cell>
              <cell>0.4971</cell>
              <cell>0.5142</cell>
            </row>
          </rows>
        </body>
      </table>
      <table>
        <id>5</id>
        <source>TableSeer</source>
        <caption>Table 5: QG configuration comparison. The name of each configuration, following Smith and Eisner (2006), refers to the relationship between a(&#964; t(j)) and a(j) in &#964; s.</caption>
        <reference_text>None</reference_text>
        <page_num>9</page_num>
        <head>
          <rows>
            <row>
              <cell>QDG Configurations</cell>
              <cell>BLEU</cell>
              <cell>METEOR</cell>
            </row>
          </rows>
        </head>
        <body>
          <rows>
            <row>
              <cell>synchronous</cell>
              <cell>0.4008</cell>
              <cell>0.6949</cell>
            </row>
            <row>
              <cell>+ nulls, root-any</cell>
              <cell>0.4108</cell>
              <cell>0.6931</cell>
            </row>
            <row>
              <cell>+ child-parent, same node</cell>
              <cell>0.4337</cell>
              <cell>0.6815</cell>
            </row>
            <row>
              <cell>+ sibling</cell>
              <cell>0.4881</cell>
              <cell>0.7216</cell>
            </row>
            <row>
              <cell>+ grandparent/child</cell>
              <cell>0.5015</cell>
              <cell>0.7365</cell>
            </row>
            <row>
              <cell>+ c-command</cell>
              <cell>0.5156</cell>
              <cell>0.7441</cell>
            </row>
            <row>
              <cell>+ other</cell>
              <cell>0.5142</cell>
              <cell>0.7472</cell>
            </row>
          </rows>
        </body>
      </table>
    </tables>
    <references>
      <reference>
        <id>0</id>
        <authors>
          <author>H Alshawi</author>
          <author>S Bangalore</author>
          <author>S Douglas</author>
        </authors>
        <title>Learning dependency translation modles as colections of finite-state head transducers.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2000</date>
      </reference>
      <reference>
        <id>1</id>
        <authors>
          <author>S Banerjee</author>
          <author>A Lavie</author>
        </authors>
        <title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
        <publication>In Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>2</id>
        <authors>
          <author>J E Besag</author>
        </authors>
        <title>Statistical analysis of non-lattice data.</title>
        <publication>None</publication>
        <pages>24--179</pages>
        <date>1975</date>
      </reference>
      <reference>
        <id>3</id>
        <authors>
          <author>Blunsom</author>
          <author>M Osborne</author>
        </authors>
        <title>Probabilistic inference for machine translation.</title>
        <publication>In Proc. of EMNLP.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>4</id>
        <authors>
          <author>P Blunsom</author>
          <author>T Cohn</author>
          <author>M Osborne</author>
        </authors>
        <title>A discriminative latent variable model for statistical machine translation.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>5</id>
        <authors>
          <author>P F Brown</author>
          <author>S A Della Pietra</author>
          <author>V J Della Pietra</author>
          <author>R L Mercer</author>
        </authors>
        <title>The mathematics of statistical machine translation: Parameter estimation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1993</date>
      </reference>
      <reference>
        <id>6</id>
        <authors>
          <author>S Chen</author>
          <author>J Goodman</author>
        </authors>
        <title>An empirical study of smoothing techniques for language modeling.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1998</date>
      </reference>
      <reference>
        <id>7</id>
        <authors>
          <author>D Chiang</author>
          <author>Y Marton</author>
          <author>P Resnik</author>
        </authors>
        <title>Online large-margin training of syntactic and structural translation features.</title>
        <publication>In Proc. of EMNLP.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>8</id>
        <authors>
          <author>D Chiang</author>
        </authors>
        <title>A hierarchical phrase-based model for statistical machine translation.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>9</id>
        <authors>
          <author>D Chiang</author>
        </authors>
        <title>Hierarchical phrase-based translation.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>10</id>
        <authors>
          <author>M Collins</author>
        </authors>
        <title>Head-Driven Statistical Models for Natural Language Parsing.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>1999</date>
      </reference>
      <reference>
        <id>11</id>
        <authors>
          <author>D Das</author>
          <author>N A Smith</author>
        </authors>
        <title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
        <publication>In Proc. of ACL-IJCNLP.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>12</id>
        <authors>
          <author>Y Ding</author>
          <author>M Palmer</author>
        </authors>
        <title>Machine translation using probabilistic synchronous dependency insertion grammar.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>13</id>
        <authors>
          <author>J Eisner</author>
          <author>E Goldlust</author>
          <author>N A Smith</author>
        </authors>
        <title>Compiling Comp Ling: Practical weighted dynamic programming and the Dyna language.</title>
        <publication>In Proc. of HLTEMNLP.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>14</id>
        <authors>
          <author>J Eisner</author>
        </authors>
        <title>Bilexical grammars and a cubic-time probabilistic parser.</title>
        <publication>In Proc. of IWPT.</publication>
        <pages>None</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>15</id>
        <authors>
          <author>M Galley</author>
          <author>J Graehl</author>
          <author>K Knight</author>
          <author>D Marcu</author>
          <author>S DeNeefe</author>
          <author>W Wang</author>
          <author>I Thayer</author>
        </authors>
        <title>Scalable inference and training of context-rich syntactic translation models.</title>
        <publication>In Proc. of COLING-ACL.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>16</id>
        <authors>
          <author>K Gimpel</author>
          <author>N A Smith</author>
        </authors>
        <title>Rich sourceside context for statistical machine translation.</title>
        <publication>In Proc. of ACL-2008 Workshop on Statistical Machine Translation.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>17</id>
        <authors>
          <author>K Gimpel</author>
          <author>N A Smith</author>
        </authors>
        <title>Cube summing, approximate inference with non-local features, and dynamic programming without semirings.</title>
        <publication>In Proc. of EACL.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>18</id>
        <authors>
          <author>R Haque</author>
          <author>S K Naskar</author>
          <author>Y Ma</author>
          <author>A Way</author>
        </authors>
        <title>Using supertags as source language context in SMT.</title>
        <publication>In Proc. of EAMT.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>19</id>
        <authors>
          <author>L Huang</author>
          <author>D Chiang</author>
        </authors>
        <title>Forest rescoring: Faster decoding with integrated language models.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>20</id>
        <authors>
          <author>A Ittycheriah</author>
          <author>S Roukos</author>
        </authors>
        <title>Direct translation model 2. In</title>
        <publication>Proc. of HLT-NAACL.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>21</id>
        <authors>
          <author>D Klein</author>
          <author>C D Manning</author>
        </authors>
        <title>Fast exact inference with a factored model for natural language parsing.</title>
        <publication>In Advances in NIPS 15.</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>22</id>
        <authors>
          <author>D Klein</author>
          <author>C D Manning</author>
        </authors>
        <title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2004</date>
      </reference>
      <reference>
        <id>23</id>
        <authors>
          <author>P Koehn</author>
          <author>F J Och</author>
          <author>D Marcu</author>
        </authors>
        <title>Statistical phrase-based translation.</title>
        <publication>In Proc. of HLT-NAACL.</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>24</id>
        <authors>
          <author>P Koehn</author>
          <author>H Hoang</author>
          <author>A Birch</author>
          <author>C Callison-Burch</author>
          <author>M Federico</author>
          <author>N Bertoldi</author>
          <author>B Cowan</author>
          <author>W Shen</author>
          <author>C Moran</author>
          <author>R Zens</author>
          <author>C Dyer</author>
          <author>O Bojar</author>
          <author>A Constantin</author>
          <author>E Herbst</author>
        </authors>
        <title>Moses: Open source toolkit for statistical machine translation.</title>
        <publication>In Proc. of ACL</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
      <reference>
        <id>25</id>
        <authors>
          <author>T Koo</author>
          <author>M Collins</author>
        </authors>
        <title>Hidden-variable models for discriminative reranking.</title>
        <publication>In Proc. of EMNLP.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>26</id>
        <authors>
          <author>P Liang</author>
          <author>A Bouchard-C&#244;t&#233;</author>
          <author>D Klein</author>
          <author>B Taskar</author>
        </authors>
        <title>An end-to-end discriminative approach to machine translation.</title>
        <publication>In Proc. of COLING-ACL.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>27</id>
        <authors>
          <author>A Lopez</author>
        </authors>
        <title>Translation as weighted deduction.</title>
        <publication>In Proc. of EACL.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>28</id>
        <authors>
          <author>D Marcu</author>
          <author>W Wang</author>
          <author>A Echihabi</author>
          <author>K Knight</author>
        </authors>
        <title>Statistical machine translation with syntactified target language phrases.</title>
        <publication>In Proc. of EMNLP.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>29</id>
        <authors>
          <author>H Mi</author>
          <author>L Huang</author>
          <author>Q Liu</author>
        </authors>
        <title>Forest-based translation.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>30</id>
        <authors>
          <author>F J Och</author>
          <author>H Ney</author>
        </authors>
        <title>Discriminative training and maximum entropy models for statistical machine translation.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>31</id>
        <authors>
          <author>F J Och</author>
          <author>H Ney</author>
        </authors>
        <title>A systematic comparison of various statistical alignment models.</title>
        <publication>None</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>32</id>
        <authors>
          <author>F J Och</author>
        </authors>
        <title>Minimum error rate training for statistical machine translation.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2003</date>
      </reference>
      <reference>
        <id>33</id>
        <authors>
          <author>K Papineni</author>
          <author>S Roukos</author>
          <author>T Ward</author>
        </authors>
        <title>Featurebased language understanding.</title>
        <publication>In EUROSPEECH.</publication>
        <pages>None</pages>
        <date>1997</date>
      </reference>
      <reference>
        <id>34</id>
        <authors>
          <author>K Papineni</author>
          <author>S Roukos</author>
          <author>T Ward</author>
          <author>W J Zhu</author>
        </authors>
        <title>BLEU: a method for automatic evaluation of machine translation.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2001</date>
      </reference>
      <reference>
        <id>35</id>
        <authors>
          <author>F C N Pereira</author>
          <author>Y Schabes</author>
        </authors>
        <title>Inside-outside reestimation from partially bracketed corpora.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>1992</date>
      </reference>
      <reference>
        <id>36</id>
        <authors>
          <author>C Quirk</author>
          <author>A Menezes</author>
          <author>C Cherry</author>
        </authors>
        <title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2005</date>
      </reference>
      <reference>
        <id>37</id>
        <authors>
          <author>L Shen</author>
          <author>J Xu</author>
          <author>R Weischedel</author>
        </authors>
        <title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
        <publication>In Proc. of ACL.</publication>
        <pages>None</pages>
        <date>2008</date>
      </reference>
      <reference>
        <id>38</id>
        <authors>
          <author>D A Smith</author>
          <author>J Eisner</author>
        </authors>
        <title>Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies.</title>
        <publication>In Proc. of HLT-NAACL Workshop on Statistical Machine Translation.</publication>
        <pages>None</pages>
        <date>2006</date>
      </reference>
      <reference>
        <id>39</id>
        <authors>
          <author>D A Smith</author>
          <author>J Eisner</author>
        </authors>
        <title>Parser adaptation and projection with quasi-synchronous features.</title>
        <publication>In Proc. of EMNLP.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>40</id>
        <authors>
          <author>A Stolcke</author>
        </authors>
        <title>SRILM&#8212;an extensible language modeling toolkit.</title>
        <publication>In Proc. of ICSLP.</publication>
        <pages>None</pages>
        <date>2002</date>
      </reference>
      <reference>
        <id>41</id>
        <authors>
          <author>X Sun</author>
          <author>J Tsujii</author>
        </authors>
        <title>Sequential labeling with latent variables: An exact inference algorithm and its efficient approximation.</title>
        <publication>In Proc. of EACL.</publication>
        <pages>None</pages>
        <date>2009</date>
      </reference>
      <reference>
        <id>42</id>
        <authors>
          <author>M Wang</author>
          <author>N A Smith</author>
          <author>T Mitamura</author>
        </authors>
        <title>What is the Jeopardy model? a quasi-synchronous grammar for QA.</title>
        <publication>In Proc. of EMNLP-CoNLL.</publication>
        <pages>None</pages>
        <date>2007</date>
      </reference>
    </references>
    <citations>
      <citation>
        <id>0</id>
        <reference_id>0</reference_id>
        <string>Alshawi et al., 2000</string>
        <sentence_id>4718</sentence_id>
        <char_offset>266</char_offset>
      </citation>
      <citation>
        <id>1</id>
        <reference_id>1</reference_id>
        <string>Banerjee and Lavie, 2005</string>
        <sentence_id>4909</sentence_id>
        <char_offset>117</char_offset>
      </citation>
      <citation>
        <id>2</id>
        <reference_id>2</reference_id>
        <string>Besag, 1975</string>
        <sentence_id>4626</sentence_id>
        <char_offset>120</char_offset>
      </citation>
      <citation>
        <id>3</id>
        <reference_id>3</reference_id>
        <string>Blunsom and Osborne, 2008</string>
        <sentence_id>4795</sentence_id>
        <char_offset>79</char_offset>
      </citation>
      <citation>
        <id>4</id>
        <reference_id>4</reference_id>
        <string>Blunsom et al., 2008</string>
        <sentence_id>4642</sentence_id>
        <char_offset>97</char_offset>
      </citation>
      <citation>
        <id>5</id>
        <reference_id>4</reference_id>
        <string>Blunsom et al., 2008</string>
        <sentence_id>4875</sentence_id>
        <char_offset>155</char_offset>
      </citation>
      <citation>
        <id>6</id>
        <reference_id>5</reference_id>
        <string>Brown et al., 1993</string>
        <sentence_id>4766</sentence_id>
        <char_offset>128</char_offset>
      </citation>
      <citation>
        <id>7</id>
        <reference_id>6</reference_id>
        <string>Chen and Goodman, 1998</string>
        <sentence_id>4915</sentence_id>
        <char_offset>145</char_offset>
      </citation>
      <citation>
        <id>8</id>
        <reference_id>7</reference_id>
        <string>Chiang et al., 2008</string>
        <sentence_id>4659</sentence_id>
        <char_offset>162</char_offset>
      </citation>
      <citation>
        <id>9</id>
        <reference_id>8</reference_id>
        <string>Chiang, 2005</string>
        <sentence_id>4646</sentence_id>
        <char_offset>263</char_offset>
      </citation>
      <citation>
        <id>10</id>
        <reference_id>9</reference_id>
        <string>Chiang (2007)</string>
        <sentence_id>4779</sentence_id>
        <char_offset>9</char_offset>
      </citation>
      <citation>
        <id>11</id>
        <reference_id>9</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>4614</sentence_id>
        <char_offset>107</char_offset>
      </citation>
      <citation>
        <id>12</id>
        <reference_id>9</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>4614</sentence_id>
        <char_offset>131</char_offset>
      </citation>
      <citation>
        <id>13</id>
        <reference_id>9</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>4625</sentence_id>
        <char_offset>146</char_offset>
      </citation>
      <citation>
        <id>14</id>
        <reference_id>9</reference_id>
        <string>Chiang, 2007</string>
        <sentence_id>4823</sentence_id>
        <char_offset>75</char_offset>
      </citation>
      <citation>
        <id>15</id>
        <reference_id>10</reference_id>
        <string>Collins, 1999</string>
        <sentence_id>4916</sentence_id>
        <char_offset>102</char_offset>
      </citation>
      <citation>
        <id>16</id>
        <reference_id>11</reference_id>
        <string>Das and Smith, 2009</string>
        <sentence_id>4631</sentence_id>
        <char_offset>156</char_offset>
      </citation>
      <citation>
        <id>17</id>
        <reference_id>12</reference_id>
        <string>Ding and Palmer, 2005</string>
        <sentence_id>4718</sentence_id>
        <char_offset>288</char_offset>
      </citation>
      <citation>
        <id>18</id>
        <reference_id>13</reference_id>
        <string>Eisner et al. (2005)</string>
        <sentence_id>4891</sentence_id>
        <char_offset>127</char_offset>
      </citation>
      <citation>
        <id>19</id>
        <reference_id>14</reference_id>
        <string>Eisner, 1997</string>
        <sentence_id>4736</sentence_id>
        <char_offset>142</char_offset>
      </citation>
      <citation>
        <id>20</id>
        <reference_id>15</reference_id>
        <string>Galley et al., 2006</string>
        <sentence_id>4658</sentence_id>
        <char_offset>65</char_offset>
      </citation>
      <citation>
        <id>21</id>
        <reference_id>16</reference_id>
        <string>Gimpel and Smith, 2008</string>
        <sentence_id>4659</sentence_id>
        <char_offset>118</char_offset>
      </citation>
      <citation>
        <id>22</id>
        <reference_id>17</reference_id>
        <string>Gimpel and Smith, 2009</string>
        <sentence_id>4625</sentence_id>
        <char_offset>160</char_offset>
      </citation>
      <citation>
        <id>23</id>
        <reference_id>17</reference_id>
        <string>Gimpel and Smith, 2009</string>
        <sentence_id>4822</sentence_id>
        <char_offset>131</char_offset>
      </citation>
      <citation>
        <id>24</id>
        <reference_id>18</reference_id>
        <string>Haque et al., 2009</string>
        <sentence_id>4659</sentence_id>
        <char_offset>142</char_offset>
      </citation>
      <citation>
        <id>25</id>
        <reference_id>19</reference_id>
        <string>Huang and Chiang, 2007</string>
        <sentence_id>4614</sentence_id>
        <char_offset>121</char_offset>
      </citation>
      <citation>
        <id>26</id>
        <reference_id>20</reference_id>
        <string>Ittycheriah and Roukos, 2007</string>
        <sentence_id>4646</sentence_id>
        <char_offset>277</char_offset>
      </citation>
      <citation>
        <id>27</id>
        <reference_id>21</reference_id>
        <string>Klein and Manning, 2003</string>
        <sentence_id>4918</sentence_id>
        <char_offset>100</char_offset>
      </citation>
      <citation>
        <id>28</id>
        <reference_id>22</reference_id>
        <string>Klein and Manning (2004)</string>
        <sentence_id>4916</sentence_id>
        <char_offset>165</char_offset>
      </citation>
      <citation>
        <id>29</id>
        <reference_id>23</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>4613</sentence_id>
        <char_offset>197</char_offset>
      </citation>
      <citation>
        <id>30</id>
        <reference_id>23</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>4641</sentence_id>
        <char_offset>56</char_offset>
      </citation>
      <citation>
        <id>31</id>
        <reference_id>23</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>4645</sentence_id>
        <char_offset>1</char_offset>
      </citation>
      <citation>
        <id>32</id>
        <reference_id>23</reference_id>
        <string>Koehn et al., 2003</string>
        <sentence_id>4680</sentence_id>
        <char_offset>149</char_offset>
      </citation>
      <citation>
        <id>33</id>
        <reference_id>24</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>4680</sentence_id>
        <char_offset>204</char_offset>
      </citation>
      <citation>
        <id>34</id>
        <reference_id>24</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>4762</sentence_id>
        <char_offset>36</char_offset>
      </citation>
      <citation>
        <id>35</id>
        <reference_id>24</reference_id>
        <string>Koehn et al., 2007</string>
        <sentence_id>4911</sentence_id>
        <char_offset>85</char_offset>
      </citation>
      <citation>
        <id>36</id>
        <reference_id>25</reference_id>
        <string>Koo and Collins, 2005</string>
        <sentence_id>4875</sentence_id>
        <char_offset>70</char_offset>
      </citation>
      <citation>
        <id>37</id>
        <reference_id>26</reference_id>
        <string>Liang et al., 2006</string>
        <sentence_id>4642</sentence_id>
        <char_offset>77</char_offset>
      </citation>
      <citation>
        <id>38</id>
        <reference_id>27</reference_id>
        <string>Lopez (2009)</string>
        <sentence_id>4615</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>39</id>
        <reference_id>28</reference_id>
        <string>Marcu et al., 2006</string>
        <sentence_id>4658</sentence_id>
        <char_offset>86</char_offset>
      </citation>
      <citation>
        <id>40</id>
        <reference_id>29</reference_id>
        <string>Mi et al. (2008)</string>
        <sentence_id>4657</sentence_id>
        <char_offset>100</char_offset>
      </citation>
      <citation>
        <id>41</id>
        <reference_id>30</reference_id>
        <string>Och and Ney, 2002</string>
        <sentence_id>4621</sentence_id>
        <char_offset>110</char_offset>
      </citation>
      <citation>
        <id>42</id>
        <reference_id>30</reference_id>
        <string>Och and Ney, 2002</string>
        <sentence_id>4640</sentence_id>
        <char_offset>81</char_offset>
      </citation>
      <citation>
        <id>43</id>
        <reference_id>31</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>4912</sentence_id>
        <char_offset>40</char_offset>
      </citation>
      <citation>
        <id>44</id>
        <reference_id>31</reference_id>
        <string>Och and Ney, 2003</string>
        <sentence_id>4919</sentence_id>
        <char_offset>133</char_offset>
      </citation>
      <citation>
        <id>45</id>
        <reference_id>32</reference_id>
        <string>Och, 2003</string>
        <sentence_id>4826</sentence_id>
        <char_offset>74</char_offset>
      </citation>
      <citation>
        <id>46</id>
        <reference_id>33</reference_id>
        <string>Papineni et al., 1997</string>
        <sentence_id>4621</sentence_id>
        <char_offset>87</char_offset>
      </citation>
      <citation>
        <id>47</id>
        <reference_id>34</reference_id>
        <string>Papineni et al., 2001</string>
        <sentence_id>4909</sentence_id>
        <char_offset>60</char_offset>
      </citation>
      <citation>
        <id>48</id>
        <reference_id>35</reference_id>
        <string>Pereira and Schabes, 1992</string>
        <sentence_id>4807</sentence_id>
        <char_offset>158</char_offset>
      </citation>
      <citation>
        <id>49</id>
        <reference_id>36</reference_id>
        <string>Quirk et al. (2005)</string>
        <sentence_id>4657</sentence_id>
        <char_offset>13</char_offset>
      </citation>
      <citation>
        <id>50</id>
        <reference_id>37</reference_id>
        <string>Shen et al., 2008</string>
        <sentence_id>4658</sentence_id>
        <char_offset>106</char_offset>
      </citation>
      <citation>
        <id>51</id>
        <reference_id>37</reference_id>
        <string>Shen et al., 2008</string>
        <sentence_id>4665</sentence_id>
        <char_offset>131</char_offset>
      </citation>
      <citation>
        <id>52</id>
        <reference_id>38</reference_id>
        <string>Smith and Eisner, 2006</string>
        <sentence_id>4608</sentence_id>
        <char_offset>100</char_offset>
      </citation>
      <citation>
        <id>53</id>
        <reference_id>38</reference_id>
        <string>Smith and Eisner, 2006</string>
        <sentence_id>4624</sentence_id>
        <char_offset>118</char_offset>
      </citation>
      <citation>
        <id>54</id>
        <reference_id>38</reference_id>
        <string>Smith and Eisner, 2006</string>
        <sentence_id>4708</sentence_id>
        <char_offset>45</char_offset>
      </citation>
      <citation>
        <id>55</id>
        <reference_id>38</reference_id>
        <string>Smith and Eisner, 2006</string>
        <sentence_id>4880</sentence_id>
        <char_offset>139</char_offset>
      </citation>
      <citation>
        <id>56</id>
        <reference_id>38</reference_id>
        <string>Smith and Eisner, 2006</string>
        <sentence_id>4925</sentence_id>
        <char_offset>54</char_offset>
      </citation>
      <citation>
        <id>57</id>
        <reference_id>38</reference_id>
        <string>Smith and Eisner (2006)</string>
        <sentence_id>4719</sentence_id>
        <char_offset>0</char_offset>
      </citation>
      <citation>
        <id>58</id>
        <reference_id>38</reference_id>
        <string>Smith and Eisner (2006)</string>
        <sentence_id>4721</sentence_id>
        <char_offset>5</char_offset>
      </citation>
      <citation>
        <id>59</id>
        <reference_id>39</reference_id>
        <string>Smith and Eisner, 2009</string>
        <sentence_id>4631</sentence_id>
        <char_offset>57</char_offset>
      </citation>
      <citation>
        <id>60</id>
        <reference_id>40</reference_id>
        <string>Stolcke, 2002</string>
        <sentence_id>4915</sentence_id>
        <char_offset>94</char_offset>
      </citation>
      <citation>
        <id>61</id>
        <reference_id>41</reference_id>
        <string>Sun and Tsujii, 2009</string>
        <sentence_id>4795</sentence_id>
        <char_offset>106</char_offset>
      </citation>
      <citation>
        <id>62</id>
        <reference_id>42</reference_id>
        <string>Wang et al., 2007</string>
        <sentence_id>4631</sentence_id>
        <char_offset>137</char_offset>
      </citation>
      <citation>
        <id>63</id>
        <reference_id>42</reference_id>
        <string>Wang et al., 2007</string>
        <sentence_id>4880</sentence_id>
        <char_offset>163</char_offset>
      </citation>
    </citations>
  </content>
</document>
